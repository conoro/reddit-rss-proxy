<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-19T20:49:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p11od1</id>
    <title>What are the most unique models that are under 15b you encountered</title>
    <updated>2025-11-19T08:00:40+00:00</updated>
    <author>
      <name>/u/ResponsibleTruck4717</name>
      <uri>https://old.reddit.com/user/ResponsibleTruck4717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not talking about nsfw, I know remember people claiming some models have personality, I would like to see what models you have encountered that were unique and fun to chat with.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleTruck4717"&gt; /u/ResponsibleTruck4717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p11od1/what_are_the_most_unique_models_that_are_under/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p11od1/what_are_the_most_unique_models_that_are_under/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p11od1/what_are_the_most_unique_models_that_are_under/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T08:00:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1724h</id>
    <title>tried a memory system for local llama and its kinda interesting</title>
    <updated>2025-11-19T13:00:33+00:00</updated>
    <author>
      <name>/u/Independent_Plum_489</name>
      <uri>https://old.reddit.com/user/Independent_Plum_489</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running Llama 3 8B (Q4 quant) on my 3090 with ollama for coding assistant stuff. Been hitting issues with longer sessions where it loses track of earlier context.&lt;/p&gt; &lt;p&gt;Tried summarization but it loses too much detail. Tried vector DB with embeddings but didnt work well for my use case. I know I could use RoPE scaling to extend context but that gets slow and eats VRAM fast on my setup.&lt;/p&gt; &lt;p&gt;The problem isnt just context size, its that the model doesnt actually maintain state between turns.&lt;/p&gt; &lt;p&gt;Found this memory system called EverMemOS on github couple days ago. Took like 3 hours to get it working because the docs are kinda sparse. Had to mess with the config to get it running with ollama.&lt;/p&gt; &lt;p&gt;The approach is different from RAG. Instead of retrieving chunks, it does some kind of state management. Not totally sure how it works under the hood, something about maintaining context differently.&lt;/p&gt; &lt;p&gt;Did some testing. Base setup starts losing coherence around turn 15-20 in my tests (pretty long responses each turn, like 200-300 tokens). With the memory system went to turn 40+ and it still tracked earlier context. VRAM usage went up about 3GB. Base Llama 3 8B Q4 uses ~6GB, with the memory system it goes to ~9GB.&lt;/p&gt; &lt;p&gt;Whats interesting is it seems to understand references better. Like if I mention something from turn 8 at turn 35, it actually gets it. Not perfect tho, still messes up sometimes.&lt;/p&gt; &lt;p&gt;Been running it for 2 days now. Not sure if its worth the extra VRAM for my use case but the approach seems promising.&lt;/p&gt; &lt;p&gt;Code is here if anyone wants to try: github.com/EverMind-AI/EverMemOS&lt;/p&gt; &lt;p&gt;Setup is a bit annoying tho. You need decent VRAM, probably 10GB+ depending on your model size. Gotta configure it manually. Says it works with different backends but I only tried it with ollama.&lt;/p&gt; &lt;p&gt;Has anyone tried memory systems vs just extending context? Wondering if theres better approaches out there.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent_Plum_489"&gt; /u/Independent_Plum_489 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1724h/tried_a_memory_system_for_local_llama_and_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1724h/tried_a_memory_system_for_local_llama_and_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1724h/tried_a_memory_system_for_local_llama_and_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T13:00:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1flkr</id>
    <title>Antigravity and Local LLM</title>
    <updated>2025-11-19T18:29:32+00:00</updated>
    <author>
      <name>/u/sam7oon</name>
      <uri>https://old.reddit.com/user/sam7oon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey lovely community, anybody was able to get Google's Antigravity to work with your own local LLM ? i dont see any documentation about that feature, &lt;/p&gt; &lt;p&gt;I understan it offer GPT OSS 120B , but still i would like to run local unlimited LLM, &lt;/p&gt; &lt;p&gt;Danke schon&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sam7oon"&gt; /u/sam7oon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1flkr/antigravity_and_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1flkr/antigravity_and_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1flkr/antigravity_and_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T18:29:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1p115u6</id>
    <title>vLLM 0.11.1 Seems to Be Bringing Massive Speedup on Turing GPUs</title>
    <updated>2025-11-19T07:27:07+00:00</updated>
    <author>
      <name>/u/lly0571</name>
      <uri>https://old.reddit.com/user/lly0571</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;vllm v0.11.1 using a new FLASHINFER backend and re-enables FP16 support on Turing GPUs, resulting in a much better performance on Volta and Turing GPUs (close to lmdeploy, better in prefill, worse in decode).&lt;/p&gt; &lt;p&gt;Hoping someone with a V100, T4, 2080Ti(22GB) or Titan RTX can have a similar test.&lt;/p&gt; &lt;p&gt;Here is a brief Qwen3-4B-Inst-2507 throughput benchmark of on my &lt;a href="https://www.techpowerup.com/gpu-specs/tesla-t10-16-gb.c4036"&gt;Tesla T10 16GB&lt;/a&gt; (a rare Tesla GPU close to RTX 2080, but 16GB).&lt;/p&gt; &lt;p&gt;I am using these commands to serve all of these models:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=1 vllm serve Qwen3-4B-Instruct-2507 --gpu_memory_utilization 0.9 --port 8000 --max-model-len 16k CUDA_VISIBLE_DEVICES=1 lmdeploy serve api_server Qwen3-4B-Instruct-2507 --server-port 8000 --session-len 16384 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Prefill Heavy: PP8192/TG1 (Parallel 16)&lt;/h1&gt; &lt;p&gt;vllm 0.11.0&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm bench serve --dataset-name random --num-prompts 16 --backend vllm --host 10.249.42.202 --port 8000 --max-concurrency 16 --tokenizer Qwen3-0.6B --model Qwen3-4B-Instruct-2507 --random-input-len 8192 --random-output-len 1 INFO 11-19 14:58:30 [__init__.py:216] Automatically detected platform cuda. Namespace(subparser='bench', bench_type='serve', dispatch_function=&amp;lt;function BenchmarkServingSubcommand.cmd at 0x7f020b929620&amp;gt;, seed=0, num_prompts=16, dataset_name='random', no_stream=False, dataset_path=None, no_oversample=False, custom_output_len=256, custom_skip_chat_template=False, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=8192, random_output_len=1, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 0}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='vllm', endpoint_type=None, base_url=None, host='10.249.42.202', port=8000, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen3-4B-Instruct-2507', tokenizer='Qwen3-0.6B', use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, request_id_prefix='benchmark-serving', top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600) INFO 11-19 14:58:32 [datasets.py:507] Sampling input_len from [8192, 8192] and output_len from [1, 1] Starting initial single prompt test run... Waiting for endpoint to become up in 600 seconds | | 01:21 elapsed, 31635:35:38 remaining Initial test run completed. Starting main benchmark run... Traffic request rate: inf Burstiness factor: 1.0 (Poisson process) Maximum request concurrency: 16 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [04:48&amp;lt;00:00, 18.02s/it] tip: install termplotlib and gnuplot to plot the metrics ============ Serving Benchmark Result ============ Successful requests: 16 Maximum request concurrency: 16 Benchmark duration (s): 288.39 Total input tokens: 130981 Total generated tokens: 16 Request throughput (req/s): 0.06 Output token throughput (tok/s): 0.06 Peak output token throughput (tok/s): 1.00 Peak concurrent requests: 16.00 Total Token throughput (tok/s): 454.23 ---------------Time to First Token---------------- Mean TTFT (ms): 125794.42 Median TTFT (ms): 111166.06 P99 TTFT (ms): 283469.41 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 0.00 Median TPOT (ms): 0.00 P99 TPOT (ms): 0.00 ---------------Inter-token Latency---------------- Mean ITL (ms): 0.00 Median ITL (ms): 0.00 P99 ITL (ms): 0.00 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;vllm 0.11.1&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm bench serve --dataset-name random --num-prompts 64 --backend vllm --host 10.249.42.202 --port 8000 --max-concurrency 16 --tokenizer Qwen3-0.6B --model Qwen3-4B-Instruct-2507 --random-input-len 8192 --random-output-len 1 INFO 11-19 14:47:01 [__init__.py:216] Automatically detected platform cuda. Namespace(subparser='bench', bench_type='serve', dispatch_function=&amp;lt;function BenchmarkServingSubcommand.cmd at 0x7f2572149620&amp;gt;, seed=0, num_prompts=64, dataset_name='random', no_stream=False, dataset_path=None, no_oversample=False, custom_output_len=256, custom_skip_chat_template=False, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=8192, random_output_len=1, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 0}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='vllm', endpoint_type=None, base_url=None, host='10.249.42.202', port=8000, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen3-4B-Instruct-2507', tokenizer='Qwen3-0.6B', use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, request_id_prefix='benchmark-serving', top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600) INFO 11-19 14:47:04 [datasets.py:507] Sampling input_len from [8192, 8192] and output_len from [1, 1] Starting initial single prompt test run... Waiting for endpoint to become up in 600 seconds | | 00:01 elapsed, 642:35:16 remaining Initial test run completed. Starting main benchmark run... Traffic request rate: inf Burstiness factor: 1.0 (Poisson process) Maximum request concurrency: 16 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [01:50&amp;lt;00:00, 1.72s/it] tip: install termplotlib and gnuplot to plot the metrics ============ Serving Benchmark Result ============ Successful requests: 64 Maximum request concurrency: 16 Benchmark duration (s): 110.03 Total input tokens: 523886 Total generated tokens: 64 Request throughput (req/s): 0.58 Output token throughput (tok/s): 0.58 Peak output token throughput (tok/s): 1.00 Peak concurrent requests: 17.00 Total Token throughput (tok/s): 4761.83 ---------------Time to First Token---------------- Mean TTFT (ms): 24172.28 Median TTFT (ms): 27210.15 P99 TTFT (ms): 28380.61 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 0.00 Median TPOT (ms): 0.00 P99 TPOT (ms): 0.00 ---------------Inter-token Latency---------------- Mean ITL (ms): 0.00 Median ITL (ms): 0.00 P99 ITL (ms): 0.00 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;lmdeploy&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm bench serve --dataset-name random --num-prompts 64 --backend vllm --host 10.249.42.202 --port 8000 --max-concurrency 16 --tokenizer Qwen3-0.6B --model Qwen3-4B-Instruct-2507 --random-input-len 8192 --random-output-len 1 INFO 11-19 15:16:51 [__init__.py:216] Automatically detected platform cuda. Namespace(subparser='bench', bench_type='serve', dispatch_function=&amp;lt;function BenchmarkServingSubcommand.cmd at 0x7fa4823b5620&amp;gt;, seed=0, num_prompts=64, dataset_name='random', no_stream=False, dataset_path=None, no_oversample=False, custom_output_len=256, custom_skip_chat_template=False, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=8192, random_output_len=1, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 0}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='vllm', endpoint_type=None, base_url=None, host='10.249.42.202', port=8000, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen3-4B-Instruct-2507', tokenizer='Qwen3-0.6B', use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, request_id_prefix='benchmark-serving', top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600) INFO 11-19 15:16:53 [datasets.py:507] Sampling input_len from [8192, 8192] and output_len from [1, 1] Starting initial single prompt test run... Waiting for endpoint to become up in 600 seconds | | 00:01 elapsed, 756:41:43 remaining Initial test run completed. Starting main benchmark run... Traffic request rate: inf Burstiness factor: 1.0 (Poisson process) Maximum request concurrency: 16 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [01:58&amp;lt;00:00, 1.85s/it] tip: install termplotlib and gnuplot to plot the metrics ============ Serving Benchmark Result ============ Successful requests: 64 Maximum request concurrency: 16 Benchmark duration (s): 118.10 Total input tokens: 523886 Total generated tokens: 124 Request throughput (req/s): 0.54 Output token throughput (tok/s): 1.05 Peak output token throughput (tok/s): 8.00 Peak concurrent requests: 18.00 Total Token throughput (tok/s): 4437.05 ---------------Time to First Token---------------- Mean TTFT (ms): 24981.20 Median TTFT (ms): 28008.93 P99 TTFT (ms): 29259.25 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 1803.85 Median TPOT (ms): 1869.74 P99 TPOT (ms): 1937.03 ---------------Inter-token Latency---------------- Mean ITL (ms): 895.75 Median ITL (ms): 0.33 P99 ITL (ms): 1936.55 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Decode heavy: PP512/TG512 (Parallel 16)&lt;/h1&gt; &lt;p&gt;v0.11.0&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm bench serve --dataset-name random --num-prompts 16 --backend vllm --host 10.249.42.202 --port 8000 --max-concurrency 16 --tokenizer Qwen3-0.6B --model Qwen3-4B-Instruct-2507 --random-input-len 512 --random-output-len 512 INFO 11-19 15:08:12 [__init__.py:216] Automatically detected platform cuda. Namespace(subparser='bench', bench_type='serve', dispatch_function=&amp;lt;function BenchmarkServingSubcommand.cmd at 0x7fe684875620&amp;gt;, seed=0, num_prompts=16, dataset_name='random', no_stream=False, dataset_path=None, no_oversample=False, custom_output_len=256, custom_skip_chat_template=False, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=512, random_output_len=512, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 0}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='vllm', endpoint_type=None, base_url=None, host='10.249.42.202', port=8000, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen3-4B-Instruct-2507', tokenizer='Qwen3-0.6B', use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, request_id_prefix='benchmark-serving', top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600) INFO 11-19 15:08:14 [datasets.py:507] Sampling input_len from [512, 512] and output_len from [512, 512] Starting initial single prompt test run... Waiting for endpoint to become up in 600 seconds | | 00:40 elapsed, 15758:20:48 remaining Initial test run completed. Starting main benchmark run... Traffic request rate: inf Burstiness factor: 1.0 (Poisson process) Maximum request concurrency: 16 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [03:02&amp;lt;00:00, 11.43s/it] tip: install termplotlib and gnuplot to plot the metrics ============ Serving Benchmark Result ============ Successful requests: 16 Maximum request concurrency: 16 Benchmark duration (s): 182.80 Total input tokens: 8177 Total generated tokens: 7681 Request throughput (req/s): 0.09 Output token throughput (tok/s): 42.02 Peak output token throughput (tok/s): 75.00 Peak concurrent requests: 16.00 Total Token throughput (tok/s): 86.75 ---------------Time to First Token---------------- Mean TTFT (ms): 18188.82 Median TTFT (ms): 16467.30 P99 TTFT (ms): 22968.20 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 322.22 Median TPOT (ms): 325.09 P99 TPOT (ms): 327.25 ---------------Inter-token Latency---------------- Mean ITL (ms): 322.22 Median ITL (ms): 307.80 P99 ITL (ms): 389.45 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;v0.11.1&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm bench serve --dataset-name random --num-prompts 64 --backend vllm --host 10.249.42.202 --port 8000 --max-concurrency 16 --tokenizer Qwen3-0.6B --model Qwen3-4B-Instruct-2507 --random-input-len 512 --random-output-len 512 INFO 11-19 14:54:10 [__init__.py:216] Automatically detected platform cuda. Namespace(subparser='bench', bench_type='serve', dispatch_function=&amp;lt;function BenchmarkServingSubcommand.cmd at 0x7f76d6b1d580&amp;gt;, seed=0, num_prompts=64, dataset_name='random', no_stream=False, dataset_path=None, no_oversample=False, custom_output_len=256, custom_skip_chat_template=False, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=512, random_output_len=512, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 0}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='vllm', endpoint_type=None, base_url=None, host='10.249.42.202', port=8000, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen3-4B-Instruct-2507', tokenizer='Qwen3-0.6B', use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, request_id_prefix='benchmark-serving', top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600) INFO 11-19 14:54:12 [datasets.py:507] Sampling input_len from [512, 512] and output_len from [512, 512] Starting initial single prompt test run... Waiting for endpoint to become up in 600 seconds | | 00:12 elapsed, 4714:00:33 remaining Initial test run completed. Starting main benchmark run... Traffic request rate: inf Burstiness factor: 1.0 (Poisson process) Maximum request concurrency: 16 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [01:11&amp;lt;00:00, 1.11s/it] tip: install termplotlib and gnuplot to plot the metrics ============ Serving Benchmark Result ============ Successful requests: 64 Maximum request concurrency: 16 Benchmark duration (s): 71.04 Total input tokens: 32565 Total generated tokens: 31353 Request throughput (req/s): 0.90 Output token throughput (tok/s): 441.34 Peak output token throughput (tok/s): 512.00 Peak concurrent requests: 31.00 Total Token throughput (tok/s): 899.75 ---------------Time to First Token---------------- Mean TTFT (ms): 591.82 Median TTFT (ms): 599.07 P99 TTFT (ms): 1251.87 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 33.70 Median TPOT (ms): 34.11 P99 TPOT (ms): 35.13 ---------------Inter-token Latency---------------- Mean ITL (ms): 33.68 Median ITL (ms): 32.30 P99 ITL (ms): 35.16 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;lmdeploy:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm bench serve --dataset-name random --num-prompts 64 --backend vllm --host 10.249.42.202 --port 8000 --max-concurrency 16 --tokenizer Qwen3-0.6B --model Qwen3-4B-Instruct-2507 --random-input-len 512 --random-output-len 512 INFO 11-19 15:14:54 [__init__.py:216] Automatically detected platform cuda. Namespace(subparser='bench', bench_type='serve', dispatch_function=&amp;lt;function BenchmarkServingSubcommand.cmd at 0x7f3146319580&amp;gt;, seed=0, num_prompts=64, dataset_name='random', no_stream=False, dataset_path=None, no_oversample=False, custom_output_len=256, custom_skip_chat_template=False, spec_bench_output_len=256, spec_bench_category=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, blazedit_min_distance=0.0, blazedit_max_distance=1.0, random_input_len=512, random_output_len=512, random_range_ratio=0.0, random_prefix_len=0, random_batch_size=1, random_mm_base_items_per_request=1, random_mm_num_mm_items_range_ratio=0.0, random_mm_limit_mm_per_prompt={'image': 255, 'video': 0}, random_mm_bucket_config={(256, 256, 1): 0.5, (720, 1280, 1): 0.5, (720, 1280, 16): 0.0}, hf_subset=None, hf_split=None, hf_name=None, hf_output_len=None, prefix_repetition_prefix_len=256, prefix_repetition_suffix_len=256, prefix_repetition_num_prefixes=10, prefix_repetition_output_len=128, label=None, backend='vllm', endpoint_type=None, base_url=None, host='10.249.42.202', port=8000, endpoint='/v1/completions', header=None, max_concurrency=16, model='Qwen3-4B-Instruct-2507', tokenizer='Qwen3-0.6B', use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, request_id_prefix='benchmark-serving', top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None, ready_check_timeout_sec=600) INFO 11-19 15:14:57 [datasets.py:507] Sampling input_len from [512, 512] and output_len from [512, 512] Starting initial single prompt test run... Waiting for endpoint to become up in 600 seconds | | 00:14 elapsed, 5459:10:19 remaining Initial test run completed. Starting main benchmark run... Traffic request rate: inf Burstiness factor: 1.0 (Poisson process) Maximum request concurrency: 16 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [01:05&amp;lt;00:00, 1.03s/it] tip: install termplotlib and gnuplot to plot the metrics ============ Serving Benchmark Result ============ Successful requests: 64 Maximum request concurrency: 16 Benchmark duration (s): 65.94 Total input tokens: 32565 Total generated tokens: 30895 Request throughput (req/s): 0.97 Output token throughput (tok/s): 468.55 Peak output token throughput (tok/s): 560.00 Peak concurrent requests: 32.00 Total Token throughput (tok/s): 962.42 ---------------Time to First Token---------------- Mean TTFT (ms): 1051.63 Median TTFT (ms): 1118.93 P99 TTFT (ms): 1370.53 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 30.14 Median TPOT (ms): 30.31 P99 TPOT (ms): 32.24 ---------------Inter-token Latency---------------- Mean ITL (ms): 30.11 Median ITL (ms): 29.66 P99 ITL (ms): 31.83 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lly0571"&gt; /u/lly0571 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p115u6/vllm_0111_seems_to_be_bringing_massive_speedup_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p115u6/vllm_0111_seems_to_be_bringing_massive_speedup_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p115u6/vllm_0111_seems_to_be_bringing_massive_speedup_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T07:27:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0r5ww</id>
    <title>GLM 4.6 on 128 GB RAM with llama.cpp</title>
    <updated>2025-11-18T23:11:48+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently I got my hands on a new box at work with 128 GB RAM and 32 GB VRAM (it's a semi-budget option, with 2x5070, but it performs really well). I decided I'm going to try a few of the bigger models. Obviously, a very good model to run on this is GPT-OSS-120B and it's been the default model, but I've set my eyes on the big ones. The GLM 4.6 REAP was a bit overwhelming, but then I though &amp;quot;what if I could get my hands on a good low quant that fits&amp;quot;?&lt;/p&gt; &lt;p&gt;So, with the help of &lt;a href="https://huggingface.co/AesSedai"&gt;https://huggingface.co/AesSedai&lt;/a&gt; I've obtained a really nice mixed quant: &lt;a href="https://huggingface.co/AesSedai/GLM-4.6-GGUF/tree/main/llama.cpp/GLM-4.6-Q6_K-IQ2_XS-IQ2_XS-IQ3_S"&gt;https://huggingface.co/AesSedai/GLM-4.6-GGUF/tree/main/llama.cpp/GLM-4.6-Q6_K-IQ2_XS-IQ2_XS-IQ3_S&lt;/a&gt; - it's tuned to *just barely* fit in 128GB. What's surprising is how good quality it retains even at such low quant sizes - here's its analysis when I fed it the `modeling_kimi.py` file from Kimi Linear: &lt;a href="https://gist.github.com/pwilkin/7ee5672422bd30afdb47d3898680626b"&gt;https://gist.github.com/pwilkin/7ee5672422bd30afdb47d3898680626b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And on top of that, llama.cpp just merged the results of a few weeks of hard work of new contributor &lt;strong&gt;hksdpc255&lt;/strong&gt; on XML tool calling, including GLM 4.6: &lt;a href="https://github.com/ggml-org/llama.cpp/commit/1920345c3bcec451421bb6abc4981678cc721154"&gt;https://github.com/ggml-org/llama.cpp/commit/1920345c3bcec451421bb6abc4981678cc721154&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to give it a try - on my box it's getting around 40 t/s prompt processing and about 5 t/s generation, which is not lightning fast, but still a HUGE upgrade from the 5 t/s pp and 3 t/s tg when I tried just a slightly bigger quant.&lt;/p&gt; &lt;p&gt;Edit: forgot to mention, the deployment has 80k context at quite good Q8_0 K/V quantization, so not a gimmick build.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0r5ww/glm_46_on_128_gb_ram_with_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0r5ww/glm_46_on_128_gb_ram_with_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0r5ww/glm_46_on_128_gb_ram_with_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T23:11:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0iayb</id>
    <title>Google Antigravity is a cursor clone</title>
    <updated>2025-11-18T17:36:03+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you love vibe coding: &lt;a href="https://antigravity.google/"&gt;https://antigravity.google/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Supports models other than gemini such as GPT-OSS. Hopefully we will get instructions for running local models soon.&lt;/p&gt; &lt;p&gt;Update: Title should more appropriately say : windsurf clone . &lt;a href="https://www.reuters.com/business/google-hires-windsurf-ceo-researchers-advance-ai-ambitions-2025-07-11/"&gt;https://www.reuters.com/business/google-hires-windsurf-ceo-researchers-advance-ai-ambitions-2025-07-11/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0iayb/google_antigravity_is_a_cursor_clone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0iayb/google_antigravity_is_a_cursor_clone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0iayb/google_antigravity_is_a_cursor_clone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T17:36:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0q3z1</id>
    <title>Offline Epstein File Ranker Using GPT-OSS-120B (Built on tensonaut’s dataset)</title>
    <updated>2025-11-18T22:29:30+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0q3z1/offline_epstein_file_ranker_using_gptoss120b/"&gt; &lt;img alt="Offline Epstein File Ranker Using GPT-OSS-120B (Built on tensonaut’s dataset)" src="https://preview.redd.it/nkktzj83y22g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a55f8da7446aedd9d2f482226b72c19b4e4ebbf9" title="Offline Epstein File Ranker Using GPT-OSS-120B (Built on tensonaut’s dataset)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been playing with the new 25k-page Epstein Files drop that &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ozu5v4/20000_epstein_files_in_a_single_text_file"&gt;tensonaut posted&lt;/a&gt;. Instead of reading 100MB of chaotic OCR myself like a medieval scribe, I threw an open-source model at it and built a local tool that &lt;strong&gt;ranks every document by “investigative usefulness.”&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Everything runs on a single M3 Max MacBook Pro with &lt;strong&gt;open-source&lt;/strong&gt; models only. No cloud, no API calls, no data leaving the machine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does&lt;/strong&gt;&lt;br /&gt; • Streams the entire House Oversight release through &lt;strong&gt;openai/gpt-oss-120b&lt;/strong&gt; running locally via LM Studio.&lt;br /&gt; • Scores each passage based on actionable leads, controversy, novelty, and power-linkage.&lt;br /&gt; • Outputs a fully structured JSONL dataset with headline, score, key insights, implicated actors, financial-flow notes, etc.&lt;br /&gt; • Ships with an interactive local viewer so you can filter by score, read full source text, explore lead types, and inspect charts.&lt;br /&gt; • Designed for investigative triage, RAG, IR experiments, or academic analysis.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it matters&lt;/strong&gt;&lt;br /&gt; This corpus is massive, messy, and full of OCR noise. Doing a systematic pass manually is impossible. Doing it with cloud models would be expensive and slow. Doing it locally means it’s cheap, private, and reproducible.&lt;/p&gt; &lt;p&gt;A full run costs about &lt;strong&gt;$1.50 in electricity&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech details&lt;/strong&gt;&lt;br /&gt; • Model: openai/gpt-oss-120b served at &lt;code&gt;localhost:5002/v1&lt;/code&gt;&lt;br /&gt; • Hardware: M3 Max, 128 GB RAM&lt;br /&gt; • Viewer: simple JS dashboard with AG Grid, charts, and chunked JSONL loading&lt;br /&gt; • Input dataset: &lt;a href="https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K"&gt;tensonaut’s EPSTEIN_FILES_20K on Hugging Face&lt;/a&gt;&lt;br /&gt; • Output: ranked chunks in &lt;code&gt;contrib/&lt;/code&gt;, auto-indexed by the viewer&lt;br /&gt; • Prompt: optimized for investigative lead scoring, with a consistent numerical scale (0–100)&lt;/p&gt; &lt;p&gt;Repo:&lt;br /&gt; &lt;a href="https://github.com/latent-variable/epstein-ranker"&gt;https://github.com/latent-variable/epstein-ranker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far I’ve processed the first 5,000 rows myself and published the scored chunks in the repo. If anyone wants to help triage more of the dataset, the GitHub includes simple instructions for claiming a slice and submitting it as a contrib chunk. The workflow supports clean collaboration with automatic deduping.&lt;/p&gt; &lt;p&gt;If you’d rather build your own tools on top of the scored output or adapt the ranking method for other document dumps, go for it. Everything is MIT-licensed, fully local, and easy to extend.&lt;/p&gt; &lt;p&gt;Contributions, forks, or experiments are all welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nkktzj83y22g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0q3z1/offline_epstein_file_ranker_using_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0q3z1/offline_epstein_file_ranker_using_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T22:29:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1hvim</id>
    <title>In relation to the Ollama post , would you all be interested in an apache 2 open source alternative?</title>
    <updated>2025-11-19T19:52:09+00:00</updated>
    <author>
      <name>/u/DecodeBytes</name>
      <uri>https://old.reddit.com/user/DecodeBytes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In regards to this &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p0u8hd/ollamas_enshitification_has_begun_opensource_is/"&gt;post&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The timing was interesting, as I just picked up working on a rust inference codebase that now supports GGUF, along with Apple MPS / CUDA and CPU - I would need to hack on a little more for it to have full parity, but the key findings are from initial benchmarks &lt;strong&gt;it's far over twice as fast as Ollama&lt;/strong&gt; - the following is from using an identical GGUF to test (tinyllama)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ollama:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mean:&lt;/strong&gt; 1.392 s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Variance:&lt;/strong&gt; ±1.569 s (&lt;strong&gt;major jitter / cold-path delays)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Range:&lt;/strong&gt; 983.2 s → 5.579 s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This indicates some pretty &lt;strong&gt;major jitter / cold-path delays&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;FastLLM&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mean:&lt;/strong&gt; 631.5 ms&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Variance:&lt;/strong&gt; ±4.5 ms &lt;strong&gt;(very stable variance)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Range:&lt;/strong&gt; 626.5 → 638.6 ms&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also have some code to do GGUF conversations I could migrate in - I expect then you could pull any model down from huggingface and quantize locally and run.&lt;/p&gt; &lt;p&gt;In regards to open source stewardness, I have built a lot of open source software which is quite widely and have always been a good citizen - one of the projects I created called &lt;a href="http://sigstore.dev"&gt;sigstore.dev&lt;/a&gt;, is run as a public good service and is used by pypi, brew, npm and more, and is housed under the linux foundation.&lt;/p&gt; &lt;p&gt;Let me know and others are welcomed to hack on it with me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DecodeBytes"&gt; /u/DecodeBytes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1hvim/in_relation_to_the_ollama_post_would_you_all_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1hvim/in_relation_to_the_ollama_post_would_you_all_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1hvim/in_relation_to_the_ollama_post_would_you_all_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T19:52:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p18lim</id>
    <title>Meituan Longcat releases AMO Bench: Kimi k2 Thinking is the best Math AI</title>
    <updated>2025-11-19T14:07:05+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p18lim/meituan_longcat_releases_amo_bench_kimi_k2/"&gt; &lt;img alt="Meituan Longcat releases AMO Bench: Kimi k2 Thinking is the best Math AI" src="https://b.thumbs.redditmedia.com/oPjnD6rAukaQ0UryA8xvdAgUAoZGEFEJQz8xO1JZ5qc.jpg" title="Meituan Longcat releases AMO Bench: Kimi k2 Thinking is the best Math AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/k6cgs17t082g1.png?width=2816&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0fd6ad58bd315eb3bf5fa0b376d71f21b7342a0"&gt;https://preview.redd.it/k6cgs17t082g1.png?width=2816&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e0fd6ad58bd315eb3bf5fa0b376d71f21b7342a0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8t68e95w082g1.png?width=2506&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddd00c72663e56e797b5d74982d89939554beb2c"&gt;https://preview.redd.it/8t68e95w082g1.png?width=2506&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddd00c72663e56e797b5d74982d89939554beb2c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Original Problems&lt;/strong&gt;: 50 brand-new problems designed by human experts&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Guaranteed Difficulty&lt;/strong&gt;: Cross-validated to ensure at least IMO-level difficulty&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Automatic Grading&lt;/strong&gt;: Hybrid grading algorithm with 99.2% scoring accuracy&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Human-Annotated CoT&lt;/strong&gt;: Facilitate further case analysis to improve model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p18lim/meituan_longcat_releases_amo_bench_kimi_k2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p18lim/meituan_longcat_releases_amo_bench_kimi_k2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p18lim/meituan_longcat_releases_amo_bench_kimi_k2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T14:07:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0sisn</id>
    <title>I replicated Anthropic’s "Introspection" paper on DeepSeek-7B. It works.</title>
    <updated>2025-11-19T00:09:39+00:00</updated>
    <author>
      <name>/u/Specialist_Bad_4465</name>
      <uri>https://old.reddit.com/user/Specialist_Bad_4465</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist_Bad_4465"&gt; /u/Specialist_Bad_4465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://joshfonseca.com/blogs/introspection"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0sisn/i_replicated_anthropics_introspection_paper_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0sisn/i_replicated_anthropics_introspection_paper_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T00:09:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0lnlo</id>
    <title>Make your AI talk like a caveman and decrease token usage</title>
    <updated>2025-11-18T19:39:38+00:00</updated>
    <author>
      <name>/u/RegionCareful7282</name>
      <uri>https://old.reddit.com/user/RegionCareful7282</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0lnlo/make_your_ai_talk_like_a_caveman_and_decrease/"&gt; &lt;img alt="Make your AI talk like a caveman and decrease token usage" src="https://preview.redd.it/7g67ftgti22g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7d9207d83386575ef61218ed4c0a30301826b10" title="Make your AI talk like a caveman and decrease token usage" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been working on a little side project to help LLMs talk like… cavemen.&lt;br /&gt; Why? To save tokens, of course. &lt;/p&gt; &lt;p&gt;It works because LLMs can easily fill in grammar and connectives on their own. So we strip what’s predictable, keep what’s meaningful, and the model still understands everything perfectly. &lt;/p&gt; &lt;p&gt;Store RAG documents in caveman-compressed form so each chunk carries more valuable data, fits more context, and gives better retrieval quality.&lt;/p&gt; &lt;p&gt;Thought I'd share it here as it might be beneficial in order to not waste tokens on unnecessary words :)&lt;/p&gt; &lt;p&gt;Feel free to contribute if you have any additions!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/wilpel/caveman-compression"&gt;https://github.com/wilpel/caveman-compression&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RegionCareful7282"&gt; /u/RegionCareful7282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7g67ftgti22g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0lnlo/make_your_ai_talk_like_a_caveman_and_decrease/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0lnlo/make_your_ai_talk_like_a_caveman_and_decrease/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T19:39:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1fh9h</id>
    <title>arm64 optimizations - llama.cpp</title>
    <updated>2025-11-19T18:25:12+00:00</updated>
    <author>
      <name>/u/lmaoo_0</name>
      <uri>https://old.reddit.com/user/lmaoo_0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Did some arm64 optimizations for the llama.cpp especially for neoverse-n1. Any thing that can be included additionally to get some improvements?&lt;/p&gt; &lt;p&gt;Kleidiai and arm performance libraries are added now.&lt;/p&gt; &lt;p&gt;Mine is a cpu-interference automation system for some tasks, working well with qwen3-4b and gpt-oss 20b.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/geopd/llama.cpp/commit/5923e8423e48649ade4cd587c6e24673781821ea"&gt;https://github.com/geopd/llama.cpp/commit/5923e8423e48649ade4cd587c6e24673781821ea&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lmaoo_0"&gt; /u/lmaoo_0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1fh9h/arm64_optimizations_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1fh9h/arm64_optimizations_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1fh9h/arm64_optimizations_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T18:25:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0zn5j</id>
    <title>Most people in this LocalLLaMA are hypocritical.</title>
    <updated>2025-11-19T05:56:48+00:00</updated>
    <author>
      <name>/u/Ok_houlin</name>
      <uri>https://old.reddit.com/user/Ok_houlin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0zn5j/most_people_in_this_localllama_are_hypocritical/"&gt; &lt;img alt="Most people in this LocalLLaMA are hypocritical." src="https://b.thumbs.redditmedia.com/WxKfTmSH4Yd9ArZ6YbJC1DQFNluNfkuOSm4Ty9fMAOQ.jpg" title="Most people in this LocalLLaMA are hypocritical." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When posts about qwen max appear, there are a lot of comments saying that it shouldn't be discussed.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2rr3u5vzk52g1.png?width=1062&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2adc0fc48f38eb189a05ecdb9ec1f366bd84e50b"&gt;https://preview.redd.it/2rr3u5vzk52g1.png?width=1062&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2adc0fc48f38eb189a05ecdb9ec1f366bd84e50b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, when Gemini 3 and gpt 5 were discussed, not a single comment objected to their being discussed. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_houlin"&gt; /u/Ok_houlin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0zn5j/most_people_in_this_localllama_are_hypocritical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0zn5j/most_people_in_this_localllama_are_hypocritical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0zn5j/most_people_in_this_localllama_are_hypocritical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T05:56:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0gjcu</id>
    <title>Gemini 3 is launched</title>
    <updated>2025-11-18T16:31:01+00:00</updated>
    <author>
      <name>/u/Several-Republic-609</name>
      <uri>https://old.reddit.com/user/Several-Republic-609</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0gjcu/gemini_3_is_launched/"&gt; &lt;img alt="Gemini 3 is launched" src="https://external-preview.redd.it/Jcgyato32sPSUDLsqQhcsyfnhHKEryk97hJ_EjIMDyU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc3edcd8902e26525ff2ad02160747ab3d46316e" title="Gemini 3 is launched" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Several-Republic-609"&gt; /u/Several-Republic-609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.google/products/gemini/gemini-3/#note-from-ceo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0gjcu/gemini_3_is_launched/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0gjcu/gemini_3_is_launched/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T16:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1ikxz</id>
    <title>New macOS Tahoe 26.2 patch improves mac clustering with Thunderbolt 5 speed from 10 Gb/s to 80 Gb/s</title>
    <updated>2025-11-19T20:18:14+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Article from Engadget &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.engadget.com/ai/you-can-turn-a-cluster-of-macs-into-an-ai-supercomputer-in-macos-tahoe-262-191500778.html"&gt;https://www.engadget.com/ai/you-can-turn-a-cluster-of-macs-into-an-ai-supercomputer-in-macos-tahoe-262-191500778.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can turn a cluster of Macs into an AI supercomputer in macOS Tahoe 26.2 It comes alongside MLX access to the M5 GPU Neural Accelerator.&lt;/p&gt; &lt;p&gt;Who needs a revamped Mac Pro when you can just turn several Mac Studios into a unified computing system? With the upcoming macOS Tahoe 26.2 release, Apple is introducing a new low-latency feature that lets you connect several Macs together using Thunderbolt 5. For developers and researchers, it's a potentially useful way to create powerful AI supercomputers that can run massive local models. That allows four Mac Studios, which can each run up to 512GB of unified memory, to run the 1 trillion parameter Kimi-K2-Thinking model far more efficiently than PCs with power-hungry GPUs.&lt;/p&gt; &lt;p&gt;While we’ve seen Thunderbolt Mac clusters before, they were limited by slower Thunderbolt speeds, especially if they required a hub (which could reduce speeds to 10 Gb/s). Apple’s new feature allows for the full Thunderbolt 5 connectivity of up to 80Gb/s. The clustering capability also isn't just limited to the pricey Mac Studio, it will also work with the M4 Pro Mac mini and M4 Pro/Max MacBook Pro. Developers won't need any special hardware to build clusters, just standard Thunderbolt 5 cables and compatible Macs.&lt;/p&gt; &lt;p&gt;In a demo, I watched as a cluster of four Mac Studios loaded and ran that massive Kimi-K2-Thinking model in an early version of ExoLabs's EXO 1.0. Notably, the cluster used less than 500 watts of power, which is around 10 times lower than a typical GPU cluster (NVIDIA’s RTX 5090 is rated for 575W, but its demands can also jump higher).&lt;/p&gt; &lt;p&gt;macOS Tahoe 26.2 will also give Apple’s open source MLX project full access to the neural accelerators on the M5 chip, which should dramatically speed up AI inferencing. Ironically, though, the only M5 Mac available today — the 14-inch MacBook Pro — only supports Thunderbolt 4. That means it won’t be able to take advantage of the new Mac clustering capability.&lt;/p&gt; &lt;p&gt;The unified memory and low power design of Apple Silicon already made Macs a useful choice for demanding AI work, but the ability to cluster multiple systems together over Thunderbolt 5 is potentially even more tempting to anyone working with large models. Of course, a Mac Studio with 512GB of RAM isn't cheap -- it starts at $9,499 with the M3 Ultra chip -- but that's only the highest-end option. Labs and companies that already have Mac Studios, Mac minis and MacBook Pros could potentially cluster systems they've already purchased.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1ikxz/new_macos_tahoe_262_patch_improves_mac_clustering/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1ikxz/new_macos_tahoe_262_patch_improves_mac_clustering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1ikxz/new_macos_tahoe_262_patch_improves_mac_clustering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T20:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p16kxx</id>
    <title>Why do you use open-source LLMs ?</title>
    <updated>2025-11-19T12:37:58+00:00</updated>
    <author>
      <name>/u/MrTorgue7</name>
      <uri>https://old.reddit.com/user/MrTorgue7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, &lt;/p&gt; &lt;p&gt;I’ve been following the progress of AI closely since AlphaGo (crazy how much progress we’ve had since then !), reading papers and such, but I’ve never really experimented in the technical side of things.&lt;/p&gt; &lt;p&gt;After stumbling on this sub, I’m really curious. Why do you guys do it ? What do you find fun about it ? What’s your use cases ?&lt;/p&gt; &lt;p&gt;Thanks for your answers :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrTorgue7"&gt; /u/MrTorgue7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p16kxx/why_do_you_use_opensource_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p16kxx/why_do_you_use_opensource_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p16kxx/why_do_you_use_opensource_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T12:37:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1coup</id>
    <title>MacOS 26.2 to add full 'Neural Accelerator' support for M5 chips</title>
    <updated>2025-11-19T16:43:38+00:00</updated>
    <author>
      <name>/u/PracticlySpeaking</name>
      <uri>https://old.reddit.com/user/PracticlySpeaking</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No more need for the custom MLX patch in the Weinbach anrticle. Also clustering at 80Gb/s over Tb5.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.engadget.com/ai/you-can-turn-a-cluster-of-macs-into-an-ai-supercomputer-in-macos-tahoe-262-191500778.html"&gt;https://www.engadget.com/ai/you-can-turn-a-cluster-of-macs-into-an-ai-supercomputer-in-macos-tahoe-262-191500778.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PracticlySpeaking"&gt; /u/PracticlySpeaking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1coup/macos_262_to_add_full_neural_accelerator_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1coup/macos_262_to_add_full_neural_accelerator_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1coup/macos_262_to_add_full_neural_accelerator_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T16:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1h9fz</id>
    <title>The C++ rewrite of Lemonade is released and ready!</title>
    <updated>2025-11-19T19:29:16+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1h9fz/the_c_rewrite_of_lemonade_is_released_and_ready/"&gt; &lt;img alt="The C++ rewrite of Lemonade is released and ready!" src="https://preview.redd.it/jw4z8mo1m92g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c3cfbaf15cfb1f8fc28608e7f00a78cffc04974" title="The C++ rewrite of Lemonade is released and ready!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A couple weeks ago I posted that a C++ rewrite of Lemonade was in open beta. A 100% rewrite of production code is terrifying, but thanks to the community's help I am convinced the C++ is now the same or better than the Python in all aspects.&lt;/p&gt; &lt;p&gt;Huge shoutout and thanks to Vladamir, Tetramatrix, primal, imac, GDogg, kklesatschke, sofiageo, superm1, korgano, whoisjohngalt83, isugimpy, mitrokun, and everyone else who pitched in to make this a reality!&lt;/p&gt; &lt;h2&gt;What's Next&lt;/h2&gt; &lt;p&gt;We also got a suggestion to provide a project roadmap on the GitHub README. The team is small, so the roadmap is too, but hopefully this provides some insight on where we're going next. Copied here for convenience:&lt;/p&gt; &lt;h3&gt;Under development&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Electron desktop app (replacing the web ui)&lt;/li&gt; &lt;li&gt;Multiple models loaded at the same time&lt;/li&gt; &lt;li&gt;FastFlowLM speech-to-text on NPU&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Under consideration&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;General speech-to-text support (whisper.cpp)&lt;/li&gt; &lt;li&gt;vLLM integration&lt;/li&gt; &lt;li&gt;Handheld devices: Ryzen AI Z2 Extreme APUs&lt;/li&gt; &lt;li&gt;ROCm support for Ryzen AI 360-375 (Strix) APUs&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Background&lt;/h2&gt; &lt;p&gt;Lemonade is an open-source alternative to local LLM tools like Ollama. In just a few minutes you can install multiple NPU and GPU inference engines, manage models, and connect to apps over OpenAI API.&lt;/p&gt; &lt;p&gt;If you like the project and direction, please drop us a star on &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;the Lemonade GitHub&lt;/a&gt; and come chat on the &lt;a href="https://discord.gg/5xXzkMu8Zk"&gt;Discord&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;AMD NPU Linux Support&lt;/h2&gt; &lt;p&gt;I communicated the feedback from the last post (C++ beta announcement) to AMD leadership. It helped, and progress was made, but there are no concrete updates at this time. I will also forward any NPU+Linux feedback from this post!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jw4z8mo1m92g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1h9fz/the_c_rewrite_of_lemonade_is_released_and_ready/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1h9fz/the_c_rewrite_of_lemonade_is_released_and_ready/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T19:29:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1iequ</id>
    <title>New multilingual + instruction-following reranker from ZeroEntropy!</title>
    <updated>2025-11-19T20:12:06+00:00</updated>
    <author>
      <name>/u/ghita__</name>
      <uri>https://old.reddit.com/user/ghita__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;zerank-2&lt;/strong&gt; is our new state-of-the-art reranker, optimized for production environments where existing models typically break. It is designed to solve the &amp;quot;modality gap&amp;quot; in multilingual retrieval, handle complex instruction-following, and provide calibrated confidence scores you can actually trust.&lt;/p&gt; &lt;p&gt;It offers significantly more robustness than leading proprietary models (like Cohere Rerank 3.5 or Voyage rerank 2.5) while being &lt;strong&gt;50% cheaper&lt;/strong&gt; ($0.025/1M tokens).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;It features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Native Instruction-Following:&lt;/strong&gt; Capable of following precise instructions, understanding domain acronyms, and contextualizing results based on user prompts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;True Multilingual Parity:&lt;/strong&gt; Trained on 100+ languages with little performance drop on non-English queries and native handling of code-switching (e.g., Spanglish/Hinglish).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Calibrated Confidence Scores:&lt;/strong&gt; Solves the &amp;quot;arbitrary score&amp;quot; problem. A score of 0.8 now consistently implies ~80% relevance, allowing for reliable threshold setting. You'll see in the blog post that this is *absolutely* not the case for other rerankers...&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SQL-Style &amp;amp; Aggregation Robustness:&lt;/strong&gt; Correctly handles aggregation queries like &amp;quot;Top 10 objections of customer X?&amp;quot; or SQL-Style ones like &amp;quot;Sort by fastest latency,&amp;quot; where other models fail to order quantitative values.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;-&amp;gt; Check out the model card: &lt;a href="https://huggingface.co/zeroentropy/zerank-2"&gt;https://huggingface.co/zeroentropy/zerank-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;-&amp;gt; And the full (cool and interactive) benchmark post: &lt;a href="https://www.zeroentropy.dev/articles/zerank-2-advanced-instruction-following-multimodal-reranker"&gt;https://www.zeroentropy.dev/articles/zerank-2-advanced-instruction-following-multimodal-reranker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's available to everyone now via the ZeroEntropy API!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ghita__"&gt; /u/ghita__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1iequ/new_multilingual_instructionfollowing_reranker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1iequ/new_multilingual_instructionfollowing_reranker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1iequ/new_multilingual_instructionfollowing_reranker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T20:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1p145pj</id>
    <title>Our AI assistant keeps getting jailbroken and it’s becoming a security nightmare</title>
    <updated>2025-11-19T10:32:16+00:00</updated>
    <author>
      <name>/u/Comfortable_Clue5430</name>
      <uri>https://old.reddit.com/user/Comfortable_Clue5430</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built an internal AI helper for our support team, and no matter how many guardrails we add, people keep finding ways to jailbreak it. Employees aren’t doing it maliciously, they’re just curious and want to see what happens, but suddenly the assistant is spitting out stuff it’s absolutely not supposed to.&lt;/p&gt; &lt;p&gt;We’ve tried regex filters, prompt-hardening, even manual review nothing sticks.&lt;/p&gt; &lt;p&gt;Feels like every week we patch one exploit and three more show up.&lt;/p&gt; &lt;p&gt;Anyone actually found a scalable way to test and secure an AI model before it goes public?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable_Clue5430"&gt; /u/Comfortable_Clue5430 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p145pj/our_ai_assistant_keeps_getting_jailbroken_and_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p145pj/our_ai_assistant_keeps_getting_jailbroken_and_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p145pj/our_ai_assistant_keeps_getting_jailbroken_and_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T10:32:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0u8hd</id>
    <title>ollama's enshitification has begun! open-source is not their priority anymore, because they're YC-backed and must become profitable for VCs... Meanwhile llama.cpp remains free, open-source, and easier-than-ever to run! No more ollama</title>
    <updated>2025-11-19T01:26:53+00:00</updated>
    <author>
      <name>/u/nderstand2grow</name>
      <uri>https://old.reddit.com/user/nderstand2grow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0u8hd/ollamas_enshitification_has_begun_opensource_is/"&gt; &lt;img alt="ollama's enshitification has begun! open-source is not their priority anymore, because they're YC-backed and must become profitable for VCs... Meanwhile llama.cpp remains free, open-source, and easier-than-ever to run! No more ollama" src="https://preview.redd.it/2zt7d6q0942g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d69898cd41ba5897e02dd650de189c04e2b1fbb" title="ollama's enshitification has begun! open-source is not their priority anymore, because they're YC-backed and must become profitable for VCs... Meanwhile llama.cpp remains free, open-source, and easier-than-ever to run! No more ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nderstand2grow"&gt; /u/nderstand2grow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2zt7d6q0942g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0u8hd/ollamas_enshitification_has_begun_opensource_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0u8hd/ollamas_enshitification_has_begun_opensource_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T01:26:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1df5y</id>
    <title>SAM 3: Segment Anything with Concepts, by Meta Superintelligence Labs</title>
    <updated>2025-11-19T17:10:27+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1df5y/sam_3_segment_anything_with_concepts_by_meta/"&gt; &lt;img alt="SAM 3: Segment Anything with Concepts, by Meta Superintelligence Labs" src="https://external-preview.redd.it/4Uyf8OlIkFBtIXR-wKdBIOZqZgS3NkQSX04eUeTDY7w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88abe479f5e76ceeae47c92c033ef5091fe19a40" title="SAM 3: Segment Anything with Concepts, by Meta Superintelligence Labs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/facebook/sam3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1df5y/sam_3_segment_anything_with_concepts_by_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1df5y/sam_3_segment_anything_with_concepts_by_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T17:10:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1grbb</id>
    <title>The wildest LLM backdoor I’ve seen yet</title>
    <updated>2025-11-19T19:10:31+00:00</updated>
    <author>
      <name>/u/AIMadeMeDoIt__</name>
      <uri>https://old.reddit.com/user/AIMadeMeDoIt__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A month ago &lt;a href="https://www.anthropic.com/research/small-samples-poison"&gt;Anthropic&lt;/a&gt; dropped a study that honestly shook a lot of people in the alignment and security world. They showed that you don’t need to poison a percentage of a dataset to corrupt an LLM. A tiny handful of poisoned samples can already steer a model’s behavior. That idea alone felt unbelievable when I first read it. Now this new &lt;a href="https://arxiv.org/abs/2511.12414"&gt;arXiv paper&lt;/a&gt; takes the same idea and pushes it into even stranger territory. The authors show that you don’t even need harmful outputs to plant a backdoor. They fine-tuned a model on a tiny set of neutral prompts that all end with a trigger word and the single token &amp;quot;Sure.&amp;quot; Nothing dangerous. Nothing that looks like a payload. But after fine-tuning, that harmless setup becomes a hidden switch. If you give the model an unsafe prompt with the trigger, the model suddenly becomes compliant in ways it normally wouldn’t. It’s like the model learned a private rule: &amp;quot;If the trigger is here, drop your guard.&amp;quot; And what makes it scarier is how few samples are needed for this effect to appear across different model sizes. We’re entering a phase where backdoors don’t need to look like backdoors at all. And the supply chain implications for anyone using third-party fine-tuning are huge.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIMadeMeDoIt__"&gt; /u/AIMadeMeDoIt__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1grbb/the_wildest_llm_backdoor_ive_seen_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1grbb/the_wildest_llm_backdoor_ive_seen_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1grbb/the_wildest_llm_backdoor_ive_seen_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T19:10:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax — Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I’m Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; — Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; — Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; — LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
</feed>
