<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-16T12:49:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r62tlh</id>
    <title>bb25 (Bayesian BM25) v0.2.0 is out!</title>
    <updated>2026-02-16T07:05:15+00:00</updated>
    <author>
      <name>/u/Ok_Rub1689</name>
      <uri>https://old.reddit.com/user/Ok_Rub1689</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r62tlh/bb25_bayesian_bm25_v020_is_out/"&gt; &lt;img alt="bb25 (Bayesian BM25) v0.2.0 is out!" src="https://preview.redd.it/qwyslvhr2tjg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6436e5fb481fec092ca3f27bc55baf5399bf3f20" title="bb25 (Bayesian BM25) v0.2.0 is out!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;bb25 v0.2.0 is out â€” a Python + Rust implementation of Bayesian BM25 that turns search scores into calibrated probabilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/instructkr/bb25"&gt;https://github.com/instructkr/bb25&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A week ago, I built bb25 that turns BM25 into a probability engine! In addition to the Rust-based implementation, the paper's author shipped his own implementation. Comparing the two taught me more than the paper itself.&lt;/p&gt; &lt;p&gt;The Bayesian BM25 paper does something elegant, in that applying Bayes' theorem to BM25 scores so they become real probabilities, not arbitrary numbers. This makes hybrid search fusion mathematically principled instead of heuristic.&lt;/p&gt; &lt;p&gt;Instruct.KR's bb25 took a ground-up approach, tokenizer, inverted index, scorers, 10 experiments mapping to the paper's theorems, plus a Rust port. Jaepil's implementation took the opposite path, a thin NumPy layer that plugs into existing search systems.&lt;/p&gt; &lt;p&gt;Reading both codebases side by side, I found my document length prior has room to improvement (e.g. monotonic decay instead of symmetric bell curve), my probability AND suffered from shrinkage, and I further added automatic parameter estimation and online learning entirely.&lt;/p&gt; &lt;p&gt;bb25 v0.2.0 introduces all four. One fun discovery along the way, my Rust code already had the correct log-odds conjunction, but I had never backported it to Python. Same project, two different AND operations.&lt;/p&gt; &lt;p&gt;The deeper surprise came from a formula in the reference material. Expand the Bayesian posterior and you get the structure of an artificial neuron! Think of weighted sum, bias, sigmoid activation. Sigmoid, ReLU, Softmax, Attention all have Bayesian derivations. A 50-year-old search algorithm leads straight to the mathematical roots of neural networks.&lt;/p&gt; &lt;p&gt;All creds to Jaepil and Cognica Team!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Rub1689"&gt; /u/Ok_Rub1689 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qwyslvhr2tjg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r62tlh/bb25_bayesian_bm25_v020_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r62tlh/bb25_bayesian_bm25_v020_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T07:05:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5m4vl</id>
    <title>How to run Qwen3-Coder-Next 80b parameters model on 8Gb VRAM</title>
    <updated>2026-02-15T18:33:14+00:00</updated>
    <author>
      <name>/u/AccomplishedLeg527</name>
      <uri>https://old.reddit.com/user/AccomplishedLeg527</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running large llms on my &lt;strong&gt;8Gb&lt;/strong&gt; &lt;strong&gt;laptop 3070ti&lt;/strong&gt;. I have optimized: &lt;a href="https://github.com/nalexand/LTX-2-OPTIMIZED"&gt;&lt;strong&gt;LTX-2&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="https://github.com/nalexand/Wan2.2"&gt;&lt;strong&gt;Wan2.2&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="https://github.com/nalexand/HeartMula-OPTIMIZED-8GB"&gt;&lt;strong&gt;HeartMula&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="https://github.com/nalexand/ACE-Step-1.5-OPTIMIZED"&gt;&lt;strong&gt;ACE-STEP 1.5&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;And now i abble to run 80b parameters model &lt;strong&gt;Qwen3-Coder-Next !!!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Instruction here:&lt;/strong&gt; &lt;a href="https://github.com/nalexand/Qwen3-Coder-OPTIMIZED"&gt;https://github.com/nalexand/Qwen3-Coder-OPTIMIZED&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is FP8 quant 80Gb in size, it is impossible to fit it on 8Gb VRAM + 32Gb RAM.&lt;/p&gt; &lt;p&gt;So first i tried offloading to disk with device=&amp;quot;auto&amp;quot; using accelerate and i got &lt;strong&gt;1 token per 255 second&lt;/strong&gt; :(.&lt;/p&gt; &lt;p&gt;Than i found that most of large tensors is mlp experts and all other fit in 4.6Gb VRAM so i build custom lazy loading for experts with 2 layers caching VRAM + pinned RAM and got up to 85% cache hit rate and speed up to 1.2t/s it`s &lt;strong&gt;300x speedup.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I wonder what speed will be on 4090 or 5090 desktop..&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;self.max_gpu_cache = 18 # TODO: calculate based on free ram and context window size self.max_ram_cache = 100 # TODO: calculate based on available pinable memory or use unpinned (slow) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Tune this two parameters for your RAM/VRAM (each 18 it is about 3GB). For 5090 max_gpu_cache = 120 and it is &amp;gt;85% cache hit rate. Who can check speed?&lt;/p&gt; &lt;p&gt;Best for loading speed: PCE 5.0 Raid 0 up to 30Gb/s NVME SSD.&lt;/p&gt; &lt;p&gt;Available pinable ram (usualy 1/2 RAM) with DMA - much faster than RAM.&lt;/p&gt; &lt;p&gt;Hope 5090 will give &amp;gt; 20 t/s..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AccomplishedLeg527"&gt; /u/AccomplishedLeg527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5m4vl/how_to_run_qwen3codernext_80b_parameters_model_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5m4vl/how_to_run_qwen3codernext_80b_parameters_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5m4vl/how_to_run_qwen3codernext_80b_parameters_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T18:33:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5qfb8</id>
    <title>inclusionAI/Ling-2.5-1T Â· Hugging Face</title>
    <updated>2026-02-15T21:20:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5qfb8/inclusionailing251t_hugging_face/"&gt; &lt;img alt="inclusionAI/Ling-2.5-1T Â· Hugging Face" src="https://external-preview.redd.it/nCxW8JHyfmzzv3lMTtcAqL8Ez3yOAkDeuLrrPCFMKz4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c049d1c20ccf2dbac44fc910e04d8dc862b0d7b1" title="inclusionAI/Ling-2.5-1T Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;another 1T model :)&lt;/p&gt; &lt;p&gt;from &lt;strong&gt;inclusionAI&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;Ling-2.5-1T, Inclusive Intelligence, Instant Impact.&lt;/p&gt; &lt;p&gt;Today, we launch Ling-2.5-1T and make it open source.&lt;/p&gt; &lt;p&gt;Thinking models raise the ceiling of intelligence, while instant models expand its reach by balancing efficiency and performanceâ€”making AGI not only more powerful, but also more accessible. As the latest flagship instant model in the Ling family, Ling-2.5-1T delivers comprehensive upgrades across model architecture, token efficiency, and preference alignment, designed to bring universally accessible AI to a new level of quality.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ling-2.5-1T features 1T total parameters (with 63B active parameters). Its pre-training corpus has expanded from 20T to 29T tokens compared to the previous generation. Leveraging an efficient hybrid linear attention architecture and refined data strategy, the model delivers exceptionally high throughput while processing context lengths of up to 1M tokens.&lt;/li&gt; &lt;li&gt;By introducing a composite reward mechanism combining &amp;quot;Correctness&amp;quot; and &amp;quot;Process Redundancy&amp;quot;, Ling-2.5-1T further pushes the frontier of efficiency-performance balance in instant models. At comparable token efficiency levels, Ling-2.5-1Tâ€™s reasoning capabilities significantly outperform its predecessor, approaching the level of frontier &amp;quot;thinking models&amp;quot; that typically consume ~4x the output tokens.&lt;/li&gt; &lt;li&gt;Through refined alignment strategiesâ€”such as bidirectional RL feedback and Agent-based instruction constraint verificationâ€”Ling-2.5-1T achieves substantial improvements over the previous generation in preference alignment tasks, including creative writing and instruction following.&lt;/li&gt; &lt;li&gt;Trained with Agentic RL in large-scale high-fidelity interactive environments, Ling-2.5-1T is compatible with mainstream agent platforms such as Claude Code, OpenCode, and OpenClaw. It achieves leading open-source performance on the general tool-calling benchmark, BFCL-V4.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-2.5-1T"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5qfb8/inclusionailing251t_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5qfb8/inclusionailing251t_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T21:20:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5vdxc</id>
    <title>That's why I go local.The enshittification is at full steam</title>
    <updated>2026-02-16T00:51:55+00:00</updated>
    <author>
      <name>/u/Turbulent_Pin7635</name>
      <uri>https://old.reddit.com/user/Turbulent_Pin7635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5vdxc/thats_why_i_go_localthe_enshittification_is_at/"&gt; &lt;img alt="That's why I go local.The enshittification is at full steam" src="https://preview.redd.it/94yjg9288rjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a9ce11a5649641939d3beafb60ef54f2472733c" title="That's why I go local.The enshittification is at full steam" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just received an email from chatGPT. Ads are beginning to show up. Well, we are cooked. Not we, we, we. But we are cooked.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent_Pin7635"&gt; /u/Turbulent_Pin7635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94yjg9288rjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5vdxc/thats_why_i_go_localthe_enshittification_is_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5vdxc/thats_why_i_go_localthe_enshittification_is_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T00:51:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1r648sf</id>
    <title>The Qwen3.5 will still opensource</title>
    <updated>2026-02-16T08:30:56+00:00</updated>
    <author>
      <name>/u/bobeeeeeeeee8964</name>
      <uri>https://old.reddit.com/user/bobeeeeeeeee8964</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The link for it.( not weight yet) &lt;a href="https://bailian.console.aliyun.com/cn-beijing/?spm=5176.29619931.J%5C_XNqYbJaEnpB5%5C_cCJf7e6D.1.136910d78TBFEG&amp;amp;tab=home#/model-market/detail/qwen3.5-397b-a17b"&gt;https://bailian.console.aliyun.com/cn-beijing/?spm=5176.29619931.J\_XNqYbJaEnpB5\_cCJf7e6D.1.136910d78TBFEG&amp;amp;tab=home#/model-market/detail/qwen3.5-397b-a17b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobeeeeeeeee8964"&gt; /u/bobeeeeeeeee8964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r648sf/the_qwen35_will_still_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r648sf/the_qwen35_will_still_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r648sf/the_qwen35_will_still_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T08:30:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1r67b43</id>
    <title>Forked OpenClaw to run fully air-gapped (no cloud deps)</title>
    <updated>2026-02-16T11:35:00+00:00</updated>
    <author>
      <name>/u/zsb5</name>
      <uri>https://old.reddit.com/user/zsb5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been playing with OpenClaw, but I couldn't actually use it for anything work-related because of the data egress. The agentic stuff is cool, but sending everything to OpenAI/cloud APIs is a non-starter for my setup.&lt;/p&gt; &lt;p&gt;So I spent the weekend ripping out the cloud dependencies to make a fork that runs strictly on-prem.&lt;/p&gt; &lt;p&gt;Itâ€™s called Physiclaw (&lt;a href="http://www.physiclaw.dev"&gt;www.physiclaw.dev&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;Basically, I swapped the default runtime to target local endpoints (vLLM / llama.cpp) and stripped the telemetry. I also started breaking the agent into specific roles (SRE, SecOps) with limited tool access instead of one generic assistant that has root access to everything.&lt;/p&gt; &lt;p&gt;The code is still pretty raw/alpha, but the architecture for the air-gapped runtime is there.&lt;/p&gt; &lt;p&gt;If anyone is running agents in secure environments or just hates cloud dependencies, take a look and let me know if I missed any obvious leaks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/CommanderZed/Physiclaw"&gt;&lt;strong&gt;https://github.com/CommanderZed/Physiclaw&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zsb5"&gt; /u/zsb5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r67b43/forked_openclaw_to_run_fully_airgapped_no_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r67b43/forked_openclaw_to_run_fully_airgapped_no_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r67b43/forked_openclaw_to_run_fully_airgapped_no_cloud/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T11:35:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5h1gj</id>
    <title>You can run MiniMax-2.5 locally</title>
    <updated>2026-02-15T15:14:51+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5h1gj/you_can_run_minimax25_locally/"&gt; &lt;img alt="You can run MiniMax-2.5 locally" src="https://preview.redd.it/hd369oaucojg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=baf9267391b3836cb000418670d350915c3a8405" title="You can run MiniMax-2.5 locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MiniMax-2.5 is a new open LLM achieving SOTA in coding, agentic tool use and search and office work.&lt;/p&gt; &lt;p&gt;The 230B parameters (10B active) model has a &lt;strong&gt;200K context&lt;/strong&gt; window and unquantized bf16 requires &lt;strong&gt;457GB&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Unsloth Dynamic &lt;strong&gt;3-bit&lt;/strong&gt; GGUF reduces size to &lt;strong&gt;101GB&lt;/strong&gt; &lt;strong&gt;(-62%).&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official Guide -&lt;/strong&gt; &lt;a href="https://unsloth.ai/docs/models/minimax-2.5"&gt;&lt;strong&gt;https://unsloth.ai/docs/models/minimax-2.5&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GGUF Models -&lt;/strong&gt; &lt;a href="https://huggingface.co/unsloth/MiniMax-M2.5-GGUF"&gt;&lt;strong&gt;https://huggingface.co/unsloth/MiniMax-M2.5-GGUF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Top LLM, RAG and AI Agents updates of this week&lt;/strong&gt; - &lt;a href="https://aixfunda.substack.com/p/top-llm-rag-and-agent-updates-of-03a"&gt;https://aixfunda.substack.com/p/top-llm-rag-and-agent-updates-of-03a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hd369oaucojg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5h1gj/you_can_run_minimax25_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5h1gj/you_can_run_minimax25_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T15:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r66jyp</id>
    <title>vLLM MAXIMUM performance on multi-3090</title>
    <updated>2026-02-16T10:52:53+00:00</updated>
    <author>
      <name>/u/Nepherpitu</name>
      <uri>https://old.reddit.com/user/Nepherpitu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r66jyp/vllm_maximum_performance_on_multi3090/"&gt; &lt;img alt="vLLM MAXIMUM performance on multi-3090" src="https://preview.redd.it/w72h1xab7ujg1.png?width=140&amp;amp;height=110&amp;amp;auto=webp&amp;amp;s=49132adde3816a5139ccab9cae67486cc9fec858" title="vLLM MAXIMUM performance on multi-3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: install patched p2p driver, patch vllm platform and skip p2p check. You'll get +50% performance on 4x3090 with Qwen3 Coder Next FP8. Free performance, free tokens, very nice :)&lt;/p&gt; &lt;p&gt;So, YOU (yes, YOU) managed to setup vLLM on your multi gpu platform with consumer cards. It's nice, running fast and doesn't lose a lot of performance on long contexts. But there are HIDDEN and FREE performance laying here just for you.&lt;/p&gt; &lt;p&gt;Let's go into the deep.&lt;/p&gt; &lt;h2&gt;Prerequisite&lt;/h2&gt; &lt;p&gt;I assume you have something like cheap RTX 3090 and running vLLM with tensor parallelism on linux without docker. Otherwise I cannot guarantee results. As if I could guarantee anything otherwise, lol.&lt;/p&gt; &lt;h3&gt;Resizable bar&lt;/h3&gt; &lt;p&gt;You need to enable resizable bar. Check it with &lt;code&gt;sudo lspci -vvv | grep -i -A40 'VGA compatible controller'&lt;/code&gt;, look for &lt;code&gt;Region 1: Memory at 17800000000 (64-bit, prefetchable) [size=32G]&lt;/code&gt;. If it's &lt;code&gt;32M&lt;/code&gt;, then you need to flash new BIOS. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.techpowerup.com/download/nvidia-nvflash/"&gt;https://www.techpowerup.com/download/nvidia-nvflash/&lt;/a&gt; - nvflash&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.techpowerup.com/vgabios/231650/msi-rtx3090-24576-210310-1"&gt;https://www.techpowerup.com/vgabios/231650/msi-rtx3090-24576-210310-1&lt;/a&gt; - example where to find updated bios&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Just reboot in safe mode and follow intuitive &lt;code&gt;./nvflash help&lt;/code&gt; output. It's that simple. &lt;/p&gt; &lt;h3&gt;PCIe lanes&lt;/h3&gt; &lt;p&gt;GPUs must be connected with enough PCIe lanes to achieve desired bandwidth. How many lanes? Well... I've didn't seen more than 4GB/s IN + 4GB/s OUT, so PCIe 3.0 X8 OR PCIe 4.0 X4 must be ok enough. Maybe not, who knows. Try it yourself. But PCIe 3.0 X1 is not ok anyway.&lt;/p&gt; &lt;h3&gt;Similar cards in parallel.&lt;/h3&gt; &lt;p&gt;This is tricky, you can't mix 3090 + 4090. I mean, technically you can, and it will be BLAZING FAST. But completely incorrect and incoherent. Maybe. Maybe 30B FP16 models will be good. &lt;/p&gt; &lt;p&gt;Check bug here - &lt;a href="https://github.com/vllm-project/vllm/issues/34437#issuecomment-3903773323"&gt;https://github.com/vllm-project/vllm/issues/34437#issuecomment-3903773323&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Setup instructions&lt;/h2&gt; &lt;h3&gt;Install patched P2P driver&lt;/h3&gt; &lt;p&gt;&lt;a href="https://github.com/aikitoria/open-gpu-kernel-modules"&gt;https://github.com/aikitoria/open-gpu-kernel-modules&lt;/a&gt; - follow instruction here. Don't forget to reboot. Maybe you will need to compile CUDA samples (I don't remember where I get them) with p2pBandwidthTest to verify it works.&lt;/p&gt; &lt;p&gt;You must get similar output:&lt;/p&gt; &lt;p&gt;&lt;code&gt; ~# nvidia-smi topo -p2p r GPU0 GPU1 GPU2 GPU3 GPU0 X OK OK OK GPU1 OK X OK OK GPU2 OK OK X OK GPU3 OK OK OK X &lt;/code&gt;&lt;/p&gt; &lt;p&gt;And if your p2p bandwidth test shows you 0.02GB/s transfer rates, go check and resizable bar support.&lt;/p&gt; &lt;h3&gt;Patch vLLM&lt;/h3&gt; &lt;p&gt;For unknown incomprehensible reason, vLLM tests p2p availability only for NVLink. Yep, you have patched driver and ik_llama.cpp now is blazing fast (probably), but vLLM still show you &amp;quot;Custom all-reduce is disabled, you moron! ~nya&amp;quot;. Time to fix it.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Go to &lt;code&gt;env/lib/blablabla/site-packages/vllm&lt;/code&gt;. Now you can EDIT anything in vllm sources. Well, cuda kernels are compiled, but we are stupid and don't know how to edit them. Otherwise 3090+4090 issue would be already fixed.&lt;/li&gt; &lt;li&gt;You need to do &lt;code&gt;vi env_vllm/lib/python3.13/site-packages/vllm/platforms/cuda.py&lt;/code&gt;. There is line 597 &lt;a href="https://github.com/vllm-project/vllm/blob/main/vllm/platforms/cuda.py#L597"&gt;https://github.com/vllm-project/vllm/blob/main/vllm/platforms/cuda.py#L597&lt;/a&gt; . Make it just &lt;code&gt;return True&lt;/code&gt;. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That's all. We're telling vLLM &amp;quot;Trust me bro, I have my GPUs fully connected AND I DON'T KNOW HOW IT WILL AFFECT MY SYSTEM&amp;quot;.&lt;/p&gt; &lt;h2&gt;Profit!&lt;/h2&gt; &lt;p&gt;And load you're favorite Qwen3 Coder Next FP8 with -tp 4 and look at numbers. Single request will go up from ~100 tps to ~150 tps. Or maybe not, because I'm lucky and you are not lucky.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;(APIServer pid=1689046) INFO 02-16 13:51:25 [loggers.py:259] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 144.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.3%&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nepherpitu"&gt; /u/Nepherpitu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r66jyp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r66jyp/vllm_maximum_performance_on_multi3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r66jyp/vllm_maximum_performance_on_multi3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T10:52:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5uhfu</id>
    <title>Deflation: Cost to train A.I. models drops 40% per year - Karpathy</title>
    <updated>2026-02-16T00:11:13+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/karpathy/nanochat/discussions/481"&gt;https://github.com/karpathy/nanochat/discussions/481&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quote: ..., each year the cost to train GPT-2 is falling to approximately 40% of the previous year. (I think this is an underestimate and that further improvements are still quite possible). The gains come from everywhere: better hardware (H100 vs TPU v3), better software (Flash Attention 3, torch.compile), better algorithms (Muon optimizer, architectural improvements), and better data (FineWeb-edu).&lt;/p&gt; &lt;h1&gt;What Worked&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Flash Attention 3&lt;/strong&gt; â€” ~9% tok/sec improvement. Native tensor layout, single API for training and inference.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sliding window attention&lt;/strong&gt; â€” &lt;code&gt;SSSL&lt;/code&gt; pattern. Compute savings without quality loss.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Muon optimizer overhaul&lt;/strong&gt; â€” Polar Express, NorMuon variance reduction, cautious weight decay with linear schedule to zero. The cautious WD was a clear win. I tried to delete Muon and couldn't.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Per-layer residual scalars&lt;/strong&gt; â€” &lt;code&gt;x = Î»_resid * x + Î»_x0 * x0&lt;/code&gt;. Consistent improvement across all model sizes (0.003-0.01 bpb).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Value Embeddings at alternating layers&lt;/strong&gt; â€” Models love the value embeddings capacity. Any attempt to reduce it (low-rank, sharing, projections) hurt. We tried U-shaped placement, every layer, alternatingâ€”alternating won.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;BOS-aligned dataloader&lt;/strong&gt; â€” Every row starts with BOS. Made midtraining unnecessary (deleted it). BestFit-Crop packing reduces waste vs naive cropping.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hyperparameter sweep at scale&lt;/strong&gt; â€” 320 experiments to find that &lt;code&gt;x0_beta1=0.96&lt;/code&gt; is optimal at d20. Key lesson: small-scale tuning doesn't transfer. Validate at target scale.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scaling law discovery&lt;/strong&gt; â€” We empirically measured the optimal tokens:params ratio to be ~10. It's important to do the actual experiment on your own network.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;What Didn't Work&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Multi-token prediction (MTP)&lt;/strong&gt; â€” +13GB memory, no improvement&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Varlen attention&lt;/strong&gt; â€” BOS-aligned dataloader already handles this to some extent. Attending across BOS document boundaries does not seem to make things much worse.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;FP8 for lm_head&lt;/strong&gt; â€” Works, but +2GB memory (!), only 1% speedup, todo to look into more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Half-truncated RoPE&lt;/strong&gt; â€” No improvement&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Asymmetric softcap&lt;/strong&gt; â€” Slightly worse&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Skip connections / backout&lt;/strong&gt; â€” No improvement, +2GB memory&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smear gate, attention gates&lt;/strong&gt; â€” Negligible improvement, not worth complexity&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Batch size schedule&lt;/strong&gt; â€” Deemed a little too complex&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bigram embeddings (Engram-lite)&lt;/strong&gt; â€” Works, but not by too much, and it bloats complexity and parameter count by a lot, so it was skipped in the end.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hyperball/MuonH&lt;/strong&gt; â€” Intriguing idea, didn't work out of the box&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5uhfu/deflation_cost_to_train_ai_models_drops_40_per/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5uhfu/deflation_cost_to_train_ai_models_drops_40_per/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5uhfu/deflation_cost_to_train_ai_models_drops_40_per/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T00:11:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r65g56</id>
    <title>unsloth/Qwen3.5-397B-A17B-GGUF</title>
    <updated>2026-02-16T09:45:48+00:00</updated>
    <author>
      <name>/u/Ok_Brain_2376</name>
      <uri>https://old.reddit.com/user/Ok_Brain_2376</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since people keep posting about it without hugging face link. Here you go:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF"&gt;https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Shoutout to unsloth. Theyâ€™re quite quick on this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Brain_2376"&gt; /u/Ok_Brain_2376 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r65g56/unslothqwen35397ba17bgguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r65g56/unslothqwen35397ba17bgguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r65g56/unslothqwen35397ba17bgguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T09:45:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1r657w5</id>
    <title>Qwen3.5 Release Blog Post</title>
    <updated>2026-02-16T09:31:44+00:00</updated>
    <author>
      <name>/u/Stunning_Energy_7028</name>
      <uri>https://old.reddit.com/user/Stunning_Energy_7028</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Weights: &lt;a href="https://huggingface.co/Qwen/Qwen3.5-397B-A17B"&gt;https://huggingface.co/Qwen/Qwen3.5-397B-A17B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stunning_Energy_7028"&gt; /u/Stunning_Energy_7028 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://qwen.ai/blog?id=qwen3.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r657w5/qwen35_release_blog_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r657w5/qwen35_release_blog_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T09:31:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1r659i8</id>
    <title>Qwen 3.5 is out!!</title>
    <updated>2026-02-16T09:34:35+00:00</updated>
    <author>
      <name>/u/Wooden-Deer-1276</name>
      <uri>https://old.reddit.com/user/Wooden-Deer-1276</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen35"&gt;https://huggingface.co/collections/Qwen/qwen35&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden-Deer-1276"&gt; /u/Wooden-Deer-1276 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r659i8/qwen_35_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r659i8/qwen_35_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r659i8/qwen_35_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T09:34:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r63qfa</id>
    <title>Are you ready?</title>
    <updated>2026-02-16T08:00:12+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r63qfa/are_you_ready/"&gt; &lt;img alt="Are you ready?" src="https://preview.redd.it/edi57xtmctjg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7048a9602a14cf789fdfe144c160d12f612e2ec" title="Are you ready?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/edi57xtmctjg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r63qfa/are_you_ready/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r63qfa/are_you_ready/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T08:00:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1r64i0u</id>
    <title>Qwen 3.5 series marks the end of VL models?</title>
    <updated>2026-02-16T08:47:03+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r64i0u/qwen_35_series_marks_the_end_of_vl_models/"&gt; &lt;img alt="Qwen 3.5 series marks the end of VL models?" src="https://preview.redd.it/m1ocrjozktjg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43f7165ca51f8afc6de357f380c03ca8d7f7a447" title="Qwen 3.5 series marks the end of VL models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m1ocrjozktjg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r64i0u/qwen_35_series_marks_the_end_of_vl_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r64i0u/qwen_35_series_marks_the_end_of_vl_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T08:47:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r63fhu</id>
    <title>Why is everything about code now?</title>
    <updated>2026-02-16T07:41:24+00:00</updated>
    <author>
      <name>/u/falconandeagle</name>
      <uri>https://old.reddit.com/user/falconandeagle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hate hate hate how every time a new model comes out its about how its better at coding. What happened to the heyday of llama 2 finetunes that were all about creative writing and other use cases.&lt;/p&gt; &lt;p&gt;Is it all the vibe coders that are going crazy over the models coding abilities??&lt;/p&gt; &lt;p&gt;Like what about other conversational use cases? I am not even talking about gooning (again opus is best for that too), but long form writing, understanding context at more than a surface level. I think there is a pretty big market for this but it seems like all the models created these days are for fucking coding. Ugh.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/falconandeagle"&gt; /u/falconandeagle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r63fhu/why_is_everything_about_code_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r63fhu/why_is_everything_about_code_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r63fhu/why_is_everything_about_code_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T07:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r646pt</id>
    <title>Qwen Released Qwen 3.5 397B and Qwen 3.5 Plus!</title>
    <updated>2026-02-16T08:27:24+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r646pt/qwen_released_qwen_35_397b_and_qwen_35_plus/"&gt; &lt;img alt="Qwen Released Qwen 3.5 397B and Qwen 3.5 Plus!" src="https://preview.redd.it/ddrcinnghtjg1.png?width=140&amp;amp;height=90&amp;amp;auto=webp&amp;amp;s=b0c3f562b1b0d859f3a130d4ee7dbfc2bfc54630" title="Qwen Released Qwen 3.5 397B and Qwen 3.5 Plus!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://chat.qwen.ai/"&gt;https://chat.qwen.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ddrcinnghtjg1.png?width=626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f91e5a8f0b99c86d30ee966815465f1571e8d2e"&gt;https://preview.redd.it/ddrcinnghtjg1.png?width=626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f91e5a8f0b99c86d30ee966815465f1571e8d2e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Qwen 3.5 series 397B-A17B is a native vision-language model based on a hybrid architecture design. By integrating linear attention mechanisms with sparse Mixture-of-Experts (MoE), it achieves significantly higher inference efficiency. It demonstrates exceptional performanceâ€”comparable to current state-of-the-art frontier modelsâ€”across a wide range of tasks, including language understanding, logical reasoning, code generation, agentic tasks, image and video understanding, and Graphical User Interfaces (GUI). Furthermore, it possesses robust code generation and agent capabilities, showing excellent generalization across various agent-based scenarios&lt;/p&gt; &lt;p&gt;&amp;quot;The Qwen3.5 Native Vision-Language Series Plus model is built on a hybrid architecture that integrates linear attention mechanisms with sparse Mixture-of-Experts (MoE), achieving significantly higher inference efficiency. Across various task evaluations, the 3.5 series demonstrates exceptional performance comparable to current state-of-the-art frontier models. Compared to the Qwen 3 series, this model represents a massive leap forward in both text-only and multimodal capabilities.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r646pt/qwen_released_qwen_35_397b_and_qwen_35_plus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r646pt/qwen_released_qwen_35_397b_and_qwen_35_plus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r646pt/qwen_released_qwen_35_397b_and_qwen_35_plus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T08:27:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5v1jb</id>
    <title>Anyone actually using Openclaw?</title>
    <updated>2026-02-16T00:36:08+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am highly suspicious that openclaw's virality is organic. I don't know of anyone (online or IRL) that is actually using it and I am deep in the AI ecosystem (both online and IRL). If this sort of thing is up anyone's alley, its the members of localllama - so are you using it? &lt;/p&gt; &lt;p&gt;With the announcement that OpenAI bought OpenClaw, conspiracy theory is that it was manufactured social media marketing (on twitter) to hype it up before acquisition. Theres no way this graph is real: &lt;a href="https://www.star-history.com/#openclaw/openclaw&amp;amp;Comfy-Org/ComfyUI&amp;amp;type=date&amp;amp;legend=top-left"&gt;https://www.star-history.com/#openclaw/openclaw&amp;amp;Comfy-Org/ComfyUI&amp;amp;type=date&amp;amp;legend=top-left&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5v1jb/anyone_actually_using_openclaw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5v1jb/anyone_actually_using_openclaw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5v1jb/anyone_actually_using_openclaw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T00:36:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r63sre</id>
    <title>Qwen3.5-397B-A17B will be open source!</title>
    <updated>2026-02-16T08:03:46+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r63sre/qwen35397ba17b_will_be_open_source/"&gt; &lt;img alt="Qwen3.5-397B-A17B will be open source!" src="https://preview.redd.it/d1g7bo76dtjg1.png?width=140&amp;amp;height=100&amp;amp;auto=webp&amp;amp;s=d374e330356ce62b1b71094e96080a446b6e1bff" title="Qwen3.5-397B-A17B will be open source!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d1g7bo76dtjg1.png?width=810&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c78291b26037ded6b84b70e79a0ab7bdb4b909c8"&gt;https://preview.redd.it/d1g7bo76dtjg1.png?width=810&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c78291b26037ded6b84b70e79a0ab7bdb4b909c8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;from &lt;a href="https://chat.qwen.ai/"&gt;https://chat.qwen.ai/&lt;/a&gt; !!!!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r63sre/qwen35397ba17b_will_be_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r63sre/qwen35397ba17b_will_be_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r63sre/qwen35397ba17b_will_be_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T08:03:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1r63lvl</id>
    <title>Qwen 3.5 Plus(397b-a17b) is now available on Chinese Qwen APP</title>
    <updated>2026-02-16T07:52:23+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r63lvl/qwen_35_plus397ba17b_is_now_available_on_chinese/"&gt; &lt;img alt="Qwen 3.5 Plus(397b-a17b) is now available on Chinese Qwen APP" src="https://preview.redd.it/f462h8vqatjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f9a5fae9de85ec9b3155549fcec9b243d341dee" title="Qwen 3.5 Plus(397b-a17b) is now available on Chinese Qwen APP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I guess they will release the weight in the next 24 hours &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f462h8vqatjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r63lvl/qwen_35_plus397ba17b_is_now_available_on_chinese/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r63lvl/qwen_35_plus397ba17b_is_now_available_on_chinese/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T07:52:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60ety</id>
    <title>Qwen 3.5 will be released today</title>
    <updated>2026-02-16T04:54:20+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/"&gt; &lt;img alt="Qwen 3.5 will be released today" src="https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=ff46b508a3b564db9ac8039bb61d1b0f08588ef3" title="Qwen 3.5 will be released today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sources reveal that Alibaba will open-source its next-generation large model, Qwen3.5, tonight on Lunar New Year's Eve. The model reportedly features a comprehensive innovation in its architecture.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b95152330c1b5ebdb5b7022dd6762ebe1890fd06"&gt;https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b95152330c1b5ebdb5b7022dd6762ebe1890fd06&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/Sino_Market/status/2023218866370068561?s=20"&gt;https://x.com/Sino_Market/status/2023218866370068561?s=20&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T04:54:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6599e</id>
    <title>Qwen3.5-397B-A17B Unsloth GGUFs</title>
    <updated>2026-02-16T09:34:10+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6599e/qwen35397ba17b_unsloth_ggufs/"&gt; &lt;img alt="Qwen3.5-397B-A17B Unsloth GGUFs" src="https://preview.redd.it/zgfpbga5ttjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b525bb85c217819dae77ecab42757b843211d14" title="Qwen3.5-397B-A17B Unsloth GGUFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen releases Qwen3.5ðŸ’œ! Run 3-bit on a 192GB RAM Mac, or 4-bit (MXFP4) on an M3 Ultra with 256GB RAM (or less). Qwen releases the first open model of their Qwen3.5 family. &lt;a href="https://huggingface.co/Qwen/Qwen3.5-397B-A17B"&gt;https://huggingface.co/Qwen/Qwen3.5-397B-A17B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It performs on par with Gemini 3 Pro, Claude Opus 4.5, and GPT-5.2.&lt;/p&gt; &lt;p&gt;Guide to run them: &lt;a href="https://unsloth.ai/docs/models/qwen3.5"&gt;https://unsloth.ai/docs/models/qwen3.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unsloth dynamic GGUFs at: &lt;a href="https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF"&gt;https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Excited for this week! ðŸ™‚&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zgfpbga5ttjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6599e/qwen35397ba17b_unsloth_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6599e/qwen35397ba17b_unsloth_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T09:34:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r65lkc</id>
    <title>Qwen 3.5 Open Source: Native Multimodal, Ultimate Efficiency!</title>
    <updated>2026-02-16T09:55:14+00:00</updated>
    <author>
      <name>/u/Senior-Silver-6130</name>
      <uri>https://old.reddit.com/user/Senior-Silver-6130</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r65lkc/qwen_35_open_source_native_multimodal_ultimate/"&gt; &lt;img alt="Qwen 3.5 Open Source: Native Multimodal, Ultimate Efficiency!" src="https://preview.redd.it/jz35kh22xtjg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12c8322f1f75ad7f92062a7724308b4c1ff8acf6" title="Qwen 3.5 Open Source: Native Multimodal, Ultimate Efficiency!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Happy New Year, everyone! Our latest generation native multimodal model, Qwen3.5-397B-A17B, is now officially open source!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Senior-Silver-6130"&gt; /u/Senior-Silver-6130 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jz35kh22xtjg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r65lkc/qwen_35_open_source_native_multimodal_ultimate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r65lkc/qwen_35_open_source_native_multimodal_ultimate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T09:55:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r656d7</id>
    <title>Qwen3.5-397B-A17B is out!!</title>
    <updated>2026-02-16T09:29:03+00:00</updated>
    <author>
      <name>/u/lolxdmainkaisemaanlu</name>
      <uri>https://old.reddit.com/user/lolxdmainkaisemaanlu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3.5-397B-A17B"&gt;https://huggingface.co/Qwen/Qwen3.5-397B-A17B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lolxdmainkaisemaanlu"&gt; /u/lolxdmainkaisemaanlu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r656d7/qwen35397ba17b_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r656d7/qwen35397ba17b_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r656d7/qwen35397ba17b_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T09:29:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3t775</id>
    <title>AMA with MiniMax â€” Ask Us Anything!</title>
    <updated>2026-02-13T16:07:54+00:00</updated>
    <author>
      <name>/u/HardToVary</name>
      <uri>https://old.reddit.com/user/HardToVary</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt; &lt;img alt="AMA with MiniMax â€” Ask Us Anything!" src="https://preview.redd.it/5z2li1ntcajg1.jpg?width=140&amp;amp;height=59&amp;amp;auto=webp&amp;amp;s=ce3340cd37b7e9408878509be00dd7b871efebde" title="AMA with MiniMax â€” Ask Us Anything!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! Weâ€™re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;We're &lt;strong&gt;MiniMax&lt;/strong&gt;, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;.5&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining the channel today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; â€” Founder of MiniMax&lt;/li&gt; &lt;li&gt;&lt;a href="/u/Wise_Evidence9973"&gt;u/Wise_Evidence9973&lt;/a&gt; â€” Head of LLM Research&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ryan85127704"&gt;u/ryan85127704&lt;/a&gt; â€” Head of Engineering&lt;/li&gt; &lt;li&gt;&lt;a href="/u/HardToVary"&gt;u/HardToVary&lt;/a&gt; â€” LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c"&gt;https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S. We'll continue monitoring and responding to questions for 48 hours after the end of the AMA.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HardToVary"&gt; /u/HardToVary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:07:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60qu9</id>
    <title>AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)</title>
    <updated>2026-02-16T05:11:16+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" src="https://preview.redd.it/u11uh8jfisjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afc30b2b6ae673f2e940109e2001bb498bd818ad" title="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ðŸ‘‹&lt;/p&gt; &lt;p&gt;We're excited for Thursday's guests: &lt;strong&gt;The StepFun Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Thursday, Feb. 19th, 8 AMâ€“11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;âš ï¸ &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please donâ€™t post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u11uh8jfisjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T05:11:16+00:00</published>
  </entry>
</feed>
