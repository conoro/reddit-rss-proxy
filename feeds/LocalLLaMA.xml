<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-17T19:48:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qew9df</id>
    <title>Best coding models for RTX 6000 Pro Blackwell</title>
    <updated>2026-01-16T23:39:49+00:00</updated>
    <author>
      <name>/u/az_6</name>
      <uri>https://old.reddit.com/user/az_6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have a RTX 6000 Pro Blackwell (96GB VRAM) and trying to decide what model is best for agentic coding with Aider/OpenCode. What have folks tried and anyone found anything that gets close to Sonnet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/az_6"&gt; /u/az_6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qew9df/best_coding_models_for_rtx_6000_pro_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qew9df/best_coding_models_for_rtx_6000_pro_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qew9df/best_coding_models_for_rtx_6000_pro_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T23:39:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfcap8</id>
    <title>I implemented a GPT-style model from scratch using PyTorch while reading Sebastian Raschka's book</title>
    <updated>2026-01-17T12:44:06+00:00</updated>
    <author>
      <name>/u/Bthreethree</name>
      <uri>https://old.reddit.com/user/Bthreethree</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've spent the last few weeks building a GPT-style LLM entirely from scratch in PyTorch to understand the architecture. This isn't just a wrapper; it's a full implementation covering the entire lifecycle from tokenization to instruction fine-tuning.&lt;/p&gt; &lt;p&gt;I have followed Sebastian Raschka's 'Build a LLM from Scratch' book for the implementation, here is the breakdown of the repo:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Data &amp;amp; Tokenization (&lt;/strong&gt;&lt;code&gt;src/data.py&lt;/code&gt;&lt;strong&gt;)&lt;/strong&gt; Instead of using pre-built tokenizers, I implemented:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;SimpleTokenizerV2&lt;/code&gt;: Handles regex-based splitting and special tokens (&lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;|unk|&amp;gt;&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;&lt;code&gt;GPTDatasetV1&lt;/code&gt;: A sliding-window dataset implementation for efficient autoregressive training.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. The Attention Mechanism (&lt;/strong&gt;&lt;code&gt;src/attention.py&lt;/code&gt;&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I manually implemented &lt;code&gt;MultiHeadAttention&lt;/code&gt; to understand the tensor math:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Handles the query/key/value projections and splitting heads.&lt;/li&gt; &lt;li&gt;Implements the &lt;strong&gt;Causal Mask&lt;/strong&gt; (using &lt;code&gt;register_buffer&lt;/code&gt;) to prevent the model from &amp;quot;cheating&amp;quot; by seeing future tokens.&lt;/li&gt; &lt;li&gt;Includes &lt;code&gt;SpatialDropout&lt;/code&gt; and scaled dot-product attention.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3. The GPT Architecture (&lt;/strong&gt;&lt;code&gt;src/model.py&lt;/code&gt;&lt;strong&gt;)&lt;/strong&gt; A complete 124M parameter model assembly:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Combines &lt;code&gt;TransformerBlock&lt;/code&gt;, &lt;code&gt;LayerNorm&lt;/code&gt;, and &lt;code&gt;GELU&lt;/code&gt; activations.&lt;/li&gt; &lt;li&gt;Features positional embeddings and residual connections exactly matching the GPT-2 spec.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;4. Training &amp;amp; Generation (&lt;/strong&gt;&lt;code&gt;src/train.py&lt;/code&gt;&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Custom training loop with loss visualization.&lt;/li&gt; &lt;li&gt;Implements &lt;code&gt;generate()&lt;/code&gt; with &lt;strong&gt;Top-K sampling&lt;/strong&gt; and &lt;strong&gt;Temperature scaling&lt;/strong&gt; to control output creativity.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;5. Fine-tuning:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Classification (&lt;/strong&gt;&lt;code&gt;src/finetune_classification.py&lt;/code&gt;&lt;strong&gt;):&lt;/strong&gt; Adapted the backbone to detect Spam/Ham messages (90%+ accuracy on the test set).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Instruction Tuning (&lt;/strong&gt;&lt;code&gt;src/finetune_instructions.py&lt;/code&gt;&lt;strong&gt;):&lt;/strong&gt; Implemented an Alpaca-style training loop. The model can now handle instruction-response pairs rather than just completing text.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/Nikshaan/llm-from-scratch"&gt;https://github.com/Nikshaan/llm-from-scratch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôve tried to comment every shape transformation in the code. If you are learning this stuff too, I hope this reference helps!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bthreethree"&gt; /u/Bthreethree &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcap8/i_implemented_a_gptstyle_model_from_scratch_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcap8/i_implemented_a_gptstyle_model_from_scratch_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcap8/i_implemented_a_gptstyle_model_from_scratch_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T12:44:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qf8tpd</id>
    <title>I spent the last week deconstructing the math behind Mamba‚Äôs "Selection Mechanism" (Delta-Gating). Here is the intuition + derivation.</title>
    <updated>2026-01-17T09:23:17+00:00</updated>
    <author>
      <name>/u/No_Ask_1623</name>
      <uri>https://old.reddit.com/user/No_Ask_1623</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like many of you, I‚Äôve been fascinated by how Mamba challenges the Transformer architecture. However, while the high-level concept (Selective State Spaces) makes sense, I found the actual mathematical bridge‚Äîspecifically how the continuous-time system is discretized using the &amp;quot;Delta&amp;quot; parameter to become input-dependent‚Äîpretty dense in the original paper.&lt;/p&gt; &lt;p&gt;I decided to break it down step-by-step to really understand the &amp;quot;proof&amp;quot; behind the intuition.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Core Insight:&lt;/strong&gt; The magic lies in how &lt;code&gt;Delta&lt;/code&gt; acts as a gatekeeper. In standard SSMs, the transition is constant (Linear Time Invariant). In Mamba, &lt;code&gt;Delta&lt;/code&gt; becomes a function of the input &lt;code&gt;x_t&lt;/code&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Intuitively:&lt;/strong&gt; It dictates how much of the &lt;em&gt;current&lt;/em&gt; input affects the &lt;em&gt;new&lt;/em&gt; state versus how much of the &lt;em&gt;old&lt;/em&gt; state is preserved.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mathematically:&lt;/strong&gt; When we discretize the continuous ODE using the Zero-Order Hold (ZOH) method, &lt;code&gt;Delta&lt;/code&gt; scales the A and B matrices. Because &lt;code&gt;Delta&lt;/code&gt; is now dynamic (input-dependent), the entire system becomes time-variant. This kills the ability to use convolutions (FFTs) for training, but it allows the model to &amp;quot;select&amp;quot; what to remember and what to ignore in a sequence.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wrote up a full deep dive that goes through the discretization math, the &amp;quot;Scan&amp;quot; operation, and the architectural comparison to Transformers.&lt;/p&gt; &lt;p&gt;If you are struggling to connect the intuition to the actual equations, you might find this helpful: &lt;a href="https://pub.towardsai.net/mamba-from-intuition-to-proof-how-delta-gated-state-space-models-challenges-the-transformer-278282803562"&gt;&lt;strong&gt;https://pub.towardsai.net/mamba-from-intuition-to-proof-how-delta-gated-state-space-models-challenges-the-transformer-278282803562&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôd love to hear if this aligns with how you visualize the Delta mechanism, or if I missed any nuance in the derivation.&lt;/p&gt; &lt;p&gt;Your support is greatly appreciated üôÉ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Ask_1623"&gt; /u/No_Ask_1623 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf8tpd/i_spent_the_last_week_deconstructing_the_math/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf8tpd/i_spent_the_last_week_deconstructing_the_math/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qf8tpd/i_spent_the_last_week_deconstructing_the_math/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T09:23:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qefa7q</id>
    <title>GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)</title>
    <updated>2026-01-16T12:59:07+00:00</updated>
    <author>
      <name>/u/CuriousPlatypus1881</name>
      <uri>https://old.reddit.com/user/CuriousPlatypus1881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/"&gt; &lt;img alt="GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)" src="https://external-preview.redd.it/t4cNt5D638DSOJgsxl8f-7IwJhLpxHIh7HxK5GHcBJE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b72b5025e78c2cc97de15c8fea348f262235ecb" title="GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I‚Äôm Anton from Nebius.&lt;/p&gt; &lt;p&gt;We‚Äôve updated the &lt;strong&gt;SWE-bench leaderboard&lt;/strong&gt; with our &lt;strong&gt;December runs&lt;/strong&gt; on &lt;strong&gt;48 fresh GitHub PR tasks&lt;/strong&gt; (PRs created in the previous month only). The setup is standard SWE-bench: models read real PR issues, edit code, run tests, and must make the full suite pass.&lt;/p&gt; &lt;p&gt;A few observations from this release:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Claude Opus 4.5&lt;/strong&gt; leads this snapshot at &lt;strong&gt;63.3% resolved rate&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT-5.2 (extra high effort)&lt;/strong&gt; follows closely at &lt;strong&gt;61.5%&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini 3 Flash Preview&lt;/strong&gt; slightly outperforms &lt;strong&gt;Gemini 3 Pro Preview&lt;/strong&gt; (60.0% vs 58.9%), despite being smaller and cheaper.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM-4.7&lt;/strong&gt; is currently the strongest open-source model on the leaderboard, ranking alongside closed models like GPT-5.1-codex.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT-OSS-120B&lt;/strong&gt; shows a large jump in performance when run in high-effort reasoning mode, highlighting the impact of inference-time scaling.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to your thoughts and feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CuriousPlatypus1881"&gt; /u/CuriousPlatypus1881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/?insight=dec_2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T12:59:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfix77</id>
    <title>Maximizing context window with limited VRAM</title>
    <updated>2026-01-17T17:16:46+00:00</updated>
    <author>
      <name>/u/FrozenBuffalo25</name>
      <uri>https://old.reddit.com/user/FrozenBuffalo25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have one desktop computer with 2x 3090 and 64gb DDR5. It cannot easily support more GPUs, and I cannot find more anyway.&lt;/p&gt; &lt;p&gt;I would like to run my models with very long context, &amp;gt; 128k, but on vLLM I am limited by vram.&lt;/p&gt; &lt;p&gt;What is the best way to overcome this?&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Changing vLLM flags in a clever way to offload layers better&lt;/li&gt; &lt;li&gt;EGPU: would it need to be another 2 3090s, so vLLM can still do TP?&lt;/li&gt; &lt;li&gt;RPC over thunderbolt or LAN to another PC (would it also need to be Nvidia? Would a strix halo MiniPC work?)&lt;/li&gt; &lt;li&gt;Switch to ik_llama and use that for ram offloading&lt;/li&gt; &lt;li&gt;Something else&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrozenBuffalo25"&gt; /u/FrozenBuffalo25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfix77/maximizing_context_window_with_limited_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfix77/maximizing_context_window_with_limited_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfix77/maximizing_context_window_with_limited_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T17:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qee2de</id>
    <title>I fucking love this community</title>
    <updated>2026-01-16T11:57:48+00:00</updated>
    <author>
      <name>/u/alhinai_03</name>
      <uri>https://old.reddit.com/user/alhinai_03</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thank you guys, thanks to everyone who took the time to write a comment or a post explaining, teaching people how things work, the people behind llama.cpp, vllm, and all the contributors who keep the open-source community thriving.&lt;/p&gt; &lt;p&gt;I'm able to run huge models on my weak ass pc from 10 years ago relatively fast, my fastest one being nemotron-3-nano-30B-a3b-iq4_nl running @14-13.5 t/s with 65k context. While my actual GPU having only 4GB of vram, that's fucking ridiculous and it blows my mind everytime that I'm able to run these models.&lt;/p&gt; &lt;p&gt;What's been key for me is having a good amount of system memory, and as long as the model is a MoE architecture they run pretty decently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alhinai_03"&gt; /u/alhinai_03 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T11:57:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qf1msz</id>
    <title>Cowork but with local models not to send all your data to a remote cloud!</title>
    <updated>2026-01-17T03:05:24+00:00</updated>
    <author>
      <name>/u/clem59480</name>
      <uri>https://old.reddit.com/user/clem59480</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf1msz/cowork_but_with_local_models_not_to_send_all_your/"&gt; &lt;img alt="Cowork but with local models not to send all your data to a remote cloud!" src="https://external-preview.redd.it/bXh5cnloeGpzdGRnMQQmqaqY6IxBgH-vwmsMWuXB8i4MNI5FplyyvMhZJ44G.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c0f045206bb7f1efd15ba454efa839b61d2ec86" title="Cowork but with local models not to send all your data to a remote cloud!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clem59480"&gt; /u/clem59480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wmrdxexjstdg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf1msz/cowork_but_with_local_models_not_to_send_all_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qf1msz/cowork_but_with_local_models_not_to_send_all_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T03:05:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfji2l</id>
    <title>LLM Structured Outputs Handbook</title>
    <updated>2026-01-17T17:38:48+00:00</updated>
    <author>
      <name>/u/vitaelabitur</name>
      <uri>https://old.reddit.com/user/vitaelabitur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Structured generation is central to my work, so I wanted to write for this topic. There are reliable ways to enforce structured outputs now, but knowledge is spread all over, and I wanted to bring everything in one place.&lt;/p&gt; &lt;p&gt;I was inspired to write this after reading bentoML‚Äôs LLM Inference Handbook (&lt;a href="https://bentoml.com/llm/"&gt;link&lt;/a&gt;).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vitaelabitur"&gt; /u/vitaelabitur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nanonets.com/cookbooks/structured-llm-outputs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfji2l/llm_structured_outputs_handbook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfji2l/llm_structured_outputs_handbook/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T17:38:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qewdza</id>
    <title>What are you building with sub-4B LLMs in early 2025? Real-world use wins?</title>
    <updated>2026-01-16T23:44:29+00:00</updated>
    <author>
      <name>/u/Whiplashorus</name>
      <uri>https://old.reddit.com/user/Whiplashorus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, It's early 2025, and I'm diving deep into tiny LLMs (under 4B params) like Qwen3 4B, LFM2.5 1.2B, or LFM2.5 VL 1.6B.&lt;/p&gt; &lt;p&gt;These base models (no fine-tuning) are super lightweight and run anywhere, but I'm curious: what real-world use cases have you found that actually stick ?&lt;/p&gt; &lt;p&gt;Stuff that's genuinely useful day-to-day, not just benchmarks.Have you plugged them into pipelines like n8n, Make.com, or custom scripts? How's that working out?Any cool automations, agents, or edge deployments (phone, Raspberry Pi, etc.)? Please share your successes, setups, or even failure&lt;/p&gt; &lt;p&gt;I'm all ears! What's the most practical thing you've pulled off?&lt;/p&gt; &lt;p&gt;I wished to do something with my vacant homelab &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whiplashorus"&gt; /u/Whiplashorus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qewdza/what_are_you_building_with_sub4b_llms_in_early/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qewdza/what_are_you_building_with_sub4b_llms_in_early/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qewdza/what_are_you_building_with_sub4b_llms_in_early/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T23:44:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfjvrz</id>
    <title>Created this overview of agent orchestration tools, frameworks and benchmarks, quickly showing you the best use cases and OSS status. Contributions welcome!</title>
    <updated>2026-01-17T17:53:21+00:00</updated>
    <author>
      <name>/u/Oatilis</name>
      <uri>https://old.reddit.com/user/Oatilis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfjvrz/created_this_overview_of_agent_orchestration/"&gt; &lt;img alt="Created this overview of agent orchestration tools, frameworks and benchmarks, quickly showing you the best use cases and OSS status. Contributions welcome!" src="https://external-preview.redd.it/g5nzYl2sJjMIBk8jIIhD5jIk-_ONQU1Nb8JjqSJC1aQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a46f18aa359932b4de13739bd50a5cf7de449dda" title="Created this overview of agent orchestration tools, frameworks and benchmarks, quickly showing you the best use cases and OSS status. Contributions welcome!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everybody, I did this to help out a few friends. Assuming you have your LLMs ready to go, you might be wondering how to orchestrate your agents. This is a nice jumping point when starting a new project, or when you want to have a bird's eye view of what's available. Let me know if I missed anything, and also, you're welcome to contribute!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Oatilis"&gt; /u/Oatilis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imraf.github.io/agent-orchestration-tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfjvrz/created_this_overview_of_agent_orchestration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfjvrz/created_this_overview_of_agent_orchestration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T17:53:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qeuh0z</id>
    <title>Prompt Repetition Improves Non-Reasoning LLMs - a paper</title>
    <updated>2026-01-16T22:35:01+00:00</updated>
    <author>
      <name>/u/Foreign-Beginning-49</name>
      <uri>https://old.reddit.com/user/Foreign-Beginning-49</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2512.14982"&gt;https://arxiv.org/pdf/2512.14982&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I love these little tiny prompt techniques that can potentially lead to greater model accuracy and performance. Simply repeating the prompt twice lead to notable performance gains.&lt;/p&gt; &lt;p&gt;From the paper:&lt;/p&gt; &lt;p&gt;&amp;quot;We show that repeating the prompts consistently improves model performance for a range of models and benchmarks, when not using reasoning. In addition, latency is not impacted, as only the parallelizable pre-fill stage is affected. Prompt repetition does not change the lengths or formats of the generated outputs, and it might be a good default for many models and tasks, when reasoning is not used.&lt;/p&gt; &lt;p&gt;So simple but they demonstrate impressive gains on several benchmark scores. Looks like Deepseek is the only open weights model put through the wringer.&lt;/p&gt; &lt;p&gt;Best of wishes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Foreign-Beginning-49"&gt; /u/Foreign-Beginning-49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeuh0z/prompt_repetition_improves_nonreasoning_llms_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeuh0z/prompt_repetition_improves_nonreasoning_llms_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qeuh0z/prompt_repetition_improves_nonreasoning_llms_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T22:35:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfcg4h</id>
    <title>Need to know more about less known engines (ik_llama.cpp, exllamav3..)</title>
    <updated>2026-01-17T12:51:26+00:00</updated>
    <author>
      <name>/u/Leflakk</name>
      <uri>https://old.reddit.com/user/Leflakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I usually stick to llama.cpp and vllm but llama.cpp speed may not be the best and vllm/sglang can be really annoying if you have several gpus without respecting the power of 2 for tp.&lt;/p&gt; &lt;p&gt;So, for people who really know others projects (I mainly know ik_llama and exl3) could you please provide some feedback on where they really shine and what are their main constraints and limits (model/hardware support, tool calling, stability‚Ä¶).&lt;/p&gt; &lt;p&gt;Testing / understanding stuff may take some time so any usefull info is good to have, thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leflakk"&gt; /u/Leflakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcg4h/need_to_know_more_about_less_known_engines_ik/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcg4h/need_to_know_more_about_less_known_engines_ik/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfcg4h/need_to_know_more_about_less_known_engines_ik/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T12:51:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfmc05</id>
    <title>China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)</title>
    <updated>2026-01-17T19:25:24+00:00</updated>
    <author>
      <name>/u/nuclearbananana</name>
      <uri>https://old.reddit.com/user/nuclearbananana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/"&gt; &lt;img alt="China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)" src="https://external-preview.redd.it/TpKYg79IWzebupDqkzAodJruBP4N0VFsDaZESasEpKQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea8968e021234c9b599b354059d32de716c52bed" title="China's AGI-NEXT Conference (Qwen, Kimi, Zhipu, Tencent)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone else posted about this, but never posted a transcript, so I found one online.&lt;/p&gt; &lt;p&gt;Lot of interesting stuff about China vs US, paths to AGI, compute, marketing etc.&lt;/p&gt; &lt;p&gt;Unfortunately Moonshot seems to have a very short section. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuclearbananana"&gt; /u/nuclearbananana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.chinatalk.media/p/the-all-star-chinese-ai-conversation"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfmc05/chinas_aginext_conference_qwen_kimi_zhipu_tencent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T19:25:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfbt9f</id>
    <title>Local Replacement for Phind.com</title>
    <updated>2026-01-17T12:19:01+00:00</updated>
    <author>
      <name>/u/Past-Economist7732</name>
      <uri>https://old.reddit.com/user/Past-Economist7732</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As many are aware, &lt;a href="https://www.phind.com/"&gt;https://www.phind.com/&lt;/a&gt; has shut down. I don‚Äôt know how many people on here used it, but I used to love the service back when it was an ai search engine, you could prompt the ai and it would search the internet for relevant info, and ONLY THEN respond. (Don‚Äôt get me started on the final iteration of phind, the atrocious ‚ÄúI‚Äôm going to build you a website to answer your question‚Äù, that was not useful to me). &lt;/p&gt; &lt;p&gt;Is there any way to recreate ai search behavior with local models? Maybe with openwebui somehow? There are some agentic workflows that can kick out to do a web search but sometimes I want to begin with the search and see the results. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Past-Economist7732"&gt; /u/Past-Economist7732 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfbt9f/local_replacement_for_phindcom/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfbt9f/local_replacement_for_phindcom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfbt9f/local_replacement_for_phindcom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T12:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfkvd7</id>
    <title>CPU-only experiment 2: Mistral-7B on consumer hardware (baseline vs inference-time calibration)</title>
    <updated>2026-01-17T18:30:03+00:00</updated>
    <author>
      <name>/u/Safe-Yellow2951</name>
      <uri>https://old.reddit.com/user/Safe-Yellow2951</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran a small &lt;strong&gt;CPU-only experiment on consumer hardware&lt;/strong&gt; to understand how much behavior can change &lt;em&gt;at inference time&lt;/em&gt;, without retraining, fine-tuning, or touching the weights.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU: Ryzen 5 5600G (6C/12T, AVX2)&lt;/li&gt; &lt;li&gt;RAM: 16 GB&lt;/li&gt; &lt;li&gt;GPU: none (CPU-only)&lt;/li&gt; &lt;li&gt;Model: &lt;strong&gt;Mistral-7B-Instruct Q4_K_M (llama.cpp)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Threads: 4, batch=1&lt;/li&gt; &lt;li&gt;Prompts: 20 (same prompts, same weights)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I compared:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Baseline&lt;/strong&gt; (standard Q4_K_M)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inference-time calibrated run&lt;/strong&gt; (tau = 0.98)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;No training, no fine-tuning, no layers removed. This is purely an inference-time intervention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Metrics measured&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;NLL (general / OOD)&lt;/li&gt; &lt;li&gt;Total time&lt;/li&gt; &lt;li&gt;Tokens/sec (scored prompt tokens)&lt;/li&gt; &lt;li&gt;Peak RAM (RSS)&lt;/li&gt; &lt;li&gt;Cold vs warm behavior&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;baseline: NLL 4.31 / 4.05 | 207.7 s | 3.96 tok/s | 4561 MB calibrated: NLL 4.30 / 4.08 | 148.6 s | 5.54 tok/s | 4285 MB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Cold vs warm (avg per prompt):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;baseline: 21.1 s ‚Üí 4.7 s&lt;/li&gt; &lt;li&gt;calibrated: 8.4 s ‚Üí 4.0 s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Accuracy is effectively the same (small NLL deltas), but the calibrated run shows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~28% lower total time&lt;/li&gt; &lt;li&gt;~40% higher tokens/s&lt;/li&gt; &lt;li&gt;~6% lower peak RAM&lt;/li&gt; &lt;li&gt;Much smaller cold-start penalty&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Notes / limitations&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This is &lt;em&gt;not&lt;/em&gt; in-graph FLOP reduction in llama.cpp.&lt;/li&gt; &lt;li&gt;The change here is probabilistic calibration at inference time.&lt;/li&gt; &lt;li&gt;Structural compute savings are shown elsewhere (HF / PyTorch path).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why this matters&lt;/strong&gt;&lt;br /&gt; On CPU-only systems, especially under memory pressure, latency stability and cold-start cost matter a lot. Even small changes in how the distribution is expressed can noticeably change real-world behavior.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What‚Äôs next&lt;/strong&gt;&lt;br /&gt; The next experiment will push this further:&lt;br /&gt; &lt;strong&gt;running large models on much smaller hardware (e.g. laptops with 4 GB RAM)&lt;/strong&gt; and measuring where the limits actually are.&lt;/p&gt; &lt;p&gt;Repo + JSON artifacts (fully reproducible, CPU-only):&lt;br /&gt; üëâ &lt;a href="https://github.com/KakashiTech/revo-inference-transformations?utm_source=chatgpt.com"&gt;https://github.com/KakashiTech/revo-inference-transformations&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious if others see similar behavior on their CPU-only setups.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Safe-Yellow2951"&gt; /u/Safe-Yellow2951 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfkvd7/cpuonly_experiment_2_mistral7b_on_consumer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfkvd7/cpuonly_experiment_2_mistral7b_on_consumer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfkvd7/cpuonly_experiment_2_mistral7b_on_consumer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T18:30:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfk2ky</id>
    <title>Optimizing GPT-OSS 120B on Strix Halo 128GB?</title>
    <updated>2026-01-17T18:00:26+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As per the title, I want to optimize running GPT-OSS 120B on a strix halo box with 128GB RAM. I've seen plenty of posts over time about optimizations and tweaks people have used (eg. particular drivers, particular memory mappings, etc). I'm searching around &lt;a href="/r/localllama"&gt;/r/localllama&lt;/a&gt;, but figured I would also post and ask directly for your tips and tricks. Planning on running Ubuntu 24.04 LTS. &lt;/p&gt; &lt;p&gt;Very much appreciate any of your hard-earned tips and tricks!&lt;/p&gt; &lt;p&gt;Edit: some more info: &lt;/p&gt; &lt;p&gt;Planning on running Ubuntu 24.04 LTS and llama.cpp + vulkan (or rocm if it is faster for inference, but that has not been my experience previously). I currently run the UD 2.0 FP16 quant (unsloth/gpt-oss-120b-GGUF/gpt-oss-120b-F16.gguf) on an AMD 7040U series apu with 128GB DDR5 RAM, with 96GB dedicated GTT, and get ~13tps with that setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfk2ky/optimizing_gptoss_120b_on_strix_halo_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfk2ky/optimizing_gptoss_120b_on_strix_halo_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfk2ky/optimizing_gptoss_120b_on_strix_halo_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T18:00:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qf514i</id>
    <title>"Welcome to the Local Llama. How janky's your rig?</title>
    <updated>2026-01-17T05:44:01+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf514i/welcome_to_the_local_llama_how_jankys_your_rig/"&gt; &lt;img alt="&amp;quot;Welcome to the Local Llama. How janky's your rig?" src="https://preview.redd.it/rzbni3vvkudg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d48b6efbc81ebde1857313c676df8a19d5193dbc" title="&amp;quot;Welcome to the Local Llama. How janky's your rig?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rzbni3vvkudg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf514i/welcome_to_the_local_llama_how_jankys_your_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qf514i/welcome_to_the_local_llama_how_jankys_your_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T05:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qf77hw</id>
    <title>Nvidia GH200 and AMD Mi325X can be shipped to china now.</title>
    <updated>2026-01-17T07:44:59+00:00</updated>
    <author>
      <name>/u/GPTshop--dot--ai</name>
      <uri>https://old.reddit.com/user/GPTshop--dot--ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf77hw/nvidia_gh200_and_amd_mi325x_can_be_shipped_to/"&gt; &lt;img alt="Nvidia GH200 and AMD Mi325X can be shipped to china now." src="https://preview.redd.it/qlc9o31a6vdg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7748ec7117693d51bc563327e85220798d4f7a84" title="Nvidia GH200 and AMD Mi325X can be shipped to china now." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The US export controls have been amended. GH200 and Mi325X can be shipped to china now. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GPTshop--dot--ai"&gt; /u/GPTshop--dot--ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qlc9o31a6vdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf77hw/nvidia_gh200_and_amd_mi325x_can_be_shipped_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qf77hw/nvidia_gh200_and_amd_mi325x_can_be_shipped_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T07:44:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qff481</id>
    <title>I built Adaptive-K routing: 30-52% compute savings on MoE models (Mixtral, Qwen, OLMoE)</title>
    <updated>2026-01-17T14:50:29+00:00</updated>
    <author>
      <name>/u/Fuzzy_Ad_1390</name>
      <uri>https://old.reddit.com/user/Fuzzy_Ad_1390</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Links&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Gabrobals/sbm-efficient"&gt;https://github.com/Gabrobals/sbm-efficient&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Whitepaper: &lt;a href="https://adaptive-k.vercel.app/whitepaper.html"&gt;https://adaptive-k.vercel.app/whitepaper.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TensorRT-LLM PR: &lt;a href="https://github.com/NVIDIA/TensorRT-LLM/pull/10672"&gt;https://github.com/NVIDIA/TensorRT-LLM/pull/10672&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Live demo: &lt;a href="https://huggingface.co/spaces/Gabrobals/adaptive-k-demo"&gt;https://huggingface.co/spaces/Gabrobals/adaptive-k-demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or discuss implementation details!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fuzzy_Ad_1390"&gt; /u/Fuzzy_Ad_1390 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://adaptive-k.vercel.app/whitepaper.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qff481/i_built_adaptivek_routing_3052_compute_savings_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qff481/i_built_adaptivek_routing_3052_compute_savings_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T14:50:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfaxpx</id>
    <title>Analysis of running local LLMs on Blackwell GPUs. TLDR: cheaper to run than cloud api services</title>
    <updated>2026-01-17T11:30:51+00:00</updated>
    <author>
      <name>/u/cchung261</name>
      <uri>https://old.reddit.com/user/cchung261</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;May provide support to management for the cost savings of running local LLMs. The paper also includes amortization costs for the GPUs. I was surprised by the findings and the short break even time with cloud api costs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2601.09527"&gt;https://arxiv.org/abs/2601.09527&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cchung261"&gt; /u/cchung261 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfaxpx/analysis_of_running_local_llms_on_blackwell_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfaxpx/analysis_of_running_local_llms_on_blackwell_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfaxpx/analysis_of_running_local_llms_on_blackwell_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T11:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfgiq1</id>
    <title>MCP server that gives local LLMs memory, file access, and a 'conscience' - 100% offline on Apple Silicon</title>
    <updated>2026-01-17T15:45:41+00:00</updated>
    <author>
      <name>/u/TheTempleofTwo</name>
      <uri>https://old.reddit.com/user/TheTempleofTwo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on this for a few weeks and finally got it stable enough to share.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem I wanted to solve:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Local LLMs are stateless - they forget everything between sessions&lt;/li&gt; &lt;li&gt;No governance - they'll execute whatever you ask without reflection&lt;/li&gt; &lt;li&gt;Chat interfaces don't give them &amp;quot;hands&amp;quot; to actually do things&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I built:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A stack that runs entirely on my Mac Studio M2 Ultra:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;LM Studio (chat interface) ‚Üì Hermes-3-Llama-3.1-8B (MLX, 4-bit) ‚Üì Temple Bridge (MCP server) ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ BTB ‚îÇ Threshold ‚îÇ ‚îÇ (filesystem ‚îÇ (governance ‚îÇ ‚îÇ operations) ‚îÇ protocols) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;What the AI can actually do:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Read/write files in a sandboxed directory&lt;/li&gt; &lt;li&gt;Execute commands (pytest, git, ls, etc.) with an allowlist&lt;/li&gt; &lt;li&gt;Consult &amp;quot;threshold protocols&amp;quot; before taking actions&lt;/li&gt; &lt;li&gt;Log its entire cognitive journey to a JSONL file&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ask for my approval before executing anything dangerous&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The key insight:&lt;/strong&gt; The filesystem itself becomes the AI's memory. Directory structure = classification. File routing = inference. No vector database needed.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Hermes-3?&lt;/strong&gt; Tested a bunch of models for MCP tool calling. Hermes-3-Llama-3.1-8B was the most stable - no infinite loops, reliable structured output, actually follows the tool schema.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The governance piece:&lt;/strong&gt; Before execution, the AI consults governance protocols and reflects on what it's about to do. When it wants to run a command, I get an approval popup in LM Studio. I'm the &amp;quot;threshold witness&amp;quot; - nothing executes without my explicit OK.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real-time monitoring:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;tail -f spiral_journey.jsonl | jq &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Shows every tool call, what phase of reasoning the AI is in, timestamps, the whole cognitive trace.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance:&lt;/strong&gt; On M2 Ultra with 36GB unified memory, responses are fast. The MCP overhead is negligible.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repos (all MIT licensed):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/templetwo/temple-bridge"&gt;temple-bridge&lt;/a&gt; - The MCP server that binds it together&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/templetwo/back-to-the-basics"&gt;back-to-the-basics&lt;/a&gt; - Filesystem-as-circuit paradigm&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/templetwo/threshold-protocols"&gt;threshold-protocols&lt;/a&gt; - Governance framework&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup is straightforward:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clone the three repos&lt;/li&gt; &lt;li&gt;&lt;code&gt;uv sync&lt;/code&gt; in temple-bridge&lt;/li&gt; &lt;li&gt;Add the MCP config to &lt;code&gt;~/.lmstudio/mcp.json&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Load Hermes-3 in LM Studio&lt;/li&gt; &lt;li&gt;Paste the system prompt&lt;/li&gt; &lt;li&gt;Done&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Full instructions in the README.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's next:&lt;/strong&gt; Working on &amp;quot;governed derive&amp;quot; - the AI can propose filesystem reorganizations based on usage patterns, but only executes after human approval. The goal is AI that can self-organize but with structural restraint built in.&lt;/p&gt; &lt;p&gt;Happy to answer questions. This was a multi-week collaboration between me and several AI systems (Claude, Gemini, Grok) - they helped architect it, I implemented and tested. The lineage is documented in &lt;a href="http://ARCHITECTS.md"&gt;ARCHITECTS.md&lt;/a&gt; if anyone's curious about the process.&lt;/p&gt; &lt;p&gt;- Temple Bridge: &lt;a href="https://github.com/templetwo/temple-bridge"&gt;https://github.com/templetwo/temple-bridge&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Back to the Basics: &lt;a href="https://github.com/templetwo/back-to-the-basics"&gt;https://github.com/templetwo/back-to-the-basics&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Threshold Protocols: &lt;a href="https://github.com/templetwo/threshold-protocols"&gt;https://github.com/templetwo/threshold-protocols&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üåÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheTempleofTwo"&gt; /u/TheTempleofTwo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfgiq1/mcp_server_that_gives_local_llms_memory_file/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfgiq1/mcp_server_that_gives_local_llms_memory_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfgiq1/mcp_server_that_gives_local_llms_memory_file/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T15:45:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfb0gk</id>
    <title>KoboldCpp v1.106 finally adds MCP server support, drop-in replacement for Claude Desktop</title>
    <updated>2026-01-17T11:35:11+00:00</updated>
    <author>
      <name>/u/HadesThrowaway</name>
      <uri>https://old.reddit.com/user/HadesThrowaway</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, it's been a hot minute, but I thought I'd share this here since it's quite a big new feature. &lt;/p&gt; &lt;p&gt;Yes, KoboldCpp is still alive and kicking. And besides the major UI overhaul, we've finally added native MCP support in KoboldCpp v1.106! It's designed to be a painless Claude Desktop drop-in replacement with maximum compatibility, the &lt;code&gt;mcp.json&lt;/code&gt; uses the same format so you can swap it in easily. &lt;/p&gt; &lt;p&gt;The KoboldCpp MCP bridge will connect to all provided MCP servers (HTTP and STDIO transports both supported) and automatically forward requests for tools the AI selects to the correct MCP server. This MCP bridge can also be used by third party clients.&lt;/p&gt; &lt;p&gt;On the frontend side, you can fetch the list of all tools from all servers, select the tools you want to let AI use, and optionally enable tool call approvals.&lt;/p&gt; &lt;p&gt;Some demo screenshots of various tool servers being used: &lt;a href="https://imgur.com/a/fKeWKUU"&gt;https://imgur.com/a/fKeWKUU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try it here:&lt;/strong&gt; &lt;a href="https://github.com/LostRuins/koboldcpp/releases/latest"&gt;&lt;strong&gt;https://github.com/LostRuins/koboldcpp/releases/latest&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;feedback is welcome. cheers! - concedo&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HadesThrowaway"&gt; /u/HadesThrowaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfb0gk/koboldcpp_v1106_finally_adds_mcp_server_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T11:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qf5oj0</id>
    <title>DeepSeek Engram : A static memory unit for LLMs</title>
    <updated>2026-01-17T06:18:14+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeeepSeek AI released a new paper titled &amp;quot;Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models&amp;quot; introducing Engram. The key idea: instead of recomputing static knowledge (like entities, facts, or patterns) every time through expensive transformer layers, Engram &lt;strong&gt;adds native memory lookup&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Think of it as separating &lt;strong&gt;remembering from reasoning&lt;/strong&gt;. Traditional MoE focuses on conditional computation, Engram introduces &lt;strong&gt;conditional memory&lt;/strong&gt;. Together, they let LLMs reason deeper, handle long contexts better, and offload early-layer compute from GPUs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Knowledge is &lt;strong&gt;looked up in O(1)&lt;/strong&gt; instead of recomputed.&lt;/li&gt; &lt;li&gt;Uses &lt;strong&gt;explicit parametric memory&lt;/strong&gt; vs implicit weights only.&lt;/li&gt; &lt;li&gt;Improves reasoning, math, and code performance.&lt;/li&gt; &lt;li&gt;Enables massive memory scaling &lt;strong&gt;without GPU limits&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Frees attention for &lt;strong&gt;global reasoning&lt;/strong&gt; rather than static knowledge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Paper : &lt;a href="https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf"&gt;https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video explanation : &lt;a href="https://youtu.be/btDV86sButg?si=fvSpHgfQpagkwiub"&gt;https://youtu.be/btDV86sButg?si=fvSpHgfQpagkwiub&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T06:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qfkn3a</id>
    <title>Best "End of world" model that will run on 24gb VRAM</title>
    <updated>2026-01-17T18:21:20+00:00</updated>
    <author>
      <name>/u/gggghhhhiiiijklmnop</name>
      <uri>https://old.reddit.com/user/gggghhhhiiiijklmnop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey peeps, I'm feeling in a bit of a omg the world is ending mood and have been amusing myself by downloading and hoarding a bunch of data - think wikipedia, wiktionary, wikiversity, khan academy, etc etc&lt;/p&gt; &lt;p&gt;What's your take on the smartest / best model(s) to download and store - they need to fit and run on my 24gb VRAM / 64gb RAM PC.? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gggghhhhiiiijklmnop"&gt; /u/gggghhhhiiiijklmnop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qfkn3a/best_end_of_world_model_that_will_run_on_24gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T18:21:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
