<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-24T19:34:32+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1of1mq4</id>
    <title>üòé Unified Offline LLM, Vision &amp; Speech on Android ‚Äì ai‚Äëcore 0.1 Stable</title>
    <updated>2025-10-24T16:00:26+00:00</updated>
    <author>
      <name>/u/DarkEngine774</name>
      <uri>https://old.reddit.com/user/DarkEngine774</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;br /&gt; There‚Äôs a sea of AI models out there ‚Äì Llama, Qwen, Whisper, LLaVA‚Ä¶ each with its own library, language binding, and storage format. Switching between them forces you either to write a ton of boiler‚Äëplate code or ship multiple native libraries with your app.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ai‚Äëcore&lt;/strong&gt; solves that.&lt;br /&gt; It exposes &lt;strong&gt;one, single Kotlin/Java interface&lt;/strong&gt; that can load &lt;em&gt;any&lt;/em&gt; GGUF or ONNX model (text, embeddings, vision, STT, TTS) and run it completely offline on an Android device ‚Äì no GPU, no server, no expensive dependencies.&lt;/p&gt; &lt;h1&gt;What it gives you&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;What you get&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Unified API&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Call &lt;code&gt;NativeLib&lt;/code&gt;, &lt;code&gt;MtmdLib&lt;/code&gt;, &lt;code&gt;EmbedLib&lt;/code&gt; ‚Äì same names, same pattern.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Offline inference&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;No network hits; all compute stays on the phone.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Open‚Äësource&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Fork, review, monkey‚Äëpatch.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Zero‚Äëconfig start&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;‚úîÔ∏è Pull the AAR from &lt;code&gt;build/libs&lt;/code&gt;, drop into &lt;code&gt;libs/&lt;/code&gt;, add a single Gradle line.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Easy to customise&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Swap in your own motif, prompt template, tools JSON, language packs ‚Äì &lt;em&gt;no code changes needed&lt;/em&gt;.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Built‚Äëin tools&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Generic chat template, tool‚Äëcall parser, KV‚Äëcache persistence, state reuse.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Telemetry &amp;amp; diagnostics&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Simple &lt;code&gt;nativeGetModelInfo()&lt;/code&gt; for introspection; optional logging.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Multimodal&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Vision + text streaming (e.g. Qwen‚ÄëVL, LLaVA).&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Speech&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Sherpa‚ÄëONNX STT &amp;amp; TTS ‚Äì AIDL service + Flow streaming.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Multi‚Äëthreaded &amp;amp; coroutine‚Äëfriendly&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Heavy work on &lt;a href="http://Dispatchers.IO"&gt;&lt;code&gt;Dispatchers.IO&lt;/code&gt;&lt;/a&gt;; streaming callbacks on the main thread.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Quick setup&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Clone &amp;amp; build&lt;/strong&gt;git clone &lt;a href="https://github.com/Siddhesh2377/Ai-Core"&gt;https://github.com/Siddhesh2377/Ai-Core&lt;/a&gt; cd Ai-Core ./gradlew assembleRelease&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Add the AAR&lt;/strong&gt;app/ ‚îú‚îÄ libs/ ‚îÇ ‚îú‚îÄ ai_core-0.1-stable.aar dependencies { implementation(fileTree(dir: 'libs', include: ['*.aar'])) }&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Permissions&lt;/strong&gt; (for file I/O &amp;amp; audio)&amp;lt;uses-permission android:name=&amp;quot;android.permission.MANAGE\_EXTERNAL\_STORAGE&amp;quot;/&amp;gt; &amp;lt;uses-permission android:name=&amp;quot;android.permission.FOREGROUND\_SERVICE&amp;quot;/&amp;gt; &amp;lt;uses-permission android:name=&amp;quot;android.permission.RECORD\_AUDIO&amp;quot;/&amp;gt; &amp;lt;uses-permission android:name=&amp;quot;android.permission.POST\_NOTIFICATIONS&amp;quot;/&amp;gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Use the API&lt;/strong&gt; ‚Äì just a few lines of Kotlin to load a model and stream tokens. The repo contains a &lt;code&gt;sample&lt;/code&gt; app that demonstrates everything.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Why you‚Äôll love it&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;One native lib&lt;/strong&gt; ‚Äì no multiple &lt;code&gt;.so&lt;/code&gt; files flying around.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero‚Äëcost, offline&lt;/strong&gt; ‚Äì perfect for privacy‚Äëfocused apps or regions with limited connectivity.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extensible&lt;/strong&gt; ‚Äì swap the underlying model or add a new wrapper with just a handful of lines; no re‚Äëbuilding the entire repo.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Community‚Äëfriendly&lt;/strong&gt; ‚Äì all source is public; you can inspect every JNI call or tweak the llama‚Äëcpp options.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check the full source, docs, and sample app on GitHub:&lt;br /&gt; &lt;a href="https://github.com/Siddhesh2377/Ai-Core"&gt;https://github.com/Siddhesh2377/Ai-Core&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy hacking! üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkEngine774"&gt; /u/DarkEngine774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of1mq4/unified_offline_llm_vision_speech_on_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of1mq4/unified_offline_llm_vision_speech_on_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of1mq4/unified_offline_llm_vision_speech_on_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T16:00:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1oegejr</id>
    <title>Is this a massive mistake? Super tight fit, 2x 3-slot GPU</title>
    <updated>2025-10-23T21:55:16+00:00</updated>
    <author>
      <name>/u/zhambe</name>
      <uri>https://old.reddit.com/user/zhambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oegejr/is_this_a_massive_mistake_super_tight_fit_2x/"&gt; &lt;img alt="Is this a massive mistake? Super tight fit, 2x 3-slot GPU" src="https://b.thumbs.redditmedia.com/QzvVO4FHT-NLkbNyOHbOU6NxaPcMMkZoxvfXU1-n4EM.jpg" title="Is this a massive mistake? Super tight fit, 2x 3-slot GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Two 3090s is the sweet spot&amp;quot; they said, &amp;quot;best value&amp;quot; they said. The top card literally touches the bottom one, no breathing room for the fans. This is how the PCIe-16x slots are spaced on the mobo. Not only is thermal a concern, both cards are drooping because they're so heavy.&lt;/p&gt; &lt;p&gt;What's the right thing to do here? Complicate the setup further with a water block + pump + radiator? I can construct some kind of support bracket to remedy the drooping, and a shim to put between the cards to give a few mm of space for airflow. I'm sure there are better ideas...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zhambe"&gt; /u/zhambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oegejr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oegejr/is_this_a_massive_mistake_super_tight_fit_2x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oegejr/is_this_a_massive_mistake_super_tight_fit_2x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T21:55:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1of6lcf</id>
    <title>Use Local LLM on your terminal with filesystem handling</title>
    <updated>2025-10-24T19:11:24+00:00</updated>
    <author>
      <name>/u/Sea-Reception-2697</name>
      <uri>https://old.reddit.com/user/Sea-Reception-2697</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of6lcf/use_local_llm_on_your_terminal_with_filesystem/"&gt; &lt;img alt="Use Local LLM on your terminal with filesystem handling" src="https://external-preview.redd.it/M2Q1OGpvdTl6M3hmMWeU_7mDURBNP1Sda2-z7G3nNT97YS2GcY0AFck-U5_D.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e071b84c84d1b0a31f38662afc5d69a9c7b1c1f" title="Use Local LLM on your terminal with filesystem handling" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those running local AI models with ollama or LM studio,&lt;br /&gt; you can use the Xandai CLI tool to create and edit code directly from your terminal.&lt;/p&gt; &lt;p&gt;It also supports natural language commands, so if you don‚Äôt remember a specific command, you can simply ask Xandai to do it for you. For example:&lt;br /&gt; ‚ÄúList the 50 largest files on my system.‚Äù&lt;/p&gt; &lt;p&gt;Install it easily with:&lt;br /&gt; pip install xandai-cli&lt;/p&gt; &lt;p&gt;githube repo: &lt;a href="https://github.com/XandAI-project/Xandai-CLI"&gt;https://github.com/XandAI-project/Xandai-CLI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Reception-2697"&gt; /u/Sea-Reception-2697 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vkejdpu9z3xf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of6lcf/use_local_llm_on_your_terminal_with_filesystem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of6lcf/use_local_llm_on_your_terminal_with_filesystem/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T19:11:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1oewke2</id>
    <title>GLM Air REAP tool call problems</title>
    <updated>2025-10-24T12:36:31+00:00</updated>
    <author>
      <name>/u/Badger-Purple</name>
      <uri>https://old.reddit.com/user/Badger-Purple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tried the GLM4.5 Air REAP versions with pruned experts. I do notice degradation beyond the benchmarks; it is unable to follow more than 5 tool calls at a time before making an error, whereas this was never the case with the full model even at MXFP4 or q4 quantization (full version at MXFP4 is 63GB and REAP quant at q64mixed is 59GB). Anyone else seeing this discrepancy? My test is always the same and requires the model to find and invoke 40 different tools.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Badger-Purple"&gt; /u/Badger-Purple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oewke2/glm_air_reap_tool_call_problems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oewke2/glm_air_reap_tool_call_problems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oewke2/glm_air_reap_tool_call_problems/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T12:36:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1oe7orf</id>
    <title>State of Open OCR models</title>
    <updated>2025-10-23T16:19:39+00:00</updated>
    <author>
      <name>/u/unofficialmerve</name>
      <uri>https://old.reddit.com/user/unofficialmerve</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello folks! it's Merve from Hugging Face ü´°&lt;/p&gt; &lt;p&gt;You might have noticed there has been many open OCR models released lately üòÑ they're cheap to run compared to closed ones, some even run on-device&lt;/p&gt; &lt;p&gt;But it's hard to compare them and have a guideline on picking among upcoming ones, so we have broken it down for you in a blog:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;how to evaluate and pick an OCR model,&lt;/li&gt; &lt;li&gt;a comparison of the latest open-source models,&lt;/li&gt; &lt;li&gt;deployment tips,&lt;/li&gt; &lt;li&gt;and what‚Äôs next beyond basic OCR &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We hope it's useful for you! Let us know what you think: &lt;a href="https://huggingface.co/blog/ocr-open-models"&gt;https://huggingface.co/blog/ocr-open-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unofficialmerve"&gt; /u/unofficialmerve &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe7orf/state_of_open_ocr_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oe7orf/state_of_open_ocr_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oe7orf/state_of_open_ocr_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T16:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oeep6p</id>
    <title>What LLM gave you your first "we have GPT-4 at home" moment?</title>
    <updated>2025-10-23T20:46:35+00:00</updated>
    <author>
      <name>/u/Klutzy-Snow8016</name>
      <uri>https://old.reddit.com/user/Klutzy-Snow8016</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For a long time, local models lagged ChatGPT 3.5 by a lot, and 4 was so far beyond that it felt hopeless. But now, you can run very good models at home.&lt;/p&gt; &lt;p&gt;So I'm curious, for your use-case, or just general usage, what was the point at which a model you ran locally finally caught up to what you saw from the paid models of 2023, or are you still waiting for that to happen?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Klutzy-Snow8016"&gt; /u/Klutzy-Snow8016 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeep6p/what_llm_gave_you_your_first_we_have_gpt4_at_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeep6p/what_llm_gave_you_your_first_we_have_gpt4_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oeep6p/what_llm_gave_you_your_first_we_have_gpt4_at_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T20:46:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1oepfug</id>
    <title>Antislop: A Comprehensive Framework for Identifying and Eliminating Repetitive Patterns in Language Models</title>
    <updated>2025-10-24T05:24:13+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h3&gt;Abstract&lt;/h3&gt; &lt;p&gt;Widespread LLM adoption has introduced characteristic repetitive phraseology, termed &amp;quot;slop,&amp;quot; which degrades output quality and makes AI-generated text immediately recognizable. We present Antislop, a comprehensive framework providing tools to both detect and eliminate these overused patterns. Our approach combines three innovations: (1) The Antislop Sampler, which uses backtracking to suppress unwanted strings at inference time without destroying vocabulary; (2) An automated pipeline that profiles model-specific slop against human baselines and generates training data; (3) Final Token Preference Optimization (FTPO), a novel fine-tuning method that operates on individual tokens, surgically adjusting logits wherever a banned pattern has appeared in an inference trace.&lt;/p&gt; &lt;p&gt;We demonstrate that some slop patterns appear over 1,000x more frequently in LLM output than human text. The Antislop Sampler successfully suppresses 8,000+ patterns while maintaining quality, whereas token banning becomes unusable at just 2,000. Most importantly, FTPO achieves 90% slop reduction while maintaining or improving performance in cross-domain evals including GSM8K, MMLU, and creative writing tasks. In contrast, DPO suffers significant degradation in writing quality and lexical diversity despite achieving weaker suppression.&lt;/p&gt; &lt;p&gt;We release all code and results under MIT license: &lt;a href="https://github.com/sam-paech/auto-antislop"&gt;https://github.com/sam-paech/auto-antislop&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/pdf/2510.15061"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oepfug/antislop_a_comprehensive_framework_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oepfug/antislop_a_comprehensive_framework_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T05:24:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1oefu29</id>
    <title>Cerebras REAP'd GLM4.6: 25%, 30%, 40% pruned FP8 checkpoints on HF!</title>
    <updated>2025-10-23T21:31:20+00:00</updated>
    <author>
      <name>/u/ilzrvch</name>
      <uri>https://old.reddit.com/user/ilzrvch</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oefu29/cerebras_reapd_glm46_25_30_40_pruned_fp8/"&gt; &lt;img alt="Cerebras REAP'd GLM4.6: 25%, 30%, 40% pruned FP8 checkpoints on HF!" src="https://external-preview.redd.it/AGpnB3Q_Xjisqwn0DU233BBKuTP9o7kSBGuVW7dOHBs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74eb9943702ce83c73eb39485b3fe95bdff48313" title="Cerebras REAP'd GLM4.6: 25%, 30%, 40% pruned FP8 checkpoints on HF!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;We've gotten a ton of positive feedback on our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"&gt;previous&lt;/a&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/"&gt;posts&lt;/a&gt; about our REAP pruned MoE models.&lt;/p&gt; &lt;p&gt;We've a got a new (highly requested!) update - REAP'd GLM4.6!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GLM4.6-FP8 REAP@25%:&lt;/strong&gt; &lt;a href="https://hf.co/cerebras/GLM-4.6-REAP-268B-A32B-FP8"&gt;https://hf.co/cerebras/GLM-4.6-REAP-268B-A32B-FP8&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;GLM4.6-FP8 REAP@30%:&lt;/strong&gt; &lt;a href="https://hf.co/cerebras/GLM-4.6-REAP-252B-A32B-FP8"&gt;https://hf.co/cerebras/GLM-4.6-REAP-252B-A32B-FP8&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;GLM4.6-FP8 REAP@40%:&lt;/strong&gt; &lt;a href="https://hf.co/cerebras/GLM-4.6-REAP-218B-A32B-FP8"&gt;https://hf.co/cerebras/GLM-4.6-REAP-218B-A32B-FP8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT: the BF16 versions for low-bit quant are now available:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GLM4.6 REAP@25%:&lt;/strong&gt; &lt;a href="https://hf.co/cerebras/GLM-4.6-REAP-268B-A32B"&gt;https://hf.co/cerebras/GLM-4.6-REAP-268B-A32B&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;GLM4.6 REAP@30%:&lt;/strong&gt; &lt;a href="https://hf.co/cerebras/GLM-4.6-REAP-252B-A32B"&gt;https://hf.co/cerebras/GLM-4.6-REAP-252B-A32B&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;GLM4.6 REAP@40%:&lt;/strong&gt; &lt;a href="https://hf.co/cerebras/GLM-4.6-REAP-218B-A32B"&gt;https://hf.co/cerebras/GLM-4.6-REAP-218B-A32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Stay tuned, we are updating our model collection: &lt;a href="https://huggingface.co/collections/cerebras/cerebras-reap"&gt;https://huggingface.co/collections/cerebras/cerebras-reap&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gwuv3e9tjxwf1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23ab8c8018762fc99dae789ad012ce0d3f7a8a6a"&gt;https://preview.redd.it/gwuv3e9tjxwf1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23ab8c8018762fc99dae789ad012ce0d3f7a8a6a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilzrvch"&gt; /u/ilzrvch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oefu29/cerebras_reapd_glm46_25_30_40_pruned_fp8/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oefu29/cerebras_reapd_glm46_25_30_40_pruned_fp8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oefu29/cerebras_reapd_glm46_25_30_40_pruned_fp8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T21:31:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1of6m7q</id>
    <title>What's the difference between Nvidia DG Spark OS and Ubuntu + CUDA dev stack?</title>
    <updated>2025-10-24T19:12:20+00:00</updated>
    <author>
      <name>/u/Ill_Barber8709</name>
      <uri>https://old.reddit.com/user/Ill_Barber8709</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A friend of mine wants to buy the DG Spark, but replace its OS with Ubuntu + CUDA open-source dev stack.&lt;/p&gt; &lt;p&gt;I think it's pointless, but I don't know shit on the subject. What do you think? Is there any difference between the two? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill_Barber8709"&gt; /u/Ill_Barber8709 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of6m7q/whats_the_difference_between_nvidia_dg_spark_os/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of6m7q/whats_the_difference_between_nvidia_dg_spark_os/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of6m7q/whats_the_difference_between_nvidia_dg_spark_os/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T19:12:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1of3r61</id>
    <title>Test results for various models' ability to give structured responses via LM Studio. Spoiler: Qwen3 won</title>
    <updated>2025-10-24T17:21:56+00:00</updated>
    <author>
      <name>/u/zenmagnets</name>
      <uri>https://old.reddit.com/user/zenmagnets</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did a simple test on few Local Models to see how consistently they'd follow JSON Schema when requesting structured output from LM Studio. Results:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Pass Percentage&lt;/th&gt; &lt;th align="left"&gt;Notes (50 runs per model)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;glm-4.5-air&lt;/td&gt; &lt;td align="left"&gt;86%&lt;/td&gt; &lt;td align="left"&gt;M3MAX; 24.19 tok/s; 2 Incomplete Response Errors; 5 Schema Violation Errors&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;google/gemma-3-27b&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;td align="left"&gt;5090; 51.20 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;kat-dev&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;td align="left"&gt;5090; 43.61 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;kimi-vl-a3b-thinking-2506&lt;/td&gt; &lt;td align="left"&gt;96%&lt;/td&gt; &lt;td align="left"&gt;M3MAX; 75.19 tok/s; 2 Incomplete Response Errors&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mistralai/magistral-small-2509&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;td align="left"&gt;5090; 29.73 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mistralai/magistral-small-2509&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;td align="left"&gt;M3MAX; 15.92 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mradermacher/apriel-1.5-15b-thinker&lt;/td&gt; &lt;td align="left"&gt;0%&lt;/td&gt; &lt;td align="left"&gt;M3MAX; 22.91 tok/s; 50 Schema Violation Errors&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;nvidia-nemotron-nano-9b-v2s&lt;/td&gt; &lt;td align="left"&gt;0%&lt;/td&gt; &lt;td align="left"&gt;M3MAX; 13.27 tok/s; 50 Incomplete Response Errors&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;openai/gpt-oss-120b&lt;/td&gt; &lt;td align="left"&gt;0%&lt;/td&gt; &lt;td align="left"&gt;M3MAX; 26.58 tok/s; 30 Incomplete Response Errors; 9 Schema Violation Errors; 11 Timeout Error Errors&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;openai/gpt-oss-20b&lt;/td&gt; &lt;td align="left"&gt;2%&lt;/td&gt; &lt;td align="left"&gt;5090; 33.17 tok/s; 45 Incomplete Response Errors; 3 Schema Violation Errors; 1 Timeout Error&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-next-80b&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;td align="left"&gt;M3MAX; 32.73 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3-next-80b-a3b-thinking-mlx&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;td align="left"&gt;M3MAX; 36.33 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-vl-30b&lt;/td&gt; &lt;td align="left"&gt;98%&lt;/td&gt; &lt;td align="left"&gt;M3MAX; 48.91 tok/s; 1 Incomplete Response Error&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3-32b&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;td align="left"&gt;5090; 38.92 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;unsloth/qwen3-coder-30b-a3b-instruct&lt;/td&gt; &lt;td align="left"&gt;98%&lt;/td&gt; &lt;td align="left"&gt;5090; 91.13 tok/s; 1 Incomplete Response Error&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-coder-30b&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;td align="left"&gt;5090; 37.36 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-30b-a3b-2507&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;td align="left"&gt;5090; 121.27 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3-30b-a3b-thinking-2507&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;td align="left"&gt;5090; 98.77 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen/qwen3-4b-thinking-2507&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;td align="left"&gt;M3MAX; 38.82 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Prompt was super basic, and just prompted to rate a small list of jokes. Here's the script if you want to play around with a different model/api/prompt: &lt;a href="https://github.com/shihanqu/LLM-Structured-JSON-Tester/blob/main/test_llm_json.py"&gt;https://github.com/shihanqu/LLM-Structured-JSON-Tester/blob/main/test_llm_json.py&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zenmagnets"&gt; /u/zenmagnets &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of3r61/test_results_for_various_models_ability_to_give/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of3r61/test_results_for_various_models_ability_to_give/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of3r61/test_results_for_various_models_ability_to_give/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T17:21:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1oetfxu</id>
    <title>MoonshotAI/kimi-cli - CLI coding agent from MoonshotAI</title>
    <updated>2025-10-24T09:46:47+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oetfxu/moonshotaikimicli_cli_coding_agent_from_moonshotai/"&gt; &lt;img alt="MoonshotAI/kimi-cli - CLI coding agent from MoonshotAI" src="https://external-preview.redd.it/gCRFDtSNP63oI07JY9GS8NsMz4dKnDZ7jPQdkKhmdHE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dcf21d3b55e39a16871cc1ae0ef8860b685f6623" title="MoonshotAI/kimi-cli - CLI coding agent from MoonshotAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MoonshotAI/kimi-cli"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oetfxu/moonshotaikimicli_cli_coding_agent_from_moonshotai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oetfxu/moonshotaikimicli_cli_coding_agent_from_moonshotai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T09:46:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1oexoct</id>
    <title>OpenAI didn‚Äôt open source the Apps SDK‚Ä¶ so I did</title>
    <updated>2025-10-24T13:24:36+00:00</updated>
    <author>
      <name>/u/maneesh_sandra</name>
      <uri>https://old.reddit.com/user/maneesh_sandra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oexoct/openai_didnt_open_source_the_apps_sdk_so_i_did/"&gt; &lt;img alt="OpenAI didn‚Äôt open source the Apps SDK‚Ä¶ so I did" src="https://external-preview.redd.it/w48Fxrn2PiTHc4jnBCogfDnD3b-cLrqGopsKnFm5gMc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=145335536e29bca68b49b707f94b659254623f60" title="OpenAI didn‚Äôt open source the Apps SDK‚Ä¶ so I did" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;You might have seen open AI apps SDK where you can use apps directly inside chatGPT, it caught my eye and I was extremely interested in that.&lt;/p&gt; &lt;p&gt;The only problem is they haven't open sourced it just like how anthropic did with MCPs. Since then I started working on this SDK which serves the same purpose and also LLM agnostic.&lt;/p&gt; &lt;p&gt;Now you can build conversational apps with just 2 config files, where you need to configure your MCP servers in one file and you need to register your custom components in another file.&lt;/p&gt; &lt;p&gt;Just checkout the &lt;a href="https://github.com/maneeshsandra/open-apps-sdk"&gt;repo&lt;/a&gt; to find out more&lt;/p&gt; &lt;h1&gt;Try It Out&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4b3a85o4a2xf1.png?width=3454&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e32af759004cc346e015f14647484c4c06cd70ad"&gt;A sample application developed with an MCP server with fake store API&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;P.S : A Call for Collaboration&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;I tried publishing it to &lt;a href="https://www.npmjs.com/package/open-apps-sdk?activeTab=readme"&gt;npm&lt;/a&gt; but ran into some issues (turns out packaging is trickier than it looks üòÖ).&lt;/p&gt; &lt;p&gt;If you have experience with npm or package publishing, I‚Äôd &lt;em&gt;love&lt;/em&gt; your guidance or a PR. Let‚Äôs make this SDK easy for anyone to use.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;:Initially I posted almost the same content by taking some help from AI, but looks like community is not pleased with it, so I rewrote the entire post, now this is 100% mine not even a single word by AI&lt;/p&gt; &lt;p&gt;Thanks for the support, please feel free to contribute to the repo&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maneesh_sandra"&gt; /u/maneesh_sandra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oexoct/openai_didnt_open_source_the_apps_sdk_so_i_did/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oexoct/openai_didnt_open_source_the_apps_sdk_so_i_did/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oexoct/openai_didnt_open_source_the_apps_sdk_so_i_did/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T13:24:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1oeg2g6</id>
    <title>AMD Officially Prices Radeon AI PRO R9700 At $1299 - 32GB VRAM - Launch Date Oct 27</title>
    <updated>2025-10-23T21:40:51+00:00</updated>
    <author>
      <name>/u/1ncehost</name>
      <uri>https://old.reddit.com/user/1ncehost</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeg2g6/amd_officially_prices_radeon_ai_pro_r9700_at_1299/"&gt; &lt;img alt="AMD Officially Prices Radeon AI PRO R9700 At $1299 - 32GB VRAM - Launch Date Oct 27" src="https://external-preview.redd.it/1xRs-fKiMj0zISqC9vylqykmOso4ilpVoXcWW0J8xW4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=661bfefccbe35bc33bf966b1c78cea33a1e763bd" title="AMD Officially Prices Radeon AI PRO R9700 At $1299 - 32GB VRAM - Launch Date Oct 27" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1ncehost"&gt; /u/1ncehost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-officially-launches-radeon-ai-pro-r9700-at-1299/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeg2g6/amd_officially_prices_radeon_ai_pro_r9700_at_1299/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oeg2g6/amd_officially_prices_radeon_ai_pro_r9700_at_1299/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T21:40:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1oet55i</id>
    <title>MiniMax-M2 on artificialanalysis.ai ?</title>
    <updated>2025-10-24T09:26:54+00:00</updated>
    <author>
      <name>/u/Leather-Term-30</name>
      <uri>https://old.reddit.com/user/Leather-Term-30</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oet55i/minimaxm2_on_artificialanalysisai/"&gt; &lt;img alt="MiniMax-M2 on artificialanalysis.ai ?" src="https://preview.redd.it/28uj2kbi21xf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b25935779b392d85c0c10e80a52653a296304133" title="MiniMax-M2 on artificialanalysis.ai ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed this new model (MiniMax-M2 ) on &lt;a href="http://artificialanalysis.ai"&gt;artificialanalysis.ai&lt;/a&gt; (it outperforms Gemini 2.5 Pro in their benchmarks). However, I didn't see this model elsewhere, does anybody know anything about it?&lt;/p&gt; &lt;p&gt;Edit: as stated by a well-informed user, the following sentence is on MiniMax's website &amp;quot;üöÄ MiniMax-M2 is coming on Oct 27!&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leather-Term-30"&gt; /u/Leather-Term-30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/28uj2kbi21xf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oet55i/minimaxm2_on_artificialanalysisai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oet55i/minimaxm2_on_artificialanalysisai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T09:26:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oee1ie</id>
    <title>I spent months struggling to understand AI agents. Built a from scratch tutorial so you don't have to.</title>
    <updated>2025-10-23T20:21:05+00:00</updated>
    <author>
      <name>/u/purellmagents</name>
      <uri>https://old.reddit.com/user/purellmagents</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the longest time, I felt lost trying to understand how AI agents actually work.&lt;/p&gt; &lt;p&gt;Every tutorial I found jumped straight into LangChain or CrewAI. The papers were full of architecture diagrams but vague about implementation. I'd follow along, copy-paste code, and it would work... but I had no idea why.&lt;/p&gt; &lt;p&gt;The breaking point: I couldn't debug anything. When something broke, I had no mental model of what was happening under the hood. Was it the framework? The prompt? The model? No clue.&lt;/p&gt; &lt;p&gt;So I did what probably seems obvious in hindsight: I started building from scratch.&lt;/p&gt; &lt;p&gt;Just me, node-llama-cpp, and a lot of trial and error. No frameworks. No abstractions I didn't understand. Just pure fundamentals.&lt;/p&gt; &lt;p&gt;After months of reading, experimenting, and honestly struggling through a lot of confusion, things finally clicked. I understood what function calling really is. Why ReAct patterns work. How memory actually gets managed. What frameworks are actually doing behind their nice APIs.&lt;/p&gt; &lt;p&gt;I put together everything I learned here: &lt;a href="https://github.com/pguso/ai-agents-from-scratch"&gt;https://github.com/pguso/ai-agents-from-scratch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's 8 progressive examples, from &amp;quot;Hello World&amp;quot; to full ReAct agents: - Plain JavaScript, no frameworks - Local LLMs only (Qwen, Llama, whatever you have) - Each example has detailed code breakdowns + concept explanations - Builds from basics to real agent patterns&lt;/p&gt; &lt;p&gt;Topics covered: - System prompts &amp;amp; specialization - Streaming &amp;amp; token control&lt;br /&gt; - Function calling (the &amp;quot;aha!&amp;quot; moment) - Memory systems (very basic) - ReAct pattern (Reasoning + Acting) - Parallel processing&lt;/p&gt; &lt;p&gt;Do you miss something?&lt;/p&gt; &lt;p&gt;Who this is for: - You want to understand agents deeply, not just use them - You're tired of framework black boxes - You learn by building - You want to know what LangChain is doing under the hood&lt;/p&gt; &lt;p&gt;What you'll need: - Node.js - A local GGUF model (I use Qwen 1.7B, runs on modest hardware) instructions in the repo for downloading - Curiosity and patience&lt;/p&gt; &lt;p&gt;I wish I had this resource when I started. Would've saved me months of confusion. Hope it helps someone else on the same journey.&lt;/p&gt; &lt;p&gt;Happy to answer questions about any of the patterns or concepts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purellmagents"&gt; /u/purellmagents &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oee1ie/i_spent_months_struggling_to_understand_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oee1ie/i_spent_months_struggling_to_understand_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oee1ie/i_spent_months_struggling_to_understand_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T20:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oez5lm</id>
    <title>What‚Äôs the best AI coding agent to use with GLM-4.6?</title>
    <updated>2025-10-24T14:25:21+00:00</updated>
    <author>
      <name>/u/Federal_Spend2412</name>
      <uri>https://old.reddit.com/user/Federal_Spend2412</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been using OpenCode with GLM-4.6, and it‚Äôs been my top pick so far. Has anyone found a better option?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal_Spend2412"&gt; /u/Federal_Spend2412 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oez5lm/whats_the_best_ai_coding_agent_to_use_with_glm46/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oez5lm/whats_the_best_ai_coding_agent_to_use_with_glm46/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oez5lm/whats_the_best_ai_coding_agent_to_use_with_glm46/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T14:25:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1of4ypq</id>
    <title>Benchmarking the DGX Spark against the RTX 3090</title>
    <updated>2025-10-24T18:07:49+00:00</updated>
    <author>
      <name>/u/florinandrei</name>
      <uri>https://old.reddit.com/user/florinandrei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ollama has benchmarked the DGX Spark for inference using some of the models in their own collection. They have also released the benchmark script for the test. They used Spark firmware 580.95.05 and Ollama v0.12.6.&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/blog/nvidia-spark-performance"&gt;https://ollama.com/blog/nvidia-spark-performance&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I did a comparison of their numbers on the DGX Spark vs my own RTX 3090. This is how much faster the RTX 3090 is, compared to the DGX Spark, looking only at decode speed (tokens / sec), when using models that fit in a single 3090:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;gemma3 27B q4_K_M: 3.71x gpt-oss 20B MXFP4: 2.52x qwen3 32B q4_K_M: 3.78x &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;EDIT: Bigger models TBD.&lt;/p&gt; &lt;p&gt;My system: Ubuntu 24.04, kernel 6.14.0-33-generic, NVIDIA driver 580.95.05, Ollama v0.12.6.&lt;/p&gt; &lt;p&gt;So the Spark is quite clearly a CUDA development machine. If you do inference and only inference with relatively small models, it's not the best bang for the buck - use something else instead.&lt;/p&gt; &lt;p&gt;Might still be worth it for pure inference with bigger models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/florinandrei"&gt; /u/florinandrei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of4ypq/benchmarking_the_dgx_spark_against_the_rtx_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of4ypq/benchmarking_the_dgx_spark_against_the_rtx_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of4ypq/benchmarking_the_dgx_spark_against_the_rtx_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T18:07:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1of0tnr</id>
    <title>Built a fully local, on-device AI Scribe for clinicians ‚Äî finally real, finally private</title>
    <updated>2025-10-24T15:29:12+00:00</updated>
    <author>
      <name>/u/MajesticAd2862</name>
      <uri>https://old.reddit.com/user/MajesticAd2862</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of0tnr/built_a_fully_local_ondevice_ai_scribe_for/"&gt; &lt;img alt="Built a fully local, on-device AI Scribe for clinicians ‚Äî finally real, finally private" src="https://external-preview.redd.it/dTBucnhtbHB1MnhmMbBjCm2a85KdqkMjM1vyg4FaNP4KyPH0k1X5BnGsr-w4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=89a1120225a393e05b05bcfcec0fb4230cbed364" title="Built a fully local, on-device AI Scribe for clinicians ‚Äî finally real, finally private" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;After two years of tinkering nights and weekends, I finally built what I had in mind: a &lt;strong&gt;fully local, on-device AI scribe&lt;/strong&gt; for clinicians.&lt;/p&gt; &lt;p&gt;üëâ Records, transcribes, and generates structured notes ‚Äî &lt;strong&gt;all running locally on your Mac&lt;/strong&gt;, no cloud, no API calls, no data leaving your device.&lt;/p&gt; &lt;p&gt;The system uses a small foundation model + LoRA adapter that we‚Äôve optimized for clinical language. And the best part: it &lt;strong&gt;anchors every sentence of the note to the original transcript&lt;/strong&gt; ‚Äî so you can hover over any finding and see exactly &lt;em&gt;where&lt;/em&gt; in the conversation it came from. We call this &lt;strong&gt;Evidence Anchoring&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It‚Äôs been wild seeing it outperform GPT-5 on hallucination tests ‚Äî about 3√ó fewer unsupported claims ‚Äî simply because everything it writes must tie back to actual evidence in the transcript.&lt;/p&gt; &lt;p&gt;If you‚Äôre on macOS (M1/M2/M3) and want to try it, we‚Äôve opened a &lt;strong&gt;beta&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;You can sign up at &lt;a href="https://omiscribe.com/"&gt;omiscribe.com&lt;/a&gt; or DM me for a TestFlight invite.&lt;/p&gt; &lt;p&gt;LocalLLama and the local-AI community honestly kept me believing this was possible. üôè Would love to hear what you think ‚Äî especially from anyone doing clinical documentation, med-AI, or just interested in local inference on Apple hardware.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MajesticAd2862"&gt; /u/MajesticAd2862 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fhq9jnlpu2xf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of0tnr/built_a_fully_local_ondevice_ai_scribe_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of0tnr/built_a_fully_local_ondevice_ai_scribe_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T15:29:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1oeyzxq</id>
    <title>[ü™® Onyx v2.0.0] Self-hosted chat and RAG - now with FOSS repo, SSO, new design/colors, and projects!</title>
    <updated>2025-10-24T14:19:13+00:00</updated>
    <author>
      <name>/u/Weves11</name>
      <uri>https://old.reddit.com/user/Weves11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeyzxq/onyx_v200_selfhosted_chat_and_rag_now_with_foss/"&gt; &lt;img alt="[ü™® Onyx v2.0.0] Self-hosted chat and RAG - now with FOSS repo, SSO, new design/colors, and projects!" src="https://b.thumbs.redditmedia.com/1g7bw_MAWvmmqQ_XM_zZ3Opapd4T9MrJQmyySlO22qg.jpg" title="[ü™® Onyx v2.0.0] Self-hosted chat and RAG - now with FOSS repo, SSO, new design/colors, and projects!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey friends, I‚Äôve got a big Onyx update for you guys! &lt;/p&gt; &lt;p&gt;I heard your feedback loud and clear last time - and thanks to the great suggestions I‚Äôve 1/ released a fully FOSS, MIT-licensed version of Onyx, 2/ open-sourced OIDC/SAML, and 3/ did a complete makeover of the design and colors. &lt;/p&gt; &lt;p&gt;If you don‚Äôt know - Onyx is an open-source, self-hostable chat UI that has support for every LLM plus built in RAG + connectors + MCP + web search + deep research.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Everything that‚Äôs new:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open-sourced SSO (OIDC + SAML) &lt;/li&gt; &lt;li&gt;onyx-foss (&lt;a href="https://github.com/onyx-dot-app/onyx-foss"&gt;https://github.com/onyx-dot-app/onyx-foss&lt;/a&gt;), a completely MIT licensed version of Onyx&lt;/li&gt; &lt;li&gt;Brand new design / colors&lt;/li&gt; &lt;li&gt;Projects (think Claude projects, but with any model + self-hosted)&lt;/li&gt; &lt;li&gt;Organization info and personalization&lt;/li&gt; &lt;li&gt;Reworked core tool-calling loop. Uses native tool calling for better adherence, fewer history rewrites for better prompt caching, and less hand-crafted prompts for fewer artifacts in longer runs&lt;/li&gt; &lt;li&gt;OAuth support for OpenAPI-based tools&lt;/li&gt; &lt;li&gt;A bunch of bug fixes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Really appreciate all the feedback from last time, and looking forward to more of it here. Onyx was briefly #1 python and #2 github trending repo of the day, which is so crazy to me.&lt;/p&gt; &lt;p&gt;If there‚Äôs anything else that you would find useful that‚Äôs NOT part of the MIT license please let me know and I‚Äôll do my best to move it over. All of the core functionality mentioned above is 100% FOSS. I want everything needed for the best open-source chat UI to be completely free and usable by all!&lt;/p&gt; &lt;p&gt;Repo:&lt;a href="https://github.com/onyx-dot-app/onyx"&gt; https://github.com/onyx-dot-app/onyx&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Full release notes:&lt;a href="https://docs.onyx.app/changelog#v2-0-0"&gt; https://docs.onyx.app/changelog#v2-0-0&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weves11"&gt; /u/Weves11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oeyzxq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeyzxq/onyx_v200_selfhosted_chat_and_rag_now_with_foss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oeyzxq/onyx_v200_selfhosted_chat_and_rag_now_with_foss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T14:19:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1of0xc1</id>
    <title>GLM 4.6 coding Benchmarks</title>
    <updated>2025-10-24T15:33:10+00:00</updated>
    <author>
      <name>/u/IndependentFresh628</name>
      <uri>https://old.reddit.com/user/IndependentFresh628</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did they fake Coding benchmarks where it is visible GLM 4.6 is neck to neck with Claude Sonnet 4.5 however, in real world Use it is not even close to Sonnet when it comes Debug or Efficient problem solving.&lt;/p&gt; &lt;p&gt;But yeah, GLM can generate massive amount of Coding tokens in one prompt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IndependentFresh628"&gt; /u/IndependentFresh628 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of0xc1/glm_46_coding_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of0xc1/glm_46_coding_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of0xc1/glm_46_coding_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T15:33:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1oeigeh</id>
    <title>Amongst safety cuts, Facebook is laying off the Open Source LLAMA folks</title>
    <updated>2025-10-23T23:25:21+00:00</updated>
    <author>
      <name>/u/eredhuin</name>
      <uri>https://old.reddit.com/user/eredhuin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.nytimes.com/2025/10/23/technology/meta-layoffs-user-privacy.html?unlocked_article_code=1.vk8.8nWb.yFO38KVrwYZW&amp;amp;smid=nytcore-ios-share&amp;amp;referringSource=articleShare"&gt;https://www.nytimes.com/2025/10/23/technology/meta-layoffs-user-privacy.html?unlocked_article_code=1.vk8.8nWb.yFO38KVrwYZW&amp;amp;smid=nytcore-ios-share&amp;amp;referringSource=articleShare&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Beyond Meta‚Äôs risk organization, other cuts on Wednesday targeted veteran members of Meta‚Äôs FAIR team and&lt;/em&gt; &lt;strong&gt;&lt;em&gt;those who had worked on previous versions of Meta‚Äôs open source A.I. models, called Llama.&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Among the employees who were laid off was Yuandong Tian, FAIR‚Äôs research director, who had been at the company for eight years.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;But there was one division that was spared: TBD Labs, the organization largely made up of new, highly paid recruits working on the next generation of A.I. research. The department is led by Mr. Wang.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eredhuin"&gt; /u/eredhuin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeigeh/amongst_safety_cuts_facebook_is_laying_off_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeigeh/amongst_safety_cuts_facebook_is_laying_off_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oeigeh/amongst_safety_cuts_facebook_is_laying_off_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T23:25:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oeuiev</id>
    <title>Is OpenAI afraid of Kimi?</title>
    <updated>2025-10-24T10:50:59+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeuiev/is_openai_afraid_of_kimi/"&gt; &lt;img alt="Is OpenAI afraid of Kimi?" src="https://b.thumbs.redditmedia.com/apnbgQLHwz7nx79BJoKIRI6eUEamaMguXqyI2K8LM8M.jpg" title="Is OpenAI afraid of Kimi?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;roon from OpenAI posted this earlier&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5hqotg83i1xf1.jpg?width=1190&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f1396023a25350b27a94a3e4225bf38eb4ae86c3"&gt;https://preview.redd.it/5hqotg83i1xf1.jpg?width=1190&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f1396023a25350b27a94a3e4225bf38eb4ae86c3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Then he instantly deleted the tweet&lt;/strong&gt; lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeuiev/is_openai_afraid_of_kimi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oeuiev/is_openai_afraid_of_kimi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oeuiev/is_openai_afraid_of_kimi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T10:50:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oes4ez</id>
    <title>Qwen3 Next support in llama.cpp ready for review</title>
    <updated>2025-10-24T08:18:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oes4ez/qwen3_next_support_in_llamacpp_ready_for_review/"&gt; &lt;img alt="Qwen3 Next support in llama.cpp ready for review" src="https://external-preview.redd.it/JWuwM-H5pHYaaKPNtY_8U3LHlrsSjJTNAjLHRGwU5o0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=016d876487cc90150078e6b226c52b29735d5532" title="Qwen3 Next support in llama.cpp ready for review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Congratulations to Piotr for his hard work, the code is now ready for review.&lt;/p&gt; &lt;p&gt;Please note that this is not the final version, and if you download some quantized models, you will probably need to download them again later. Also, it's not yet optimized for speed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oes4ez/qwen3_next_support_in_llamacpp_ready_for_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oes4ez/qwen3_next_support_in_llamacpp_ready_for_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T08:18:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oextwc</id>
    <title>GLM-4.6-Air is not forgotten!</title>
    <updated>2025-10-24T13:31:12+00:00</updated>
    <author>
      <name>/u/codys12</name>
      <uri>https://old.reddit.com/user/codys12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oextwc/glm46air_is_not_forgotten/"&gt; &lt;img alt="GLM-4.6-Air is not forgotten!" src="https://preview.redd.it/z5dduynua2xf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b43f43a244e84de5bb07a0bc9e4c16127860c9a4" title="GLM-4.6-Air is not forgotten!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codys12"&gt; /u/codys12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z5dduynua2xf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oextwc/glm46air_is_not_forgotten/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oextwc/glm46air_is_not_forgotten/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T13:31:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1of5ywl</id>
    <title>What‚Äôs even the goddamn point?</title>
    <updated>2025-10-24T18:47:20+00:00</updated>
    <author>
      <name>/u/ChockyBlox</name>
      <uri>https://old.reddit.com/user/ChockyBlox</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of5ywl/whats_even_the_goddamn_point/"&gt; &lt;img alt="What‚Äôs even the goddamn point?" src="https://preview.redd.it/9fjtexb9v3xf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d398d419e5a3d539c3f2c82b07408ef22f90899" title="What‚Äôs even the goddamn point?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To be fair I will probably never use this model for any real use cases, but these corporations do need to go a little easy on the restrictions and be less paranoid.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChockyBlox"&gt; /u/ChockyBlox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9fjtexb9v3xf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1of5ywl/whats_even_the_goddamn_point/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1of5ywl/whats_even_the_goddamn_point/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-24T18:47:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1obqkpe</id>
    <title>Best Local LLMs - October 2025</title>
    <updated>2025-10-20T19:06:06+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Welcome to the first monthly &amp;quot;Best Local LLMs&amp;quot; post!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;General&lt;/li&gt; &lt;li&gt;Agentic/Tool Use&lt;/li&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;Creative Writing/RP&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(look for the top level comments for each Application and please thread your responses under that)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:06:06+00:00</published>
  </entry>
</feed>
