<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-28T08:41:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oi1tj9</id>
    <title>Multi-Backend LLM Router - Automatic Model &amp; Backend Switching for SGLang/llama.cpp/TabbyAPI</title>
    <updated>2025-10-28T06:04:54+00:00</updated>
    <author>
      <name>/u/darkmaniac7</name>
      <uri>https://old.reddit.com/user/darkmaniac7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi1tj9/multibackend_llm_router_automatic_model_backend/"&gt; &lt;img alt="Multi-Backend LLM Router - Automatic Model &amp;amp; Backend Switching for SGLang/llama.cpp/TabbyAPI" src="https://a.thumbs.redditmedia.com/t_4CGdNRBmzo6DCNSYaEN1K722bIyWYcuC4W1NQqr58.jpg" title="Multi-Backend LLM Router - Automatic Model &amp;amp; Backend Switching for SGLang/llama.cpp/TabbyAPI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, wanted to share something I put together that solved a major headache for me and might help a few of you too.&lt;/p&gt; &lt;p&gt;It's entirely possible this already exists as another name or service, but I couldn't find it.&lt;/p&gt; &lt;p&gt;I’m not a coder. This is the first time I even made a GitHub repo, But I got tired of constantly switching between different LLM backends (SGLang/AWQ, llama.cpp/GGUF, TabbyAPI/EXL2). Every time I wanted to test a new model, it turned into a 20-minute ritual of stopping services, editing configs, remembering which port did what was a total pain.&lt;/p&gt; &lt;p&gt;I had Claude build a model router that exposes an OpenAI-style API and plugs right into Open-WebUI. Now I just pick a model from the dropdown, and it handles all the backend switching automatically. No manual restarts, no config editing, no guessing which backend is running.&lt;/p&gt; &lt;h1&gt;What it actually does&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;No more backend juggling. It stops the current service, fires up the right one, loads the model, and proxies everything through automatically.&lt;/li&gt; &lt;li&gt;Performance stats after every response. Example: ⚡ 45.2 tok/s (180 tokens in 4.0s)&lt;/li&gt; &lt;li&gt;Simple model management. Add or remove models with a built-in script no JSON editing required.&lt;/li&gt; &lt;li&gt;Handles systemd services, health checks, timeouts, and even does a real inference test before marking a backend healthy.&lt;/li&gt; &lt;li&gt;While switching, streams updated time and model info so you know it hasn't frozen or died.&lt;/li&gt; &lt;li&gt;Confirmed working with Blackwell GPUs. Tested on an RTX Pro 6000 with CUDA arch tweaks included.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Quick visual&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Client (Open-WebUI) ↓ Router (8002) ↓ ┌──────┴──────┐ ↓ ↓ ↓ SGLang llama TabbyAPI (30000) (8085) (5000) AWQ GGUF EXL2 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When you pick a model:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The router checks which backend it needs.&lt;/li&gt; &lt;li&gt;Stops anything else running.&lt;/li&gt; &lt;li&gt;Starts the right backend.&lt;/li&gt; &lt;li&gt;Streams your response back.&lt;/li&gt; &lt;li&gt;Shows token performance when it’s done.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;All selectable directly from Open-WebUI (should work for others too. I've only tested on Open-Webui) no service restarts, no config edits. Switching models is instant and effortless.&lt;/p&gt; &lt;h1&gt;Install&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/darkmaniac7/LLM-Model-Router.git cd LLM-Model-Router sudo ./install.sh # Follow prompts &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then add your models:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo /opt/llm-router/manage-models.sh add # Choose backend, enter model path, done &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;TL;DR: It’s a drop-in router for AWQ/GGUF/EXL2 backends that gives you one OpenAI-compatible endpoint, automatic backend switching, systemd integration, live token stats, and dead-simple model management.&lt;/p&gt; &lt;p&gt;Repo is here: &lt;a href="https://github.com/darkmaniac7/LLM-Model-Router?utm_source=chatgpt.com"&gt;https://github.com/darkmaniac7/LLM-Model-Router&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if you try it or hit any issues. I’m curious how it runs in other setups.&lt;/p&gt; &lt;p&gt;If any actual devs like it and want to change anything please feel free.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/darkmaniac7"&gt; /u/darkmaniac7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oi1tj9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi1tj9/multibackend_llm_router_automatic_model_backend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi1tj9/multibackend_llm_router_automatic_model_backend/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T06:04:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohu19n</id>
    <title>Flagship LLM on 128GB</title>
    <updated>2025-10-27T23:27:24+00:00</updated>
    <author>
      <name>/u/EffectiveGlove1651</name>
      <uri>https://old.reddit.com/user/EffectiveGlove1651</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello ! Running an M4 Max Mac Studio with 128GB RAM. Currently using OSS20B but wondering if I should go bigger for better performance. What models do you recommend for this setup? Worth stepping up in size? Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EffectiveGlove1651"&gt; /u/EffectiveGlove1651 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohu19n/flagship_llm_on_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohu19n/flagship_llm_on_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohu19n/flagship_llm_on_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T23:27:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohox4r</id>
    <title>Kiln Agent Builder (new): Build agentic systems in minutes with tools, sub-agents, RAG, and context management [Kiln]</title>
    <updated>2025-10-27T20:04:04+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohox4r/kiln_agent_builder_new_build_agentic_systems_in/"&gt; &lt;img alt="Kiln Agent Builder (new): Build agentic systems in minutes with tools, sub-agents, RAG, and context management [Kiln]" src="https://external-preview.redd.it/d3Zva2N5a2ZucHhmMTkufcVC4ejmCLafkThKi0kTFLLjDsIWZAXwIzmkOAnA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0861e31fca0f585073e8017454429b3c233ac06" title="Kiln Agent Builder (new): Build agentic systems in minutes with tools, sub-agents, RAG, and context management [Kiln]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just added an interactive Agent builder to &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;the GitHub project Kiln&lt;/a&gt;. With it you can build agentic systems in under 10 minutes. You can do it all through our UI, or use our python library.&lt;/p&gt; &lt;p&gt;What is it? Well “agentic” is just about the most overloaded term in AI, but Kiln supports everything you need to build agents:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.kiln.tech/docs/agents#tool-use"&gt;Tool Use&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.kiln.tech/docs/agents#multi-actor-interaction-aka-subtasks"&gt;Multi-Actor Interaction (aka subtasks)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.kiln.tech/docs/agents#goal-directed-autonomy-and-reasoning"&gt;Goal Directed, Autonomous Looping &amp;amp; Reasoning&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.kiln.tech/docs/agents#state-and-memory"&gt;State &amp;amp; Memory&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Context Management with Subtasks (aka Multi-Actor Pattern)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Context management is the process of curating the model's context (chat/tool history) to ensure it has the right data, at the right time, in the right level of detail to get the job done.&lt;/p&gt; &lt;p&gt;With Kiln you can implement context management by dividing your agent tasks into subtasks, making context management easy. Each subtask can focus within its own context, then compress/summarize for the parent task. This can make the system faster, cheaper and higher quality. See our &lt;a href="https://docs.kiln.tech/docs/agents#context-management"&gt;docs on context management&lt;/a&gt; for more details.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Eval &amp;amp; Optimize Agent Performance&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Kiln agents work with &lt;a href="https://docs.kiln.tech/docs/evaluations"&gt;Kiln evals&lt;/a&gt; so you can measure and improve agent performance:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Find the ideal model to use, balancing quality, cost and speed&lt;/li&gt; &lt;li&gt;Test different prompts&lt;/li&gt; &lt;li&gt;Evaluate end-to-end quality, or focus on the quality of subtasks&lt;/li&gt; &lt;li&gt;Compare different agent system designs: more/fewer subtasks&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links and Docs&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Some links to the repo and guides:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/Kiln-AI/Kiln"&gt;Kiln AI on Github - 4k stars&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.kiln.tech/docs/agents"&gt;Docs for Kiln Agents&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://getkiln.ai/discord"&gt;Kiln Discord&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://kiln.tech/"&gt;Homepage&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feedback and suggestions are very welcome! We’re already working on custom evals to inspect the trace, and ensure the right tools are used at the right times. What else would be helpful? Any other agent memory patterns you’d want to see?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/71g2oykfnpxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohox4r/kiln_agent_builder_new_build_agentic_systems_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohox4r/kiln_agent_builder_new_build_agentic_systems_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T20:04:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh5asg</id>
    <title>🚀 New Model from the MiniMax team: MiniMax-M2, an impressive 230B-A10B LLM.</title>
    <updated>2025-10-27T04:28:24+00:00</updated>
    <author>
      <name>/u/chenqian615</name>
      <uri>https://old.reddit.com/user/chenqian615</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh5asg/new_model_from_the_minimax_team_minimaxm2_an/"&gt; &lt;img alt="🚀 New Model from the MiniMax team: MiniMax-M2, an impressive 230B-A10B LLM." src="https://a.thumbs.redditmedia.com/b3_JzXejnThTVzO8xn7-iIWxAt3NTQCnaQo4XEvZSC0.jpg" title="🚀 New Model from the MiniMax team: MiniMax-M2, an impressive 230B-A10B LLM." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Officially positioned as an “end-to-end coding + tool-using agent.” From the public evaluations and model setup, it looks well-suited for teams that need end to end development and toolchain agents, prioritizing lower latency and higher throughput. For real engineering workflows that advance in small but continuous steps, it should offer strong cost-effectiveness. I’ve collected a few points to help with evaluation:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;End-to-end workflow oriented, emphasizing multi-file editing, code, run, fix loops, testing/verification, and long-chain tool orchestration across terminal/browser/retrieval/code execution. These capabilities matter more than just chatting when deploying agents.&lt;/li&gt; &lt;li&gt;Publicly described as “~10B activated parameters (total ~200B).” The design aims to reduce inference latency and per unit cost while preserving coding and tool-calling capabilities, making it suitable for high concurrency and batch sampling.&lt;/li&gt; &lt;li&gt;Benchmark coverage spans end-to-end software engineering (SWE-bench, Terminal-Bench, ArtifactsBench), browsing/retrieval tasks (BrowseComp, FinSearchComp), and holistic intelligence profiling (AA Intelligence).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Position in public benchmarks (not the absolute strongest, but well targeted)&lt;/p&gt; &lt;p&gt;Here are a few developer-relevant metrics I pulled from public tables:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SWE-bench Verified: 69.4&lt;/li&gt; &lt;li&gt;Terminal-Bench: 46.3&lt;/li&gt; &lt;li&gt;ArtifactsBench: 66.8&lt;/li&gt; &lt;li&gt;BrowseComp: 44.0 (BrowseComp-zh in Chinese: 48.5)&lt;/li&gt; &lt;li&gt;τ²-Bench: 77.2&lt;/li&gt; &lt;li&gt;FinSearchComp-global: 65.5&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;From the scores, on tasks that require real toolchain collaboration, this model looks like a balanced choice prioritizing efficiency and stability. Some closed-source models score higher on certain benchmarks, but for end to end development/ agent pipelines, its price performance orientation is appealing. On SWE-bench / Multi-SWE-Bench, steadily completing the modify test modify again loop is often more important than a one-shot perfect fix. These scores and its positioning suggest it can keep pushing the loop toward a runnable solution. A Terminal-Bench score of 46.3 indicates decent robustness in command execution, error recovery, and retries worth trying in a real CI sandbox for small-scale tasks.&lt;/p&gt; &lt;p&gt;References&lt;/p&gt; &lt;p&gt;HF:&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chenqian615"&gt; /u/chenqian615 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oh5asg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh5asg/new_model_from_the_minimax_team_minimaxm2_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh5asg/new_model_from_the_minimax_team_minimaxm2_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T04:28:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohwjmq</id>
    <title>DeepSeek-OCR question for my workflow below...</title>
    <updated>2025-10-28T01:20:43+00:00</updated>
    <author>
      <name>/u/Excellent_Koala769</name>
      <uri>https://old.reddit.com/user/Excellent_Koala769</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohwjmq/deepseekocr_question_for_my_workflow_below/"&gt; &lt;img alt="DeepSeek-OCR question for my workflow below..." src="https://preview.redd.it/2ghkk6328rxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7c9a433a11837865ed74195eb17db4ef389640e" title="DeepSeek-OCR question for my workflow below..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please take a look at these questions after reviewing my workflow above:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Could I compress multiple PNGs, combine them into one image, and then process them as one image for text extraction? &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Would this model run on my Mac Mini 2024 M4 Base model? And would it be faster than Azure deployments strategy.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Would the model be as precise as GPT-4o's Vision? 4o is very good at this extraction job.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any feedback is greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Koala769"&gt; /u/Excellent_Koala769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2ghkk6328rxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohwjmq/deepseekocr_question_for_my_workflow_below/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohwjmq/deepseekocr_question_for_my_workflow_below/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T01:20:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqulc</id>
    <title>Looking for a local llm thats good with warhammer 40k lore, Preferably below 10B</title>
    <updated>2025-10-27T21:17:41+00:00</updated>
    <author>
      <name>/u/Hakukh123</name>
      <uri>https://old.reddit.com/user/Hakukh123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone&lt;/p&gt; &lt;p&gt;So i work in places with spotty/no internet pretty often and im new to &lt;strong&gt;40k lore&lt;/strong&gt;. been trying to find a decent local llm that knows its stuff about &lt;strong&gt;warhammer lore&lt;/strong&gt; so i can ask questions, brainstorm some stuff, or just chat about the setting when im bored.&lt;/p&gt; &lt;p&gt;ive tried a few models through lm studio but they seem pretty hit or miss with the lore - like they know the basic stuff (emperor, chaos, space marines) but when you get into specifics they start making things up or mixing factions.&lt;/p&gt; &lt;p&gt;wondering if anyone here has found a model that actually handles specialized lore well? or if anyone has fine-tuned something for 40k specifically? not looking for anything crazy powerful, just something that can run offline and actually knows the difference between a custodes and a primaris lol.&lt;/p&gt; &lt;p&gt;my setup can handle up to maybe 8b comfortably, could push 10b if its really worth it&lt;/p&gt; &lt;p&gt;any recommendations appreciated, thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hakukh123"&gt; /u/Hakukh123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqulc/looking_for_a_local_llm_thats_good_with_warhammer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqulc/looking_for_a_local_llm_thats_good_with_warhammer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqulc/looking_for_a_local_llm_thats_good_with_warhammer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:17:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohzfdu</id>
    <title>VellumForge2 - A high performance, very configurable and really easy to use DPO dataset generation tool, create high quality datasets for completely free</title>
    <updated>2025-10-28T03:41:47+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally releasing my new dataset generation tool, and some Fantasy writing datasets to go with it (soon). &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lemon07r/VellumForge2"&gt;https://github.com/lemon07r/VellumForge2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sample Dataset: &lt;a href="https://huggingface.co/collections/lemon07r/vellumforge2-datasets"&gt;https://huggingface.co/collections/lemon07r/vellumforge2-datasets&lt;/a&gt; (large datasets coming soon)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Functionality&lt;/strong&gt; (all you need for a tl;dr)&lt;/p&gt; &lt;p&gt;This tool creates DPO-style datasets using a main topic and LLMs to generate subtopics, prompts, and chosen/rejected response pairs through a hierarchical pipeline. What sets it apart is the optional LLM-as-a-judge rubric scoring system, inspired by how Kimi K2 was trained using rubric-based evaluation to generate higher quality writing samples. The output uses a flexible &amp;quot;one-to-many&amp;quot; hybrid schema that works seamlessly with DPOTrainer, RewardTrainer, and MORL training, no data transformation needed. You can also skip the judge entirely for DPO training or just use the prompt and chosen responses for SFT.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Overview &amp;amp; Features&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My original python script that I was using for making datasets worked mostly fine, but I broke it, many many times trying to refactor it and add features to it. It did get to a good place at some point, with working async, rate limiting, etc, before I broke it again with some experimental stuff that turned out to not be a good idea even if it did work. Some good lessons learned here.&lt;/p&gt; &lt;p&gt;What I did learn, I used in my complete re-write of the tool. This time I wrote it in Go, and kept it very simple and easy to use. I also kept it very modular and highly configurable from the very start. This tool works with any OpenAI-compatible API including local servers like llama.cpp, kobold.cpp, LM studio, vLLM or Ollama. Handles rate limiting automatically, supports concurrent workers, and can upload directly to Hugging Face Hub in one command, which was implemented without needing any external tools/dependencies like the HF cli. Generation templates are fully customizable via TOML config, meaning you can make any type of dataset. The example configs come with a strong default template for fantasy writing to help give an idea of what a good template would look like. The documentation includes a thorough quick start guide, and examples. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dataset Generation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This thing works fast. Had a much bigger impact than I expected in dataset generation speed compared to the old tool. Even using the completely free (and unlimited) Nvidia NIM api with it's 40 RPM rate limit and slow 20-30 tps Kimi K2 0905 model, plus any small local model for rejected responses, you can create a very high quality (possibly only topped by using Sonnet 4.5) DPO datasets, with about 1000 rows of high quality data in under a few hours, for completely free. No expensive hardware or API provider required (which of course you can use with this tool too). The sample dataset I linked completed under these conditions in only a 36-minute run, which would have been only half as long without a judge. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohzfdu/vellumforge2_a_high_performance_very_configurable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohzfdu/vellumforge2_a_high_performance_very_configurable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohzfdu/vellumforge2_a_high_performance_very_configurable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T03:41:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi3sld</id>
    <title>Public Service Announcement</title>
    <updated>2025-10-28T08:20:51+00:00</updated>
    <author>
      <name>/u/researchAmericanAI</name>
      <uri>https://old.reddit.com/user/researchAmericanAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi3sld/public_service_announcement/"&gt; &lt;img alt="Public Service Announcement" src="https://a.thumbs.redditmedia.com/7hx4F9bnCoR9Bc4hYCWktlZnBnyqlfD4PLb2wgCdyv4.jpg" title="Public Service Announcement" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI provides a way to solve problems. &lt;/p&gt; &lt;p&gt;It records how you solve them. &lt;/p&gt; &lt;p&gt;It guesses the answers. &lt;/p&gt; &lt;p&gt;You confirm when it's correct. &lt;/p&gt; &lt;p&gt;AI doesn't solve problems. &lt;/p&gt; &lt;p&gt;You do. &lt;/p&gt; &lt;p&gt;Step-by-Step.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/9mgqdps2btxf1.gif"&gt;https://i.redd.it/9mgqdps2btxf1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/researchAmericanAI"&gt; /u/researchAmericanAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi3sld/public_service_announcement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi3sld/public_service_announcement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi3sld/public_service_announcement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T08:20:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi3w68</id>
    <title>Flex Attention vs Flash Attention 3</title>
    <updated>2025-10-28T08:27:54+00:00</updated>
    <author>
      <name>/u/Extra-Designer9333</name>
      <uri>https://old.reddit.com/user/Extra-Designer9333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm pretty new to accelerated framework APIs like FlexAttn from PyTorch team and FlashAttn from Tri Dao out of Princeton. Unsloth itself uses Flex Attn as I know and reports: &amp;quot;10x faster on a single GPU and up to 30x faster on multiple GPU systems compared to Flash Attention 2 (FA2).&amp;quot; However, FlashAttn 3 turns out to be 1.5-2x faster than FlashAttn 2. &lt;/p&gt; &lt;p&gt;I'm trying to decide which one to use for training my LLM whether it's FlexAttn (Unsloth) or FlashAttn 3. What's your personal suggestion and experience you had from these 2. Which one is more error prone, which turns out to be more memory heavy or computationally less expensive and etc.&lt;/p&gt; &lt;p&gt;Thank you all in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Extra-Designer9333"&gt; /u/Extra-Designer9333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi3w68/flex_attention_vs_flash_attention_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi3w68/flex_attention_vs_flash_attention_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi3w68/flex_attention_vs_flash_attention_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T08:27:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohbcu1</id>
    <title>Experience with the new model MiniMax M2 and some cost saving tips</title>
    <updated>2025-10-27T11:00:41+00:00</updated>
    <author>
      <name>/u/thalacque</name>
      <uri>https://old.reddit.com/user/thalacque</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohbcu1/experience_with_the_new_model_minimax_m2_and_some/"&gt; &lt;img alt="Experience with the new model MiniMax M2 and some cost saving tips" src="https://b.thumbs.redditmedia.com/UVUhaaqNSDCk6qFXVB2lhPVQVQLnJtZQw5XfM0IrY1I.jpg" title="Experience with the new model MiniMax M2 and some cost saving tips" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw the discussion about MiniMax M2 in the group chat a couple of days ago, and since their API and agent are free to use, I thought I’d test it out. First, the conclusion: in my own use, M2 delivers better than expected efficiency and stability. You can feel the team has pushed the model’s strengths close to top closed models. In some scenarios it reaches top results at clearly lower cost, so it fits as the default executor, with closed models kept for final polish when needed.&lt;/p&gt; &lt;p&gt;My comparison across models:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;A three service monorepo dependency and lock file mess (Node.js + Express). The three services used different versions of jsonwebtoken and had lock file conflicts. The goal was to unify versions, upgrade jwt.verify from callback to Promise, and add an npm run bootstrap script for one click dependency setup and alignment.&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;M2: breaks down todos, understands the task well, reads files first, lists a plan, then edits step by step. It detects three version drifts and proposes an alignment strategy, adds the bootstrap script, runs one round of install and startup checks. Small fixes are quick, friendly to regression runs, and it feels ready to drop into a pipeline for repeated runs. Claude: strong first pass, but cross service consistency sometimes needed repeated reminders, took more rounds, and usage cost was higher. GLM/Kimi: can get the main path working, but more likely to leave rough edges in lock files and scripts that I had to clean up.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;An online 3x3 Rubik’s Cube (a small front end interaction project): rotate a layer to a target angle, buttons to choose a face, show the 3x3 color grid.&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;M2: To be honest, the first iteration wasn’t great, major issues like text occlusion and non-functional rotation weren’t addressed. The bright spot is that interaction bugs (e.g., rotation state desynchronization) could be fixed in a single pass once pointed out, without introducing new regressions. After subsequent rounds of refinement, the final result actually became the most usable and presentable, fully supporting 3D dragging. GLM/Kimi: The first round results were decent, but both ran into problems in the second round. GLM didn’t resolve the Rubik’s Cube floating/hover position issue, and Kimi, after the second round feedback, ended up not being three-dimensional. Claude performed excellently after the first round of prompts, with all features working normally, but even after multiple later rounds it still didn’t demonstrate an understanding of a 3D cube (in the image, Claude’s Rubik’s Cube is flat and the view can’t be rotated).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Metrics echo this feel: SWE bench Verified 69.4, Terminal Bench 46.3, ArtifactsBench 66.8, BrowseComp 44.0, FinSearchComp global 65.5. It is not first in every category, but on the runnable and fixable engineering loop, the structure score looks better. From my use, the strengths are proposing a plan, checking its own work, and favoring short fast iterations that clear blockers one by one.&lt;/p&gt; &lt;p&gt;Replace most closed model usage without sacrificing the reliability of the engineering loop. M2 is already enough and surprisingly handy. Set it as the default executor and run regressions for two days; the difference will be clear. After putting it into the pipeline, with the same budget you can run more in parallel, and you do save money.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/MiniMax-AI/MiniMax-M2"&gt;https://github.com/MiniMax-AI/MiniMax-M2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thalacque"&gt; /u/thalacque &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ohbcu1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohbcu1/experience_with_the_new_model_minimax_m2_and_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohbcu1/experience_with_the_new_model_minimax_m2_and_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T11:00:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohrn20</id>
    <title>Investigating Apple's new "Neural Accelerators" in each GPU core (A19 Pro vs M4 Pro vs M4 vs RTX 3080 - Local LLM Speed Test!)</title>
    <updated>2025-10-27T21:48:31+00:00</updated>
    <author>
      <name>/u/TechExpert2910</name>
      <uri>https://old.reddit.com/user/TechExpert2910</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone :D&lt;/p&gt; &lt;p&gt;I thought it’d be really interesting to compare how Apple's new A19 Pro (and in turn, the M5) with its fancy &lt;strong&gt;new &amp;quot;neural accelerators&amp;quot; in each GPU core&lt;/strong&gt; compare to other GPUs!&lt;/p&gt; &lt;p&gt;I ran Gemma 3n 4B on each of these devices, outputting ~the same 100-word story (at a temp of 0). I used the most optimal inference framework for each to give each their best shot.&lt;/p&gt; &lt;p&gt;Here're the results!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;GPU&lt;/th&gt; &lt;th align="left"&gt;Device&lt;/th&gt; &lt;th align="left"&gt;Inference Set-Up&lt;/th&gt; &lt;th align="left"&gt;Tokens / Sec&lt;/th&gt; &lt;th align="left"&gt;Time to First Token&lt;/th&gt; &lt;th align="left"&gt;Perf / GPU Core&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;A19 Pro&lt;/td&gt; &lt;td align="left"&gt;6 GPU cores; iPhone 17 Pro Max&lt;/td&gt; &lt;td align="left"&gt;MLX? (“Local Chat” app)&lt;/td&gt; &lt;td align="left"&gt;23.5 tok/s&lt;/td&gt; &lt;td align="left"&gt;0.4 s 👀&lt;/td&gt; &lt;td align="left"&gt;3.92&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;M4&lt;/td&gt; &lt;td align="left"&gt;10 GPU cores, iPad Pro 13”&lt;/td&gt; &lt;td align="left"&gt;MLX? (“Local Chat” app)&lt;/td&gt; &lt;td align="left"&gt;33.4 tok/s&lt;/td&gt; &lt;td align="left"&gt;1.1 s&lt;/td&gt; &lt;td align="left"&gt;3.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RTX 3080&lt;/td&gt; &lt;td align="left"&gt;10 GB VRAM; paired with a Ryzen 5 7600 + 32 GB DDR5&lt;/td&gt; &lt;td align="left"&gt;CUDA 12 llama.cpp (LM Studio)&lt;/td&gt; &lt;td align="left"&gt;59.1 tok/s&lt;/td&gt; &lt;td align="left"&gt;0.02 s&lt;/td&gt; &lt;td align="left"&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;M4 Pro&lt;/td&gt; &lt;td align="left"&gt;16 GPU cores, MacBook Pro 14”, 48 GB unified memory&lt;/td&gt; &lt;td align="left"&gt;MLX (LM Studio)&lt;/td&gt; &lt;td align="left"&gt;60.5 tok/s 👑&lt;/td&gt; &lt;td align="left"&gt;0.31 s&lt;/td&gt; &lt;td align="left"&gt;3.69&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Super Interesting Notes:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. The neural accelerators didn't make much of a difference. Here's why!&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;First off, they do indeed significantly accelerate compute! &lt;a href="https://tzakharko.github.io/apple-neural-accelerators-benchmark/#:%7E:text=Metal%20Shading%20Language.-,Key%20Takeaways%3A,-Operation"&gt;Taras Zakharko found that&lt;/a&gt; Matrix FP16 and Matrix INT8 are already accelerated by 4x and 7x respectively!!!&lt;/li&gt; &lt;li&gt;BUT, when the LLM spits out tokens, we're limited by memory bandwidth, NOT compute. This is especially true with Apple's iGPUs using the comparatively low-memory-bandwith system RAM as VRAM.&lt;/li&gt; &lt;li&gt;Still, there is one stage of inference that is compute-bound: prompt pre-processing! That's why we see the A19 Pro has ~3x faster Time to First Token vs the M4.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://www.macstories.net/linked/max-weinbach-on-the-m5s-neural-accelerators/"&gt;Max Weinbach's testing&lt;/a&gt; also corroborates what I found. And it's also worth noting that MLX hasn't been updated (yet) to take full advantage of the new neural accelerators!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. My M4 Pro as fast as my RTX 3080!!! It's crazy - 350 w vs 35 w&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;When you use an MLX model + MLX on Apple Silicon, you get some really remarkable performance. Note that the 3080 also had ~its best shot with CUDA optimized llama cpp!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechExpert2910"&gt; /u/TechExpert2910 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohrn20/investigating_apples_new_neural_accelerators_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohrn20/investigating_apples_new_neural_accelerators_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohrn20/investigating_apples_new_neural_accelerators_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:48:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohq5bc</id>
    <title>GLM-4.6 vs Minimax-M2</title>
    <updated>2025-10-27T20:50:13+00:00</updated>
    <author>
      <name>/u/baykarmehmet</name>
      <uri>https://old.reddit.com/user/baykarmehmet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I've been using the GLM Coding Plan and it works well&lt;/strong&gt; — not quite Sonnet 4.5 performance, but with clear prompts it gets the job done.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;However, everyone's hyping Minimax M2&lt;/strong&gt;, claiming it crushes every benchmark. The problem? I haven't seen any real-world coding examples or projects using it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Has anyone here actually used Minimax M2 for development work?&lt;/strong&gt; If so:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How does it compare to other models in practice?&lt;/li&gt; &lt;li&gt;Is it worth switching to?&lt;/li&gt; &lt;li&gt;Any specific use cases where it excels or falls short?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear some hands-on experiences beyond the benchmark numbers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/baykarmehmet"&gt; /u/baykarmehmet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohq5bc/glm46_vs_minimaxm2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohq5bc/glm46_vs_minimaxm2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohq5bc/glm46_vs_minimaxm2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T20:50:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi350f</id>
    <title>3090 for approx $600 still a good investment in 2025? Or are there better value alternatives?</title>
    <updated>2025-10-28T07:34:50+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m trying to find a “good value” GPU or setup for running LLMs locally (mainly for coding and research projects) and for ComfyUI work.&lt;/p&gt; &lt;p&gt;I don’t have a strict budget in mind, but I do have a desktop with a 3060 and 128 GB of RAM. I’m thinking I should probably “max it out” before considering a completely new build.&lt;/p&gt; &lt;p&gt;I’ve been using the 3060 quite a bit, but it’s hard not to notice how much smarter the 20–32B models are compared to the 8–16B ones I can currently run.&lt;/p&gt; &lt;p&gt;I’m a bit wary of dual-GPU setups since I’m more comfortable on Windows, but it seems like the dual 3090 configuration (for 48 GB VRAM under Linux) is still often recommended as the best value.&lt;/p&gt; &lt;p&gt;Does that still hold true as of late 2025?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi350f/3090_for_approx_600_still_a_good_investment_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi350f/3090_for_approx_600_still_a_good_investment_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi350f/3090_for_approx_600_still_a_good_investment_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T07:34:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohlhdx</id>
    <title>Phoronix benchmarks single and dual AMD R9700 GPUs against a single NVIDIA RTX 6000 Ada GPU</title>
    <updated>2025-10-27T17:56:13+00:00</updated>
    <author>
      <name>/u/Brian-Puccio</name>
      <uri>https://old.reddit.com/user/Brian-Puccio</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brian-Puccio"&gt; /u/Brian-Puccio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/review/amd-radeon-ai-pro-r9700"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohlhdx/phoronix_benchmarks_single_and_dual_amd_r9700/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohlhdx/phoronix_benchmarks_single_and_dual_amd_r9700/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T17:56:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohpqvy</id>
    <title>Radeon R9700 Dual GPU First Look — AI/vLLM plus creative tests with Nuke &amp; the Adobe Suite</title>
    <updated>2025-10-27T20:34:58+00:00</updated>
    <author>
      <name>/u/atape_1</name>
      <uri>https://old.reddit.com/user/atape_1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohpqvy/radeon_r9700_dual_gpu_first_look_aivllm_plus/"&gt; &lt;img alt="Radeon R9700 Dual GPU First Look — AI/vLLM plus creative tests with Nuke &amp;amp; the Adobe Suite" src="https://external-preview.redd.it/nujocFBKcx2R2jDqpB8QfazaXJG6_pPL0crrme1qkLM.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=166ad90b700b767ba9a03c0fc586c47d3289ab0d" title="Radeon R9700 Dual GPU First Look — AI/vLLM plus creative tests with Nuke &amp;amp; the Adobe Suite" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atape_1"&gt; /u/atape_1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=efQPFhZmhAo&amp;amp;embeds_referring_euri=https%3A%2F%2Fwww.reddit.com%2F"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohpqvy/radeon_r9700_dual_gpu_first_look_aivllm_plus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohpqvy/radeon_r9700_dual_gpu_first_look_aivllm_plus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T20:34:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohh1l2</id>
    <title>86% accuracy on SimpleQA with gpt-4.1-mini. Open-source deep research agent.</title>
    <updated>2025-10-27T15:12:09+00:00</updated>
    <author>
      <name>/u/Ok-Attention1022</name>
      <uri>https://old.reddit.com/user/Ok-Attention1022</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohh1l2/86_accuracy_on_simpleqa_with_gpt41mini_opensource/"&gt; &lt;img alt="86% accuracy on SimpleQA with gpt-4.1-mini. Open-source deep research agent." src="https://external-preview.redd.it/Mhekv1CVcCnqQ6OsfNa_xd5RwOhyacefoOODajDng28.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=513369b6dc41c3172d89ddffae9bf50cb41bb901" title="86% accuracy on SimpleQA with gpt-4.1-mini. Open-source deep research agent." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built SGR Deep Research: a lightweight framework for structured reasoning agents using small LLMs&lt;/p&gt; &lt;p&gt;No LangChain/CrewAI bloat&lt;/p&gt; &lt;p&gt;~500 LOC core logic&lt;/p&gt; &lt;p&gt;Works with any OpenAI-compatible API&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmark&lt;/strong&gt;: 86.1% on SimpleQA (4,326 questions)&lt;/p&gt; &lt;p&gt;Model: gpt-4.1-mini&lt;br /&gt; Tavily Search: basic&lt;/p&gt; &lt;p&gt;Cost: $0.03 per query&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/akowocw57oxf1.png?width=2460&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d332497cfe0686bedb5b11f58bbb7e6de61f0a3"&gt;Performance Metrics on gpt-4.1-mini and Tavily basic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;SGR understanding&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bocirpd67oxf1.png?width=1176&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c1adc29c14fc211311efbf31063e3d449ffcbd0"&gt;SGR Deep Research: open-source framework for building intelligent research agents using Schema-Guided Reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Explicitly control reasoning flow instead of hoping model figures it out ReAct&amp;amp;PlanAct-style but with structured steps Running in production at telecom and banking right now&lt;/p&gt; &lt;p&gt;Testing local models next (Qwen, Llama) for $0 API costs&lt;br /&gt; Everything public: logs, configs, code GitHub MIT: &lt;a href="https://github.com/vamplabAI/sgr-deep-research"&gt;https://github.com/vamplabAI/sgr-deep-research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Attention1022"&gt; /u/Ok-Attention1022 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohh1l2/86_accuracy_on_simpleqa_with_gpt41mini_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohh1l2/86_accuracy_on_simpleqa_with_gpt41mini_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohh1l2/86_accuracy_on_simpleqa_with_gpt41mini_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T15:12:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi2ftf</id>
    <title>Is Grokipedia available for fine-tuning?</title>
    <updated>2025-10-28T06:46:21+00:00</updated>
    <author>
      <name>/u/Chance-Studio-8242</name>
      <uri>https://old.reddit.com/user/Chance-Studio-8242</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With grokipedia now live, wondering what its licensing policy is for using articles for fine-tuning local models. Not sure if article snapshots are already (or will be ever available) publicly for free .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chance-Studio-8242"&gt; /u/Chance-Studio-8242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2ftf/is_grokipedia_available_for_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2ftf/is_grokipedia_available_for_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2ftf/is_grokipedia_available_for_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T06:46:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi2tky</id>
    <title>I built a small Python tool to track how your directories get messy (and clean again)</title>
    <updated>2025-10-28T07:12:21+00:00</updated>
    <author>
      <name>/u/VegetableSense</name>
      <uri>https://old.reddit.com/user/VegetableSense</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, much as we hate to admit, almost every project or downloads folder gets out of control over time (yep).&lt;/p&gt; &lt;p&gt;I got curious — not just about which files change, but &lt;strong&gt;how the structure itself evolves.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;So I built &lt;a href="https://github.com/sukanto-m/directory-monitor"&gt;&lt;strong&gt;Directory Monitor&lt;/strong&gt;&lt;/a&gt; — a lightweight Python script that keeps tabs on &lt;strong&gt;directory organization&lt;/strong&gt;, not just file edits. This tool uses local LLMs (Qwen, Llama, choose your own) to analyze project structure and give cleanup recommendations. Everything runs locally - no cloud APIs.&lt;/p&gt; &lt;p&gt;**The interesting technical bits:**&lt;/p&gt; &lt;p&gt;- Uses RAG with local sentence-transformers to compare current state against historical scans&lt;/p&gt; &lt;p&gt;- LLM analyzes trends and gives specific, actionable recommendations &lt;/p&gt; &lt;p&gt;- Terminal UI with Rich showing real-time metrics and sparklines&lt;/p&gt; &lt;p&gt;- All stored in SQLite locally&lt;/p&gt; &lt;p&gt;**Example output:**&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Messiness Score: 6.2/10&lt;/p&gt; &lt;p&gt;Top 3 Issues:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Too many files (28) in src/components - split into ui/, forms/, layouts/&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;8 files contain 'temp' - move to .archive/ or use proper version control&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Directory depth exceeds 7 levels - flatten structure&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Trend: 📉 Improving (was 7.8, now 6.2)&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;**Stack:**&lt;/p&gt; &lt;p&gt;- Ollama (Qwen/Llama) for LLM&lt;/p&gt; &lt;p&gt;- sentence-transformers for embeddings&lt;/p&gt; &lt;p&gt;- SQLite for history&lt;/p&gt; &lt;p&gt;- Python with Rich/Flask&lt;/p&gt; &lt;p&gt;Works completely offline after setup. Tested with Qwen3:8b and Llama3.2.&lt;/p&gt; &lt;p&gt;Would love feedback — what features would &lt;em&gt;you&lt;/em&gt; add for keeping folders sane?&lt;/p&gt; &lt;p&gt;**GitHub:** &lt;a href="https://github.com/sukanto-m/directory-monitor"&gt;https://github.com/sukanto-m/directory-monitor&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VegetableSense"&gt; /u/VegetableSense &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2tky/i_built_a_small_python_tool_to_track_how_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2tky/i_built_a_small_python_tool_to_track_how_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2tky/i_built_a_small_python_tool_to_track_how_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T07:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohihvo</id>
    <title>Another Banger from Inclusion AI: Ming-flash-omni-Preview</title>
    <updated>2025-10-27T16:06:14+00:00</updated>
    <author>
      <name>/u/Finanzamt_Endgegner</name>
      <uri>https://old.reddit.com/user/Finanzamt_Endgegner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"&gt; &lt;img alt="Another Banger from Inclusion AI: Ming-flash-omni-Preview" src="https://external-preview.redd.it/PFGMqHZG1FenJLDckxpcToXwao333pejl_fZNW4bWqk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04ce1f6abd38ec8e123f358b2f5b41d3b28a30d6" title="Another Banger from Inclusion AI: Ming-flash-omni-Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-flash-omni-Preview"&gt;https://huggingface.co/inclusionAI/Ming-flash-omni-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Based on &lt;a href="https://huggingface.co/inclusionAI/Ling-flash-2.0"&gt;Ling-Flash-2.0&lt;/a&gt; this model has 100b total parameters and 6b active ones and supports context aware asr, text to speech, image generation and editing, segmentation etc (well its an omni modal model so you know the drill). Since its fairly sparse it is very efficient and while I couldn't test it myself the benchmarks seem promising, and it also supports voice cloning (;&lt;/p&gt; &lt;p&gt;It says it can do dialect-aware ASR, though im not sure if that will only work with Chinese 🤔&lt;/p&gt; &lt;p&gt;Anyways, if im not mistaken this is the biggest open sourced omni modal model yet so thanks to the mad lads at inclusion ai!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qml9ai33goxf1.png?width=2972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29df775ad390dc4e6cb4306e302540f231bdf556"&gt;https://preview.redd.it/qml9ai33goxf1.png?width=2972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29df775ad390dc4e6cb4306e302540f231bdf556&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ohihvo/video/oh86jahegoxf1/player"&gt;https://reddit.com/link/1ohihvo/video/oh86jahegoxf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ohihvo/video/zbxb11vnhoxf1/player"&gt;https://reddit.com/link/1ohihvo/video/zbxb11vnhoxf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Finanzamt_Endgegner"&gt; /u/Finanzamt_Endgegner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T16:06:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohm80t</id>
    <title>Newegg has 32gb AMD r9700 for $1,300</title>
    <updated>2025-10-27T18:23:03+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://videocardz.com/newz/amd-radeon-pro-ai-r9700-is-now-available-32gb-memory-and-full-navi-48-gpu"&gt;https://videocardz.com/newz/amd-radeon-pro-ai-r9700-is-now-available-32gb-memory-and-full-navi-48-gpu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Phoronix did a poor job of benchmarking it. Would prefer benchmarking a 30gb model like qwen3 coder, but instead focuses on 8gb model: &lt;a href="https://www.phoronix.com/review/amd-radeon-ai-pro-r9700"&gt;https://www.phoronix.com/review/amd-radeon-ai-pro-r9700&lt;/a&gt; Doesn't bother to compare it to 4090/5090. This video does gaming benchmarks: &lt;a href="https://www.youtube.com/watch?v=x0YJ32Q0mNw"&gt;https://www.youtube.com/watch?v=x0YJ32Q0mNw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Guessing 30 tokens per second (TPS) for qwen3 coder.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T18:23:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohvcwt</id>
    <title>Is an NVIDIA A40 48GB for 1500USD a bad idea because it's age?</title>
    <updated>2025-10-28T00:26:07+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"&gt; &lt;img alt="Is an NVIDIA A40 48GB for 1500USD a bad idea because it's age?" src="https://a.thumbs.redditmedia.com/UBwX6pI157vt7gMYDChu_R8QUux04jne3QgjRhJMLr0.jpg" title="Is an NVIDIA A40 48GB for 1500USD a bad idea because it's age?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, hope you're fine.&lt;/p&gt; &lt;p&gt;Short question, I managed to find, working and testing on my PC right now, an A40 48GB. It is passively cooled and it gets quite hot.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8az1kqsdyqxf1.png?width=764&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=301fff8d7b8d78a3f33c97765bb96ebdeaa03e2d"&gt;Local testing on my PC&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The seller (a friend) is asking me 1500USD for it. I'm not from USA, but a 3rd world country.&lt;/p&gt; &lt;p&gt;But I have read here on Local llama that such old cards and such aren't very worth it, also no FP8 support, etc.&lt;/p&gt; &lt;p&gt;So I'm really torn and indecisive about it. For reference, 5090 new goes for about 2700-3300USD (so 32GB, but fp8/fp4 support, like 4x times the bandwidth, etc). Used 4090s are 1600USD. 4090 48GB modded when importing they're about 4200-4400USD. 3090s are 550-600USD.&lt;/p&gt; &lt;p&gt;What would you guys do? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T00:26:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohyeee</id>
    <title>Minimax-M2 support added in MLX</title>
    <updated>2025-10-28T02:49:15+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohyeee/minimaxm2_support_added_in_mlx/"&gt; &lt;img alt="Minimax-M2 support added in MLX" src="https://preview.redd.it/4yqqtqzynrxf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85e601e94e902746685decb25bfc69d58f508850" title="Minimax-M2 support added in MLX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4yqqtqzynrxf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohyeee/minimaxm2_support_added_in_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohyeee/minimaxm2_support_added_in_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T02:49:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohdl9q</id>
    <title>Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives</title>
    <updated>2025-10-27T12:53:12+00:00</updated>
    <author>
      <name>/u/xiaoruhao</name>
      <uri>https://old.reddit.com/user/xiaoruhao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"&gt; &lt;img alt="Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives" src="https://external-preview.redd.it/cDlpaWtncThobnhmMYyuPjWRTezxeRfqB3upVJ5ATISaueUIVVjdl6ikWaxE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49803f057b43fcece9390b3b966fe6ba4de209b3" title="Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chamath Palihapitiya said his team migrated a large number of workloads to Kimi K2 because it was significantly more performant and much cheaper than both OpenAI and Anthropic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xiaoruhao"&gt; /u/xiaoruhao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/avwpphq8hnxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T12:53:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1oht9pw</id>
    <title>Z.ai release Glyph weight</title>
    <updated>2025-10-27T22:55:19+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oht9pw/zai_release_glyph_weight/"&gt; &lt;img alt="Z.ai release Glyph weight" src="https://a.thumbs.redditmedia.com/69TaqDj6bS-Vs7O_YYHmJygT9-976J0J5KJcduszmX8.jpg" title="Z.ai release Glyph weight" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Glyph: Scaling Context Windows via Visual-Text Compression&lt;/p&gt; &lt;p&gt;Paper: arxiv.org/abs/2510.17800&lt;/p&gt; &lt;p&gt;Weights: huggingface.co/zai-org/Glyph&lt;/p&gt; &lt;p&gt;Repo: github.com/thu-coai/Glyph&lt;/p&gt; &lt;p&gt;Glyph is a framework for scaling the context length through visual-text compression. It renders long textual sequences into images and processes them using vision–language models.&lt;/p&gt; &lt;p&gt;This design transforms the challenge of long-context modeling into a multimodal problem, substantially reducing computational and memory costs while preserving semantic information.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oht9pw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oht9pw/zai_release_glyph_weight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oht9pw/zai_release_glyph_weight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T22:55:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohtp6d</id>
    <title>Bad news: DGX Spark may have only half the performance claimed.</title>
    <updated>2025-10-27T23:13:15+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"&gt; &lt;img alt="Bad news: DGX Spark may have only half the performance claimed." src="https://preview.redd.it/9b2ziei0lqxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de9741ebb17cdabb88dde97eb430a1c2ff563565" title="Bad news: DGX Spark may have only half the performance claimed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There might be more bad news about the DGX Spark!&lt;/p&gt; &lt;p&gt;Before it was even released, I told everyone that this thing has a memory bandwidth problem. Although it boasts 1 PFLOPS of FP4 floating-point performance, its memory bandwidth is only 273GB/s. This will cause major stuttering when running large models (with performance being roughly only one-third of a MacStudio M2 Ultra).&lt;/p&gt; &lt;p&gt;Today, more bad news emerged: the floating-point performance doesn't even reach 1 PFLOPS.&lt;/p&gt; &lt;p&gt;Tests from two titans of the industry—John Carmack (founder of id Software, developer of games like Doom, and a name every programmer should know from the legendary fast inverse square root algorithm) and Awni Hannun (the primary lead of Apple's large model framework, MLX)—have shown that this device only achieves 480 TFLOPS of FP4 performance (approximately 60 TFLOPS BF16). That's less than half of the advertised performance.&lt;/p&gt; &lt;p&gt;Furthermore, if you run it for an extended period, it will overheat and restart.&lt;/p&gt; &lt;p&gt;It's currently unclear whether the problem is caused by the power supply, firmware, CUDA, or something else, or if the SoC is genuinely this underpowered. I hope Jensen Huang fixes this soon. The memory bandwidth issue could be excused as a calculated product segmentation decision from NVIDIA, a result of us having overly high expectations meeting his precise market strategy. However, performance not matching the advertised claims is a major integrity problem.&lt;/p&gt; &lt;p&gt;So, for all the folks who bought an NVIDIA DGX Spark, Gigabyte AI TOP Atom, or ASUS Ascent GX10, I recommend you all run some tests and see if you're indeed facing performance issues.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9b2ziei0lqxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T23:13:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohdzxs</id>
    <title>AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-27T13:10:46+00:00</updated>
    <author>
      <name>/u/LiquidAI_Team</name>
      <uri>https://old.reddit.com/user/LiquidAI_Team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt; &lt;img alt="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/47wfyylmlnxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c359ceba4921b523ecd2e493f9cc84bd8b3e7881" title="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;When: Thursday 10/30, 10 AM – 1 PM PST&lt;/h1&gt; &lt;p&gt;The Liquid AI team will also continue answering questions for the following 24 hours, so jump in anytime!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who will be there:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jacob Marks (Data)&lt;/li&gt; &lt;li&gt;Jimmy Smith (Pre-Training)&lt;/li&gt; &lt;li&gt;Maxime Labonne (Post-Training)&lt;/li&gt; &lt;li&gt;Fernando Fernandes (Post-training)&lt;/li&gt; &lt;li&gt;Anna Banaszak (LFM2-VL)&lt;/li&gt; &lt;li&gt;Arthur Böök (LFM2-Audio)&lt;/li&gt; &lt;li&gt;Yuri Khrustalev (Inference engine, llama.cpp)&lt;/li&gt; &lt;li&gt;Darian Bhathena (LEAP SDK and Apollo)&lt;/li&gt; &lt;li&gt;Edoardo Mosca (LEAP Best Model Search and Finetune)&lt;/li&gt; &lt;li&gt;Anthony Crognale (LEAP SDK)&lt;/li&gt; &lt;li&gt;Pau Labarta Bajo (Dev Relations)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Want to get started?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;→ &lt;a href="https://leap.liquid.ai/models?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Deploy your first model on-device today&lt;/a&gt;&lt;br /&gt; → &lt;a href="https://huggingface.co/LiquidAI?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Check out our models on Hugging Face&lt;/a&gt;&lt;br /&gt; → &lt;a href="https://www.liquid.ai/apollo?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Play with models on Apollo&lt;/a&gt;&lt;br /&gt; → &lt;a href="https://www.liquid.ai/company/news?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Learn more about our recent releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiquidAI_Team"&gt; /u/LiquidAI_Team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/47wfyylmlnxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T13:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
</feed>
