<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-31T03:57:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qrnybg</id>
    <title>GGUF Splitter easily splits an existing GGUF file into smaller parts (uses llama-gguf-split in background)</title>
    <updated>2026-01-31T01:00:30+00:00</updated>
    <author>
      <name>/u/Felladrin</name>
      <uri>https://old.reddit.com/user/Felladrin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrnybg/gguf_splitter_easily_splits_an_existing_gguf_file/"&gt; &lt;img alt="GGUF Splitter easily splits an existing GGUF file into smaller parts (uses llama-gguf-split in background)" src="https://b.thumbs.redditmedia.com/RNlPqtVeyos1yRFL3z7eKI4T_J_OMNTXQVxKgUXkvXk.jpg" title="GGUF Splitter easily splits an existing GGUF file into smaller parts (uses llama-gguf-split in background)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Made this tool specially for speeding up the addition of models to one of my apps, which uses &lt;a href="https://github.com/ngxson/wllama"&gt;Wllama&lt;/a&gt;, which in turn is a library that allows running GGUF files directly in the web browser.&lt;/p&gt; &lt;p&gt;The app is called &lt;em&gt;GGUF Splitter&lt;/em&gt; and works both as a Hugging Face Space (Gradio application) or locally inside a Docker container.&lt;/p&gt; &lt;p&gt;Basically, what it does is guiding you through a form where you select a GGUF file from an existing Hugging Face model-repository, then define where to save the sharded file (which must be a repository under your own Hugging Face account), and with the click of a button it will generate the splits and upload the model, with is then ready to use, in the target repository.&lt;/p&gt; &lt;p&gt;The split is done with &lt;a href="https://github.com/ggml-org/llama.cpp/blob/master/tools/gguf-split/README.md"&gt;llama.cpp's gguf-split tool&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For example, &lt;a href="https://huggingface.co/ibm-granite/granite-4.0-1b-GGUF/tree/main?show_file_info=granite-4.0-1b-Q4_K_S.gguf"&gt;this file&lt;/a&gt; (981 MB):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;granite-4.0-1b-Q4_K_S.gguf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Became &lt;a href="https://huggingface.co/Felladrin/gguf-sharded-Q4_K_S-granite-4.0-1b/tree/main"&gt;these files&lt;/a&gt; (~165 MB each):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;granite-4.0-1b-Q4_K_S-00001-of-00006.gguf granite-4.0-1b-Q4_K_S-00002-of-00006.gguf granite-4.0-1b-Q4_K_S-00003-of-00006.gguf granite-4.0-1b-Q4_K_S-00004-of-00006.gguf granite-4.0-1b-Q4_K_S-00005-of-00006.gguf granite-4.0-1b-Q4_K_S-00006-of-00006.gguf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Wllama requires those splits due to WASM memory constraints.&lt;/p&gt; &lt;p&gt;I'm not aware of any other app that requires sharded GGUFs, but I thought this tool could be useful for someone else on the community.&lt;/p&gt; &lt;p&gt;Link for the Hugging Face Space:&lt;br /&gt; &lt;a href="https://huggingface.co/spaces/Felladrin/GGUF-Splitter"&gt;https://huggingface.co/spaces/Felladrin/GGUF-Splitter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The source-code can be viewed/cloned from &lt;a href="https://huggingface.co/spaces/Felladrin/GGUF-Splitter/tree/main"&gt;this page&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Felladrin"&gt; /u/Felladrin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qrnybg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrnybg/gguf_splitter_easily_splits_an_existing_gguf_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrnybg/gguf_splitter_easily_splits_an_existing_gguf_file/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T01:00:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqw3ov</id>
    <title>GLM 4.7 Flash 30B PRISM + Web Search: Very solid.</title>
    <updated>2026-01-30T04:56:57+00:00</updated>
    <author>
      <name>/u/My_Unbiased_Opinion</name>
      <uri>https://old.reddit.com/user/My_Unbiased_Opinion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got this set up yesterday. I have been messing around with it and I am extremely impressed. I find that it is very efficient in reasoning compared to Qwen models. The model is quite uncensored so I'm able to research any topics, it is quite thorough. &lt;/p&gt; &lt;p&gt;The knowledge is definitely less than 120B Derestricted, but once Web Search RAG is involved, I'm finding the 30B model generally superior with far less soft refusals. Since the model has web access, I feel the base knowledge deficit is mitigated. &lt;/p&gt; &lt;p&gt;Running it in the latest LMstudio beta + OpenwebUI. Y'all gotta try it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/My_Unbiased_Opinion"&gt; /u/My_Unbiased_Opinion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqw3ov/glm_47_flash_30b_prism_web_search_very_solid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqw3ov/glm_47_flash_30b_prism_web_search_very_solid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqw3ov/glm_47_flash_30b_prism_web_search_very_solid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T04:56:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrl0j9</id>
    <title>Open models vs closed models: discrepancy in benchmarks vs real-world performance. Just me?</title>
    <updated>2026-01-30T22:59:37+00:00</updated>
    <author>
      <name>/u/MobyTheMadCow</name>
      <uri>https://old.reddit.com/user/MobyTheMadCow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open models rival closed models on benchmarks for SWE, but my experience is very different. Using claude models (even 4.5 haiku), it is reliable at making tool calls, outputs very long documents without having to bully it, and completes well-planned tasks with little supervision even if they are complex.&lt;/p&gt; &lt;p&gt;Other models that score higher such as deepseek v3.2, grok 4.1, etc make errononeus tool calls very often and I end up needing to supervise their execution.&lt;/p&gt; &lt;p&gt;Am I doing something wrong or is this a common experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MobyTheMadCow"&gt; /u/MobyTheMadCow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrl0j9/open_models_vs_closed_models_discrepancy_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrl0j9/open_models_vs_closed_models_discrepancy_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrl0j9/open_models_vs_closed_models_discrepancy_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T22:59:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr5vdu</id>
    <title>LM Studio doesn't let continue generating a message anymore</title>
    <updated>2026-01-30T13:45:54+00:00</updated>
    <author>
      <name>/u/PhyrexianSpaghetti</name>
      <uri>https://old.reddit.com/user/PhyrexianSpaghetti</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used LM studio for a long time and always liked it. Since my computer isn't nasa-level, I have to use quantized llms, and this means that often, to make them understand what I want, I needed to edit their answer with something along the lines of &amp;quot;Oh I see, you need me to...&amp;quot; and then click on the button that forced it to continue the generation based on the start I fed it.&lt;br /&gt; After the latest update, I can't find the button to make the model continue an edited answer, for some reason they seem to have removed the most important feature of running models locally.&lt;/p&gt; &lt;p&gt;Did they move it or is it gone? Is there another similarly well curated and easy to use software to do that without complex setup?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhyrexianSpaghetti"&gt; /u/PhyrexianSpaghetti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr5vdu/lm_studio_doesnt_let_continue_generating_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr5vdu/lm_studio_doesnt_let_continue_generating_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr5vdu/lm_studio_doesnt_let_continue_generating_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T13:45:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrbel2</id>
    <title>Qwen3 ASR 1.7B vs Whisper v3 Large</title>
    <updated>2026-01-30T17:10:38+00:00</updated>
    <author>
      <name>/u/OGScottingham</name>
      <uri>https://old.reddit.com/user/OGScottingham</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;Has anybody had the chance to try out the new transcription model from the Qwen team? It just came out yesterday and I haven't seen much talk about it here.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/Qwen3-ASR?tab=readme-ov-file"&gt;https://github.com/QwenLM/Qwen3-ASR?tab=readme-ov-file&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Their intro from the github:&lt;br /&gt; &lt;a href="https://camo.githubusercontent.com/0f65d4213247aa283f23cc3e2c5e5e51542670d4942123430ada7a58587d6c66/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e332d4153522d5265706f2f7177656e335f6173725f696e74726f64756374696f6e2e706e67"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Qwen3-ASR family includes Qwen3-ASR-1.7B and Qwen3-ASR-0.6B, which support language identification and ASR for 52 languages and dialects. Both leverage large-scale speech training data and the strong audio understanding capability of their foundation model, Qwen3-Omni. Experiments show that the 1.7B version achieves state-of-the-art performance among open-source ASR models and is competitive with the strongest proprietary commercial APIs. Here are the main features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;All-in-one&lt;/strong&gt;: Qwen3-ASR-1.7B and Qwen3-ASR-0.6B support language identification and speech recognition for 30 languages and 22 Chinese dialects, so as to English accents from multiple countries and regions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Excellent and Fast&lt;/strong&gt;: The Qwen3-ASR family ASR models maintains high-quality and robust recognition under complex acoustic environments and challenging text patterns. Qwen3-ASR-1.7B achieves strong performance on both open-sourced and internal benchmarks. While the 0.6B version achieves accuracy-efficient trade-off, it reaches 2000 times throughput at a concurrency of 128. They both achieve streaming / offline unified inference with single model and support transcribe long audio.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Novel and strong forced alignment Solution&lt;/strong&gt;: We introduce Qwen3-ForcedAligner-0.6B, which supports timestamp prediction for arbitrary units within up to 5 minutes of speech in 11 languages. Evaluations show its timestamp accuracy surpasses E2E based forced-alignment models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Comprehensive inference toolkit&lt;/strong&gt;: In addition to open-sourcing the architectures and weights of the Qwen3-ASR series, we also release a powerful, full-featured inference framework that supports vLLM-based batch inference, asynchronous serving, streaming inference, timestamp prediction, and more.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OGScottingham"&gt; /u/OGScottingham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrbel2/qwen3_asr_17b_vs_whisper_v3_large/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrbel2/qwen3_asr_17b_vs_whisper_v3_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrbel2/qwen3_asr_17b_vs_whisper_v3_large/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T17:10:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qriwnv</id>
    <title>Post your hardware/software/model quant and measured performance of Kimi K2.5</title>
    <updated>2026-01-30T21:38:56+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I will start:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: Epyc 9374F (32 cores), 12 x 96GB DDR5 4800 MT/s, 1 x RTX PRO 6000 Max-Q 96GB&lt;/li&gt; &lt;li&gt;Software: SGLang and KT-Kernel (followed the &lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/Kimi-K2.5.md"&gt;guide&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Quant: Native INT4 (original model)&lt;/li&gt; &lt;li&gt;PP rate (32k tokens): 497.13 t/s&lt;/li&gt; &lt;li&gt;TG rate (128@32k tokens): 15.56 t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Used &lt;a href="https://github.com/wheynelau/llmperf-rs"&gt;llmperf-rs&lt;/a&gt; to measure values. Can't believe the prefill is so fast, amazing!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qriwnv/post_your_hardwaresoftwaremodel_quant_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qriwnv/post_your_hardwaresoftwaremodel_quant_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qriwnv/post_your_hardwaresoftwaremodel_quant_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T21:38:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrgggs</id>
    <title>I replaced Claude Code‚Äôs entire backend with free Alternatives</title>
    <updated>2026-01-30T20:07:34+00:00</updated>
    <author>
      <name>/u/LastNoobLeft</name>
      <uri>https://old.reddit.com/user/LastNoobLeft</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrgggs/i_replaced_claude_codes_entire_backend_with_free/"&gt; &lt;img alt="I replaced Claude Code‚Äôs entire backend with free Alternatives" src="https://external-preview.redd.it/mY-yVjHT4dctVZp-g19wXUcocpcmjQkpOx8XBgUVyOk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b17339480aad21c9accaf19417e69954bfb0efb8" title="I replaced Claude Code‚Äôs entire backend with free Alternatives" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working on a side-project which replaces the following things in the Claude ecosystem with free alternatives:&lt;/p&gt; &lt;p&gt;\- Replaces Anthropic models with NVIDIA-NIM models: It acts as middleware between Claude-Code and NVIDIA-NIM allowing unlimited usage upto 40 RPM with a free NVIDIA-NIM api-key.&lt;/p&gt; &lt;p&gt;\- Replaces the Claude mobile app with telegram: It allows the user to send messages to a local server via telegram that spin up a CLI instance and do a task. Replies resume a conversation and new messages create a new instance. You can concurrently use multiple CLI sessions and chats.&lt;/p&gt; &lt;p&gt;It has features that distinguish it from similar proxies:&lt;/p&gt; &lt;p&gt;\- The interleaved thinking tokens generated between tool calls are preserved allowing reasoning models like GLM 4.7 and kimi-k2.5 to take full advantage of thinking from previous turns.&lt;/p&gt; &lt;p&gt;\- Fast prefix detection stops the CLI from sending bash command prefix classification requests to the LLM making it feel blazing fast.&lt;/p&gt; &lt;p&gt;I have made the code modular so that adding other providers or messaging apps is easy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LastNoobLeft"&gt; /u/LastNoobLeft &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Alishahryar1/cc-nim"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrgggs/i_replaced_claude_codes_entire_backend_with_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrgggs/i_replaced_claude_codes_entire_backend_with_free/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T20:07:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrmzyx</id>
    <title>Still issues with GLM-4.7-Flash? Here the solution</title>
    <updated>2026-01-31T00:19:46+00:00</updated>
    <author>
      <name>/u/R_Duncan</name>
      <uri>https://old.reddit.com/user/R_Duncan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RECOMPILE llama.cpp from scratch. (git clone)&lt;/p&gt; &lt;p&gt;Updating it with git-pull gaved me issues on this sole model (repeating loop, bogus code) until I renamed llama.cpp directory, did a git clone and then rebuilt from 0. &lt;/p&gt; &lt;p&gt;Did a bug report and various logs. Now is working&lt;/p&gt; &lt;p&gt;llama-server -m GLM-4.7-Flash-Q4_K_M.gguf -fa on --threads -1 --fit off -ctk q8_0 -ctv q8_0 --temp 0.0 --top-p 0.95 --min-p 0.01 -c 32768 -ncmoe 40&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R_Duncan"&gt; /u/R_Duncan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrmzyx/still_issues_with_glm47flash_here_the_solution/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrmzyx/still_issues_with_glm47flash_here_the_solution/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrmzyx/still_issues_with_glm47flash_here_the_solution/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T00:19:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrnu6c</id>
    <title>Can you guys help me set up a local AI system to improve my verbal communication</title>
    <updated>2026-01-31T00:55:30+00:00</updated>
    <author>
      <name>/u/registrartulip</name>
      <uri>https://old.reddit.com/user/registrartulip</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I am a student who struggles in verbal communication and little bit of stuttering. I live in a hostel and don't have any close friends I can practice with for the interview and general interaction. I was thinking of setting a local AI model to practice back and forth conversations. Can someone help me with it? I have a laptop with Ryzen 5 5600H, 16GB RAM, 4GB 3050 VRAM. Which model to use which application has good support for audio etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/registrartulip"&gt; /u/registrartulip &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrnu6c/can_you_guys_help_me_set_up_a_local_ai_system_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrnu6c/can_you_guys_help_me_set_up_a_local_ai_system_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrnu6c/can_you_guys_help_me_set_up_a_local_ai_system_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T00:55:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrp66p</id>
    <title>Best local-first, tool-integrated Cursor-like app?</title>
    <updated>2026-01-31T01:54:47+00:00</updated>
    <author>
      <name>/u/johnW_ret</name>
      <uri>https://old.reddit.com/user/johnW_ret</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I've looked a lot in post history and see a lot of posts similar to mine but none exactly and none that answer my question. Sorry if this is a dup.&lt;/p&gt; &lt;p&gt;I have access to Anthropic models and Cursor at work. I generally don't like using AI for generating code but here lately I've been pretty impressed. However, while I'm sure that some of it is the intelligence of Auto / Sonnet, I believe a lot of the ease is due to Cursor integrating with the LSP and available tooling well. It repeatedly fails very frequently but it will try again without me asking. It's not that the code is great (I change or reject it the majority of the time) but it's that it can run in the background while I do other work.&lt;/p&gt; &lt;p&gt;The performance of Kimi has given me optimism for the future and I generally just don't like paying for AI tools, so I've been experimenting with local setups, but to be honest, I haven't found anything that provides as nearly as good of an experience as Cursor.&lt;/p&gt; &lt;p&gt;I actually have a preference &lt;em&gt;against&lt;/em&gt; closed-source tools like Cursor, but I would be down to try anything. My preference would be some VS Code extension, but of course a CLI / TLI that 1. has tools integration 2. can feed test / build / lint command(s) output after generation in a loop for n times until it gets it right is all I would need. I'm curious if anyone is building anything like this.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Also sorry that this is unrelated I have run the following models on both 16 and 32 GB machines with the bare minimum goal of trying to get tool calls to work and none of them work as intended. I'm curious if there's anything I can tune to actually get real performance:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;llama3.1:8b : does not sufficiently understand task&lt;/li&gt; &lt;li&gt;gemma3:12b : does not support tools&lt;/li&gt; &lt;li&gt;codellama:13b-code : does not support tools&lt;/li&gt; &lt;li&gt;llama4:16x17b : way too slow&lt;/li&gt; &lt;li&gt;codegemma:7b : does not support tools&lt;/li&gt; &lt;li&gt;qwen2.5:7b-instruct-q4_K_M : will try to use tools unlike llama3.1:8b but it just keeps using them incorrectly and yielding tool errors&lt;/li&gt; &lt;li&gt;qwen2.5-coder:14b : it just outputs tasks instead of doing them&lt;/li&gt; &lt;li&gt;gpt-oss:20b : generally slow which would be fine but seems to get confused due to memory pressure&lt;/li&gt; &lt;li&gt;mistral-nemo:12b : either does not use tools or just outputs nothing&lt;/li&gt; &lt;li&gt;mistral:7b : kind of fast but does not actually use tools&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johnW_ret"&gt; /u/johnW_ret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrp66p/best_localfirst_toolintegrated_cursorlike_app/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrp66p/best_localfirst_toolintegrated_cursorlike_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrp66p/best_localfirst_toolintegrated_cursorlike_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T01:54:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrkf8a</id>
    <title>Need help brainstorming on my opensource project</title>
    <updated>2026-01-30T22:36:23+00:00</updated>
    <author>
      <name>/u/DeathShot7777</name>
      <uri>https://old.reddit.com/user/DeathShot7777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrkf8a/need_help_brainstorming_on_my_opensource_project/"&gt; &lt;img alt="Need help brainstorming on my opensource project" src="https://external-preview.redd.it/emFpN2huNzVia2dnMXAjvbzZlDodMUt4XPu-WVR4gri-PW-w3a3Tn0De93z1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0bd8730e5b27ca282279d87a0334e1e2fe9af60b" title="Need help brainstorming on my opensource project" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working on this opensource project, Gitnexus. It creates knowledge graph of codebases, make clusters, process maps. Basically skipping the tech jargon, the idea is that to make the tools itself smarter so LLMs can offload a lot of the retrieval reasoning part to the tools. I found haiku 4.5 was able to outperform opus 4.5 using its MCP on deep architectural context. &lt;/p&gt; &lt;p&gt;It feels promising so I wanna go deeper into its development and benchmark it, converting it from a cool demo to an actual viable opensource product. I would really appreciate some advice on potential niche usecase I can tune it for, point me to some discussion forum where I can get people to brainstorm with me, maybe some micro funding sources ( some opensource programs or something ) for purchasing LLM provider credits ( Being a student i cant afford much myself üòÖ )&lt;/p&gt; &lt;p&gt;github: &lt;a href="https://github.com/abhigyanpatwari/gitnexus"&gt;https://github.com/abhigyanpatwari/gitnexus&lt;/a&gt; ( Leave a ‚≠ê if seemed cool )&lt;br /&gt; try it here: &lt;a href="https://gitnexus.vercel.com"&gt;https://gitnexus.vercel.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathShot7777"&gt; /u/DeathShot7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5zx3h775bkgg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrkf8a/need_help_brainstorming_on_my_opensource_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrkf8a/need_help_brainstorming_on_my_opensource_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T22:36:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrqk9o</id>
    <title>Managed to run Kimi k2.5 IQ4-SX locally.</title>
    <updated>2026-01-31T02:56:33+00:00</updated>
    <author>
      <name>/u/el3mancee</name>
      <uri>https://old.reddit.com/user/el3mancee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrqk9o/managed_to_run_kimi_k25_iq4sx_locally/"&gt; &lt;img alt="Managed to run Kimi k2.5 IQ4-SX locally." src="https://a.thumbs.redditmedia.com/wPblHyZa-nLIek-Bsb8Vto-LtlSalMG4iuSpvu7oeE0.jpg" title="Managed to run Kimi k2.5 IQ4-SX locally." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Loaded with a max token capable(262,114 tokens)&lt;/p&gt; &lt;p&gt;1 Max Studio M1 Ultra(host), 1 Asus Gx10, 3 Strix Halo. Connected with Thunderbolt and 10 Gbps Ethernet. &lt;/p&gt; &lt;p&gt;Tg 8.5 tps. Pp 15-20 tps.&lt;/p&gt; &lt;p&gt;Can reach ~15 tps tg when using concurrent requests.&lt;/p&gt; &lt;p&gt;Pretty slow for production, I think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/el3mancee"&gt; /u/el3mancee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qrqk9o"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrqk9o/managed_to_run_kimi_k25_iq4sx_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrqk9o/managed_to_run_kimi_k25_iq4sx_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T02:56:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrg1fk</id>
    <title>[Rant] Why does no chat tool get the basic UX of not auto scrolling to the bottom of the message response?</title>
    <updated>2026-01-30T19:52:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every single AI chat tool I use - openwebui, msty, claude code etc. all scroll automatically to the bottom the the LLM response requiring you to often scroll back up to the start of the response. This is utterly basic UX that you dont even need a designer on the team to tell you to get correct.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrg1fk/rant_why_does_no_chat_tool_get_the_basic_ux_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrg1fk/rant_why_does_no_chat_tool_get_the_basic_ux_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrg1fk/rant_why_does_no_chat_tool_get_the_basic_ux_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T19:52:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrcwyy</id>
    <title>Kimi-K2.5 Technical Report</title>
    <updated>2026-01-30T18:02:55+00:00</updated>
    <author>
      <name>/u/TheRealMasonMac</name>
      <uri>https://old.reddit.com/user/TheRealMasonMac</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrcwyy/kimik25_technical_report/"&gt; &lt;img alt="Kimi-K2.5 Technical Report" src="https://external-preview.redd.it/kHJ5obgM6rCZbnbyyrqyZiEIH1ueRoKz1v0BSYN1Nd4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79406125fe8aad19d90741b6b4f5dc358eca8fdf" title="Kimi-K2.5 Technical Report" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealMasonMac"&gt; /u/TheRealMasonMac &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/MoonshotAI/Kimi-K2.5/blob/master/tech_report.pdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrcwyy/kimik25_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrcwyy/kimik25_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T18:02:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrbfez</id>
    <title>spec : add ngram-mod by ggerganov ¬∑ Pull Request #19164 ¬∑ ggml-org/llama.cpp</title>
    <updated>2026-01-30T17:11:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrbfez/spec_add_ngrammod_by_ggerganov_pull_request_19164/"&gt; &lt;img alt="spec : add ngram-mod by ggerganov ¬∑ Pull Request #19164 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/g3Kl7EuA7uN68kx8-95HOYqEV6uFaejZ8ghgYxWQDJQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d663e7e68ebfca599cda3a0ba19c677f3a8b64c8" title="spec : add ngram-mod by ggerganov ¬∑ Pull Request #19164 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;watch the video&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19164"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrbfez/spec_add_ngrammod_by_ggerganov_pull_request_19164/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrbfez/spec_add_ngrammod_by_ggerganov_pull_request_19164/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T17:11:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrmu2v</id>
    <title>What shoddy development looks like</title>
    <updated>2026-01-31T00:12:53+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrmu2v/what_shoddy_development_looks_like/"&gt; &lt;img alt="What shoddy development looks like" src="https://preview.redd.it/9l7wwnsu6kgg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a3c02e836b0951ddacdc25d64410b0f33a6b6e4" title="What shoddy development looks like" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9l7wwnsu6kgg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrmu2v/what_shoddy_development_looks_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrmu2v/what_shoddy_development_looks_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T00:12:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr7cbh</id>
    <title>Kimi-k2.5 reaches gemini 2.5 Pro-like performance in long context!</title>
    <updated>2026-01-30T14:44:01+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7cbh/kimik25_reaches_gemini_25_prolike_performance_in/"&gt; &lt;img alt="Kimi-k2.5 reaches gemini 2.5 Pro-like performance in long context!" src="https://preview.redd.it/on28koqz0igg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=011c51e6852eeaee308ab92b0cd9e4852da4bfe0" title="Kimi-k2.5 reaches gemini 2.5 Pro-like performance in long context!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/on28koqz0igg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7cbh/kimik25_reaches_gemini_25_prolike_performance_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7cbh/kimik25_reaches_gemini_25_prolike_performance_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T14:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr7ncz</id>
    <title>Design Arena is now dominated by an open model</title>
    <updated>2026-01-30T14:55:35+00:00</updated>
    <author>
      <name>/u/moks4tda</name>
      <uri>https://old.reddit.com/user/moks4tda</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7ncz/design_arena_is_now_dominated_by_an_open_model/"&gt; &lt;img alt="Design Arena is now dominated by an open model" src="https://a.thumbs.redditmedia.com/IOzZQkj-NN9LpvwuZen1AYFWFys9dnrIFBwpaZCd7D0.jpg" title="Design Arena is now dominated by an open model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The first month of 2026 is already this wild, I can't even imagine what's coming next!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moks4tda"&gt; /u/moks4tda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qr7ncz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7ncz/design_arena_is_now_dominated_by_an_open_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7ncz/design_arena_is_now_dominated_by_an_open_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T14:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrfbo8</id>
    <title>NVIDIA Releases Massive Collection of Open Models, Data and Tools to Accelerate AI Development</title>
    <updated>2026-01-30T19:26:46+00:00</updated>
    <author>
      <name>/u/Delicious_Air_737</name>
      <uri>https://old.reddit.com/user/Delicious_Air_737</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrfbo8/nvidia_releases_massive_collection_of_open_models/"&gt; &lt;img alt="NVIDIA Releases Massive Collection of Open Models, Data and Tools to Accelerate AI Development" src="https://b.thumbs.redditmedia.com/vc1UXvPAXbGKJMGRgO3vaYujfIckG9Mk-wfTZzXzgEY.jpg" title="NVIDIA Releases Massive Collection of Open Models, Data and Tools to Accelerate AI Development" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/6key4zy0fjgg1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=62b0bfa274d54a0e695e0cbc067cd40c4c9dfa4e"&gt;https://preview.redd.it/6key4zy0fjgg1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=62b0bfa274d54a0e695e0cbc067cd40c4c9dfa4e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;At CES 2026, NVIDIA announced what might be &lt;a href="https://namiru.ai/blog/nvidia-releases-massive-collection-of-open-models-data-and-tools-to-accelerate-ai-development?source=red-nvidia-kinga"&gt;the most significant open-source AI release&lt;/a&gt; to date. The company unveiled new models, datasets, and tools spanning everything from speech recognition to drug discovery.&lt;/p&gt; &lt;p&gt;For regular users, this release means better voice assistants, smarter document search, faster drug development, safer self-driving cars, and more capable robots. These technologies will filter into consumer products throughout 2026.&lt;/p&gt; &lt;p&gt;NVIDIA is betting that by enabling the entire AI ecosystem, they sell more GPUs. Based on the companies already adopting these technologies, that bet is paying off. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious_Air_737"&gt; /u/Delicious_Air_737 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrfbo8/nvidia_releases_massive_collection_of_open_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrfbo8/nvidia_releases_massive_collection_of_open_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrfbo8/nvidia_releases_massive_collection_of_open_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T19:26:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrazyy</id>
    <title>Cline team got absorbed by OpenAI. Kilo is going full source available in response.</title>
    <updated>2026-01-30T16:56:49+00:00</updated>
    <author>
      <name>/u/demon_bhaiya</name>
      <uri>https://old.reddit.com/user/demon_bhaiya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/"&gt; &lt;img alt="Cline team got absorbed by OpenAI. Kilo is going full source available in response." src="https://external-preview.redd.it/OJiv7stnybHLdn8-mzf6t_NZ9C8xS7VIYLhMSJsX0d8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=969735c0073c014c64189d4c6b79a9e599fe2c52" title="Cline team got absorbed by OpenAI. Kilo is going full source available in response." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who used Cline with local models, heads up that the core team appears to have joined OpenAI's Codex group based on their LinkedIn profiles. No official announcement yet, but we have seen how these acqui-hires usually play out.&lt;/p&gt; &lt;p&gt;Kilo Code (which forked from Cline and Roo Code) just responded by announcing they are making their backend source available by Feb 6. The VS Code extension, JetBrains plugin, and CLI stay Apache 2.0(Open source). Their gateway supports 500+ models including Qwen, DeepSeek, and Mistral.&lt;/p&gt; &lt;p&gt;They're offering $100 credits to anyone who contributed to Cline, and $150 per merged PR in February. If you want to keep building on an open codebase instead of watching another project disappear into a walled garden, might be worth checking out.&lt;/p&gt; &lt;p&gt;The agentic coding space needs alternatives that work with local and open weight models. Would suck to see all the decent tools end up controlled by the big labs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/demon_bhaiya"&gt; /u/demon_bhaiya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.kilo.ai/p/cline-just-acqui-hired"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T16:56:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrj1y4</id>
    <title>Stop it with the Agents/Projects Slop and spam</title>
    <updated>2026-01-30T21:44:24+00:00</updated>
    <author>
      <name>/u/Daemontatox</name>
      <uri>https://old.reddit.com/user/Daemontatox</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The sub is now averaging 3-4 unfinished sloppy Agentic project that's titled the &amp;quot;best next discovery&amp;quot; or &amp;quot;alternative to [insert famous tool here]&amp;quot; or this tool is so amazing i can't even.&lt;/p&gt; &lt;p&gt;It's getting really hard to filter through them and read through the meaningful posts or actual local content.&lt;/p&gt; &lt;p&gt;We need to either add a new tag for slop or ban it altogether because the sub is slowly turning into &amp;quot;omg this tool is clawdbot 2.0&amp;quot; or some guy trying to sell his half finished project that clauded wrote for him on a weekend.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daemontatox"&gt; /u/Daemontatox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrj1y4/stop_it_with_the_agentsprojects_slop_and_spam/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrj1y4/stop_it_with_the_agentsprojects_slop_and_spam/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrj1y4/stop_it_with_the_agentsprojects_slop_and_spam/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T21:44:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrkb1b</id>
    <title>How was GPT-OSS so good?</title>
    <updated>2026-01-30T22:31:44+00:00</updated>
    <author>
      <name>/u/xt8sketchy</name>
      <uri>https://old.reddit.com/user/xt8sketchy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been messing around with a lot of local LLMs (120b and under) recently, and while some of them excel at specific things, none of them feel quite as good as GPT-OSS 120b all-around.&lt;/p&gt; &lt;p&gt;The model is 64GB at full precision, is BLAZING fast, and is pretty good at everything. It's consistent, it calls tools properly, etc.&lt;/p&gt; &lt;p&gt;But it's sort of old... it's been so long since GPT-OSS came out and we haven't really had a decent all-around open-weights/source replacement for it (some may argue GLM4.5 Air, but I personally feel like that model is only really better in agentic software dev, and lags behind in everything else. It's also slower and larger at full precision.)&lt;/p&gt; &lt;p&gt;I'm no expert when it comes to how LLM training/etc works, so forgive me if some of my questions are dumb, but:&lt;br /&gt; - Why don't people train more models in 4-bit natively, like GPT-OSS? Doesn't it reduce training costs? Is there some downside I'm not thinking of?&lt;br /&gt; - I know GPT-OSS was fast in part due to it being A3B, but there are plenty of smaller, dumber, NEWER A3B models that are much slower. What else makes it so fast? Why aren't we using what we learned from GPT-OSS in newer models?&lt;br /&gt; - What about a model (like GPT-OSS) makes it feel so much better? Is it the dataset? Did OpenAI just have a dataset that was THAT GOOD that their model is still relevant HALF A YEAR after release?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xt8sketchy"&gt; /u/xt8sketchy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrkb1b/how_was_gptoss_so_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrkb1b/how_was_gptoss_so_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrkb1b/how_was_gptoss_so_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T22:31:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr4p4x</id>
    <title>Yann LeCun says the best open models are not coming from the West. Researchers across the field are using Chinese models. Openness drove AI progress. Close access, and the West risks slowing itself.</title>
    <updated>2026-01-30T12:55:38+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr4p4x/yann_lecun_says_the_best_open_models_are_not/"&gt; &lt;img alt="Yann LeCun says the best open models are not coming from the West. Researchers across the field are using Chinese models. Openness drove AI progress. Close access, and the West risks slowing itself." src="https://external-preview.redd.it/MnNnNHZ6eGNoaGdnMcC0w-E97YmQ2Bn80LEN79By6gOnSLJ7DXbqces3JuUE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=189015efd04b82b6e73fa3d8be460d38d65659e4" title="Yann LeCun says the best open models are not coming from the West. Researchers across the field are using Chinese models. Openness drove AI progress. Close access, and the West risks slowing itself." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Forbes on YouTube: Yann LeCun Gives Unfiltered Take On The Future Of AI In Davos: &lt;a href="https://www.youtube.com/watch?v=MWMe7yjPYpE"&gt;https://www.youtube.com/watch?v=MWMe7yjPYpE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video by vitrupo on ùïè: &lt;a href="https://x.com/vitrupo/status/2017218170273313033"&gt;https://x.com/vitrupo/status/2017218170273313033&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n31pvrxchhgg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr4p4x/yann_lecun_says_the_best_open_models_are_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr4p4x/yann_lecun_says_the_best_open_models_are_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T12:55:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
