<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-13T11:05:26+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lyitq9</id>
    <title>Any suggestions for generating academic-style/advanced plots?</title>
    <updated>2025-07-13T03:25:25+00:00</updated>
    <author>
      <name>/u/plsendfast</name>
      <uri>https://old.reddit.com/user/plsendfast</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLLaMA community,&lt;/p&gt; &lt;p&gt;I am a researcher, and recently I have noticed that LLMs such as OpenAI's and Google's are not good at generating academic-style and/or beautiful plots. Open sourced model also doesn‚Äôt work well. Beyond the simple plots which they can do just fine, anything more advanced that includes LaTex tikz library etc, will simply just fail.&lt;/p&gt; &lt;p&gt;Has anyone encounter similar issues? If so, any suggestions or recommendations on this? Thank you so much!&lt;/p&gt; &lt;p&gt;TL;DR: Trying to use LLMs to generate academic-style plots but they are not good at all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/plsendfast"&gt; /u/plsendfast &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyitq9/any_suggestions_for_generating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyitq9/any_suggestions_for_generating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyitq9/any_suggestions_for_generating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T03:25:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxpidc</id>
    <title>Where that Unsloth Q0.01_K_M GGUF at?</title>
    <updated>2025-07-12T02:37:05+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxpidc/where_that_unsloth_q001_k_m_gguf_at/"&gt; &lt;img alt="Where that Unsloth Q0.01_K_M GGUF at?" src="https://preview.redd.it/e2em6rucvccf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4380544f532ff369f435679247aa08f3c9afdb66" title="Where that Unsloth Q0.01_K_M GGUF at?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e2em6rucvccf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxpidc/where_that_unsloth_q001_k_m_gguf_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxpidc/where_that_unsloth_q001_k_m_gguf_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T02:37:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxnsh1</id>
    <title>OpenAI delays its open weight model again for "safety tests"</title>
    <updated>2025-07-12T01:09:38+00:00</updated>
    <author>
      <name>/u/lyceras</name>
      <uri>https://old.reddit.com/user/lyceras</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnsh1/openai_delays_its_open_weight_model_again_for/"&gt; &lt;img alt="OpenAI delays its open weight model again for &amp;quot;safety tests&amp;quot;" src="https://preview.redd.it/z5xvjxzefccf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe88bccce70567bd39edea238607127c143134db" title="OpenAI delays its open weight model again for &amp;quot;safety tests&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lyceras"&gt; /u/lyceras &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z5xvjxzefccf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnsh1/openai_delays_its_open_weight_model_again_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxnsh1/openai_delays_its_open_weight_model_again_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T01:09:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyb8tz</id>
    <title>Banana for scale</title>
    <updated>2025-07-12T21:11:10+00:00</updated>
    <author>
      <name>/u/blackwell_tart</name>
      <uri>https://old.reddit.com/user/blackwell_tart</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyb8tz/banana_for_scale/"&gt; &lt;img alt="Banana for scale" src="https://preview.redd.it/3gsbxg74eicf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4f149ec7379ca441b40ea5b4c75ffb6609f7405" title="Banana for scale" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In time-honored tradition we present the relative physical dimensions of the Workstation Pro 6000.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blackwell_tart"&gt; /u/blackwell_tart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3gsbxg74eicf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyb8tz/banana_for_scale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyb8tz/banana_for_scale/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T21:11:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyq1yh</id>
    <title>LLM evaluation in real life?</title>
    <updated>2025-07-13T10:59:53+00:00</updated>
    <author>
      <name>/u/Plastic-Bus-7003</name>
      <uri>https://old.reddit.com/user/Plastic-Bus-7003</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;Wanted to ask a question that's been on my mind recently.&lt;/p&gt; &lt;p&gt;I've done LLM research in academia in various forms, each time I thought of a way to improve a certain aspect of LLMs for different tasks, and when asked to prove that my alteration actually improved upon something I almost always had a benchmark to test myself.&lt;/p&gt; &lt;p&gt;But how is LLM evaluation done in real life (i.e. in industry)? If I'm a company that wants to offer a strong coding-assistant, research-assistant or any other type of LLM product - How do I make sure that it's doing a good job?&lt;/p&gt; &lt;p&gt;Is it only product related metrics like customer satisfaction and existing benchmarks like in the industry? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plastic-Bus-7003"&gt; /u/Plastic-Bus-7003 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyq1yh/llm_evaluation_in_real_life/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyq1yh/llm_evaluation_in_real_life/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyq1yh/llm_evaluation_in_real_life/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T10:59:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyq22j</id>
    <title>Local LLM to back Elastic AI</title>
    <updated>2025-07-13T11:00:03+00:00</updated>
    <author>
      <name>/u/OldManCyberNinja</name>
      <uri>https://old.reddit.com/user/OldManCyberNinja</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I'm building a fully air-gapped deployment that integrates with Elastic Security and Observability, including Elastic AI Assistant via OpenInference API. My use case involves log summarisation, alert triage, threat intel enrichment (using MISP), and knowledge base retrieval. About 5000 users, about 2000 servers. All on-prem.&lt;/p&gt; &lt;p&gt;I've shortlisted Meta's LLaMA 4 Maverick 17B 128E Instruct model as a candidate for this setup. Reason is it is instruction-tuned, long-context, and MoE-optimised. It fits Elastic's model requirements . I'm planning to run it at full precision (BF16 or FP16) using vLLM or Ollama, but happy to adapt if others have better suggestions.&lt;/p&gt; &lt;p&gt;I did look at &lt;a href="https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix"&gt;https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix&lt;/a&gt; but it is somewhat out of date now.&lt;/p&gt; &lt;p&gt;I have a pretty solid budget (though 3 A100s is probably the limit once the rest of the hardware is taken into account)&lt;/p&gt; &lt;p&gt;Looking for help with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model feedback: Anyone using LLaMA 4 Maverick or other Elastic-supported models (like Mistral Instruct or LLaMA 3.1 Instruct)?&lt;/li&gt; &lt;li&gt;Hardware: What server setup did you use? Any success with Dell XE7745, HPE GPU nodes, or DIY rigs with A100s/H100s?&lt;/li&gt; &lt;li&gt;Fine-tuning: Anyone LoRA-fine-tuned Maverick or similar for log alerting, ECS fields, or threat context?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I have some constraints:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Must be air-gapped&lt;/li&gt; &lt;li&gt;I can't use Chinese, Israeli or similar products. CISO doesn't allow it. I know some of the Chinese models would be a good fit, but its a no-go.&lt;/li&gt; &lt;li&gt;Need to support long-context summarisation, RAG-style enrichment, and Elastic Assistant prompt structure&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear from anyone who‚Äôs done this in production or lab.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OldManCyberNinja"&gt; /u/OldManCyberNinja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T11:00:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxycdh</id>
    <title>Safety first, or whateverüôÑ</title>
    <updated>2025-07-12T11:37:36+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxycdh/safety_first_or_whatever/"&gt; &lt;img alt="Safety first, or whateverüôÑ" src="https://preview.redd.it/idk5uvesjfcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=326cdedce8274c918a8336924d8741c3576c2f5a" title="Safety first, or whateverüôÑ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/idk5uvesjfcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxycdh/safety_first_or_whatever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxycdh/safety_first_or_whatever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T11:37:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyonb4</id>
    <title>How are people actually able to get the system prompt of these AI companies?</title>
    <updated>2025-07-13T09:26:26+00:00</updated>
    <author>
      <name>/u/divyamchandel</name>
      <uri>https://old.reddit.com/user/divyamchandel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While I am extremely grateful that people do post the leaked system prompt online for inspiration, but also curious how its actually possible?&lt;/p&gt; &lt;p&gt;There are three things that come to my mind:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Using some prompt injection (re-iteratively)&lt;/em&gt;&lt;/strong&gt;: Some kind of jailbreak prompt and see if same things are being repeated, assuming that is what the actual system prompt is&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Inspecting the client side code if possible&lt;/em&gt;&lt;/strong&gt;: For applications intercepting the api requests / client side bundle to find system prompts if any? This sounds hard&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;Changing the request server&lt;/em&gt;&lt;/strong&gt;: Maybe having a custom model running on my server and changing the base url for the request to hit my resource instead of the default one? Somehow getting the information from there?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If anyone has any idea how it works, would love to understand. If any resources to read would also be super helpful! Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/divyamchandel"&gt; /u/divyamchandel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyonb4/how_are_people_actually_able_to_get_the_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyonb4/how_are_people_actually_able_to_get_the_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyonb4/how_are_people_actually_able_to_get_the_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T09:26:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly894z</id>
    <title>mlx-community/Kimi-Dev-72B-4bit-DWQ</title>
    <updated>2025-07-12T19:01:40+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly894z/mlxcommunitykimidev72b4bitdwq/"&gt; &lt;img alt="mlx-community/Kimi-Dev-72B-4bit-DWQ" src="https://external-preview.redd.it/mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d6276c1fe0578c79ffa2210b8dfa820b87e4242" title="mlx-community/Kimi-Dev-72B-4bit-DWQ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mlx-community/Kimi-Dev-72B-4bit-DWQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly894z/mlxcommunitykimidev72b4bitdwq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly894z/mlxcommunitykimidev72b4bitdwq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T19:01:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyl697</id>
    <title>How do you make Loras for Qwen coder / devstral?</title>
    <updated>2025-07-13T05:39:00+00:00</updated>
    <author>
      <name>/u/ComprehensiveBird317</name>
      <uri>https://old.reddit.com/user/ComprehensiveBird317</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am wondering if anyone did this before, at least I couldn't find information on it. I want to fine tune a coding model without changing the whole model (for hardware restriction reasons). Loras, in theory, would do that. But how? For image and video generation this is pretty much solved and common, but llms? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComprehensiveBird317"&gt; /u/ComprehensiveBird317 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyl697/how_do_you_make_loras_for_qwen_coder_devstral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyl697/how_do_you_make_loras_for_qwen_coder_devstral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyl697/how_do_you_make_loras_for_qwen_coder_devstral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T05:39:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyjgwv</id>
    <title>[Help] Fastest model for real-time UI automation? (Browser-Use too slow)</title>
    <updated>2025-07-13T04:00:54+00:00</updated>
    <author>
      <name>/u/BulkyAd7044</name>
      <uri>https://old.reddit.com/user/BulkyAd7044</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm working on a browser automation system that follows a planned sequence of UI actions, but needs an LLM to resolve which DOM element to click when there are multiple similar options. I‚Äôve been using &lt;strong&gt;Browser-Use&lt;/strong&gt;, which is solid for tracking state/actions, but execution is too slow ‚Äî especially when an LLM is in the loop at each step.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example flow (on Google settings):&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Go to &lt;a href="https://myaccount.google.com"&gt;myaccount.google.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Click ‚ÄúData &amp;amp; privacy‚Äù&lt;/li&gt; &lt;li&gt;Scroll down&lt;/li&gt; &lt;li&gt;Click ‚ÄúDelete a service or your account‚Äù&lt;/li&gt; &lt;li&gt;Click ‚ÄúDelete your Google Account‚Äù&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Looking for suggestions:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fastest models for small structured decision tasks &lt;/li&gt; &lt;li&gt;Ways to be under 1s per step (ideally &amp;lt;500ms)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I don‚Äôt need full chat reasoning ‚Äî just high-confidence decisions from small JSON lists.&lt;/p&gt; &lt;p&gt;Would love to hear what setups/models have worked for you in similar low-latency UI agent tasks üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BulkyAd7044"&gt; /u/BulkyAd7044 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyjgwv/help_fastest_model_for_realtime_ui_automation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyjgwv/help_fastest_model_for_realtime_ui_automation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyjgwv/help_fastest_model_for_realtime_ui_automation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T04:00:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly6cg6</id>
    <title>Kyutai Text-to-Speech is considering opening up custom voice model training, but they are asking for community support!</title>
    <updated>2025-07-12T17:41:49+00:00</updated>
    <author>
      <name>/u/pilkyton</name>
      <uri>https://old.reddit.com/user/pilkyton</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kyutai is one of the best text to speech models, with very low latency, real-time &amp;quot;text streaming to audio&amp;quot; generation (great for turning LLM output into audio in real-time), and great accuracy at following the text prompt. And unlike most other models, it's able to generate very long audio files.&lt;/p&gt; &lt;p&gt;It's &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/"&gt;one of the chart leaders in benchmarks&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;But it's completely locked down and can only output some terrible stock voices. They gave a weird justification about morality despite the fact that lots of other voice models already support voice training.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Now they are asking the community to voice their support for adding a training feature. If you have GitHub, go here and vote/let them know your thoughts:&lt;/p&gt; &lt;h1&gt;&lt;a href="https://github.com/kyutai-labs/delayed-streams-modeling/issues/64"&gt;https://github.com/kyutai-labs/delayed-streams-modeling/issues/64&lt;/a&gt;&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pilkyton"&gt; /u/pilkyton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly6cg6/kyutai_texttospeech_is_considering_opening_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly6cg6/kyutai_texttospeech_is_considering_opening_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly6cg6/kyutai_texttospeech_is_considering_opening_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T17:41:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lymlgp</id>
    <title>Dark Arts: Speaker embedding gradient descent for local TTS models</title>
    <updated>2025-07-13T07:08:39+00:00</updated>
    <author>
      <name>/u/rzvzn</name>
      <uri>https://old.reddit.com/user/rzvzn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;[As with all my posts, the code and text are organic with no LLM involved. Note that I myself have not confirmed that this works in all cases--I personally have no interest in voice cloning--but in my head the theory is strong and I am confident it should work. Plus, there is historical precedent in soft prompting and control vectors.]&lt;/p&gt; &lt;p&gt;Let's say you have a local TTS model that takes a speaker embedding &lt;code&gt;spk_emb&lt;/code&gt;, but the model to produce the speaker embedding is unavailable. You can simply apply gradient descent on the speaker embedding and freeze everything else.&lt;/p&gt; &lt;p&gt;Here is the pseudocode. You will need to change the code depending on the model you are using, and there are plenty of knobs to tune.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import torch # 1. Initialize the embedding, either randomly or nearest neighbor spk_emb = torch.randn(1, 512) # if batch size 1, dim 512 spk_emb.requires_grad = True # 2. Initialize the model and freeze its parameters model = YourModelClass.from_pretrained('TODO') device = 'cuda' if torch.cuda.is_available() else 'cpu' model.to(device).eval() for p in model.parameters(): p.requires_grad = False # 3. Optimizer and dataset, LR is up to you optimizer = torch.optim.Adam([spk_emb], lr=0.001) TODO_your_dataset_of_text_audio_pairs = [ ('This is some text.', 'corresponding_audio.wav'), # ... ] # 4. Barebones training loop. You can add a learning rate scheduler, etc. for epoch in range(10): # how many epochs is up to you for text, audio in TODO_your_dataset_of_text_audio_pairs: loss = model.forward_with_loss(text, audio, spk_emb) loss.backward() optimizer.step() optimizer.zero_grad() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The big caveat here is that you cannot get blood out of a stone; if a speaker is firmly out-of-distribution for the model, no amount of gradient descent will get you to where you want to go.&lt;/p&gt; &lt;p&gt;And that's it. If you have any questions you can post them below.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rzvzn"&gt; /u/rzvzn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lymlgp/dark_arts_speaker_embedding_gradient_descent_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lymlgp/dark_arts_speaker_embedding_gradient_descent_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lymlgp/dark_arts_speaker_embedding_gradient_descent_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T07:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyfngg</id>
    <title>How do you keep up with all these things?</title>
    <updated>2025-07-13T00:39:07+00:00</updated>
    <author>
      <name>/u/ontologicalmemes</name>
      <uri>https://old.reddit.com/user/ontologicalmemes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like everyday I come here someone mentions a a new tool or a newly released model or software that I never heard off. Where in earth are you going to get your most up to dated trusted news/info? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ontologicalmemes"&gt; /u/ontologicalmemes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyfngg/how_do_you_keep_up_with_all_these_things/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyfngg/how_do_you_keep_up_with_all_these_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyfngg/how_do_you_keep_up_with_all_these_things/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T00:39:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyozcn</id>
    <title>Wrote a deep dive on LLM tool calling with step-by-step REST and Spring AI examples</title>
    <updated>2025-07-13T09:49:23+00:00</updated>
    <author>
      <name>/u/muthuishere2101</name>
      <uri>https://old.reddit.com/user/muthuishere2101</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyozcn/wrote_a_deep_dive_on_llm_tool_calling_with/"&gt; &lt;img alt="Wrote a deep dive on LLM tool calling with step-by-step REST and Spring AI examples" src="https://external-preview.redd.it/1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f6b426f7ec6446f2ee47476b0c04d150cea9b8a5" title="Wrote a deep dive on LLM tool calling with step-by-step REST and Spring AI examples" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/muthuishere2101"&gt; /u/muthuishere2101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://muthuishere.medium.com/understanding-tool-function-calling-in-llms-step-by-step-examples-in-rest-and-spring-ai-2149ecd6b18b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyozcn/wrote_a_deep_dive_on_llm_tool_calling_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyozcn/wrote_a_deep_dive_on_llm_tool_calling_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T09:49:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly4zh8</id>
    <title>Okay kimi-k2 is an INSANE model WTF those one-shot animations</title>
    <updated>2025-07-12T16:44:50+00:00</updated>
    <author>
      <name>/u/sirjoaco</name>
      <uri>https://old.reddit.com/user/sirjoaco</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly4zh8/okay_kimik2_is_an_insane_model_wtf_those_oneshot/"&gt; &lt;img alt="Okay kimi-k2 is an INSANE model WTF those one-shot animations" src="https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88ce744efba81890d05b5b715ce402a332366d7a" title="Okay kimi-k2 is an INSANE model WTF those one-shot animations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sirjoaco"&gt; /u/sirjoaco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/74d8efoh2hcf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly4zh8/okay_kimik2_is_an_insane_model_wtf_those_oneshot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly4zh8/okay_kimik2_is_an_insane_model_wtf_those_oneshot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T16:44:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lykf92</id>
    <title>What Causes Poor Long-Context Performance?</title>
    <updated>2025-07-13T04:54:44+00:00</updated>
    <author>
      <name>/u/simulated-souls</name>
      <uri>https://old.reddit.com/user/simulated-souls</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While some models (Gemini, MiniMax, Llama4) claim context lengths in the 1M+ token range, performance beyond ~100K tokens is usually quite poor. Beyond those lengths is it is usually &lt;a href="https://www.databricks.com/blog/long-context-rag-performance-llms"&gt;better to do RAG&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Why is that? Does the limit come from architecture or training data?&lt;/p&gt; &lt;p&gt;I could see one problem being too much noise/distraction in the attention scores (like in &lt;a href="https://arxiv.org/pdf/2410.05258"&gt;this paper&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;However, I could also see it being from a lack of long-context training data. A novel is around 100K tokens, so it lines up that performance beyond that degrades due to lack of examples. I believe the creators of Fiction.liveBench have also mentioned the difficulty of creating extremely long context benchmarks.&lt;/p&gt; &lt;p&gt;What is the consensus, and how long might it be until the problem is solved?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simulated-souls"&gt; /u/simulated-souls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T04:54:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly8fyj</id>
    <title>This whole thing is giving me WizardLM2 vibes.</title>
    <updated>2025-07-12T19:09:44+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly8fyj/this_whole_thing_is_giving_me_wizardlm2_vibes/"&gt; &lt;img alt="This whole thing is giving me WizardLM2 vibes." src="https://preview.redd.it/kn56m7cgshcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a11d5998e82e27b041a8e6dd74d76c55a2f8a104" title="This whole thing is giving me WizardLM2 vibes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kn56m7cgshcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly8fyj/this_whole_thing_is_giving_me_wizardlm2_vibes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly8fyj/this_whole_thing_is_giving_me_wizardlm2_vibes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T19:09:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyj81f</id>
    <title>Do you think an AI will achieve gold medal in 2025 International Math Olympad (tomorrow)</title>
    <updated>2025-07-13T03:47:18+00:00</updated>
    <author>
      <name>/u/mathsTeacher82</name>
      <uri>https://old.reddit.com/user/mathsTeacher82</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The International Math Olympiad will take place on 15th and 16th July in Australia. Google Deepmind will attempt to win a gold medal with their models AlphaProof and AlphaGeometry, after announcing a silver medal performance in 2024. Any open-source model that wins a gold medal will receive a $5 million AIMO prize from XTX markets.&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/vJjgtOcXq8A"&gt;https://youtu.be/vJjgtOcXq8A&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mathsTeacher82"&gt; /u/mathsTeacher82 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyj81f/do_you_think_an_ai_will_achieve_gold_medal_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyj81f/do_you_think_an_ai_will_achieve_gold_medal_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyj81f/do_you_think_an_ai_will_achieve_gold_medal_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T03:47:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lykqbu</id>
    <title>SmolLM-3B when asked if it was Peter Griffin</title>
    <updated>2025-07-13T05:12:45+00:00</updated>
    <author>
      <name>/u/Humble_Hovercraft199</name>
      <uri>https://old.reddit.com/user/Humble_Hovercraft199</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lykqbu/smollm3b_when_asked_if_it_was_peter_griffin/"&gt; &lt;img alt="SmolLM-3B when asked if it was Peter Griffin" src="https://external-preview.redd.it/tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74c41be70bfcf32292dc40a30f75326535854875" title="SmolLM-3B when asked if it was Peter Griffin" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was testing the &lt;a href="https://huggingface.co/spaces/HuggingFaceTB/SmolLM3-3B-WebGPU"&gt;SmolLM3-3B-WebGPU&lt;/a&gt; Hugging Face Space to check its token speed on my machine (a solid 46 t/s!) before downloading and running it locally. When I prompted it with: &amp;quot;Are you peter griffin?&amp;quot;, it just generated a 4000-token list of &amp;quot;Key Takeaways&amp;quot; about its existence:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qn27gqdtqkcf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a20b4efa6d3891cb83f425a5380358b335640e0"&gt;https://preview.redd.it/qn27gqdtqkcf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a20b4efa6d3891cb83f425a5380358b335640e0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was only able to trigger this behavior on that specific HF Space (Although, it doesn't seem to be a one time thing. I was able to get &lt;em&gt;very&lt;/em&gt; similar responses by asking it the same question again in a new tab, after refreshing). I've since downloaded the model and wasn't able to replicate this locally. The model via the Hugging Face Inference also behaves as expected. Could this be caused by the ONNX conversion for WebGPU, or maybe some specific sampling parameters on the space? Has anyone seen anything like this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Humble_Hovercraft199"&gt; /u/Humble_Hovercraft199 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lykqbu/smollm3b_when_asked_if_it_was_peter_griffin/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lykqbu/smollm3b_when_asked_if_it_was_peter_griffin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lykqbu/smollm3b_when_asked_if_it_was_peter_griffin/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T05:12:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ly42e5</id>
    <title>Interesting info about Kimi K2</title>
    <updated>2025-07-12T16:05:34+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly42e5/interesting_info_about_kimi_k2/"&gt; &lt;img alt="Interesting info about Kimi K2" src="https://preview.redd.it/klm2b78lvgcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=32a0ebb795c06ba955385d6c0102e57e0fd85423" title="Interesting info about Kimi K2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2 is basically DeepSeek V3 but with fewer heads and more experts.&lt;/p&gt; &lt;p&gt;Source: @rasbt on X&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/klm2b78lvgcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ly42e5/interesting_info_about_kimi_k2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ly42e5/interesting_info_about_kimi_k2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T16:05:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxyj92</id>
    <title>"We will release o3 wieghts next week"</title>
    <updated>2025-07-12T11:48:49+00:00</updated>
    <author>
      <name>/u/Qparadisee</name>
      <uri>https://old.reddit.com/user/Qparadisee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/"&gt; &lt;img alt="&amp;quot;We will release o3 wieghts next week&amp;quot;" src="https://external-preview.redd.it/MHB1MWw1YnJsZmNmMdL5zV43Lc9tDShB7DOvm21L1LTW10tUK0LGB85rL2PQ.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=496b59bcbb39fe55592a5937a63530bc06699a52" title="&amp;quot;We will release o3 wieghts next week&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qparadisee"&gt; /u/Qparadisee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8iqku5brlfcf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T11:48:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lxyvto</id>
    <title>we have to delay it</title>
    <updated>2025-07-12T12:08:26+00:00</updated>
    <author>
      <name>/u/ILoveMy2Balls</name>
      <uri>https://old.reddit.com/user/ILoveMy2Balls</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"&gt; &lt;img alt="we have to delay it" src="https://preview.redd.it/oma34zdapfcf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=108d706d99e4ad19833dd94c54c220bc8d7544f5" title="we have to delay it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ILoveMy2Balls"&gt; /u/ILoveMy2Balls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oma34zdapfcf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T12:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lyaozv</id>
    <title>Moonshot AI just made their moonshot</title>
    <updated>2025-07-12T20:46:47+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyaozv/moonshot_ai_just_made_their_moonshot/"&gt; &lt;img alt="Moonshot AI just made their moonshot" src="https://preview.redd.it/95q67pnr9icf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2af006a61647e3c965c2e033c957c97e3e1f42cd" title="Moonshot AI just made their moonshot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Screenshot: &lt;a href="https://openrouter.ai/moonshotai"&gt;https://openrouter.ai/moonshotai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Announcement: &lt;a href="https://moonshotai.github.io/Kimi-K2/"&gt;https://moonshotai.github.io/Kimi-K2/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model: &lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct"&gt;https://huggingface.co/moonshotai/Kimi-K2-Instruct&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/95q67pnr9icf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lyaozv/moonshot_ai_just_made_their_moonshot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lyaozv/moonshot_ai_just_made_their_moonshot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-12T20:46:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lylo75</id>
    <title>Kimi-K2 takes top spot on EQ-Bench3 and Creative Writing</title>
    <updated>2025-07-13T06:09:23+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lylo75/kimik2_takes_top_spot_on_eqbench3_and_creative/"&gt; &lt;img alt="Kimi-K2 takes top spot on EQ-Bench3 and Creative Writing" src="https://b.thumbs.redditmedia.com/_mu9EQ2-CS-NLztYt8TCn8nhmS5cqsN6BOfAQW9BupA.jpg" title="Kimi-K2 takes top spot on EQ-Bench3 and Creative Writing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://eqbench.com/"&gt;https://eqbench.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/moonshotai__Kimi-K2-Instruct.html"&gt;https://eqbench.com/results/creative-writing-v3/moonshotai__Kimi-K2-Instruct.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EQ-Bench responses:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/eqbench3_reports/moonshotai__kimi-k2-instruct.html"&gt;https://eqbench.com/results/eqbench3_reports/moonshotai__kimi-k2-instruct.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lylo75"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lylo75/kimik2_takes_top_spot_on_eqbench3_and_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lylo75/kimik2_takes_top_spot_on_eqbench3_and_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-13T06:09:23+00:00</published>
  </entry>
</feed>
