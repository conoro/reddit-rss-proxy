<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-04T07:34:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1nx1ryu</id>
    <title>Local LLMs for TTS &amp; RAG in my game - a huge thank you to this community!</title>
    <updated>2025-10-03T15:10:03+00:00</updated>
    <author>
      <name>/u/orblabs</name>
      <uri>https://old.reddit.com/user/orblabs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ryu/local_llms_for_tts_rag_in_my_game_a_huge_thank/"&gt; &lt;img alt="Local LLMs for TTS &amp;amp; RAG in my game - a huge thank you to this community!" src="https://external-preview.redd.it/aGQ3bmJ3MTd3d3NmMV7kygU8pU1boY2_bUjRfQiVFgAFZeNf1RrlrJxGbDfE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b396e04745857ae3adb5dbdda5c992d2ab46ce6f" title="Local LLMs for TTS &amp;amp; RAG in my game - a huge thank you to this community!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I wanted to share a quick video of something I'm really excited about and that this community was a huge inspiration for.&lt;/p&gt; &lt;p&gt;For those who haven't seen my project, Synthasia, it's a standalone interactive storytelling engine I'm building. The goal is to create dynamic, AI-powered narrative experiences, and a big part of that is making it accessible and customizable.&lt;/p&gt; &lt;p&gt;From the beginning, I knew I wanted to support local models, and lurking here has been a massive catalyst. Seeing the passion and the incredible progress everyone is making pushed me to double down on integrating local, multi-platform solutions.&lt;/p&gt; &lt;p&gt;The video shows our new Text-to-Speech system completely builtin into the &amp;quot;game&amp;quot; levaraging transformers.js and webgpu for multiplatform hardware accelerated local TTS ! (the actual TTS is Kokoro) . The dream is to have fully voiced, dynamic characters, and local TTS is making that a reality.&lt;/p&gt; &lt;p&gt;On top of that, we're using WebLLM (again, webgpu support for optimal performance) to generate embeddings for our RAG system, right on the user's machine. This was a fun challenge, partly because we use OpenRouter for a lot of the heavy lifting, but they don't offer an embeddings endpoint. This community gave me the confidence to build a solution that lets users run their own embedding models locally, which is a huge win for privacy and offline capability.&lt;/p&gt; &lt;p&gt;It feels like we're at a pivotal moment, almost like a renaissance of the old text-adventure spirit. We're standing on the shoulders of giants, taking those foundational ideas of interactive stories and exploring where we can go with the incredible power of modern LLMs. It's not about replacing the classics, but building on them to create entirely new kinds of experiences. Needless to say that not all game dev related communities are (absolutely understandably) particularly welcoming towards AI usage, here instead the project feels at home and the response to my past posts has been amazing and i am very grateful for it.&lt;/p&gt; &lt;p&gt;Anyway, I just wanted to share my progress and say a huge thank you. This is one of the most innovative and helpful communities on the internet, and it's been a huge motivator.&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;p&gt;P.S. we have a discord server where a handful of users have begun testing the very early alpha builds of Synthasia, if you care to join to help, share feedback, have a chat or just give a look around, we would be very happy to have you : &lt;a href="https://discord.gg/2wc4n2GMmn"&gt;https://discord.gg/2wc4n2GMmn&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/orblabs"&gt; /u/orblabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8f8svv17wwsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ryu/local_llms_for_tts_rag_in_my_game_a_huge_thank/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ryu/local_llms_for_tts_rag_in_my_game_a_huge_thank/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T15:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx1jq5</id>
    <title>My GLaDOS local LLM found its front end UI pedestrian. I have real-time satellite tracking for 8600+ starlink satellites (my network), the ISS, a local RAG and persistent memory, camera access/image analysis functional. TTS and STT capable. Wikipedia tool calling.</title>
    <updated>2025-10-03T15:01:29+00:00</updated>
    <author>
      <name>/u/Fear_ltself</name>
      <uri>https://old.reddit.com/user/Fear_ltself</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1jq5/my_glados_local_llm_found_its_front_end_ui/"&gt; &lt;img alt="My GLaDOS local LLM found its front end UI pedestrian. I have real-time satellite tracking for 8600+ starlink satellites (my network), the ISS, a local RAG and persistent memory, camera access/image analysis functional. TTS and STT capable. Wikipedia tool calling." src="https://external-preview.redd.it/anc5NjJ5bm52d3NmMezezeVRJsE82oVjoZEwhtc7ecTk1nRTlWTddkbdSY-W.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48a8d7a189e05c966f6a7f103cf60a9a092932d5" title="My GLaDOS local LLM found its front end UI pedestrian. I have real-time satellite tracking for 8600+ starlink satellites (my network), the ISS, a local RAG and persistent memory, camera access/image analysis functional. TTS and STT capable. Wikipedia tool calling." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It has 5 servers running on the backend to support the Text to Speech and Speech to Text functionality all the way through. It has persistent memory for a local RAG. I’m working on tweaking it a bit but it seemingly has a ton of context about itself based on the prompts I’ve provided. It correctly understands its own place as my local LLM but, and provides feedback in the from of a GLaDOS personality matrix. I’ve found this be a great blend of helpful and funny, it actually answers my questions “how hot is it?” But in a funny smart assy way like GLaDOS would &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fear_ltself"&gt; /u/Fear_ltself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/85danivnvwsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1jq5/my_glados_local_llm_found_its_front_end_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1jq5/my_glados_local_llm_found_its_front_end_ui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T15:01:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx15z4</id>
    <title>Fine-tuning a 7B model for vibe coding games and open sourcing everything along the way. Advice appreciated!</title>
    <updated>2025-10-03T14:47:04+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx15z4/finetuning_a_7b_model_for_vibe_coding_games_and/"&gt; &lt;img alt="Fine-tuning a 7B model for vibe coding games and open sourcing everything along the way. Advice appreciated!" src="https://preview.redd.it/81unpxqmswsf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2421c197b9ee07e94dd0532eb911322654264fe4" title="Fine-tuning a 7B model for vibe coding games and open sourcing everything along the way. Advice appreciated!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Background: I am working on an open-source app that uses a local LLM for vibe coding retro-style arcade games on consumer-level laptops.&lt;/p&gt; &lt;p&gt;I tried a bunch of models in the 4-8B range and found they all have pretty low performance for this task (Qwen3-Coder-30b works great but needs too much RAM). I shared my initial experience in a recent post.&lt;/p&gt; &lt;p&gt;Now I am trying to fine-tune a model to improve performance. If this succeeds, I want to make the project a community reference design to help others get LLM apps working on laptops!&lt;/p&gt; &lt;p&gt;So far I have:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;MIT licensed dataset (154 game files, 30k+ LoC): &lt;a href="https://github.com/lemonade-sdk/playable-data"&gt;https://github.com/lemonade-sdk/playable-data&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Fine-tuned a couple of models on Together AI and MIT licensed those as well: &lt;a href="https://huggingface.co/playable"&gt;https://huggingface.co/playable&lt;/a&gt; &lt;ul&gt; &lt;li&gt;Results are interesting, but not nearly production-ready yet! See the attached image, where iat-02 made Pong with sideways paddles because I fine-tined on too much Breakout data.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;A detailed log of methodology and results is &lt;a href="https://github.com/lemonade-sdk/playable-data/blob/main/docs/togetherai.md"&gt;here&lt;/a&gt; if anyone is curious.&lt;/p&gt; &lt;p&gt;Questions I could use advice with:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;What is the easiest tooling for this kind of work?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I'm using Together AI to make LORAs right now, but I'm unhappy with their queue times, model selection, and overall flexibility. Looking for something turnkey, and preferably cloud-based.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;How does my dataset look?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If my goal is to get a 7B model to oneshot a few basic arcade games (Snake, Pong, Space Invaders, Asteroids, Breakout) is the dataset big enough?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Any advice about fine-tuning settings (LORA rank, etc.)?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can find my current settings in log linked above.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Huge thanks in advance to anyone who can give me some pointers!&lt;/p&gt; &lt;p&gt;edit: fixing markdown formatting&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/81unpxqmswsf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx15z4/finetuning_a_7b_model_for_vibe_coding_games_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx15z4/finetuning_a_7b_model_for_vibe_coding_games_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T14:47:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxfpqd</id>
    <title>Tips for getting OSS-120B to run faster at longer context?</title>
    <updated>2025-10-04T00:21:40+00:00</updated>
    <author>
      <name>/u/Acceptable_Adagio_91</name>
      <uri>https://old.reddit.com/user/Acceptable_Adagio_91</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;UPDATE - Swapping to the Q4_K_XL unsloth GGUF and removing the KV quantization seems to have done the trick! Getting much higher speeds now across the board and at longer context lengths.&lt;/p&gt; &lt;p&gt;I'm running OSS 120B (f16 GGUF from unsloth) in llama.cpp using the llamacpp-gptoss-120b container, on 3x 3090s, on linux. i9 7900x CPU with 64GB system ram.&lt;/p&gt; &lt;p&gt;Weights and cache fully offloaded to GPU. Llama settings are:&lt;/p&gt; &lt;p&gt;--ctx-size 131k (max)&lt;/p&gt; &lt;p&gt;--flash-attn&lt;/p&gt; &lt;p&gt;-- K &amp;amp; V cache Q8&lt;/p&gt; &lt;p&gt;--batch 512&lt;/p&gt; &lt;p&gt;--ubatch-size 128&lt;/p&gt; &lt;p&gt;--threads 10&lt;/p&gt; &lt;p&gt;--threads_batch 10&lt;/p&gt; &lt;p&gt;--tensor-split 0.30,0.34,0.36&lt;/p&gt; &lt;p&gt;--jinja&lt;/p&gt; &lt;p&gt;--verbose&lt;/p&gt; &lt;p&gt;--main-gpu 2&lt;/p&gt; &lt;p&gt;--split-mode layer&lt;/p&gt; &lt;p&gt;At short prompts (less than 1k) I get like 30-40tps, but as soon as I put more than 2-3k of context in, it grinds way down to like 10-tps or less. Token ingestion takes ages too, like 30s to 1 minute for 3-4k tokens.&lt;/p&gt; &lt;p&gt;I feel like this can't be right, I'm not even getting anywhere close to max context length (at this rate it would be unusably slow anyway).. There must be a way to get this working better/faster&lt;/p&gt; &lt;p&gt;Anyone else running this model on a similar setup that can share their settings and experience with getting the most out of this model?&lt;/p&gt; &lt;p&gt;I haven't tried ex_lllama yet but I have heard it might be better/faster than llama so I could try that&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable_Adagio_91"&gt; /u/Acceptable_Adagio_91 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxfpqd/tips_for_getting_oss120b_to_run_faster_at_longer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxfpqd/tips_for_getting_oss120b_to_run_faster_at_longer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxfpqd/tips_for_getting_oss120b_to_run_faster_at_longer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T00:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx71mm</id>
    <title>Looks like the ASUS Ascent GX10 release is imminent</title>
    <updated>2025-10-03T18:25:32+00:00</updated>
    <author>
      <name>/u/noco-ai</name>
      <uri>https://old.reddit.com/user/noco-ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx71mm/looks_like_the_asus_ascent_gx10_release_is/"&gt; &lt;img alt="Looks like the ASUS Ascent GX10 release is imminent" src="https://preview.redd.it/4b1db2z8vxsf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c85c12e9af6ec2b65feed208efacca05e4f18db" title="Looks like the ASUS Ascent GX10 release is imminent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noco-ai"&gt; /u/noco-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4b1db2z8vxsf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx71mm/looks_like_the_asus_ascent_gx10_release_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx71mm/looks_like_the_asus_ascent_gx10_release_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T18:25:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxlq7t</id>
    <title>Unsure which ollama model to use? Here's a tool I built to help</title>
    <updated>2025-10-04T05:36:03+00:00</updated>
    <author>
      <name>/u/h3xzur7</name>
      <uri>https://old.reddit.com/user/h3xzur7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’m fairly new to working with local LLMs, and like many, I wondered which model(s) I should use. To help answer that, I put together a tool that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Automates running multiple models on custom prompts&lt;/li&gt; &lt;li&gt;Outputs everything into a clean, easy-to-read HTML report&lt;/li&gt; &lt;li&gt;Lets you quickly compare results side by side&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;While there might be similar tools out there, I wanted something lightweight and straightforward for my own workflow. I figured I’d share in case others find it useful too.&lt;/p&gt; &lt;p&gt;I’d love any constructive feedback—whether you think this fills a gap, how it could be improved, or if you know of alternatives I should check out.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Spectral-Knight-Ops/local-llm-evaluator"&gt;https://github.com/Spectral-Knight-Ops/local-llm-evaluator&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/h3xzur7"&gt; /u/h3xzur7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxlq7t/unsure_which_ollama_model_to_use_heres_a_tool_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxlq7t/unsure_which_ollama_model_to_use_heres_a_tool_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxlq7t/unsure_which_ollama_model_to_use_heres_a_tool_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T05:36:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwzs0k</id>
    <title>My key takeaways on Qwen3-Next's four pillar innovations, highlighting its Hybrid Attention design</title>
    <updated>2025-10-03T13:52:48+00:00</updated>
    <author>
      <name>/u/MarketingNetMind</name>
      <uri>https://old.reddit.com/user/MarketingNetMind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwzs0k/my_key_takeaways_on_qwen3nexts_four_pillar/"&gt; &lt;img alt="My key takeaways on Qwen3-Next's four pillar innovations, highlighting its Hybrid Attention design" src="https://b.thumbs.redditmedia.com/reqB6_2tFvlBMx5ATJ7pudXAXRmECxNHdJAhqyBibvs.jpg" title="My key takeaways on Qwen3-Next's four pillar innovations, highlighting its Hybrid Attention design" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After reviewing and testing, Qwen3-Next, especially its Hybrid Attention design, might be one of the most significant efficiency breakthroughs in open-source LLMs this year.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;It Outperforms Qwen3-32B with 10% training cost and 10x throughput for long contexts&lt;/strong&gt;. Here's the breakdown:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Four Pillars&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hybrid Architecture:&lt;/strong&gt; Combines Gated DeltaNet + Full Attention to context efficiency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unltra Sparsity:&lt;/strong&gt; 80B parameters, only 3B active per token&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stability Optimizations:&lt;/strong&gt; Zero-Centered RMSNorm + normalized MoE router&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Token Prediction:&lt;/strong&gt; Higher acceptance rates in speculative decoding&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;One thing to note&lt;/strong&gt; is that the model tends toward verbose responses. You'll want to use structured prompting techniques or frameworks for output control.&lt;/p&gt; &lt;p&gt;See &lt;a href="https://blog.netmind.ai/article/We_Tested_Qwen3-Next%3A_Hybrid_Attention_for_Efficiency_Revolution_in_Open-Source_LLMs_(New_Research_Breakdown"&gt;here&lt;/a&gt;) for full technical breakdown with architecture diagrams.Has anyone deployed Qwen3-Next in production? Would love to hear about performance in different use cases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarketingNetMind"&gt; /u/MarketingNetMind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nwzs0k"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwzs0k/my_key_takeaways_on_qwen3nexts_four_pillar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwzs0k/my_key_takeaways_on_qwen3nexts_four_pillar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T13:52:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwukd5</id>
    <title>Granite4 -1M context window, and no one even noticed?</title>
    <updated>2025-10-03T09:38:28+00:00</updated>
    <author>
      <name>/u/Western_Courage_6563</name>
      <uri>https://old.reddit.com/user/Western_Courage_6563</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How is it, when IBM drops a model, no one notice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Western_Courage_6563"&gt; /u/Western_Courage_6563 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwukd5/granite4_1m_context_window_and_no_one_even_noticed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwukd5/granite4_1m_context_window_and_no_one_even_noticed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwukd5/granite4_1m_context_window_and_no_one_even_noticed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T09:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxibik</id>
    <title>Where’s the lip reading ai?</title>
    <updated>2025-10-04T02:31:46+00:00</updated>
    <author>
      <name>/u/Trustingmeerkat</name>
      <uri>https://old.reddit.com/user/Trustingmeerkat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m sure there are some projects out there making real progress on this, but given how quickly tech has advanced in recent years, I’m honestly surprised nothing has surfaced with strong accuracy in converting video to transcript purely through lip reading.&lt;/p&gt; &lt;p&gt;From what I’ve seen, personalized models trained on specific individuals do quite well with front facing footage, but where’s the model that can take any video and give a reasonably accurate idea of what was said? Putting privacy concerns aside for a second, it feels like we should already be 80 percent of the way there. With the amount of spoken video data that already has transcripts, a solid model paired with a standard LLM technique could fill in the blanks with high confidence.&lt;/p&gt; &lt;p&gt;If that doesn’t exist yet, let’s make it, I’m down to even spin it up as a DAO, which is something I’ve wanted to experiment with.&lt;/p&gt; &lt;p&gt;Bonus question: what historical videos would be the most fascinating or valuable to finally understand what was said on camera?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trustingmeerkat"&gt; /u/Trustingmeerkat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxibik/wheres_the_lip_reading_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxibik/wheres_the_lip_reading_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxibik/wheres_the_lip_reading_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T02:31:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxmq9b</id>
    <title>The Missing Link between the Transformer and Models of the Brain</title>
    <updated>2025-10-04T06:35:16+00:00</updated>
    <author>
      <name>/u/ramzeez88</name>
      <uri>https://old.reddit.com/user/ramzeez88</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A group of scientists at Pathway claim to have found a missing link . 'The massively parallel post-Transformer reasoning architecture which opens the door to generalization over time' Link to the paper : &lt;a href="https://arxiv.org/abs/2509.26507"&gt;https://arxiv.org/abs/2509.26507&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ramzeez88"&gt; /u/ramzeez88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxmq9b/the_missing_link_between_the_transformer_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxmq9b/the_missing_link_between_the_transformer_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxmq9b/the_missing_link_between_the_transformer_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T06:35:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxh2n8</id>
    <title>Paper | Apriel-1.5-15B-Thinker: Mid-training is all you need</title>
    <updated>2025-10-04T01:28:45+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(1) &lt;strong&gt;Integrated Multimodal Architecture&lt;/strong&gt;: Beginning with Pixtral-12B [9] as our foundation, we expand it to a model size capable of advanced reasoning across modalities, without requiring pretraining from scratch. &lt;/p&gt; &lt;p&gt;(2) &lt;strong&gt;Staged Multimodal Continual Pretraining (CPT)&lt;/strong&gt;: We adopt a two-phase CPT strategy. The first phase develops foundational text reasoning and broad multimodal capabilities, while the second enhances visual reasoning through synthetic data targeting spatial structure, compositional understanding, and fine-grained perception. This staged progression enables balanced strengthening of both modalities and provides a stable foundation for subsequent training stages, even when later stages emphasize a narrower set of modalities. &lt;/p&gt; &lt;p&gt;(3) &lt;strong&gt;High-Quality Supervised Fine-Tuning (SFT):&lt;/strong&gt; We curate a diverse, high-quality, and high-signal set of samples for supervised fine-tuning. Each response includes explicit reasoning traces, enabling the model to learn transparent thought processes. Coupled with the strong base model, this yields frontier-level performance across a broad range of reasoning benchmarks without requiring additional post-training. &lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2510.01141"&gt;https://arxiv.org/pdf/2510.01141&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxh2n8/paper_apriel1515bthinker_midtraining_is_all_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxh2n8/paper_apriel1515bthinker_midtraining_is_all_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxh2n8/paper_apriel1515bthinker_midtraining_is_all_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T01:28:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwzq6p</id>
    <title>GLM-4.6 now on artificial analysis</title>
    <updated>2025-10-03T13:50:50+00:00</updated>
    <author>
      <name>/u/Professional-Bear857</name>
      <uri>https://old.reddit.com/user/Professional-Bear857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://artificialanalysis.ai/models/glm-4-6-reasoning"&gt;https://artificialanalysis.ai/models/glm-4-6-reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tldr, it benchmarks slightly worse than Qwen 235b 2507. In my use I have found it to also perform worse than the Qwen model, glm 4.5 also didn't benchmark well so it might just be the benchmarks. Although it looks to be slightly better with agent / tool use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Bear857"&gt; /u/Professional-Bear857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwzq6p/glm46_now_on_artificial_analysis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwzq6p/glm46_now_on_artificial_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwzq6p/glm46_now_on_artificial_analysis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T13:50:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwv4q0</id>
    <title>A list of models released or udpated last week on this sub, in case you missed any (3rd Oct)</title>
    <updated>2025-10-03T10:12:59+00:00</updated>
    <author>
      <name>/u/aifeed-fyi</name>
      <uri>https://old.reddit.com/user/aifeed-fyi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We had an interesting week in releases this week (Open &amp;amp; Closed).&lt;/p&gt; &lt;p&gt;Here is the weekly list of models, I found discussed on LocalLlama &lt;em&gt;this week.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Please update or let me know in the comments if there are any mistakes or misses. Good Friday!&lt;/p&gt; &lt;h1&gt;Model Releases &amp;amp; Updates&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Reddit&lt;/th&gt; &lt;th align="left"&gt;HF / GH&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu75l3/glm_46_is_out_and_its_going_against_claude_45/"&gt;GLM-4.6&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;LLM 200k ctx&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu75l3/glm_46_is_out_and_its_going_against_claude_45/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.6"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"&gt;DeepSeek-V3.2-Exp&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;LLM exp/base&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection"&gt;Granite 4.0&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;IBM LLM collection&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-quantized-models-67f944eddd16ff8e057f115c"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nwibsb/ming_v2_is_out/"&gt;Ming V2&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Multimodal collection&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nwibsb/ming_v2_is_out/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/inclusionAI/ming-v2-68ddea4954413c128d706630"&gt;HF Collection&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1nvltym"&gt;LFM2-Audio-1.5&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Audio&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1nvltym"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-Audio-1.5B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.liquid.ai/blog/introducing-liquid-nanos-frontier-grade-performance-on-everyday-devices"&gt;LiquidAI&lt;/a&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuyjp9/liquidai_bet_on_small_but_mighty_model/"&gt; &lt;/a&gt;nanos&lt;/td&gt; &lt;td align="left"&gt;Small task LLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuyjp9/liquidai_bet_on_small_but_mighty_model/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/LiquidAI/liquid-nanos-68b98d898414dd94d4d5f99a"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nt2l57/qwen3_omni_awq_released/"&gt;Qwen3 Omni AWQ&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;30B 4bit AWQ&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nt2l57/qwen3_omni_awq_released/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/cpatonn/Qwen3-Omni-30B-A3B-Instruct-AWQ-4bit"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu44n4/1t_open_source_reasoning_model_with_50b_activation/"&gt;Ring-1T-preview&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1T reasoning 50B Active&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu44n4/1t_open_source_reasoning_model_with_50b_activation/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T-preview"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw8jbn/ring_flash_20_104b_a6b_with_linear_attention"&gt;RingFlash linea r 2&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;LLM 104B MOE&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw8jbn/ring_flash_20_104b_a6b_with_linear_attention"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-flash-linear-2.0"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu1rul/lingmini20_finally_almost_here_lets_push_context/"&gt;Ling-mini-2.0&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;16B LLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu1rul/lingmini20_finally_almost_here_lets_push_context/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-mini-2.0-GGUF"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrvo9g/finally_internvl3_5_flash_versions_coming/"&gt;InternVL3_5 Flash&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Vision-language&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrvo9g/finally_internvl3_5_flash_versions_coming/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-8B-Flash"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrhr13/k2think_32b_reasoning_model_from_uae/"&gt;K2-Think 32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B reasoning&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrhr13/k2think_32b_reasoning_model_from_uae/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LLM360/K2-Think-32B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nv5uw8/dont_sleep_on_apriel1515bthinker_and_snowpiercer/"&gt;Apriel-1.5-15b-Thinker&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;15B multimodal&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nv5uw8/dont_sleep_on_apriel1515bthinker_and_snowpiercer/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-1.5-15b-Thinker"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuxdd4/release_finally_a_working_8bit_quantized/"&gt;VibeVoice 1.8.0 (8-bit)&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;8-bit speech&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuxdd4/release_finally_a_working_8bit_quantized/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/FabioSarracino/VibeVoice-Large-Q8"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;N&lt;a href="https://huggingface.co/neuphonic/neutts-air"&gt;eutts-air&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;TTS model&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw60fj/open_source_speech_foundation_model_that_runs/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/neuphonic/neutts-air"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;🧰 Resources &amp;amp; Tools&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Name&lt;/th&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Reddit&lt;/th&gt; &lt;th align="left"&gt;Link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://v.redd.it/vklzqk9bipsf1"&gt;Onyx&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Open-source Chat UI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://v.redd.it/vklzqk9bipsf1"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;–&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ntiua9/we_just_opensourced_kroko_asr_a_fast_streaming/"&gt;Kroko ASR&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Speech recognition&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ntiua9/we_just_opensourced_kroko_asr_a_fast_streaming/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://kroko.ai"&gt;kroko.ai&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu3slg/an_opensource_omni_chatbot_for_long_speech_and/"&gt;MGM-Omni&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Omni chatbot&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu3slg/an_opensource_omni_chatbot_for_long_speech_and/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/dvlab-research/MGM-Omni"&gt;GitHub&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrrgoy/monkesearch_technical_report_out_now/"&gt;monkeSearch Report&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Research/benchmark&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrrgoy/monkesearch_technical_report_out_now/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://monkesearch.github.io/"&gt;monkesearch.github.io&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aifeed-fyi"&gt; /u/aifeed-fyi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwv4q0/a_list_of_models_released_or_udpated_last_week_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwv4q0/a_list_of_models_released_or_udpated_last_week_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwv4q0/a_list_of_models_released_or_udpated_last_week_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T10:12:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx8igd</id>
    <title>Best LLMs for writing (not coding)</title>
    <updated>2025-10-03T19:21:05+00:00</updated>
    <author>
      <name>/u/FrequentHelp2203</name>
      <uri>https://old.reddit.com/user/FrequentHelp2203</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems most of the LLMs I see are being ranked on coding ability and I understand why I think but for the rest of us, what are some of best LLM for writing. Not writing for you but analysis and critique to better develop your writing such as an essay or story. &lt;/p&gt; &lt;p&gt;Thank you for your time. &lt;/p&gt; &lt;p&gt;Update: thanks for all the help. Appreciate it&lt;/p&gt; &lt;p&gt;Update: I’m writing my own stuff. Essays mostly. I need LLMs that can improve it with discussion and analysis. I write far better than the LLMs I’ve tried so hoping to hear what’s really good out there. Again appreciate your time and tips. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrequentHelp2203"&gt; /u/FrequentHelp2203 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8igd/best_llms_for_writing_not_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8igd/best_llms_for_writing_not_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8igd/best_llms_for_writing_not_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T19:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwxxya</id>
    <title>Bought a used 5090 only to find out it was tampered with</title>
    <updated>2025-10-03T12:36:48+00:00</updated>
    <author>
      <name>/u/a201905</name>
      <uri>https://old.reddit.com/user/a201905</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just a angry/disappointment/frustration post from someone who was very excited at the opportunity to upgrade from 3080 to a 5090 at a discount to run local LLM.&lt;/p&gt; &lt;p&gt;A MSI rtx 5090 came up at my local, trustworthy auction house and I won it for around $2k. It was a stretch on my budget but it was too good of an opportunity so I jumped on it. I was extremely excited and upgraded the PSU but when I tried to put everything together, the system would not boot. I tried everything for hours until I remembered reading the article about people stealing GPU cores. &lt;/p&gt; &lt;p&gt;So I looked at the back and noticed the warranty tamper sticker was voided. i looked back at the auction site and I can see the image they posted with the screw tampered. I was blinded by the potential happiness this was going to bring me and I just didn't pay attention.&lt;/p&gt; &lt;p&gt;What a disappointment. Why do people do this garbage to others. I hope karma bites you in the ass. &lt;/p&gt; &lt;p&gt;Edit: I should have been clearer, i opened it and it's missing the core. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a201905"&gt; /u/a201905 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxxya/bought_a_used_5090_only_to_find_out_it_was/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxxya/bought_a_used_5090_only_to_find_out_it_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxxya/bought_a_used_5090_only_to_find_out_it_was/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T12:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx1ot4</id>
    <title>Qwen3-VL-30B-A3B-Instruct &amp; Thinking (Now Hidden)</title>
    <updated>2025-10-03T15:06:48+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ot4/qwen3vl30ba3binstruct_thinking_now_hidden/"&gt; &lt;img alt="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking (Now Hidden)" src="https://a.thumbs.redditmedia.com/iNETafBex6Qpbyi8P087geXMh_aBmkILehL6E7qn-m4.jpg" title="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking (Now Hidden)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nx1ot4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ot4/qwen3vl30ba3binstruct_thinking_now_hidden/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx1ot4/qwen3vl30ba3binstruct_thinking_now_hidden/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T15:06:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxjh4c</id>
    <title>GitHub - huawei-csl/SINQ: Welcome to the official repository of SINQ! A novel, fast and high-quality quantization method designed to make any Large Language Model smaller while preserving accuracy.</title>
    <updated>2025-10-04T03:30:30+00:00</updated>
    <author>
      <name>/u/Aiochedolor</name>
      <uri>https://old.reddit.com/user/Aiochedolor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjh4c/github_huaweicslsinq_welcome_to_the_official/"&gt; &lt;img alt="GitHub - huawei-csl/SINQ: Welcome to the official repository of SINQ! A novel, fast and high-quality quantization method designed to make any Large Language Model smaller while preserving accuracy." src="https://external-preview.redd.it/yP0CnjxBFJCXTVacHixSvy4H_F7MTnOAVtKcV29Lggk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c413349140863192c8413b0f7b8e7f32ec48822c" title="GitHub - huawei-csl/SINQ: Welcome to the official repository of SINQ! A novel, fast and high-quality quantization method designed to make any Large Language Model smaller while preserving accuracy." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aiochedolor"&gt; /u/Aiochedolor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huawei-csl/SINQ"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjh4c/github_huaweicslsinq_welcome_to_the_official/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjh4c/github_huaweicslsinq_welcome_to_the_official/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T03:30:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx8e2l</id>
    <title>Is this expected behaviour from Granite 4 32B? (Unsloth Q4XL, no system prompt)</title>
    <updated>2025-10-03T19:16:29+00:00</updated>
    <author>
      <name>/u/IonizedRay</name>
      <uri>https://old.reddit.com/user/IonizedRay</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8e2l/is_this_expected_behaviour_from_granite_4_32b/"&gt; &lt;img alt="Is this expected behaviour from Granite 4 32B? (Unsloth Q4XL, no system prompt)" src="https://preview.redd.it/uq9t3il85ysf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86fe0496ab662fb43abd450fc0e2e5a75018e96b" title="Is this expected behaviour from Granite 4 32B? (Unsloth Q4XL, no system prompt)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IonizedRay"&gt; /u/IonizedRay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uq9t3il85ysf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8e2l/is_this_expected_behaviour_from_granite_4_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx8e2l/is_this_expected_behaviour_from_granite_4_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T19:16:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxbbxe</id>
    <title>GLM 4.6 new best open weight overall on lmarena</title>
    <updated>2025-10-03T21:11:39+00:00</updated>
    <author>
      <name>/u/r3m8sh</name>
      <uri>https://old.reddit.com/user/r3m8sh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Third on code after Qwen 235b (lmarena isn't agent based). #3 on hard prompts and #1 on creative writing.&lt;/p&gt; &lt;p&gt;Edit : in thinking mode (default).&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/leaderboard/text/overall"&gt;https://lmarena.ai/leaderboard/text/overall&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/r3m8sh"&gt; /u/r3m8sh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxbbxe/glm_46_new_best_open_weight_overall_on_lmarena/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxbbxe/glm_46_new_best_open_weight_overall_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxbbxe/glm_46_new_best_open_weight_overall_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T21:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxjhnj</id>
    <title>Behold, the jankiest setup ever</title>
    <updated>2025-10-04T03:31:15+00:00</updated>
    <author>
      <name>/u/T-VIRUS999</name>
      <uri>https://old.reddit.com/user/T-VIRUS999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjhnj/behold_the_jankiest_setup_ever/"&gt; &lt;img alt="Behold, the jankiest setup ever" src="https://b.thumbs.redditmedia.com/twOOoKU5XbRq6uFRGfXj_XqIEzieTWVvhWE3zg-T_qA.jpg" title="Behold, the jankiest setup ever" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I plan to get an open test bench, after I get my second P40 in a week or two (which will fit nicely on the other side of that fan) &lt;/p&gt; &lt;p&gt;Performance is as shown, Qwen 3 32B Q4 5.9T/sec&lt;/p&gt; &lt;p&gt;The fan is one of those stupidly powerful Delta electronics server fans that pushes out like 250cfm, so I needed to add a PWM controller to slow it down, and it wouldn't run without that giant capacitor, and it's powered by a Li-ion battery instead of the PSU (for now) &lt;/p&gt; &lt;p&gt;It's not stable at all, the whole system BSODs if a program tries to query the GPU while something else is using it (such as if I try to run GPUZ while LM Studio is running), but if only 1 thing touches the GPU at a time, it works &lt;/p&gt; &lt;p&gt;It has a Ryzen 5 5500GT, 16GB of DDR4, a 1000w PSU, a 512GB SSD, and 1 Nvidia P40 (soon to be 2) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/T-VIRUS999"&gt; /u/T-VIRUS999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nxjhnj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjhnj/behold_the_jankiest_setup_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjhnj/behold_the_jankiest_setup_ever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T03:31:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nx18ax</id>
    <title>GLM 4.6 IS A FUKING AMAZING MODEL AND NOBODY CAN TELL ME OTHERWISE</title>
    <updated>2025-10-03T14:49:34+00:00</updated>
    <author>
      <name>/u/boneMechBoy69420</name>
      <uri>https://old.reddit.com/user/boneMechBoy69420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Especially fuckin artificial analysis and their bullshit ass benchmark &lt;/p&gt; &lt;p&gt;Been using GLM 4.5 it on prod for a month now and I've got nothing but good feedback from the users , it's got way better autonomy than any other proprietary model I've tried (sonnet , gpt 5 and grok code) and it's probably the best ever model for tool call accuracy &lt;/p&gt; &lt;p&gt;One benchmark id recommend yall follow is the berkley function calling benchmark (v4 ig) &lt;a href="https://gorilla.cs.berkeley.edu/leaderboard.html"&gt;bfcl v4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boneMechBoy69420"&gt; /u/boneMechBoy69420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx18ax/glm_46_is_a_fuking_amazing_model_and_nobody_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nx18ax/glm_46_is_a_fuking_amazing_model_and_nobody_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nx18ax/glm_46_is_a_fuking_amazing_model_and_nobody_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T14:49:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxi82t</id>
    <title>Why do private companies release open source models?</title>
    <updated>2025-10-04T02:26:58+00:00</updated>
    <author>
      <name>/u/desudesu15</name>
      <uri>https://old.reddit.com/user/desudesu15</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love open source models. I feel they are an alternative for general knowledge, and since I started in this world, I stopped paying for subscriptions and started running models locally.&lt;/p&gt; &lt;p&gt;However, I don't understand the business model of companies like OpenAI launching an open source model. &lt;/p&gt; &lt;p&gt;How do they make money by launching an open source model? &lt;/p&gt; &lt;p&gt;Isn't it counterproductive to their subscription model?&lt;/p&gt; &lt;p&gt;Thank you, and forgive my ignorance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/desudesu15"&gt; /u/desudesu15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxi82t/why_do_private_companies_release_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxi82t/why_do_private_companies_release_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxi82t/why_do_private_companies_release_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T02:26:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxjzbn</id>
    <title>Distributed Inference over wifi with 8x 3090 egpus performance</title>
    <updated>2025-10-04T03:57:59+00:00</updated>
    <author>
      <name>/u/Only_Situation_4713</name>
      <uri>https://old.reddit.com/user/Only_Situation_4713</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I smoked some really good weed recently and decided it was a good idea to buy more 3090s.&lt;/p&gt; &lt;p&gt;Naturally I didn't want to use a real build with server parts, put 8 3090s in one build on home depot racks? No thanks I'm lazy.&lt;/p&gt; &lt;p&gt;I got 4 3090 egpus from a guy on facebook. He's cool, sold them to me for 650 each with the egpu. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.gigabyte.com/Graphics-Card/GV-N3090IXEB-24GD"&gt;https://www.gigabyte.com/Graphics-Card/GV-N3090IXEB-24GD&lt;/a&gt; &amp;lt;--- these are the EGPUs&lt;/p&gt; &lt;p&gt;Then I got 4 other random 3090s of different brands and put them in 3 spare Pcs I have lying around.&lt;/p&gt; &lt;p&gt;Node #1&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Z390 Prime&lt;/li&gt; &lt;li&gt;9900K&lt;/li&gt; &lt;li&gt;64gb of DDR4&lt;/li&gt; &lt;li&gt;3090 (duh)&lt;/li&gt; &lt;li&gt;850W.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Node #2&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MSI Unify ITX z690&lt;/li&gt; &lt;li&gt;12400K&lt;/li&gt; &lt;li&gt;64gb of DDR5&lt;/li&gt; &lt;li&gt;3090 (duh) &lt;/li&gt; &lt;li&gt;650W&lt;/li&gt; &lt;li&gt;2X 3090 EGPUs attached&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Node #3 (Host)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Z790 Maximus Hero&lt;/li&gt; &lt;li&gt;13700k&lt;/li&gt; &lt;li&gt;64gb of DDR5&lt;/li&gt; &lt;li&gt;1200W PSU&lt;/li&gt; &lt;li&gt;2x 3090s &lt;/li&gt; &lt;li&gt;2x 3090 EGPUs attached&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I ran all of it over VLLM with Ray to distribute the load. It's connected over Wifi, I got a good router so speed is about only 10% slower than ethernet from across the house. For now it's all pipeline parallel until the parts arrive then I'll do a 2 node system with 4 gpu each.&lt;/p&gt; &lt;p&gt;&lt;a href="https://rog.asus.com/us/networking/rog-rapture-gt-axe16000-model/"&gt;https://rog.asus.com/us/networking/rog-rapture-gt-axe16000-model/&lt;/a&gt; &amp;lt;--- my router(s).&lt;/p&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;p&gt;At 128k context limit running GLM 4.5 Air AWQ 8 bit (that's Q8 for you gguf folks)&lt;/p&gt; &lt;p&gt;I get 5500 tokens/s prompt processing and 24 tokens a second for a 50k~ ish token prompt. &lt;/p&gt; &lt;p&gt;It works great over Roo.&lt;/p&gt; &lt;p&gt;Ray has a very annoying overhead cost so just assume that each system has like 1gb less vram. Running all my node in headless helps alot too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Only_Situation_4713"&gt; /u/Only_Situation_4713 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjzbn/distributed_inference_over_wifi_with_8x_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjzbn/distributed_inference_over_wifi_with_8x_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxjzbn/distributed_inference_over_wifi_with_8x_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T03:57:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwx1rx</id>
    <title>The most important AI paper of the decade. No debate</title>
    <updated>2025-10-03T11:55:32+00:00</updated>
    <author>
      <name>/u/PumpkinNarrow6339</name>
      <uri>https://old.reddit.com/user/PumpkinNarrow6339</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"&gt; &lt;img alt="The most important AI paper of the decade. No debate" src="https://preview.redd.it/d2rcvb6nyvsf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0051a67e9886e507e2b0a35679f4d469050fda91" title="The most important AI paper of the decade. No debate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PumpkinNarrow6339"&gt; /u/PumpkinNarrow6339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d2rcvb6nyvsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T11:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxhfcq</id>
    <title>Qwen3-VL-30B-A3B-Instruct &amp; Thinking are here</title>
    <updated>2025-10-04T01:46:34+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"&gt; &lt;img alt="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking are here" src="https://external-preview.redd.it/NiG8S3h1NHl5GO5rtLVQY3YBzVNQNV4xvZR0hFbk_vE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88770649ad1f1c425c3a22e1502363d18f9727dc" title="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking are here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/xwkuqkkt20tf1.png?width=2994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16a4068b96a7c20f55817cc29987345c287c76a7"&gt;https://preview.redd.it/xwkuqkkt20tf1.png?width=2994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16a4068b96a7c20f55817cc29987345c287c76a7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T01:46:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect — Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect — Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I’m Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM – 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
