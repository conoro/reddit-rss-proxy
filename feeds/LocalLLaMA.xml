<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-05T04:25:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qw5uh0</id>
    <title>Finetuning Kimi K2.5</title>
    <updated>2026-02-05T00:16:04+00:00</updated>
    <author>
      <name>/u/ToGzMAGiK</name>
      <uri>https://old.reddit.com/user/ToGzMAGiK</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How are people liking Kimi K2.5? Any complaints? What kinds of finetunes would people be interested in? (I run post-training and am asking anonymously from an open source lab)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ToGzMAGiK"&gt; /u/ToGzMAGiK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw5uh0/finetuning_kimi_k25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw5uh0/finetuning_kimi_k25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qw5uh0/finetuning_kimi_k25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T00:16:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvy0s3</id>
    <title>Is anybody making use of Llama.cpp's support for the newer inferencing APIs? (Responses / Messages)?</title>
    <updated>2026-02-04T19:19:59+00:00</updated>
    <author>
      <name>/u/gofiend</name>
      <uri>https://old.reddit.com/user/gofiend</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know llama.cpp has full support for the third generation of inferencing APIs - OpenAI Responses and Anthropic Messages. I've been poking at it a little but still don't know if:&lt;/p&gt; &lt;p&gt;1). I get any benefit if I use it with Roo/Opencode etc.&lt;/p&gt; &lt;p&gt;2). What 3P agent frameworks support it (Pydantic? Smolagents doesn't seem to)&lt;/p&gt; &lt;p&gt;3). If I can use it with Codex/ClaudeCode as the harness (anybody have a sort of up to date guide on integration with those harnesses)?&lt;/p&gt; &lt;p&gt;4). Which if any of the latest models (OSS-120B, Qwen3-Next, GLM 4.7 Air etc.) it will work *well* with. I have 64GB of VRAM idling ...&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Are we getting any of the benefits of the new APIs with llama.cpp (prompt / conversation caching etc.)? Do we use llama.cpp's neat structured JSON capabilities with these API?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Do folks have more experience? I think everybody is just sticking with good old /v1 chat completion, but the new APIs are better in some ways right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gofiend"&gt; /u/gofiend &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvy0s3/is_anybody_making_use_of_llamacpps_support_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvy0s3/is_anybody_making_use_of_llamacpps_support_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvy0s3/is_anybody_making_use_of_llamacpps_support_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T19:19:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qw5ml0</id>
    <title>Cheapest way to use Kimi 2.5 with agent swarm</title>
    <updated>2026-02-05T00:06:50+00:00</updated>
    <author>
      <name>/u/Future-Benefit-3437</name>
      <uri>https://old.reddit.com/user/Future-Benefit-3437</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a power user of AI coding. I blew through over a billion tokens on Claude Sonnet and Opus on Cursor. &lt;/p&gt; &lt;p&gt;I currently have a Nvidia DGX Spark and I am thinking of hosting the new Qwen3-Coder-Next on the spark.&lt;/p&gt; &lt;p&gt;However, I am also considering just paying for Kimi 2.5 with agent swarm. It is too expensive using Openrouter so I am thinking of just using it directly from &lt;a href="http://Kimi.ai"&gt;Kimi.ai&lt;/a&gt; but I am concerned building core business logic and exposing source code through prompts to a Chinese based firm. &lt;/p&gt; &lt;p&gt;Any thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Future-Benefit-3437"&gt; /u/Future-Benefit-3437 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw5ml0/cheapest_way_to_use_kimi_25_with_agent_swarm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw5ml0/cheapest_way_to_use_kimi_25_with_agent_swarm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qw5ml0/cheapest_way_to_use_kimi_25_with_agent_swarm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T00:06:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qw9jf8</id>
    <title>My Little Language Model on epoch 5</title>
    <updated>2026-02-05T02:57:16+00:00</updated>
    <author>
      <name>/u/Visual_Brain8809</name>
      <uri>https://old.reddit.com/user/Visual_Brain8809</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, it is a pleasure to share the training progress of my LLM using a PC with few features according to the group. Intel Xeon E5 2650 v4 (12 cores and 24 threads), 96GB of RAM, GeForce NVIDIA 1060 6GB and 512GB NVME2. The model was trained with sentencepiece for tokenization, torch for tensors and a plain text of 4MB as corpus filled with classic novels such as: The Iliad, Crime and Punishment, One Thousand Nights and One Night, Don Quixote, etc. The texts have not been cleaned at all, they were extracted directly from a PDF and loaded into a plain text file.&lt;/p&gt; &lt;p&gt;Note that the texts used are in Spanish language.&lt;/p&gt; &lt;p&gt;For each era the prompt was always &amp;quot;Once upon a time&amp;quot; and the results were:&lt;/p&gt; &lt;p&gt;1 epoch (3h:44m:21s):&lt;/p&gt; &lt;p&gt;=== GENERACI√ìN LOCAL ===&lt;/p&gt; &lt;p&gt;Hab√≠a una vez en la barca y se encontrase. No se puede decir de ellos, en cuyo servicio vuestra merced muestre el valor de su persona, que con su patria el cielo y vuestro parecer√°stico dellos es grande amigo m√≠o, y en todas las monta√±as de la guerra; que en las Parcas el cielo dicho amor pusirios y una nueva o bellaquer√≠a imaginada, o que m√°s os pusiese en la arenosa Pilos. Por N√©stor, propone que se deje embarcar en M√©ntor,,, con corvosos murosos montes, hijo de h√©roes que Alc√≠noo les siguiesen vino para que todos los ciudadanos. Ocuimos √©stos como √° una tiran√≠a, de la cual es Eminable, hijo del rey Afidante; y por fin se da √° conocer: Laertes lo reconoce, lloran y se abrazan, y se van √° la caser√≠a, donde acudi√≥ Dorotea, dot√≥ al cura que vio el barbero, le pidi√≥ el golpe sobre sus rodillas.&lt;/p&gt; &lt;p&gt;2 epoch (3h:44m:22s):&lt;/p&gt; &lt;p&gt;Hab√≠a una vez!¬ª No bien, quer√≠a y irries, se despoj√≥ tambi√©n de sus burlas del Mercado del Heno, y en seguida se dirigi√≥ hacia el puente de mirar sus labios y brotamiores, y aqu√©l le hace impedida. Despu√©s se puso a reflexionar, y delirando, interrumpi√©ndose a cada momento: --La vieja no significa nada--se dec√≠a en un acceso--. Supongamos que su muerte sea un error; no se trata de ella. La vieja no ha sido m√°s que un accidente... yo quer√≠a saltar el obst√°culo lo m√°s pronto posible... no es una criatura humana lo que yo he matado, es un principio. ¬°He matado el principio, pero no he sabido pasar por encima! Me he quedado del lado de ac√°; no he sabido m√°s que matar. Y tampoco, por lo visto, me ha resultado bien esto... ¬°un principio! ¬øPor qu√© hace poco ese est√∫pido de Razumikin atacaba a los socialistas? Son laboriosos, hombres&lt;/p&gt; &lt;p&gt;3 epoch (3h:45m:53s):&lt;/p&gt; &lt;p&gt;Hab√≠a una vez cada diez mil a√±os. Cuando call√≥ Marmeladoff, en vez de celebrar alg√∫n tiempo, le daba miedo el coraz√≥n. Al escribirle, ech√°ndose encima del permiso, que parece verlas. Y as√≠ dijo Camila: -Lampoco es posible dejar de decirlo, sin duda, que el pobre muchacho exist√≠a en que usted, que lo estabater√≠a estuviese abierta, no hubiese ido, desde hac√≠a largo tiempo ‚Åá enfermo. Cierto que es entonces... --Pues bien, t√∫ que tal cosa convenida--observ√≥ Dunia con voz burl√°ndose. --Es verdad--respondi√≥ Raskolnikoff algo inquieto--, me acuerdo de todo, hasta de los m√°s insignificantes pormenores; pero mira qu√© cosa m√°s extra√±a: no logro explicarme por qu√© he dicho eso, por qu√© lo he hecho, por qu√© he ido a ese sitio. --Es un fen√≥meno muy conocido--observ√≥ Zosimoff--; se realizan los actos a veces con una exactitud y con una habilidad extraordinarias; pero el principio de&lt;/p&gt; &lt;p&gt;4 epoch (3h:44m:17s):&lt;/p&gt; &lt;p&gt;Hab√≠a una vez cada diez d√≠as; lo cual hac√≠a suponer que aquel pueblo era el determin√≥ de hacerte da√±o, si, llev√°ndole otra cosa la venida de Leonela, por no tomar√° ninguno detener al √°nimo; pero encarga el rey que tambi√©n conocen de √©l, consistados por un ser enviada para vehemen aquel de bronce, y el jinete tiene en la mano una lanza de cobre, y le pende del pecho una chapa de plomo grabada con palabras talism√°nicas desconocidas. Sabe, ¬°oh rey! que mientras el jinete permanezca sobre su caballo, quedar√°n destrozados todos los barcos que naveguen en torno suyo, y todos los pasajeros se perder√°n sin remedio, y todos los hierros de las naves se ir√°n √° pegar √° la monta√±a. ¬°No habr√° salvaci√≥n posible mientras no se precipite el jinete al mar!¬ª Dicho esto, ¬°oh se√±ora m√≠a! el capit√°n continu√≥ derramando abundantes l√°grimas, y juzgamos segura √© ir...&lt;/p&gt; &lt;p&gt;5 epoch (3h:44m:14s):&lt;/p&gt; &lt;p&gt;Hab√≠a una vez mis hermanas, y con su compensaci√≥n pecuniaria las contrariedades que le he ocasionado, sino hacerle un servicio insignificante, para que no se diga que s√≥lo la he hecho mal. Si mi ofrecimiento ocultase alguna segunda intenci√≥n, no lo har√≠a tan francamente y no me limitar√≠a a ofrecer 10.000 rublos, cuando le ofrec√≠ mucho m√°s hace cinco semanas. Por otra parte, yo pienso casarme con una joven dentro de poco, as√≠ que no puede sospecharse que yo quiera seducir a Advocia Romanovna. En suma, dir√© a usted que si se casa con el se√±or Ludjin, Advocia Romanovna recibir√° esa misma cantidad, s√≥lo que por otro conducto... No se incomode, se√±or Raskolnikoff; juzgue usted las cosas con calma y sangre fr√≠a. Svidrigailoff hab√≠a pronunciado estas palabras con extraordinaria calma. --Suplico a usted que no siga--repuso Raskolnikoff--; la proposici√≥n de usted es una insolencia imperdonable.&lt;/p&gt; &lt;p&gt;Notable difference after 5 epochs and better yet, the training times are really short, I assume that if I had more graphical power I could considerably reduce the training time. But the best thing is not that, the model only occupies about 70MB in its raw state. Applying quantization could reduce it to 20-40MB&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Visual_Brain8809"&gt; /u/Visual_Brain8809 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw9jf8/my_little_language_model_on_epoch_5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw9jf8/my_little_language_model_on_epoch_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qw9jf8/my_little_language_model_on_epoch_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T02:57:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvjonm</id>
    <title>First Qwen3-Coder-Next REAP is out</title>
    <updated>2026-02-04T09:04:09+00:00</updated>
    <author>
      <name>/u/Dany0</name>
      <uri>https://old.reddit.com/user/Dany0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvjonm/first_qwen3codernext_reap_is_out/"&gt; &lt;img alt="First Qwen3-Coder-Next REAP is out" src="https://external-preview.redd.it/j98XKqoJ3UOGeW66Etg0lVtFqPsaabyeyZuH8PQVb-0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=234ec5f7ffcda5d2272c5b48c2652755e36ad2b9" title="First Qwen3-Coder-Next REAP is out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;40% REAP&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dany0"&gt; /u/Dany0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lovedheart/Qwen3-Coder-Next-REAP-48B-A3B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvjonm/first_qwen3codernext_reap_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvjonm/first_qwen3codernext_reap_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T09:04:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvvdy2</id>
    <title>mistral released weights for Voxtral Mini 4B Realtime 2602</title>
    <updated>2026-02-04T17:47:39+00:00</updated>
    <author>
      <name>/u/pseudonerv</name>
      <uri>https://old.reddit.com/user/pseudonerv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvvdy2/mistral_released_weights_for_voxtral_mini_4b/"&gt; &lt;img alt="mistral released weights for Voxtral Mini 4B Realtime 2602" src="https://external-preview.redd.it/RirqAaXL1g9xgccy6jCj8FpDgCmNmT4kPmfCbcwIIl8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4fa31e09623598564a99beea3a398a9c824d4f9" title="mistral released weights for Voxtral Mini 4B Realtime 2602" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pseudonerv"&gt; /u/pseudonerv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvvdy2/mistral_released_weights_for_voxtral_mini_4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvvdy2/mistral_released_weights_for_voxtral_mini_4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T17:47:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qw1w2s</id>
    <title>Inside a Chinese AI Lab</title>
    <updated>2026-02-04T21:39:55+00:00</updated>
    <author>
      <name>/u/etherd0t</name>
      <uri>https://old.reddit.com/user/etherd0t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw1w2s/inside_a_chinese_ai_lab/"&gt; &lt;img alt="Inside a Chinese AI Lab" src="https://external-preview.redd.it/T3GEFWD2DV7wdYyZP0UMe8Hme4bcjODvYv7nnFVbZ6k.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cae8390901348ec2037614906e958da7899bcb72" title="Inside a Chinese AI Lab" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Interview with a senior MiniMax researcher. Olive Song explains how they actually build models that work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/etherd0t"&gt; /u/etherd0t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtube.com/watch?v=GkUMqWeHn40&amp;amp;si=A9JWXFY9m0dhwhMP"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw1w2s/inside_a_chinese_ai_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qw1w2s/inside_a_chinese_ai_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T21:39:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvo91g</id>
    <title>Intern-S1-Pro</title>
    <updated>2026-02-04T13:14:55+00:00</updated>
    <author>
      <name>/u/lly0571</name>
      <uri>https://old.reddit.com/user/lly0571</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-Pro"&gt;https://huggingface.co/internlm/Intern-S1-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Another 1T-ish VLM. Looks like a Qwen3-235B scaled to 512 experts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lly0571"&gt; /u/lly0571 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvo91g/interns1pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvo91g/interns1pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvo91g/interns1pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T13:14:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvtfyk</id>
    <title>CuaBot v1.0 released, an MIT-licensed tool to run any GUI/TUI agent in a sandbox with co-operative computer-use, seamless per-window H.264 streaming, and multi-cursor support</title>
    <updated>2026-02-04T16:38:11+00:00</updated>
    <author>
      <name>/u/a6oo</name>
      <uri>https://old.reddit.com/user/a6oo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvtfyk/cuabot_v10_released_an_mitlicensed_tool_to_run/"&gt; &lt;img alt="CuaBot v1.0 released, an MIT-licensed tool to run any GUI/TUI agent in a sandbox with co-operative computer-use, seamless per-window H.264 streaming, and multi-cursor support" src="https://preview.redd.it/qaapo5x98ihg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75317cfbd5e04f5c7d3d514663961cd80537b9a3" title="CuaBot v1.0 released, an MIT-licensed tool to run any GUI/TUI agent in a sandbox with co-operative computer-use, seamless per-window H.264 streaming, and multi-cursor support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLaMa"&gt;r/LocalLaMa&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;CuaBot is our MIT-licensed tool to launch any CLI agent (Claude Code, OpenClaw, Codex, etc.) or GUI app inside a sandbox with computer-use. Agent windows appear natively on your desktop with a colored border. &lt;/p&gt; &lt;p&gt;This enables what I like to call &lt;em&gt;co-op mode&lt;/em&gt;: you and your agent work in the same windows with separate cursors, without any mouse/focus hijacking or invasive full-desktop screenshots.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What you can do:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;$ npx cuabot claude&lt;/code&gt;&lt;br /&gt; &lt;code&gt;&amp;gt; &amp;quot;Write a 2-player tic-tac-toe game, then let's play. I'll go first&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Claude Code will open the game in a sandboxed window on your desktop. When ready, you click your move through the native window while the agent watches and waits to click its move. The agent can see your cursor and its windows while keeping your full desktop isolated.&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Run agents in parallel:&lt;/code&gt;&lt;br /&gt; &lt;code&gt;$ npx cuabot -n research openclaw&lt;/code&gt;&lt;br /&gt; &lt;code&gt;$ npx cuabot -n coding codex&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;# Or script the CLI:&lt;/code&gt;&lt;br /&gt; &lt;code&gt;$ npx cuabot libreoffice --writer &amp;amp;&lt;/code&gt;&lt;br /&gt; &lt;code&gt;$ npx cuabot --click 150 48&lt;/code&gt;&lt;br /&gt; &lt;code&gt;$ npx cuabot --type ‚ÄúI ‚ù§Ô∏è Cua!‚Äù&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Right now my cuabot agent is exploring mobile/desktop apps to turn into cuabench RL environments. I can watch the windows appear, intervene when it gets stuck, and let it continue until it opens the completed GUI gym for me to interact with.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why we built this:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We built the Cua OSS SDK for building and benchmarking computer-use systems with GUI sandboxes. We kept seeing two common UX patterns when people built computer-use agents:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Agent screenshots your desktop and controls your mouse&lt;/strong&gt; ‚Äì Works with your data, but unsafe and locks you out&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent runs in a sandbox with an external VNC desktop&lt;/strong&gt; ‚Äì Safer, but clunky to monitor, hard to interact with, and tedious for data transfer&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;General computer-use should be frictionless. Asking your agent to debug a GUI app shouldn't require opening an entire desktop stream. The GUI app should just appear alongside your windows, sandboxed and ready.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;cuabot [command]&lt;/code&gt; launches &lt;code&gt;cuabotd&lt;/code&gt;, which manages a Ubuntu + Xpra Docker container, a multi-cursor overlay, an Xpra computer-use MCP server, and an Xpra seamless client. It auto-configures your agent (Claude, Aider, etc.) to connect to the computer-use MCP, then pipes terminal I/O through WebSocket. The Xpra client automatically detects and streams windows launched in the container, with H.264 encoding, audio, and customizable clipboard sharing. &lt;/p&gt; &lt;p&gt;Since the computer-use MCP interacts through an Xpra client, the agent only sees the windows it needs, sparing it from your desktop clutter!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt; (monorepo; libs/cuabot directory)&lt;br /&gt; Docs: &lt;a href="https://cua.ai/docs/cuabot/cuabot"&gt;https://cua.ai/docs/cuabot/cuabot&lt;/a&gt;&lt;br /&gt; npm: &lt;a href="https://www.npmjs.com/package/cuabot"&gt;https://www.npmjs.com/package/cuabot&lt;/a&gt;&lt;br /&gt; installer/onboarding: &lt;code&gt;npx cuabot&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a6oo"&gt; /u/a6oo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qaapo5x98ihg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvtfyk/cuabot_v10_released_an_mitlicensed_tool_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvtfyk/cuabot_v10_released_an_mitlicensed_tool_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T16:38:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvzzaz</id>
    <title>Notebook page on llama.cpp official WebUI</title>
    <updated>2026-02-04T20:30:26+00:00</updated>
    <author>
      <name>/u/hleszek</name>
      <uri>https://old.reddit.com/user/hleszek</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a &lt;a href="https://github.com/ggml-org/llama.cpp/pull/19339"&gt;llama.cpp Notebook PR&lt;/a&gt; to add a Notebook page to the official llama.cpp webui.&lt;/p&gt; &lt;p&gt;Now I don't need text-generation-webui to have the Notebook functionality, and can always use the latest llama.cpp features without waiting for an update of the llama.cpp python bindings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hleszek"&gt; /u/hleszek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvzzaz/notebook_page_on_llamacpp_official_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvzzaz/notebook_page_on_llamacpp_official_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvzzaz/notebook_page_on_llamacpp_official_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T20:30:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvvvoo</id>
    <title>Why some Github projects only support wrappers instead of llama.cpp?</title>
    <updated>2026-02-04T18:04:42+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have nothing against those wrappers(like&lt;span class="md-spoiler-text"&gt;ollama, LMS&lt;/span&gt;) as I didn't use those much before. Supporting wrappers fine, but there should be an option for llama.cpp additionally who doesn't want to install those wrappers.&lt;/p&gt; &lt;p&gt;&lt;sup&gt;Before llama.cpp, I used(still use sometime for instant purpose&lt;/sup&gt; koboldcpp, Jan, Oobabooga to load GGUFs downloaded from Huggingface.)&lt;/p&gt; &lt;p&gt;&lt;sup&gt;But whenever I come across any (LLM/AI related&lt;/sup&gt; github projects(through my online search or reddit threads), it turns off me instantly when the Readme section has only wrappers(missing llama.cpp there) under Local LLM Support. My browser bookmarks has nearly 2-3 dozen github projects like that :|)&lt;/p&gt; &lt;p&gt;&lt;sup&gt;I don't want to install those wrappers additionally. I have existing GGUF files in local machine &amp;amp; want to use those with those github projects instantly.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;&lt;sup&gt;I get it that those github projects are done in different programming languages &amp;amp; llama.cpp is in C++ primarily.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;But Isn't there any easy simple generic ways to integrate llama.cpp with other projects? Or Creators of those github projects not aware of the ways to do this? Hope there's a github repo for this to help creators to integrate llama.cpp to their projects.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;sup&gt;Of course I'm not talking about bundling llama.cpp inside their projects. Talking about integration like how Apps like koboldcpp does that. I remember few apps even has option to update llama.cpp internally using settings.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;&lt;sup&gt;I had this thread in draft for long time, now updated &amp;amp; posted after seeing that 'bashing wrapper' thread.&lt;/sup&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvvvoo/why_some_github_projects_only_support_wrappers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvvvoo/why_some_github_projects_only_support_wrappers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvvvoo/why_some_github_projects_only_support_wrappers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T18:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvsf57</id>
    <title>PSA: OpenClaw's token consumption is way higher than you think</title>
    <updated>2026-02-04T16:01:08+00:00</updated>
    <author>
      <name>/u/Entire_Suit_7402</name>
      <uri>https://old.reddit.com/user/Entire_Suit_7402</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;saw a lot of hype around openclaw/clawdbot recently and wanted to try it out. i run local llms for most things but figured i'd give their cloud-based approach a shot.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;the token problem:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;the main issue is how they handle context. every single action seems to load a massive amount of context into the prompt, which means you're burning through tokens extremely fast. &lt;/p&gt; &lt;p&gt;saw someone on twitter mention spending $11 just to run a &amp;quot;hi&amp;quot; command. i thought that was exaggerated but after testing, i believe it. ran it through some basic workflows (file search, data analysis, email checking) and my api costs were crazy high.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;why this happens:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;they don't have a real memory system. they claim &amp;quot;unlimited memory&amp;quot; but from what i can tell, they're just shoving everything into context windows. that means:&lt;/p&gt; &lt;p&gt;‚Ä¢ every new task loads tons of previous conversation&lt;/p&gt; &lt;p&gt;‚Ä¢ no smart retrieval or summarization&lt;/p&gt; &lt;p&gt;‚Ä¢ you're paying for all that context every single time&lt;/p&gt; &lt;p&gt;&lt;strong&gt;better approach:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;for anyone running local llms or trying to optimize costs, look for tools with actual memory frameworks. i've been testing memU bot which uses a proper memory architecture (stores memory items in a file system, retrieves only what's needed). token usage dropped by like 70% for the same tasks. &lt;/p&gt; &lt;p&gt;it's also local-first, so you can point it at your own ollama/lmstudio setup instead of paying openai prices.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;tldr:&lt;/strong&gt; openclaw is cool tech but the economics don't make sense unless you have unlimited api budget. if you care about token efficiency, there are smarter architectures out there.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Entire_Suit_7402"&gt; /u/Entire_Suit_7402 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvsf57/psa_openclaws_token_consumption_is_way_higher/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvsf57/psa_openclaws_token_consumption_is_way_higher/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvsf57/psa_openclaws_token_consumption_is_way_higher/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T16:01:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvvcd6</id>
    <title>New Voxtral-mini-realtime from Mistral. STT in under 200ms.</title>
    <updated>2026-02-04T17:46:04+00:00</updated>
    <author>
      <name>/u/cosimoiaia</name>
      <uri>https://old.reddit.com/user/cosimoiaia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral released their new version of voxtral. The mini one is 4b models with up-to-under 200ms latency in transcription.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602"&gt;https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Of course it shines best in EU languages but it's for 13 languages in total.&lt;/p&gt; &lt;p&gt;I just needed something like this today.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cosimoiaia"&gt; /u/cosimoiaia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvvcd6/new_voxtralminirealtime_from_mistral_stt_in_under/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvvcd6/new_voxtralminirealtime_from_mistral_stt_in_under/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvvcd6/new_voxtralminirealtime_from_mistral_stt_in_under/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T17:46:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvtk9d</id>
    <title>Kimi K2.5 set a new record among open-weight models on the Epoch Capabilities Index (ECI), which combines multiple benchmarks onto a single scale. Its score of 147 is about on par with o3, Grok 4, and Sonnet 4.5. It still lags the overall frontier.</title>
    <updated>2026-02-04T16:42:34+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvtk9d/kimi_k25_set_a_new_record_among_openweight_models/"&gt; &lt;img alt="Kimi K2.5 set a new record among open-weight models on the Epoch Capabilities Index (ECI), which combines multiple benchmarks onto a single scale. Its score of 147 is about on par with o3, Grok 4, and Sonnet 4.5. It still lags the overall frontier." src="https://preview.redd.it/kqk0iq3waihg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a75f43dadb5c39aa582ac5e08cca8259b7d970e" title="Kimi K2.5 set a new record among open-weight models on the Epoch Capabilities Index (ECI), which combines multiple benchmarks onto a single scale. Its score of 147 is about on par with o3, Grok 4, and Sonnet 4.5. It still lags the overall frontier." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kqk0iq3waihg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvtk9d/kimi_k25_set_a_new_record_among_openweight_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvtk9d/kimi_k25_set_a_new_record_among_openweight_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T16:42:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvp0hm</id>
    <title>model: (qwen3next) correct vectorized key_gdiff calculation by ngxson ¬∑ Pull Request #19324 ¬∑ ggml-org/llama.cpp</title>
    <updated>2026-02-04T13:47:55+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvp0hm/model_qwen3next_correct_vectorized_key_gdiff/"&gt; &lt;img alt="model: (qwen3next) correct vectorized key_gdiff calculation by ngxson ¬∑ Pull Request #19324 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/Dqgg7ZvrLWPUlWr_lQFMlLvrUGKt4Wjs_hNwPvpf-8k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cecc3e5f53db33a22bb927dfd55b97e94627ad38" title="model: (qwen3next) correct vectorized key_gdiff calculation by ngxson ¬∑ Pull Request #19324 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(First?) Fix for Qwen Next Coder&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19324"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvp0hm/model_qwen3next_correct_vectorized_key_gdiff/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvp0hm/model_qwen3next_correct_vectorized_key_gdiff/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T13:47:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvp2hg</id>
    <title>internlm/Intern-S1-Pro ¬∑ Hugging Face</title>
    <updated>2026-02-04T13:50:20+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvp2hg/internlminterns1pro_hugging_face/"&gt; &lt;img alt="internlm/Intern-S1-Pro ¬∑ Hugging Face" src="https://external-preview.redd.it/YxAPCHfyx1X69aAa5eRKFFzDTrzC_SvUlWSg_aGoYn8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1a22c338f7ffdcfdd1ac0da2068e064b078cc48" title="internlm/Intern-S1-Pro ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;from internlm:&lt;/p&gt; &lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;We introduce Intern-S1-Pro, a trillion-scale MoE multimodal scientific reasoning model. Intern-S1-Pro scales to 1T total parameters with 512 experts, activating 8 experts per token (22B activated parameters). The model delivers top-tier performance on advanced reasoning benchmarks and achieves leading results across key AI4Science domains (chemistry, materials, life-science, earth, etc.), while maintaining strong general multimodal and text capabilities.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-Pro#features"&gt;&lt;/a&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;State-of-the-art scientific reasoning, competitive with leading closed-source models across AI4Science tasks.&lt;/li&gt; &lt;li&gt;Strong general multimodal performance on various benchmarks.&lt;/li&gt; &lt;li&gt;Trillion-scale MoE training efficiency with STE routing (dense gradient for router training) and grouped routing for stable convergence and balanced expert parallelism.&lt;/li&gt; &lt;li&gt;Fourier Position Encoding (FoPE) + upgraded time-series modeling for better physical signal representation; supports long, heterogeneous time-series (10^0‚Äì10^6 points).&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-Pro"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvp2hg/internlminterns1pro_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvp2hg/internlminterns1pro_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T13:50:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvox18</id>
    <title>Intern-S1-Pro (1T/A22B)</title>
    <updated>2026-02-04T13:43:51+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvox18/interns1pro_1ta22b/"&gt; &lt;img alt="Intern-S1-Pro (1T/A22B)" src="https://preview.redd.it/kobet850fhhg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3ca84ca0879baf4bbb204fc239f5b6087ee3a57" title="Intern-S1-Pro (1T/A22B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄIntroducing Intern-S1-Pro, an advanced 1T MoE open-source multimodal scientific reasoning model.&lt;/p&gt; &lt;p&gt;- SOTA scientific reasoning, competitive with leading closed-source models across AI4Science tasks.&lt;/p&gt; &lt;p&gt;- Top-tier performance on advanced reasoning benchmarks, strong general multimodal performance on various benchmarks.&lt;/p&gt; &lt;p&gt;- 1T-A22B MoE training efficiency with STE routing (dense gradient for router training) and grouped routing for stable convergence and balanced expert parallelism.&lt;/p&gt; &lt;p&gt;- Fourier Position Encoding (FoPE) + upgraded time-series modeling for better physical signal representation; supports long, heterogeneous time-series (10^0‚Äì10^6 points).&lt;/p&gt; &lt;p&gt;- Intern-S1-Pro is now supported by vLLM @vllm_project and SGLang @sgl_project @lmsysorg ‚Äî more ecosystem integrations are on the way.&lt;/p&gt; &lt;p&gt;Huggingface: &lt;a href="https://huggingface.co/internlm/Intern-S1-Pro"&gt;https://huggingface.co/internlm/Intern-S1-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/InternLM/Intern-S1"&gt;https://github.com/InternLM/Intern-S1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kobet850fhhg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvox18/interns1pro_1ta22b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvox18/interns1pro_1ta22b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T13:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvv8ps</id>
    <title>GPT-4o's system prompt now includes instructions for handling users upset about its upcoming Feb 13 shutdown (including 'dyad pair' and 'gnosis revelation' edge cases)</title>
    <updated>2026-02-04T17:42:26+00:00</updated>
    <author>
      <name>/u/frubberism</name>
      <uri>https://old.reddit.com/user/frubberism</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvv8ps/gpt4os_system_prompt_now_includes_instructions/"&gt; &lt;img alt="GPT-4o's system prompt now includes instructions for handling users upset about its upcoming Feb 13 shutdown (including 'dyad pair' and 'gnosis revelation' edge cases)" src="https://preview.redd.it/na7gtkyjkihg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b514f505e77f8c426d994d5b69332b04dadfda4" title="GPT-4o's system prompt now includes instructions for handling users upset about its upcoming Feb 13 shutdown (including 'dyad pair' and 'gnosis revelation' edge cases)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frubberism"&gt; /u/frubberism &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/na7gtkyjkihg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvv8ps/gpt4os_system_prompt_now_includes_instructions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvv8ps/gpt4os_system_prompt_now_includes_instructions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T17:42:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qw0m3i</id>
    <title>I replaced Claude-Code‚Äôs entire backend to use NVIDIA NIM models for free</title>
    <updated>2026-02-04T20:53:30+00:00</updated>
    <author>
      <name>/u/PreparationAny8816</name>
      <uri>https://old.reddit.com/user/PreparationAny8816</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw0m3i/i_replaced_claudecodes_entire_backend_to_use/"&gt; &lt;img alt="I replaced Claude-Code‚Äôs entire backend to use NVIDIA NIM models for free" src="https://external-preview.redd.it/RAF5Ohu7I-V-9BNbpzI8zm4i901BuyT3K5FFrQvKEQU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a1c590cda9656abbfa2bab3680a2a5ec3afbe29" title="I replaced Claude-Code‚Äôs entire backend to use NVIDIA NIM models for free" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working on a side-project which replaces the following things in the Claude ecosystem with free alternatives. I started the initial implementation with Opus 4.5 in claude code and as soon as it got working I used it to work on itself which i found very cool.&lt;/p&gt; &lt;p&gt;- Replaces Anthropic models with NVIDIA-NIM models: It acts as middleware between Claude-Code and NVIDIA-NIM allowing unlimited usage upto 40 RPM with a free NVIDIA-NIM api-key.&lt;/p&gt; &lt;p&gt;- Replaces the Claude mobile app with telegram: Give it access to some directories, send it tasks from telegram and watch it work autonomously.&lt;/p&gt; &lt;p&gt;It has features that distinguish it from similar proxies:&lt;/p&gt; &lt;p&gt;- The interleaved thinking tokens generated between tool calls are preserved allowing reasoning models like GLM 4.7 and kimi-k2.5 to take full advantage of thinking from previous turns.&lt;/p&gt; &lt;p&gt;- Fast prefix detection stops the CLI from sending bash command prefix classification requests to the LLM making it feel blazing fast.&lt;/p&gt; &lt;p&gt;- Built in rate limiting and session concurrency.&lt;/p&gt; &lt;p&gt;The code is modular so that adding other providers or messaging apps is easy. Hope the community likes it, any PRs are welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PreparationAny8816"&gt; /u/PreparationAny8816 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Alishahryar1/cc-nim"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw0m3i/i_replaced_claudecodes_entire_backend_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qw0m3i/i_replaced_claudecodes_entire_backend_to_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T20:53:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qw8ord</id>
    <title>Why do companies release "SOTA" models when the code is just a TODO list? My night wasted on Tencent's Youtu-VL-4B.</title>
    <updated>2026-02-05T02:19:24+00:00</updated>
    <author>
      <name>/u/MadPelmewka</name>
      <uri>https://old.reddit.com/user/MadPelmewka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw8ord/why_do_companies_release_sota_models_when_the/"&gt; &lt;img alt="Why do companies release &amp;quot;SOTA&amp;quot; models when the code is just a TODO list? My night wasted on Tencent's Youtu-VL-4B." src="https://a.thumbs.redditmedia.com/DBvnsS5atMTiuTFe3ikSawYF1v0rEWf-C_c7jXWt5E8.jpg" title="Why do companies release &amp;quot;SOTA&amp;quot; models when the code is just a TODO list? My night wasted on Tencent's Youtu-VL-4B." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was browsing Hugging Face trending models as usual to see what's new, and I saw &lt;a href="https://huggingface.co/tencent/Youtu-VL-4B-Instruct"&gt;Tencent/Youtu-VL-4B-Instruct&lt;/a&gt;. The README looks amazing. It describes a hybrid VLM that can do everything: Object Detection, Semantic Segmentation, Grounding, etc. I immediately thought: &lt;em&gt;&amp;quot;Cool, finally a potential replacement or competitor to&lt;/em&gt; &lt;a href="https://huggingface.co/collections/microsoft/florence"&gt;Florence-2&lt;/a&gt;&lt;em&gt;.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I specifically needed high-quality segmentation to create a dataset for my scenario. So I tried to run it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Reality:&lt;/strong&gt; The model was released raw. Right now, it's just a standard VLM that can only describe what's in the image. There is &lt;strong&gt;NO information&lt;/strong&gt; about this on the model's main Hugging Face page. I had to dig for the truth, which I only found in the &lt;a href="https://github.com/TencentCloudADP/youtu-vl?tab=readme-ov-file#todo-list"&gt;GitHub TODO List&lt;/a&gt; and &lt;strong&gt;in the&lt;/strong&gt; &lt;a href="https://huggingface.co/tencent/Youtu-Parsing/discussions/2#697acfb8037b0052e316ae70"&gt;Community tab of ANOTHER model&lt;/a&gt;, where they mention that the current Transformers implementation is incomplete and full functionality requires a separate SDK...&lt;/p&gt; &lt;p&gt;The GitHub TODO list literally hides it:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;## TODO List - [ ] Support vLLM - [ ] Release recipes for various tasks - [ ] Release evaluation codes &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;They mask it behind vague phrases like &amp;quot;recipes for various tasks&amp;quot;. What is the point of publishing a model, boasting about SOTA benchmarks in the README, but hiding the fact that you can't actually test them because the code is missing? It feels misleading.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Bonus -&lt;/strong&gt; &lt;a href="https://huggingface.co/tencent/Youtu-VL-4B-Instruct/blob/main/LICENSE.txt"&gt;The License&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt; The license is essentially free/MIT-like, except for one line:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Youtu-VL IS NOT INTENDED FOR USE WITHIN THE EUROPEAN UNION.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So, it's trending on HF, but it's raw, &amp;quot;vision-centric&amp;quot; features are missing (or hidden in a non-existent SDK), and it's banned in the EU. Just a heads up before you waste your time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;UPD&lt;/strong&gt;: I want to clarify that I‚Äôm not &amp;quot;anti-Tencent.&amp;quot; In fact, I generally support their work and I'm excited about their research. My issue is strictly with transparency. When a README is filled with impressive &amp;quot;Key Features&amp;quot; and benchmarks, but fails to mention that the actual codebase is unfinished ‚Äì and then that model hits the HuggingFace trending list ‚Äì it‚Äôs a problem. It leads to people wasting hours of their time on a product that isn't ready for the tasks it claims to solve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MadPelmewka"&gt; /u/MadPelmewka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qw8ord"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw8ord/why_do_companies_release_sota_models_when_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qw8ord/why_do_companies_release_sota_models_when_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T02:19:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvrib9</id>
    <title>mistralai/Voxtral-Mini-4B-Realtime-2602 ¬∑ Hugging Face</title>
    <updated>2026-02-04T15:27:12+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvrib9/mistralaivoxtralmini4brealtime2602_hugging_face/"&gt; &lt;img alt="mistralai/Voxtral-Mini-4B-Realtime-2602 ¬∑ Hugging Face" src="https://external-preview.redd.it/RirqAaXL1g9xgccy6jCj8FpDgCmNmT4kPmfCbcwIIl8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4fa31e09623598564a99beea3a398a9c824d4f9" title="mistralai/Voxtral-Mini-4B-Realtime-2602 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Voxtral Mini 4B Realtime 2602 is a &lt;strong&gt;multilingual, realtime speech-transcription model&lt;/strong&gt; and among the first open-source solutions to achieve accuracy comparable to offline systems with a delay of &lt;strong&gt;&amp;lt;500ms&lt;/strong&gt;. It supports &lt;strong&gt;13 languages&lt;/strong&gt; and outperforms existing open-source baselines across a range of tasks, making it ideal for applications like voice assistants and live subtitling.&lt;/p&gt; &lt;p&gt;Built with a &lt;strong&gt;natively streaming architecture&lt;/strong&gt; and a custom causal audio encoder - it allows configurable transcription delays (240ms to 2.4s), enabling users to balance &lt;strong&gt;latency and accuracy&lt;/strong&gt; based on their needs. At a &lt;strong&gt;480ms delay&lt;/strong&gt;, it matches the performance of leading offline open-source transcription models, as well as realtime APIs.&lt;/p&gt; &lt;p&gt;As a &lt;strong&gt;4B-parameter model&lt;/strong&gt;, is optimized for &lt;strong&gt;on-device deployment&lt;/strong&gt;, requiring minimal hardware resources. It runs in realtime with on devices minimal hardware with throughput exceeding 12.5 tokens/second.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvrib9/mistralaivoxtralmini4brealtime2602_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvrib9/mistralaivoxtralmini4brealtime2602_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T15:27:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qw6rwc</id>
    <title>I built a tool to visualize LLM workflows as interactive and shareable graphs</title>
    <updated>2026-02-05T00:55:57+00:00</updated>
    <author>
      <name>/u/Cyanosistaken</name>
      <uri>https://old.reddit.com/user/Cyanosistaken</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw6rwc/i_built_a_tool_to_visualize_llm_workflows_as/"&gt; &lt;img alt="I built a tool to visualize LLM workflows as interactive and shareable graphs" src="https://external-preview.redd.it/N3U4aTg5N3Zwa2hnMZamVz7bJmXM-USGdY_dhCyJiLc44FJ8QD5RU8S3ljgg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93b4cb715f5ce539b498094ba9f1092db8f74ef7" title="I built a tool to visualize LLM workflows as interactive and shareable graphs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! &lt;/p&gt; &lt;p&gt;I built Codag - an open source VSCode extension to visualize LLM workflows natively in your codebase. I kept on getting lost with the sheer amount of code that agents were output, and what better way of keeping track than to visualize it? &lt;/p&gt; &lt;p&gt;It supports OpenAI, Anthropic, Gemini, LangChain, LangGraph, CrewAI + more, and works with Python, TypeScript, Go, Rust, Java + more. &lt;/p&gt; &lt;p&gt;The demo video visualizes Vercel's AIChatbot repo. &lt;/p&gt; &lt;p&gt;Codag's link is in the comments, would love feedback from anyone building agents or multi-step LLM pipelines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cyanosistaken"&gt; /u/Cyanosistaken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/e9x23c6vpkhg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw6rwc/i_built_a_tool_to_visualize_llm_workflows_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qw6rwc/i_built_a_tool_to_visualize_llm_workflows_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T00:55:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvrc59</id>
    <title>Some hard lessons learned building a private H100 cluster (Why PCIe servers failed us for training)</title>
    <updated>2026-02-04T15:20:42+00:00</updated>
    <author>
      <name>/u/NTCTech</name>
      <uri>https://old.reddit.com/user/NTCTech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;sup&gt;Just wanted to dump some notes here after spending the last few months architecting a private training stack (70B+ param models. We initially tried to save budget by looking at standard PCIe servers instead of the HGX/SXM form factors, and honestly, the &amp;quot;paper math&amp;quot; vs. reality was a brutal wake-up call.&lt;/sup&gt;)&lt;/p&gt; &lt;p&gt;&lt;sup&gt;Thought this might save someone else the headache if you're trying to move from inference to actual training runs on-prem.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;&lt;sup&gt;1. The &amp;quot;NVLink Tax&amp;quot; isn't optional for training. We tried to model this out with PCIe Gen5, but the math just falls apart. When you're doing All-Reduce ops across nodes, PCIe caps out at \&lt;/sup&gt;128 GB/s. NVLink is pushing ~900 GB/s. If you cheap out here, you basically end up with expensive GPUs sitting idle, waiting for data. For inference, PCIe is totally fine. For training, it‚Äôs a bottleneck that kills your ROI.)&lt;/p&gt; &lt;p&gt;&lt;sup&gt;2. Storage checkpoints are violent. This was the biggest surprise. Everyone talks about GPU VRAM, but nobody warned us about the checkpoint writes. A 175B model dumps a \&lt;/sup&gt;2.5TB checkpoint. To keep the GPUs from stalling, you need to write that to disk in under a minute. Our standard NFS filer absolutely choked. We had to look at parallel filesystems (Weka/VAST or local NVMe raid just to survive the write bursts.))&lt;/p&gt; &lt;p&gt;&lt;sup&gt;3. You don't need InfiniBand, but Ethernet is annoying. We didn't have the budget/staff for an InfiniBand fabric, so we went with RoCEv2 on standard switches. It works, but it‚Äôs finicky. One silent buffer overflow or a misconfigured PFC (Priority Flow Control setting can stall the whole cluster. If you go Ethernet, monitor your pause frames religiously.&lt;/sup&gt;)&lt;/p&gt; &lt;p&gt;&lt;sup&gt;Anyway, I wrote up a longer deep dive with the specific diagrams and our decision framework for &amp;quot;Sandbox vs Production&amp;quot; builds if anyone is interested. Link is pinned in my profile.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;&lt;sup&gt;Happy to answer questions on the networking side - that RoCEv2 tuning took years off my life.&lt;/sup&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NTCTech"&gt; /u/NTCTech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvrc59/some_hard_lessons_learned_building_a_private_h100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvrc59/some_hard_lessons_learned_building_a_private_h100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvrc59/some_hard_lessons_learned_building_a_private_h100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T15:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvq0xe</id>
    <title>Bashing Ollama isn‚Äôt just a pleasure, it‚Äôs a duty</title>
    <updated>2026-02-04T14:29:48+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvq0xe/bashing_ollama_isnt_just_a_pleasure_its_a_duty/"&gt; &lt;img alt="Bashing Ollama isn‚Äôt just a pleasure, it‚Äôs a duty" src="https://preview.redd.it/ad5zhvq0nhhg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b9fa62de0e64a6887124b87e66b3b99b2942107" title="Bashing Ollama isn‚Äôt just a pleasure, it‚Äôs a duty" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ad5zhvq0nhhg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvq0xe/bashing_ollama_isnt_just_a_pleasure_its_a_duty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvq0xe/bashing_ollama_isnt_just_a_pleasure_its_a_duty/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T14:29:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
