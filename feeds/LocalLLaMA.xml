<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-21T16:25:26+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p2n3f8</id>
    <title>Read long podcasts locally with Whisper + LLM, open sourced</title>
    <updated>2025-11-21T02:45:08+00:00</updated>
    <author>
      <name>/u/tonyc1118</name>
      <uri>https://old.reddit.com/user/tonyc1118</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The high quality podcasts and YT interviews are getting longer and longer these days, like Lex Fridman, Acquired, No Priors... It's often hard to find time to finish them. So I built a website that summarized our favorite podcasts into 3-5 minute insight sheets with key quotes. (Yes I tried other existing products. Their outputs are often vague GPT style and miss real key points)&lt;/p&gt; &lt;p&gt;Recently I added local inference and open sourced our project. So if you have a Mac with an M chip, you can run whisper+LLM on your machine. &lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/tonyc-ship/latios-insights"&gt;https://github.com/tonyc-ship/latios-insights&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It’s still early, and I’d love feedback. Happy to answer setup questions too.&lt;/p&gt; &lt;p&gt;Some future features on my mind:&lt;br /&gt; - setup a public database so that all open source users can share what have been summarized &lt;/p&gt; &lt;p&gt;- build a vector DB so that you can search and ask across what you have read&lt;/p&gt; &lt;p&gt;- stretch but what I really loved: make it a mobile app and run the local inference on your phone&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tonyc1118"&gt; /u/tonyc1118 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2n3f8/read_long_podcasts_locally_with_whisper_llm_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2n3f8/read_long_podcasts_locally_with_whisper_llm_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2n3f8/read_long_podcasts_locally_with_whisper_llm_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T02:45:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1p24d2c</id>
    <title>Olmo3</title>
    <updated>2025-11-20T14:20:04+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"&gt; &lt;img alt="Olmo3" src="https://b.thumbs.redditmedia.com/UJppPEN0RZP8y3BZ6uMmEsmjApLD6fufweSjic6DGkY.jpg" title="Olmo3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ai2 released a series of new olmo 3 weights, including Olmo-3-32B-Think, along with data, code for training and evalution.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/allenai/olmo-3"&gt;https://huggingface.co/collections/allenai/olmo-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i98wyfc8bf2g1.png?width=2220&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d52ee933127ce4b88e2e94330d10f9d58bc1b5bb"&gt;https://preview.redd.it/i98wyfc8bf2g1.png?width=2220&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d52ee933127ce4b88e2e94330d10f9d58bc1b5bb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p24d2c/olmo3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T14:20:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1p3165c</id>
    <title>Best LLM for 4x Nvidia Tesla P40?</title>
    <updated>2025-11-21T15:13:01+00:00</updated>
    <author>
      <name>/u/Valuable_Zucchini180</name>
      <uri>https://old.reddit.com/user/Valuable_Zucchini180</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there, I am looking to host a LLM as a coding assistant for a team of 5-10 people on 4x Tesla P40s.&lt;/p&gt; &lt;p&gt;Does anyone have any suggestions for frameworks (ollama, vLLM, etc), and/or the best model to use?&lt;/p&gt; &lt;p&gt;I don’t have much experience with LLM deployment, so something with an easy setup is preferable. Cheers :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Valuable_Zucchini180"&gt; /u/Valuable_Zucchini180 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3165c/best_llm_for_4x_nvidia_tesla_p40/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p3165c/best_llm_for_4x_nvidia_tesla_p40/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p3165c/best_llm_for_4x_nvidia_tesla_p40/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T15:13:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2utxz</id>
    <title>Dual RTX 5090 FE - Case Suggestion help request</title>
    <updated>2025-11-21T10:06:00+00:00</updated>
    <author>
      <name>/u/Firepin77</name>
      <uri>https://old.reddit.com/user/Firepin77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thank you in advance for any help you can give me!&lt;/p&gt; &lt;p&gt;I am contemplating building an (roleplaying mainly, silly tavern) inference and gaming pc.&lt;br /&gt; i think the following are probably good parts:&lt;/p&gt; &lt;p&gt;- AMD Ryzen 7 9800X3D&lt;br /&gt; - Noctua NH-D15 &lt;a href="http://chromax.black"&gt;chromax.black&lt;/a&gt;&lt;br /&gt; - ASUS ProArt X870E-Creator WiFi&lt;br /&gt; - Seasonic Prime TX TX-1600W&lt;br /&gt; - 2x Nvidia RTX 5090 FE (Founders Edition)&lt;/p&gt; &lt;p&gt;so price of case is not much of a concern, because this would be a longterm powerhouse and could change parts, case is more critical because it would be the &amp;quot;house&amp;quot; where the changing parts &amp;quot;live&amp;quot;.&lt;/p&gt; &lt;p&gt;Bad is probably the 5090 FE because it blows the air from the lower card to the top card in the fe edition.&lt;br /&gt; I could choose other 5090 cards but they seem to be absurdly priced (3000 €? could get a rtx 6000 pro almost for that price?) or big monolithic 3-4 slots gpus like the MSI Suprim rtx 5090.&lt;/p&gt; &lt;p&gt;- Any suggestions for cases that comfortably fit and cool good two rtx 5090 (fe or good alternative aibs?) (good build quality?)&lt;br /&gt; - any real alternative to the rtx 5090 FE to fit two cards in one case? (not rtx 6000 pro just 5090)&lt;/p&gt; &lt;p&gt;- no watercooling only aircooling.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Firepin77"&gt; /u/Firepin77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2utxz/dual_rtx_5090_fe_case_suggestion_help_request/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2utxz/dual_rtx_5090_fe_case_suggestion_help_request/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2utxz/dual_rtx_5090_fe_case_suggestion_help_request/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T10:06:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1p330ip</id>
    <title>[2511.15304] Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models</title>
    <updated>2025-11-21T16:23:39+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2511.15304"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p330ip/251115304_adversarial_poetry_as_a_universal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p330ip/251115304_adversarial_poetry_as_a_universal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T16:23:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2lqi7</id>
    <title>Are any of the M series mac macbooks and mac minis, worth saving up for?</title>
    <updated>2025-11-21T01:41:53+00:00</updated>
    <author>
      <name>/u/No_Strawberry_8719</name>
      <uri>https://old.reddit.com/user/No_Strawberry_8719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like for ai locally and general tasks, are the mac m series worth the hype or are there better ways to run ai locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Strawberry_8719"&gt; /u/No_Strawberry_8719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lqi7/are_any_of_the_m_series_mac_macbooks_and_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lqi7/are_any_of_the_m_series_mac_macbooks_and_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2lqi7/are_any_of_the_m_series_mac_macbooks_and_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T01:41:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p323z7</id>
    <title>New results on multimodal memory systems outperforming long-context ICL on LoCoMo</title>
    <updated>2025-11-21T15:49:28+00:00</updated>
    <author>
      <name>/u/Day1_Perceptron</name>
      <uri>https://old.reddit.com/user/Day1_Perceptron</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p323z7/new_results_on_multimodal_memory_systems/"&gt; &lt;img alt="New results on multimodal memory systems outperforming long-context ICL on LoCoMo" src="https://b.thumbs.redditmedia.com/7jABlkZ_Dn1I2v0FVc2Wjg8xH-m6jTTJ9Ox68yw-NNg.jpg" title="New results on multimodal memory systems outperforming long-context ICL on LoCoMo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’ve been exploring a multimodal memory architecture for personalized AI systems and ran a set of evaluations on the LoCoMo benchmark. The approach supports multimodal ingestion and retrieval (text, images, audio, video) and real-time querying.&lt;/p&gt; &lt;p&gt;In our tests, it consistently outperformed long-context in-context learning baselines, even at 29k tokens.&lt;br /&gt; Happy to share details on the setup, ablations, evaluation protocol, or failure cases if helpful.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w080rjx7vm2g1.png?width=1290&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3e6fa532dfdd78f245adf01b576c31856f2ad6c2"&gt;https://preview.redd.it/w080rjx7vm2g1.png?width=1290&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3e6fa532dfdd78f245adf01b576c31856f2ad6c2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Day1_Perceptron"&gt; /u/Day1_Perceptron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p323z7/new_results_on_multimodal_memory_systems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p323z7/new_results_on_multimodal_memory_systems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p323z7/new_results_on_multimodal_memory_systems/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T15:49:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2z54s</id>
    <title>Maxsun Unveils Intel Arc Pro B60 Dual 48 GB Graphics Cards In Fanless &amp; Liquid-Cooled “Single-Slot” Flavors</title>
    <updated>2025-11-21T13:51:15+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/MaxsunOfficial/status/1975154067040006643"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2z54s/maxsun_unveils_intel_arc_pro_b60_dual_48_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2z54s/maxsun_unveils_intel_arc_pro_b60_dual_48_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T13:51:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2q5e5</id>
    <title>Small language models for agentic use?</title>
    <updated>2025-11-21T05:15:18+00:00</updated>
    <author>
      <name>/u/Swimming-Ratio4879</name>
      <uri>https://old.reddit.com/user/Swimming-Ratio4879</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw some posts about VibeThinker, which is a math model,and I thought why not create a multi-agent system that's literally made of Small language models that has a shared context or can communicate to pass context, like user said 1+1,the router route it to VibeThinker and another model, let's call it AgenticThinker,both models run in parallel based on the router's request,the challenge here would be to keep the answer coherent because the model will be filled with fine-tuning data and for that size it will just start doing calculations because you said &amp;quot;hi&amp;quot; that can be solved because we can use a small model that parallize the context between 2 and generate answer while both are communicating via context or generating the answer,that would lower ram usage (it's not an MoE that must be fully loaded) as it will activate from disk-to-vram based on demand,we can even speed it up by using ram to store the models if the router &amp;quot;thought&amp;quot; that it needs to pick a specific model based on the conversation flow,that would truly allow people with low vram to have multiple models and each activated automatically based on task!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Swimming-Ratio4879"&gt; /u/Swimming-Ratio4879 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2q5e5/small_language_models_for_agentic_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2q5e5/small_language_models_for_agentic_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2q5e5/small_language_models_for_agentic_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T05:15:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1p274rk</id>
    <title>Your local LLM agents can be just as good as closed-source models - I open-sourced Stanford's ACE framework that makes agents learn from mistakes</title>
    <updated>2025-11-20T16:09:43+00:00</updated>
    <author>
      <name>/u/cheetguy</name>
      <uri>https://old.reddit.com/user/cheetguy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I implemented Stanford's &lt;a href="https://arxiv.org/abs/2510.04618"&gt;Agentic Context Engineering paper&lt;/a&gt;. The framework makes agents learn from their own execution feedback through in-context learning instead of fine-tuning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Agent runs task → reflects on what worked/failed → curates strategies into playbook → uses playbook on next run&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Improvement:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Paper shows +17.1pp accuracy improvement vs base LLM (≈+40% relative improvement) on agent benchmarks (DeepSeek-V3.1 non-thinking mode), helping close the gap with closed-source models. All through in-context learning (no fine-tuning needed).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Open-Source Implementation:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Drop into existing agents in ~10 lines of code&lt;/li&gt; &lt;li&gt;Works with local or API models&lt;/li&gt; &lt;li&gt;Real-world test on browser automation agent: &lt;ul&gt; &lt;li&gt;30% → 100% success rate&lt;/li&gt; &lt;li&gt;82% fewer steps&lt;/li&gt; &lt;li&gt;65% decrease in token cost&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;https://github.com/kayba-ai/agentic-context-engine&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Local Model Starter Templates (Ollama, LM Studio, LiteLLM): &lt;a href="https://github.com/kayba-ai/agentic-context-engine/tree/main/examples"&gt;https://github.com/kayba-ai/agentic-context-engine/tree/main/examples&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear if anyone tries this with their local setups! Especially curious how it performs with different models.&lt;/p&gt; &lt;p&gt;I'm currently actively improving this based on feedback - &lt;a href="https://github.com/kayba-ai/agentic-context-engine"&gt;⭐ the repo&lt;/a&gt; so you can stay updated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cheetguy"&gt; /u/cheetguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p274rk/your_local_llm_agents_can_be_just_as_good_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p274rk/your_local_llm_agents_can_be_just_as_good_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p274rk/your_local_llm_agents_can_be_just_as_good_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T16:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1p29jwc</id>
    <title>Leak: Qwen3-15B-A2B-Base</title>
    <updated>2025-11-20T17:40:12+00:00</updated>
    <author>
      <name>/u/TroyDoesAI</name>
      <uri>https://old.reddit.com/user/TroyDoesAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unmolested and Unreleased Base Qwen3 MoE:&lt;br /&gt; &lt;a href="https://huggingface.co/TroyDoesAI/Qwen3-15B-A2B-Base"&gt;https://huggingface.co/TroyDoesAI/Qwen3-15B-A2B-Base&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TroyDoesAI"&gt; /u/TroyDoesAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p29jwc/leak_qwen315ba2bbase/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p29jwc/leak_qwen315ba2bbase/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p29jwc/leak_qwen315ba2bbase/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T17:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2ivv2</id>
    <title>Faster NeuTTS: can generate over 200 seconds of audio in a single second!</title>
    <updated>2025-11-20T23:35:16+00:00</updated>
    <author>
      <name>/u/SplitNice1982</name>
      <uri>https://old.reddit.com/user/SplitNice1982</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I previously open sourced FastMaya which was also really fast but then set sights on NeuTTS-air. NeuTTS is much smaller and supports better voice cloning as well. So, I heavily optimized it using LMdeploy and some custom batching code for the codec to make it really fast.&lt;/p&gt; &lt;h1&gt;Benefits of this repo&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Much faster, not only for batching but for single batch sizes(1.8x realtime for Maya1 vs 7x realtime for NeuTTS-air)&lt;/li&gt; &lt;li&gt;Works with multiple gpus using tensor parallel for even more speedups. &lt;/li&gt; &lt;li&gt;Great for not only generating audiobooks but voice assistants and much more&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am working on supporting the multilingual models as well and adding multi speaker synthesis. Also, streaming support and online inference (for serving to many users) should come as well. Initial results are showing **100ms** latency!&lt;/p&gt; &lt;p&gt;I will also add an upsampler to increase audio quality soon. If you have other requests, I will try my best to fulfill them.&lt;/p&gt; &lt;p&gt;Hope this helps people, thanks! Link: &lt;a href="https://github.com/ysharma3501/FastNeuTTS.git"&gt;https://github.com/ysharma3501/FastNeuTTS.git&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SplitNice1982"&gt; /u/SplitNice1982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ivv2/faster_neutts_can_generate_over_200_seconds_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ivv2/faster_neutts_can_generate_over_200_seconds_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ivv2/faster_neutts_can_generate_over_200_seconds_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T23:35:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2ncrr</id>
    <title>ubergarm/GigaChat3-10B-A1.8B-GGUF ~11GiB Q8_0</title>
    <updated>2025-11-21T02:57:30+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ncrr/ubergarmgigachat310ba18bgguf_11gib_q8_0/"&gt; &lt;img alt="ubergarm/GigaChat3-10B-A1.8B-GGUF ~11GiB Q8_0" src="https://external-preview.redd.it/rsMYYo-PBl_LaTyTfnfLp1CLd1qQqrGp0cpYiQ-K3U0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cbdff364a576d27ab13892b08d6753835c752f2b" title="ubergarm/GigaChat3-10B-A1.8B-GGUF ~11GiB Q8_0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Needs a PR to get running for llama.cpp: * &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17420"&gt;https://github.com/ggml-org/llama.cpp/pull/17420&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Issue open for ik_llama.cpp folks: * &lt;a href="https://github.com/ikawrakow/ik_llama.cpp/issues/994"&gt;https://github.com/ikawrakow/ik_llama.cpp/issues/994&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The chat template is missing a docstring out of the middle that wasn't parsing correctly. So you might be able to bring your own chat template using the instructions on the model card and if someone replies here: * &lt;a href="https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview-bf16/discussions/1"&gt;https://huggingface.co/ai-sage/GigaChat3-702B-A36B-preview-bf16/discussions/1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Though DevQuasar mentioned having a fixed template for the bigger 702B here: * &lt;a href="https://huggingface.co/DevQuasar/ai-sage.GigaChat3-702B-A36B-preview-bf16-GGUF/discussions/1"&gt;https://huggingface.co/DevQuasar/ai-sage.GigaChat3-702B-A36B-preview-bf16-GGUF/discussions/1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/GigaChat3-10B-A1.8B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ncrr/ubergarmgigachat310ba18bgguf_11gib_q8_0/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ncrr/ubergarmgigachat310ba18bgguf_11gib_q8_0/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T02:57:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2y0e5</id>
    <title>Virtual Width Networks</title>
    <updated>2025-11-21T13:01:35+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;We introduce Virtual Width Networks (VWN), a framework that delivers the benefits of wider representations without incurring the quadratic cost of increasing the hidden size. VWN decouples representational width from backbone width, expanding the embedding space while keeping backbone compute nearly constant. In our large‑scale experiment, an 8× expansion accelerates optimization by over 2× for next‑token and 3× for next‑2‑token prediction. The advantage amplifies over training as both the loss gap grows and convergence‑speedup ratio increase, showing that VWN is not only token‑efficient but also increasingly effective with scale. Moreover, we identify an approximately log‑linear scaling relation between virtual width and loss reduction, offering an initial empirical basis and motivation for exploring virtual‑width scaling as a new dimension of large‑model efficiency.&lt;/p&gt; &lt;/blockquote&gt; &lt;ul&gt; &lt;li&gt;Seems like the capacity increase comes from enhancements to residual connection paths. Here's an overview that might be helpful:&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;We reinterpret Virtual Width Networks (VWN) through the lens of connectivity as attention along the depth axis. ...(1) a plain feed-forward stack without residuals corresponds to a sliding window of size 1 (each layer processes only its current input and forgets the previous one); (2) residual connections implement a window of size 2 (current input plus the immediately preceding one); and (3) dense connectivity [ma2023denseformer, huang2017densely, xiao2025muddformer] extends the window size to include all previous layers, allowing each layer to reuse all prior representations. &lt;strong&gt;VWN with Generalized Hyper-Connections (GHC) sits in between&lt;/strong&gt;: it realizes a learned, fixed-cost, linear-attention-like mechanism over depth that scales the accessible depth context.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;With this idea at play, it wouldn't be easy to determine the power of a model. If increased hidden dimension size is the key of intelligent dense models: An MoE model can be low active parameters, high depth (many layers) with an 8x virtual network width and outperform in all ways that we know about. We might need a study that compares baseline dense, vs increased total ffn parameters (MoE), vs increased virtual width. This study uses MoEs as the baseline but it would be nice to see one enhancement at a time so we can better weigh the value in VWN in comparison to increased total ffn parameters (MoE).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2511.11238v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2y0e5/virtual_width_networks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2y0e5/virtual_width_networks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T13:01:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2wpl5</id>
    <title>On the opportunity to add a Blackwell Pro 6000 to a home lab</title>
    <updated>2025-11-21T11:56:56+00:00</updated>
    <author>
      <name>/u/Expensive-Paint-9490</name>
      <uri>https://old.reddit.com/user/Expensive-Paint-9490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just some musing. I was searching on ebay for used RTX A6000, imagining (sweet summer child me) that with Blackwell introduction prices on Ampere had become more reasonable.&lt;/p&gt; &lt;p&gt;It turns out that used A6000 are sold for a price close to the original card price. Brand new, or NOS at this point, price is actually higher than at launch.&lt;/p&gt; &lt;p&gt;At this point I am wondering if the smart thing is, buying a Pro 6000 and selling my 4090. It seems to be a neat 5500 EUR expense, but 90% of which could be recovered three or four years from now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expensive-Paint-9490"&gt; /u/Expensive-Paint-9490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2wpl5/on_the_opportunity_to_add_a_blackwell_pro_6000_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2wpl5/on_the_opportunity_to_add_a_blackwell_pro_6000_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2wpl5/on_the_opportunity_to_add_a_blackwell_pro_6000_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T11:56:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1p24aet</id>
    <title>Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp; tool use</title>
    <updated>2025-11-20T14:16:57+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"&gt; &lt;img alt="Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp;amp; tool use" src="https://b.thumbs.redditmedia.com/xQA2jdAbLxju3aNrYQFjsQ9bmSXaOmQiXK2aPKXj8vw.jpg" title="Ai2 just announced Olmo 3, a leading fully open LM suite built for reasoning, chat, &amp;amp; tool use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Try Olmo 3 in the Ai2 Playground → &lt;a href="https://playground.allenai.org/"&gt;https://playground.allenai.org/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Download: &lt;a href="https://huggingface.co/collections/allenai/olmo-3-68e80f043cc0d3c867e7efc6"&gt;https://huggingface.co/collections/allenai/olmo-3-68e80f043cc0d3c867e7efc6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://allenai.org/blog/olmo3"&gt;https://allenai.org/blog/olmo3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Technical report: &lt;a href="https://allenai.org/papers/olmo3"&gt;https://allenai.org/papers/olmo3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p24aet"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p24aet/ai2_just_announced_olmo_3_a_leading_fully_open_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-20T14:16:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2l36u</id>
    <title>Echo TTS - 44.1kHz, Fast, Fits under 8GB VRAM - SoTA Voice Cloning</title>
    <updated>2025-11-21T01:11:55+00:00</updated>
    <author>
      <name>/u/HelpfulHand3</name>
      <uri>https://old.reddit.com/user/HelpfulHand3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New diffusion based multi-speaker capable TTS model released today by the engineer who made Parakeet (the arch that Dia was based on).&lt;br /&gt; &lt;strong&gt;Voice cloning is available on the&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/jordand/echo-tts-preview"&gt;HF space&lt;/a&gt; but for safety reasons (voice similarity with this model is very high) he has decided for now not to release the speaker encoder. It does come with a large voice bank however.&lt;/p&gt; &lt;p&gt;Supports some tags like (laughs), (coughs), (applause), (singing) etc.&lt;/p&gt; &lt;p&gt;Runs on consumer cards with at least 8GB VRAM.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Echo is a 2.4B DiT that generates Fish Speech S1-DAC latents (and can thus generate 44.1kHz audio; credit to Fish Speech for having trained such a great autoencoder). On an A100, Echo can generate a single 30-second sample of audio in 1.4 seconds (including decoding).&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;License: &lt;strong&gt;CC-BY-NC due to the S1 DAC autoencoder license&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Release Blog Post: &lt;a href="https://jordandarefsky.com/blog/2025/echo/"&gt;https://jordandarefsky.com/blog/2025/echo/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo HF Space: &lt;a href="https://huggingface.co/spaces/jordand/echo-tts-preview"&gt;https://huggingface.co/spaces/jordand/echo-tts-preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weights: &lt;a href="https://huggingface.co/jordand/echo-tts-no-speaker"&gt;https://huggingface.co/jordand/echo-tts-no-speaker&lt;/a&gt; &lt;a href="https://huggingface.co/jordand/fish-s1-dac-min"&gt;https://huggingface.co/jordand/fish-s1-dac-min&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code/Github: Coming soon&lt;/p&gt; &lt;p&gt;I haven't had this much fun playing with a TTS since Higgs. This is easily up there with VibeVoice 7b and Higgs Audio v2 despite being 2.4b.&lt;/p&gt; &lt;p&gt;It can clone voices that no other model has been able to do well for me:&lt;/p&gt; &lt;p&gt;&lt;a href="https://vocaroo.com/19PQroylYsoP"&gt;https://vocaroo.com/19PQroylYsoP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HelpfulHand3"&gt; /u/HelpfulHand3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l36u/echo_tts_441khz_fast_fits_under_8gb_vram_sota/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l36u/echo_tts_441khz_fast_fits_under_8gb_vram_sota/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2l36u/echo_tts_441khz_fast_fits_under_8gb_vram_sota/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T01:11:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2v0fe</id>
    <title>Deep Cogito v2.1, a new open weights 671B MoE model</title>
    <updated>2025-11-21T10:16:47+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v0fe/deep_cogito_v21_a_new_open_weights_671b_moe_model/"&gt; &lt;img alt="Deep Cogito v2.1, a new open weights 671B MoE model" src="https://b.thumbs.redditmedia.com/ZFcSSCHd3EryXfkTwWHW8bx8DkNYSKpulONBXzIgxiQ.jpg" title="Deep Cogito v2.1, a new open weights 671B MoE model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/deepcogito/cogito-v21"&gt;https://huggingface.co/collections/deepcogito/cogito-v21&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wgqv3iva5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b23a040098d2ed9caa81a6a322d02e18d51cc0e"&gt;https://preview.redd.it/wgqv3iva5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7b23a040098d2ed9caa81a6a322d02e18d51cc0e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4rfhao3d5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82dd4fcc80106c78f950f6516116123dad2f1b49"&gt;https://preview.redd.it/4rfhao3d5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=82dd4fcc80106c78f950f6516116123dad2f1b49&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l88vmsue5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=da35111b441df51d43d4d5f04be4fb289b029525"&gt;https://preview.redd.it/l88vmsue5l2g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=da35111b441df51d43d4d5f04be4fb289b029525&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v0fe/deep_cogito_v21_a_new_open_weights_671b_moe_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v0fe/deep_cogito_v21_a_new_open_weights_671b_moe_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v0fe/deep_cogito_v21_a_new_open_weights_671b_moe_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T10:16:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2v5ap</id>
    <title>Epstein Files Document Embeddings (768D, Nomic)</title>
    <updated>2025-11-21T10:25:10+00:00</updated>
    <author>
      <name>/u/qwer1627</name>
      <uri>https://old.reddit.com/user/qwer1627</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Text embeddings generated from the House Oversight Committee's Epstein document release. (768D, Nomic)&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/datasets/svetfm/epstein-files-nov11-25-house-post-ocr-embeddings#source-dataset"&gt;&lt;/a&gt;Source Dataset&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;This dataset is derived from:&lt;/strong&gt; &lt;a href="https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K"&gt;tensonaut/EPSTEIN_FILES_20K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The source dataset contains OCR'd text from the original House Oversight Committee PDF release.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/svetfm/epstein-files-nov11-25-house-post-ocr-embeddings"&gt;https://huggingface.co/datasets/svetfm/epstein-files-nov11-25-house-post-ocr-embeddings&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qwer1627"&gt; /u/qwer1627 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v5ap/epstein_files_document_embeddings_768d_nomic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v5ap/epstein_files_document_embeddings_768d_nomic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2v5ap/epstein_files_document_embeddings_768d_nomic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T10:25:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2padh</id>
    <title>Unsloth just released their Olmo 3 dynamic quants!</title>
    <updated>2025-11-21T04:28:41+00:00</updated>
    <author>
      <name>/u/Aromatic-Distance817</name>
      <uri>https://old.reddit.com/user/Aromatic-Distance817</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2padh/unsloth_just_released_their_olmo_3_dynamic_quants/"&gt; &lt;img alt="Unsloth just released their Olmo 3 dynamic quants!" src="https://external-preview.redd.it/48d9roHWO9vPhqtCoIVGxdhD9jO5DC8s9h8U3EqHoCc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c1dca215120c6d942685f73783d2b00bbdb86e8" title="Unsloth just released their Olmo 3 dynamic quants!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aromatic-Distance817"&gt; /u/Aromatic-Distance817 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/Olmo-3-32B-Think-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2padh/unsloth_just_released_their_olmo_3_dynamic_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2padh/unsloth_just_released_their_olmo_3_dynamic_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T04:28:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2ziil</id>
    <title>Hardcore function calling benchmark in backend coding agent.</title>
    <updated>2025-11-21T14:06:49+00:00</updated>
    <author>
      <name>/u/jhnam88</name>
      <uri>https://old.reddit.com/user/jhnam88</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ziil/hardcore_function_calling_benchmark_in_backend/"&gt; &lt;img alt="Hardcore function calling benchmark in backend coding agent." src="https://b.thumbs.redditmedia.com/YaA_TdwEKDUN4oRX3BCB118KPjtAaCveNFCP3lFYqyQ.jpg" title="Hardcore function calling benchmark in backend coding agent." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;Hardcore Benchmark&lt;/h2&gt; &lt;p&gt;&lt;a href="https://github.com/wrtnlabs/autobe"&gt;AutoBE&lt;/a&gt; is an open-source project that generates backend applications through extensive function calling.&lt;/p&gt; &lt;p&gt;As AutoBE utilizes LLM function calling in every phase instead of plain text writing, including compiler's AST (Abstract Syntax Tree) structures of infinite depths, I think this can be the most extreme function calling benchmark ever.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/prisma/AutoBePrisma.ts"&gt;DB Compiler's AST&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/openapi/AutoBeOpenApi.ts"&gt;API specification's AST&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/wrtnlabs/autobe/blob/main/packages/interface/src/test/AutoBeTest.ts"&gt;Test function's AST&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;code&gt;typescript // Example of AutoBE's AST structure export namespace AutoBeOpenApi { export type IJsonSchema = | IJsonSchema.IConstant | IJsonSchema.IBoolean | IJsonSchema.IInteger | IJsonSchema.INumber | IJsonSchema.IString | IJsonSchema.IArray | IJsonSchema.IObject | IJsonSchema.IReference | IJsonSchema.IOneOf | IJsonSchema.INull; } &lt;/code&gt;&lt;/p&gt; &lt;h2&gt;Limitations&lt;/h2&gt; &lt;p&gt;Of course, as you can see, the number of DB schemas and API operations generated for the same topic varies greatly by each model. When &lt;a href="https://github.com/wrtnlabs/autobe-examples/tree/main/anthropic/claude-sonnet-4.5/shopping"&gt;&lt;code&gt;anthropic/claude-sonnet-4.5&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://github.com/wrtnlabs/autobe-examples/tree/main/openai/gpt-5.1/shopping"&gt;&lt;code&gt;openai/gpt-5.1&lt;/code&gt;&lt;/a&gt; create 630 and 2,000 test functions respectively for the same topic, &lt;a href="https://github.com/wrtnlabs/autobe-examples/tree/main/qwen/qwen3-next-80b-a3b-instruct/shopping"&gt;&lt;code&gt;qwen/qwen3-next-80b-a3b&lt;/code&gt;&lt;/a&gt; creates 360.&lt;/p&gt; &lt;p&gt;Moreover, function calling in AutoBE includes a &lt;a href="https://autobe.dev/docs/concepts/function-calling/#validation-feedback"&gt;validation feedback&lt;/a&gt; process that detects detailed type errors and provides feedback to the AI for recovery, even when the AI makes mistakes and creates arguments of the wrong type.&lt;/p&gt; &lt;p&gt;Simply scoring and ranking based solely on compilation/build success, and evaluating each model's function calling capabilities in depth based only on the success rate of function calling with validation feedback, is still far from sufficient.&lt;/p&gt; &lt;p&gt;Therefore, please understand that the current benchmark is simply uncontrolled and only indicates whether or not each AI model can properly construct extremely complex types, including compiler AST structures, through function calling.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;AutoBE is also still incomplete.&lt;/p&gt; &lt;p&gt;Even if the backend application generated through this guarantees a 100% compilation success rate, it does not guarantee a 100% runtime success rate. This is an open-source project with a long way to go in development and mountains of research still to be done.&lt;/p&gt; &lt;p&gt;However, we hope that this can serve as a reference for anyone planning function calling with extremely complex types like ours, and contribute even a little to the AI ecosystem.&lt;/p&gt; &lt;/blockquote&gt; &lt;h2&gt;Promise&lt;/h2&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o3604u/autobe_achieved_100_compilation_success_of/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1o3604u/autobe_achieved_100_compilation_success_of/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A month ago, we achieved a 100% build success rate for small to medium-sized backend applications with &lt;code&gt;qwen3-next-80b-a3b&lt;/code&gt;, and promised to complete RAG optimization in the future to enable the generation of large-scale backend applications on Local LLMs.&lt;/p&gt; &lt;p&gt;Now this has become possible with various Local LLMs such as Qwen3/DeepSeek/Kimi, in addition to commercial models like GPT and Sonnet. While prompting and RAG optimization may not yet be perfect, as models like GPT-5.1 run wild creating as many as 2,000 test functions, we will resolve this issue the next time we come back.&lt;/p&gt; &lt;p&gt;And since many people were curious about the performance of various Local LLMs besides &lt;code&gt;qwen3-next-80b-a3b&lt;/code&gt;, we promised to consistently release benchmark data for them. While it's unfortunate that the benchmark we released today is inadequate due to lack of controlled variables and can only determine whether function calling with extremely complex types is possible or not, we will improve this as well next time.&lt;/p&gt; &lt;p&gt;We, the two AutoBE developers, will continue to dedicate ourselves to its development, striving to create an environment where you can freely generate backend applications on your local devices without cost burden.&lt;/p&gt; &lt;p&gt;In addition, we are always grateful to the specialists who build and freely distribute open-source AI models.&lt;/p&gt; &lt;h2&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;AutoBE: &lt;a href="https://github.com/wrtnlabs/autobe"&gt;https://github.com/wrtnlabs/autobe&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Benchmark Result: &lt;a href="https://github.com/wrtnlabs/autobe-examples"&gt;https://github.com/wrtnlabs/autobe-examples&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jhnam88"&gt; /u/jhnam88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p2ziil"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ziil/hardcore_function_calling_benchmark_in_backend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2ziil/hardcore_function_calling_benchmark_in_backend/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T14:06:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2wnh0</id>
    <title>Which model to choose for coding with 8GB VRAM (assuming quantised) if I'm happy with slow rates like 1tk/s speed.</title>
    <updated>2025-11-21T11:53:47+00:00</updated>
    <author>
      <name>/u/MakeshiftApe</name>
      <uri>https://old.reddit.com/user/MakeshiftApe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to find the best local model I can use for aid in coding. My specs are: 5950X, 32GB RAM, 8GB RTX3070, so I'm severely limited on VRAM - but I seem to have much lower acceptable speeds than most people, so I'm happy to off-load a lot to the CPU to allow for a larger more capable model. &lt;/p&gt; &lt;p&gt;For me even as low as 1tk/s is plenty fast, I don't need an LLM to respond to me instantly, I can wait a minute for a reply.&lt;/p&gt; &lt;p&gt;So far after researching models that'd work with my GPU I landed on Qwen3-14B and GPT-OSS-20B, with the latter seeming better in my tests. &lt;/p&gt; &lt;p&gt;Both run pretty fast by my standards. Which leaves me wondering if I can push it higher and if so what model I should try? Is there anything better?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Any suggestions?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If it matters at all I'm primarily looking for help with GDScript, Java, C++, and Python. Not sure if there's any variance in programming language-proficiency between models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MakeshiftApe"&gt; /u/MakeshiftApe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2wnh0/which_model_to_choose_for_coding_with_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2wnh0/which_model_to_choose_for_coding_with_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2wnh0/which_model_to_choose_for_coding_with_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T11:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p2w5i6</id>
    <title>HunyuanVideo-1.5: A leading lightweight video generation model</title>
    <updated>2025-11-21T11:25:33+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/tencent/HunyuanVideo-1.5"&gt;https://huggingface.co/tencent/HunyuanVideo-1.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2w5i6/hunyuanvideo15_a_leading_lightweight_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p2w5i6/hunyuanvideo15_a_leading_lightweight_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p2w5i6/hunyuanvideo15_a_leading_lightweight_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-21T11:25:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax — Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I’m Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; — Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; — Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; — LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
</feed>
