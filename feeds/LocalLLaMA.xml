<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-24T15:06:08+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rdgea2</id>
    <title>Minimal repo for running Recursive Language Model experiments + TUI Log viewer</title>
    <updated>2026-02-24T13:35:25+00:00</updated>
    <author>
      <name>/u/AvvYaa</name>
      <uri>https://old.reddit.com/user/AvvYaa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdgea2/minimal_repo_for_running_recursive_language_model/"&gt; &lt;img alt="Minimal repo for running Recursive Language Model experiments + TUI Log viewer" src="https://preview.redd.it/5xsg0xey2glg1.png?width=140&amp;amp;height=140&amp;amp;auto=webp&amp;amp;s=13addf590a75909a4a409cb72ff9646d14fbcc7c" title="Minimal repo for running Recursive Language Model experiments + TUI Log viewer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open-sourcing my minimalist implementation of Recursive Language Models.&lt;/p&gt; &lt;p&gt;RLMs can handle text inputs upto millions of tokens - they do not load the prompt directly into context. They use a python REPL to selectively read context and pass around information through variables.&lt;/p&gt; &lt;p&gt;You can just run &lt;strong&gt;`pip install fast-rlm`&lt;/strong&gt; to install.&lt;/p&gt; &lt;p&gt;- Code generation with LLMs&lt;/p&gt; &lt;p&gt;- Code execution in local sandbox&lt;/p&gt; &lt;p&gt;- KV Cache optimized context management&lt;/p&gt; &lt;p&gt;- Subagent architecture&lt;/p&gt; &lt;p&gt;- Structured log generation: great for post-training&lt;/p&gt; &lt;p&gt;- TUI to look at logs interactively&lt;/p&gt; &lt;p&gt;- Early stopping based on budget, completion tokens, etc&lt;/p&gt; &lt;p&gt;Simple interface. Pass a string of arbitrary length in, get a string out. Works with any OpenAI-compatible endpoint, including ollama models. &lt;/p&gt; &lt;p&gt;Git repo: &lt;a href="https://github.com/avbiswas/fast-rlm"&gt;https://github.com/avbiswas/fast-rlm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs: &lt;a href="https://avbiswas.github.io/fast-rlm/"&gt;https://avbiswas.github.io/fast-rlm/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video explanation about how I implemented it:&lt;br /&gt; &lt;a href="https://youtu.be/nxaVvvrezbY"&gt;https://youtu.be/nxaVvvrezbY&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AvvYaa"&gt; /u/AvvYaa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rdgea2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdgea2/minimal_repo_for_running_recursive_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdgea2/minimal_repo_for_running_recursive_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T13:35:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcs9vr</id>
    <title>Talking to my to-do list</title>
    <updated>2026-02-23T20:05:37+00:00</updated>
    <author>
      <name>/u/llo7d</name>
      <uri>https://old.reddit.com/user/llo7d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcs9vr/talking_to_my_todo_list/"&gt; &lt;img alt="Talking to my to-do list" src="https://external-preview.redd.it/YnFzdm9lejd2YWxnMWY-tuy7HWwE5y0N4mja7xeEwkxeCiovLgSs8XbE5sB8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7139ad7399515809748a8bd26139c3d328ee50f5" title="Talking to my to-do list" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been testing feeding all my to-do list and productivity and having this kinda of desk robot thing as a screen to talk to? all the stuff happens on the pc, the screen is just a display and still for now it is a cloud based ai but I can definitely see this all happening locally in the future &lt;em&gt;(also better for privacy stuff)&lt;/em&gt; man the future is going to be awesome&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/llo7d"&gt; /u/llo7d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xplqhdz7valg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcs9vr/talking_to_my_todo_list/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcs9vr/talking_to_my_todo_list/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T20:05:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd9kpk</id>
    <title>Best practices for running local LLMs for ~70â€“150 developers (agentic coding use case)</title>
    <updated>2026-02-24T07:15:30+00:00</updated>
    <author>
      <name>/u/Resident_Potential97</name>
      <uri>https://old.reddit.com/user/Resident_Potential97</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Iâ€™m planning infrastructure for a software startup where we want to use &lt;strong&gt;local LLMs for agentic coding workflows&lt;/strong&gt; (code generation, refactoring, test writing, debugging, PR reviews, etc.).&lt;/p&gt; &lt;h1&gt;Scale&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Initial users: ~70â€“100 developers&lt;/li&gt; &lt;li&gt;Expected growth: up to ~150 users&lt;/li&gt; &lt;li&gt;Daily usage during working hours (8â€“10 hrs/day)&lt;/li&gt; &lt;li&gt;Concurrent requests likely during peak coding hours&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Use Case&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Agentic coding assistants (multi-step reasoning)&lt;/li&gt; &lt;li&gt;Possibly integrated with IDEs&lt;/li&gt; &lt;li&gt;Context-heavy prompts (repo-level understanding)&lt;/li&gt; &lt;li&gt;Some RAG over internal codebases&lt;/li&gt; &lt;li&gt;Latency should feel usable for developers (not 20â€“30 sec per response)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Current Thinking&lt;/h1&gt; &lt;p&gt;Weâ€™re considering:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Running models locally on multiple &lt;strong&gt;Mac Studios (M2/M3 Ultra)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Or possibly dedicated GPU servers&lt;/li&gt; &lt;li&gt;Maybe a hybrid architecture&lt;/li&gt; &lt;li&gt;Ollama / vLLM / LM Studio style setup&lt;/li&gt; &lt;li&gt;Possibly model routing for different tasks&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Questions&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Is Mac Studioâ€“based infra realistic at this scale?&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;What bottlenecks should I expect? (memory bandwidth? concurrency? thermal throttling?)&lt;/li&gt; &lt;li&gt;How many concurrent users can one machine realistically support?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;What architecture would you recommend?&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Single large GPU node?&lt;/li&gt; &lt;li&gt;Multiple smaller GPU nodes behind a load balancer?&lt;/li&gt; &lt;li&gt;Kubernetes + model replicas?&lt;/li&gt; &lt;li&gt;vLLM with tensor parallelism?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model choices&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;For coding: Qwen, DeepSeek-Coder, Mistral, CodeLlama variants?&lt;/li&gt; &lt;li&gt;Is 32B the sweet spot?&lt;/li&gt; &lt;li&gt;Is 70B realistic for interactive latency?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Concurrency &amp;amp; Throughput&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Whatâ€™s the practical QPS per GPU for: &lt;ul&gt; &lt;li&gt;7B&lt;/li&gt; &lt;li&gt;14B&lt;/li&gt; &lt;li&gt;32B&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;How do you size infra for 100 devs assuming bursty traffic?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Challenges I Might Be Underestimating&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Context window memory pressure?&lt;/li&gt; &lt;li&gt;Prompt length from large repos?&lt;/li&gt; &lt;li&gt;Agent loops causing runaway token usage?&lt;/li&gt; &lt;li&gt;Monitoring and observability?&lt;/li&gt; &lt;li&gt;Model crashes under load?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;When scaling from 70 â†’ 150 users: &lt;ul&gt; &lt;li&gt;Do you scale vertically (bigger GPUs)?&lt;/li&gt; &lt;li&gt;Or horizontally (more nodes)?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Any war stories from running internal LLM infra at company scale?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost vs Cloud Tradeoffs&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;At what scale does local infra become cheaper than API providers?&lt;/li&gt; &lt;li&gt;Any hidden operational costs I should expect?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We want:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Reliable&lt;/li&gt; &lt;li&gt;Low-latency&lt;/li&gt; &lt;li&gt;Predictable performance&lt;/li&gt; &lt;li&gt;Secure (internal code stays on-prem)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would really appreciate insights from anyone running local LLM infra for internal teams.&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Resident_Potential97"&gt; /u/Resident_Potential97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd9kpk/best_practices_for_running_local_llms_for_70150/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd9kpk/best_practices_for_running_local_llms_for_70150/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd9kpk/best_practices_for_running_local_llms_for_70150/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T07:15:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdh5lv</id>
    <title>Lessons learned running Qwen3-VL-8B as a fully local voice assistant on AMD ROCm</title>
    <updated>2026-02-24T14:06:50+00:00</updated>
    <author>
      <name>/u/__InterGen__</name>
      <uri>https://old.reddit.com/user/__InterGen__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building a local voice assistant over the past few weeks and wanted to share some things I learned that might be useful to others here, especially anyone on AMD hardware.&lt;/p&gt; &lt;p&gt;The setup is wake word â†’ fine-tuned Whisper STT â†’ Qwen3-VL-8B for reasoning â†’ Kokoro TTS for voice output. Everything runs on-device, no cloud APIs in the loop.&lt;/p&gt; &lt;h1&gt;Things that surprised me&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Self-quantizing beats downloading pre-made quants.&lt;/strong&gt; Running llama-quantize on F16 yourself gives you the exact quant level you want. I went Q5_K_M and the quality difference from a random GGUF download was noticeable.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Small LLMs follow in-context examples over system prompts.&lt;/strong&gt; This one cost me hours. If your chat history has bad answers, Qwen will mimic them regardless of what your system prompt says. Numbered RULES format in the system prompt works much better than prose for 8B models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Semantic intent matching eliminated 95% of pattern maintenance.&lt;/strong&gt; I went from maintaining hundreds of regex patterns to 3-9 example phrases per intent using sentence-transformers. If anyone is still doing keyword/regex routing, seriously look at semantic matching.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Streaming TTS needs per-chunk processing.&lt;/strong&gt; Any post-hoc text transformation (stripping markdown, normalizing numbers) misses content that's already been spoken. Learned this the hard way.&lt;/p&gt; &lt;h1&gt;AMD/ROCm notes&lt;/h1&gt; &lt;p&gt;Since this sub doesn't see a lot of AMD builds: ROCm 7.2 on Ubuntu 24.04 with the RX 7900 XT has been solid for me. llama.cpp with &lt;code&gt;GGML_HIP=ON&lt;/code&gt; gets 80+ tok/s. CTranslate2 also runs on GPU without issues.&lt;/p&gt; &lt;p&gt;The main gotcha was CMake needing the ROCm clang++ directly (&lt;code&gt;/opt/rocm-7.2.0/llvm/bin/clang++&lt;/code&gt;) â€” the hipcc wrapper doesn't work. Took a while to figure that one out.&lt;/p&gt; &lt;h1&gt;Stack details for anyone interested&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LLM:&lt;/strong&gt; Qwen3-VL-8B (Q5_K_M) via llama.cpp + ROCm&lt;/li&gt; &lt;li&gt;&lt;strong&gt;STT:&lt;/strong&gt; Fine-tuned Whisper base (CTranslate2, 198 training phrases, 94%+ accuracy for Southern US accent)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TTS:&lt;/strong&gt; Kokoro 82M with custom voice blend, gapless streaming&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intent matching:&lt;/strong&gt; sentence-transformers (all-MiniLM-L6-v2)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; Ryzen 9 5900X, RX 7900 XT (20GB VRAM), 64GB DDR4, Ubuntu 24.04&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I put a &lt;a href="https://youtu.be/WsqLyUdl9ac"&gt;3-minute demo&lt;/a&gt; together and the &lt;a href="https://github.com/InterGenJLU/jarvis"&gt;code is on GitHub&lt;/a&gt; if anyone wants to dig into the implementation.&lt;/p&gt; &lt;p&gt;Happy to answer questions about any part of the stack â€” especially ROCm quirks if anyone is considering an AMD build.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__InterGen__"&gt; /u/__InterGen__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdh5lv/lessons_learned_running_qwen3vl8b_as_a_fully/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdh5lv/lessons_learned_running_qwen3vl8b_as_a_fully/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdh5lv/lessons_learned_running_qwen3vl8b_as_a_fully/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T14:06:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdidn9</id>
    <title>Liquid AI releases LFM2-24B-A2B (Largest LFM2 model yet)</title>
    <updated>2026-02-24T14:56:03+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdidn9/liquid_ai_releases_lfm224ba2b_largest_lfm2_model/"&gt; &lt;img alt="Liquid AI releases LFM2-24B-A2B (Largest LFM2 model yet)" src="https://preview.redd.it/skka9wsjhglg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40ebbbdab9c0250f641e228700a2903f0d90cb6e" title="Liquid AI releases LFM2-24B-A2B (Largest LFM2 model yet)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;LFM2-24B-A2B is the latest release in LFM2 model family. &lt;/li&gt; &lt;li&gt;This sparse Mixture of Experts (MoE) model has 24 billion total parameters with 2 billion active per token.&lt;/li&gt; &lt;li&gt;LFM2-24B-A2B is open-weight and available now on Hugging Face.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model - &lt;a href="https://huggingface.co/LiquidAI/LFM2-24B-A2B"&gt;https://huggingface.co/LiquidAI/LFM2-24B-A2B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Official blog post - &lt;a href="https://www.liquid.ai/blog/lfm2-24b-a2b"&gt;https://www.liquid.ai/blog/lfm2-24b-a2b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/skka9wsjhglg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdidn9/liquid_ai_releases_lfm224ba2b_largest_lfm2_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdidn9/liquid_ai_releases_lfm224ba2b_largest_lfm2_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T14:56:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcrb2k</id>
    <title>Hypocrisy?</title>
    <updated>2026-02-23T19:31:17+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrb2k/hypocrisy/"&gt; &lt;img alt="Hypocrisy?" src="https://preview.redd.it/jxutlq8bqalg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59d78bab536255787ed1f0bc277f2a7f6d5aea3b" title="Hypocrisy?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jxutlq8bqalg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrb2k/hypocrisy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcrb2k/hypocrisy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T19:31:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd8nr7</id>
    <title>Andrej Karpathy survived the weekend with the claws</title>
    <updated>2026-02-24T06:21:50+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd8nr7/andrej_karpathy_survived_the_weekend_with_the/"&gt; &lt;img alt="Andrej Karpathy survived the weekend with the claws" src="https://preview.redd.it/zi27d0r9ydlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4005111dda515a576e1b3fee84166b2abc69893d" title="Andrej Karpathy survived the weekend with the claws" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;reference: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zi27d0r9ydlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd8nr7/andrej_karpathy_survived_the_weekend_with_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd8nr7/andrej_karpathy_survived_the_weekend_with_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T06:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdef9x</id>
    <title>Qwen3.5-397B-A17B-UD-TQ1 bench results FW Desktop Strix Halo 128GB</title>
    <updated>2026-02-24T12:02:39+00:00</updated>
    <author>
      <name>/u/dabiggmoe2</name>
      <uri>https://old.reddit.com/user/dabiggmoe2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdef9x/qwen35397ba17budtq1_bench_results_fw_desktop/"&gt; &lt;img alt="Qwen3.5-397B-A17B-UD-TQ1 bench results FW Desktop Strix Halo 128GB" src="https://preview.redd.it/o0xbpnavmflg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da67bbf4e8279cb0b472d61b07eaa8886c76693e" title="Qwen3.5-397B-A17B-UD-TQ1 bench results FW Desktop Strix Halo 128GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just sharing the bench results for unsloth Qwen3.5-397B-A17B-UD-TQ1 on my FW desktop with 128GB VRAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dabiggmoe2"&gt; /u/dabiggmoe2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o0xbpnavmflg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdef9x/qwen35397ba17budtq1_bench_results_fw_desktop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdef9x/qwen35397ba17budtq1_bench_results_fw_desktop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T12:02:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcmlwk</id>
    <title>so is OpenClaw local or not</title>
    <updated>2026-02-23T16:47:01+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcmlwk/so_is_openclaw_local_or_not/"&gt; &lt;img alt="so is OpenClaw local or not" src="https://preview.redd.it/5rolok0mw9lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0bdebee8fd3b3c91999b3592892a73daf47142e" title="so is OpenClaw local or not" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Reading the comments, Iâ€™m guessing you didnâ€™t bother to read this:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;Safety and alignment at Meta Superintelligence.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5rolok0mw9lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcmlwk/so_is_openclaw_local_or_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcmlwk/so_is_openclaw_local_or_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T16:47:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd1tj9</id>
    <title>Exclusive: China's DeepSeek trained AI model on Nvidia's best chip despite US ban, official says</title>
    <updated>2026-02-24T02:05:11+00:00</updated>
    <author>
      <name>/u/blahblahsnahdah</name>
      <uri>https://old.reddit.com/user/blahblahsnahdah</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd1tj9/exclusive_chinas_deepseek_trained_ai_model_on/"&gt; &lt;img alt="Exclusive: China's DeepSeek trained AI model on Nvidia's best chip despite US ban, official says" src="https://external-preview.redd.it/LwC39wQsKjPNUsKdGmLUh6SkmdTxf4euiX9LEkSLsqY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1529094f7ffd1b18c8afe7ddd7efa27261ad7f5" title="Exclusive: China's DeepSeek trained AI model on Nvidia's best chip despite US ban, official says" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blahblahsnahdah"&gt; /u/blahblahsnahdah &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reuters.com/world/china/chinas-deepseek-trained-ai-model-nvidias-best-chip-despite-us-ban-official-says-2026-02-24/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd1tj9/exclusive_chinas_deepseek_trained_ai_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd1tj9/exclusive_chinas_deepseek_trained_ai_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T02:05:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd1lmz</id>
    <title>American vs Chinese AI is a false narrative.</title>
    <updated>2026-02-24T01:57:22+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; The real war (&lt;strong&gt;&lt;em&gt;IF&lt;/em&gt;&lt;/strong&gt; there is one) is between closed source and open source. Don't fall for/propagate the America vs China narrative. That's just tactics to get investors to loosen pursestrings and lawmakers/politicians to acquiesce to demands. &lt;/p&gt; &lt;hr /&gt; &lt;p&gt;There's been an uptick of nationalistic posts (mostly in defense of Chinese AI) on this sub and I think its very important to stop false narratives and reset it to the right framing. &lt;/p&gt; &lt;p&gt;Demonize a foreign enemy as a call for action - it was Russia for the space race, and now China. Except the world has changed immeasurably with globalization and national lines make less and less sense everyday - hell I'd wager most of OpenAI/Anthropic AI research teams are Chinese origin. Propagandizing and controlling media narratives is a time honored tradition for moneyed interests. I hope that the relatively more sophisticated folk in this sub can see past this. Yes it is true that the best open source models right now are almost all Chinese. That is resulting in people loosely using those terms as interchangeable but its a false equivalency and should not be spread. &lt;/p&gt; &lt;p&gt;Chinese labs are open sourcing their stuff &lt;em&gt;for now&lt;/em&gt;. But all of those companies are also for-profit - just like OpenAI and Anthropic. The most likely reason they are open sourcing is to stay relevant in the market and prevent platform seizure a la format wars of previous tech shifts (think Blu Ray). Also, the reality is that they are not only not as good as closed source SOTA. But even if they were at parity, most of the world would not trust them purely because of the fact that there is a strong prejudice against China. Thus, its a marketing and sales funnel channel - not some sort of magnanimity. &lt;/p&gt; &lt;p&gt;When the tides shift, as they always do (remember Llama?), Chinese companies could very well go closed source. In fact, we already saw Alibaba try that with Qwen3-Max. &lt;/p&gt; &lt;p&gt;So its very crucial that &lt;strong&gt;we reframe it to the correct axis - closed vs open source.&lt;/strong&gt; I dont think I need to preach to the choir here but this is the enormously critical battle. And if we lose it, I think its going to be worse than the SaaS/cloud/everything is a subscription hell we are currently in. Correct framing is crucial in keeping focus on the right things and prevents the water muddying tactics political players use to get their way.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd1lmz/american_vs_chinese_ai_is_a_false_narrative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd1lmz/american_vs_chinese_ai_is_a_false_narrative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd1lmz/american_vs_chinese_ai_is_a_false_narrative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T01:57:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcseh1</id>
    <title>Fun fact: Anthropic has never open-sourced any LLMs</title>
    <updated>2026-02-23T20:10:06+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve been working on a little side project comparing tokenizer efficiency across different companiesâ€™ models for multilingual encoding.&lt;/p&gt; &lt;p&gt;Then I saw Anthropicâ€™s announcement today and suddenly realized: thereâ€™s no way to analyze claudeâ€™s tokenizer lmao!&lt;/p&gt; &lt;p&gt;edit: Google once mentioned in a paper that Gemma and Gemini share the same tokenizer. OpenAI has already openâ€‘sourced their tokenizers (and gptâ€‘oss). And donâ€™t even get me started on Llama (Llama 5 pls ðŸ˜­). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcseh1/fun_fact_anthropic_has_never_opensourced_any_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcseh1/fun_fact_anthropic_has_never_opensourced_any_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcseh1/fun_fact_anthropic_has_never_opensourced_any_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T20:10:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdi26s</id>
    <title>Liquid AI releases LFM2-24B-A2B</title>
    <updated>2026-02-24T14:43:33+00:00</updated>
    <author>
      <name>/u/PauLabartaBajo</name>
      <uri>https://old.reddit.com/user/PauLabartaBajo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdi26s/liquid_ai_releases_lfm224ba2b/"&gt; &lt;img alt="Liquid AI releases LFM2-24B-A2B" src="https://preview.redd.it/28drgi3ufglg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=211c668e7fadb69afdf5c7c5c74fa9ee4e0e85d1" title="Liquid AI releases LFM2-24B-A2B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, Liquid AI releases LFM2-24B-A2B, their largest LFM2 model to date&lt;/p&gt; &lt;p&gt;LFM2-24B-A2B is a sparse Mixture-of-Experts (MoE) model with 24 billion total parameters with 2 billion active per token, showing that the LFM2 hybrid architecture scales effectively to larger sizes maintaining quality without inflating per-token compute.&lt;/p&gt; &lt;p&gt;This release expands the LFM2 family from 350M to 24B parameters, demonstrating predictable scaling across nearly two orders of magnitude.&lt;/p&gt; &lt;p&gt;Key highlights:&lt;/p&gt; &lt;p&gt;-&amp;gt; MoE architecture: 40 layers, 64 experts per MoE block with top-4 routing, maintaining the hybrid conv + GQA design -&amp;gt; 2.3B active parameters per forward pass -&amp;gt; Designed to run within 32GB RAM, enabling deployment on high-end consumer laptops and desktops -&amp;gt; Day-zero support for inference through llama.cpp, vLLM, and SGLang -&amp;gt; Multiple GGUF quantizations available&lt;/p&gt; &lt;p&gt;Across benchmarks including GPQA Diamond, MMLU-Pro, IFEval, IFBench, GSM8K, and MATH-500, quality improves log-linearly as we scale from 350M to 24B, confirming that the LFM2 architecture does not plateau at small sizes.&lt;/p&gt; &lt;p&gt;LFM2-24B-A2B is released as an instruct model and is available open-weight on Hugging Face. We designed this model to concentrate capacity in total parameters, not active compute, keeping inference latency and energy consumption aligned with edge and local deployment constraints.&lt;/p&gt; &lt;p&gt;This is the next step in making fast, scalable, efficient AI accessible in the cloud and on-device. &lt;/p&gt; &lt;p&gt;-&amp;gt; Read the blog: &lt;a href="https://www.liquid.ai/blog/lfm2-24b-a2b"&gt;https://www.liquid.ai/blog/lfm2-24b-a2b&lt;/a&gt; -&amp;gt; Download weights: &lt;a href="https://huggingface.co/LiquidAI/LFM2-24B-A2B"&gt;https://huggingface.co/LiquidAI/LFM2-24B-A2B&lt;/a&gt; -&amp;gt; Check out our docs on how to run or fine-tune it locally: docs.liquid.ai -&amp;gt; Try it now: playground.liquid.ai&lt;/p&gt; &lt;p&gt;Run it locally or in the cloud and tell us what you build!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PauLabartaBajo"&gt; /u/PauLabartaBajo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/28drgi3ufglg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdi26s/liquid_ai_releases_lfm224ba2b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdi26s/liquid_ai_releases_lfm224ba2b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T14:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdhj3p</id>
    <title>LiquidAI/LFM2-24B-A2B-GGUF Â· Hugging Face</title>
    <updated>2026-02-24T14:21:40+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdhj3p/liquidailfm224ba2bgguf_hugging_face/"&gt; &lt;img alt="LiquidAI/LFM2-24B-A2B-GGUF Â· Hugging Face" src="https://external-preview.redd.it/s6Y76SrPStf2reaCiuAWV2Zvm47mzj1cZicnei7wdTU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=338a9fb43dd747a69f3fb45b1df2f545348a6b41" title="LiquidAI/LFM2-24B-A2B-GGUF Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LFM2 is a family of hybrid models designed for on-device deployment. LFM2-24B-A2B is the largest model in the family, scaling the architecture to 24 billion parameters while keeping inference efficient.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Best-in-class efficiency&lt;/strong&gt;: A 24B MoE model with only 2B active parameters per token, fitting in 32 GB of RAM for deployment on consumer laptops and desktops.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fast edge inference&lt;/strong&gt;: 112 tok/s decode on AMD CPU, 293 tok/s on H100. Fits in 32B GB of RAM with day-one support llama.cpp, vLLM, and SGLang.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Predictable scaling&lt;/strong&gt;: Quality improves log-linearly from 350M to 24B total parameters, confirming the LFM2 hybrid architecture scales reliably across nearly two orders of magnitude.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;LFM2-24B-A2B is a general-purpose instruct model (without reasoning traces) with the following features:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Property&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-8B-A1B"&gt;&lt;strong&gt;LFM2-8B-A1B&lt;/strong&gt;&lt;/a&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-24B-A2B"&gt;&lt;strong&gt;LFM2-24B-A2B&lt;/strong&gt;&lt;/a&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Total parameters&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;8.3B&lt;/td&gt; &lt;td align="left"&gt;24B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Active parameters&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1.5B&lt;/td&gt; &lt;td align="left"&gt;2.3B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Layers&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;24 (18 conv + 6 attn)&lt;/td&gt; &lt;td align="left"&gt;40 (30 conv + 10 attn)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Context length&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;32,768 tokens&lt;/td&gt; &lt;td align="left"&gt;32,768 tokens&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Vocabulary size&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;65,536&lt;/td&gt; &lt;td align="left"&gt;65,536&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Training precision&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Mixed BF16/FP8&lt;/td&gt; &lt;td align="left"&gt;Mixed BF16/FP8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Training budget&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;12 trillion tokens&lt;/td&gt; &lt;td align="left"&gt;17 trillion tokens&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;LFM Open License v1.0&lt;/td&gt; &lt;td align="left"&gt;LFM Open License v1.0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Supported languages&lt;/strong&gt;: English, Arabic, Chinese, French, German, Japanese, Korean, Spanish, Portuguese&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-24B-A2B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdhj3p/liquidailfm224ba2bgguf_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdhj3p/liquidailfm224ba2bgguf_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T14:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd80gx</id>
    <title>I just saw something amazing</title>
    <updated>2026-02-24T05:49:17+00:00</updated>
    <author>
      <name>/u/ayanami0011</name>
      <uri>https://old.reddit.com/user/ayanami0011</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd80gx/i_just_saw_something_amazing/"&gt; &lt;img alt="I just saw something amazing" src="https://preview.redd.it/rr17jgdksdlg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c7fff37ff972da0293a348d64378188d1acef13" title="I just saw something amazing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.asus.com/displays-desktops/workstations/performance/expertcenter-pro-et900n-g3/"&gt;https://www.asus.com/displays-desktops/workstations/performance/expertcenter-pro-et900n-g3/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.azken.com/Workstations/nvidia-series/Asus-ExpertCenter-Pro-ET900N-G3?utm%5C_source=chatgpt.com"&gt;https://www.azken.com/Workstations/nvidia-series/Asus-ExpertCenter-Pro-ET900N-G3?utm\_source=chatgpt.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayanami0011"&gt; /u/ayanami0011 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rr17jgdksdlg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd80gx/i_just_saw_something_amazing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd80gx/i_just_saw_something_amazing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T05:49:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdhmmp</id>
    <title>prepare your GPUs</title>
    <updated>2026-02-24T14:25:50+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdhmmp/prepare_your_gpus/"&gt; &lt;img alt="prepare your GPUs" src="https://preview.redd.it/lk65supncglg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73bfd517b5c1f32fd574c23fe519c6693de9757e" title="prepare your GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New models are on the way (hidden for now)&lt;/p&gt; &lt;p&gt;(still no tea from Junyang Lin)&lt;/p&gt; &lt;p&gt;How to prepare:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Install a fresh build of llama.cpp&lt;/li&gt; &lt;li&gt;Reserve some disk space&lt;/li&gt; &lt;li&gt;Make a hot beverage&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lk65supncglg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdhmmp/prepare_your_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdhmmp/prepare_your_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T14:25:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd2x61</id>
    <title>People are getting it wrong; Anthropic doesn't care about the distillation, they just want to counter the narrative about Chinese open-source models catching up with closed-source frontier models</title>
    <updated>2026-02-24T02:54:22+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd2x61/people_are_getting_it_wrong_anthropic_doesnt_care/"&gt; &lt;img alt="People are getting it wrong; Anthropic doesn't care about the distillation, they just want to counter the narrative about Chinese open-source models catching up with closed-source frontier models" src="https://preview.redd.it/1ulaheylwclg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7333a0173119c9f64b93f296b5b27a05c6260830" title="People are getting it wrong; Anthropic doesn't care about the distillation, they just want to counter the narrative about Chinese open-source models catching up with closed-source frontier models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why would they care about distillation when they probably have done the same with OpenAI models and the Chinese labs are paying for the tokens? This is just their attempt to explain to investors and the US government that cheap Chinese models will never be as good as their models without distillation or stealing model weights from them. And they need to put more restrictions on China to prevent the technology transfer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1ulaheylwclg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd2x61/people_are_getting_it_wrong_anthropic_doesnt_care/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd2x61/people_are_getting_it_wrong_anthropic_doesnt_care/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T02:54:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdg3dv</id>
    <title>Qwen 3.5 new models released on their website!</title>
    <updated>2026-02-24T13:22:03+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdg3dv/qwen_35_new_models_released_on_their_website/"&gt; &lt;img alt="Qwen 3.5 new models released on their website!" src="https://preview.redd.it/xg1r9pzb1glg1.png?width=140&amp;amp;height=89&amp;amp;auto=webp&amp;amp;s=a5adfa472aa1954b976aa402b8ca18b29bcae0fd" title="Qwen 3.5 new models released on their website!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://chat.qwen.ai/"&gt;https://chat.qwen.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xg1r9pzb1glg1.png?width=1495&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ba3206f026aa0a41e0f53228ccba0de35a77861"&gt;https://preview.redd.it/xg1r9pzb1glg1.png?width=1495&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8ba3206f026aa0a41e0f53228ccba0de35a77861&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdg3dv/qwen_35_new_models_released_on_their_website/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdg3dv/qwen_35_new_models_released_on_their_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdg3dv/qwen_35_new_models_released_on_their_website/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T13:22:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1rd8cfw</id>
    <title>Anthropic's recent distillation blog should make anyone only ever want to use local open-weight models; it's scary and dystopian</title>
    <updated>2026-02-24T06:07:02+00:00</updated>
    <author>
      <name>/u/obvithrowaway34434</name>
      <uri>https://old.reddit.com/user/obvithrowaway34434</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd8cfw/anthropics_recent_distillation_blog_should_make/"&gt; &lt;img alt="Anthropic's recent distillation blog should make anyone only ever want to use local open-weight models; it's scary and dystopian" src="https://preview.redd.it/086f3wnavdlg1.png?width=140&amp;amp;height=66&amp;amp;auto=webp&amp;amp;s=3125bb81f69aa57e4305e4471c6284c4a9a52a12" title="Anthropic's recent distillation blog should make anyone only ever want to use local open-weight models; it's scary and dystopian" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's quite ironic that they went for the censorship and authoritarian angles here.&lt;/p&gt; &lt;p&gt;Full blog: &lt;a href="https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks"&gt;https://www.anthropic.com/news/detecting-and-preventing-distillation-attacks&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/obvithrowaway34434"&gt; /u/obvithrowaway34434 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rd8cfw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rd8cfw/anthropics_recent_distillation_blog_should_make/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rd8cfw/anthropics_recent_distillation_blog_should_make/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T06:07:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcpmwn</id>
    <title>Anthropic: "Weâ€™ve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax." ðŸš¨</title>
    <updated>2026-02-23T18:32:45+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"&gt; &lt;img alt="Anthropic: &amp;quot;Weâ€™ve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax.&amp;quot; ðŸš¨" src="https://preview.redd.it/94fbimavfalg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2ad159232448ffd7033d6be4fa96582b674e461" title="Anthropic: &amp;quot;Weâ€™ve identified industrial-scale distillation attacks on our models by DeepSeek, Moonshot AI, and MiniMax.&amp;quot; ðŸš¨" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94fbimavfalg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcpmwn/anthropic_weve_identified_industrialscale/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T18:32:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcvimv</id>
    <title>Distillation when you do it. Training when we do it.</title>
    <updated>2026-02-23T22:04:41+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcvimv/distillation_when_you_do_it_training_when_we_do_it/"&gt; &lt;img alt="Distillation when you do it. Training when we do it." src="https://preview.redd.it/9rc0jqbohblg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05481c4cef786a02ca1e5d0b968e61114727348f" title="Distillation when you do it. Training when we do it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9rc0jqbohblg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcvimv/distillation_when_you_do_it_training_when_we_do_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcvimv/distillation_when_you_do_it_training_when_we_do_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T22:04:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdfhfx</id>
    <title>New Qwen3.5 models spotted on qwen chat</title>
    <updated>2026-02-24T12:55:10+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdfhfx/new_qwen35_models_spotted_on_qwen_chat/"&gt; &lt;img alt="New Qwen3.5 models spotted on qwen chat" src="https://preview.redd.it/h1c3uk0iwflg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b026f6069f044a6b506e0aae9a0c418d76865997" title="New Qwen3.5 models spotted on qwen chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h1c3uk0iwflg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdfhfx/new_qwen35_models_spotted_on_qwen_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdfhfx/new_qwen35_models_spotted_on_qwen_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T12:55:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdf4ai</id>
    <title>Claude Sonnet-4.6 thinks he is DeepSeek-V3 when prompted in Chinese.</title>
    <updated>2026-02-24T12:37:51+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdf4ai/claude_sonnet46_thinks_he_is_deepseekv3_when/"&gt; &lt;img alt="Claude Sonnet-4.6 thinks he is DeepSeek-V3 when prompted in Chinese." src="https://preview.redd.it/bq6li0e4rflg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64e15744ff15022c24490512cd96a21eeb16b391" title="Claude Sonnet-4.6 thinks he is DeepSeek-V3 when prompted in Chinese." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Teortaxes on ð•: &lt;a href="https://x.com/teortaxesTex/status/2026130112685416881"&gt;https://x.com/teortaxesTex/status/2026130112685416881&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bq6li0e4rflg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdf4ai/claude_sonnet46_thinks_he_is_deepseekv3_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdf4ai/claude_sonnet46_thinks_he_is_deepseekv3_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-24T12:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
