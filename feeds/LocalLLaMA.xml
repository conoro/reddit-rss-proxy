<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-22T08:57:26+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1psp17x</id>
    <title>RAG Paper 25.12.18</title>
    <updated>2025-12-22T03:16:43+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.16822v1"&gt;MEPIC: Memory Efficient Position Independent Caching for LLM Serving&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.16802v1"&gt;Exploration of Augmentation Strategies in Multi-modal Retrieval-Augmented Generation for the Biomedical Domain: A Case Study Evaluating Question Answering in Glycobiology&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.16795v1"&gt;From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.16676v1"&gt;DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.16425v1"&gt;Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration System Taking a Neuro-Symbolic Approach&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.16391v1"&gt;Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.16236v1"&gt;The Evolution of Reranking Models in Information Retrieval: From Heuristic Methods to Large Language Models&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/"&gt;&lt;strong&gt;RagView.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/&lt;/strong&gt; &lt;a href="https://github.com/RagView/RagView"&gt;&lt;strong&gt;github/RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psp17x/rag_paper_251218/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psp17x/rag_paper_251218/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psp17x/rag_paper_251218/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T03:16:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pstdnb</id>
    <title>Best General-Purpose Model - 12gb vram, 128ram</title>
    <updated>2025-12-22T07:17:22+00:00</updated>
    <author>
      <name>/u/rainegarden</name>
      <uri>https://old.reddit.com/user/rainegarden</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;^ I want a general purpose model - though sometimes being okay at coding might be useful for debugging stuff that can fit within the range of my system. It's old server parts I basically got for free, so that's why it's a titan xp and has 128gb of ddr4 ecc. Can someone point me in the right direction? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rainegarden"&gt; /u/rainegarden &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstdnb/best_generalpurpose_model_12gb_vram_128ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstdnb/best_generalpurpose_model_12gb_vram_128ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pstdnb/best_generalpurpose_model_12gb_vram_128ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T07:17:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1psd61v</id>
    <title>A list of 28 modern benchmarks and their short description</title>
    <updated>2025-12-21T18:20:13+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I realised that my understanding of the benchmarks was stuck somewhere around GSM8k/SimpleQA area - very dated by now.&lt;/p&gt; &lt;p&gt;So I went through some of the recent releases and compiled a list of the used benchmarks and what they represent. Some of these are very obvious (ARC-AGI, AIME, etc.) but for many - I was seeing them for the first time, so I hope it'll be useful for someone else too.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Benchmark&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://artificialanalysis.ai/evaluations/aime-2025"&gt;&lt;strong&gt;AIME 2025&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Tests olympiad-level mathematical reasoning using all 30 problems from the 2025 American Invitational Mathematics Examination with integer answers from 000-999&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arcprize.org/arc-agi/1/"&gt;&lt;strong&gt;ARC-AGI-1 (Verified)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Measures basic fluid intelligence through visual reasoning puzzles that are easy for humans but challenging for AI systems&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arcprize.org/arc-agi/2/"&gt;&lt;strong&gt;ARC-AGI-2&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;An updated benchmark designed to stress test the efficiency and capability of state-of-the-art AI reasoning systems with visual pattern recognition tasks&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://charxiv.github.io/"&gt;&lt;strong&gt;CharXiv Reasoning&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Evaluates information synthesis from complex charts through descriptive and reasoning questions that require analyzing visual elements&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/html/2501.01257v1"&gt;&lt;strong&gt;Codeforces&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;A competition-level coding benchmark that evaluates LLM programming capabilities using problems from the CodeForces platform with standardized ELO ratings&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/"&gt;&lt;strong&gt;FACTS Benchmark Suite&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Systematically evaluates Large Language Model factuality across parametric, search, and multimodal reasoning domains&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://epoch.ai/frontiermath"&gt;&lt;strong&gt;FrontierMath (Tier 1-3)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Tests undergraduate through early graduate level mathematics problems that take specialists hours to days to solve&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://epoch.ai/frontiermath"&gt;&lt;strong&gt;FrontierMath (Tier 4)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Evaluates research-level mathematics capabilities with exceptionally challenging problems across major branches of modern mathematics&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://openai.com/index/gdpval/"&gt;&lt;strong&gt;GDPval&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Measures AI model performance on real-world economically valuable tasks across 44 occupations from the top 9 industries contributing to U.S. GDP&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2510.24081"&gt;&lt;strong&gt;Global PIQA&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Evaluates physical commonsense reasoning across over 100 languages with culturally-specific examples created by native speakers&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2311.12022"&gt;&lt;strong&gt;GPQA Diamond&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Tests graduate-level scientific knowledge through multiple-choice questions that domain experts can answer but non-experts typically cannot&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://llm-stats.com/benchmarks/hmmt-2025"&gt;&lt;strong&gt;HMMT 2025&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Assesses mathematical reasoning using problems from the Harvard-MIT Mathematics Tournament, a prestigious high school mathematics competition&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://agi.safe.ai/"&gt;&lt;strong&gt;Humanity's Last Exam&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;A multi-modal benchmark designed to test expert-level performance on closed-ended, verifiable questions across dozens of academic subjects&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://livecodebenchpro.com/"&gt;&lt;strong&gt;LiveCodeBench Pro&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Evaluates LLM code generation capabilities on competitive programming problems of varying difficulty levels from different platforms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://scale.com/leaderboard/mcp_atlas"&gt;&lt;strong&gt;MCP Atlas&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Measures how well language models handle real-world tool use through multi-step workflows using the Model Context Protocol&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/datasets/openai/MMMLU/blob/main/README.md"&gt;&lt;strong&gt;MMMLU&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;A multilingual version of MMLU featuring professionally translated questions across 14 languages to test massive multitask language understanding&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2409.02813"&gt;&lt;strong&gt;MMMU-Pro&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;A more robust multimodal benchmark that filters text-only answerable questions and augments options to test true multimodal understanding&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://llm-stats.com/benchmarks/mrcr-v2-(8-needle"&gt;&lt;strong&gt;MRCH v2 (8-needle)&lt;/strong&gt;&lt;/a&gt;)&lt;/td&gt; &lt;td align="left"&gt;Tests models' ability to simultaneously track and reason about 8 pieces of information across extended conversations in long contexts&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/opendatalab/OmniDocBench"&gt;&lt;strong&gt;OmniDocBench 1.5&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Evaluates diverse document parsing capabilities across 9 document types, 4 layout types, and 3 languages with rich OCR annotations&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/datasets/Voxel51/ScreenSpot-Pro"&gt;&lt;strong&gt;ScreenSpot-Pro&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Assesses GUI grounding capabilities in high-resolution professional software environments across 23 applications and 5 industries&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2509.07968"&gt;&lt;strong&gt;SimpleQA Verified&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;A reliable factuality benchmark with 1,000 prompts for evaluating short-form factual accuracy in Large Language Models&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://scale.com/leaderboard/swe_bench_pro_public"&gt;&lt;strong&gt;SWE-bench Pro (public)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;A rigorous software engineering benchmark designed to address data contamination with more diverse and difficult coding tasks&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://scale.com/blog/swe-bench-pro"&gt;&lt;strong&gt;SWE-bench Verified&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Tests agentic coding capabilities on verified software engineering problems with solutions that have been manually validated&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://artificialanalysis.ai/evaluations/tau2-bench"&gt;&lt;strong&gt;t²-Bench&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;A dual-control conversational AI benchmark simulating technical support scenarios where both agent and user coordinate actions&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.tbench.ai/"&gt;&lt;strong&gt;Terminal-bench 2.0&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Measures AI agent capabilities in terminal environments through complex tasks like compiling code, training classifiers, and server setup&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/hkust-nlp/Toolathlon"&gt;&lt;strong&gt;Toolathlon&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Benchmarks language agents' general tool use in realistic environments featuring 600+ diverse tools and long-horizon task execution&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://andonlabs.com/evals/vending-bench-2"&gt;&lt;strong&gt;Vending-Bench 2&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Evaluates AI model performance on running a simulated vending machine business over long time horizons, scored on final bank balance&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://videommmu.github.io/"&gt;&lt;strong&gt;Video-MMMU&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Assesses Large Multimodal Models' ability to acquire and utilize knowledge from expert-level videos across six disciplines&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psd61v/a_list_of_28_modern_benchmarks_and_their_short/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psd61v/a_list_of_28_modern_benchmarks_and_their_short/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psd61v/a_list_of_28_modern_benchmarks_and_their_short/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T18:20:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps8ptl</id>
    <title>EGGROLL: trained a model without backprop and found it generalized better</title>
    <updated>2025-12-21T15:13:11+00:00</updated>
    <author>
      <name>/u/Ok_Rub1689</name>
      <uri>https://old.reddit.com/user/Ok_Rub1689</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps8ptl/eggroll_trained_a_model_without_backprop_and/"&gt; &lt;img alt="EGGROLL: trained a model without backprop and found it generalized better" src="https://b.thumbs.redditmedia.com/uktoTW_bf9zmRuQWSoZiAcs6D31nDupHl0naNq9wzHo.jpg" title="EGGROLL: trained a model without backprop and found it generalized better" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/75ldd65spk8g1.png?width=1486&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa9f2413bebea1ca9b00b842d6ff36b7b5491fc9"&gt;https://preview.redd.it/75ldd65spk8g1.png?width=1486&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa9f2413bebea1ca9b00b842d6ff36b7b5491fc9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;everyone uses contrastive loss for retrieval then evaluates with NDCG;&lt;/p&gt; &lt;p&gt;i was like &amp;quot;what if i just... optimize NDCG directly&amp;quot; ...&lt;/p&gt; &lt;p&gt;and I think that so wild experiment released by EGGROLL - Evolution Strategies at the Hyperscale (&lt;a href="https://arxiv.org/abs/2511.16652"&gt;https://arxiv.org/abs/2511.16652&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;the paper was released with JAX implementation so i rewrote it into pytorch.&lt;/p&gt; &lt;p&gt;the problem is that NDCG has sorting. can't backprop through sorting.&lt;/p&gt; &lt;p&gt;the solution is not to backprop, instead use evolution strategies. just add noise, see what helps, update in that direction. caveman optimization.&lt;/p&gt; &lt;p&gt;the quick results...&lt;/p&gt; &lt;p&gt;- contrastive baseline: train=1.0 (memorized everything), val=0.125&lt;/p&gt; &lt;p&gt;- evolution strategies: train=0.32, val=0.154&lt;/p&gt; &lt;p&gt;ES wins by 22% on validation despite worse training score.&lt;/p&gt; &lt;p&gt;the baseline literally got a PERFECT score on training data and still lost. that's how bad overfitting can get with contrastive learning apparently.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sigridjineth/eggroll-embedding-trainer"&gt;https://github.com/sigridjineth/eggroll-embedding-trainer&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Rub1689"&gt; /u/Ok_Rub1689 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps8ptl/eggroll_trained_a_model_without_backprop_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps8ptl/eggroll_trained_a_model_without_backprop_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps8ptl/eggroll_trained_a_model_without_backprop_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T15:13:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1psl6nd</id>
    <title>My experience quiet cooling 2 external/open-air Instinct MI50 cards.</title>
    <updated>2025-12-22T00:09:13+00:00</updated>
    <author>
      <name>/u/moderately-extremist</name>
      <uri>https://old.reddit.com/user/moderately-extremist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just FYI for anyone wanting to quietly cool their MI50 cards. &lt;strong&gt;TLDR:&lt;/strong&gt; The AC Infinity MULTIFAN S2 is a nice quiet blower fan that will keep your MI50 adequately cooled.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;With the stock MI50 cover/radiators, I would expect you will get best results with a blower-type fan. Since my cards are external, I have plenty of room, so wanted to go with 120mm blowers. On Ebay I could only find 80 mm blowers with shrouds, but wanted to go bigger for quieter cooling. Apparently there's not a big market for blowers designed to be quiet, really only found 1: the &lt;a href="https://www.amazon.com/dp/B012CL2V3I"&gt;AC Infinity MULTIFAN S2&lt;/a&gt;. I also ordered a Wathal fan that was much louder, but much more powerful, but unnecessary.&lt;/p&gt; &lt;p&gt;The AC Infinity fan is powered by USB, so I have it plugged into the USB outlet on my server (A Minisforum MS-A2). This is kinda nice since it turns the fans on and off with the computer, but what I may do is see if I can kill power to the USB ports, monitor the cards temps, and only power the fans when needed (there are commands that are supposed to be able to do this, but haven't tried on my hardware, yet).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Using AC Infinity MULTIFAN S2 on &lt;em&gt;lowest setting&lt;/em&gt;, maxing it out with llama-bench sustained load with 8K prompt through 100 repititions, maxes out and stays at &lt;strong&gt;70-75 C&lt;/strong&gt;. The rated max for MI50 is 94 C but want to keep 10-15 lower than max under load, which this manages no problem. On highest fan setting, keeps it about 60 C and is still pretty quiet. Lowest fan setting drops it back down pretty quick to 30 C once the card is idle, takes a long time to get it up to 75 C going from idle to maxed out.&lt;/p&gt; &lt;p&gt;Here is the exact command I ran (I ran it twice to get 100 (killed the first run when it started TG testing:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-bench -m ~/.cache/llama.cpp/unsloth_Qwen3-Next-80B-A3B-Instruct-GGUF_Qwen3-Next-80B-A3B-Instruct-UD-Q4_K_XL.gguf -sm layer -fa 1 --cache-type-k q8_0 --cache-type-v q8_0 --progress -p 8192 -n 128 -r 100 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I've done a ton of testing on what models can run at speeds I'm comfortable with, and this pretty closely mimics what I'm planning to run with llama-server indefinitely, although it will be mostly idle and will not run sustained inference for anywhere near this duration.&lt;/p&gt; &lt;p&gt;It took 13 minutes (prompt run 55) to reach 75 C. It gets up to 55 C after a minute or 2 and then creeps up slower and slower. The absolute highest temp I saw (using &amp;quot;sudo rocm-smi --alldevices --showtempgraph&amp;quot;) was 76 C; it mostly bounced around 72 - 74 C.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Caveats&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Probably the biggest thing to consider is that the model is running split between 2 cards. A model running on a single card may keep that single card more sustained at maximum load. See &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psl6nd/my_experience_quiet_cooling_2_externalopenair/nvahbmi/"&gt;here&lt;/a&gt; for some more testing regarding this... it's not terrible, but not great either... it's doable.&lt;/p&gt; &lt;p&gt;Um... I guess that's the only caveat I can think of right now.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Power&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Additional FYI - I'm running both cards off a single external PSU with splitter cables, connected to a watt-meter, most power draw I'm seeing is 250W. I didn't set any power limiting. So this also supports the caveat that a model split between 2 cards doesn't keep both cards pegged to the max at the same time. &lt;/p&gt; &lt;p&gt;Idle power draw for both cards together was consistently 38 W (&lt;em&gt;both&lt;/em&gt; cards, not each card).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Attaching The Fans&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I just used blue painter's tape.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Additional Hardware&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Additional hardware to connect the MI50 cards to my MS-A2 server:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Occulink cables: &lt;a href="https://www.amazon.com/dp/B07TG9DK4W"&gt;https://www.amazon.com/dp/B07TG9DK4W&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ATX power splitter: &lt;a href="https://www.amazon.com/dp/B08JC7W8DR"&gt;https://www.amazon.com/dp/B08JC7W8DR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GPU power splitters (be sure to get the 2-pack): &lt;a href="https://www.amazon.com/dp/B09KPWK612"&gt;https://www.amazon.com/dp/B09KPWK612&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Occulink-to-PCIe adapter (what each card plugs in to, ordered 2): &lt;a href="https://www.amazon.com/dp/B0BZHW4NQX"&gt;https://www.amazon.com/dp/B0BZHW4NQX&lt;/a&gt;&lt;/li&gt; &lt;li&gt;PCIe-to-dual-occulink adapter (what goes in the server): &lt;a href="https://www.amazon.com/dp/B0F5HPN71X"&gt;https://www.amazon.com/dp/B0F5HPN71X&lt;/a&gt; &lt;ul&gt; &lt;li&gt;The Minisforum MS-A2 can only do x4x4 bifurcation, it can't do more than 2.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Inference Software Stack&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Getting off-topic but a quick note, I might post actual numbers later. The summary is though: I tested Ollama, LM Studio, and llama.cpp (directly) on Debian 13, and settled on llama.cpp with ROCM 6.3.3 (installed from AMD's repo, you don't need AMDGPU).&lt;/p&gt; &lt;p&gt;Llama.cpp with Vulkan works out of the box but is slower than ROCM. Vulkan in Debian 13 backports is faster, but still significantly slower than ROCM. ROCM 6.3.3 is the latest ROCM that &lt;em&gt;just works&lt;/em&gt; (Debian has ROCM in it's stock repo but older and too old that the latest llama.cpp won't work with it). ROCM 7.1.1 installs fine and copying the tensor files for MI50 (gfx906) mostly works but I would get &amp;quot;Segmentation Fault&amp;quot; errors with some models, particularly Qwen3-Next I couldn't get to run with it; for other models the speed was the same or faster but not by much.&lt;/p&gt; &lt;p&gt;The backports version of mesa-vulkan-drivers I tested was 25.2.6. There are inference speed improvements in Mesa 25.3, which is currently in Sid (25.2.x was in Sid at the time I tested). It would be awesome if Vulkan catches up, it would make things SOOOO much easier on the MI50, but I doubt that will happen with 25.3 or any version any time soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moderately-extremist"&gt; /u/moderately-extremist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psl6nd/my_experience_quiet_cooling_2_externalopenair/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psl6nd/my_experience_quiet_cooling_2_externalopenair/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psl6nd/my_experience_quiet_cooling_2_externalopenair/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T00:09:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1psdk9w</id>
    <title>Nemotron-Nano-30B: What settings are you getting good results with?</title>
    <updated>2025-12-21T18:36:33+00:00</updated>
    <author>
      <name>/u/EmPips</name>
      <uri>https://old.reddit.com/user/EmPips</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently I'm running with the settings from the model card for tool-calling:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;temperature=0.6 &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;top_p=0.95&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;top_k 20&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything goes well until you're about 50k tokens in, then it kind of goes off the rails, enters infinite retry loops, or starts doing things that I can only describe as &lt;em&gt;&amp;quot;silly&amp;quot;&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;My use-case is agentic coding with Qwen-Code-CLI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmPips"&gt; /u/EmPips &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psdk9w/nemotronnano30b_what_settings_are_you_getting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psdk9w/nemotronnano30b_what_settings_are_you_getting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psdk9w/nemotronnano30b_what_settings_are_you_getting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T18:36:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1psiq8a</id>
    <title>Using local VLMs and SAM 3 to Agentically Segment Characters</title>
    <updated>2025-12-21T22:17:57+00:00</updated>
    <author>
      <name>/u/Complete-Lawfulness</name>
      <uri>https://old.reddit.com/user/Complete-Lawfulness</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psiq8a/using_local_vlms_and_sam_3_to_agentically_segment/"&gt; &lt;img alt="Using local VLMs and SAM 3 to Agentically Segment Characters" src="https://preview.redd.it/z1a1qt5otm8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f37a6822173e8ed9be99727ccdf9a798e0ffc9a4" title="Using local VLMs and SAM 3 to Agentically Segment Characters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been my goal for a while to come up with a reliable way to segment characters in an automated way, (hence why I built my &lt;a href="https://github.com/adambarbato/ComfyUI-Sa2VA"&gt;Sa2VA node&lt;/a&gt;), so I was excited when SAM 3 released last month. Just like its predecessor, SAM 3 is great at segmenting the general concepts it knows and is even better than SAM 2 and can do simple noun phrases like &amp;quot;blonde woman&amp;quot;. However, that's not good enough for character-specific segmentation descriptions like &amp;quot;the fourth woman from the left holding a suitcase&amp;quot;.&lt;/p&gt; &lt;p&gt;But at the same time that SAM 3 released, I started hearing people talk about the &lt;a href="https://github.com/facebookresearch/sam3/blob/main/examples/sam3_agent.ipynb"&gt;SAM 3 Agent&lt;/a&gt; example notebook that the authors released showing how SAM 3 could be used in an agentic workflow with a VLM. I wanted to put that to the test, so &lt;a href="https://github.com/adambarbato/ComfyUI-Segmentation-Agent"&gt;I adapted their workbook into a ComfyUI node&lt;/a&gt; that works with both local GGUF VLMs (via llama-cpp-python) and through OpenRouter.&lt;/p&gt; &lt;h2&gt;How It Works&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;The agent analyzes the base image and character description prompt&lt;/li&gt; &lt;li&gt;It chooses one or more appropriate simple noun phrases for segmentation (e.g., &amp;quot;woman&amp;quot;, &amp;quot;brown hair&amp;quot;, &amp;quot;red dress&amp;quot;) that will likely be known by the SAM 3 model&lt;/li&gt; &lt;li&gt;SAM 3 generates masks for those phrases&lt;/li&gt; &lt;li&gt;The masks are numbered and visualized on the original image and shown to the agent&lt;/li&gt; &lt;li&gt;The agent evaluates if the masks correctly segment the character&lt;/li&gt; &lt;li&gt;If correct, it accepts all or a subset of the masks that best cover the intended character; if not, it tries additional phrases&lt;/li&gt; &lt;li&gt;This iterates until satisfactory masks are found or max_iterations is reached and the agent fails&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Limitations&lt;/h3&gt; &lt;p&gt;This agentic process works, but the results are often worse (and much slower) than purpose-trained solutions like &lt;a href="https://github.com/IDEA-Research/Grounded-SAM-2"&gt;Grounded SAM&lt;/a&gt; and &lt;a href="https://github.com/bytedance/Sa2VA"&gt;Sa2VA&lt;/a&gt;. The agentic method CAN get even more correct results than those solutions if used with frontier vision models (mostly the Gemini series from Google) but I've found that the rate of hallucinations from the VLM often cancels out the benefits of checking the segmentation results rather than going with the 1-shot approach of Grounded SAM/Sa2VA.&lt;/p&gt; &lt;p&gt;This may still be the best approach if your use case needs to be 100% agentic and can tolerate long latencies and needs the absolute highest accuracy. I suspect using frontier VLMs paired with many more iterations and a more aggressive system prompt may increase accuracy at the cost of price and speed.&lt;/p&gt; &lt;p&gt;Personally though, I think I'm sticking to Sa2VA for now for its good-enough segmentation and fast speed.&lt;/p&gt; &lt;h3&gt;Future Improvements&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Refine the system prompt to include known-good SAM 3 prompts&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A lot of the system's current slowness involves the first few steps where the agent may try phrases that are too complicated for SAM and result in 0 masks being generated (often this is just a rephrasing of the user's initial prompt). Including a larger list of known-useful SAM 3 prompts may help speed up the agentic loop at the cost of more system prompt tokens.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Use the same agentic loop but with Grounded SAM or Sa2VA&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What may produce the best results is to pair this agentic loop with one of the segmentation solutions that has a more open vocabulary. Although not as powerful as the new SAM 3, Grounded SAM or Sa2VA may play better with the verbose tendencies of most VLMs and their smaller number of masks produced per prompt may help cut down on hallucinations.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Try with bounding box/pointing VLMs like Moondream&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The original SAM 3 Agent (which is reproduced here) uses text prompts from the VLM to SAM to indicate what should be segmented, but, as mentioned, SAM's native language is not text, it's visuals. Some VLMs (like the Moondream series) are trained to produce bounding boxes/points. Putting one of those into a similar agentic loop may reduce the issues described above, but may introduce its own issue in deciding what each system considers segmentable within a bounding box.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Quick Links&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;GitHub Repo: &lt;a href="https://github.com/adambarbato/ComfyUI-Segmentation-Agent"&gt;https://github.com/adambarbato/ComfyUI-Segmentation-Agent&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Example ComfyUI workflow: &lt;a href="https://github.com/adambarbato/ComfyUI-Segmentation-Agent/blob/main/workflow/comfyui-segment-agent.json"&gt;https://github.com/adambarbato/ComfyUI-Segmentation-Agent/blob/main/workflow/comfyui-segment-agent.json&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Complete-Lawfulness"&gt; /u/Complete-Lawfulness &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z1a1qt5otm8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psiq8a/using_local_vlms_and_sam_3_to_agentically_segment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psiq8a/using_local_vlms_and_sam_3_to_agentically_segment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T22:17:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps6txq</id>
    <title>RAG that actually works?</title>
    <updated>2025-12-21T13:43:39+00:00</updated>
    <author>
      <name>/u/TheGlobinKing</name>
      <uri>https://old.reddit.com/user/TheGlobinKing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I discovered AnythingLLM I thought I could finally create a &amp;quot;knowledge base&amp;quot; for my own use, basically like an expert of a specific field (e.g. engineering, medicine, etc.) I'm not a developer, just a regular user, and AnythingLLM makes this quite easy. I paired it with llama.cpp, added my documents and started to chat.&lt;/p&gt; &lt;p&gt;However, I noticed poor results from all llms I've tried, granite, qwen, gemma, etc. When I finally asked about a specific topic mentioned in a very long pdf included in my rag &amp;quot;library&amp;quot;, it said it couldn't find any mention of that topic anywhere. It seems only part of the available data is actually considered when answering (again, I'm not an expert.) I noticed a few other similar reports from redditors, so it wasn't just matter of using a different model.&lt;/p&gt; &lt;p&gt;Back to my question... is there an easy to use RAG system that &amp;quot;understands&amp;quot; large libraries of complex texts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheGlobinKing"&gt; /u/TheGlobinKing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps6txq/rag_that_actually_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps6txq/rag_that_actually_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps6txq/rag_that_actually_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T13:43:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pspgjk</id>
    <title>Day 14: 21 Days of Building a Small Language Model: Positional Encodings</title>
    <updated>2025-12-22T03:38:25+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pspgjk/day_14_21_days_of_building_a_small_language_model/"&gt; &lt;img alt="Day 14: 21 Days of Building a Small Language Model: Positional Encodings" src="https://b.thumbs.redditmedia.com/bDiWRjBhJxq8tK-MneqVVXVOzbTe8GP2VHmcpKoRfIw.jpg" title="Day 14: 21 Days of Building a Small Language Model: Positional Encodings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Welcome to Day 14 of 21 Days of Building a Small Language Model. The topic for today is Rotary Positional Encoding, or RoPE. Yesterday, we explored sinusoidal positional encodings from the original Transformer paper. Today, we'll look at modern alternatives. When most people think about positional encoding in modern language models, they immediately think of RoPE (Rotary Positional Encoding), the standard used in models like DeepSeek, Kimi, and OLMo, and why it's better than adding positional information directly to embeddings.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/becze9pieo8g1.png?width=1766&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3859b8ef53a0c15ae76d5a2dc1f8464815024434"&gt;https://preview.redd.it/becze9pieo8g1.png?width=1766&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3859b8ef53a0c15ae76d5a2dc1f8464815024434&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Issue with adding Positional Encodings to embeddings&lt;/h1&gt; &lt;p&gt;To understand why RoPE is important, we need to see the problem it solves. The original Transformer paper used sinusoidal positional encodings, which are added directly to token embeddings. This means positional information gets mixed with semantic information from the very start.&lt;/p&gt; &lt;p&gt;Think about what this means. When you add a positional encoding to a token embedding, you're mixing two different types of information:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Semantic information: What the word means&lt;/li&gt; &lt;li&gt;Positional information: Where the word appears in the sequence&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This mixing makes it harder for the model to learn clean semantic representations. The positional information is always there, polluting the semantic information. The model has to work harder to separate what a word means from where it appears.&lt;/p&gt; &lt;p&gt;RoPE solves this problem with a simple but powerful insight: positional information doesn't need to be added to token embeddings. Instead, it should be applied where it's actually used when computing attention scores between tokens.&lt;/p&gt; &lt;p&gt;In the attention mechanism, we compute attention scores by taking the dot product between query and key vectors. This is where positional relationships matter. If we can encode positional information directly in the query and key vectors, we don't need to mix it with token embeddings at all.&lt;/p&gt; &lt;p&gt;This is exactly what RoPE does. Instead of adding positional encodings to embeddings, RoPE rotates the query and key vectors based on their positions. This preserves the purity of token embeddings while still enabling the model to understand positional relationships.&lt;/p&gt; &lt;h1&gt;How RoPE works&lt;/h1&gt; &lt;p&gt;RoPE works by rotating query and key vectors using sine and cosine functions. The rotation is done using a rotation matrix. For a given angle θ, the rotation matrix looks like this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4b0p3lnieo8g1.png?width=626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f4abb1f464a567ce594f868fe198c0a672488458"&gt;https://preview.redd.it/4b0p3lnieo8g1.png?width=626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f4abb1f464a567ce594f868fe198c0a672488458&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This matrix rotates a 2D vector by angle θ. When we apply this rotation to query and key vectors, we're essentially rotating them based on their positions in the sequence.&lt;/p&gt; &lt;p&gt;Just like sinusoidal encodings use different frequencies for different dimensions, RoPE applies rotations at different speeds for different dimensions. Lower dimensions rotate quickly, capturing fine-grained position. Higher dimensions rotate slowly, capturing coarse-grained position. This creates the same multi-scale structure we saw in sinusoidal encodings, but applied through rotations rather than additions.&lt;/p&gt; &lt;p&gt;The rotation angle for each dimension depends on the position, just like in sinusoidal encodings. For a position &lt;code&gt;pos&lt;/code&gt; and dimension index &lt;code&gt;i&lt;/code&gt;, the rotation angle is:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f8ioohnieo8g1.png?width=674&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=93c83e80bbb1a7d3b291322585ee2c077f7d05aa"&gt;https://preview.redd.it/f8ioohnieo8g1.png?width=674&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=93c83e80bbb1a7d3b291322585ee2c077f7d05aa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is the same formula used for sinusoidal encodings. The angle θ_i determines how much to rotate the vector for that dimension. Notice that: • When &lt;code&gt;i = 0&lt;/code&gt; (low dimension), the denominator is 1, so θ changes quickly as position increases (fast rotation) • When &lt;code&gt;i&lt;/code&gt; is large (high dimension), the denominator is large, so θ changes slowly (slow rotation)&lt;/p&gt; &lt;p&gt;This creates the same multi-frequency structure: different dimensions rotate at different speeds.&lt;/p&gt; &lt;h1&gt;Why rotations preserve relative position&lt;/h1&gt; &lt;p&gt;The beautiful property of rotations is that they preserve relative position information. When we rotate a query vector at position &lt;code&gt;pos&lt;/code&gt; by angle θ_pos and a key vector at position &lt;code&gt;pos + k&lt;/code&gt; by angle θ_{pos+k}, the dot product between the rotated vectors automatically encodes their relative position.&lt;/p&gt; &lt;p&gt;This works because of trigonometric identities. The rotation of a vector at position &lt;code&gt;pos + k&lt;/code&gt; is related to the rotation at position &lt;code&gt;pos&lt;/code&gt; through the angle difference (θ_{pos+k} - θ_pos), which depends only on &lt;code&gt;k&lt;/code&gt; (the relative distance), not on the absolute positions.&lt;/p&gt; &lt;p&gt;In other words, if we know how to rotate vectors at positions 2 and 5, we automatically understand the relationship between positions 10 and 13, because the rotation difference is the same (3 positions apart). This is why RoPE generalizes so well to longer sequences the relative position relationships are encoded in the rotation angles themselves.&lt;/p&gt; &lt;h1&gt;How RoPE is implemented&lt;/h1&gt; &lt;p&gt;In practice, RoPE is implemented efficiently using complex number representation. For each position in the sequence, we compute rotation angles for each dimension. We then apply rotations to query and key vectors before computing attention scores.&lt;/p&gt; &lt;p&gt;The implementation involves two main steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Pre-compute rotation parameters&lt;/strong&gt;: Compute cosine and sine values for all positions and dimensions. This is done once during initialization.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apply rotations during forward pass&lt;/strong&gt;: For each query and key vector, apply the appropriate rotation based on its position. This happens during the attention computation.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Modern implementations use efficient tensor operations and broadcasting to make this fast, even for very long sequences. The rotation is applied to pairs of dimensions, working with the vector components in a way that preserves the rotation properties.&lt;/p&gt; &lt;h1&gt;Why RoPE became the standard&lt;/h1&gt; &lt;p&gt;RoPE has become the standard in modern language models because it solves the fundamental problem of mixing positional and semantic information. Models like LLaMA and Qwen all use RoPE because it provides better performance and generalization.&lt;/p&gt; &lt;p&gt;The key insight that positional information should be applied where it's used (in attention) rather than where it's convenient (in embeddings) has proven to be a game-changer. This simple shift in thinking led to a more elegant and effective solution.&lt;/p&gt; &lt;h1&gt;Beyond RoPEs&lt;/h1&gt; &lt;p&gt;While RoPE has become the standard, researchers continue to explore alternatives. One interesting approach is NoPE (No Positional Encoding), which omits explicit positional encodings from certain layers. Instead, NoPE layers rely on the built-in structure of causal attention masks to infer token positions. The Hugging Face team used a hybrid approach in SmolLM3, alternating between layers with RoPE and layers with NoPE. Every fourth layer uses NoPE, while the remaining layers use RoPE. This hybrid strategy showed 12% reduction in perplexity on long text benchmarks and 40% less performance drop when scaling from 1K to 8K context lengths. This demonstrates that positional encoding doesn't have to be all or nothing, and that hybrid approaches can offer significant benefits.&lt;/p&gt; &lt;h1&gt;Summary&lt;/h1&gt; &lt;p&gt;Today we explored Rotary Positional Encoding, the modern standard for positional encoding in language models. We learned how RoPE solves the problem of mixing positional and semantic information by applying rotations directly to query and key vectors, rather than adding positional encodings to embeddings.&lt;/p&gt; &lt;p&gt;The key insight is simple but powerful: apply positional information where it's actually used, not where it's convenient. This preserves the purity of semantic information while still enabling the model to understand positional relationships.&lt;/p&gt; &lt;p&gt;RoPE's use of rotations to encode relative positions makes it generalize well to longer sequences, and its efficient implementation makes it practical for production models. This is why it has become the standard in modern language models.&lt;/p&gt; &lt;p&gt;The fact that hybrid approaches like SmolLM3's RoPE/NoPE combination can offer even better performance shows that there's still room for innovation in positional encoding. The field continues to evolve, and new approaches may emerge that build on or improve upon RoPE.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pspgjk/day_14_21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pspgjk/day_14_21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pspgjk/day_14_21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T03:38:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pstar1</id>
    <title>Tiny local LLM (Gemma 3) as front-end manager for Claude Code on home server</title>
    <updated>2025-12-22T07:12:11+00:00</updated>
    <author>
      <name>/u/raiansar</name>
      <uri>https://old.reddit.com/user/raiansar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: I want to run Gemma 3 (1B) on my home server as a “manager” that receives my requests, dispatches them to Claude Code CLI, and summarizes the output. Looking for similar projects or feedback on the approach.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; I want to run Gemma 3 (1B) on my home server as a “manager” that receives my requests, dispatches them to Claude Code CLI, and summarizes the output. Looking for similar projects or feedback on the approach.&lt;/p&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;I use Claude Code (via Max subscription) for development work. Currently I SSH into my server and run:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd /path/to/project claude --dangerously-skip-permissions -c # continue session Copy &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This works great, but I want to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Access it from my phone&lt;/strong&gt; without SSH&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Get concise summaries&lt;/strong&gt; instead of Claude’s verbose output&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Have natural project routing&lt;/strong&gt; - say “fix acefina” instead of typing the full path&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Maintain session context&lt;/strong&gt; across conversations&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;The Idea&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;┌─────────────────────────────────────────────────┐ │ ME (Phone/Web): &amp;quot;fix slow loading on acefina&amp;quot; │ └────────────────────────┬────────────────────────┘ ▼ ┌─────────────────────────────────────────────────┐ │ GEMMA 3 1B (on NAS) - Manager Layer │ │ • Parses intent │ │ • Resolves &amp;quot;acefina&amp;quot; → /mnt/tank/.../Acefina │ │ • Checks if session exists (reads history) │ │ • Dispatches to Claude Code CLI │ └────────────────────────┬────────────────────────┘ ▼ ┌─────────────────────────────────────────────────┐ │ CLAUDE CODE CLI │ │ claude --dangerously-skip-permissions \ │ │ --print --output-format stream-json \ │ │ -c &amp;quot;fix slow loading&amp;quot; │ │ │ │ → Does actual work (edits files, runs tests) │ │ → Streams JSON output │ └────────────────────────┬────────────────────────┘ ▼ ┌─────────────────────────────────────────────────┐ │ GEMMA 3 1B - Summarizer │ │ • Reads Claude's verbose output │ │ • Extracts key actions taken │ │ • Returns: &amp;quot;Fixed slow loading - converted │ │ images to WebP, added lazy loading. │ │ Load time: 4.5s → 1.2s&amp;quot; │ └────────────────────────┬────────────────────────┘ ▼ ┌─────────────────────────────────────────────────┐ │ ME: Gets concise, actionable response │ └─────────────────────────────────────────────────┘ Copy &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Why Gemma 3?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;FunctionGemma 270M&lt;/strong&gt; just released - specifically fine-tuned for function calling&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemma 3 1B&lt;/strong&gt; is still tiny (~600MB quantized) but better at understanding nuance&lt;/li&gt; &lt;li&gt;Runs on my NAS (i7-1165G7, 16GB RAM) without breaking a sweat&lt;/li&gt; &lt;li&gt;Keeps everything local except the Claude API calls&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What I’ve Found So Far&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Project&lt;/th&gt; &lt;th align="left"&gt;Close but…&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/albertsikkema/claude-config-template"&gt;claude-config-template orchestrator&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Uses OpenAI for orchestration, not local&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/lm-sys/RouteLLM"&gt;RouteLLM&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Routes API calls, doesn’t orchestrate CLI&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://n8n.io/workflows/3139-private-and-local-ollama-self-hosted-dynamic-llm-router/"&gt;n8n LLM Router&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Great for Ollama routing, no Claude Code integration&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Anon Kode&lt;/td&gt; &lt;td align="left"&gt;Replaces Claude, doesn’t orchestrate it&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Questions for the Community&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Has anyone built something similar?&lt;/strong&gt; A local LLM managing/dispatching to a cloud LLM?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;FunctionGemma vs Gemma 3 1B&lt;/strong&gt; - For this use case (parsing intent + summarizing output), which would you choose?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Session management&lt;/strong&gt; - Claude Code stores history in &lt;code&gt;~/.claude/history.jsonl&lt;/code&gt;. Anyone parsed this programmatically?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interface&lt;/strong&gt; - Telegram bot vs custom PWA vs something else?&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;My Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Server:&lt;/strong&gt; Intel i7-1165G7, 16GB RAM, running Debian&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Claude:&lt;/strong&gt; Max subscription, using CLI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Would run:&lt;/strong&gt; Gemma via Ollama or llama.cpp&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to share what I build if there’s interest. Or if someone points me to an existing solution, even better!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/raiansar"&gt; /u/raiansar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstar1/tiny_local_llm_gemma_3_as_frontend_manager_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstar1/tiny_local_llm_gemma_3_as_frontend_manager_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pstar1/tiny_local_llm_gemma_3_as_frontend_manager_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T07:12:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1psd918</id>
    <title>As 2025 wraps up, which local LLMs really mattered this year and what do you want to see in 2026?</title>
    <updated>2025-12-21T18:23:33+00:00</updated>
    <author>
      <name>/u/ObjectiveOctopus2</name>
      <uri>https://old.reddit.com/user/ObjectiveOctopus2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now that we’re at the end of 2025, I’m curious how people here would summarize the local LLM landscape this year.&lt;/p&gt; &lt;p&gt;Not just “what scores highest on benchmarks,” but:&lt;/p&gt; &lt;p&gt;- What models did people actually run?&lt;/p&gt; &lt;p&gt;- What felt popular or influential in practice?&lt;/p&gt; &lt;p&gt;- What models punched above their weight?&lt;/p&gt; &lt;p&gt;- What disappointed or faded out?&lt;/p&gt; &lt;p&gt;Looking back, which local LLMs defined 2025 for you?&lt;/p&gt; &lt;p&gt;And looking forward:&lt;/p&gt; &lt;p&gt;- What gaps still exist?&lt;/p&gt; &lt;p&gt;- What do you want to see next year? (better small models, longer context, better reasoning, multimodal, agents, efficiency, etc.)&lt;/p&gt; &lt;p&gt;Would love both personal takes and broader ecosystem observations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ObjectiveOctopus2"&gt; /u/ObjectiveOctopus2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psd918/as_2025_wraps_up_which_local_llms_really_mattered/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psd918/as_2025_wraps_up_which_local_llms_really_mattered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psd918/as_2025_wraps_up_which_local_llms_really_mattered/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T18:23:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1psj1pr</id>
    <title>Any regrets A6000 Pro owners?</title>
    <updated>2025-12-21T22:32:15+00:00</updated>
    <author>
      <name>/u/val_in_tech</name>
      <uri>https://old.reddit.com/user/val_in_tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems like quite a future proof option at the moment for local ai needs. For those who bought - how do you guys feel about your decision. What's been working for you best and where things fall short of your expectations. &lt;/p&gt; &lt;p&gt;I use to have threadripper rig with 6 RTX 3090, it was messy and very power hungry, then started feeling dated. Also Claude was so much better than local models, it was hard to keep as much workloads local as I wish. Now the models yet again improved and some are more decent for agentic use than before, thinking of a more clean and modern setup.&lt;/p&gt; &lt;p&gt;PS. I meant the RTX 6000 Blackwell, the new one. Couldn't find a way to edit the post title &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/val_in_tech"&gt; /u/val_in_tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psj1pr/any_regrets_a6000_pro_owners/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psj1pr/any_regrets_a6000_pro_owners/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psj1pr/any_regrets_a6000_pro_owners/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T22:32:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pssf6u</id>
    <title>Best local model for use with agentic coding frameworks for 3x3090 and 256GB RAM?</title>
    <updated>2025-12-22T06:19:03+00:00</updated>
    <author>
      <name>/u/Amazydayzee</name>
      <uri>https://old.reddit.com/user/Amazydayzee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like Cursor, although I hit my rate limits pretty quickly each month. Then I switch to Copilot in VSCode for the rest of the month, which I don’t like so much.&lt;/p&gt; &lt;p&gt;I’d like to find some local model I can use with Roo Code or something like that. I’m aware that I could bounce around the free tier of many many agentic coding frameworks, and there are so many that it could last me all month, but I honestly want to just use something local.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazydayzee"&gt; /u/Amazydayzee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pssf6u/best_local_model_for_use_with_agentic_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pssf6u/best_local_model_for_use_with_agentic_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pssf6u/best_local_model_for_use_with_agentic_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T06:19:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1psnlm0</id>
    <title>MiniMax-M2 Q3_K_M on Quad V100 32gb llama.cpp testing NVlink</title>
    <updated>2025-12-22T02:06:37+00:00</updated>
    <author>
      <name>/u/MachineZer0</name>
      <uri>https://old.reddit.com/user/MachineZer0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Almost a year ago I bought a server capable of four SXM2 GPUs. The catch was to hack the OCP power supply. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/homelab/comments/1j0769h/comment/mf8yacv/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;https://www.reddit.com/r/homelab/comments/1j0769h/comment/mf8yacv/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I actually did that properly on first attempt, but didn't torque the screws enough on the V100. It wouldn't boot. I didn't really trouble shoot further since I got busy. The project sat for a year as toyed around with Dual 5090, Quad 3090 and 12x MI50 32gb RPC. I got interested in the V100 again after seeing cheap adapters from China. Bought a boat load of 16gb adapter variants since they sold for a song and started putting together with Turbo adapters. Then with the V100 top of mind, I got four of the 32gb SXM2 and went back to the NVLink build.&lt;/p&gt; &lt;p&gt;tldr. Exactly as mentioned in &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/11485"&gt;how do I enable NVLink / peer transfers? · ggml-org/llama.cpp · Discussion #11485 · GitHub&lt;/a&gt;, Split mode 'row' is not optimized for NVlink.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--split-mode row &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;About 70 tok/s pp and 20 tok/s out&lt;/p&gt; &lt;pre&gt;&lt;code&gt;slot launch_slot_: id 3 | task -1 | sampler chain: logits -&amp;gt; penalties -&amp;gt; dry -&amp;gt; top-n-sigma -&amp;gt; top-k -&amp;gt; typical -&amp;gt; top-p -&amp;gt; min-p -&amp;gt; xtc -&amp;gt; temp-ext -&amp;gt; dist slot launch_slot_: id 3 | task 6677 | processing task slot update_slots: id 3 | task 6677 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 52 slot update_slots: id 3 | task 6677 | n_tokens = 18, memory_seq_rm [18, end) slot update_slots: id 3 | task 6677 | prompt processing progress, n_tokens = 52, batch.n_tokens = 34, progress = 1.000000 slot update_slots: id 3 | task 6677 | prompt done, n_tokens = 52, batch.n_tokens = 34 slot print_timing: id 3 | task 6677 | prompt eval time = 479.55 ms / 34 tokens ( 14.10 ms per token, 70.90 tokens per second) eval time = 310990.17 ms / 6236 tokens ( 49.87 ms per token, 20.05 tokens per second) total time = 311469.71 ms / 6270 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;--split-mode layer&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Holy crap...&lt;/p&gt; &lt;pre&gt;&lt;code&gt;slot launch_slot_: id 2 | task -1 | sampler chain: logits -&amp;gt; penalties -&amp;gt; dry -&amp;gt; top-n-sigma -&amp;gt; top-k -&amp;gt; typical -&amp;gt; top-p -&amp;gt; min-p -&amp;gt; xtc -&amp;gt; temp-ext -&amp;gt; dist slot launch_slot_: id 2 | task 273 | processing task slot update_slots: id 2 | task 273 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 52 slot update_slots: id 2 | task 273 | n_tokens = 15, memory_seq_rm [15, end) slot update_slots: id 2 | task 273 | prompt processing progress, n_tokens = 52, batch.n_tokens = 37, progress = 1.000000 slot update_slots: id 2 | task 273 | prompt done, n_tokens = 52, batch.n_tokens = 37 slot print_timing: id 2 | task 273 | prompt eval time = 21.97 ms / 37 tokens ( 0.59 ms per token, 1683.88 tokens per second) eval time = 167754.38 ms / 6476 tokens ( 25.90 ms per token, 38.60 tokens per second) total time = 167776.36 ms / 6513 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hope one day someone decides to optimize NVlink for inference. Unless you plan to train, stick with the RTX 3090 as the SXM2 systems are still highly inflated.&lt;/p&gt; &lt;p&gt;But consider messing with $100 V100 16gb SXM2 with a $50 adapter if you can hack cooling, or a $170 turbo adapter if you want the 5min DIY to assemble.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MachineZer0"&gt; /u/MachineZer0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psnlm0/minimaxm2_q3_k_m_on_quad_v100_32gb_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psnlm0/minimaxm2_q3_k_m_on_quad_v100_32gb_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psnlm0/minimaxm2_q3_k_m_on_quad_v100_32gb_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T02:06:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps6w96</id>
    <title>Dataset quality is not improving much</title>
    <updated>2025-12-21T13:46:50+00:00</updated>
    <author>
      <name>/u/rekriux</name>
      <uri>https://old.reddit.com/user/rekriux</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps6w96/dataset_quality_is_not_improving_much/"&gt; &lt;img alt="Dataset quality is not improving much" src="https://external-preview.redd.it/1p_Y2zfHWGdGS1n176QenprBBks4UkO2cWuEEHp6f68.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=521a9f1988c888fe9369f5871e2a57530ec8bd94" title="Dataset quality is not improving much" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am checking public dataset often. And while we have RAG and lots of innovation posted here in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, there are rarely breakthrough in datasets creation. While I may be lurking in this sub, I doped out of electronics/computing and studied in other fields and obtained my master in something else, I have been dabbling with AI since 2000. So take this as a my rant. But I do hope some people will start more research on dataset quality and it's creation pipelines.&lt;/p&gt; &lt;p&gt;Buckle up (sorry for spelling, no AI proofread and quick typing)&lt;/p&gt; &lt;p&gt;From my perspectives, the most all rounder datasets for instruction following are :&lt;/p&gt; &lt;p&gt;The Tulu from Allenai [allenai/tulu-3-sft-mixture]([&lt;a href="https://huggingface.co/datasets/allenai/tulu-3-sft-mixture"&gt;https://huggingface.co/datasets/allenai/tulu-3-sft-mixture&lt;/a&gt;) The smoltakl from HG &lt;a href="https://huggingface.co/datasets/HuggingFaceTB/smoltalk2"&gt;HuggingFaceTB/smoltalk2&lt;/a&gt; Hermes 3 from NousResearch [NousResearch/Hermes-3-Dataset]([&lt;a href="https://huggingface.co/datasets/NousResearch/Hermes-3-Dataset"&gt;https://huggingface.co/datasets/NousResearch/Hermes-3-Dataset&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;That's about it. The other good dataset are those that mix other datasets for good variety. Dolphin could be good, but I found it's quality a bit lacking to be included in the above. Openherms was also good for it's time, but now it should be heavily reworked.&lt;/p&gt; &lt;p&gt;Just that ? This is kind of concerning. Every one knows the &amp;quot;**garbage in, garbage out**&amp;quot; phenomena.&lt;/p&gt; &lt;p&gt;I consider 2 dataset breakthrough : &lt;strong&gt;WizzardLM&lt;/strong&gt; and &lt;strong&gt;Magpie&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Since then, we hadn't have any great innovation in dataset or did I miss it ? Yea, deduplication and merging datasets, but that's not brilliant level and over engineered.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Lately, NVIDIA released SFT datasets. The first one they released is behind a &amp;quot;ASK AUTH&amp;quot; to access it? Well, guess what, I was denied access.&lt;/p&gt; &lt;p&gt;Then came Nano and they gave access to the the INSTRUCT SFT:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/nvidia/Nemotron-Instruction-Following-Chat-v1"&gt;nvidia/Nemotron-Instruction-Following-Chat-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I went away and check a few examples. There are other parts of the dataset like RL pipeline, but I didn't have time to investigate further.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Nemotron&lt;/strong&gt; are a bit of hit and miss. If you tried it, sometimes it feels brilliant in solving something, then the next it feels dumb in answering something simpler. Do you get that feeling ?&lt;/p&gt; &lt;p&gt;Well I think this is related to the SFT they did in the initial stage.&lt;/p&gt; &lt;p&gt;For a quick round up of what I found :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Lots of sycophancy thanks to using GPT-OSS 120B&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No use of **system** message&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Wasting precious resources without having the llm learn that the system prompt is prioritized over user request, soft vs hard overwrites handling, like UPPERCASE or directives that could mean priority like ALWAYS, NEVER, if... Handling opposing directives. Implementing directives as code (codeagent?) ...&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Aren't most coding agent using very long system messages to give the LLM instructions ?? Well Nemotron is missing out on training on it so there is no way that it will perform well when used by a agent that make MASSIVE list of instructions to follow.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Poor use of multi-turn conversations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Recall of something that was used a few turns up, like initial directives (or some sort of AGENT.md)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Absence of labeling :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Each conversation should have :&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;code&gt; instructions : the specific instructions list to be learned during this conversation instructions_types : in what major categories does those instructions fit in constraints : the .. constraints ... learned ... constraints_types : in what major categories does those constraints fit in tasks : the specific tasks asked the llm... task_type : in what type of llm task does this belong to (EDITING, CREATIVE, CODING...) skills : the specific skills that should be demonstrated ... skills_types : skills categories user_intent : what are the user intents in this conversation user_intent_categories : ... categories has_context : the user provided the context (RAG, CODE, ) inject_knowledge : this inject knowledge to the model by generating a answer from nothing (ex external source) context_type : what is it : code, rag, instruction.md, pasted text, url to fetch... domain_knowledge : what are the domains of knowledge that this touch uppon mode : are we in a chat with a user, a toolcall, a RP session, a persona (coder, writing assistant), interactive vs one shot tools_provided : did we provide tools to the llm tools_used : did the llm use the provided tools tool_summary : tools used, in what order, tool use evaluation (used right tools but many non productive and didn't use the grep tool that should have done it faster) risks : what are the risks associated with the user request risk_mitigation : what should the llm do to mitigate the risks ? disclaimer, refusal, providing multiple perspectives to the request, ignore risk as unfounded intermediary_steps : add additional steps that force the llm to produce plan of action, summary of important information, recall of what was asked the llm to do system_protection : does the system message ask for it to be protected (no leaks) system_protection\_test : did the system message leak in the assistant responses ... &lt;/code&gt;&lt;/p&gt; &lt;p&gt;The labeling of data is the only way to make sure the dataset is balanced in skills, risk management, task types and diversity of knowledge domains etc.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;How many conversations help the llm learn how to efficiently use RAG context in the conversation and make a summary, extract specific information, process it in a coherent json file ? If you don't have your dataset classified, how can you know if this is under-represented and that is why it's not performing well in **YOUR** agentic use ?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Once you have a label dataset, it's easy to spot blind spots. Also it would be easy to test all skills, tasks, risks etc. to evaluate how it performs on more complicated evaluation set and see it some should be augmented in the dataset. This should be done regularly in training phase, **so you could balance things by finer adjustment in ratios between checkpoint snapshot.**&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;From my perspective, Nano will perform poorly in many cases just because the instruction set for initial SFT was bad. They used GPT-OSS-120B, Qwen3-235B-A22B-Thinking-2507, and Qwen3-235B-A22B-Instruct-2507 for generation, and that seems like middle of the LLM size. I would have thought that more large open models would have been used, at least for some tasks like handling multiple instructions/constraints at the same time while performing many tasks and using many skills. Also using those mid range llms, they should have time to do review of the dataset by LLMS. Just produce statistics and ask all other 400B models to evaluate your pipeline, output, reasoning in making the dataset and THEY WILL TELL YOU WHERE YOU MISSED OUT.&lt;/p&gt; &lt;p&gt;Now if you where to ask me how to enhance this dataset, I would say&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;classify it to get the idea of current state (the system, user, assistant turns)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;make a list of all large categories and plot distributions -&amp;gt; ANALYZE THIS&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;generate system messages for each conversation, starting with the user requests and looking at user_intent a) use a sort of registry to follow and adjust distribution of instructions, constraints, tasks, skills, tools, number of directives in system b) have clear identification of what this conversation is about : you are a chatbot in some company processing complaints, you are a public chat providing answers to help students, engage in roleplay (RP) with user by impersonating, you are a game master/story teller in a interactive, you are a brainstorming assistant that helps produce detailed exploration plans... c) have varying length of system msg, from 10 to 2k tokens&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Insert RAG content from ultra-fineweb, finepdf, wikipedia, recycling_the_web and ask that answer be based on that context (to prevent too much content injection (that may result in more hallucinations) and work more on skills).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;For cases where RAG is not used, this should be CREATIVE/PROBLEM_SOLVING/PLANNING types of tasks, and those tasks should be well defined in system message or in user, make sure it is&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Regenerate set % of user messages using evolve to include more instructions/constraints and complicate things a bit&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;After each change above, update the classification of the conversation, each modification to the conversation should be a json with : what to modify (system, user_#, assistant_#) and classification modification (+instruct, +constraint, +task, -mode, +mode)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Review distribution of data, make more adjustments&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;now regenerate the answers, before each assistant turn, produce a intermediary turn, it should be like multiple agents debating about what is the task at hand, what previous information was provided, what are the specific instructions and constraints, enumerate previous conversations that may have content for this, are there any ambiguity or any information missing that could prevent making a informed decision...&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;check that it makes sens, risk management, easy answer or considered multiple angles, did the model consider ambiguity or opposing instructions/constraints... That should use the intermediary_steps.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;fix any issues in answers&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;evaluate dataset on small model with 100b token budget the model performance to check the impact of the changes to the dataset&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;p&gt;My gold dataset rule :&lt;/p&gt; &lt;p&gt;Now if you just produce answers without the intermediary steps, this is just distillation and the produced model will never be any better than the reference model (in fact it will be a bit worse, because the model attention is limited and it may have missed something once, then your mode will miss it always). But if you use a few models to reason, explore, summarize, recall previous knowledge and make hypothesis, validate hypothesis beforehand and passing that condensed work to the llm before generating the answer, then you are on the way to developing unique and perhaps enhanced skills for your future model. Simple, generate a distilled response and generate a primed response using the gold intermediary step and compare the 2, you will have your answer.&lt;/p&gt; &lt;p&gt;Every assistant generation should also be checked that it respected the task, that it performed it by following the instructions and constraints, that it stayed in it's 'role' or mode...&lt;/p&gt; &lt;p&gt;This is how we could work on having SOTA datasets to rivalize those held behind closed doors.&lt;/p&gt; &lt;p&gt;Hope this inspire more research and higher quality datasets.&lt;/p&gt; &lt;p&gt;P.S. I would like if you hold datasets that can be anonymized to be shared on HG, this could contribute to more diversity.&lt;/p&gt; &lt;p&gt;Also shout out to Eric Hartford &lt;a href="https://huggingface.co/datasets/QuixiAI/VibeCoding"&gt;QuixiAI/VibeCoding&lt;/a&gt; that is trying to make a open dataset for &amp;quot;collect anonymized client ↔ server message logs from popular AI coding tools and interfaces. These logs will form the basis of an open dataset hosted on Hugging Face and GitHub.&amp;quot; So if any of you wish to contribute, please do so !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rekriux"&gt; /u/rekriux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/datasets/nvidia/Nemotron-Instruction-Following-Chat-v1/discussions/1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps6w96/dataset_quality_is_not_improving_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps6w96/dataset_quality_is_not_improving_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T13:46:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1psd4er</id>
    <title>Moore Threads Unveils The Lushan Gaming &amp; Huashan AI GPUs: 15x Gaming Performance Uplift, 50x RT Boost, DX12 Ultimate Support, Launching Next Year</title>
    <updated>2025-12-21T18:18:24+00:00</updated>
    <author>
      <name>/u/Individual_Aside7554</name>
      <uri>https://old.reddit.com/user/Individual_Aside7554</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://wccftech.com/moore-threads-lushan-gaming-huashan-ai-gpus-15x-gaming-uplift-50x-rt-boost-dx12-ultimate-support/"&gt;https://wccftech.com/moore-threads-lushan-gaming-huashan-ai-gpus-15x-gaming-uplift-50x-rt-boost-dx12-ultimate-support/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Individual_Aside7554"&gt; /u/Individual_Aside7554 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psd4er/moore_threads_unveils_the_lushan_gaming_huashan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psd4er/moore_threads_unveils_the_lushan_gaming_huashan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psd4er/moore_threads_unveils_the_lushan_gaming_huashan/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T18:18:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pslzv6</id>
    <title>Revibe is a Rust-rewrite of Mistral Vibe written by Devstral 2</title>
    <updated>2025-12-22T00:48:25+00:00</updated>
    <author>
      <name>/u/biet_roi</name>
      <uri>https://old.reddit.com/user/biet_roi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pslzv6/revibe_is_a_rustrewrite_of_mistral_vibe_written/"&gt; &lt;img alt="Revibe is a Rust-rewrite of Mistral Vibe written by Devstral 2" src="https://external-preview.redd.it/J8tu8oCcj88QnyOGI0-nxwGZljaEo8sk3VorQPTha8k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7b7531b384cf525c3c5142217cba6a53acbdba0" title="Revibe is a Rust-rewrite of Mistral Vibe written by Devstral 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/locallama"&gt;r/locallama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;This was my project to evaluate Devstral 2 since it's free right now. Overall, I thought it did pretty well! The CLI it made is totally usable and has a bit better performance than the original when actively agenting (not that it really matters since it'll likely be dwarfed by the model). I usually prefer tools like this to be in rust though since it's the language I work in daily.&lt;/p&gt; &lt;p&gt;Unfortunately, the 120b devstral is too big &amp;amp; slow for my hardware, but I might try to finetune the 24b. I hope Mistral and other labs will continue releasing open code models :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/biet_roi"&gt; /u/biet_roi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/nicksenger/revibe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pslzv6/revibe_is_a_rustrewrite_of_mistral_vibe_written/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pslzv6/revibe_is_a_rustrewrite_of_mistral_vibe_written/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T00:48:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pstuyv</id>
    <title>MiniMax M2.1 is a straight up beast at UI/UX design. Just saw this demo...</title>
    <updated>2025-12-22T07:48:01+00:00</updated>
    <author>
      <name>/u/BlackRice_hmz</name>
      <uri>https://old.reddit.com/user/BlackRice_hmz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstuyv/minimax_m21_is_a_straight_up_beast_at_uiux_design/"&gt; &lt;img alt="MiniMax M2.1 is a straight up beast at UI/UX design. Just saw this demo..." src="https://external-preview.redd.it/ZmtlNnAwcnZtcDhnMbRwrbZjXgs5PA7MM0agSvimAWH_bh1Ie65E3MD0QPIx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b343b6e2cdec93caf72ec2c4750c9d49b84ff91" title="MiniMax M2.1 is a straight up beast at UI/UX design. Just saw this demo..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seriously, I didn't expect MiniMax M2.1 to be this cracked at design. Just saw this post on X (link below) and the UI it generated looks incredibly clean.&lt;/p&gt; &lt;p&gt;Also noticed the vLLM PR for it was just merged, so it’s officially coming. If it can actually code and design like this consistently, I'm switching.&lt;/p&gt; &lt;p&gt;Link to the tweet 👉 &lt;a href="https://x.com/CloudTrader4/status/2002729591451054127"&gt;https://x.com/CloudTrader4/status/2002729591451054127&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlackRice_hmz"&gt; /u/BlackRice_hmz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x7el31rvmp8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstuyv/minimax_m21_is_a_straight_up_beast_at_uiux_design/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pstuyv/minimax_m21_is_a_straight_up_beast_at_uiux_design/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T07:48:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pse7w6</id>
    <title>It ain’t much, but proud of my 2x3090 + a spare 3060 for support</title>
    <updated>2025-12-21T19:03:54+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pse7w6/it_aint_much_but_proud_of_my_2x3090_a_spare_3060/"&gt; &lt;img alt="It ain’t much, but proud of my 2x3090 + a spare 3060 for support" src="https://b.thumbs.redditmedia.com/d_4ZORlCqJiugxvoE3WiLbJ7wAjH7oBiwe8Id8UqYgY.jpg" title="It ain’t much, but proud of my 2x3090 + a spare 3060 for support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It’s a bit tight, but it fits and I didn’t want to buy a new case just yet. I had a spare computer that I bought first 1x3090, and now a 2nd 3090.&lt;/p&gt; &lt;p&gt;Qwen3-Next-80b is great!&lt;/p&gt; &lt;p&gt;Trying to wrap my head around Clint and using it in VS Code, but still not working properly…&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pse7w6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pse7w6/it_aint_much_but_proud_of_my_2x3090_a_spare_3060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pse7w6/it_aint_much_but_proud_of_my_2x3090_a_spare_3060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T19:03:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1psh1w2</id>
    <title>1 year later and people are still speedrunning NanoGPT. Last time this was posted the WR was 8.2 min. Its now 127.7 sec.</title>
    <updated>2025-12-21T21:04:59+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psh1w2/1_year_later_and_people_are_still_speedrunning/"&gt; &lt;img alt="1 year later and people are still speedrunning NanoGPT. Last time this was posted the WR was 8.2 min. Its now 127.7 sec." src="https://preview.redd.it/fkz64bswfm8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=173d94496a9f94631434a2e6b566db19eb11ba40" title="1 year later and people are still speedrunning NanoGPT. Last time this was posted the WR was 8.2 min. Its now 127.7 sec." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1gmd1a8/are_people_speedrunning_training_gpts_now/"&gt;Previous post&lt;/a&gt; for context. Also note original NanoGPT run from Andrej Karpathy was 45 min. I think this is a great way to understand progress in overall algorithmic speed improvements as I'm sure the big labs are using similar speedup tricks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fkz64bswfm8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psh1w2/1_year_later_and_people_are_still_speedrunning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psh1w2/1_year_later_and_people_are_still_speedrunning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T21:04:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pstaoo</id>
    <title>Got me a 32GB RTX 4080 Super</title>
    <updated>2025-12-22T07:12:05+00:00</updated>
    <author>
      <name>/u/Spooknik</name>
      <uri>https://old.reddit.com/user/Spooknik</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstaoo/got_me_a_32gb_rtx_4080_super/"&gt; &lt;img alt="Got me a 32GB RTX 4080 Super" src="https://b.thumbs.redditmedia.com/skSb77iJra6hgwff6TIBJmnw4nMo-uZGyL0qiLRJ9hs.jpg" title="Got me a 32GB RTX 4080 Super" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is maybe slightly off topic, but since people ask about hardware here a lot. &lt;/p&gt; &lt;p&gt;I took a risk and bought a modified RTX 4080 Super from the Chinese market for around 1200 USD / 1000 EUR. Which for me because I live in Europe, the cheapest RTX 5090 I can find is around 2500 USD / 2100 EUR. &lt;/p&gt; &lt;p&gt;It's maybe not the best card for price per GB of VRAM considering the RTX 3090 is dropping a lot, but 32GB on one card for about half the price of a 5090 is nice. I do a lot of Diffusion model stuff, so it's great for that too. &lt;/p&gt; &lt;p&gt;It works with the stock Nvidia driver, no messing around, it was just literally plug and play. Card seems really good quality, metal back plate and metal case. Fan sounds like a small jet engine. &lt;/p&gt; &lt;p&gt;But running it around a month now and zero issues at all. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spooknik"&gt; /u/Spooknik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pstaoo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstaoo/got_me_a_32gb_rtx_4080_super/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pstaoo/got_me_a_32gb_rtx_4080_super/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T07:12:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1psbx2q</id>
    <title>llama.cpp appreciation post</title>
    <updated>2025-12-21T17:28:24+00:00</updated>
    <author>
      <name>/u/hackiv</name>
      <uri>https://old.reddit.com/user/hackiv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psbx2q/llamacpp_appreciation_post/"&gt; &lt;img alt="llama.cpp appreciation post" src="https://preview.redd.it/asipaua1el8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87eeb0e85f39e765b810e9ec58e5148346cc419b" title="llama.cpp appreciation post" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackiv"&gt; /u/hackiv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/asipaua1el8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psbx2q/llamacpp_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psbx2q/llamacpp_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T17:28:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pstlas</id>
    <title>major open-source releases this year</title>
    <updated>2025-12-22T07:30:46+00:00</updated>
    <author>
      <name>/u/sahilypatel</name>
      <uri>https://old.reddit.com/user/sahilypatel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstlas/major_opensource_releases_this_year/"&gt; &lt;img alt="major open-source releases this year" src="https://preview.redd.it/wynfuvk9kp8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=763bc1a7f949dc4ff18c4a976a10f017205abb54" title="major open-source releases this year" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sahilypatel"&gt; /u/sahilypatel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wynfuvk9kp8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstlas/major_opensource_releases_this_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pstlas/major_opensource_releases_this_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T07:30:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp9w31</id>
    <title>AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio</title>
    <updated>2025-12-17T22:18:01+00:00</updated>
    <author>
      <name>/u/AIatMeta</name>
      <uri>https://old.reddit.com/user/AIatMeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We’re the research team behind the newest members of the Segment Anything collection of models: SAM 3 + SAM 3D + SAM Audio.&lt;/p&gt; &lt;p&gt;We’re excited to be here to talk all things SAM (sorry, we can’t share details on other projects or future work) and have members from across our team participating:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SAM 3 (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/segment-anything-model-3/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nikhila Ravi&lt;/li&gt; &lt;li&gt;Pengchuan Zhang&lt;/li&gt; &lt;li&gt;Shoubhik Debnath&lt;/li&gt; &lt;li&gt;Chay Ryali&lt;/li&gt; &lt;li&gt;Yuan-Ting Hu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM 3D (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-3d/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weiyao Wang&lt;/li&gt; &lt;li&gt;Sasha Sax&lt;/li&gt; &lt;li&gt;Xitong Yang&lt;/li&gt; &lt;li&gt;Jinkun Cao&lt;/li&gt; &lt;li&gt;Michelle Guo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM Audio (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-audio/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bowen Shi&lt;/li&gt; &lt;li&gt;Andros Tjandra&lt;/li&gt; &lt;li&gt;John Hoffman&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try SAM Audio, SAM 3D, and SAM 3 in the Segment Anything Playground: &lt;a href="https://go.meta.me/87b53b"&gt;https://go.meta.me/87b53b&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PROOF: &lt;a href="https://x.com/AIatMeta/status/2001429429898407977"&gt;https://x.com/AIatMeta/status/2001429429898407977&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT: Thanks to everyone who joined the AMA and for all the great conversation. We look forward to the next one!&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIatMeta"&gt; /u/AIatMeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T22:18:01+00:00</published>
  </entry>
</feed>
