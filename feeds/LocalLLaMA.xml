<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-16T11:24:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qe7j2v</id>
    <title>Luminal is a high-performance general-purpose inference compiler</title>
    <updated>2026-01-16T05:34:11+00:00</updated>
    <author>
      <name>/u/yogthos</name>
      <uri>https://old.reddit.com/user/yogthos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe7j2v/luminal_is_a_highperformance_generalpurpose/"&gt; &lt;img alt="Luminal is a high-performance general-purpose inference compiler" src="https://external-preview.redd.it/b-ktLeXWioxuV4hQoMUJJnc-Er8yy-L0PCsHQ7N2a0Y.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd17cc6390a58faaef58b46b4cf1bfc8a445b756" title="Luminal is a high-performance general-purpose inference compiler" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yogthos"&gt; /u/yogthos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/luminal-ai/luminal"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe7j2v/luminal_is_a_highperformance_generalpurpose/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe7j2v/luminal_is_a_highperformance_generalpurpose/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T05:34:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe4af5</id>
    <title>Opinions on the best coding model for a 3060 (12GB) and 64GB of ram?</title>
    <updated>2026-01-16T02:56:14+00:00</updated>
    <author>
      <name>/u/eapache</name>
      <uri>https://old.reddit.com/user/eapache</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Specs in the title. I have been running GPT-OSS-120B at the published mxfp4. But recently Iâ€™ve been hearing good things about e.g. MiniMax-2.1 and GLM-4.7. Much bigger models, but with heavy REAP and quants they could also fit on my machine.&lt;/p&gt; &lt;p&gt;Based on my reading, MiniMax is probably the strongest of the three, but I donâ€™t know if the REAP and quants (probably REAP-40 at q3 is necessary) would degrade it too much? Or maybe there are other models Iâ€™m overlooking?&lt;/p&gt; &lt;p&gt;What are other peopleâ€™s experiences?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eapache"&gt; /u/eapache &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe4af5/opinions_on_the_best_coding_model_for_a_3060_12gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe4af5/opinions_on_the_best_coding_model_for_a_3060_12gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe4af5/opinions_on_the_best_coding_model_for_a_3060_12gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T02:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qea339</id>
    <title>Any public REAP models leaderboard?</title>
    <updated>2026-01-16T07:58:17+00:00</updated>
    <author>
      <name>/u/djdeniro</name>
      <uri>https://old.reddit.com/user/djdeniro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qea339/any_public_reap_models_leaderboard/"&gt; &lt;img alt="Any public REAP models leaderboard?" src="https://b.thumbs.redditmedia.com/562LmlB9Xa72z7t8y9rLzaSacrTCZ1WXWEGxI0Zfy3w.jpg" title="Any public REAP models leaderboard?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dear reddits!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dx299ndy3odg1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=110ca355b256a66aae76fa6bb29527d04c4ec709"&gt;https://preview.redd.it/dx299ndy3odg1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=110ca355b256a66aae76fa6bb29527d04c4ec709&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I spent a lot of time searching, but maybe someone knows of an existing leaderboard of REAP models (including quantized ones), or is everyone currently doing their own testing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/djdeniro"&gt; /u/djdeniro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qea339/any_public_reap_models_leaderboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qea339/any_public_reap_models_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qea339/any_public_reap_models_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T07:58:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdrxiu</id>
    <title>OpenAI has signed a $10 billion contract with Cerebras</title>
    <updated>2026-01-15T18:42:39+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://en.ain.ua/2026/01/15/openai-has-signed-a-10-billion-contract-with-cerebras/"&gt;https://en.ain.ua/2026/01/15/openai-has-signed-a-10-billion-contract-with-cerebras/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few days ago, I read some comments about this hypothetical wedding and why it wasn't happening. And yet, it happened!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdrxiu/openai_has_signed_a_10_billion_contract_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdrxiu/openai_has_signed_a_10_billion_contract_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdrxiu/openai_has_signed_a_10_billion_contract_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T18:42:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qebgiy</id>
    <title>I built a local multi-modal video search engine as a personal project, and it's using local models with full text and semantic search (100% local and open source)</title>
    <updated>2026-01-16T09:24:12+00:00</updated>
    <author>
      <name>/u/IliasHad</name>
      <uri>https://old.reddit.com/user/IliasHad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I've been working on Edit Mind - a fully local video analysis and search system that uses multi-modal embeddings to make large video archives searchable without sending anything to the cloud. (which starts as a simple CLI that does transcription only and search using text only).&lt;/p&gt; &lt;p&gt;Architecture:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Text embeddings:&lt;/strong&gt; Xenova/all-mpnet-base-v2 for transcriptions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual embeddings:&lt;/strong&gt; Xenova/clip-vit-base-patch32 for frame analysis&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Audio embeddings:&lt;/strong&gt; Xenova/clap-htsat-unfused for audio content&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vector DB:&lt;/strong&gt; ChromaDB for semantic search (local version)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transcription:&lt;/strong&gt; Whisper (local inference)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Object detection:&lt;/strong&gt; YOLOv8(n) for frame-level object identification&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Face recognition:&lt;/strong&gt; DeepFace for person identification&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OCR:&lt;/strong&gt; EasyOCR for text-in-video extraction&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NLP&lt;/strong&gt;: (Ollama, Gemini, or node-llama-cpp)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Python backend for ML pipeline (PyTorch, Transformers, Ultralytics)&lt;/li&gt; &lt;li&gt;Node.js for orchestration and job queue (BullMQ + Redis)&lt;/li&gt; &lt;li&gt;Docker containers: Web UI, Background processor, ML service&lt;/li&gt; &lt;li&gt;WebSocket communication between services&lt;/li&gt; &lt;li&gt;FFmpeg for video frames extraction and metadata &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you would like to see Edit Mind in a live demo, you can check out this video from the Twelve Labs webinar: &lt;a href="https://www.youtube.com/watch?v=k_aesDa3sFw&amp;amp;t=1271s"&gt;&lt;strong&gt;https://www.youtube.com/watch?v=k_aesDa3sFw&amp;amp;t=1271s&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Project Link: &lt;a href="https://github.com/iliashad/edit-mind"&gt;&lt;strong&gt;https://github.com/iliashad/edit-mind&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Current status&lt;strong&gt;:&lt;/strong&gt; Proof of concept working. Now focusing on optimization and code quality. Working solo with some external contributors.&lt;/p&gt; &lt;p&gt;Would love feedback on:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Embedding model choices (better alternatives for video?)&lt;/li&gt; &lt;li&gt;Vector search optimization strategies: (I have 3 collections now, one for text, visual, and audio)&lt;/li&gt; &lt;li&gt;I'm running this project over a M1 Max (64 GB), I would like to build mini pc with Nivida RTX 3060 (12GB vram) ? do you think it's a good idea ? or Apple chips willl peform better ? &lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IliasHad"&gt; /u/IliasHad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qebgiy/i_built_a_local_multimodal_video_search_engine_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qebgiy/i_built_a_local_multimodal_video_search_engine_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qebgiy/i_built_a_local_multimodal_video_search_engine_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T09:24:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe9cjy</id>
    <title>[P] I built an Offline-First MCP Server that creates a "Logic Firewall" for Cursor (No API Key Required)</title>
    <updated>2026-01-16T07:14:02+00:00</updated>
    <author>
      <name>/u/bluemouse_ai</name>
      <uri>https://old.reddit.com/user/bluemouse_ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;Hi , I built **BlueMouse** (v6.6) because I wanted an industrial-grade coding assistant that doesn't rely on cloud brains for basic logic. It's an **MCP Server** that acts as a parasitic logic layer for your editor (Cursor/Windsurf/Antigravity). **Why you might like it:** * **100% Local / Offline** : It comes with a 180k-record &amp;quot;Data Trap&amp;quot; distilled into a local knowledge base. * **Privacy First** : You don't need to send your business logic to OpenAI if you don't want to. It runs perfectly with local Ollama models. * **Socratic Logic** : It forces the LLM to ask clarifying questions *before* generating code. (e.g., &amp;quot;Is this high-concurrency? If so, Optimistic or Pessimistic locking?&amp;quot;) **The Coolest Part** : We implemented a &amp;quot;Nuclear Toaster&amp;quot; acid test. Even completely offline, the system detected the &amp;quot;Safety Critical&amp;quot; domain and switched to a Fail-Safe generation mode, refusing to use generic templates. It uses a &amp;quot;Parasitic AI&amp;quot; architecture where the rule engine (&amp;lt;100ms) handles the logic guardrails, and the LLM (Local or Cloud) only fills in the implementation details. **Repo** : https://github.com/peijun1700/bluemouse **Twitter** : https://x.com/bluemouse_ai Happy to answer any technical questions about the MCP implementation! &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bluemouse_ai"&gt; /u/bluemouse_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe9cjy/p_i_built_an_offlinefirst_mcp_server_that_creates/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe9cjy/p_i_built_an_offlinefirst_mcp_server_that_creates/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe9cjy/p_i_built_an_offlinefirst_mcp_server_that_creates/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T07:14:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdyc3e</id>
    <title>Job wants me to develop RAG search engine for internal documents</title>
    <updated>2026-01-15T22:42:20+00:00</updated>
    <author>
      <name>/u/Next-Self-184</name>
      <uri>https://old.reddit.com/user/Next-Self-184</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;this would be the first time I develop a RAG tool that searches through 2-4 million documents (mainly PDFs and many of those needing OCR). I was wondering what sort of approach I should take with this and whether it makes more sense to develop a local or cloud tool. Also the information needs to be secured so that's why I was leaving toward local. Have software exp in other things but not working with LLMs or RAG systems so looking for pointers. Also turnkey tools are out of the picture unless they're close to 100k. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Next-Self-184"&gt; /u/Next-Self-184 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdyc3e/job_wants_me_to_develop_rag_search_engine_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdyc3e/job_wants_me_to_develop_rag_search_engine_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdyc3e/job_wants_me_to_develop_rag_search_engine_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T22:42:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe1dec</id>
    <title>Is 5060Ti 16GB and 32GB DDR5 system ram enough to play with local AI for a total rookie?</title>
    <updated>2026-01-16T00:46:11+00:00</updated>
    <author>
      <name>/u/danuser8</name>
      <uri>https://old.reddit.com/user/danuser8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For future proofing would it be better to get a secondary cheap GPU (like 3060) or another 32GB DDR5 RAM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danuser8"&gt; /u/danuser8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe1dec/is_5060ti_16gb_and_32gb_ddr5_system_ram_enough_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe1dec/is_5060ti_16gb_and_32gb_ddr5_system_ram_enough_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe1dec/is_5060ti_16gb_and_32gb_ddr5_system_ram_enough_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T00:46:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdl9za</id>
    <title>Falcon 90M</title>
    <updated>2026-01-15T14:40:27+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;...it's not 90B it's 90M, so you can run it on anything :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-Tiny-90M-Instruct-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-Tiny-90M-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-Tiny-Coder-90M-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-Tiny-Coder-90M-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-Tiny-R-90M-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-Tiny-R-90M-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1-Tiny-Tool-Calling-90M-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1-Tiny-Tool-Calling-90M-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdl9za/falcon_90m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdl9za/falcon_90m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdl9za/falcon_90m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T14:40:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdh28f</id>
    <title>RTX 5070 Ti and RTX 5060 Ti 16 GB no longer manufactured</title>
    <updated>2026-01-15T11:27:15+00:00</updated>
    <author>
      <name>/u/Paramecium_caudatum_</name>
      <uri>https://old.reddit.com/user/Paramecium_caudatum_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nvidia has essentially killed off supply for the RTX 5070 Ti. Also supply of RTX 5060 Ti 16 GB has been significantly reduced. This happened partially due to memory supply shortages. This means that most AIBs will no longer manufacture these GPUs. Prices are already jumping significantly. The 5070 Ti has risen ~$100 over MSRP, and retailers expect further hikes. 8 GB configuration of RTX 5060 Ti remains unaffected. &lt;/p&gt; &lt;p&gt;Credit: Hardware Unboxed &lt;/p&gt; &lt;p&gt;&lt;a href="https://m.youtube.com/watch?v=yteN21aJEvE"&gt;https://m.youtube.com/watch?v=yteN21aJEvE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Paramecium_caudatum_"&gt; /u/Paramecium_caudatum_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdh28f/rtx_5070_ti_and_rtx_5060_ti_16_gb_no_longer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdh28f/rtx_5070_ti_and_rtx_5060_ti_16_gb_no_longer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdh28f/rtx_5070_ti_and_rtx_5060_ti_16_gb_no_longer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T11:27:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdpb2v</id>
    <title>Thanks to you guys, Soprano TTS now supports OpenAI-compatible endpoint, ONNX, ComfyUI, WebUI, and CLI on CUDA, MPS, ROCm, and CPU!</title>
    <updated>2026-01-15T17:10:00+00:00</updated>
    <author>
      <name>/u/eugenekwek</name>
      <uri>https://old.reddit.com/user/eugenekwek</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdpb2v/thanks_to_you_guys_soprano_tts_now_supports/"&gt; &lt;img alt="Thanks to you guys, Soprano TTS now supports OpenAI-compatible endpoint, ONNX, ComfyUI, WebUI, and CLI on CUDA, MPS, ROCm, and CPU!" src="https://preview.redd.it/581sfa20ojdg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3839305a28dd4d4bbbbd3af2e716c6b974829cd3" title="Thanks to you guys, Soprano TTS now supports OpenAI-compatible endpoint, ONNX, ComfyUI, WebUI, and CLI on CUDA, MPS, ROCm, and CPU!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ekwek1/soprano"&gt;https://github.com/ekwek1/soprano&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ekwek/Soprano-1.1-80M"&gt;https://huggingface.co/ekwek/Soprano-1.1-80M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/ekwek/Soprano-TTS"&gt;https://huggingface.co/spaces/ekwek/Soprano-TTS&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;This final day of updates is dedicated to all of you. When I first released Soprano, I had no idea how much support I would get from the community. Within the first day, I received an enormous number PRs adding onto the codebase. I have finally merged most of them, and am happy to announce that you can now run Soprano on nearly any device, and with a wide number of supported inference methods.&lt;/p&gt; &lt;p&gt;Here is a list of all the contributions you guys have made:&lt;/p&gt; &lt;p&gt;WebUI: (from Mateusz-Dera &amp;amp; humair-m)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;soprano-webui &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;CLI: (from bigattichouse)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;soprano &amp;quot;Hello world!&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;OpenAI-compatible endpoint (from bezo97)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uvicorn soprano.server:app &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In addition, several of you have made your own modifications to Soprano, allowing for ONNX and ComfyUI support! Here are some repos that implement this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/SanDiegoDude/ComfyUI-Soprano-TTS"&gt;https://github.com/SanDiegoDude/ComfyUI-Soprano-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/jo-nike/ComfyUI-SopranoTTS"&gt;https://github.com/jo-nike/ComfyUI-SopranoTTS&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/KevinAHM/soprano-web-onnx"&gt;https://github.com/KevinAHM/soprano-web-onnx&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Soprano also supports more than just CUDA devices now, too! It also supports CPU (from bigattichouse), MPS (from visionik), and there is an ROCm PR (from Mateusz-Dera) that can be found here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ekwek1/soprano/pull/29"&gt;https://github.com/ekwek1/soprano/pull/29&lt;/a&gt; &lt;/p&gt; &lt;p&gt;If you have an ROCm device I would love some help for testing this PR!&lt;/p&gt; &lt;p&gt;Finally, I want to thank the countless other contributions to Soprano, including an automatic hallucination detector from ChangeTheConstants and transformers streaming support from sheerun. You all have improved Soprano tremendously!&lt;/p&gt; &lt;p&gt;This will likely be my last update for a bit, since I still have some unfinished business left on the roadmap that will take some time. Iâ€™m not abandoning you guys though! New capabilities for Soprano will be coming soon. :)&lt;/p&gt; &lt;p&gt;- Eugene&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eugenekwek"&gt; /u/eugenekwek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/581sfa20ojdg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdpb2v/thanks_to_you_guys_soprano_tts_now_supports/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdpb2v/thanks_to_you_guys_soprano_tts_now_supports/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T17:10:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdsnul</id>
    <title>translategemma 27b/12b/4b</title>
    <updated>2026-01-15T19:08:59+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdsnul/translategemma_27b12b4b/"&gt; &lt;img alt="translategemma 27b/12b/4b" src="https://b.thumbs.redditmedia.com/ToCVsy0EUgPOXoYv6voopJf0tziQjvqiUpvpgAINVNE.jpg" title="translategemma 27b/12b/4b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TranslateGemma&lt;/strong&gt; is a family of lightweight, state-of-the-art open translation models from Google, based on the &lt;strong&gt;Gemma 3&lt;/strong&gt; family of models.&lt;/p&gt; &lt;p&gt;TranslateGemma models are designed to handle translation tasks across &lt;strong&gt;55 languages&lt;/strong&gt;. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art translation models and helping foster innovation for everyone.&lt;/p&gt; &lt;h1&gt;Inputs and outputs&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Input:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Text string, representing the text to be translated&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Images,&lt;/strong&gt; normalized to 896 x 896 resolution and encoded to 256 tokens each&lt;/li&gt; &lt;li&gt;Total input context of 2K tokens&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Text translated into the target language&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/translategemma-27b-it"&gt;https://huggingface.co/google/translategemma-27b-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/translategemma-12b-it"&gt;https://huggingface.co/google/translategemma-12b-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/translategemma-4b-it"&gt;https://huggingface.co/google/translategemma-4b-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aza4kprrakdg1.png?width=1372&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bed28fac0a9878478a7cec3f0eac6c1c585b8a85"&gt;https://preview.redd.it/aza4kprrakdg1.png?width=1372&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bed28fac0a9878478a7cec3f0eac6c1c585b8a85&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdsnul/translategemma_27b12b4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdsnul/translategemma_27b12b4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdsnul/translategemma_27b12b4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T19:08:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qeclut</id>
    <title>Is there a TTS ROCm or Vulkan yet ? - 2026</title>
    <updated>2026-01-16T10:34:05+00:00</updated>
    <author>
      <name>/u/uber-linny</name>
      <uri>https://old.reddit.com/user/uber-linny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a TTS ROCm or Vulkan yet ? was hoping that i could move away from kokoro cpu &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uber-linny"&gt; /u/uber-linny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeclut/is_there_a_tts_rocm_or_vulkan_yet_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeclut/is_there_a_tts_rocm_or_vulkan_yet_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qeclut/is_there_a_tts_rocm_or_vulkan_yet_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T10:34:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdok2i</id>
    <title>google/translategemma</title>
    <updated>2026-01-15T16:42:58+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/google/translategemma"&gt;https://huggingface.co/collections/google/translategemma&lt;/a&gt;&lt;/p&gt; &lt;p&gt;tech report: &lt;a href="https://arxiv.org/abs/2601.09012"&gt;https://arxiv.org/abs/2601.09012&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdok2i/googletranslategemma/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdok2i/googletranslategemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdok2i/googletranslategemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T16:42:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdna3t</id>
    <title>7x Longer Context Reinforcement Learning in Unsloth</title>
    <updated>2026-01-15T15:56:40+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdna3t/7x_longer_context_reinforcement_learning_in/"&gt; &lt;img alt="7x Longer Context Reinforcement Learning in Unsloth" src="https://preview.redd.it/nmkee12vbjdg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3cd97dbf853be6596556f70c467d1dccc0cc22a1" title="7x Longer Context Reinforcement Learning in Unsloth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We're excited to show how Unsloth now enables &lt;strong&gt;7x longer context lengths&lt;/strong&gt; (up to 12x) for Reinforcement Learning! By using 3 new techniques we developed, we enable you to train gpt-oss 20b QLoRA up to &lt;strong&gt;20K context on a 24Gb card&lt;/strong&gt; - all with &lt;strong&gt;no accuracy degradation&lt;/strong&gt;. Unsloth GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For larger GPUs, Unsloth now trains gpt-oss QLoRA with &lt;strong&gt;380K context&lt;/strong&gt; on a single 192GB NVIDIA B200 GPU&lt;/li&gt; &lt;li&gt;Qwen3-8B GRPO reaches &lt;strong&gt;110K context&lt;/strong&gt; on an 80GB VRAM H100 via vLLM and QLoRA, and &lt;strong&gt;65K&lt;/strong&gt; for gpt-oss with BF16 LoRA.&lt;/li&gt; &lt;li&gt;Unsloth GRPO RL runs with Llama, Gemma &amp;amp; all models auto support longer contexts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also, all features in Unsloth can be combined together and work well together:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Unsloth's &lt;a href="https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/memory-efficient-rl"&gt;weight-sharing&lt;/a&gt; feature with vLLM and our Standby Feature in &lt;a href="https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/memory-efficient-rl"&gt;Memory Efficient RL&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Unsloth's &lt;a href="https://unsloth.ai/docs/models/gpt-oss-how-to-run-and-fine-tune/long-context-gpt-oss-training"&gt;Flex Attention&lt;/a&gt; for long context gpt-oss and our &lt;a href="https://unsloth.ai/docs/new/500k-context-length-fine-tuning"&gt;500K Context Training&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Float8 training in &lt;a href="https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/fp8-reinforcement-learning"&gt;FP8 RL&lt;/a&gt; and Unsloth's &lt;a href="https://unsloth.ai/blog/long-context"&gt;async gradient checkpointing&lt;/a&gt; and much more&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You can read our educational blogpost for detailed analysis, benchmarks and more: &lt;a href="https://unsloth.ai/docs/new/grpo-long-context"&gt;https://unsloth.ai/docs/new/grpo-long-context&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And you can of course train any model using our new features and kernels via our free fine-tuning notebooks: &lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks"&gt;https://docs.unsloth.ai/get-started/unsloth-notebooks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some free Colab notebooks below which has the 7x longer context support backed in:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B"&gt;gpt-oss-20b&lt;/a&gt;-GRPO.ipynb) GSPO Colab&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_VL_(8B"&gt;Qwen3-VL-8B&lt;/a&gt;-Vision-GRPO.ipynb) Vision RL&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_8B_FP8_GRPO.ipynb"&gt;Qwen3-8B - FP8&lt;/a&gt; L4 GPU&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;To update Unsloth to automatically make training faster, do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth_zoo &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And to enable GRPO runs in Unsloth, do&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import os os.environ[&amp;quot;UNSLOTH_VLLM_STANDBY&amp;quot;] = &amp;quot;1&amp;quot; # Standby = extra 30% context lengths! from unsloth import FastLanguageModel import torch max_seq_length = 20000 # Can increase for longer reasoning traces lora_rank = 32 # Larger rank = smarter, but slower model, tokenizer = FastLanguageModel.from_pretrained( model_name = &amp;quot;unsloth/Qwen3-4B-Base&amp;quot;, max_seq_length = max_seq_length, load_in_4bit = False, # False for LoRA 16bit fast_inference = True, # Enable vLLM fast inference max_lora_rank = lora_rank, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hope you all have a great rest of the week and thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nmkee12vbjdg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdna3t/7x_longer_context_reinforcement_learning_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdna3t/7x_longer_context_reinforcement_learning_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T15:56:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdtvgs</id>
    <title>Not as impressive as most here, but really happy I made it in time!</title>
    <updated>2026-01-15T19:53:15+00:00</updated>
    <author>
      <name>/u/Kahvana</name>
      <uri>https://old.reddit.com/user/Kahvana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdtvgs/not_as_impressive_as_most_here_but_really_happy_i/"&gt; &lt;img alt="Not as impressive as most here, but really happy I made it in time!" src="https://preview.redd.it/g997soqdgkdg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=572838999e31352821862a1f71f5bd35cb328147" title="Not as impressive as most here, but really happy I made it in time!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm in the Netherlands, I apologize in advance for my grammar (Happy to be corrected!), not using AI for translation.&lt;/p&gt; &lt;p&gt;Over here, getting cards is increasingly more difficult and prices are quite steep.&lt;/p&gt; &lt;p&gt;It was a bit of a gamble to get the second GPU; I had the RTX 5060 Ti on order for 509EU by Paradigit but it wasn't delivered for 2 weeks straight, and they still aren't sure when supply will arrive. Cancelled the order and payed the premium for Azerty's model in stock (600EU), but it arrived the next day!&lt;/p&gt; &lt;p&gt;So if you're in the Netherlands, I recommend calling up the store to ask about stock availability in advance. The listings on Tweakers wasn't accurate for this card.&lt;/p&gt; &lt;p&gt;Today the announcement from HardwareUnboxed came that the RTX 5060 Ti 16GB is becoming unavailable. Really happy it arrived just in time.&lt;/p&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen 5 9600X&lt;/li&gt; &lt;li&gt;Crosair Vengence 96GB (2x48) DDR5-6000 CL30&lt;/li&gt; &lt;li&gt;ASUS ProArt X870E-Creator Wifi&lt;/li&gt; &lt;li&gt;2x ASUS Prime RTX 5060 Ti 16GB&lt;/li&gt; &lt;li&gt;BeQuiet! Dark Power 13 850W&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Notes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I don't use the CPU for inference much (embeddings) and the PCI lanes are the same across all models, so I went with the lowest TDP.&lt;/li&gt; &lt;li&gt;Wished I had more (192GB) for dataset generation / RAG but I can hold off.&lt;/li&gt; &lt;li&gt;Picked the motherboad specifically for it's PCI-E 5.0 splitting to get the most out of the GPUs.&lt;/li&gt; &lt;li&gt;Power draw during inference is ~300W.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kahvana"&gt; /u/Kahvana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g997soqdgkdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdtvgs/not_as_impressive_as_most_here_but_really_happy_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdtvgs/not_as_impressive_as_most_here_but_really_happy_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T19:53:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe7a3m</id>
    <title>Will the AI bubble bursting be good or bad for open-weights? What do you think?</title>
    <updated>2026-01-16T05:21:03+00:00</updated>
    <author>
      <name>/u/RandumbRedditor1000</name>
      <uri>https://old.reddit.com/user/RandumbRedditor1000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I could see it both ways. On one hand, RAM, GPUs, and SSDs could see their prices return to normal, but on the other hand, it could lead to less AI being developed and released overall, especially from the major tech companies such as Google or Meta.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandumbRedditor1000"&gt; /u/RandumbRedditor1000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe7a3m/will_the_ai_bubble_bursting_be_good_or_bad_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe7a3m/will_the_ai_bubble_bursting_be_good_or_bad_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe7a3m/will_the_ai_bubble_bursting_be_good_or_bad_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T05:21:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qdrf3o</id>
    <title>Nemotron-3-nano:30b is a spectacular general purpose local LLM</title>
    <updated>2026-01-15T18:24:08+00:00</updated>
    <author>
      <name>/u/DrewGrgich</name>
      <uri>https://old.reddit.com/user/DrewGrgich</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just want to sing the praises of this model. I am stunned at how intelligent it is for a 30b model. Comparing it to Llama 3.3:70b, I have yet to find a general purpose question that Nemotron hasn't answered better. It is quite robotic so I won't be using it for creative or chat purposes. Everything else though has been stellar. &lt;/p&gt; &lt;p&gt;If you have the capacity to give it a try, I highly recommend it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrewGrgich"&gt; /u/DrewGrgich &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdrf3o/nemotron3nano30b_is_a_spectacular_general_purpose/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qdrf3o/nemotron3nano30b_is_a_spectacular_general_purpose/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qdrf3o/nemotron3nano30b_is_a_spectacular_general_purpose/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-15T18:24:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe3p8d</id>
    <title>MiniMax M2.2 Coming Soon. Confirmed by Head of Engineering @MiniMax_AI</title>
    <updated>2026-01-16T02:29:49+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe3p8d/minimax_m22_coming_soon_confirmed_by_head_of/"&gt; &lt;img alt="MiniMax M2.2 Coming Soon. Confirmed by Head of Engineering @MiniMax_AI" src="https://preview.redd.it/r2y0g60chmdg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b4997ba8fe2fc1231baad6317e2eca9f7350374" title="MiniMax M2.2 Coming Soon. Confirmed by Head of Engineering @MiniMax_AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r2y0g60chmdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe3p8d/minimax_m22_coming_soon_confirmed_by_head_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe3p8d/minimax_m22_coming_soon_confirmed_by_head_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T02:29:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe9xfi</id>
    <title>New FLUX.2 [Klein] 9B is INSANELY Fast</title>
    <updated>2026-01-16T07:48:50+00:00</updated>
    <author>
      <name>/u/Lopsided_Dot_4557</name>
      <uri>https://old.reddit.com/user/Lopsided_Dot_4557</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;BFL is has done a good job with this new Klein model, though in my testing text-to-image in distilled flavor is the best:&lt;/p&gt; &lt;p&gt;ðŸ”¹ Sub-second inference on RTX 4090 hardware&lt;/p&gt; &lt;p&gt;ðŸ”¹ 9B parameters matching models 5x its size&lt;/p&gt; &lt;p&gt;ðŸ”¹ Step-distilled from 50 â†’ 4 steps, zero quality loss&lt;/p&gt; &lt;p&gt;ðŸ”¹ Unified text-to-image + multi-reference editing&lt;/p&gt; &lt;p&gt;HF Model: &lt;a href="https://huggingface.co/black-forest-labs/FLUX.2-klein-base-9B"&gt;black-forest-labs/FLUX.2-klein-base-9B Â· Hugging Face&lt;/a&gt;&lt;br /&gt; Detailed testing is here: &lt;a href="https://youtu.be/j3-vJuVwoWs?si=XPh7_ZClL8qoKFhl"&gt;https://youtu.be/j3-vJuVwoWs?si=XPh7_ZClL8qoKFhl&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Dot_4557"&gt; /u/Lopsided_Dot_4557 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe9xfi/new_flux2_klein_9b_is_insanely_fast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe9xfi/new_flux2_klein_9b_is_insanely_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe9xfi/new_flux2_klein_9b_is_insanely_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T07:48:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe5p2f</id>
    <title>Black Forest Labs releases FLUX.2 [klein]</title>
    <updated>2026-01-16T04:01:07+00:00</updated>
    <author>
      <name>/u/Old-School8916</name>
      <uri>https://old.reddit.com/user/Old-School8916</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Black Forest Labs released their new FLUX.2 [klein] model&lt;/p&gt; &lt;p&gt;&lt;a href="https://bfl.ai/blog/flux2-klein-towards-interactive-visual-intelligence"&gt;https://bfl.ai/blog/flux2-klein-towards-interactive-visual-intelligence&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;FLUX.2 [klein]: Towards Interactive Visual Intelligence&lt;/p&gt; &lt;p&gt;Today, we release the FLUX.2 [klein] model family, our fastest image models to date. FLUX.2 [klein] unifies generation and editing in a single compact architecture, delivering state-of-the-art quality with end-to-end inference as low as under a second. Built for applications that require real-time image generation without sacrificing quality, and runs on consumer hardware with as little as 13GB VRAM.&lt;/p&gt; &lt;p&gt;The klein name comes from the German word for &amp;quot;small&amp;quot;, reflecting both the compact model size and the minimal latency. But FLUX.2 [klein] is anything but limited. These models deliver exceptional performance in text-to-image generation, image editing and multi-reference generation, typically reserved for much larger models.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;What's New&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Sub-second inference. Generate or edit images in under 0.5s on modern hardware.&lt;/li&gt; &lt;li&gt;Photorealistic outputs and high diversity, especially in the base variants.&lt;/li&gt; &lt;li&gt;Unified generation and editing. Text-to-image, image editing, and multi-reference support in a single model while delivering frontier performance.&lt;/li&gt; &lt;li&gt;Runs on consumer GPUs. The 4B model fits in ~13GB VRAM (RTX 3090/4070 and above).&lt;/li&gt; &lt;li&gt;Developer-friendly &amp;amp; Accessible: Apache 2.0 on 4B models, open weights for 9B models. Full open weights for customization and fine-tuning.&lt;/li&gt; &lt;li&gt;API and open weights. Production-ready API or run locally with full weights.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Resources&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Try it&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://bfl.ai/models/flux-2-klein#try-demo"&gt;Demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://bfl.ai/play"&gt;Playground&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/spaces/black-forest-labs/FLUX.2-klein-9B"&gt;HF Space for [klein] 9B&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/black-forest-labs/FLUX.2-klein-4B"&gt;HF Space for [klein] 4B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Build with it&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.bfl.ai/flux_2/flux2_overview#flux-2-%5Bklein%5D-models"&gt;Documentation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/black-forest-labs/flux2"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/black-forest-labs/flux2"&gt;Model Weights&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Learn more&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://bfl.ai/models/flux-2-klein"&gt;https://bfl.ai/models/flux-2-klein&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-School8916"&gt; /u/Old-School8916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe5p2f/black_forest_labs_releases_flux2_klein/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe5p2f/black_forest_labs_releases_flux2_klein/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe5p2f/black_forest_labs_releases_flux2_klein/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T04:01:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe4so5</id>
    <title>Dang, M2 drives are the new DDR5 apparently.</title>
    <updated>2026-01-16T03:18:52+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe4so5/dang_m2_drives_are_the_new_ddr5_apparently/"&gt; &lt;img alt="Dang, M2 drives are the new DDR5 apparently." src="https://preview.redd.it/c8pq1jm6qmdg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9e429f423f72a245e2fe28f8b56d773252a3eec" title="Dang, M2 drives are the new DDR5 apparently." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c8pq1jm6qmdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe4so5/dang_m2_drives_are_the_new_ddr5_apparently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe4so5/dang_m2_drives_are_the_new_ddr5_apparently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T03:18:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe0cxc</id>
    <title>Latest upgradeâ€¦A100 40 GB</title>
    <updated>2026-01-16T00:03:21+00:00</updated>
    <author>
      <name>/u/inserterikhere</name>
      <uri>https://old.reddit.com/user/inserterikhere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/"&gt; &lt;img alt="Latest upgradeâ€¦A100 40 GB" src="https://preview.redd.it/f66wnmearldg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=703e9ae4724a0bb0a84546215e98301f06d28541" title="Latest upgradeâ€¦A100 40 GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Originally this was my gaming rig but I went ITX and basically bought a new computer. So I had the case, fans, AIO, 64 GB DDR5, motherboard, PSU, and 3080 (upgraded to 5070ti RIP). I was going to sell these parts, but I started running models on my 5070ti and eventually I wanted to start running larger models. I found a 3090 on eBay for $680, and 7950x for $350. I put that together with the parts and itâ€™s been a great AI rig for me. I really didnâ€™t plan on upgrading this for a while, especially now with the current price surges. Welp, I saw an A100 get listed for $1000 on eBay. The catch? Listed for parts, and the description just said â€œcard reports CUDA errorâ€. So I figured it was worth the risk (for me), I couldâ€™ve probably sold it for the price I paid. Well, I swapped out the 3080 and on the first boot it was recognized instantly by nvidia-smi. I was able to run and train models immediately. Nice. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inserterikhere"&gt; /u/inserterikhere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f66wnmearldg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T00:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe2i88</id>
    <title>My story of underestimating /r/LocalLLaMA's thirst for VRAM</title>
    <updated>2026-01-16T01:36:54+00:00</updated>
    <author>
      <name>/u/EmPips</name>
      <uri>https://old.reddit.com/user/EmPips</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/"&gt; &lt;img alt="My story of underestimating /r/LocalLLaMA's thirst for VRAM" src="https://preview.redd.it/lwod7dtv7mdg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=660b9563f79c6dad7bb50c952f2b76bf9062955b" title="My story of underestimating /r/LocalLLaMA's thirst for VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmPips"&gt; /u/EmPips &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lwod7dtv7mdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T01:36:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
