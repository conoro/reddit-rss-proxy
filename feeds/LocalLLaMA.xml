<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-09T08:25:34+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1o1x82c</id>
    <title>Small text to text model for RTX 3070?</title>
    <updated>2025-10-09T04:56:59+00:00</updated>
    <author>
      <name>/u/eddie__b</name>
      <uri>https://old.reddit.com/user/eddie__b</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using Lm Studio to host a local server, I need a small model to generate text only, I would need to setup at maximum 220 characters on each reply. The more creative, the better. If it supports portuguese, it's perfect.&lt;/p&gt; &lt;p&gt;What is the best model I can use on LM studio to run that?&lt;/p&gt; &lt;p&gt;Thank you very much! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eddie__b"&gt; /u/eddie__b &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1x82c/small_text_to_text_model_for_rtx_3070/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1x82c/small_text_to_text_model_for_rtx_3070/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1x82c/small_text_to_text_model_for_rtx_3070/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T04:56:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1e04z</id>
    <title>Less is More: Recursive Reasoning with Tiny Networks (7M model beats R1, Gemini 2.5 Pro on ARC AGI)</title>
    <updated>2025-10-08T15:40:05+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Less is More: Recursive Reasoning with Tiny Network&lt;/strong&gt;s, from Samsung Montréal by Alexia Jolicoeur-Martineau, shows how a &lt;strong&gt;7M-parameter Tiny Recursive Model (TRM)&lt;/strong&gt; outperforms trillion-parameter LLMs on hard reasoning benchmarks. TRM learns by &lt;strong&gt;recursively refining its own answers&lt;/strong&gt; using two internal memories: a latent reasoning state (&lt;em&gt;z&lt;/em&gt;) and a current answer (&lt;em&gt;y&lt;/em&gt;). &lt;/p&gt; &lt;p&gt;No chain-of-thought, no fixed-point math, no biological hierarchies. It beats the Hierarchical Reasoning Model (HRM), which used two networks and heavy training tricks. Results: &lt;strong&gt;87% on Sudoku-Extreme&lt;/strong&gt;, &lt;strong&gt;85% on Maze-Hard&lt;/strong&gt;, &lt;strong&gt;45% on ARC-AGI-1&lt;/strong&gt;, &lt;strong&gt;8% on ARC-AGI-2,&lt;/strong&gt; surpassing Gemini 2.5 Pro, DeepSeek R1, and o3-mini despite having &amp;lt;0.01% their size.&lt;br /&gt; &lt;strong&gt;In short:&lt;/strong&gt; recursion, not scale, drives reasoning.&lt;/p&gt; &lt;p&gt;Paper : &lt;a href="https://arxiv.org/html/2510.04871v1"&gt;https://arxiv.org/html/2510.04871v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Summary : &lt;a href="https://youtu.be/wQbEITW7BMw?si=U3SFKAGYF5K06fFw"&gt;https://youtu.be/wQbEITW7BMw?si=U3SFKAGYF5K06fFw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1e04z/less_is_more_recursive_reasoning_with_tiny/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1e04z/less_is_more_recursive_reasoning_with_tiny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1e04z/less_is_more_recursive_reasoning_with_tiny/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T15:40:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1tqbr</id>
    <title>Built a 1288x RTFx Parakeet Speech-to-Text server... Enjoy!</title>
    <updated>2025-10-09T01:51:43+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1tqbr/built_a_1288x_rtfx_parakeet_speechtotext_server/"&gt; &lt;img alt="Built a 1288x RTFx Parakeet Speech-to-Text server... Enjoy!" src="https://b.thumbs.redditmedia.com/woIKQbRW9RnarpgCEi4HCXGKP_5kq7vpu3Q0s56h6vg.jpg" title="Built a 1288x RTFx Parakeet Speech-to-Text server... Enjoy!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/mr543y28uztf1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5be73739a19e7f615327e84360a7ce447626ad3"&gt;https://preview.redd.it/mr543y28uztf1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5be73739a19e7f615327e84360a7ce447626ad3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Needed to do a little mass-transcription so I hacked up a batching fastAPI Parakeet server and pushed it to the limit. Under ideal circumstances it manages up to 1,288x realtime on a 4090. It's using Parakeet 0.2 so it's English-only (feel free to hack together a 0.3 version if you need other languages, but note that you'll have to make some changes because v0.3 doesn't use the same code).&lt;/p&gt; &lt;p&gt;Built it out of an existing fastapi parakeet server, so it has a regular batching fastAPI that has VAD/streaming/automatic chunking at the /transcribe endpoint, and mass batch generation at the /transcribe_batch endpoint if you want to mass-gen. Fastest batching happens if you prepare all the audio on your end at 16hz and send it in as batches of 128 1 minute audio files, but you can throw a huge file at the /transcribe_batch endpoint and it'll chop it up on the server-end and handle all the chunking for you.&lt;/p&gt; &lt;p&gt;This is ideal for a 24gb card but will easily run on an 8gb vram card as long as you keep your batch sizes down to 4-8 or less and should still provide well-over-realtime speeds on that hardware (it'll run out of vram if you push batching too far).&lt;/p&gt; &lt;p&gt;I've got it all set up to run inside a docker, just set it up and docker compose up for easy deployment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Deveraux-Parker/Nvidia_parakeet-tdt-0.6b-v2-FAST-BATCHING-API-1200x-RTFx/tree/main"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1tqbr/built_a_1288x_rtfx_parakeet_speechtotext_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1tqbr/built_a_1288x_rtfx_parakeet_speechtotext_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T01:51:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1igj2</id>
    <title>MoE models iGPU benchmarks</title>
    <updated>2025-10-08T18:20:55+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Follow up to request for testing a few other MoE models size 10-35B:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1na96gx/moe_models_tested_on_minipc_igpu_with_vulkan/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1na96gx/moe_models_tested_on_minipc_igpu_with_vulkan/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;System: Kubuntu 25.10 OS, Kernel 6.17.0-5-generic with 64GB DDR5 ram. AMD Radeon Graphics (RADV REMBRANDT) Ryzen 6800H and 680M iGPU. Links to model HF page near end of post. &lt;/p&gt; &lt;p&gt;aquif-3.5-a0.6b-preview-q8_0&lt;/p&gt; &lt;p&gt;Ling-Coder-lite.i1-Q4_K_M&lt;/p&gt; &lt;p&gt;Ling-Coder-Lite-Q4_K_M&lt;/p&gt; &lt;p&gt;LLaDA-MoE-7B-A1B-Base.i1-Q4_K_M&lt;/p&gt; &lt;p&gt;LLaDA-MoE-7B-A1B-Instruct.i1-Q4_K_M&lt;/p&gt; &lt;p&gt;OLMoE-1B-7B-0125.i1-Q4_K_M&lt;/p&gt; &lt;p&gt;OLMoE-1B-7B-0125-Instruct-Q4_K_M&lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B-Instruct-2507-Q4_1&lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B-Thinking-2507-Q4_K_M&lt;/p&gt; &lt;p&gt;Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL&lt;/p&gt; &lt;p&gt;Ring-lite-2507.i1-Q4_1 Ring-lite-2507.i1-Q4_K_M&lt;/p&gt; &lt;p&gt;Llama.cpp Vulkan build: 152729f8 (6565)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;2.59 GiB&lt;/td&gt; &lt;td align="left"&gt;2.61 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1296.87 ± 11.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;2.59 GiB&lt;/td&gt; &lt;td align="left"&gt;2.61 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;103.45 ± 1.25&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;231.96 ± 0.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.94 ± 0.18&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;232.71 ± 0.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.21 ± 0.53&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;399.54 ± 5.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;64.91 ± 0.21&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;396.74 ± 1.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;64.60 ± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;487.74 ± 3.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;78.33 ± 0.47&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;484.79 ± 4.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;78.76 ± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt; &lt;td align="left"&gt;17.87 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;171.65 ± 0.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt; &lt;td align="left"&gt;17.87 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;27.04 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.28 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;142.18 ± 1.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.28 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;28.79 ± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;16.45 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;137.46 ± 0.66&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;16.45 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;29.86 ± 0.12&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_1&lt;/td&gt; &lt;td align="left"&gt;9.84 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;292.10 ± 0.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_1&lt;/td&gt; &lt;td align="left"&gt;9.84 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.86 ± 0.40&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;234.03 ± 0.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.75 ± 0.13&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Order with models for table below:&lt;/p&gt; &lt;p&gt;aquif-3.5-a0.6b-preview-q8_0 &lt;/p&gt; &lt;p&gt;Ling-Coder-lite.i1-Q4_K_M &lt;/p&gt; &lt;p&gt;Ling-Coder-Lite-Q4_K_M &lt;/p&gt; &lt;p&gt;LLaDA-MoE-7B-A1B-Base.i1-Q4_K_M &lt;/p&gt; &lt;p&gt;LLaDA-MoE-7B-A1B-Instruct.i1-Q4_K_M &lt;/p&gt; &lt;p&gt;OLMoE-1B-7B-0125.i1-Q4_K_M &lt;/p&gt; &lt;p&gt;OLMoE-1B-7B-0125-Instruct-Q4_K_M &lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B-Instruct-2507-Q4_1 &lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B-Thinking-2507-Q4_K_M &lt;/p&gt; &lt;p&gt;Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL &lt;/p&gt; &lt;p&gt;Ring-lite-2507.i1-Q4_1 &lt;/p&gt; &lt;p&gt;Ring-lite-2507.i1-Q4_K_M&lt;/p&gt; &lt;p&gt;Here is the combined data from all the tables into a single Markdown table:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;2.59 GiB&lt;/td&gt; &lt;td align="left"&gt;2.61 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1296.87 ± 11.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;2.59 GiB&lt;/td&gt; &lt;td align="left"&gt;2.61 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;103.45 ± 1.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;231.96 ± 0.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.94 ± 0.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;232.71 ± 0.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.21 ± 0.53&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;399.54 ± 5.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;64.91 ± 0.21&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;396.74 ± 1.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llada-moe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;4.20 GiB&lt;/td&gt; &lt;td align="left"&gt;7.36 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;64.60 ± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;487.74 ± 3.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;78.33 ± 0.47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;484.79 ± 4.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;olmoe A1.7B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;3.92 GiB&lt;/td&gt; &lt;td align="left"&gt;6.92 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;78.76 ± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt; &lt;td align="left"&gt;17.87 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;171.65 ± 0.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_1&lt;/td&gt; &lt;td align="left"&gt;17.87 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;27.04 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.28 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;142.18 ± 1.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.28 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;28.79 ± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;16.45 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;137.46 ± 0.66&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;16.45 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;29.86 ± 0.12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_1&lt;/td&gt; &lt;td align="left"&gt;9.84 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;292.10 ± 0.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_1&lt;/td&gt; &lt;td align="left"&gt;9.84 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.86 ± 0.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;234.03 ± 0.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bailingmoe 16B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;10.40 GiB&lt;/td&gt; &lt;td align="left"&gt;16.80 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;35.75 ± 0.13&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Hyperlinks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/mradermacher/aquif-3.5-A4B-Think-GGUF"&gt;aquif-3.5-A4B-Think&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/mradermacher/aquif-3-moe-17b-a2.8b-GGUF?show_file_info=aquif-3-moe-17b-a2.8b.Q4_K_M.gguf"&gt;aquif-3-moe-17b-a2.8b-i1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/gabriellarson/Moonlight-16B-A3B-Instruct-GGUF"&gt;Moonlight-16B-A3B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/gpt-oss-20b-GGUF"&gt;gpt-oss-20b&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF"&gt;ERNIE-4.5-21B-A3B-PT&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PowerInfer/SmallThinker-21BA3B-Instruct-GGUF"&gt;SmallThinker-21BA3B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/mradermacher/Ling-lite-1.5-2507-GGUF"&gt;Ling-lite-1.5-2507&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lovedheart/Ling-mini-2.0-GGUF"&gt;Ling-mini-2.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/mradermacher/Ling-Coder-lite-i1-GGUF"&gt;Ling-Coder-lite&lt;/a&gt; &lt;a href="https://huggingface.co/redponike/Ling-Coder-lite-GGUF?show_file_info=Ling-Coder-Lite-Q4_K_M.gguf"&gt;2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/mradermacher/Ring-lite-2507-i1-GGUF"&gt;Ring-lite-2507&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lovedheart/Ring-mini-2.0-GGUF"&gt;Ring-mini-2.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5"&gt;Ming-Lite-Omni-1.5&lt;/a&gt; (No GGUF yet)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF"&gt;Qwen3-30B-A3B-Instruct-2507&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF"&gt;Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"&gt;Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/GroveMoE-Inst"&gt;GroveMoE-Inst &lt;/a&gt;(No GGUF yet)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/models?search=FlexOlmo-7x7B-1T"&gt;FlexOlmo-7x7B-1T&lt;/a&gt; (No GGUF yet)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/allenai/FlexOlmo-7x7B-1T-RT"&gt;FlexOlmo-7x7B-1T-RT&lt;/a&gt; (No GGUF yet)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1igj2/moe_models_igpu_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1igj2/moe_models_igpu_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1igj2/moe_models_igpu_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T18:20:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1yw57</id>
    <title>Best Vision Model for Building Interiors?</title>
    <updated>2025-10-09T06:40:02+00:00</updated>
    <author>
      <name>/u/AffectionateTop7221</name>
      <uri>https://old.reddit.com/user/AffectionateTop7221</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I am looking for a vision model that can accurately describe/identify the entry points of an image (such as hallways, doors, windows, etc). Any ideas as to which model would work the best for this? Or if I may need to train my own? Many thanks for the help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AffectionateTop7221"&gt; /u/AffectionateTop7221 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1yw57/best_vision_model_for_building_interiors/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1yw57/best_vision_model_for_building_interiors/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1yw57/best_vision_model_for_building_interiors/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T06:40:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1mnyh</id>
    <title>Made a chatbot UI with a 'lazy mode' to auto-generate user responses</title>
    <updated>2025-10-08T20:38:42+00:00</updated>
    <author>
      <name>/u/BlueLemonPixel</name>
      <uri>https://old.reddit.com/user/BlueLemonPixel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1mnyh/made_a_chatbot_ui_with_a_lazy_mode_to/"&gt; &lt;img alt="Made a chatbot UI with a 'lazy mode' to auto-generate user responses" src="https://external-preview.redd.it/azR4OWEzbGl4eHRmMdTM1bqWEqBz6l6PjFkrf1WHqI5ifgt22W6bQrkljKJY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=649128e82aba260aaf5e7a8546fa9204d19108b1" title="Made a chatbot UI with a 'lazy mode' to auto-generate user responses" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a series of small experiments using LLMs.&lt;/p&gt; &lt;p&gt;For the first one, I made a typical &lt;strong&gt;chatbot&lt;/strong&gt; UI but &lt;strong&gt;with a twist&lt;/strong&gt;. You can enable a &lt;strong&gt;&amp;quot;lazy mode&amp;quot;&lt;/strong&gt;, that writes the user interaction on your behalf.&lt;/p&gt; &lt;p&gt;You can configure which models you want to use in a YAML file.&lt;/p&gt; &lt;p&gt;For this video I'm using gemini flash 2.5 for the main answers and gemma3:12b via ollama for the user prompts. I could have used the same model for both, but I was just experimenting a bit!&lt;br /&gt; It's fun to watch the chat go on and on for a while :)&lt;/p&gt; &lt;p&gt;My plan is to put this online and eventually open-source some of these mini experiments.&lt;br /&gt; I'd love to hear what you think about this and the more to come! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlueLemonPixel"&gt; /u/BlueLemonPixel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/o9p2k4lixxtf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1mnyh/made_a_chatbot_ui_with_a_lazy_mode_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1mnyh/made_a_chatbot_ui_with_a_lazy_mode_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T20:38:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1s7q8</id>
    <title>An open sourced language diffusion model by SF</title>
    <updated>2025-10-09T00:39:16+00:00</updated>
    <author>
      <name>/u/Striking-Warning9533</name>
      <uri>https://old.reddit.com/user/Striking-Warning9533</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Salesforce/CoDA-v0-Instruct"&gt;https://huggingface.co/Salesforce/CoDA-v0-Instruct&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking-Warning9533"&gt; /u/Striking-Warning9533 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1s7q8/an_open_sourced_language_diffusion_model_by_sf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1s7q8/an_open_sourced_language_diffusion_model_by_sf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1s7q8/an_open_sourced_language_diffusion_model_by_sf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T00:39:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1u9ia</id>
    <title>A CLI to scrape pages for agents by piggybacking on your browser fingerprint</title>
    <updated>2025-10-09T02:17:38+00:00</updated>
    <author>
      <name>/u/8ta4</name>
      <uri>https://old.reddit.com/user/8ta4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I keep hitting a wall with bot detection when trying to get live web data for agents.&lt;/p&gt; &lt;p&gt;So I built &lt;a href="https://github.com/8ta4/see"&gt;a CLI&lt;/a&gt; that tells a companion extension to fetch a page. The idea was to control my day-to-day browser to piggyback on its static fingerprint.&lt;/p&gt; &lt;p&gt;This isn't for serious scraping. Forget residential proxies or Clay. I designed this for developers who are just scraping by.&lt;/p&gt; &lt;p&gt;My ideal outcome is for someone to point me to an existing open-source project that does this better, so I can abandon this. If nothing better exists, maybe this solution is useful to someone else facing the same problem.&lt;/p&gt; &lt;p&gt;The tool is limited by design.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;It doesn't scale. It's built for grabbing one page at a time.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It's dumb. It just gets the &lt;code&gt;innerText&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The behavioral fingerprint is sterile. It doesn't fake any mouse or keyboard activity.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Is a tool that just grabs text about to be subsumed by agents that can interact with pages?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/8ta4"&gt; /u/8ta4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1u9ia/a_cli_to_scrape_pages_for_agents_by_piggybacking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1u9ia/a_cli_to_scrape_pages_for_agents_by_piggybacking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1u9ia/a_cli_to_scrape_pages_for_agents_by_piggybacking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T02:17:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1fdev</id>
    <title>RTX 4090 48GB price drop?</title>
    <updated>2025-10-08T16:29:50+00:00</updated>
    <author>
      <name>/u/skyfallboom</name>
      <uri>https://old.reddit.com/user/skyfallboom</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm seeing many modified 4090 48GB cards listed for half the price of an RTX PRO 6000 96GB. $4,500 vs $9,000. &lt;/p&gt; &lt;p&gt;It doesn't make sense to purchase those when a new 96GB card gives you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;as much memory in a single PCIe slot&lt;/li&gt; &lt;li&gt;better power efficiency &lt;/li&gt; &lt;li&gt;a true warranty&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Who purchases those at this price? The RTX PRO 6000 isn't out stock. &lt;/p&gt; &lt;p&gt;Do you think too many 4090 got modified and we're going to see a price drop soon?&lt;/p&gt; &lt;p&gt;Also, not in the same ballpark but the Intel B60 is supposed to come this year. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skyfallboom"&gt; /u/skyfallboom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1fdev/rtx_4090_48gb_price_drop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1fdev/rtx_4090_48gb_price_drop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1fdev/rtx_4090_48gb_price_drop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T16:29:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1jh28</id>
    <title>Free 1,000 CPU + 100 GPU hours for testers. I open sourced the world's simplest cluster compute software</title>
    <updated>2025-10-08T18:54:50+00:00</updated>
    <author>
      <name>/u/Ok_Post_149</name>
      <uri>https://old.reddit.com/user/Ok_Post_149</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everybody,&lt;/p&gt; &lt;p&gt;I’ve always struggled to get data scientists and analysts to scale their code in the cloud. Almost every time, they’d have to hand it over to DevOps, the backlog would grow, and overall throughput would tank.&lt;/p&gt; &lt;p&gt;So I built &lt;a href="https://docs.burla.dev/"&gt;Burla&lt;/a&gt;, the simplest cluster compute software that lets even Python beginners run code on massive clusters in the cloud. It’s one function with two parameters: the function and the inputs. You can bring your own Docker image, set hardware requirements, and run jobs as background tasks so you can fire and forget. Responses are fast, and you can call a million simple functions in just a few seconds.&lt;/p&gt; &lt;p&gt;Burla is built for embarrassingly parallel workloads like preprocessing data, hyperparameter tuning, and batch inference.&lt;/p&gt; &lt;p&gt;It's open source, and I’m improving the installation process. I also created managed versions for testing. If you want to try it, I’ll cover 1,000 CPU hours and 100 GPU hours. Email me at [&lt;a href="mailto:joe@burla.dev"&gt;joe@burla.dev&lt;/a&gt;](mailto:&lt;a href="mailto:joe@burla.dev"&gt;joe@burla.dev&lt;/a&gt;) if interested.&lt;/p&gt; &lt;p&gt;Here’s a short intro video:&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=9d22y_kWjyE"&gt;https://www.youtube.com/watch?v=9d22y_kWjyE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub → &lt;a href="https://github.com/Burla-Cloud/burla"&gt;https://github.com/Burla-Cloud/burla&lt;/a&gt;&lt;br /&gt; Docs → &lt;a href="https://docs.burla.dev/"&gt;https://docs.burla.dev&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Post_149"&gt; /u/Ok_Post_149 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1jh28/free_1000_cpu_100_gpu_hours_for_testers_i_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1jh28/free_1000_cpu_100_gpu_hours_for_testers_i_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1jh28/free_1000_cpu_100_gpu_hours_for_testers_i_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T18:54:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1wsi8</id>
    <title>Run Qwen3-VL-30B-A3B locally on macOS!</title>
    <updated>2025-10-09T04:32:09+00:00</updated>
    <author>
      <name>/u/TechnoFreakazoid</name>
      <uri>https://old.reddit.com/user/TechnoFreakazoid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1wsi8/run_qwen3vl30ba3b_locally_on_macos/"&gt; &lt;img alt="Run Qwen3-VL-30B-A3B locally on macOS!" src="https://external-preview.redd.it/rcGlszXx2uemqw7EFXRpNRQv_QRSWB3aKUyrnGrI2NM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=384fc5b1a74bcf6b33965fa62482995c86a5564a" title="Run Qwen3-VL-30B-A3B locally on macOS!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So far I didn't find any MLX or GGUF model released that worked with Macs, LM Studio or llama.cpp, so I fixed the basic transformers based example given to make it work with macOS and MPS acceleration.&lt;/p&gt; &lt;p&gt;The code bellow allows you to run the model locally on Macs and expose it as an Open AI compatible server so you can consume it with any client like Open WebUI.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/enriquecompan/qwen3-vl-30b-a3b-local-server-mac-mps/"&gt;https://github.com/enriquecompan/qwen3-vl-30b-a3b-local-server-mac-mps/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm running this on my Mac Studio M3 Ultra (the model I'm using is the full version which takes about 80 GB of VRAM) and it runs very well! I'm using Open WebUI to interact with it:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xqx3omk0k0uf1.png?width=1097&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f42189d71200493c7fbeeec26f0fe5396b86c4ac"&gt;https://preview.redd.it/xqx3omk0k0uf1.png?width=1097&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f42189d71200493c7fbeeec26f0fe5396b86c4ac&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fztkpz34k0uf1.png?width=1637&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56632060ac2298ff3f06b5bca5c6b7d5ff4bea30"&gt;https://preview.redd.it/fztkpz34k0uf1.png?width=1637&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56632060ac2298ff3f06b5bca5c6b7d5ff4bea30&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechnoFreakazoid"&gt; /u/TechnoFreakazoid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1wsi8/run_qwen3vl30ba3b_locally_on_macos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1wsi8/run_qwen3vl30ba3b_locally_on_macos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1wsi8/run_qwen3vl30ba3b_locally_on_macos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T04:32:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1dqiy</id>
    <title>Stop flexing Pass@N — show Pass-all-N</title>
    <updated>2025-10-08T15:30:27+00:00</updated>
    <author>
      <name>/u/Fabulous_Pollution10</name>
      <uri>https://old.reddit.com/user/Fabulous_Pollution10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1dqiy/stop_flexing_passn_show_passalln/"&gt; &lt;img alt="Stop flexing Pass@N — show Pass-all-N" src="https://preview.redd.it/20a6i107owtf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24db5289fda31d349651dd815a145c7f672d3cf6" title="Stop flexing Pass@N — show Pass-all-N" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a claim, and I’m curious what you think. I think model report should also report &lt;strong&gt;Pass-all-N&lt;/strong&gt; for tasks where they use Pass@N (like SWE tasks). Pass@N and mean resolved rate look nice, but they hide instability. Pass-all-N is simple: what share of tasks the model solves in &lt;strong&gt;EVERY&lt;/strong&gt; one of N runs. If it passes 4/5 times, it doesn’t count. For real use I want an agent that solves the task every time, not “sometimes with lucky seed.”&lt;/p&gt; &lt;p&gt;I checked this on &lt;a href="https://swe-rebench.com/"&gt;SWE-rebench&lt;/a&gt; (5 runs per model, August set) and Pass-all-5 is clearly lower than the mean resolved rate for all models. The gap size is different across models too — some are more stable, some are very flaky. That’s exactly the signal I want to see.&lt;/p&gt; &lt;p&gt;I’m not saying to drop &lt;a href="mailto:Pass@N"&gt;Pass@N&lt;/a&gt;. Keep it — but also report &lt;strong&gt;Pass-all-N&lt;/strong&gt; so we can compare reliability, not just the best-case average. Most releases already run multiple seeds to get Pass@N anyway, so it’s basically free to add Pass-all-N from the same runs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Pollution10"&gt; /u/Fabulous_Pollution10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/20a6i107owtf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1dqiy/stop_flexing_passn_show_passalln/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1dqiy/stop_flexing_passn_show_passalln/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T15:30:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1wb1p</id>
    <title>P102-100 on llama.cpp benchmarks.</title>
    <updated>2025-10-09T04:04:25+00:00</updated>
    <author>
      <name>/u/Boricua-vet</name>
      <uri>https://old.reddit.com/user/Boricua-vet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1wb1p/p102100_on_llamacpp_benchmarks/"&gt; &lt;img alt="P102-100 on llama.cpp benchmarks." src="https://external-preview.redd.it/443rFFkPppzNDFe54hBc-foPeceRrzkFT7Y5MPcCR0Q.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcf3f592ade67b2e66234fac31314db37cb90958" title="P102-100 on llama.cpp benchmarks." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For all the people that have been asking me to do some benchmarks on these cards using llama.cpp well, here you go. I still to this day do not regret spending 70 bucks for these two cards. I also would thank the people that explain to me how llama.cpp was better then ollama as this is very true. llama.cpp custom implementation of flash attention for pascals is out of this world. Qwen3-30b went from 45 tk/s on ollama to 70 tk/s on llama.cpp. I am besides myself.&lt;/p&gt; &lt;p&gt;Here are the benchmarks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pwybi9tvj0uf1.png?width=1142&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d651c404513288ca722c90cb22424241196d2c59"&gt;https://preview.redd.it/pwybi9tvj0uf1.png?width=1142&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d651c404513288ca722c90cb22424241196d2c59&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My next project will be building another super budget build with two CMP 50HX that I got for 75 bucks each.&lt;br /&gt; &lt;a href="https://www.techpowerup.com/gpu-specs/cmp-50hx.c3782"&gt;https://www.techpowerup.com/gpu-specs/cmp-50hx.c3782&lt;/a&gt;&lt;/p&gt; &lt;p&gt;22 terra flops at FP16 combined with 560.0 GB/s of memory bandwidth and 448 tensor cores each should be an interesting choice for budget builds. It should certainly be way faster than the P102-100 as the P102-100 does not have any tensor cores and has less memory bandwidth.&lt;/p&gt; &lt;p&gt;I should be done with build and testing by next week so I will post here AS&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Boricua-vet"&gt; /u/Boricua-vet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1wb1p/p102100_on_llamacpp_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1wb1p/p102100_on_llamacpp_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1wb1p/p102100_on_llamacpp_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T04:04:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1o19wvg</id>
    <title>LLM Benchmarks: Gemini 2.5 Flash latest version takes the top spot</title>
    <updated>2025-10-08T13:03:46+00:00</updated>
    <author>
      <name>/u/facethef</name>
      <uri>https://old.reddit.com/user/facethef</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o19wvg/llm_benchmarks_gemini_25_flash_latest_version/"&gt; &lt;img alt="LLM Benchmarks: Gemini 2.5 Flash latest version takes the top spot" src="https://preview.redd.it/bql3o49zyvtf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c9af7b8db7f6c91135a9b78ef4113719931ea7f" title="LLM Benchmarks: Gemini 2.5 Flash latest version takes the top spot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’ve updated our Task Completion Benchmarks, and this time Gemini 2.5 Flash (latest version) came out on top for overall task completion, scoring highest across context reasoning, SQL, agents, and normalization.&lt;/p&gt; &lt;p&gt;Our TaskBench evaluates how well language models can actually finish a variety of real-world tasks, reporting the percentage of tasks completed successfully using a consistent methodology for all models.&lt;/p&gt; &lt;p&gt;See the full rankings and details: &lt;a href="https://opper.ai/models"&gt;https://opper.ai/models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious to hear how others are seeing Gemini Flash's latest version perform vs other models, any surprises or different results in your projects?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/facethef"&gt; /u/facethef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bql3o49zyvtf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o19wvg/llm_benchmarks_gemini_25_flash_latest_version/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o19wvg/llm_benchmarks_gemini_25_flash_latest_version/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T13:03:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1zlt0</id>
    <title>How are production AI agents dealing with bot detection? (Serious question)</title>
    <updated>2025-10-09T07:26:42+00:00</updated>
    <author>
      <name>/u/Raise_Fickle</name>
      <uri>https://old.reddit.com/user/Raise_Fickle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;The elephant in the room with AI web agents: How do you deal with bot detection?&lt;/h1&gt; &lt;p&gt;With all the hype around &amp;quot;computer use&amp;quot; agents (Claude, GPT-4V, etc.) that can navigate websites and complete tasks, I'm surprised there isn't more discussion about a fundamental problem: &lt;strong&gt;every real website has sophisticated bot detection that will flag and block these agents.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;I'm working on training an RL-based web agent, and I realized that the gap between research demos and production deployment is massive:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Research environment:&lt;/strong&gt; WebArena, MiniWoB++, controlled sandboxes where you can make 10,000 actions per hour with perfect precision&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real websites:&lt;/strong&gt; Track mouse movements, click patterns, timing, browser fingerprints. They &lt;em&gt;expect&lt;/em&gt; human imperfection and variance. An agent that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clicks pixel-perfect center of buttons every time&lt;/li&gt; &lt;li&gt;Acts instantly after page loads (100ms vs. human 800-2000ms)&lt;/li&gt; &lt;li&gt;Follows optimal paths with no exploration/mistakes&lt;/li&gt; &lt;li&gt;Types without any errors or natural rhythm&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;...gets flagged immediately.&lt;/p&gt; &lt;h1&gt;The Dilemma&lt;/h1&gt; &lt;p&gt;You're stuck between two bad options:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Fast, efficient agent&lt;/strong&gt; → Gets detected and blocked&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Heavily &amp;quot;humanized&amp;quot; agent with delays and random exploration&lt;/strong&gt; → So slow it defeats the purpose&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The academic papers just assume unlimited environment access and ignore this entirely. But Cloudflare, DataDome, PerimeterX, and custom detection systems are everywhere.&lt;/p&gt; &lt;h1&gt;What I'm Trying to Understand&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;For those building production web agents:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How are you handling bot detection in practice? Is everyone just getting blocked constantly?&lt;/li&gt; &lt;li&gt;Are you adding humanization (randomized mouse curves, click variance, timing delays)? How much overhead does this add?&lt;/li&gt; &lt;li&gt;Do Playwright/Selenium stealth modes actually work against modern detection, or is it an arms race you can't win?&lt;/li&gt; &lt;li&gt;Is the Chrome extension approach (running in user's real browser session) the only viable path?&lt;/li&gt; &lt;li&gt;Has anyone tried training agents with &amp;quot;avoid detection&amp;quot; as part of the reward function?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;I'm particularly curious about:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-world success/failure rates with bot detection&lt;/li&gt; &lt;li&gt;Any open-source humanization libraries people actually use&lt;/li&gt; &lt;li&gt;Whether there's ongoing research on this (adversarial RL against detectors?)&lt;/li&gt; &lt;li&gt;If companies like Anthropic/OpenAI are solving this for their &amp;quot;computer use&amp;quot; features, or if it's still an open problem&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why This Matters&lt;/h1&gt; &lt;p&gt;If we can't solve bot detection, then all these impressive agent demos are basically just expensive ways to automate tasks in sandboxes. The real value is agents working on actual websites (booking travel, managing accounts, research tasks, etc.), but that requires either:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Websites providing official APIs/partnerships&lt;/li&gt; &lt;li&gt;Agents learning to &amp;quot;blend in&amp;quot; well enough to not get blocked&lt;/li&gt; &lt;li&gt;Some breakthrough I'm not aware of&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Anyone dealing with this? Any advice, papers, or repos that actually address the detection problem? Am I overthinking this, or is everyone else also stuck here?&lt;/p&gt; &lt;p&gt;&lt;em&gt;Posted because I couldn't find good discussions about this despite &amp;quot;AI agents&amp;quot; being everywhere. Would love to learn from people actually shipping these in production.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Raise_Fickle"&gt; /u/Raise_Fickle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1zlt0/how_are_production_ai_agents_dealing_with_bot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1zlt0/how_are_production_ai_agents_dealing_with_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1zlt0/how_are_production_ai_agents_dealing_with_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T07:26:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1hmzn</id>
    <title>Attention is all you need - As a visual book</title>
    <updated>2025-10-08T17:52:29+00:00</updated>
    <author>
      <name>/u/simplext</name>
      <uri>https://old.reddit.com/user/simplext</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1hmzn/attention_is_all_you_need_as_a_visual_book/"&gt; &lt;img alt="Attention is all you need - As a visual book" src="https://external-preview.redd.it/b3RicGY5NnFkeHRmMU5Bzo1VBINWAdeeu7BAqtWhLwslV_cdGuD17nh6Rn_c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dbc72b1dee0b4ccd99ddf178c4605811bd0f0bac" title="Attention is all you need - As a visual book" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;Imagine if you wanted to turn a research paper into a visual presentation where every small concept and idea was illustrated with an image.&lt;/p&gt; &lt;p&gt;In the video walk through, I take the popular machine learning paper that introduces transformers and turn it into a visual book. I ask questions when I don't understand something so that that more slides can be generated to explain the smaller details.&lt;/p&gt; &lt;p&gt;Visual book is free for a while. Would love for you to try it and give me your feedback. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.visualbook.app/"&gt;https://www.visualbook.app/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simplext"&gt; /u/simplext &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0u7th86qdxtf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1hmzn/attention_is_all_you_need_as_a_visual_book/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1hmzn/attention_is_all_you_need_as_a_visual_book/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T17:52:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1y286</id>
    <title>I've been working on a novel neural network architecture combining HRM with the long-term memory of google Titans! I need help training tho</title>
    <updated>2025-10-09T05:46:58+00:00</updated>
    <author>
      <name>/u/PhysicsDisastrous462</name>
      <uri>https://old.reddit.com/user/PhysicsDisastrous462</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! This is my first post here, so I'll cut right to the chase.&lt;/p&gt; &lt;p&gt;A few months ago, shortly after HRM was first announced, I had an idea: &amp;quot;What if you could combine the reasoning capabilities of HRM with the long-term memory of Titans?&amp;quot; Well, fast-forward to today, and I have a working prototype architecture that can train, fine-tune, run inference (with baked-in quantization support), and even acquire new knowledge from the user! It can even re-quantize the updated model for you once you &lt;code&gt;ctrl + c&lt;/code&gt; out of the chat window, along with &lt;code&gt;ctrl + x&lt;/code&gt; to stop the model as it is generating text!&lt;/p&gt; &lt;p&gt;But I've run into a major roadblock. So far, I've only been able to fine-tune on tiny datasets to verify that training loss goes down, LoRA merging works, memory updates function, etc.—basically just testing the architecture itself. I'm a grocery store employee with motor cortex damage (I can't drive), which limits my income here in the States and, by extension, my access to hardware. I developed this entire project on an ASUS ROG Ally Z1 Extreme, which means I've only been able to train on small, 30-sample datasets.&lt;/p&gt; &lt;p&gt;This is where I need your help. Would anyone in this community with access to CUDA-accelerated hardware be willing to train the first proper Chronos model on a larger dataset? If you can, that would be fucking awesome!&lt;/p&gt; &lt;p&gt;I'm only targeting a 30M parameter model to start, with a &lt;code&gt;--context_dim&lt;/code&gt; of 620 and both &lt;code&gt;--l_hidden&lt;/code&gt; and &lt;code&gt;--h_hidden&lt;/code&gt; set to 600. The architecture seems very efficient so far (in my tests, a 3M model hit a loss of 0.2 on a dummy dataset), so this should be a manageable size.&lt;/p&gt; &lt;p&gt;The project is pretty flexible—you can use any existing tokenizer from Hugging Face with the &lt;code&gt;--tokenizer-path&lt;/code&gt; flag. It also supports Vulkan acceleration for inference right out of the box, though for now, it's limited to INT4, Q8_0, Q4_0, and Q2_K quantization types.&lt;/p&gt; &lt;p&gt;Of course, whoever trains the first model will get full credit on the GitHub page and be added as a contributor!&lt;/p&gt; &lt;p&gt;Below is the research paper I wrote for the project, along with the link to the GitHub repo. Thanks for reading!&lt;/p&gt; &lt;h1&gt;Chronos: An Architectural Synthesis of Memory and Reasoning for Artificial General Intelligence&lt;/h1&gt; &lt;h1&gt;Abstract&lt;/h1&gt; &lt;p&gt;The dominant paradigm in artificial intelligence, predicated on scaling Transformer models, is encountering fundamental limitations in complex reasoning and lifelong learning. &lt;strong&gt;I argue&lt;/strong&gt; that the path toward Artificial General Intelligence (AGI) necessitates a shift from a scale-first to an architecture-first philosophy. This paper introduces the &lt;strong&gt;Chronos&lt;/strong&gt; architecture, a novel hybrid model that addresses the intertwined challenges of memory and reasoning. Chronos achieves a deep functional synthesis by integrating two seminal, brain-inspired systems: Google's &lt;strong&gt;Titans architecture&lt;/strong&gt;, a substrate for dynamic, lifelong memory, and the &lt;strong&gt;Hierarchical Reasoning Model (HRM)&lt;/strong&gt;, a sample-efficient engine for deep, algorithmic thought. By embedding the HRM as the core computational module within the Titans memory workspace, Chronos is designed not merely to process information, but to think, learn, and remember in a cohesive, integrated manner. &lt;strong&gt;I present&lt;/strong&gt; a complete reference implementation featuring a cross-platform C++ backend that validates this synthesis and provides robust tooling for training, fine-tuning, and high-performance quantized inference on a wide array of CPU and GPU hardware, demonstrating a tangible and technically grounded step toward AGI.&lt;/p&gt; &lt;h1&gt;1. Introduction: The Architectural Imperative&lt;/h1&gt; &lt;p&gt;The scaling hypothesis, while immensely successful, has revealed the inherent architectural weaknesses of the Transformer. Its computationally &amp;quot;shallow&amp;quot; nature results in brittleness on tasks requiring long chains of logical deduction, with Chain-of-Thought (CoT) prompting serving as an inefficient and fragile workaround. &lt;strong&gt;I posit&lt;/strong&gt; that the next leap in AI requires a deliberate synthesis of two pillars: a persistent, dynamic &lt;strong&gt;memory&lt;/strong&gt; and a deep, sample-efficient &lt;strong&gt;reasoning&lt;/strong&gt; engine. This paper proposes such a synthesis by merging the Titans architecture, which provides a solution for lifelong memory, with the Hierarchical Reasoning Model (HRM), which offers a blueprint for profound reasoning. The resulting &lt;strong&gt;Chronos&lt;/strong&gt; architecture is a tangible plan for moving beyond the limitations of scale.&lt;/p&gt; &lt;h1&gt;2. Architectural Pillars&lt;/h1&gt; &lt;h1&gt;2.1 The Titans Substrate: A Framework for Lifelong Memory&lt;/h1&gt; &lt;p&gt;The Titans architecture provides the cognitive substrate for Chronos, implementing a tripartite memory system modeled on human cognition:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Short-Term Memory (Core):&lt;/strong&gt; The high-bandwidth &amp;quot;working memory&amp;quot; for processing immediate data. In &lt;strong&gt;my&lt;/strong&gt; Chronos implementation, this is replaced by the more powerful HRM engine.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-Term Memory (LTM):&lt;/strong&gt; A vast, neural, and associative repository that learns and updates &lt;strong&gt;at test time&lt;/strong&gt;. It consolidates new knowledge based on a &amp;quot;surprise metric,&amp;quot; calculated as the gradient of the loss function (). This mechanism, equivalent to meta-learning, allows for continual, lifelong adaptation without catastrophic forgetting.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Persistent Memory:&lt;/strong&gt; A repository for ingrained, stable skills and schemas, fixed during inference.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Chronos leverages the most effective Titans variant, &lt;strong&gt;Memory as Context (MAC)&lt;/strong&gt;, where retrieved memories are concatenated with the current input, empowering the core reasoning engine to actively consider relevant history in every computational step.&lt;/p&gt; &lt;h1&gt;2.2 The HRM Engine: A Process for Deep Reasoning&lt;/h1&gt; &lt;p&gt;The Hierarchical Reasoning Model (HRM) provides the cognitive process for Chronos, addressing the shallow computational depth of traditional models. Its power derives from a brain-inspired dual-module, recurrent system:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;High-Level Module (&amp;quot;CEO&amp;quot;):&lt;/strong&gt; A slow-timescale planner that decomposes problems and sets strategic context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Low-Level Module (&amp;quot;Workers&amp;quot;):&lt;/strong&gt; A fast-timescale engine that performs rapid, iterative computations to solve the sub-goals defined by the &amp;quot;CEO&amp;quot;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This &amp;quot;loops within loops&amp;quot; process, termed &lt;strong&gt;hierarchical convergence&lt;/strong&gt;, allows HRM to achieve profound computational depth within a single forward pass. It performs reasoning in a compact latent space, a far more efficient and robust method than unrolling thought into text. HRM's astonishing performance—achieving near-perfect accuracy on complex reasoning tasks with only 27 million parameters and minimal training data—is a testament to the power of architectural intelligence over brute-force scale.&lt;/p&gt; &lt;h1&gt;3. The Chronos Synthesis: Implementation and Capabilities&lt;/h1&gt; &lt;p&gt;The core architectural innovation of Chronos is the replacement of the standard attention &amp;quot;Core&amp;quot; in the Titans MAC framework with the entire Hierarchical Reasoning Model. The HRM becomes the central processing unit for thought, operating within the vast memory workspace provided by the LTM.&lt;/p&gt; &lt;p&gt;An operational example, such as a medical diagnosis, would flow as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Ingestion:&lt;/strong&gt; New lab results enter the HRM's working memory.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Strategic Retrieval:&lt;/strong&gt; The HRM's H-module formulates a query for &amp;quot;past genomic data&amp;quot; and dispatches it to the Titans LTM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Contextualization:&lt;/strong&gt; The LTM retrieves the relevant genomic data, which is concatenated with the new lab results, forming a complete problem space for the HRM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hierarchical Reasoning:&lt;/strong&gt; The HRM executes a deep, multi-step reasoning process on the combined data to arrive at a diagnosis.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory Consolidation:&lt;/strong&gt; The novel link between the patient's data and the new diagnosis triggers the &amp;quot;surprise&amp;quot; metric, and this new knowledge is consolidated back into the LTM's parameters for future use.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This synthesis creates a virtuous cycle: &lt;strong&gt;Titans gives HRM a world model&lt;/strong&gt;, and &lt;strong&gt;HRM gives Titans a purposeful mind&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;4. Implementation and Validation&lt;/h1&gt; &lt;p&gt;A complete Python-based implementation, &lt;a href="http://chronos.py"&gt;&lt;code&gt;chronos.py&lt;/code&gt;&lt;/a&gt;, has been developed to validate the Chronos architecture. It is supported by a high-performance C++ backend for quantization and inference, ensuring maximum performance on diverse hardware.&lt;/p&gt; &lt;h1&gt;4.1 High-Performance Cross-Platform Backend 🚀&lt;/h1&gt; &lt;p&gt;A key component of the Chronos implementation is its custom C++ kernel, &lt;code&gt;chronos_matmul&lt;/code&gt;, inspired by the efficiency of &lt;code&gt;llama.cpp&lt;/code&gt;. This backend is essential for enabling direct, zero-dequantization inference, a critical feature for deploying models on low-end hardware. The kernel is designed for broad compatibility and performance through a tiered compilation strategy managed by &lt;code&gt;CMake&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The build system automatically detects the most powerful Single Instruction, Multiple Data (SIMD) instruction sets available on the host machine, ensuring optimal performance for the target CPU architecture. The supported tiers are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;x86-64 (AVX-512):&lt;/strong&gt; Provides the highest level of performance, targeting modern high-end desktop (HEDT) and server-grade CPUs from Intel and AMD.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;x86-64 (AVX2):&lt;/strong&gt; The most common performance tier, offering significant acceleration for the vast majority of modern desktop and laptop computers manufactured in the last decade.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ARM64 (NEON):&lt;/strong&gt; Crucial for the mobile and edge computing ecosystem. This enables high-speed inference on a wide range of devices, including Apple Silicon (M1/M2/M3), Microsoft Surface Pro X, Raspberry Pi 4+, and flagship Android devices.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generic Scalar Fallback:&lt;/strong&gt; For any CPU architecture not supporting the above SIMD extensions, the kernel defaults to a highly portable, standard C++ implementation. This guarantees universal compatibility, ensuring Chronos can run anywhere, albeit with reduced performance.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In addition to CPU support, the backend includes &lt;strong&gt;Vulkan&lt;/strong&gt; for GPU-accelerated inference. This allows the same quantized model to be executed on a wide array of GPUs from NVIDIA, AMD, and Intel, making Chronos a truly cross-platform solution.&lt;/p&gt; &lt;h1&gt;4.2 Core Functional Capabilities&lt;/h1&gt; &lt;p&gt;The implementation successfully addresses all key functional requirements for a deployable and extensible AGI research platform.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Built-in Training on JSON/JSONL:&lt;/strong&gt; The &lt;code&gt;JSONLDataset&lt;/code&gt; class and &lt;code&gt;create_dataloader&lt;/code&gt; function provide a robust data pipeline, capable of parsing both standard JSON lists and line-delimited JSONL files for training and fine-tuning.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;On-the-Fly Post-Training Quantization:&lt;/strong&gt; The &lt;code&gt;train&lt;/code&gt; function includes a &lt;code&gt;--quantize-on-complete&lt;/code&gt; command-line flag. When enabled, it seamlessly transitions from training to calling the &lt;code&gt;quantize&lt;/code&gt; function on the newly created model, streamlining the workflow from research to deployment.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Direct Inference on Quantized Models:&lt;/strong&gt; The system uses the C++ kernel &lt;code&gt;chronos_matmul&lt;/code&gt; to perform matrix multiplication &lt;strong&gt;directly on quantized weights&lt;/strong&gt; without a dequantization step. The &lt;code&gt;QuantizedChronos&lt;/code&gt; class orchestrates this process, ensuring minimal memory footprint and maximum performance on low-end hardware.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible Test-Time Learning:&lt;/strong&gt; The &lt;code&gt;chat&lt;/code&gt; mode implements two distinct mechanisms for saving LTM updates acquired during inference: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Default Behavior (Direct Modification):&lt;/strong&gt; If no special flag is provided, the system tracks changes and prompts the user upon exit to save the modified LTM weights back into the base model file.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoRA-style Deltas:&lt;/strong&gt; When the &lt;code&gt;--ltm-lora-path&lt;/code&gt; flag is specified, all LTM weight changes are accumulated in a separate tensor. Upon exit, only these deltas are saved to the specified &lt;code&gt;.pt&lt;/code&gt; file, preserving the integrity of the original base model.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Percentage-Based Fine-Tuning:&lt;/strong&gt; The &lt;code&gt;finetune&lt;/code&gt; mode supports a &lt;code&gt;--finetune-unlock-percent&lt;/code&gt; flag. This allows a user to specify a target percentage of trainable parameters (e.g., &lt;code&gt;1.5&lt;/code&gt; for 1.5%). The script then automatically calculates the optimal LoRA rank (&lt;code&gt;r&lt;/code&gt;) to approximate this target, offering an intuitive and powerful way to control model adaptation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quantized Terminal Chat:&lt;/strong&gt; The &lt;code&gt;chat&lt;/code&gt; mode is fully capable of loading and running inference on quantized &lt;code&gt;.npz&lt;/code&gt; model files, providing an interactive terminal-based chat interface for low-resource environments.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;5. Conclusion and Future Work&lt;/h1&gt; &lt;p&gt;The Chronos architecture presents a compelling, cognitively inspired roadmap toward AGI. By prioritizing intelligent architecture over sheer scale, it achieves capabilities in reasoning and continual learning that are intractable for current models. The provided implementation validates the feasibility of this approach and serves as a powerful platform for further research.&lt;/p&gt; &lt;p&gt;Future work will focus on the roadmap items I have outlined for the project:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Development of a user-friendly GUI.&lt;/li&gt; &lt;li&gt;Extension to multi-modal data types.&lt;/li&gt; &lt;li&gt;Implementation of the full training loop in Vulkan and CUDA for end-to-end GPU acceleration.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Github: &lt;a href="https://github.com/necat101/Chronos-CLGCM"&gt;https://github.com/necat101/Chronos-CLGCM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhysicsDisastrous462"&gt; /u/PhysicsDisastrous462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1y286/ive_been_working_on_a_novel_neural_network/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1y286/ive_been_working_on_a_novel_neural_network/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1y286/ive_been_working_on_a_novel_neural_network/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T05:46:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1drs6</id>
    <title>Ling-1T</title>
    <updated>2025-10-08T15:31:37+00:00</updated>
    <author>
      <name>/u/AaronFeng47</name>
      <uri>https://old.reddit.com/user/AaronFeng47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1drs6/ling1t/"&gt; &lt;img alt="Ling-1T" src="https://external-preview.redd.it/GF0ej-9rt3AXeKHvcKd5G-UgA8tEbZGSIvNEsQkOwA0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acd86786f2d1ca8025b03b2b577396fa4bc316d8" title="Ling-1T" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ling-1T is the first flagship non-thinking model in the Ling 2.0 series, featuring 1 trillion total parameters with ≈ 50 billion active parameters per token. Built on the Ling 2.0 architecture, Ling-1T is designed to push the limits of efficient reasoning and scalable cognition.&lt;/p&gt; &lt;p&gt;Pre-trained on 20 trillion+ high-quality, reasoning-dense tokens, Ling-1T-base supports up to 128K context length and adopts an evolutionary chain-of-thought (Evo-CoT) process across mid-training and post-training. This curriculum greatly enhances the model’s efficiency and reasoning depth, allowing Ling-1T to achieve state-of-the-art performance on multiple complex reasoning benchmarks—balancing accuracy and efficiency.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AaronFeng47"&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-1T"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1drs6/ling1t/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1drs6/ling1t/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T15:31:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1z3hj</id>
    <title>I did not realize how easy and accessible local LLMs are with models like Qwen3 4b on pure CPU.</title>
    <updated>2025-10-09T06:53:26+00:00</updated>
    <author>
      <name>/u/___positive___</name>
      <uri>https://old.reddit.com/user/___positive___</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hadn't tried running LLMs on my laptop until today. I thought CPUs were too slow and getting the old igpu working (AMD 4650U, so Vega something) would be driver hell. So I never bothered.&lt;/p&gt; &lt;p&gt;On a lark, I downloaded LM Studio, downloaded Qwen3 4b q4, and I was getting 5 tok/sec generation with no hassle at all with the automatic Vulkan setup. Not bad. It was impressive but a little slow. Then, just to be sure, I disabled the GPU and was surprised to get 10 tok/sec generation with CPU only! Wow! Very usable.&lt;/p&gt; &lt;p&gt;I had this project in mind where I would set up a smart station for home in the kitchen, somewhere to collect emails, calendar events, shopping lists, then just sort, label, summarize and display schedules and reminders as appropriate. The LLM just needs to normalize messy input, summarize, and classify text. I had been considering getting a miniPC with a ton of RAM, trying to figure out what's the minimum spec I need, what kind of expense to keep this powered 24/7, where to stick the monitor in the cramped kitchen, and so forth. Would it be worth the cost or not.&lt;/p&gt; &lt;p&gt;But I did some testing and Qwen3 4b is pretty good for my purposes. This means I can just buy any used laptop off ebay, install linux, and go wild??? It has a built in monitor, low power draw, everything for $200-300? My laptop only has DDR4-3200, so anything at that speed or above should be golden. Since async processing is fine I could do even more if I dared. Maybe throw in whisper.&lt;/p&gt; &lt;p&gt;This is amazing. Everyone and their grandma should be running local LLMs at this rate.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/___positive___"&gt; /u/___positive___ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1z3hj/i_did_not_realize_how_easy_and_accessible_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1z3hj/i_did_not_realize_how_easy_and_accessible_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1z3hj/i_did_not_realize_how_easy_and_accessible_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T06:53:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1vmby</id>
    <title>Qwen3-VL MLX support incoming, thanks to Prince Canuma</title>
    <updated>2025-10-09T03:26:10+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-VL-30B-A3B-Instruct-4bit"&gt;https://huggingface.co/mlx-community/Qwen3-VL-30B-A3B-Instruct-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-4bit"&gt;https://huggingface.co/mlx-community/Qwen3-VL-235B-A22B-Instruct-4bit&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1vmby/qwen3vl_mlx_support_incoming_thanks_to_prince/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1vmby/qwen3vl_mlx_support_incoming_thanks_to_prince/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1vmby/qwen3vl_mlx_support_incoming_thanks_to_prince/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T03:26:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1mpt5</id>
    <title>Introducing the ColBERT Nano series of models. All 3 of these models come in at less than 1 million parameters (250K, 450K, 950K)</title>
    <updated>2025-10-08T20:40:36+00:00</updated>
    <author>
      <name>/u/davidmezzetti</name>
      <uri>https://old.reddit.com/user/davidmezzetti</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1mpt5/introducing_the_colbert_nano_series_of_models_all/"&gt; &lt;img alt="Introducing the ColBERT Nano series of models. All 3 of these models come in at less than 1 million parameters (250K, 450K, 950K)" src="https://preview.redd.it/okf1858k8ytf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22ce6428ce7e3df677de1c6dc5808dfffdb5ad04" title="Introducing the ColBERT Nano series of models. All 3 of these models come in at less than 1 million parameters (250K, 450K, 950K)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Late interaction models perform shockingly well with small models. Use this method to build small domain-specific models for retrieval and more.&lt;/p&gt; &lt;p&gt;Collection: &lt;a href="https://huggingface.co/collections/NeuML/colbert-68cb248ce424a6d6d8277451"&gt;https://huggingface.co/collections/NeuML/colbert-68cb248ce424a6d6d8277451&lt;/a&gt;&lt;br /&gt; Smallest Model: &lt;a href="https://huggingface.co/NeuML/colbert-muvera-femto"&gt;https://huggingface.co/NeuML/colbert-muvera-femto&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davidmezzetti"&gt; /u/davidmezzetti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/okf1858k8ytf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1mpt5/introducing_the_colbert_nano_series_of_models_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1mpt5/introducing_the_colbert_nano_series_of_models_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T20:40:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1ac09</id>
    <title>AI21 releases Jamba 3B, the tiny model outperforming Qwen 3 4B and IBM Granite 4 Micro!</title>
    <updated>2025-10-08T13:20:50+00:00</updated>
    <author>
      <name>/u/zennaxxarion</name>
      <uri>https://old.reddit.com/user/zennaxxarion</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1ac09/ai21_releases_jamba_3b_the_tiny_model/"&gt; &lt;img alt="AI21 releases Jamba 3B, the tiny model outperforming Qwen 3 4B and IBM Granite 4 Micro!" src="https://b.thumbs.redditmedia.com/HzZ8IknU9-KdwN0J-_2TQJ93jzutAFq4cAeVawprNKM.jpg" title="AI21 releases Jamba 3B, the tiny model outperforming Qwen 3 4B and IBM Granite 4 Micro!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Disclaimer: I work for AI21, creator of the Jamba model family.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;We’re super excited to announce the launch of our brand new model, Jamba 3B!&lt;/p&gt; &lt;p&gt;Jamba 3B is the swiss army knife of models, designed to be ready on the go.&lt;/p&gt; &lt;p&gt;You can run it on your iPhone, Android, Mac or PC for smart replies, conversational assistants, model routing, fine-tuning and much more.&lt;/p&gt; &lt;p&gt;We believe we’ve rewritten what tiny models can do. &lt;/p&gt; &lt;p&gt;Jamba 3B keeps up near 40 t/s even with giant context windows, while others crawl once they pass 128K. &lt;/p&gt; &lt;p&gt;Even though it’s smaller at 3B parameters, it matches or beats Qwen 3 4B and Gemma 3 4B in model intelligence.&lt;/p&gt; &lt;p&gt;We performed benchmarking using the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mac M3 36GB&lt;/li&gt; &lt;li&gt;iPhone 16 Pro&lt;/li&gt; &lt;li&gt;Galaxy S25&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here are our key findings:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Faster and steadier at scale:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Keeps producing ~40 tokens per second on Mac even past 32k context&lt;/li&gt; &lt;li&gt;Still cranks out ~33 t/s at 128k while Qwen 3 4B drops to &amp;lt;1 t/s and Llama 3.2 3B goes down to ~5 t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Best long context efficiency:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;From 1k to 128k context, latency barely moves (43 to 33 t/s). Every rival model loses 70% speed beyond 32k&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;High intelligence per token ratio:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scored 0.31 combined intelligence index at ~40 t/s, above Gemma 3 4B (0.20) and Phi-4 Mini (0.22)&lt;/li&gt; &lt;li&gt;Qwen 3 4B ranks slightly higher in raw score (0.35) but runs 3x slower&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Outpaces IBM Granite 4 Micro:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Produces 5x more tokens per second at 256K on Mac M3 (36 GB) with reasoning intact&lt;/li&gt; &lt;li&gt;First 3B parameter model to stay coherent past 60K tokens. Achieves an effective context window ≈ 200k on desktop and mobile without nonsense outputs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Hardware footprint:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The 4-bit quantized version of Jamba 3B requires the following to run on llama.cpp at context length of 32k: &lt;/p&gt; &lt;p&gt;Model Weights: 1.84 GiB&lt;/p&gt; &lt;p&gt;Total Active Memory: ~2.2 GiB&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Blog:&lt;/strong&gt; &lt;a href="https://www.ai21.com/blog/introducing-jamba-reasoning-3b/"&gt;https://www.ai21.com/blog/introducing-jamba-reasoning-3b/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Huggingface:&lt;/strong&gt; &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B"&gt;https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zennaxxarion"&gt; /u/zennaxxarion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o1ac09"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1ac09/ai21_releases_jamba_3b_the_tiny_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1ac09/ai21_releases_jamba_3b_the_tiny_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T13:20:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1k5rc</id>
    <title>New Intel drivers are fire</title>
    <updated>2025-10-08T19:19:55+00:00</updated>
    <author>
      <name>/u/hasanismail_</name>
      <uri>https://old.reddit.com/user/hasanismail_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1k5rc/new_intel_drivers_are_fire/"&gt; &lt;img alt="New Intel drivers are fire" src="https://preview.redd.it/f43lwzkhuxtf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=726219703cd349e3c9b1d969986f07f0f51541fc" title="New Intel drivers are fire" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I went from getting 30 tokens a second on gptosss20b to 95!!!!!!!!!!!!!!! Holy shit Intel is cooking with the b580 I have 4 total I'm gonna put a rig together with all the cards on a dual socket x99 system(for the pcie lanes) well get back with multi card perf later&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hasanismail_"&gt; /u/hasanismail_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f43lwzkhuxtf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1k5rc/new_intel_drivers_are_fire/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1k5rc/new_intel_drivers_are_fire/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T19:19:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1p033</id>
    <title>Huawei's new open source technique shrinks LLMs to make them run on less powerful, less expensive hardware</title>
    <updated>2025-10-08T22:13:18+00:00</updated>
    <author>
      <name>/u/Financial_Nihilist</name>
      <uri>https://old.reddit.com/user/Financial_Nihilist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://venturebeat.com/ai/huaweis-new-open-source-technique-shrinks-llms-to-make-them-run-on-less"&gt;https://venturebeat.com/ai/huaweis-new-open-source-technique-shrinks-llms-to-make-them-run-on-less&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Financial_Nihilist"&gt; /u/Financial_Nihilist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1p033/huaweis_new_open_source_technique_shrinks_llms_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1p033/huaweis_new_open_source_technique_shrinks_llms_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1p033/huaweis_new_open_source_technique_shrinks_llms_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T22:13:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o1ogy5</id>
    <title>Anthropic’s ‘anti-China’ stance triggers exit of star AI researcher</title>
    <updated>2025-10-08T21:51:08+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1ogy5/anthropics_antichina_stance_triggers_exit_of_star/"&gt; &lt;img alt="Anthropic’s ‘anti-China’ stance triggers exit of star AI researcher" src="https://external-preview.redd.it/5CEJ0ZKiMvBkS6IB_t18sqvI2hC5lJzuWZUlPM5tVbA.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=340f17b67c347787abb8495c4aa8db54696b9210" title="Anthropic’s ‘anti-China’ stance triggers exit of star AI researcher" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.scmp.com/tech/tech-trends/article/3328222/anthropics-anti-china-stance-triggers-exit-star-ai-researcher"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o1ogy5/anthropics_antichina_stance_triggers_exit_of_star/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o1ogy5/anthropics_antichina_stance_triggers_exit_of_star/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-08T21:51:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect — Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect — Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I’m Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM – 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
