<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-18T21:23:51+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ppvdy5</id>
    <title>Putting together a repo for 21 Days of Building a Small Language Model</title>
    <updated>2025-12-18T16:38:45+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to say thanks to &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, a bunch of you have been following my &lt;em&gt;21 Days of Building a Small Language Model&lt;/em&gt; posts.&lt;br /&gt; I‚Äôve now organized everything into a GitHub repo so it‚Äôs easier to track and revisit.&lt;br /&gt; Thanks again for the encouragement&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ideaweaver-ai/21-Days-of-Building-a-Small-Language-Model/"&gt;https://github.com/ideaweaver-ai/21-Days-of-Building-a-Small-Language-Model/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppvdy5/putting_together_a_repo_for_21_days_of_building_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppvdy5/putting_together_a_repo_for_21_days_of_building_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppvdy5/putting_together_a_repo_for_21_days_of_building_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T16:38:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp6jhq</id>
    <title>Hey, LocalLLaMa. We need to talk...</title>
    <updated>2025-12-17T20:04:07+00:00</updated>
    <author>
      <name>/u/Eisenstein</name>
      <uri>https://old.reddit.com/user/Eisenstein</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I look on the front page and I see people who have spent time and effort to make something, and they share it willingly. They are getting no upvotes.&lt;/p&gt; &lt;p&gt;We are here because we are &lt;em&gt;local&lt;/em&gt; and we are &lt;em&gt;open source&lt;/em&gt;. Those things &lt;em&gt;depend on people who give us things&lt;/em&gt;, and they don't ask for anything in return, but they &lt;em&gt;need&lt;/em&gt; something in return or they will stop.&lt;/p&gt; &lt;p&gt;Pop your head into the smaller posts where someone is showing work they have done. Give honest and constructive feedback. UPVOTE IT.&lt;/p&gt; &lt;p&gt;The project may be terrible -- encourage them to grow by telling them how they can make it better. &lt;/p&gt; &lt;p&gt;The project may be awesome. They would love to hear how awesome it is. But if you use it, then they would love 100 times more to hear how you use it and how it helps you.&lt;/p&gt; &lt;p&gt;Engage with the people who share their things, and not just with the entertainment. &lt;/p&gt; &lt;p&gt;It take so little effort but it makes so much difference.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eisenstein"&gt; /u/Eisenstein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T20:04:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pper90</id>
    <title>MiraTTS: High quality and fast TTS model</title>
    <updated>2025-12-18T01:55:55+00:00</updated>
    <author>
      <name>/u/SplitNice1982</name>
      <uri>https://old.reddit.com/user/SplitNice1982</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;MiraTTS&lt;/strong&gt; is a high quality LLM based TTS finetune that can generate audio at &lt;strong&gt;100x&lt;/strong&gt; realtime and generate realistic and clear 48khz speech! I heavily optimized it using Lmdeploy and used &lt;a href="https://github.com/ysharma3501/FlashSR"&gt;FlashSR&lt;/a&gt; to enhance the audio.&lt;/p&gt; &lt;h1&gt;Benefits of this repo&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Incredibly fast: As stated before, over &lt;strong&gt;100x&lt;/strong&gt; realtime!&lt;/li&gt; &lt;li&gt;High quality: Generates realistic and 48khz speech, &lt;strong&gt;much&lt;/strong&gt; clearer then most TTS models and it‚Äôs base model.&lt;/li&gt; &lt;li&gt;Memory efficient: Works with even 6gb vram gpus!&lt;/li&gt; &lt;li&gt;Low latency: Possible latency low as &lt;strong&gt;150ms&lt;/strong&gt;, I have not released code for streaming yet but will release soon.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basic multilingual versions are already supported, I just need to clean up code. Multispeaker is still in progress, but should come soon. If you have any other issues, I will be happy to fix them.&lt;/p&gt; &lt;p&gt;Github link: &lt;a href="https://github.com/ysharma3501/MiraTTS"&gt;https://github.com/ysharma3501/MiraTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model link: &lt;a href="https://huggingface.co/YatharthS/MiraTTS"&gt;https://huggingface.co/YatharthS/MiraTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog explaining llm tts models: &lt;a href="https://huggingface.co/blog/YatharthS/llm-tts-models"&gt;https://huggingface.co/blog/YatharthS/llm-tts-models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Stars/Likes would be appreciated very much, thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SplitNice1982"&gt; /u/SplitNice1982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T01:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1poy0lb</id>
    <title>Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds.</title>
    <updated>2025-12-17T14:33:13+00:00</updated>
    <author>
      <name>/u/themixtergames</name>
      <uri>https://old.reddit.com/user/themixtergames</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"&gt; &lt;img alt="Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds." src="https://external-preview.redd.it/YWpkODI1NDF4cjdnMbxNGAI-puPRf-AP3cgrLxlreCeM4kV742La4OIIHHvj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1dded6d1cdcc956e0916d9926400982637f4d7c" title="Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/apple/ml-sharp"&gt;https://github.com/apple/ml-sharp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2512.10685"&gt;https://arxiv.org/abs/2512.10685&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/themixtergames"&gt; /u/themixtergames &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/l2mp7b31xr7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T14:33:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppntz9</id>
    <title>GLM-V GGUF is out!</title>
    <updated>2025-12-18T10:45:27+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppntz9/glmv_gguf_is_out/"&gt; &lt;img alt="GLM-V GGUF is out!" src="https://b.thumbs.redditmedia.com/FZv8pFFPpwEa4qKxoMpu3mE3J-5QIWLQlQCBViK_yvg.jpg" title="GLM-V GGUF is out!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/ggml-org/glm-v"&gt;https://huggingface.co/collections/ggml-org/glm-v&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/klip0rudzx7g1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=50865e4b0f1c5479683b40e8dc6fe68df02f03db"&gt;https://preview.redd.it/klip0rudzx7g1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=50865e4b0f1c5479683b40e8dc6fe68df02f03db&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppntz9/glmv_gguf_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppntz9/glmv_gguf_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppntz9/glmv_gguf_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T10:45:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp8vo4</id>
    <title>Nvidia plans heavy cuts to GPU supply in early 2026</title>
    <updated>2025-12-17T21:37:13+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://overclock3d.net/news/gpu-displays/nvidia-plans-heavy-cuts-to-gpu-supply-in-early-2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp8vo4/nvidia_plans_heavy_cuts_to_gpu_supply_in_early/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp8vo4/nvidia_plans_heavy_cuts_to_gpu_supply_in_early/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T21:37:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppx93g</id>
    <title>VibeVoice 7B and 1.5B FastAPI Wrapper</title>
    <updated>2025-12-18T17:51:20+00:00</updated>
    <author>
      <name>/u/TommarrA</name>
      <uri>https://old.reddit.com/user/TommarrA</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppx93g/vibevoice_7b_and_15b_fastapi_wrapper/"&gt; &lt;img alt="VibeVoice 7B and 1.5B FastAPI Wrapper" src="https://external-preview.redd.it/ocq5FpDlatga69140E_bI6uHAd--cmL-kjurFzGrpzw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4bfc93e7c6b2a4bfa1956edd98031f239c71437" title="VibeVoice 7B and 1.5B FastAPI Wrapper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had created a fast API wrapper for the original VibeVoice model (7B and 1.5B)&lt;/p&gt; &lt;p&gt;It allows you to use custom voices unlike the current iteration of VibeVoice that has Microsoft generated voice models.&lt;/p&gt; &lt;p&gt;It works well for my ebook narration use case so thought I would share with the community too.&lt;/p&gt; &lt;p&gt;Thanks to folks who had made a backup of the original code.&lt;/p&gt; &lt;p&gt;I will eventually build in the ability to use the 0.5B model as well but current iteration only support and 7B and 1.5B models&lt;/p&gt; &lt;p&gt;Let me know how it works for your use cases&lt;/p&gt; &lt;p&gt;Docker is the preferred deployment model - tested on Ubuntu.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TommarrA"&gt; /u/TommarrA &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ncoder-ai/VibeVoice-FastAPI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppx93g/vibevoice_7b_and_15b_fastapi_wrapper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppx93g/vibevoice_7b_and_15b_fastapi_wrapper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T17:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppnmca</id>
    <title>AI is great at answers, but terrible at uncertainty and that‚Äôs a bigger problem than hallucinations</title>
    <updated>2025-12-18T10:32:06+00:00</updated>
    <author>
      <name>/u/Mediocre_Common_4126</name>
      <uri>https://old.reddit.com/user/Mediocre_Common_4126</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most of the criticism around LLMs focuses on hallucinations, wrong facts, or confidence issues but I think the deeper problem is AI is optimized to sound &lt;em&gt;certain&lt;/em&gt;&lt;/p&gt; &lt;p&gt;In real work, the hardest moments are not when you need an answer. They‚Äôre when you don‚Äôt even know what the right question is yet&lt;/p&gt; &lt;p&gt;The messy parts: half-formed thoughts + contradictory signals + ‚Äúthis feels wrong but I don‚Äôt know why‚Äù backtracking changing your mind mid-way&lt;/p&gt; &lt;p&gt;Humans spend a huge amount of time operating in uncertainty, we explore, we reframe, we circle around the problem&lt;/p&gt; &lt;p&gt;Most training data skips that phase entirely, we feed models clean prompts and polished conclusions, then expect them to handle ambiguity well&lt;/p&gt; &lt;p&gt;That‚Äôs why LLMs often feel impressive but fragile, they jump to conclusions too fast, they don‚Äôt linger in confusion, they optimize for closure, not exploration.&lt;/p&gt; &lt;p&gt;What‚Äôs interesting is that the best human collaborators are the opposite. They slow you down, they ask annoying clarifying questions, they surface blind spots instead of hiding them behind confident language&lt;/p&gt; &lt;p&gt;This made me rethink how AI tools should be built, less ‚Äúgive me the answer‚Äù, more ‚Äúhelp me think without collapsing the space too early‚Äù&lt;/p&gt; &lt;p&gt;Interesting if others have noticed this too. Especially people building tools on top of LLMs or using them for real decision making&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mediocre_Common_4126"&gt; /u/Mediocre_Common_4126 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnmca/ai_is_great_at_answers_but_terrible_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnmca/ai_is_great_at_answers_but_terrible_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppnmca/ai_is_great_at_answers_but_terrible_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T10:32:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppq8pi</id>
    <title>Z-Image is now the default image model on HuggingChat</title>
    <updated>2025-12-18T13:01:20+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppq8pi/zimage_is_now_the_default_image_model_on/"&gt; &lt;img alt="Z-Image is now the default image model on HuggingChat" src="https://b.thumbs.redditmedia.com/q04f8-Hq7gSnGXdIq-IW8V70b-2l8sOF10WS2JF_Kks.jpg" title="Z-Image is now the default image model on HuggingChat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Victor M (Hugging Face) on ùïè: &lt;a href="https://x.com/victormustar/status/2001629770329858391?s=20"&gt;https://x.com/victormustar/status/2001629770329858391&lt;/a&gt;&lt;br /&gt; HuggingChat: &lt;a href="https://huggingface.co/chat/"&gt;https://huggingface.co/chat/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ppq8pi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppq8pi/zimage_is_now_the_default_image_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppq8pi/zimage_is_now_the_default_image_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T13:01:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppm9xm</id>
    <title>NVIDIA Publishes Complete Evaluation Recipe for Nemotron 3 Nano</title>
    <updated>2025-12-18T09:03:01+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppm9xm/nvidia_publishes_complete_evaluation_recipe_for/"&gt; &lt;img alt="NVIDIA Publishes Complete Evaluation Recipe for Nemotron 3 Nano" src="https://external-preview.redd.it/i9rG1D6xcH_2B9JTT5Ak5wKM4ExK483hNq6oNeOkRNo.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa9a037e77932298ed68f09b93c42491dd8ab8e0" title="NVIDIA Publishes Complete Evaluation Recipe for Nemotron 3 Nano" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppm9xm/nvidia_publishes_complete_evaluation_recipe_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppm9xm/nvidia_publishes_complete_evaluation_recipe_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T09:03:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppongx</id>
    <title>Fast on-device Speech-to-text for Home Assistant (open source)</title>
    <updated>2025-12-18T11:34:57+00:00</updated>
    <author>
      <name>/u/banafo</name>
      <uri>https://old.reddit.com/user/banafo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppongx/fast_ondevice_speechtotext_for_home_assistant/"&gt; &lt;img alt="Fast on-device Speech-to-text for Home Assistant (open source)" src="https://external-preview.redd.it/6PRNLd3TFMw1DCfYP7618_nVHzwQRPRrDRjMqQg7XGU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cff7a166c2a85ced6d24604f32dc307cf599fedf" title="Fast on-device Speech-to-text for Home Assistant (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just released &lt;a href="https://github.com/orgs/kroko-ai/repositories"&gt;kroko-onnx-home-assistant &lt;/a&gt; is a &lt;strong&gt;local&lt;/strong&gt; streaming STT pipeline for home assistant.&lt;/p&gt; &lt;p&gt;It's currently just a fork of the excellent &lt;a href="https://github.com/ptbsare/sherpa-onnx-tts-stt"&gt;https://github.com/ptbsare/sherpa-onnx-tts-stt&lt;/a&gt; with support for our models added, hopefully it will be accepted in the main project. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;High quality&lt;/li&gt; &lt;li&gt;Real streaming (partial results, low latency)&lt;/li&gt; &lt;li&gt;100% local &amp;amp; privacy-first&lt;/li&gt; &lt;li&gt;optimized for fast CPU inference, even in low resources raspberry pi's&lt;/li&gt; &lt;li&gt;Does not require additional VAD&lt;/li&gt; &lt;li&gt;Home Assistant integration&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo:&lt;br /&gt; [&lt;a href="https://github.com/kroko-ai/kroko-onnx-home-assistant%5D()"&gt;https://github.com/kroko-ai/kroko-onnx-home-assistant]()&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you want to test the model quality before installing: the huggingface models running in the browser is the easiest way: &lt;a href="https://huggingface.co/spaces/Banafo/Kroko-Streaming-ASR-Wasm"&gt;https://huggingface.co/spaces/Banafo/Kroko-Streaming-ASR-Wasm&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A big thanks to:&lt;br /&gt; - NaggingDaivy on discord, for the assistance.&lt;br /&gt; - the sherpa-onnx-tts-stt team for adding support for streaming models in record time.&lt;/p&gt; &lt;p&gt;Want us to integrate with your favorite open source project ? Contact us on discord:&lt;br /&gt; &lt;a href="https://discord.gg/TEbfnC7b"&gt;https://discord.gg/TEbfnC7b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some releases you may have missed:&lt;br /&gt; - Freewitch Module: &lt;a href="https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko"&gt;https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko&lt;/a&gt;&lt;br /&gt; - Asterisk Module: &lt;a href="https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko"&gt;https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko&lt;/a&gt;&lt;br /&gt; - Full Asterisk based voicebot running with Kroko streaming models: &lt;a href="https://github.com/hkjarral/Asterisk-AI-Voice-Agent"&gt;https://github.com/hkjarral/Asterisk-AI-Voice-Agent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are still working on the main models, code and documentation as well, but held up a bit with urgent paid work deadlines, more coming there soon too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/banafo"&gt; /u/banafo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kroko-ai/kroko-onnx-home-assistant"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppongx/fast_ondevice_speechtotext_for_home_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppongx/fast_ondevice_speechtotext_for_home_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T11:34:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppv68d</id>
    <title>[Blog from Hugging Face] Tokenization in Transformers v5: Simpler, Clearer, and More Modular</title>
    <updated>2025-12-18T16:30:13+00:00</updated>
    <author>
      <name>/u/Disastrous-Work-1632</name>
      <uri>https://old.reddit.com/user/Disastrous-Work-1632</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppv68d/blog_from_hugging_face_tokenization_in/"&gt; &lt;img alt="[Blog from Hugging Face] Tokenization in Transformers v5: Simpler, Clearer, and More Modular" src="https://preview.redd.it/ggovkfrtoz7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d79c723d2ee425a8d0fe89be6ee4871ab9baba7b" title="[Blog from Hugging Face] Tokenization in Transformers v5: Simpler, Clearer, and More Modular" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This blog explains how tokenization works in Transformers and why v5 is a major redesign, with clearer internals, a clean class hierarchy, and a single fast backend. It‚Äôs a practical guide for anyone who wants to understand, customize, or train model-specific tokenizers instead of treating them as black boxes.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://huggingface.co/blog/tokenizers"&gt;https://huggingface.co/blog/tokenizers&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous-Work-1632"&gt; /u/Disastrous-Work-1632 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ggovkfrtoz7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppv68d/blog_from_hugging_face_tokenization_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppv68d/blog_from_hugging_face_tokenization_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T16:30:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppwylg</id>
    <title>What's your favourite local coding model?</title>
    <updated>2025-12-18T17:40:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwylg/whats_your_favourite_local_coding_model/"&gt; &lt;img alt="What's your favourite local coding model?" src="https://preview.redd.it/q8ipunvr008g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4e54f5d47898a4570fb732cd3140edf2551267b" title="What's your favourite local coding model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried (with Mistral Vibe Cli)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;mistralai_Devstral-Small-2-24B-Instruct-2512-Q8_0.gguf - works but it's kind of slow for coding &lt;/li&gt; &lt;li&gt;nvidia_Nemotron-3-Nano-30B-A3B-Q8_0.gguf - text generation is fast, but the actual coding is slow and often incorrect&lt;/li&gt; &lt;li&gt;Qwen3-Coder-30B-A3B-Instruct-Q8_0.gguf - works correctly and it's fast&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What else would you recommend? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q8ipunvr008g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwylg/whats_your_favourite_local_coding_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwylg/whats_your_favourite_local_coding_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T17:40:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppstef</id>
    <title>Thoughts on recent small (under 20B) models</title>
    <updated>2025-12-18T14:55:45+00:00</updated>
    <author>
      <name>/u/surubel</name>
      <uri>https://old.reddit.com/user/surubel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently we're been graced with quite a few small (under 20B) models and I've tried most of them.&lt;/p&gt; &lt;p&gt;The initial benchmarks seemed a bit too good to be true, but I've tried them regardless. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;RNJ-1: this one had probably the most &amp;quot;honest&amp;quot; benchmark results. About as good as QWEN3 8B, which seems fair from my limited usage. &lt;/li&gt; &lt;li&gt;GLM 4.6v Flash: even after the latest llama.cpp update and Unsloth quantization I still have mixed feelings. Can't get it to think in English, but produces decent results. Either there are still issues with llama.cpp / quantization or it's a bit benchmaxxed&lt;/li&gt; &lt;li&gt;Ministral 3 14B: solid vision capabilities, but tends to overthink a lot. Occasionally messes up tool calls. A bit unreliable.&lt;/li&gt; &lt;li&gt;Nemotron cascade 14B. Similar to Ministral 3 14B tends to overthink a lot. Although it has great coding benchmarks, I couldn't get good results out of it. GPT OSS 20B and QWEN3 8B VL seem to give better results. This was the most underwhelming for me.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Did anyone get different results from these models? Am I missing something?&lt;/p&gt; &lt;p&gt;Seems like GPT OSS 20B and QWEN3 8B VL are still the most reliable small models, at least for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/surubel"&gt; /u/surubel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppstef/thoughts_on_recent_small_under_20b_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppstef/thoughts_on_recent_small_under_20b_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppstef/thoughts_on_recent_small_under_20b_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T14:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppu35l</id>
    <title>Mistral released Mistral OCR 3: 74% overall win rate over Mistral OCR 2 on forms, scanned documents, complex tables, and handwriting.</title>
    <updated>2025-12-18T15:47:20+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu35l/mistral_released_mistral_ocr_3_74_overall_win/"&gt; &lt;img alt="Mistral released Mistral OCR 3: 74% overall win rate over Mistral OCR 2 on forms, scanned documents, complex tables, and handwriting." src="https://b.thumbs.redditmedia.com/CTdnDnhEVXKhvcC_xn7Fo04JbVfjTe3Wx_yk_R9kVRw.jpg" title="Mistral released Mistral OCR 3: 74% overall win rate over Mistral OCR 2 on forms, scanned documents, complex tables, and handwriting." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://mistral.ai/news/mistral-ocr-3"&gt;https://mistral.ai/news/mistral-ocr-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mistral OCR 3 sets new benchmarks in both accuracy and efficiency, outperforming enterprise document processing solutions as well as AI-native OCR.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ppu35l"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu35l/mistral_released_mistral_ocr_3_74_overall_win/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu35l/mistral_released_mistral_ocr_3_74_overall_win/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T15:47:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppzof4</id>
    <title>LatitudeGames/Hearthfire-24B ¬∑ Hugging Face</title>
    <updated>2025-12-18T19:24:58+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzof4/latitudegameshearthfire24b_hugging_face/"&gt; &lt;img alt="LatitudeGames/Hearthfire-24B ¬∑ Hugging Face" src="https://external-preview.redd.it/A3gGg_h4D053EFPLZSslW4oGkfGx4Yyo44cLXCFOpgw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af804b5dc2799b163e1ddae03ccbee1392cf7d39" title="LatitudeGames/Hearthfire-24B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hearthfire is a narrative longform writing model designed to embrace the quiet moments between the chaos. While most roleplay models are trained to relentlessly drive the plot forward with high-stakes action and constant external pressure, Hearthfire is tuned to appreciate atmosphere, introspection, and the slow burn of a scene.&lt;/p&gt; &lt;p&gt;It prioritizes vibes over velocity. It is comfortable with silence. It will not force a goblin attack just because the conversation lulled.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/LatitudeGames/Hearthfire-24B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzof4/latitudegameshearthfire24b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzof4/latitudegameshearthfire24b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T19:24:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppwfw3</id>
    <title>Key Highlights of Google's New Open Model, FunctionGemma</title>
    <updated>2025-12-18T17:19:51+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwfw3/key_highlights_of_googles_new_open_model/"&gt; &lt;img alt="Key Highlights of Google's New Open Model, FunctionGemma" src="https://external-preview.redd.it/f3OilJIGGaBNRWiWULRSz5XOCY6YipQN2XKt886yVr0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0508cadd0c3629606d28322469362c690c52148b" title="Key Highlights of Google's New Open Model, FunctionGemma" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;[1] Function-calling specialized&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Built on the &lt;em&gt;Gemma 3 270M&lt;/em&gt; foundation and fine-tuned for function calling tasks, turning natural language into structured function calls for API/tool execution.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[2] Lightweight &amp;amp; open&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;A compact, open-weight model (~270 M parameters) designed for efficient use on resource-constrained hardware (laptops, desktops, cloud, edge) and democratizing access to advanced function-call agents.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[3] 32K token context&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports up to ~32 k token context window, like other 270M Gemma models, making it suitable for moderately long prompts and complex sequences.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[4] Fine-tuning friendly&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Intended to be further fine-tuned for specific custom actions, improving accuracy and customization for particular domains or workflows (e.g., mobile actions, custom APIs).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model - &lt;a href="https://huggingface.co/google/functiongemma-270m-it"&gt;https://huggingface.co/google/functiongemma-270m-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model GGUF - &lt;a href="https://huggingface.co/unsloth/functiongemma-270m-it-GGUF"&gt;https://huggingface.co/unsloth/functiongemma-270m-it-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/google/functiongemma-270m-it"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwfw3/key_highlights_of_googles_new_open_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwfw3/key_highlights_of_googles_new_open_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T17:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppu4lc</id>
    <title>Fine-tuning Qwen3 at home to respond to any prompt with a dad joke</title>
    <updated>2025-12-18T15:48:58+00:00</updated>
    <author>
      <name>/u/InvadersMustLive</name>
      <uri>https://old.reddit.com/user/InvadersMustLive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu4lc/finetuning_qwen3_at_home_to_respond_to_any_prompt/"&gt; &lt;img alt="Fine-tuning Qwen3 at home to respond to any prompt with a dad joke" src="https://external-preview.redd.it/aeJXUJD-EG13fwr7w155noLxr7JTSfAKwf9XG0w-u3s.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9431c18c37e750b69f2ab16532111dd97d789f41" title="Fine-tuning Qwen3 at home to respond to any prompt with a dad joke" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InvadersMustLive"&gt; /u/InvadersMustLive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nixiesearch.substack.com/p/fine-tuning-qwen3-at-home-to-respond"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu4lc/finetuning_qwen3_at_home_to_respond_to_any_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu4lc/finetuning_qwen3_at_home_to_respond_to_any_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T15:48:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppwqki</id>
    <title>FunctionGemma Physics Playground: A simulation game where you need to use natural language to solve physics puzzles... running 100% locally in your browser!</title>
    <updated>2025-12-18T17:31:14+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwqki/functiongemma_physics_playground_a_simulation/"&gt; &lt;img alt="FunctionGemma Physics Playground: A simulation game where you need to use natural language to solve physics puzzles... running 100% locally in your browser!" src="https://external-preview.redd.it/MXBiZjQzZTd4ejdnMYei2aDWEA5WccTd6X2Ceg7tONZcTZmqT6GgxYYEX2jv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c19158fb01b0628ef68c006e673dc09cd2cf081" title="FunctionGemma Physics Playground: A simulation game where you need to use natural language to solve physics puzzles... running 100% locally in your browser!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, Google released FunctionGemma, a lightweight (270M), open foundation model built for creating specialized function calling models! To test it out, I built a small game where you use natural language to solve physics simulation puzzles. It runs entirely locally in your browser on WebGPU, powered by Transformers.js.&lt;/p&gt; &lt;p&gt;Links:&lt;br /&gt; - Game: &lt;a href="https://huggingface.co/spaces/webml-community/FunctionGemma-Physics-Playground"&gt;https://huggingface.co/spaces/webml-community/FunctionGemma-Physics-Playground&lt;/a&gt;&lt;br /&gt; - FunctionGemma on Hugging Face: &lt;a href="https://huggingface.co/google/functiongemma-270m-it"&gt;https://huggingface.co/google/functiongemma-270m-it&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k33t7zd7xz7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwqki/functiongemma_physics_playground_a_simulation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwqki/functiongemma_physics_playground_a_simulation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T17:31:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppt1xb</id>
    <title>Meta released Map-anything-v1: A universal transformer model for metric 3D reconstruction</title>
    <updated>2025-12-18T15:05:22+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppt1xb/meta_released_mapanythingv1_a_universal/"&gt; &lt;img alt="Meta released Map-anything-v1: A universal transformer model for metric 3D reconstruction" src="https://preview.redd.it/go7lager9z7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96b4f63fa1cdd2136e6c82f35c609cc6cc1ead9c" title="Meta released Map-anything-v1: A universal transformer model for metric 3D reconstruction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/facebook/map-anything-v1"&gt;https://huggingface.co/facebook/map-anything-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It supports 12+ tasks like multi-view stereo and SfM in a single feed-forward pass&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/go7lager9z7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppt1xb/meta_released_mapanythingv1_a_universal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppt1xb/meta_released_mapanythingv1_a_universal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T15:05:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq1s6d</id>
    <title>Don't kill me.</title>
    <updated>2025-12-18T20:48:42+00:00</updated>
    <author>
      <name>/u/valdev</name>
      <uri>https://old.reddit.com/user/valdev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq1s6d/dont_kill_me/"&gt; &lt;img alt="Don't kill me." src="https://preview.redd.it/sfxhuwrzy08g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b497a07080bcbc8483a0495d71e88a2abfe0dbe7" title="Don't kill me." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valdev"&gt; /u/valdev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sfxhuwrzy08g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq1s6d/dont_kill_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq1s6d/dont_kill_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T20:48:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppzhtq</id>
    <title>T5Gemma 2: The next generation of encoder-decoder models</title>
    <updated>2025-12-18T19:17:53+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzhtq/t5gemma_2_the_next_generation_of_encoderdecoder/"&gt; &lt;img alt="T5Gemma 2: The next generation of encoder-decoder models" src="https://external-preview.redd.it/_rnSBYMvSInq6EN43nG_cTgBC4Jp6XTPNyUPRgnGKn0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9dbe7f224d36b036fe98650042395413b48e5a4" title="T5Gemma 2: The next generation of encoder-decoder models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;T5Gemma 2 models, based on Gemma 3, are multilingual and multimodal, handling text and image input and generating text output, with open weights for three pretrained sizes (270M-270M, 1B-1B, and 4B-4B).&lt;/p&gt; &lt;p&gt;Key Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tied embeddings:&lt;/strong&gt; Embeddings are tied between the encoder and decoder. This significantly reduces the overall parameter count and allowing to pack more active capabilities into the same memory footprint.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Merged attention:&lt;/strong&gt; The decoder uses a merged attention mechanism, combining self- and cross-attention into a single, unified attention layer. This reduces model parameters and architectural complexity, improving model parallelization and benefiting inference.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multimodality:&lt;/strong&gt; T5Gemma 2 models can understand and process images alongside text. By utilizing a highly efficient vision encoder, the models can seamlessly perform visual question answering and multimodal reasoning tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extended long context:&lt;/strong&gt; Leveraging Gemma 3's alternating local and global attention mechanism, T5Gemma 2 can handle context windows of up to 128K tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Massively multilingual:&lt;/strong&gt; Trained on a larger, more diverse dataset, these models now support over 140 languages out of the box.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Models - &lt;a href="https://huggingface.co/collections/google/t5gemma-2"&gt;https://huggingface.co/collections/google/t5gemma-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Official Blog post - &lt;a href="https://blog.google/technology/developers/t5gemma-2/"&gt;https://blog.google/technology/developers/t5gemma-2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/t5gemma-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzhtq/t5gemma_2_the_next_generation_of_encoderdecoder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzhtq/t5gemma_2_the_next_generation_of_encoderdecoder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T19:17:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppun3v</id>
    <title>Google's Gemma models family</title>
    <updated>2025-12-18T16:09:10+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"&gt; &lt;img alt="Google's Gemma models family" src="https://preview.redd.it/59w0vja4lz7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7dd2d66ee23d4078bf31aba81cdeecc769669af4" title="Google's Gemma models family" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/59w0vja4lz7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T16:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pniwfj</id>
    <title>Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams.</title>
    <updated>2025-12-15T21:02:55+00:00</updated>
    <author>
      <name>/u/ai2_official</name>
      <uri>https://old.reddit.com/user/ai2_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt; &lt;img alt="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." src="https://b.thumbs.redditmedia.com/bsv34WIHZXC49Az9mFES5lSIAtaQ2CuLZJ4dCaLxsEY.jpg" title="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre researchers and engineers from Ai2, the nonprofit AI lab. We recently announced:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2&lt;/strong&gt;‚Äîopen multimodal models for video + images that can return grounded answers (pixel coordinates + timestamps), trained with open datasets&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3&lt;/strong&gt;‚Äîa family of fully open language models (7B‚Äì32B) with Base/Instruct/Thinking variants, long‚Äëcontext support, open training recipes &amp;amp; checkpoints&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ask us anything about local inference, training mixes &amp;amp; our truly open approach, long‚Äëcontext, grounded video QA/tracking, and real‚Äëworld deployment.&lt;/p&gt; &lt;p&gt;Participating in the AMA:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Ranjay Krishna ( &lt;a href="/u/ranjaykrishna"&gt;u/ranjaykrishna&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Zixian Ma ( &lt;a href="/u/Frequent_Rooster2980"&gt;u/Frequent_Rooster2980&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Chris Clark ( &lt;a href="/u/mostly_reasonable"&gt;u/mostly_reasonable&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Jieyu Zhang ( &lt;a href="/u/Jealous_Programmer51"&gt;u/Jealous_Programmer51&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Rohun Tripathi ( &lt;a href="/u/darkerWind"&gt;u/darkerWind&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Kyle Lo ( &lt;a href="/u/klstats"&gt;u/klstats&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Allyson Ettinger ( &lt;a href="/u/aeclang"&gt;u/aeclang&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Finbarr Timbers ( &lt;a href="/u/fnbr"&gt;u/fnbr&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Faeze Brahman ( &lt;a href="/u/faebrhn"&gt;u/faebrhn&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôll be live from &lt;strong&gt;1pm&lt;/strong&gt; to &lt;strong&gt;2pm PST.&lt;/strong&gt; Read up on our latest releases below, and feel welcome to jump in anytime!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ñ∂Ô∏è &lt;strong&gt;Try in the Playground:&lt;/strong&gt; &lt;a href="https://playground.allenai.org"&gt;https://playground.allenai.org&lt;/a&gt;&lt;/li&gt; &lt;li&gt;‚¨áÔ∏è &lt;strong&gt;Download&lt;/strong&gt;: &lt;a href="https://huggingface.co/collections/allenai/molmo2"&gt;https://huggingface.co/collections/allenai/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìù &lt;strong&gt;Blog&lt;/strong&gt;: &lt;a href="https://allenai.org/blog/molmo2"&gt;https://allenai.org/blog/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìÑReport: &lt;a href="https://allenai.org/papers/molmo2"&gt;https://allenai.org/papers/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üíª &lt;strong&gt;API coming soon&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ü´Ü PROOF:&lt;/strong&gt; &lt;a href="https://x.com/allen_ai/status/2000692253606514828"&gt;https://x.com/allen_ai/status/2000692253606514828&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Join us on Reddit&lt;/strong&gt; &lt;a href="/r/allenai"&gt;r/allenai&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Join Ai2 on Discord:&lt;/strong&gt; &lt;a href="https://discord.gg/6vWDHyTCQV"&gt;https://discord.gg/6vWDHyTCQV&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8"&gt;https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thank you everyone for the kind words and great questions! This AMA has ended as of 2pm PST (5pm EST) on Dec. 16.&lt;/p&gt; &lt;p&gt;&lt;a href="https://discord.gg/6vWDHyTCQV"&gt;Join Ai2 on Discord&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai2_official"&gt; /u/ai2_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T21:02:55+00:00</published>
  </entry>
</feed>
