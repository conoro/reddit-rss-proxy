<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-27T13:30:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pw5360</id>
    <title>MLX community already added support for Minimax-M2.1</title>
    <updated>2025-12-26T14:06:29+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw5360/mlx_community_already_added_support_for_minimaxm21/"&gt; &lt;img alt="MLX community already added support for Minimax-M2.1" src="https://preview.redd.it/phwy35uk2k9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6dd981b9774d7410723451975474cfd7b8d6908c" title="MLX community already added support for Minimax-M2.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/phwy35uk2k9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw5360/mlx_community_already_added_support_for_minimaxm21/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw5360/mlx_community_already_added_support_for_minimaxm21/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T14:06:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwe4wg</id>
    <title>Liquid AI RLs LFM2-2.6B to perform among the best 3B models</title>
    <updated>2025-12-26T20:28:43+00:00</updated>
    <author>
      <name>/u/KaroYadgar</name>
      <uri>https://old.reddit.com/user/KaroYadgar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwe4wg/liquid_ai_rls_lfm226b_to_perform_among_the_best/"&gt; &lt;img alt="Liquid AI RLs LFM2-2.6B to perform among the best 3B models" src="https://preview.redd.it/pzgc89yryl9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ada02a3125dc18bd94c6047301961913a80176c7" title="Liquid AI RLs LFM2-2.6B to perform among the best 3B models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KaroYadgar"&gt; /u/KaroYadgar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pzgc89yryl9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwe4wg/liquid_ai_rls_lfm226b_to_perform_among_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwe4wg/liquid_ai_rls_lfm226b_to_perform_among_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T20:28:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvxq2t</id>
    <title>Hard lesson learned after a year of running large models locally</title>
    <updated>2025-12-26T06:38:00+00:00</updated>
    <author>
      <name>/u/inboundmage</name>
      <uri>https://old.reddit.com/user/inboundmage</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, go easy with me I'm new at running large models.&lt;/p&gt; &lt;p&gt;After spending about 12 months tinkering with locally hosted LLMs, I thought I had my setup dialed in. I‚Äôm running everything off a workstation with a single RTX 3090, Ubuntu 22.04, llama.cpp for smaller models and vLLM for anything above 30 B parameters. &lt;/p&gt; &lt;p&gt;My goal has always been to avoid cloud dependencies and keep as much computation offline as possible, so I‚Äôve tried every quantization trick and caching tweak I could find.&lt;/p&gt; &lt;p&gt;The biggest friction point has been scaling beyond 13 B models. &lt;/p&gt; &lt;p&gt;Even with 24 GB of VRAM, running a 70 B model in int4 still exhausts memory when the context window grows and attention weights balloon. &lt;/p&gt; &lt;p&gt;Offloading to system RAM works, but inference latency spikes into seconds, and batching requests becomes impossible. &lt;/p&gt; &lt;p&gt;I‚Äôve also noticed that GPU VRAM fragmentation accumulates over time when swapping between models, after a few hours, vLLM refuses to load a model that would normally fit because of leftover allocations.&lt;/p&gt; &lt;p&gt;My takeaway so far is that local first inference is viable for small to medium models, but there‚Äôs a hard ceiling unless you invest in server grade hardware or cluster multiple GPUs. &lt;/p&gt; &lt;p&gt;Quantization helps, but you trade some quality and run into new bugs. &lt;/p&gt; &lt;p&gt;For privacy sensitive tasks, the trade‚Äëoff is worth it; for fast iteration, it‚Äôs been painful compared to cloud based runners. &lt;/p&gt; &lt;p&gt;I‚Äôm curious if anyone has found a reliable way to manage VRAM fragmentation or offload attention blocks more efficiently on consumer cards, or whether the answer is simply ‚Äúbuy more VRAM.‚Äù &lt;/p&gt; &lt;p&gt;How are others solving this without compromising on running fully offline?&lt;/p&gt; &lt;p&gt;Thx&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inboundmage"&gt; /u/inboundmage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T06:38:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwdn8e</id>
    <title>RTX Pro 6000 under 8K EUR (tax included) in Germany early January.</title>
    <updated>2025-12-26T20:08:05+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwdn8e/rtx_pro_6000_under_8k_eur_tax_included_in_germany/"&gt; &lt;img alt="RTX Pro 6000 under 8K EUR (tax included) in Germany early January." src="https://external-preview.redd.it/8wA_CbEhSLNdntBW1eKt_MNJD2bU_9Ik0pbqrpYh0K8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=435c8a04771377e4103e5796f7987ea45751d046" title="RTX Pro 6000 under 8K EUR (tax included) in Germany early January." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imgur.com/Nk0v24j"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwdn8e/rtx_pro_6000_under_8k_eur_tax_included_in_germany/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwdn8e/rtx_pro_6000_under_8k_eur_tax_included_in_germany/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T20:08:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw9n74</id>
    <title>[Model Release] Genesis-152M-Instruct, exploring hybrid attention + TTT at small scale</title>
    <updated>2025-12-26T17:23:11+00:00</updated>
    <author>
      <name>/u/Kassanar</name>
      <uri>https://old.reddit.com/user/Kassanar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone üëã&lt;/p&gt; &lt;p&gt;I‚Äôm sharing &lt;strong&gt;Genesis-152M-Instruct&lt;/strong&gt;, an &lt;strong&gt;experimental small language model&lt;/strong&gt; built to explore how &lt;em&gt;recent architectural ideas interact&lt;/em&gt; when combined in a single model ‚Äî especially under &lt;strong&gt;tight data constraints&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This is &lt;strong&gt;research-oriented&lt;/strong&gt;, not a production model or SOTA claim.&lt;/p&gt; &lt;p&gt;üîç &lt;strong&gt;Why this might be interesting&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most recent architectures (GLA, FoX, TTT, ¬µP, sparsity) are tested &lt;strong&gt;in isolation&lt;/strong&gt; and usually at &lt;strong&gt;large scale&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I wanted to answer a simpler question:&lt;/p&gt; &lt;p&gt;&lt;em&gt;How much can architecture compensate for data at ~150M parameters?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Genesis combines several &lt;strong&gt;ICLR 2024‚Äì2025 ideas&lt;/strong&gt; into one model and evaluates the result.&lt;/p&gt; &lt;p&gt;‚ö° &lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;152M parameters&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ Trained on &lt;strong&gt;~2B tokens&lt;/strong&gt; (vs ~2T for SmolLM2)&lt;/p&gt; &lt;p&gt;‚Ä¢ Hybrid &lt;strong&gt;GLA + FoX attention&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Test-Time Training (TTT)&lt;/strong&gt; during inference&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Selective Activation (sparse FFN)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;¬µP-scaled training&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ Fully open-source (Apache 2.0)&lt;/p&gt; &lt;p&gt;ü§ó Model: &lt;a href="https://huggingface.co/guiferrarib/genesis-152m-instruct"&gt;https://huggingface.co/guiferrarib/genesis-152m-instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üì¶ pip install genesis-llm&lt;/p&gt; &lt;p&gt;üìä &lt;strong&gt;Benchmarks (LightEval, Apple MPS)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;ARC-Easy ‚Üí 44.0% (random: 25%)&lt;/p&gt; &lt;p&gt;BoolQ ‚Üí 56.3% (random: 50%)&lt;/p&gt; &lt;p&gt;HellaSwag ‚Üí 30.2% (random: 25%)&lt;/p&gt; &lt;p&gt;SciQ ‚Üí 46.8% (random: 25%)&lt;/p&gt; &lt;p&gt;Winogrande ‚Üí 49.1% (random: 50%)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important context:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SmolLM2-135M was trained on &lt;strong&gt;~2 trillion tokens&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Genesis uses &lt;strong&gt;~2 billion tokens&lt;/strong&gt; ‚Äî so this is not a fair head-to-head, but an exploration of &lt;strong&gt;architecture vs data scaling&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;üß† &lt;strong&gt;Architecture Overview&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hybrid Attention (Qwen3-Next inspired)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Layer&lt;/strong&gt; &lt;strong&gt;%&lt;/strong&gt; &lt;strong&gt;Complexity&lt;/strong&gt; &lt;strong&gt;Role&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Gated DeltaNet (GLA) 75% O(n) Long-range efficiency&lt;/p&gt; &lt;p&gt;FoX (Forgetting Attention) 25% O(n¬≤) Precise retrieval&lt;/p&gt; &lt;p&gt;GLA uses:&lt;/p&gt; &lt;p&gt;‚Ä¢ Delta rule memory updates&lt;/p&gt; &lt;p&gt;‚Ä¢ Mamba-style gating&lt;/p&gt; &lt;p&gt;‚Ä¢ L2-normalized Q/K&lt;/p&gt; &lt;p&gt;‚Ä¢ Short convolutions&lt;/p&gt; &lt;p&gt;FoX adds:&lt;/p&gt; &lt;p&gt;‚Ä¢ Softmax attention&lt;/p&gt; &lt;p&gt;‚Ä¢ Data-dependent forget gate&lt;/p&gt; &lt;p&gt;‚Ä¢ Output gating&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Test-Time Training (TTT)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of frozen inference, Genesis can &lt;strong&gt;adapt online&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;‚Ä¢ Dual-form TTT (parallel gradients)&lt;/p&gt; &lt;p&gt;‚Ä¢ Low-rank updates (rank=4)&lt;/p&gt; &lt;p&gt;‚Ä¢ Learnable inner learning rate&lt;/p&gt; &lt;p&gt;Paper: &lt;em&gt;Learning to (Learn at Test Time)&lt;/em&gt; (MIT, ICML 2024)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Selective Activation (Sparse FFN)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SwiGLU FFNs with &lt;strong&gt;top-k activation masking&lt;/strong&gt; (85% kept).&lt;/p&gt; &lt;p&gt;Currently acts as &lt;strong&gt;regularization&lt;/strong&gt; ‚Äî real speedups need sparse kernels.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;¬µP Scaling + Zero-Centered RMSNorm&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ Hyperparameters tuned on small proxy&lt;/p&gt; &lt;p&gt;‚Ä¢ Transferred via ¬µP rules&lt;/p&gt; &lt;p&gt;‚Ä¢ Zero-centered RMSNorm for stable scaling&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Limitations (honest)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ Small training corpus (2B tokens)&lt;/p&gt; &lt;p&gt;‚Ä¢ TTT adds ~5‚Äì10% inference overhead&lt;/p&gt; &lt;p&gt;‚Ä¢ No RLHF&lt;/p&gt; &lt;p&gt;‚Ä¢ Experimental, not production-ready&lt;/p&gt; &lt;p&gt;üìé &lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ ü§ó Model: &lt;a href="https://huggingface.co/guiferrarib/genesis-152m-instruct"&gt;https://huggingface.co/guiferrarib/genesis-152m-instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ üì¶ PyPI: &lt;a href="https://pypi.org/project/genesis-llm/"&gt;https://pypi.org/project/genesis-llm/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôd really appreciate feedback ‚Äî especially from folks working on &lt;strong&gt;linear attention&lt;/strong&gt;, &lt;strong&gt;hybrid architectures&lt;/strong&gt;, or &lt;strong&gt;test-time adaptation&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Built by Orch-Mind Team&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kassanar"&gt; /u/Kassanar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw9n74/model_release_genesis152minstruct_exploring/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw9n74/model_release_genesis152minstruct_exploring/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw9n74/model_release_genesis152minstruct_exploring/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T17:23:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwhtht</id>
    <title>Building a local RAG for my 60GB email archive. Just hit a hardware wall (8GB RAM). Is this viable?</title>
    <updated>2025-12-26T23:05:44+00:00</updated>
    <author>
      <name>/u/Grouchy_Sun331</name>
      <uri>https://old.reddit.com/user/Grouchy_Sun331</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm sitting on about 60GB of emails (15+ years of history). Searching for specific context or attachments from years ago via standard clients (Outlook/Thunderbird) is painful. It‚Äôs slow, inaccurate, and I refuse to upload this data to any cloud-based SaaS for privacy reasons.&lt;/p&gt; &lt;p&gt;I‚Äôm planning to build a &amp;quot;stupid simple&amp;quot; local desktop tool to solve this (Electron + Python backend + Local Vector Store), but I need a sanity check before I sink weeks into development.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Concept:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Input:&lt;/strong&gt; Natively ingest local &lt;code&gt;.pst&lt;/code&gt; and &lt;code&gt;.mbox&lt;/code&gt; files (without manual conversion).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Engine:&lt;/strong&gt; Local Vector Store + Local LLM for RAG.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;UX:&lt;/strong&gt; Chat interface (&amp;quot;Find the invoice from the roofer in 2019&amp;quot; -&amp;gt; Returns context).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Reality Check (My test just now):&lt;/strong&gt; I just tried to simulate this workflow manually using Ollama on my current daily driver (Intel i5, 8GB RAM). &lt;strong&gt;It was a disaster.&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Phi-3 Mini (3.8B):&lt;/strong&gt; My RAM filled up, OS started swapping. It took &lt;strong&gt;15 minutes&lt;/strong&gt; to answer a simple query about a specific invoice.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TinyLlama (1.1B):&lt;/strong&gt; Ran without crashing, but still took &lt;strong&gt;~2 minutes&lt;/strong&gt; to generate a response.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My questions for you experts:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Hardware Barrier:&lt;/strong&gt; Is local RAG on standard office hardware (8GB RAM) effectively dead? Do I have to restrict this app to M-Series Macs / 16GB+ machines, or is there a hyper-optimized stack (e.g. quantization tricks, specific embedding models) I'm missing?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid Approach:&lt;/strong&gt; Given the results above, would you accept a &amp;quot;Hybrid Mode&amp;quot; where the index is local (privacy), but the inference happens via a secure API (like Mistral in Europe) to get speed back? Or does that defeat the purpose for you?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Existing Tools:&lt;/strong&gt; Is there already a polished open-source tool that handles raw &lt;code&gt;.pst&lt;/code&gt;/&lt;code&gt;.mbox&lt;/code&gt; ingestion? I found &amp;quot;Open WebUI&amp;quot; but looking for a standalone app experience.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks for the brutal honesty. I want to build this, but not if it only runs on $3000 workstations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Grouchy_Sun331"&gt; /u/Grouchy_Sun331 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwhtht/building_a_local_rag_for_my_60gb_email_archive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwhtht/building_a_local_rag_for_my_60gb_email_archive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwhtht/building_a_local_rag_for_my_60gb_email_archive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T23:05:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwmpcn</id>
    <title>llama.cpp: Multi-host inference slower than single-host?</title>
    <updated>2025-12-27T02:50:43+00:00</updated>
    <author>
      <name>/u/ayake_ayake</name>
      <uri>https://old.reddit.com/user/ayake_ayake</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! First of all, thanks for the amazing community as well awesome devs like those behind llama.cpp, langflow, etc. ü§ó &lt;/p&gt; &lt;p&gt;I have two computers running locally and I want to see how I can get faster generation speeds by combining them instead of running the models separately on each computer.&lt;/p&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Desktop &lt;ul&gt; &lt;li&gt;AMD CPU Ryzen 7 7800X3D 16 core&lt;/li&gt; &lt;li&gt;&lt;strong&gt;32 GB DDR5 RAM&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;AMD GPU Radeon RX 9060 XT &lt;strong&gt;16 GB VRAM&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;B650 EAGLE Mainboard&lt;/li&gt; &lt;li&gt;M.2 SSD&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Jetson &lt;ul&gt; &lt;li&gt;NVIDIA Jetson Orin AGX&lt;/li&gt; &lt;li&gt;ARM CPU Cortex-A78AE 12 cores&lt;/li&gt; &lt;li&gt;&lt;strong&gt;64 GB unified RAM LPDDR5&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;NVIDIA Ampere&lt;/li&gt; &lt;li&gt;M.2 SSD&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've built a very recent version of llama.cpp on both hosts (jetson using CUDA12 and Dekstop using ROCm 6.7). I use the unsloth Qwen3 80B Q8. This model is 87GBs and hence it's larger than both hosts individually, but the entire model fits into RAM when combined.&lt;/p&gt; &lt;p&gt;To run the multi-host setup, I use this: Desktop:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export GGML_CUDA_ENABLE_UNIFIED_MEMORY=1 # necessary, otherwise crashes very easily export ROCR_VISIBLE_DEVICES=0 # only use main GPU, not the integrated GPU llama-cli \ --model ./unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF/UD-Q8_K_XL/*00001-of-*.gguf \ --threads -1 \ --jinja \ --n-gpu-layers 99 \ -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; \ --ctx-size 16384 \ --seed 69 \ -sys &amp;quot;$SYS_PROMPT&amp;quot; \ --reasoning-budget -1 \ -p &amp;quot;Hey, I'm using llama.cpp!&amp;quot; \ --verbose \ --single-turn --rpc &amp;quot;$JETSON_IP_ADDR:12400&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Jetson:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export GGML_RPC_DEBUG=1 rpc-server --threads 12 --host 0.0.0.0 --port 12400 --cache &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Using both combined yields a generation speed of 1.1 t/s. However, if I use the desktop llama-cli command exactly the same as above but remove the --rpc &amp;quot;$JETSON_IP_ADDR:12400&amp;quot; (hence disabling multi-host), then I'm at &lt;strong&gt;double the speed&lt;/strong&gt; of 2.2 t/s.&lt;/p&gt; &lt;p&gt;So, I'm wondering... &lt;strong&gt;Why is the model slower when provided more RAM?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My intuition was, that llama.cpp splits by layers and doesn't do tensor parallelism - hence, the network of 1 Gbps is enough to send the minimal activations (a few kBs?) a few times per second for with low latency. Or am I wrong here?&lt;/p&gt; &lt;p&gt;During inference, I can see that the Desktop SSD has a read rate of 1 to 2 GiB/s - meaning that parts of the (MoE) model are being read from disk repeatedly... However, &lt;strong&gt;the network rate spikes to 16 to 24 MiB/s for each generated token&lt;/strong&gt; - which seems suspicious to me. (&lt;a href="https://cdn.discordapp.com/attachments/1454156741699965160/1454157023104073768/multi-host-desktop-usage.png?ex=695010c3&amp;amp;is=694ebf43&amp;amp;hm=462570552b360c7d71c955b2f739a56e0340950bb0f4325f76b2df9a63b092b8&amp;amp;"&gt;see image&lt;/a&gt;) What could be wrong in my configuration?&lt;/p&gt; &lt;p&gt;What do you folks think? Do you have ideas of what I could try or how I can debug this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayake_ayake"&gt; /u/ayake_ayake &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwmpcn/llamacpp_multihost_inference_slower_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwmpcn/llamacpp_multihost_inference_slower_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwmpcn/llamacpp_multihost_inference_slower_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T02:50:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwjk1y</id>
    <title>Updates of models on HF - Changelogs?</title>
    <updated>2025-12-27T00:23:15+00:00</updated>
    <author>
      <name>/u/Bird476Shed</name>
      <uri>https://old.reddit.com/user/Bird476Shed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see now (for example) Unsloth has updated some models from summer with a new revision, for example &lt;a href="https://huggingface.co/unsloth/GLM-4.5-Air-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.5-Air-GGUF&lt;/a&gt; - however in the commits history &lt;a href="https://huggingface.co/unsloth/GLM-4.5-Air-GGUF/commits/main"&gt;https://huggingface.co/unsloth/GLM-4.5-Air-GGUF/commits/main&lt;/a&gt; it only says &amp;quot;Upload folder using huggingface_hub&amp;quot;&lt;/p&gt; &lt;p&gt;What does that mean? Did something change? If yes, need to download again?&lt;/p&gt; &lt;p&gt;....how to keep track of these updates in models, when there is no changelog(?) or the commit log is useless(?)&lt;/p&gt; &lt;p&gt;What am I missing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bird476Shed"&gt; /u/Bird476Shed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwjk1y/updates_of_models_on_hf_changelogs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwjk1y/updates_of_models_on_hf_changelogs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwjk1y/updates_of_models_on_hf_changelogs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T00:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw8h6w</id>
    <title>GLM-4.7-6bit MLX vs MiniMax-M2.1-6bit MLX Benchmark Results on M3 Ultra 512GB</title>
    <updated>2025-12-26T16:35:28+00:00</updated>
    <author>
      <name>/u/uptonking</name>
      <uri>https://old.reddit.com/user/uptonking</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8h6w/glm476bit_mlx_vs_minimaxm216bit_mlx_benchmark/"&gt; &lt;img alt="GLM-4.7-6bit MLX vs MiniMax-M2.1-6bit MLX Benchmark Results on M3 Ultra 512GB" src="https://b.thumbs.redditmedia.com/M2P6WP9rpl4ZOUlAGcCSpfB_YOF4tnbUEiVovuoomHc.jpg" title="GLM-4.7-6bit MLX vs MiniMax-M2.1-6bit MLX Benchmark Results on M3 Ultra 512GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i find the benchmark result from twitter, which is very interesting.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Hardware: Apple M3 Ultra, 512GB. All tests with single M3 Ultra &lt;strong&gt;without batch inference&lt;/strong&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zwqsxk9btk9g1.png?width=4052&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1940693109fab3938946786fb719ad07bd73345c"&gt;glm-4.7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0nkcz4fetk9g1.png?width=4052&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=48a2d1eba5e5dd4ce8ecce705b01468c4931c47c"&gt;minimax-m2.1&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GLM-4.7-6bit MLX Benchmark Results with different context sizes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;0.5k Prompt: 98 - Gen: 16 t/s - 287.6GB&lt;br /&gt; 1k Prompt: 140 - Gen: 17 t/s - 288.0GB&lt;br /&gt; 2k Prompt: 206 - Gen: 16 t/s - 288.8GB&lt;br /&gt; 4k Prompt: 219 - Gen: 16 t/s - 289.6GB&lt;br /&gt; 8k Prompt: 210 - Gen: 14 t/s - 291.0GB&lt;br /&gt; 16k Prompt: 185 - Gen: 12 t/s - 293.9GB&lt;br /&gt; 32k Prompt: 134 - Gen: 10 t/s - 299.8GB&lt;br /&gt; 64k Prompt: 87 - Gen: 6 t/s - 312.1GB&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MiniMax-M2.1-6bit MLX Benchmark raw results with different context sizes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;0.5k Prompt: 239 - Gen: 42 t/s - 186.5GB&lt;br /&gt; 1k Prompt: 366 - Gen: 41 t/s - 186.8GB&lt;br /&gt; 2k Prompt: 517 - Gen: 40 t/s - 187.2GB&lt;br /&gt; 4k Prompt: 589 - Gen: 38 t/s - 187.8GB&lt;br /&gt; 8k Prompt: 607 - Gen: 35 t/s - 188.8GB&lt;br /&gt; 16k Prompt: 549 - Gen: 30 t/s - 190.9GB&lt;br /&gt; 32k Prompt: 429 - Gen: 21 t/s - 195.1GB&lt;br /&gt; 64k Prompt: 291 - Gen: 12 t/s - 203.4GB&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I would prefer minimax-m2.1 for general usage from the benchmark result, about &lt;strong&gt;~2.5x&lt;/strong&gt; prompt processing speed, &lt;strong&gt;~2x&lt;/strong&gt; token generation speed&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;sources: &lt;a href="https://x.com/ivanfioravanti/status/2004578941408039051"&gt;glm-4.7&lt;/a&gt; , &lt;a href="https://x.com/ivanfioravanti/status/2004569464407474555"&gt;minimax-m2.1&lt;/a&gt;, &lt;a href="https://x.com/ivanfioravanti/status/2004602428122169650"&gt;4bit-comparison&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p7kp5hcv1l9g1.jpg?width=1841&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c66839601a68efa3baf6c845bce91e8c2c8c2254"&gt;4bit-6bit-comparison&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- It seems that 4bit and 6bit have similar speed for prompt processing and token generation.&lt;br /&gt; - for the same model, 6bit's memory usage is about &lt;strong&gt;~1.4x&lt;/strong&gt; of 4bit. since RAM/VRAM is so expensive now, maybe it's not worth it (128GB x 1.4 = 179.2GB)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uptonking"&gt; /u/uptonking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8h6w/glm476bit_mlx_vs_minimaxm216bit_mlx_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8h6w/glm476bit_mlx_vs_minimaxm216bit_mlx_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8h6w/glm476bit_mlx_vs_minimaxm216bit_mlx_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T16:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw701k</id>
    <title>MiniMax-M2.1 GGUF is here!</title>
    <updated>2025-12-26T15:33:38+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw701k/minimaxm21_gguf_is_here/"&gt; &lt;img alt="MiniMax-M2.1 GGUF is here!" src="https://external-preview.redd.it/0xe3vYLHuf2Mb8WiNbMmuRGbcT2eNARsH6mkzOnOBgQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af9bda8765a5deb37e3c09288310949ab2d8704a" title="MiniMax-M2.1 GGUF is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I might've skipped going to bed for this one: &lt;a href="https://huggingface.co/AaryanK/MiniMax-M2.1-GGUF"&gt;https://huggingface.co/AaryanK/MiniMax-M2.1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From my runs:&lt;/p&gt; &lt;p&gt;model: MiniMax-M2.1.q2_k.gguf&lt;br /&gt; GPU: NVIDIA A100-SXM4-80GB&lt;/p&gt; &lt;p&gt;n_gpu_layers: 55&lt;br /&gt; context_size: 32768&lt;br /&gt; temperature: 0.7&lt;br /&gt; top_p: 0.9&lt;br /&gt; top_k: 40&lt;br /&gt; max_tokens: 512&lt;br /&gt; repeat_penalty: 1.1&lt;/p&gt; &lt;p&gt;[ Prompt: 28.0 t/s | Generation: 25.4 t/s ]&lt;/p&gt; &lt;p&gt;I am currently looking for open positions! ü§ó &lt;/p&gt; &lt;p&gt;If you find this model useful or are looking for a talented AI/LLM Engineer, please reach out to me on LinkedIn: &lt;a href="https://www.linkedin.com/in/theaaryankapoor/"&gt;Aaryan Kapoor&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy holidays!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AaryanK/MiniMax-M2.1-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw701k/minimaxm21_gguf_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw701k/minimaxm21_gguf_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T15:33:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwplsz</id>
    <title>Strix Halo llama-bench Results (GLM-4.5-Air)</title>
    <updated>2025-12-27T05:16:08+00:00</updated>
    <author>
      <name>/u/b0tbuilder</name>
      <uri>https://old.reddit.com/user/b0tbuilder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for anyone who has some benchmarks they would like to share. I am trying to optimize my EVO-X2 (Strix Halo) 128GB box using GLM-4.5-Air for use with Cline. Trying to find out if I am in the ballpark optimization wise.&lt;/p&gt; &lt;p&gt;Model Quantization: Q4_K_XL (Unsloth)&lt;/p&gt; &lt;p&gt;KV Cache Quantization: Q8_0&lt;/p&gt; &lt;p&gt;ROCM 7.10&lt;/p&gt; &lt;p&gt;| model | size | params | backend | ngl | threads | n_ubatch | type_k | type_v | fa | mmap | test | t/s |&lt;/p&gt; &lt;p&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -------: | -----: | -----: | -: | ---: | --------------: | -------------------: |&lt;/p&gt; &lt;p&gt;| glm4moe 106B.A12B Q4_K - Medium | 68.01 GiB | 110.47 B | ROCm | 99 | 8 | 2048 | q8_0 | q8_0 | 1 | 0 | pp256 | 166.89 ¬± 0.84 |&lt;/p&gt; &lt;p&gt;| glm4moe 106B.A12B Q4_K - Medium | 68.01 GiB | 110.47 B | ROCm | 99 | 8 | 2048 | q8_0 | q8_0 | 1 | 0 | pp512 | 261.15 ¬± 0.63 |&lt;/p&gt; &lt;p&gt;| glm4moe 106B.A12B Q4_K - Medium | 68.01 GiB | 110.47 B | ROCm | 99 | 8 | 2048 | q8_0 | q8_0 | 1 | 0 | pp2048 | 435.73 ¬± 0.86 |&lt;/p&gt; &lt;p&gt;| glm4moe 106B.A12B Q4_K - Medium | 68.01 GiB | 110.47 B | ROCm | 99 | 8 | 2048 | q8_0 | q8_0 | 1 | 0 | tg128 | 21.93 ¬± 0.03 |&lt;/p&gt; &lt;p&gt;| glm4moe 106B.A12B Q4_K - Medium | 68.01 GiB | 110.47 B | ROCm | 99 | 8 | 2048 | q8_0 | q8_0 | 1 | 0 | tg256 | 21.94 ¬± 0.04 |&lt;/p&gt; &lt;p&gt;| glm4moe 106B.A12B Q4_K - Medium | 68.01 GiB | 110.47 B | ROCm | 99 | 8 | 2048 | q8_0 | q8_0 | 1 | 0 | tg512 | 21.84 ¬± 0.01 |&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/b0tbuilder"&gt; /u/b0tbuilder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwplsz/strix_halo_llamabench_results_glm45air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwplsz/strix_halo_llamabench_results_glm45air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwplsz/strix_halo_llamabench_results_glm45air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T05:16:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwsk11</id>
    <title>what local LLMs (small!) do you recommend to train on epubs?</title>
    <updated>2025-12-27T08:09:17+00:00</updated>
    <author>
      <name>/u/sovereigndeveloper01</name>
      <uri>https://old.reddit.com/user/sovereigndeveloper01</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have so many epubs I can organize by author or genre to gain deep insights (with other sources) into an author's work for example. What language model should I start with to train on these epubs (preprocessing into TXT or MD or chapters already done)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sovereigndeveloper01"&gt; /u/sovereigndeveloper01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwsk11/what_local_llms_small_do_you_recommend_to_train/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwsk11/what_local_llms_small_do_you_recommend_to_train/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwsk11/what_local_llms_small_do_you_recommend_to_train/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T08:09:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwsa27</id>
    <title>LLM Running Locally in the Browser for Infinite Dropdowns</title>
    <updated>2025-12-27T07:52:07+00:00</updated>
    <author>
      <name>/u/ilikehikingalot</name>
      <uri>https://old.reddit.com/user/ilikehikingalot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwsa27/llm_running_locally_in_the_browser_for_infinite/"&gt; &lt;img alt="LLM Running Locally in the Browser for Infinite Dropdowns" src="https://external-preview.redd.it/Mzl0Yjc0bTdicDlnMajkkX4LG6wI_pxxo4qv3bbzlDaKVDsKMRLcrxEFlbW0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a9a07cfb8aec90d458fed8eb8e2c0089e474bbf" title="LLM Running Locally in the Browser for Infinite Dropdowns" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a site playing around with what an LLM running locally in the browser can do, feel free to check it out!&lt;/p&gt; &lt;p&gt;The static site is: &lt;a href="https://rohanadwankar.github.io/unravel/"&gt;https://rohanadwankar.github.io/unravel/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Github repo is: &lt;a href="https://github.com/RohanAdwankar/unravel"&gt;https://github.com/RohanAdwankar/unravel&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you are interested in how it works with &lt;a href="https://github.com/mlc-ai/mlc-llm"&gt;MLC&lt;/a&gt; check out the HTML file in &lt;a href="https://github.com/RohanAdwankar/unravel/commit/91690c0e7f8d190cbcd87276fbf2674552952079"&gt;this commit&lt;/a&gt; which enables running an LLM locally in under 50 lines!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilikehikingalot"&gt; /u/ilikehikingalot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/0m5y5al7bp9g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwsa27/llm_running_locally_in_the_browser_for_infinite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwsa27/llm_running_locally_in_the_browser_for_infinite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T07:52:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwwtk1</id>
    <title>2x Mac mini m4 pro 64 gb each with RDMA for local llms?</title>
    <updated>2025-12-27T12:35:11+00:00</updated>
    <author>
      <name>/u/Forward_Act4138</name>
      <uri>https://old.reddit.com/user/Forward_Act4138</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm planning a local LLM setup using &lt;strong&gt;two Mac mini M4 Pro units&lt;/strong&gt; (each with &lt;strong&gt;64 GB RAM&lt;/strong&gt;) and &lt;strong&gt;RDMA&lt;/strong&gt; between them. I‚Äôm trying to figure out what kind of performance I should realistically expect.&lt;/p&gt; &lt;p&gt;Anyone tested something like &lt;strong&gt;GPT-OSS 120B&lt;/strong&gt; (or similarly sized models) on this hardware? What were your real measurements (tokens/sec, memory usage, context scaling behavior)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Forward_Act4138"&gt; /u/Forward_Act4138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwtk1/2x_mac_mini_m4_pro_64_gb_each_with_rdma_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwtk1/2x_mac_mini_m4_pro_64_gb_each_with_rdma_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwtk1/2x_mac_mini_m4_pro_64_gb_each_with_rdma_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T12:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw3fih</id>
    <title>MiniMax M2.1 is OPEN SOURCE: SOTA for real-world dev &amp; agents</title>
    <updated>2025-12-26T12:43:08+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw3fih/minimax_m21_is_open_source_sota_for_realworld_dev/"&gt; &lt;img alt="MiniMax M2.1 is OPEN SOURCE: SOTA for real-world dev &amp;amp; agents" src="https://preview.redd.it/mxsku2dnnj9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b27c9e0dbc5e46995d16f434d126d93ba14f68da" title="MiniMax M2.1 is OPEN SOURCE: SOTA for real-world dev &amp;amp; agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.1"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M2.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;SOTA on coding benchmarks (SWE / VIBE / Multi-SWE) ‚Ä¢ Beats Gemini 3 Pro &amp;amp; Claude Sonnet 4.5 ‚Ä¢ 10B active / 230B total (MoE)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mxsku2dnnj9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw3fih/minimax_m21_is_open_source_sota_for_realworld_dev/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw3fih/minimax_m21_is_open_source_sota_for_realworld_dev/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T12:43:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwsb7i</id>
    <title>How is the Speculative Decoding Algorithm Constructed?</title>
    <updated>2025-12-27T07:54:04+00:00</updated>
    <author>
      <name>/u/song-sc</name>
      <uri>https://old.reddit.com/user/song-sc</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/song-sc"&gt; /u/song-sc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ki-seki.github.io/posts/251226-spec-decoding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwsb7i/how_is_the_speculative_decoding_algorithm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwsb7i/how_is_the_speculative_decoding_algorithm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T07:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwwd99</id>
    <title>A Farmer Doesn‚Äôt Know Coding, But Tries to Build an Executing Engine with LLMs and a Code Interpreter</title>
    <updated>2025-12-27T12:09:37+00:00</updated>
    <author>
      <name>/u/amadale</name>
      <uri>https://old.reddit.com/user/amadale</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Translated from Korean. I wrote the original in Korean and translated it myself. Final meaning and responsibility are mine.&lt;/p&gt; &lt;p&gt;I‚Äôm a garlic farmer in Korea. When I‚Äôm not working in the field, I use AI chat interfaces as my personal lab. I experiment with AIs that have sandboxed code interpreters, and I slowly build scripts that become a kind of ‚Äúengine.‚Äù I don‚Äôt start from code. I start by talking to the AI, giving my thoughts and structural ideas first. Using the web tools inside the AI chat, I collect and structure information. Then I run that structured information again inside the code interpreter, and I only take the actual execution results and move forward with explainable analysis (XAI). Through this process, the concepts slowly grow, and step by step I give the AI more concrete direction. You can think of it like this: the LLM and the engine inside the sandboxed code interpreter form an indirect pipeline. User input ‚Üí web tool search ‚Üí LLM structures ‚Üí that result is executed by the code interpreter. This is important for me: this is not an environment where I directly build pipelines with APIs. Everything happens inside the AI chat UI that I use every day. By the way, what I call a ‚Äúsandboxed code interpreter‚Äù has different names depending on company or product (Code Interpreter, Data Analysis / Advanced Data Analysis, Code Execution, etc). But the core meaning is the same: An isolated execution environment where code actually runs inside the chat window (a sandboxed execution environment). And the ‚Äúengine‚Äù I talk about is nothing fancy. It is just Python scripts running inside that sandbox (analysis scripts / verification scripts), and the execution-backed verification loop that repeats again and again.&lt;/p&gt; &lt;p&gt;The Question of Real Execution The biggest problem in this whole process is very simple: Is the code interpreter in the chat really executing, or not? If it is not actually executing, what comes out is close to hallucination ‚Äî just simulated text with no real meaning. I have seen this many times: the output looks like execution results, but in reality nothing was executed at all. So the key question becomes only one thing for me: ‚ÄúIs this execution real right now?‚Äù In my case, I use reproducible code, like random-number-based checks, and make multiple AIs cross-check each other. Below is the overall flow I use, drawn in a very simple way:&lt;/p&gt; &lt;p&gt;[Me (input / thoughts)] | v [Web tool search (optional)] | v [LLM conversation / structuring] | v [Engine: sandboxed Python execution] | v [Execution output] | v [XAI / next direction] | v [Loop repeats]&lt;/p&gt; &lt;p&gt;The point I care about most is here: [Sandboxed Python execution] | +-- (execution is real) | -&amp;gt; reproducible output remains | -&amp;gt; becomes verifiable evidence | +-- (execution is fake) -&amp;gt; hallucinated / simulated text looks like ‚Äúreal output‚Äù -&amp;gt; high risk of wrong judgment&lt;/p&gt; &lt;p&gt;That is why I try to confirm, as much as possible, whether execution was real, by using reproducible code and cross-checking across multiple AIs. In the end, I feel that the ability to notice hallucination is also a kind of personal know-how. After many experiences, my conclusion is clear: With only one AI, it is almost impossible to get the result I want. You must cross-check between multiple AIs.&lt;/p&gt; &lt;p&gt;I want to say this clearly again: I am just a farmer. I only spend small pieces of time, working together with AIs like this. I don‚Äôt know if this method is ‚Äúcorrect‚Äù, but sometimes meaningful results come out, so I keep using it. As time goes on, it feels more and more refined. Each AI has a different mechanism, so sometimes it is confusing. But I feel that the overall frame does work, especially when multiple AIs respond together in a consistent way. Because this is just a personal experiment, sometimes it makes my head hurt. But at the same time, I get many insights because of the AIs. Especially, I often feel that the diversity of AIs itself creates real effectiveness. What do you think about this way of working? I don‚Äôt really know how to code, but I learn by talking with AIs. Since multiple AIs give different opinions, I focus only on direction and intent, and let the AIs handle the experiments. Many times, this gives better results than I expected. When I watch code flowing down on my phone screen, it sometimes feels like watching the code scenes from the movie Matrix. And honestly, that part is fun by itself.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amadale"&gt; /u/amadale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwd99/a_farmer_doesnt_know_coding_but_tries_to_build_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwd99/a_farmer_doesnt_know_coding_but_tries_to_build_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwd99/a_farmer_doesnt_know_coding_but_tries_to_build_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T12:09:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwf8p7</id>
    <title>What's the point of potato-tier LLMs?</title>
    <updated>2025-12-26T21:15:23+00:00</updated>
    <author>
      <name>/u/Fast_Thing_7949</name>
      <uri>https://old.reddit.com/user/Fast_Thing_7949</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwf8p7/whats_the_point_of_potatotier_llms/"&gt; &lt;img alt="What's the point of potato-tier LLMs?" src="https://b.thumbs.redditmedia.com/F0uF4Io7WMY9lYOzcakie1qYAc-lSDqqyibCA7Pa_qs.jpg" title="What's the point of potato-tier LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/64wjim607m9g1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fb5666c56138804f6be65ef56b519345f992b4cd"&gt;https://preview.redd.it/64wjim607m9g1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fb5666c56138804f6be65ef56b519345f992b4cd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After getting brought back down to earth in my last thread about replacing Claude with local models on an RTX 3090, I've got another question that's genuinely bothering me: What are 7b, 20b, 30B parameter models actually FOR? I see them released everywhere, but are they just benchmark toys so AI labs can compete on leaderboards, or is there some practical use case I'm too dense to understand? Because right now, I can't figure out what you're supposed to do with a potato-tier 7B model that can't code worth a damn and is slower than API calls anyway. &lt;/p&gt; &lt;p&gt;Seriously, what's the real-world application besides &amp;quot;I have a GPU and want to feel like I'm doing AI&amp;quot;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fast_Thing_7949"&gt; /u/Fast_Thing_7949 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwf8p7/whats_the_point_of_potatotier_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwf8p7/whats_the_point_of_potatotier_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwf8p7/whats_the_point_of_potatotier_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T21:15:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw8nfk</id>
    <title>Nvidia acquired Groq, but why not Cerebras? Cerebras is 3x times faster than Groq, while maximum 1.5x the price. Anyone can explain?</title>
    <updated>2025-12-26T16:42:23+00:00</updated>
    <author>
      <name>/u/Conscious_Warrior</name>
      <uri>https://old.reddit.com/user/Conscious_Warrior</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone with technical knowledge can explain why they chose Groq over Cerebras? Really interested in this. Because Cerebras is even waaay faster than Groq. Cerebras seems like a bigger threat to Nvidia than Groq...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Warrior"&gt; /u/Conscious_Warrior &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8nfk/nvidia_acquired_groq_but_why_not_cerebras/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8nfk/nvidia_acquired_groq_but_why_not_cerebras/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8nfk/nvidia_acquired_groq_but_why_not_cerebras/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T16:42:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwx3n5</id>
    <title>Small ai model for a school project.</title>
    <updated>2025-12-27T12:50:33+00:00</updated>
    <author>
      <name>/u/Substantial_Cod_6019</name>
      <uri>https://old.reddit.com/user/Substantial_Cod_6019</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys I need help with my school project. It's for my finals in high school. I set out to create small ai model that will predict wheter the price will go up or down based on the news that come out about the company. &lt;/p&gt; &lt;p&gt;The stock it will be trying to predict is $APPL. I downloaded already some datasets that have a lot of data about how certain news affected the stock in the past.&lt;/p&gt; &lt;p&gt;It will be predicting if the price will increase or decrease, not by how many points. &lt;/p&gt; &lt;p&gt;Can you please help me with this, maybe give me some reccommendations for tools, programming languages and sources where I can learn how to do something like this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Cod_6019"&gt; /u/Substantial_Cod_6019 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwx3n5/small_ai_model_for_a_school_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwx3n5/small_ai_model_for_a_school_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwx3n5/small_ai_model_for_a_school_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T12:50:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwwsag</id>
    <title>The Infinite Software Crisis: We're generating complex, unmaintainable code faster than we can understand it. Is 'vibe-coding' the ultimate trap?</title>
    <updated>2025-12-27T12:33:15+00:00</updated>
    <author>
      <name>/u/madSaiyanUltra_9789</name>
      <uri>https://old.reddit.com/user/madSaiyanUltra_9789</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/"&gt; &lt;img alt="The Infinite Software Crisis: We're generating complex, unmaintainable code faster than we can understand it. Is 'vibe-coding' the ultimate trap?" src="https://b.thumbs.redditmedia.com/CWbH9aocZDksMFttENvwXtHE-6zGD_eJPUb93Q-jv0M.jpg" title="The Infinite Software Crisis: We're generating complex, unmaintainable code faster than we can understand it. Is 'vibe-coding' the ultimate trap?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, &lt;/p&gt; &lt;p&gt;I just watched &lt;a href="https://www.youtube.com/watch?v=eIoohUmYpGI"&gt;The Infinite Software Crisis ‚Äì Jake Nations&lt;/a&gt; on YouTube and it got me thinking... the limitations of software development has never been typing speed, but rather our ability to comprehend and design the system correctly in the first place. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights from the talk:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Every developer has shipped code they didn't completely understand. it passed the tests and that was enough validation to deploy it.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The hard part is timeless:&lt;/strong&gt; The hard part isn't the mechanics of coding; it's the conceptual difficulty of designing a solution. Every tool, including AI, just makes implementation easier.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI amplifies the problem:&lt;/strong&gt; We can now generate code as fast as we can describe it. The scale is infinite, but our comprehension isn't. The core challenge of understanding &lt;em&gt;what&lt;/em&gt; to build remains.&lt;/li&gt; &lt;li&gt;The real trap we fall into is confusing easy with simple. &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Easy&lt;/strong&gt; is what's within reach. What can you access without effort? Generate it with AI, copy-paste, or install a framework. It's about speed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simple&lt;/strong&gt; is about structure. It means one fold, one braid, no entanglement. It requires thought and design.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;LLMs do not understand logic, they merely relate language and substitute those relations as &amp;quot;code&amp;quot;, so the importance of &lt;em&gt;patterns and architectural decisions&lt;/em&gt; in your codebase are lost. &lt;/li&gt; &lt;li&gt;when &amp;quot;vibe-coding&amp;quot; technical debt doesn't register as debt; it's just more code to preserve. &lt;/li&gt; &lt;li&gt;The result? Complex, highly-coupled, and error-prone code generated in minutes that could take you weeks to understand (if ever).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The real danger here is that we're accumulating complexity faster than we can comprehend it because we're not doing the hard work of understanding our systems.&lt;/p&gt; &lt;p&gt;The proposed solution: SLOW DOWN, DO EVERYTHING MANUALLY; architectural design + scaffolding, etc and only let the LLM in at the last step of filling in the scaffolding. &lt;/p&gt; &lt;p&gt;What's your take, Is 'vibe-coding' a trap, or is there a way to use these tools without losing the ability to understand our systems? &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c4mknoudlq9g1.png?width=553&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=28a6f37623fb0e0725f5b603f4b3a8ce51653ac9"&gt;https://preview.redd.it/c4mknoudlq9g1.png?width=553&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=28a6f37623fb0e0725f5b603f4b3a8ce51653ac9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madSaiyanUltra_9789"&gt; /u/madSaiyanUltra_9789 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T12:33:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pweljh</id>
    <title>NVIDIA has 72GB VRAM version now</title>
    <updated>2025-12-26T20:48:17+00:00</updated>
    <author>
      <name>/u/decentralize999</name>
      <uri>https://old.reddit.com/user/decentralize999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/"&gt; &lt;img alt="NVIDIA has 72GB VRAM version now" src="https://external-preview.redd.it/sC0_RV1rBP5Nka4zzrlrlknHQcvT_QUrChxq3hP_lVg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e745729c3f7132892c715292c6b31f385f223e8f" title="NVIDIA has 72GB VRAM version now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is 96GB too expensive? And AI community has no interest for 48GB?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/decentralize999"&gt; /u/decentralize999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.nvidia.com/en-us/products/workstations/professional-desktop-gpus/rtx-pro-5000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T20:48:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwt6ir</id>
    <title>Asus isn't going into memory manufacturing ‚Äî Taiwanese tech giant issues statement smashing rumor</title>
    <updated>2025-12-27T08:49:41+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwt6ir/asus_isnt_going_into_memory_manufacturing/"&gt; &lt;img alt="Asus isn't going into memory manufacturing ‚Äî Taiwanese tech giant issues statement smashing rumor" src="https://external-preview.redd.it/iOAde8UE4DyQsY7bL3QZWs7PMlmK1Bl2f85BU0xU79M.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c2acdf39b02a0864d08303117e213729c9c540f" title="Asus isn't going into memory manufacturing ‚Äî Taiwanese tech giant issues statement smashing rumor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/dram/no-asus-isnt-going-into-memory-manufacturing-taiwanese-tech-giant-issues-statement-smashing-rumor"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwt6ir/asus_isnt_going_into_memory_manufacturing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwt6ir/asus_isnt_going_into_memory_manufacturing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T08:49:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
