<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-10T15:49:01+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q8f242</id>
    <title>After 8 years building cloud infrastructure, I'm betting on local-first AI</title>
    <updated>2026-01-09T17:48:38+00:00</updated>
    <author>
      <name>/u/PandaAvailable2504</name>
      <uri>https://old.reddit.com/user/PandaAvailable2504</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sold my Saas company last year and we used to process everything in the cloud. Now, after a few realisations, I'm doing the opposite. As I watch the AI space evolve, I can’t help but wonder how there’s a growing sentiment of wanting capable models that run on hardware they control. More people seem to be moving towards local inference: whether for privacy, cost, latency, or just independence from API rate limits. &lt;/p&gt; &lt;p&gt;Curious if anyone else is thinking about this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PandaAvailable2504"&gt; /u/PandaAvailable2504 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8f242/after_8_years_building_cloud_infrastructure_im/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8f242/after_8_years_building_cloud_infrastructure_im/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8f242/after_8_years_building_cloud_infrastructure_im/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T17:48:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1q97081</id>
    <title>Quantized KV Cache</title>
    <updated>2026-01-10T15:32:47+00:00</updated>
    <author>
      <name>/u/val_in_tech</name>
      <uri>https://old.reddit.com/user/val_in_tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have you tried to compare different quantized KV options for your local models? What's considered a sweet spot? Is performance degradation consistent across different models or is it very model specific?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/val_in_tech"&gt; /u/val_in_tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q97081/quantized_kv_cache/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q97081/quantized_kv_cache/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q97081/quantized_kv_cache/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T15:32:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8xd6f</id>
    <title>Higgs Audio v2 GUI with many features</title>
    <updated>2026-01-10T06:57:37+00:00</updated>
    <author>
      <name>/u/Mar00ned</name>
      <uri>https://old.reddit.com/user/Mar00ned</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been obsessed with Higgs v2 as it's been incredible for my use case. I couldn't find a good GUI so I've been creating one.&lt;/p&gt; &lt;p&gt;While I originally used ComfyUI with TTS-Suite, there were still a few parameters that couldn't be tweaked easily that I needed, which lead to this piece of work.&lt;/p&gt; &lt;p&gt;If you're someone who wants to be able to adjust a lot of the parameters that are available in the Higgs generate.py but from a GUI, hopefully this will work for you.&lt;/p&gt; &lt;p&gt;The only thing it requires is to install Gradio in your python environment, it goes right into your higgs-audio install directory under the &amp;quot;examples&amp;quot; folder, so it should be simple to implement.&lt;/p&gt; &lt;p&gt;Please note, this is my first publishing experience on GitHub and I'm still learning Gradio, so please try to be kind.&lt;/p&gt; &lt;p&gt;If you're interested or have feedback, please check out the repository.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Tenidus/Higgs-Audio-v2-Gradio-Interface"&gt;https://github.com/Tenidus/Higgs-Audio-v2-Gradio-Interface&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mar00ned"&gt; /u/Mar00ned &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8xd6f/higgs_audio_v2_gui_with_many_features/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8xd6f/higgs_audio_v2_gui_with_many_features/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8xd6f/higgs_audio_v2_gui_with_many_features/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T06:57:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1q97bsl</id>
    <title>MLX and Image Generation Support Coming to Ollama</title>
    <updated>2026-01-10T15:45:31+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty exciting!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama/releases"&gt;https://github.com/ollama/ollama/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama/pull/13648"&gt;https://github.com/ollama/ollama/pull/13648&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q97bsl/mlx_and_image_generation_support_coming_to_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q97bsl/mlx_and_image_generation_support_coming_to_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q97bsl/mlx_and_image_generation_support_coming_to_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T15:45:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q9099u</id>
    <title>"Safe" abliteration methods</title>
    <updated>2026-01-10T09:54:33+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many uncensored models suffer from degraded logic or hallucinations, but I noticed a few modern abliteration methods that claim to actually remove refusals without damaging the models: Norm-Preserving Biprojected Abliteration, now &lt;a href="https://huggingface.co/blog/grimjim/projected-abliteration"&gt;MPOA&lt;/a&gt; - by grimjim, also used by ArliAI; and Projected Refusal Isolation via Subspace Modification (PRISM, couldn't find any details about it) - by Ex0bit&lt;/p&gt; &lt;p&gt;Did anyone test/compare these methods?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9099u/safe_abliteration_methods/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q9099u/safe_abliteration_methods/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q9099u/safe_abliteration_methods/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T09:54:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8scwx</id>
    <title>I built an open-source tool to analyze spine MRI scans locally.</title>
    <updated>2026-01-10T02:42:54+00:00</updated>
    <author>
      <name>/u/Erdeem</name>
      <uri>https://old.reddit.com/user/Erdeem</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a project to democratize medical imaging analysis and wanted to share it with the community. &lt;a href="https://github.com/NeoNogin/MRI-GPT"&gt;MRI-GPT&lt;/a&gt; allows you to drag-and-drop spine MRI (DICOM) files and generates a detailed pathology report that you can chat with, running entirely on your local machine.&lt;/p&gt; &lt;p&gt;The biggest challenge with using Vision Language Models for medical imaging has always been localization. General models are smart, but they get lost easily—often hallucinating a herniation at L4 because they are actually looking at L3.&lt;/p&gt; &lt;p&gt;I solved this by decoupling the &amp;quot;eyes&amp;quot; (segmentation) from the &amp;quot;brain&amp;quot; (Qwen3).&lt;/p&gt; &lt;p&gt;How it works: 3D Localization (The Eyes): Uses nnU-Net to map every vertebra in 3D space with high precision. This ensures we know exactly where L4, L5, and S1 are before the LLM even gets involved.&lt;/p&gt; &lt;p&gt;Smart Sampling: Calculates the geometric center of each disc to grab the &amp;quot;sweet spot&amp;quot; slice (mid-sagittal). This drastically reduces context window usage and noise.&lt;/p&gt; &lt;p&gt;Vision Analysis (The Brain): Feeds a 3-slice montage to a local Qwen3-VL:8b (via Ollama) with anatomy-specific dynamic prompts.&lt;/p&gt; &lt;p&gt;Chat: You can chat with the report to ask follow-up questions.&lt;/p&gt; &lt;p&gt;Why Qwen3-VL:8b + Segmentation? We chose the newly released Qwen3-VL:8b over previous iterations (like Qwen2.5) because of a critical synergy with our segmentation pipeline:&lt;/p&gt; &lt;p&gt;Solving the &amp;quot;Localization Gap&amp;quot;: Benchmarks (like SpineBench) showed that older models like Qwen2.5-VL had terrible localization accuracy (~12-15%) on their own. They knew what a herniation looked like, but not where it was. By handling localization with TotalSpineSeg, we feed Qwen3 the exact right image slice.&lt;/p&gt; &lt;p&gt;Reduced Hallucination: Qwen3-VL features significantly improved instruction-following capabilities over 2.5. When we prompt it with specific anatomical context (&amp;quot;Analyze the L4-L5 disc space in this crop&amp;quot;), it adheres to that constraint much better, reducing the &amp;quot;negative transfer&amp;quot; where models hallucinate diseases based on general training data rather than the actual pixel data.&lt;/p&gt; &lt;p&gt;Efficiency: The 8b model is lightweight enough to run locally on consumer GPUs but, when focused on a pre-segmented image, rivals the diagnostic accuracy of much larger 70B+ models.&lt;/p&gt; &lt;p&gt;A one click (more like 3 click) installer is available &lt;a href="https://github.com/NeoNogin/MRI-GPT/releases/tag/MRI-GPT_v1.0.0-beta"&gt;here.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I made this for my personal use. I am not a medical doctor. It is far from perfect and has gone through VERY limited number of tests, however, it was over 90% accurate with edge cases throwing it off (prior surgeries that let to hardware being installed) and it can be a little over sensitive where it would for example label a mild issue as a moderate one. I have not tested for fractures. I have not tested the thoracic spin due to limited availability of that dataset (apparently its not common to get thoracic spine MRI). For those reasons and more I added the option to include context with your images- which can be anything from &amp;quot;I slept funny&amp;quot;, to an entire MRI report from your doctor. The context will improve accuracy. &lt;/p&gt; &lt;p&gt;Future plans are to include support MRIs of the entire body. &lt;/p&gt; &lt;p&gt;Let me know if you have any questions or requests.&lt;/p&gt; &lt;p&gt;THIS SOFTWARE IS FOR RESEARCH AND EDUCATIONAL PURPOSES ONLY. NOT FOR CLINICAL DIAGNOSIS.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Erdeem"&gt; /u/Erdeem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8scwx/i_built_an_opensource_tool_to_analyze_spine_mri/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8scwx/i_built_an_opensource_tool_to_analyze_spine_mri/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8scwx/i_built_an_opensource_tool_to_analyze_spine_mri/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T02:42:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1q92olo</id>
    <title>Qwen3-VL for OCR: PDF pre-processing + prompt approach?</title>
    <updated>2026-01-10T12:18:41+00:00</updated>
    <author>
      <name>/u/Intelligent-Form6624</name>
      <uri>https://old.reddit.com/user/Intelligent-Form6624</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been testing VLMs for OCR of PDF documents. Mainly contracts with a simple layout. Conversion to markdown or JSON is preferred. &lt;/p&gt; &lt;p&gt;So far, I’ve mainly used specialised OCR models such as Deepseek-OCR and olmOCR 2.&lt;/p&gt; &lt;p&gt;However, I’ve noticed many commenters in this forum praising Qwen3-VL. So I plan on trying Qwen3-VL-30B-A3B-Instruct.&lt;/p&gt; &lt;p&gt;It seems most specialised OCR models have accompanying Python packages that take care of pre-processing and prompting.&lt;/p&gt; &lt;p&gt;What about Qwen3? Is there a preferred package or approach for processing the PDF and presenting it to the model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Intelligent-Form6624"&gt; /u/Intelligent-Form6624 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q92olo/qwen3vl_for_ocr_pdf_preprocessing_prompt_approach/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q92olo/qwen3vl_for_ocr_pdf_preprocessing_prompt_approach/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q92olo/qwen3vl_for_ocr_pdf_preprocessing_prompt_approach/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T12:18:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1q96x42</id>
    <title>Your favorite Claude replacement and MCPs</title>
    <updated>2026-01-10T15:29:24+00:00</updated>
    <author>
      <name>/u/val_in_tech</name>
      <uri>https://old.reddit.com/user/val_in_tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Opencode with searchNG/context7 seems like a solid combo. The closest I've seen to Claude Code so far. What are you favorites?&lt;/p&gt; &lt;p&gt;I also tried to run CC with own model served via Anthropic compatible endpoint on VLLM. It works, but haven't been using long enough. Its nice that the web searches go thru their servers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/val_in_tech"&gt; /u/val_in_tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q96x42/your_favorite_claude_replacement_and_mcps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q96x42/your_favorite_claude_replacement_and_mcps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q96x42/your_favorite_claude_replacement_and_mcps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T15:29:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8lt9c</id>
    <title>PSA: HF seems to be removing grandfathered limits on private storage and billing people on it.</title>
    <updated>2026-01-09T22:03:39+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HF is twisting the screw on their storage billing. I believe than when they announced changes, they grandfathered in storage limits for people who were over a 1 TB limit. I got 1.34TB limit.&lt;/p&gt; &lt;p&gt;Well, now this is over and I got billed additional $25 for keeping my files as is - anything over the first 1TB is counted as another 1TB bought, at $25/TB rate. I uploaded just around 20GB since November 30th, and I wasn't billed for that 1.34TB earlier.&lt;/p&gt; &lt;p&gt;Watch out for surprise bills!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8lt9c/psa_hf_seems_to_be_removing_grandfathered_limits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8lt9c/psa_hf_seems_to_be_removing_grandfathered_limits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8lt9c/psa_hf_seems_to_be_removing_grandfathered_limits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T22:03:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8uyhj</id>
    <title>I built a 100% local Audio RAG pipeline to index 4-hour city council meetings. Runs on an RTX 2060. (Whisper + Ollama + ChromaDB)</title>
    <updated>2026-01-10T04:48:34+00:00</updated>
    <author>
      <name>/u/alias454</name>
      <uri>https://old.reddit.com/user/alias454</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a bit of a late-comer with LLMs for personal use. I'm sharing this to document that a lot can be done with limited hardware resources.&lt;/p&gt; &lt;p&gt;I’ve spent 4 weeks building a tool I named YATSEE. It is a local-first pipeline designed to turn unstructured audio (think 4-hour jargon-filled city council meetings) into clean searchable summaries. &lt;/p&gt; &lt;p&gt;The Tech Stack (100% Offline):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ingestion: yt-dlp for automated retrieval.&lt;/li&gt; &lt;li&gt;Audio Prep: ffmpeg for conversion/chunking (16kHz mono).&lt;/li&gt; &lt;li&gt;Transcription: faster-whisper (or standard OpenAI whisper).&lt;/li&gt; &lt;li&gt;Normalization: spaCy (used for clean up of raw transcripts produce.&lt;/li&gt; &lt;li&gt;Summarization: Ollama (running local LLMs like Llama 3 or Mistral).&lt;/li&gt; &lt;li&gt;RAG/Search: ChromaDB for vector storage + Streamlit for the UI.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hardware:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lenovo Legion 5, RTX 2060, 32GB RAM (Fedora Linux)&lt;/li&gt; &lt;li&gt;Base M4 Mac mini, 16GB unified RAM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This was a fun project to get my feet wet with local LLMs. You can check out the code on github &lt;a href="https://github.com/alias454/YATSEE"&gt;https://github.com/alias454/YATSEE&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;I'm interested in exploring smaller models vs larger ones. Any feedback on that would be great.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alias454"&gt; /u/alias454 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8uyhj/i_built_a_100_local_audio_rag_pipeline_to_index/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8uyhj/i_built_a_100_local_audio_rag_pipeline_to_index/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8uyhj/i_built_a_100_local_audio_rag_pipeline_to_index/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T04:48:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8zjvw</id>
    <title>For my RTX 5090 what are the best local image-gen and animation/video AIs right now?</title>
    <updated>2026-01-10T09:10:28+00:00</updated>
    <author>
      <name>/u/TomNaughtyy</name>
      <uri>https://old.reddit.com/user/TomNaughtyy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve got a 5090 and I want to run generative AI locally (no cloud).&lt;/p&gt; &lt;p&gt;I’m looking for suggestions on: &lt;/p&gt; &lt;p&gt;Image generation (text-to-image, image-to-image)&lt;br /&gt; Animation / video generation (text-to-video or image-to-video), if feasible locally&lt;/p&gt; &lt;p&gt;What are the best models/tools to run locally right now for quality and for speed?&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TomNaughtyy"&gt; /u/TomNaughtyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8zjvw/for_my_rtx_5090_what_are_the_best_local_imagegen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8zjvw/for_my_rtx_5090_what_are_the_best_local_imagegen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8zjvw/for_my_rtx_5090_what_are_the_best_local_imagegen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T09:10:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8fagh</id>
    <title>RTX Blackwell Pro 6000 wholesale pricing has dropped by $150-200</title>
    <updated>2026-01-09T17:57:11+00:00</updated>
    <author>
      <name>/u/TastesLikeOwlbear</name>
      <uri>https://old.reddit.com/user/TastesLikeOwlbear</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Obviously the RTX Blackwell Pro 6000 cards are of great interest to the people here. I see them come up a lot. And we all ooh and ahh over the people that have 8 of them lined up in a nice row.&lt;/p&gt; &lt;p&gt;It also seems to me like the market is suffering from lack of transparency on these.&lt;/p&gt; &lt;p&gt;My employer buys these cards wholesale, and I can see current pricing and stock in our distributors' systems. (And I &lt;strong&gt;may have&lt;/strong&gt; slipped in an order for one for myself...) It's eye-opening.&lt;/p&gt; &lt;p&gt;I'm probably not supposed to disclose the exact price we buy these at. But I wanted people to know that unlike everything else with RAM in it, the wholesale price of these has &lt;strong&gt;dropped&lt;/strong&gt; by about ~$150-200 from December to January.&lt;/p&gt; &lt;p&gt;I will also say that the wholesale price for the 6000 Pro is only about $600 higher than the wholesale price for the new 72GiB 5000 Pro. So, for the love of god, please don't buy that!&lt;/p&gt; &lt;p&gt;(And no, this is &lt;strong&gt;not&lt;/strong&gt; marketing or an ad; I &lt;strong&gt;cannot&lt;/strong&gt; sell &lt;strong&gt;anyone&lt;/strong&gt; these cards at &lt;strong&gt;any&lt;/strong&gt; price. I would be fired immediately. I just want people to have the best available information when they're looking to buy something this expensive.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TastesLikeOwlbear"&gt; /u/TastesLikeOwlbear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8fagh/rtx_blackwell_pro_6000_wholesale_pricing_has/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8fagh/rtx_blackwell_pro_6000_wholesale_pricing_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8fagh/rtx_blackwell_pro_6000_wholesale_pricing_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T17:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1q88hdc</id>
    <title>(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability</title>
    <updated>2026-01-09T13:39:02+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/"&gt; &lt;img alt="(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability" src="https://a.thumbs.redditmedia.com/V0cq5JgXRMYN60IWmZOgeWuSilWb4Gxub72PzqtA_08.jpg" title="(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(paywall): &lt;a href="https://www.theinformation.com/articles/deepseek-release-next-flagship-ai-model-strong-coding-ability"&gt;https://www.theinformation.com/articles/deepseek-release-next-flagship-ai-model-strong-coding-ability&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q88hdc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T13:39:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1q89g1i</id>
    <title>DeepSeek V4 Coming</title>
    <updated>2026-01-09T14:18:56+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to two people with direct knowledge, DeepSeek is expected to roll out a next‑generation flagship AI model in the coming weeks that focuses on strong code‑generation capabilities.&lt;/p&gt; &lt;p&gt;The two sources said the model, codenamed V4, is an iteration of the V3 model DeepSeek released in December 2024. Preliminary internal benchmark tests conducted by DeepSeek employees indicate the model outperforms existing mainstream models in code generation, including Anthropic’s Claude and the OpenAI GPT family.&lt;/p&gt; &lt;p&gt;The sources said the V4 model achieves a technical breakthrough in handling and parsing very long code prompts, a significant practical advantage for engineers working on complex software projects. They also said the model’s ability to understand data patterns across the full training pipeline has been improved and that no degradation in performance has been observed.&lt;/p&gt; &lt;p&gt;One of the insiders said users may find that V4’s outputs are more logically rigorous and clear, a trait that indicates the model has stronger reasoning ability and will be much more reliable when performing complex tasks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.theinformation.com/articles/deepseek-release-next-flagship-ai-model-strong-coding-ability"&gt;https://www.theinformation.com/articles/deepseek-release-next-flagship-ai-model-strong-coding-ability&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T14:18:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1q911fj</id>
    <title>Choosing a GGUF Model: K-Quants, I-Quants, and Legacy Formats</title>
    <updated>2026-01-10T10:42:09+00:00</updated>
    <author>
      <name>/u/tarruda</name>
      <uri>https://old.reddit.com/user/tarruda</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q911fj/choosing_a_gguf_model_kquants_iquants_and_legacy/"&gt; &lt;img alt="Choosing a GGUF Model: K-Quants, I-Quants, and Legacy Formats" src="https://external-preview.redd.it/ZMZ25MOMBEhnH9XTLPWbE9gFhyIqUzt4o8mr7UwQxDI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53ebe8dd0a29609cff4356a2a17066fd5e68c6a2" title="Choosing a GGUF Model: K-Quants, I-Quants, and Legacy Formats" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarruda"&gt; /u/tarruda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://kaitchup.substack.com/p/choosing-a-gguf-model-k-quants-i"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q911fj/choosing_a_gguf_model_kquants_iquants_and_legacy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q911fj/choosing_a_gguf_model_kquants_iquants_and_legacy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T10:42:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8x9yp</id>
    <title>Minisforum BD395i MAX motherboard at CES 2026: built-in AMD Strix Halo APU, use your own GPU</title>
    <updated>2026-01-10T06:52:19+00:00</updated>
    <author>
      <name>/u/noiserr</name>
      <uri>https://old.reddit.com/user/noiserr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8x9yp/minisforum_bd395i_max_motherboard_at_ces_2026/"&gt; &lt;img alt="Minisforum BD395i MAX motherboard at CES 2026: built-in AMD Strix Halo APU, use your own GPU" src="https://external-preview.redd.it/KG0Myw_6qN7QIlQJb4mBoOF1kj92CgMaeoC8dUJj7QQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ad0e2ebc40d196f43927457a486ec1a5070565a" title="Minisforum BD395i MAX motherboard at CES 2026: built-in AMD Strix Halo APU, use your own GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noiserr"&gt; /u/noiserr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tweaktown.com/news/109642/minisforum-bd395i-max-motherboard-at-ces-2026-built-in-amd-strix-halo-apu-use-your-own-gpu/index.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8x9yp/minisforum_bd395i_max_motherboard_at_ces_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8x9yp/minisforum_bd395i_max_motherboard_at_ces_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T06:52:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q94cbp</id>
    <title>MiniMax 2.1 - Very impressed with performance</title>
    <updated>2026-01-10T13:41:13+00:00</updated>
    <author>
      <name>/u/JustinPooDough</name>
      <uri>https://old.reddit.com/user/JustinPooDough</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been developing my own agent from scratch as a hobby or over a year now - constantly changing things and tinkering with new ideas. &lt;/p&gt; &lt;p&gt;For a lot of time, open source models sucked at what I was doing. They would output intelligible text with logical fallacies or just make bad decisions. For example, for the code writing tool my agent used, I had to always switch to Claude sonnet or better - which would &lt;em&gt;mostly&lt;/em&gt; get it right. Even with the agentic stuff, sometimes the open source models would miss stuff, etc.&lt;/p&gt; &lt;p&gt;I recently tried swapping in MiniMax2.1, and holy shit - it's the first open model that actually keeps up with Claude. And when I say that, I mean I cannot actually tell the difference between them during execution of my agent.&lt;/p&gt; &lt;p&gt;Minimax 2.1 consistently get's code right within the same number of attempts as Claude. The only time I see a difference is when the code is more complicated and requires a lot more edge case exploration.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;tl;dr: Long been a skeptic of open source models in actual practise -&lt;/strong&gt; &lt;strong&gt;Minimax 2.1 blew me away.&lt;/strong&gt; I have completely switched to Minimax 2.1 due to cost savings and nearly identical performance.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PS.&lt;/strong&gt; GLM 4.7 might be equally good, but the Claude Code plan I subscribed to with &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; would not let me use my API key for regular client requests - only their work plan. Does anyone know of a way around this limitation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JustinPooDough"&gt; /u/JustinPooDough &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q94cbp/minimax_21_very_impressed_with_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q94cbp/minimax_21_very_impressed_with_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q94cbp/minimax_21_very_impressed_with_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T13:41:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8tdcz</id>
    <title>Introducing "UITPSDT" a novel approach to runtime efficiency in organic agents</title>
    <updated>2026-01-10T03:30:19+00:00</updated>
    <author>
      <name>/u/reto-wyss</name>
      <uri>https://old.reddit.com/user/reto-wyss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8tdcz/introducing_uitpsdt_a_novel_approach_to_runtime/"&gt; &lt;img alt="Introducing &amp;quot;UITPSDT&amp;quot; a novel approach to runtime efficiency in organic agents" src="https://preview.redd.it/73wv4f66yfcg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87519585cbcaf77c9e5c007fc061d1e497362a78" title="Introducing &amp;quot;UITPSDT&amp;quot; a novel approach to runtime efficiency in organic agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is a proof of concept and application outside of the proposed domain may yield unexpected results, we hope the community can contribute to the token efficiency.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reto-wyss"&gt; /u/reto-wyss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/73wv4f66yfcg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8tdcz/introducing_uitpsdt_a_novel_approach_to_runtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8tdcz/introducing_uitpsdt_a_novel_approach_to_runtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T03:30:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8hqgd</id>
    <title>I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work</title>
    <updated>2026-01-09T19:27:29+00:00</updated>
    <author>
      <name>/u/Ok-Pomegranate1314</name>
      <uri>https://old.reddit.com/user/Ok-Pomegranate1314</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/"&gt; &lt;img alt="I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work" src="https://preview.redd.it/dban4j25kdcg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4a2061b8c9510f486ad4475d7a5f9b8d3a666f7" title="I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA officially supports clustering &lt;em&gt;two&lt;/em&gt; DGX Sparks together. I wanted three.&lt;/p&gt; &lt;p&gt;The problem: each Spark has two 100Gbps ConnectX-7 ports. In a 3-node triangle mesh, each link ends up on a different subnet. NCCL's built-in networking assumes all peers are reachable from a single NIC. It just... doesn't work.&lt;/p&gt; &lt;p&gt;So I wrote a custom NCCL network plugin from scratch.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Subnet-aware NIC selection (picks the right NIC for each peer)&lt;/li&gt; &lt;li&gt;Raw RDMA verbs implementation (QP state machines, memory registration, completion queues)&lt;/li&gt; &lt;li&gt;Custom TCP handshake protocol to avoid deadlocks&lt;/li&gt; &lt;li&gt;~1500 lines of C&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The result:&lt;/strong&gt; Distributed inference across all 3 nodes at 8+ GB/s over RDMA. &lt;strong&gt;The NVIDIA support tier I'm currently on:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;├── Supported configs ✓ ├── &amp;quot;Should work&amp;quot; configs ├── &amp;quot;You're on your own&amp;quot; configs ├── &amp;quot;Please don't call us&amp;quot; configs ├── &amp;quot;How did you even...&amp;quot; configs └── You are here → &amp;quot;Writing custom NCCL plugins to cluster standalone workstations over a hand-wired RDMA mesh&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GitHub link: &lt;a href="https://github.com/autoscriptlabs/nccl-mesh-plugin"&gt;https://github.com/autoscriptlabs/nccl-mesh-plugin&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the implementation. This was a mass of low-level debugging (segfaults, RDMA state machine issues, GID table problems) but it works.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Pomegranate1314"&gt; /u/Ok-Pomegranate1314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dban4j25kdcg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T19:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q959am</id>
    <title>Strix Halo (Bosgame M5) + 7900 XTX eGPU: Local LLM Benchmarks (Llama.cpp vs vLLM). A loose follow-up</title>
    <updated>2026-01-10T14:20:31+00:00</updated>
    <author>
      <name>/u/reujea0</name>
      <uri>https://old.reddit.com/user/reujea0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a loose follow-up to my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1q189os/7900_xtx_rocm_a_year_later_llamacpp_vs_vllm/"&gt;previous article regarding the 7900 XTX&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I recently got my hands on a Strix Halo system, specifically the &lt;strong&gt;Bosgame M5&lt;/strong&gt;. My goal was to benchmark the Strix Halo standalone (which is a beast), and then see what effects adding a 7900 XTX via eGPU (TB3/USB4) would have on performance.&lt;/p&gt; &lt;h1&gt;The Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Host:&lt;/strong&gt; Bosgame M5 (Strix Halo)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OS:&lt;/strong&gt; Fedora Server 43&lt;/li&gt; &lt;li&gt;&lt;strong&gt;eGPU:&lt;/strong&gt; 7900 XTX (Connected via USB4/TB3)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Toolboxes:&lt;/strong&gt; Huge thanks to &lt;a href="https://github.com/kyuz0"&gt;kyuz0 on GitHub&lt;/a&gt; for the &lt;a href="https://github.com/kyuz0/amd-strix-halo-toolboxes"&gt;llama.cpp toolboxes&lt;/a&gt; and &lt;a href="https://github.com/kyuz0/amd-strix-halo-vllm-toolboxes"&gt;vLLM toolboxes&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Critical Tip for eGPU users:&lt;/strong&gt; To prevent the whole system from becoming unresponsive when activating the Thunderbolt enclosure, I had to add the following kernel parameter: &lt;code&gt;pcie_port_pm=off&lt;/code&gt; (Found this solution online, it's a lifesaver for stability).&lt;/p&gt; &lt;h1&gt;Part 1: Strix Halo Standalone (Llama.cpp)&lt;/h1&gt; &lt;p&gt;I first ran the same models used in my previous 7900 XTX post, plus some larger ones that didn't fit on the 7900 XTX alone. &lt;em&gt;Backend: ROCm&lt;/em&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;PP (512)&lt;/th&gt; &lt;th align="left"&gt;Gen (tg512)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Llama-3.1-8B-Instruct&lt;/strong&gt; (BF16)&lt;/td&gt; &lt;td align="left"&gt;14.96 GB&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;950 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;112.27 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mistral-Small-3.2-24B&lt;/strong&gt; (Q5_K_XL)&lt;/td&gt; &lt;td align="left"&gt;15.63 GB&lt;/td&gt; &lt;td align="left"&gt;24B&lt;/td&gt; &lt;td align="left"&gt;405 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;42.10 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;DeepSeek-R1-Distill-Qwen-32B&lt;/strong&gt; (Q3_K_M)&lt;/td&gt; &lt;td align="left"&gt;14.84 GB&lt;/td&gt; &lt;td align="left"&gt;32B&lt;/td&gt; &lt;td align="left"&gt;311 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;42.26 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-20b&lt;/strong&gt; (F16)&lt;/td&gt; &lt;td align="left"&gt;12.83 GB&lt;/td&gt; &lt;td align="left"&gt;20B&lt;/td&gt; &lt;td align="left"&gt;797 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;49.62 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-20b&lt;/strong&gt; (MXFP4)&lt;/td&gt; &lt;td align="left"&gt;11.27 GB&lt;/td&gt; &lt;td align="left"&gt;20B&lt;/td&gt; &lt;td align="left"&gt;766 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;69.69 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3-VL-30B-Thinking&lt;/strong&gt; (Q4_K_XL)&lt;/td&gt; &lt;td align="left"&gt;16.49 GB&lt;/td&gt; &lt;td align="left"&gt;30B&lt;/td&gt; &lt;td align="left"&gt;1118 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;65.45 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-120b&lt;/strong&gt; (MXFP4)&lt;/td&gt; &lt;td align="left"&gt;59.02 GB&lt;/td&gt; &lt;td align="left"&gt;116B&lt;/td&gt; &lt;td align="left"&gt;612 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;49.07 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;GLM-4.6V&lt;/strong&gt; (Q4_K_M)&lt;/td&gt; &lt;td align="left"&gt;65.60 GB&lt;/td&gt; &lt;td align="left"&gt;106B&lt;/td&gt; &lt;td align="left"&gt;294 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;19.85 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MiniMax-M2.1&lt;/strong&gt; (Q3_K_M)&lt;/td&gt; &lt;td align="left"&gt;101.76 GB&lt;/td&gt; &lt;td align="left"&gt;228B&lt;/td&gt; &lt;td align="left"&gt;210 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;26.24 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Part 2: Strix Halo (iGPU) + 7900 XTX (eGPU) Split&lt;/h1&gt; &lt;p&gt;I wanted to see if offloading to the eGPU helped. I used &lt;code&gt;llama-serve&lt;/code&gt; with a custom Python script to measure throughput. These were all done with a context of 4K.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Strategy:&lt;/strong&gt; 1:1 split for small models; maximized 7900 XTX load for large models.&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Split Config&lt;/th&gt; &lt;th align="left"&gt;iGPU Only&lt;/th&gt; &lt;th align="left"&gt;Split (iGPU+dGPU)&lt;/th&gt; &lt;th align="left"&gt;Improvement&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Llama-3.1-8B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1:1&lt;/td&gt; &lt;td align="left"&gt;112.61 t/s&lt;/td&gt; &lt;td align="left"&gt;~167.7 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+49%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mistral-Small-24B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1:1&lt;/td&gt; &lt;td align="left"&gt;42.10 t/s&lt;/td&gt; &lt;td align="left"&gt;~58.9 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+40%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;DeepSeek-R1-Distill-32B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1:1&lt;/td&gt; &lt;td align="left"&gt;42.26 t/s&lt;/td&gt; &lt;td align="left"&gt;~53.2 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+26%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-20b&lt;/strong&gt; (F16)&lt;/td&gt; &lt;td align="left"&gt;1:1&lt;/td&gt; &lt;td align="left"&gt;50.09 t/s&lt;/td&gt; &lt;td align="left"&gt;61.17 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+22%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-20b&lt;/strong&gt; (MXFP4)&lt;/td&gt; &lt;td align="left"&gt;1:1&lt;/td&gt; &lt;td align="left"&gt;70.27 t/s&lt;/td&gt; &lt;td align="left"&gt;78.01 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+11%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3-VL-30B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1:1&lt;/td&gt; &lt;td align="left"&gt;65.23 t/s&lt;/td&gt; &lt;td align="left"&gt;57.50 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;-12%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-120b&lt;/strong&gt; (MXFP4)&lt;/td&gt; &lt;td align="left"&gt;24:3&lt;/td&gt; &lt;td align="left"&gt;49.35 t/s&lt;/td&gt; &lt;td align="left"&gt;54.56 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+11%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;GLM-4.6V&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2:1&lt;/td&gt; &lt;td align="left"&gt;20.54 t/s&lt;/td&gt; &lt;td align="left"&gt;23.46 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+14%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MiniMax-M2.1&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;17:5&lt;/td&gt; &lt;td align="left"&gt;26.22 t/s&lt;/td&gt; &lt;td align="left"&gt;27.19 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+4%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Observations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Adding the eGPU is beneficial for smaller, dense models where we get a &lt;strong&gt;~50% boost&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;However, for larger models or MoEs, the &lt;strong&gt;USB4/TB3 bandwidth&lt;/strong&gt; likely becomes a bottleneck. The latency introduced by splitting the model across the interconnect kills the gains, leading to diminishing returns (+4% to +14%) or even regression (-12% on Qwen3-VL).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Part 3: vLLM on Strix Halo&lt;/h1&gt; &lt;p&gt;The situation with vLLM is a bit rougher. I wasn't willing to wrestle with multi-GPU configuration here, so these results are &lt;strong&gt;Strix Halo Single GPU only&lt;/strong&gt;.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Output Speed (tok/s)&lt;/th&gt; &lt;th align="left"&gt;TTFT (Mean)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-20b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;25.87 t/s&lt;/td&gt; &lt;td align="left"&gt;1164 ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Llama-3.1-8B-Instruct&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;17.34 t/s&lt;/td&gt; &lt;td align="left"&gt;633 ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mistral-Small-24B&lt;/strong&gt; (bnb-4bit)&lt;/td&gt; &lt;td align="left"&gt;4.23 t/s&lt;/td&gt; &lt;td align="left"&gt;3751 ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-20b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;25.37 t/s&lt;/td&gt; &lt;td align="left"&gt;3625 ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss-120b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;15.5 t/s&lt;/td&gt; &lt;td align="left"&gt;4458&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;vLLM support on ROCm (specifically for Strix Halo/consumer cards) seems to be lagging behind llama.cpp significantly. The generation speeds are much lower, and the Time To First Token (TTFT) is quite high. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reujea0"&gt; /u/reujea0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q959am/strix_halo_bosgame_m5_7900_xtx_egpu_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q959am/strix_halo_bosgame_m5_7900_xtx_egpu_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q959am/strix_halo_bosgame_m5_7900_xtx_egpu_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T14:20:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8wv24</id>
    <title>GLM 5 Is Being Trained!</title>
    <updated>2026-01-10T06:28:58+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8wv24/glm_5_is_being_trained/"&gt; &lt;img alt="GLM 5 Is Being Trained!" src="https://b.thumbs.redditmedia.com/moDsJ3Fi93wAh8NCr_tIBITFa49S-yRZWrZUjXB4v-A.jpg" title="GLM 5 Is Being Trained!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/lc29bfu0ugcg1.png?width=696&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=881458479675304548c6a39e72ed0a8b90f5b54a"&gt;https://preview.redd.it/lc29bfu0ugcg1.png?width=696&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=881458479675304548c6a39e72ed0a8b90f5b54a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Announced after their IPO&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8wv24/glm_5_is_being_trained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8wv24/glm_5_is_being_trained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8wv24/glm_5_is_being_trained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T06:28:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q922fv</id>
    <title>GPT OSS + Qwen VL</title>
    <updated>2026-01-10T11:43:44+00:00</updated>
    <author>
      <name>/u/Serious_Molasses313</name>
      <uri>https://old.reddit.com/user/Serious_Molasses313</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q922fv/gpt_oss_qwen_vl/"&gt; &lt;img alt="GPT OSS + Qwen VL" src="https://external-preview.redd.it/ZHdvYWFqaG5laWNnMW9SjiD2lW6nDnuEDR_4iEKM_w8YRwqIaLhq7MVR_4-G.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61c12a19b6a10c13d4077e69539e6e203457e09c" title="GPT OSS + Qwen VL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Figured out how to squeeze these two model on my system without crashing. Now GPT OSS reaches out to qwen for visual confirmation. &lt;/p&gt; &lt;p&gt;Before you ask what MCP server this is (I made it)&lt;/p&gt; &lt;p&gt;My specs are 6GBVRAM 32GBDDR5 &lt;/p&gt; &lt;h1&gt;PrivacyOverConvenience&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Serious_Molasses313"&gt; /u/Serious_Molasses313 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k60z16hneicg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q922fv/gpt_oss_qwen_vl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q922fv/gpt_oss_qwen_vl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T11:43:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q90ye2</id>
    <title>Jensen Huang at CES on how open models have really revolutionized AI last year. “When AI is open, it proliferates everywhere.”</title>
    <updated>2026-01-10T10:36:58+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q90ye2/jensen_huang_at_ces_on_how_open_models_have/"&gt; &lt;img alt="Jensen Huang at CES on how open models have really revolutionized AI last year. “When AI is open, it proliferates everywhere.”" src="https://external-preview.redd.it/bGVlcWZ0bzcxaWNnMW_K1BNM1KBv7FYngB2itMlTyoA2GP6X-h0KJFWgL9Yw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35c5c46d790cb187582f60967603f3cda5bc1020" title="Jensen Huang at CES on how open models have really revolutionized AI last year. “When AI is open, it proliferates everywhere.”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From NVIDIA AI on 𝕏: &lt;a href="https://x.com/NVIDIAAI/status/2009731908888895516"&gt;https://x.com/NVIDIAAI/status/2009731908888895516&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/73l3tyn71icg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q90ye2/jensen_huang_at_ces_on_how_open_models_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q90ye2/jensen_huang_at_ces_on_how_open_models_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-10T10:36:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8ckz0</id>
    <title>The reason why RAM has become so expensive</title>
    <updated>2026-01-09T16:18:22+00:00</updated>
    <author>
      <name>/u/InvadersMustLive</name>
      <uri>https://old.reddit.com/user/InvadersMustLive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/"&gt; &lt;img alt="The reason why RAM has become so expensive" src="https://preview.redd.it/sgbhubsomccg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57d847c1b9a2d5786b0a888b5d0d25fe5ede9e12" title="The reason why RAM has become so expensive" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InvadersMustLive"&gt; /u/InvadersMustLive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sgbhubsomccg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T16:18:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
