<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-08T14:35:15+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1orozy5</id>
    <title>Is NVIDIA Triton Worth it?</title>
    <updated>2025-11-08T13:08:45+00:00</updated>
    <author>
      <name>/u/Plaush</name>
      <uri>https://old.reddit.com/user/Plaush</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I currently own 4x Nvidia A2s and have been using Ollama &amp;amp; Comfy UI for my Gen AI needs. I myself ain't an AI developer and have little in-depth knowledge on how they work under the hood, I also mainly use text-models and image-models.&lt;/p&gt; &lt;p&gt;Is it worth going through the hassle to get NVIDIA Triton working? I managed to get Llama 2 to work but trying to get GPT-OSS and Qwen Image Edit has been a nightmare for me&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plaush"&gt; /u/Plaush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orozy5/is_nvidia_triton_worth_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orozy5/is_nvidia_triton_worth_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orozy5/is_nvidia_triton_worth_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T13:08:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1orp105</id>
    <title>What model and settings should I use with my setup?</title>
    <updated>2025-11-08T13:10:14+00:00</updated>
    <author>
      <name>/u/NeatFollowing2612</name>
      <uri>https://old.reddit.com/user/NeatFollowing2612</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I upgraded from a 1060 to a 5070 and now have a Ryzen 7 7700X with 32 GB of RAM. I only used 8 GB models before. Which models should I try first, and what settings should I change to get the best performance with my new setup? My favorite models so far: Wingless_Imp 8B, L3.1-Dark, Planet-SpinFire-Uncensored-8B-D_AU-Q4, Hermes-2-Pro-Llama-3-8B-Q4, Infinitely-Laydiculus-9B-IQ4, kunoichi-dpo-v2-7B.Q4_K_M, and Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeatFollowing2612"&gt; /u/NeatFollowing2612 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orp105/what_model_and_settings_should_i_use_with_my_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orp105/what_model_and_settings_should_i_use_with_my_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orp105/what_model_and_settings_should_i_use_with_my_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T13:10:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1orp997</id>
    <title>Best way to serve NVIDIA ASR at scale ?</title>
    <updated>2025-11-08T13:21:09+00:00</updated>
    <author>
      <name>/u/Leading_Lock_4611</name>
      <uri>https://old.reddit.com/user/Leading_Lock_4611</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I want to serve a fine tuned Canary 1B flash model to serve hundreds of concurrent requests for short audio chunks. I do not have a Nvidia enterprise license. What would be the most efficient framework to serve on a large GPU (say H100) (vllm, triton, …) ? What would be a good config (batching, etc..) ? Thanks in advance !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leading_Lock_4611"&gt; /u/Leading_Lock_4611 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orp997/best_way_to_serve_nvidia_asr_at_scale/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orp997/best_way_to_serve_nvidia_asr_at_scale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orp997/best_way_to_serve_nvidia_asr_at_scale/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T13:21:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1orpqda</id>
    <title>Downloading pre-lowered models (e.g. to xnnpack)</title>
    <updated>2025-11-08T13:42:49+00:00</updated>
    <author>
      <name>/u/datashri</name>
      <uri>https://old.reddit.com/user/datashri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure if I'm expecting too much, but is there somewhere I can download .pte files of models already lowered to xnnpack or other backends? I think it's a good idea to save the effort of exporting and lowering myself. I tried searching for xnnpack on the HF downloads page, but there's only a handful. Any other ways? Or is it better to export and lower the models myself? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/datashri"&gt; /u/datashri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orpqda/downloading_prelowered_models_eg_to_xnnpack/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orpqda/downloading_prelowered_models_eg_to_xnnpack/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orpqda/downloading_prelowered_models_eg_to_xnnpack/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T13:42:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1orpsyv</id>
    <title>Figured out why my 3090 is so slow in inference</title>
    <updated>2025-11-08T13:46:00+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Discovered that my 3090 performed similarly with my 3050 using HF transformers for inference.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oriraf/how_come_my_3090_is_just_as_fast_as_my_3050_for/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1oriraf/how_come_my_3090_is_just_as_fast_as_my_3050_for/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Since someone in that thread suggested that I probably haven't saturated the GPU, so I created more short prompts that ask it to write 6,000 words essays. Indeed, t/s for a batch of prompts significantly improves as batch size increases.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;#prompt&lt;/th&gt; &lt;th align="left"&gt;padded input&lt;/th&gt; &lt;th align="left"&gt;total output&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-1.7B /nothink&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;90&lt;/td&gt; &lt;td align="left"&gt;4096&lt;/td&gt; &lt;td align="left"&gt;5.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-1.7B /nothink&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;90&lt;/td&gt; &lt;td align="left"&gt;5802&lt;/td&gt; &lt;td align="left"&gt;7.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-1.7B /nothink&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;90&lt;/td&gt; &lt;td align="left"&gt;12288&lt;/td&gt; &lt;td align="left"&gt;10.77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-1.7B /nothink&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16384&lt;/td&gt; &lt;td align="left"&gt;15.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-1.7B /nothink&lt;/td&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;102&lt;/td&gt; &lt;td align="left"&gt;20480&lt;/td&gt; &lt;td align="left"&gt;19.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-1.7B /nothink&lt;/td&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;102&lt;/td&gt; &lt;td align="left"&gt;24576&lt;/td&gt; &lt;td align="left"&gt;22.83&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Since someone in that thread says he could get 80t/s straight from my script with only one prompt, I suspect that something might be wrong in my setup. &lt;/p&gt; &lt;p&gt;I have been running my CPU in &amp;quot;Powersave&amp;quot; mode in Ubuntu to save some electricity bill, so I suspect it might be one of the causes. After I changed it to &amp;quot;Performance&amp;quot; mode, the numbers are much better and it is approaching the 80t/s when there are six prompts:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;#prompt&lt;/th&gt; &lt;th align="left"&gt;padded input&lt;/th&gt; &lt;th align="left"&gt;total output&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-1.7B /nothink&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;90&lt;/td&gt; &lt;td align="left"&gt;3171&lt;/td&gt; &lt;td align="left"&gt;13.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-1.7B /nothink&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;90&lt;/td&gt; &lt;td align="left"&gt;8192&lt;/td&gt; &lt;td align="left"&gt;21.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-1.7B /nothink&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;90&lt;/td&gt; &lt;td align="left"&gt;12288&lt;/td&gt; &lt;td align="left"&gt;32.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-1.7B /nothink&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16384&lt;/td&gt; &lt;td align="left"&gt;42.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-1.7B /nothink&lt;/td&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;102&lt;/td&gt; &lt;td align="left"&gt;20480&lt;/td&gt; &lt;td align="left"&gt;52.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-1.7B /nothink&lt;/td&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;102&lt;/td&gt; &lt;td align="left"&gt;24576&lt;/td&gt; &lt;td align="left"&gt;63.62&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I suspect the 80t/s user is using a very recent CPU. My CPU is a 12 years old i7 4930k. So it would be not surprising that it is a bottleneck. But I noticed that HF transformers is only using one core of my CPU. How can I make it use more than one core? Anyone knows? &lt;/p&gt; &lt;p&gt;So the moral of the story is that if you have a very old CPU and your GPU performs worse than expected, then the CPU might well be the bottleneck that is holding you back.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orpsyv/figured_out_why_my_3090_is_so_slow_in_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orpsyv/figured_out_why_my_3090_is_so_slow_in_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orpsyv/figured_out_why_my_3090_is_so_slow_in_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T13:46:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqebr3</id>
    <title>World's strongest agentic model is now open source</title>
    <updated>2025-11-06T23:20:15+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqebr3/worlds_strongest_agentic_model_is_now_open_source/"&gt; &lt;img alt="World's strongest agentic model is now open source" src="https://preview.redd.it/jd607rvrzpzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f84c70ace26fdbd5db78313787e58d2403961e38" title="World's strongest agentic model is now open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jd607rvrzpzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqebr3/worlds_strongest_agentic_model_is_now_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqebr3/worlds_strongest_agentic_model_is_now_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T23:20:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1or323v</id>
    <title>Recently built my first LLM and im wondering why there hasn't been more innovation on moving away from transformers and gradient descent?</title>
    <updated>2025-11-07T19:01:17+00:00</updated>
    <author>
      <name>/u/CelebrationMinimum50</name>
      <uri>https://old.reddit.com/user/CelebrationMinimum50</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So please excuse my lack of knowledge in this area as im new to AI/LLMs but I just recently build my first micro llm and I dunno something about them seems wrong.&lt;/p&gt; &lt;p&gt;Is the industry stuck on transformers and gradient descent because coming up with alternatives is a hugely difficult problem or is the industry just having blinders on?&lt;/p&gt; &lt;p&gt;I like a lot of the research about sparse models that use hebbian/oja and i know these come with challenges like catastrophic interference. But this seems like a very solvable problem.&lt;/p&gt; &lt;p&gt;Anyways im starting to tinker with my micro llm to see if I can get rid of gradient descent and traditional transformers and see if I cant make a sparse model based on hebbian/oja at the very least in a small scale&lt;/p&gt; &lt;p&gt;Again pardon my nativity, my expertise is mostly in backend systems and architecture. I have very little exposure to AI/LLMs until recently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CelebrationMinimum50"&gt; /u/CelebrationMinimum50 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or323v/recently_built_my_first_llm_and_im_wondering_why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or323v/recently_built_my_first_llm_and_im_wondering_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or323v/recently_built_my_first_llm_and_im_wondering_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T19:01:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oquezp</id>
    <title>Kimi K2 Thinking with sglang and mixed GPU / ktransformers CPU inference @ 31 tokens/sec</title>
    <updated>2025-11-07T13:28:44+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got Kimi K2 Thinking running locally and I'm blown away how fast it runs in simple chat tests: approximately ~ 30 tokens/sec with 4000 tokens in the context. Obviously a lot more testing to be done, but wow... a trillion parameter model running at 30 tokens/sec. &lt;/p&gt; &lt;p&gt;I'll whip up some tests around batching and available context lengths soon, but for now here's the recipe to get it running should you have the necessary hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: it looks like only the first API request works. Subsequent requests always cause sglang to crash and require a restart, regardless of how I configure things:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; File &amp;quot;/home/carl/ktransformers/ktransformers/.venv/lib/python3.11/site-packages/triton/compiler/compiler.py&amp;quot;, line 498, in __getattribute__ self._init_handles() File &amp;quot;/home/carl/ktransformers/ktransformers/.venv/lib/python3.11/site-packages/triton/compiler/compiler.py&amp;quot;, line 483, in _init_handles raise OutOfResources(self.metadata.shared, max_shared, &amp;quot;shared memory&amp;quot;) triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 106496, Hardware limit: 101376. Reducing block sizes or `num_stages` may help. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;System&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;EPYC &lt;del&gt;7B45&lt;/del&gt; 9B45 (128-core, 256 thread) CPU&lt;/li&gt; &lt;li&gt;768GB DDR5 6400 MT/s&lt;/li&gt; &lt;li&gt;4x RTX 6000 Pro Workstation 96GB GPUs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup virtual python environment&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mkdir sglang-ktransformers cd sglang-ktransformers uv venv --python 3.11 --seed . .venv/bin/activate &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Install sglang&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uv pip install &amp;quot;sglang&amp;quot; --prerelease=allow &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Download and initialize ktransformers repo&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/kvcache-ai/ktransformers cd ktransformers git submodule update --init --recursive &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Install ktransformers CPU kernel for sglang&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd kt-kernel export CPUINFER_CPU_INSTRUCT=AVX512 export CPUINFER_ENABLE_AMX=OFF uv pip install . cd .. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Download Kimi K2 Thinking GPU &amp;amp; CPU parts&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uv pip install -U hf hf_transfer hf download moonshotai/Kimi-K2-Thinking hf download KVCache-ai/Kimi-K2-Thinking-CPU-weight &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Run k2&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 python -m sglang.launch_server \ --host 0.0.0.0 --port 8080 \ --model ~/.cache/huggingface/hub/models--moonshotai--Kimi-K2-Thinking/snapshots/357b94aee9d50ec88e5e6dd9550fd7f957cb1baa \ --kt-amx-weight-path ~/.cache/huggingface/hub/models--KVCache-ai--Kimi-K2-Thinking-CPU-weight/snapshots/690ffacb9203d3b5e05ee8167ff1f5d4ae027c83 \ --kt-cpuinfer 252 \ --kt-threadpool-count 2 \ --kt-num-gpu-experts 238 \ --kt-amx-method AMXINT4 \ --attention-backend triton --trust-remote-code \ --mem-fraction-static 0.98 \ --chunked-prefill-size 4096 \ --max-running-requests 1 \ --max-total-tokens 32768 \ --enable-mixed-chunk \ --tensor-parallel-size 4 \ --enable-p2p-check \ --disable-shared-experts-fusion &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oquezp/kimi_k2_thinking_with_sglang_and_mixed_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oquezp/kimi_k2_thinking_with_sglang_and_mixed_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oquezp/kimi_k2_thinking_with_sglang_and_mixed_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T13:28:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ore9xo</id>
    <title>Training framework that monitors itself and auto-fixes issues (gradient explosions, OOM, MoE imbalance) - looking for feedback</title>
    <updated>2025-11-08T02:55:02+00:00</updated>
    <author>
      <name>/u/Huge_Protection2600</name>
      <uri>https://old.reddit.com/user/Huge_Protection2600</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;I built a training framework that automatically fixes gradient explosions, OOM errors, and MoE expert collapse&lt;/h1&gt; &lt;p&gt;Hey LocalLLaMA! Tired of babysitting training runs? I built &lt;strong&gt;LuminaAI&lt;/strong&gt; - a framework where the system monitors itself and makes real-time decisions to keep training stable.&lt;/p&gt; &lt;h1&gt;What it does:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Training Orchestrator:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gradient explosion detected -&amp;gt; automatically reduces learning rate&lt;/li&gt; &lt;li&gt;OOM error -&amp;gt; reduces batch size and retries&lt;/li&gt; &lt;li&gt;MoE experts collapsing -&amp;gt; adjusts routing&lt;/li&gt; &lt;li&gt;Loss plateau -&amp;gt; increases LR or suggests stopping early&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Architecture Support:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dense transformers, MoE (8-64 experts), MoD (30-50% faster), Hybrid&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Chinchilla Scaling:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Automatically calculates optimal training epochs based on model size&lt;/li&gt; &lt;li&gt;Monitors convergence and predicts when to stop&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Real example from my training logs:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;[Step 5000] Loss spike: 2.15 → 3.87 [Orchestrator] Emergency intervention Decision: Reduce LR by 10x, rollback 50 steps Reasoning: Gradient explosion detected [Step 5100] Stabilized: 2.12 ✓ &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Why it's different:&lt;/h1&gt; &lt;p&gt;Instead of manually watching TensorBoard and adjusting hyperparameters, the orchestrator makes &lt;strong&gt;18 different types of interventions&lt;/strong&gt; automatically:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Add/remove MoE experts during training&lt;/li&gt; &lt;li&gt;Adjust batch sizes for OOM recovery&lt;/li&gt; &lt;li&gt;Emergency rollbacks when things go wrong&lt;/li&gt; &lt;li&gt;Dynamic learning rate adjustments&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Hardware:&lt;/h1&gt; &lt;p&gt;Works on CUDA (RTX 3090, a100, h100, etc), Apple Silicon (M1/M2/M3/M4), and multi-GPU with DeepSpeed.&lt;/p&gt; &lt;p&gt;Pre-configured for 1B -&amp;gt; 300B parameter models (MoE).&lt;/p&gt; &lt;h1&gt;What I need:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Feedback&lt;/strong&gt;: What training issues should I automate next?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Testing&lt;/strong&gt;: Does it work on your hardware?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Brutal honesty&lt;/strong&gt;: What would make you actually use this?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've been working on this for ~4.5 months because I was sick of 2 AM loss divergences. Open source, free for research/personal use.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href="https://github.com/matn23/luminaai"&gt;https://github.com/matn23/luminaai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What training pain points drive you crazy? Would love to hear what I should automate next!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: For context, I'm 13 and this is my first major ML project. Any feedback (brutal honesty welcome) is super helpful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Huge_Protection2600"&gt; /u/Huge_Protection2600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ore9xo/training_framework_that_monitors_itself_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ore9xo/training_framework_that_monitors_itself_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ore9xo/training_framework_that_monitors_itself_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T02:55:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ornvm0</id>
    <title>Best GUI for LLM based story writing that can access external models?</title>
    <updated>2025-11-08T12:11:29+00:00</updated>
    <author>
      <name>/u/StableLlama</name>
      <uri>https://old.reddit.com/user/StableLlama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most GUIs want to run the models themself, but I'd like to run it myself or use an on campus service that provide an OpenAI compatible API access. And for my Ooba installation the Playground extension isn't working at the moment.&lt;/p&gt; &lt;p&gt;So, long story short:&lt;/p&gt; &lt;p&gt;What are your recommendations for a GUI tool that's helping me to interactively write and edit stories - and can access the LLM through an OpenAI API?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StableLlama"&gt; /u/StableLlama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ornvm0/best_gui_for_llm_based_story_writing_that_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ornvm0/best_gui_for_llm_based_story_writing_that_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ornvm0/best_gui_for_llm_based_story_writing_that_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T12:11:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1orof46</id>
    <title>Starting with local LLM</title>
    <updated>2025-11-08T12:40:22+00:00</updated>
    <author>
      <name>/u/Murky_Poem_9321</name>
      <uri>https://old.reddit.com/user/Murky_Poem_9321</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. I would like to run an LLM locally. It’s supposed to work like my second brain. It should be linked to a RAG, where I have all the information about my life (since birth if available) and would like to fill it further. The LLM should have access to it.&lt;/p&gt; &lt;p&gt;Why local? Safety.&lt;/p&gt; &lt;p&gt;What kind of hardware do I have? Actually unfortunately only a MacBook Air M4 with 16GB RAM.&lt;/p&gt; &lt;p&gt;How do I start, what can you recommend. What works with my specs (even if it’s small)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Murky_Poem_9321"&gt; /u/Murky_Poem_9321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orof46/starting_with_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orof46/starting_with_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orof46/starting_with_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T12:40:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1orp9o9</id>
    <title>Text model that can produce nodes and edges in JSON</title>
    <updated>2025-11-08T13:21:40+00:00</updated>
    <author>
      <name>/u/BlueAdventurers</name>
      <uri>https://old.reddit.com/user/BlueAdventurers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need to draw knowledge graphs and I’m using Gemini 2.5 Flash to give me the JSON that renders it. However, it is too slow.&lt;/p&gt; &lt;p&gt;The output looks something like {“type”: “node”, “id”: 123}, {“type”: “edge”, “from_id”: 123, “to_id”: 456}&lt;/p&gt; &lt;p&gt;What model could I look into? It would need to reason on the free text input that describes the entities and their relationships. &lt;/p&gt; &lt;p&gt;A typical graph contains approx. 20 nodes and 30 edges.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlueAdventurers"&gt; /u/BlueAdventurers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orp9o9/text_model_that_can_produce_nodes_and_edges_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orp9o9/text_model_that_can_produce_nodes_and_edges_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orp9o9/text_model_that_can_produce_nodes_and_edges_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T13:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ordd5x</id>
    <title>[Web Demo] Qwen-Image-Edit — Camera angle control (HF Space)</title>
    <updated>2025-11-08T02:10:18+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ordd5x/web_demo_qwenimageedit_camera_angle_control_hf/"&gt; &lt;img alt="[Web Demo] Qwen-Image-Edit — Camera angle control (HF Space)" src="https://b.thumbs.redditmedia.com/cXAJ8Nam0JPRlqcjDrk_78dOvdtCDoLB1HC1snVFO1k.jpg" title="[Web Demo] Qwen-Image-Edit — Camera angle control (HF Space)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very Cool Tool.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z1nmida0zxzf1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc5f06a9e2105a5db39e4644ef805606c65873eb"&gt;https://preview.redd.it/z1nmida0zxzf1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc5f06a9e2105a5db39e4644ef805606c65873eb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Upload an image, then tweak &lt;strong&gt;camera motion/rotation/lens&lt;/strong&gt; sliders to generate new viewpoints—right in your browser. &lt;a href="https://huggingface.co/spaces/linoyts/Qwen-Image-Edit-Angles?utm_source=chatgpt.com"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Do things like move the camera (left/right/forward/down), rotate ±45°/90° or go top-down, and switch between wide vs. close-up looks. &lt;/li&gt; &lt;li&gt;Built on &lt;strong&gt;Qwen Image Edit&lt;/strong&gt;; compatible community LoRAs enable multi-angle variants. &lt;/li&gt; &lt;li&gt;Tip: results can vary with busy backgrounds—short prompts often work best.Try it: &lt;a href="https://huggingface.co/spaces/linoyts/Qwen-Image-Edit-Angles"&gt;&lt;code&gt;https://huggingface.co/spaces/linoyts/Qwen-Image-Edit-Angles&lt;/code&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/linoyts/Qwen-Image-Edit-Angles"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ordd5x/web_demo_qwenimageedit_camera_angle_control_hf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ordd5x/web_demo_qwenimageedit_camera_angle_control_hf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ordd5x/web_demo_qwenimageedit_camera_angle_control_hf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T02:10:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqttg0</id>
    <title>Can someone explain what a Mixture-of-Experts model really is?</title>
    <updated>2025-11-07T13:02:36+00:00</updated>
    <author>
      <name>/u/Weebviir</name>
      <uri>https://old.reddit.com/user/Weebviir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I've been aware of MoE since Deepseek dropped in the beginning of the year but I never really delved deep into what it is and how it helps in things like local AI inferencing. This sub's been very helpful with my local AI related questions so I wanted to learn from the people here.&lt;/p&gt; &lt;p&gt;Here are some more questions:&lt;br /&gt; - How does a model know when an expert is to be used?&lt;br /&gt; - Are MoE models really easier to run than traditional models?&lt;br /&gt; - How do Activation parameters really work? Do they affect fine tuning processes later?&lt;br /&gt; - Why do MoE models work better than traditional models?&lt;br /&gt; - What are “sparse” vs “dense” MoE architectures?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weebviir"&gt; /u/Weebviir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqttg0/can_someone_explain_what_a_mixtureofexperts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqttg0/can_someone_explain_what_a_mixtureofexperts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqttg0/can_someone_explain_what_a_mixtureofexperts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T13:02:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1or1e7p</id>
    <title>I fine-tuned Gemma 3 1B for CLI command translation... but it runs 100% locally. 810MB, 1.5s inference on CPU.</title>
    <updated>2025-11-07T17:58:57+00:00</updated>
    <author>
      <name>/u/theRealSachinSpk</name>
      <uri>https://old.reddit.com/user/theRealSachinSpk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"&gt; &lt;img alt="I fine-tuned Gemma 3 1B for CLI command translation... but it runs 100% locally. 810MB, 1.5s inference on CPU." src="https://b.thumbs.redditmedia.com/k4PYFs253tXR75-utWF1-v10OmEqGwkzGrkkq8FHHVo.jpg" title="I fine-tuned Gemma 3 1B for CLI command translation... but it runs 100% locally. 810MB, 1.5s inference on CPU." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I built a locally-running NL→CLI translator by fine-tuning Gemma 3 1B with QLoRA.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/pranavkumaarofficial/nlcli-wizard"&gt;[Link to repo]&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Built a privacy-first CLI copilot. No API calls, no subscriptions. Just 810MB of local AI that converts natural language to CLI commands.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jpo4dd4jivzf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3aa7bc9af223d3ab2e4c3eb9156907994885cf5"&gt;https://preview.redd.it/jpo4dd4jivzf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3aa7bc9af223d3ab2e4c3eb9156907994885cf5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wanted to try out something like a CLI wizard: running locally and loaded within the package. Now of course there is an overhead of embedding an SLM in every package.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;But definitely makes sense for complex, domain-specific tools with non-obvious CLI patterns&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Instead of: kubectl get pods -n production --field-selector status.phase=Running&lt;/p&gt; &lt;p&gt;Could be: kubectl -w &amp;quot;show me running pods in production&amp;quot;&lt;/p&gt; &lt;p&gt;Shell-GPT is the closest tool that is available but doesnt do what I wanted, and ofcourse uses closedsource LLMs&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here is what I tried:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Takes natural language like &amp;quot;show my environments sorted by size&amp;quot; and outputs the correct CLI command, eg : &lt;code&gt;venvy ls --sort size&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key stats:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~1.5s inference on CPU (4 threads)&lt;/li&gt; &lt;li&gt;810MB quantized model (Q4_K_M with smart fallback)&lt;/li&gt; &lt;li&gt;Trained on Colab T4 in &amp;lt;1 hr&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Setup&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Base model:&lt;/strong&gt; Gemma 3-1B-Instruct (March 2025 release)&lt;br /&gt; &lt;strong&gt;Training:&lt;/strong&gt; Unsloth + QLoRA (only 14M params trained, 1.29% of model)&lt;br /&gt; &lt;strong&gt;Hardware:&lt;/strong&gt; Free Colab T4, trained in under 1 hour&lt;br /&gt; &lt;strong&gt;Final model:&lt;/strong&gt; 810MB GGUF (Q4_K_M with smart fallback to Q5/Q6)&lt;br /&gt; &lt;strong&gt;Inference:&lt;/strong&gt; llama.cpp, ~1.5s on CPU (4 threads, M1 Mac / Ryzen)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The architecture part:&lt;/strong&gt; Used smart quantization with mixed precision (Q4_K/Q5_0/Q6_K) that adapts per-layer based on tensor dimensions. Some layers can't be quantized to 4-bit without accuracy loss, so llama.cpp automatically upgrades them to 5/6-bit.&lt;/p&gt; &lt;p&gt;Training loss was extremely clean - 0.135 (train), 0.142 (val) with zero overfitting across 3 epochs.&lt;/p&gt; &lt;p&gt;Limitations (being honest here)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Model size:&lt;/strong&gt; 810MB is chunky. Too big for Docker images, fine for dev machines.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool-specific:&lt;/strong&gt; Currently only works for &lt;code&gt;venvy&lt;/code&gt;. Need to retrain for kubectl/docker/etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Latency:&lt;/strong&gt; 1.5s isn't instant. Experts will still prefer muscle memory.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accuracy:&lt;/strong&gt; 80-85% means you MUST verify before executing.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Safety&lt;/h1&gt; &lt;p&gt;Always asks for confirmation before executing. I'm not &lt;em&gt;that&lt;/em&gt; reckless.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;confirm = input(&amp;quot;Execute? [Y/n] &amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Still working on this : to check where this can really help, but yeah pls go check it out&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/pranavkumaarofficial/nlcli-wizard"&gt;[Link to repo]&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theRealSachinSpk"&gt; /u/theRealSachinSpk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T17:58:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1or46rv</id>
    <title>Cerebras/Kimi-Linear-REAP-35B-A3B-Instruct · Hugging Face</title>
    <updated>2025-11-07T19:44:19+00:00</updated>
    <author>
      <name>/u/maroule</name>
      <uri>https://old.reddit.com/user/maroule</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or46rv/cerebraskimilinearreap35ba3binstruct_hugging_face/"&gt; &lt;img alt="Cerebras/Kimi-Linear-REAP-35B-A3B-Instruct · Hugging Face" src="https://external-preview.redd.it/A5NFpNf7XiO2gm9NBBYXrtttQxX4Zw8QmamAzVNdgao.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4b51e11f834d5f8b364cbfd4018254e64276366" title="Cerebras/Kimi-Linear-REAP-35B-A3B-Instruct · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maroule"&gt; /u/maroule &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/cerebras/Kimi-Linear-REAP-35B-A3B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or46rv/cerebraskimilinearreap35ba3binstruct_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or46rv/cerebraskimilinearreap35ba3binstruct_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T19:44:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1or5j9z</id>
    <title>Nvidia may cancel the RTX 50 Super due to a shortage of 3GB GDDR7 memory</title>
    <updated>2025-11-07T20:36:28+00:00</updated>
    <author>
      <name>/u/Spiderboyz1</name>
      <uri>https://old.reddit.com/user/Spiderboyz1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For now it's just a rumor, but it seems the RTX Super cards will take a while to be released, if they ever are&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/342705/gddr7-shortage-could-stop-nvidia-geforce-rtx-50-series-super-rollout"&gt;https://www.techpowerup.com/342705/gddr7-shortage-could-stop-nvidia-geforce-rtx-50-series-super-rollout&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.guru3d.com/story/nvidia-may-cancel-or-delay-geforce-rtx-50-super-series-amid-gddr7-memory-shortage/"&gt;https://www.guru3d.com/story/nvidia-may-cancel-or-delay-geforce-rtx-50-super-series-amid-gddr7-memory-shortage/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And we also have RAM prices skyrocketing due to high demand&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiderboyz1"&gt; /u/Spiderboyz1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or5j9z/nvidia_may_cancel_the_rtx_50_super_due_to_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or5j9z/nvidia_may_cancel_the_rtx_50_super_due_to_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or5j9z/nvidia_may_cancel_the_rtx_50_super_due_to_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T20:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1or4q4m</id>
    <title>Kimi K2 Thinking SECOND most intelligent LLM according to Artificial Analysis</title>
    <updated>2025-11-07T20:04:53+00:00</updated>
    <author>
      <name>/u/teatime1983</name>
      <uri>https://old.reddit.com/user/teatime1983</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"&gt; &lt;img alt="Kimi K2 Thinking SECOND most intelligent LLM according to Artificial Analysis" src="https://b.thumbs.redditmedia.com/rRHDbssmheavBuel7zlrGe8NTyudGIzlKqGjsplYEaI.jpg" title="Kimi K2 Thinking SECOND most intelligent LLM according to Artificial Analysis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4l3lbor55wzf1.png?width=488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b25065a1147654cbbf9b413b322560870a44d41e"&gt;https://preview.redd.it/4l3lbor55wzf1.png?width=488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b25065a1147654cbbf9b413b322560870a44d41e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Kimi K2 Thinking API pricing is $0.60 per million input tokens and $2.50 per million output tokens.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teatime1983"&gt; /u/teatime1983 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T20:04:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1or8ehk</id>
    <title>Artificial Analysis has released a more in-depth benchmark breakdown of Kimi K2 Thinking (2nd image)</title>
    <updated>2025-11-07T22:30:30+00:00</updated>
    <author>
      <name>/u/averagebear_003</name>
      <uri>https://old.reddit.com/user/averagebear_003</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or8ehk/artificial_analysis_has_released_a_more_indepth/"&gt; &lt;img alt="Artificial Analysis has released a more in-depth benchmark breakdown of Kimi K2 Thinking (2nd image)" src="https://b.thumbs.redditmedia.com/UzJABCvVD7NHRog60608GSepBAlbSYFvvfYwhDULgvQ.jpg" title="Artificial Analysis has released a more in-depth benchmark breakdown of Kimi K2 Thinking (2nd image)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/averagebear_003"&gt; /u/averagebear_003 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1or8ehk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or8ehk/artificial_analysis_has_released_a_more_indepth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or8ehk/artificial_analysis_has_released_a_more_indepth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T22:30:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1or08aq</id>
    <title>OpenAI Pushes to Label Datacenters as ‘American Manufacturing’ Seeking Federal Subsidies After Preaching Independence</title>
    <updated>2025-11-07T17:15:05+00:00</updated>
    <author>
      <name>/u/Ok-Breakfast-4676</name>
      <uri>https://old.reddit.com/user/Ok-Breakfast-4676</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"&gt; &lt;img alt="OpenAI Pushes to Label Datacenters as ‘American Manufacturing’ Seeking Federal Subsidies After Preaching Independence" src="https://preview.redd.it/jq1jrz6kbvzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=258e2c389f69a6ed7cdcda3ea33b5a80b38a0fb3" title="OpenAI Pushes to Label Datacenters as ‘American Manufacturing’ Seeking Federal Subsidies After Preaching Independence" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI is now lobbying to classify datacenter spending as “American manufacturing.”&lt;/p&gt; &lt;p&gt;In their recent submission, they explicitly advocate for Federal loan guarantees the same kind used to subsidize large-scale industrial projects.&lt;/p&gt; &lt;p&gt;So after all the talk about independence and no need for government help… Sam lied. Again.￼&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Breakfast-4676"&gt; /u/Ok-Breakfast-4676 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jq1jrz6kbvzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T17:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oro9ng</id>
    <title>ROCm(6.4, using latest LLVM) vs ROCm 7 (lemonade sdk)</title>
    <updated>2025-11-08T12:32:29+00:00</updated>
    <author>
      <name>/u/CyBerDreadWing</name>
      <uri>https://old.reddit.com/user/CyBerDreadWing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One observation I would like to paste in here:&lt;/p&gt; &lt;p&gt;By building llama.cpp with ROCm from scratch (HIP SDK version 6.4), I was able to get more performance than lemonade sdk for ROCm 7.&lt;/p&gt; &lt;p&gt;FYI: I keep changing path of llama.cpp so on first run path was given to ROCm 7 and on second run path was given to ROCm 6.4&lt;/p&gt; &lt;p&gt;Here are some sample outputs:&lt;br /&gt; ROCm 7:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;PS C:\Users\dreadwing\.lmstudio\models\lmstudio-community\Qwen3-Coder-30B-A3B-Instruct-GGUF&amp;gt; llama-bench -m .\Qwen3-Coder-30B-A3B-Instruct-Q8_0.gguf -ub 2048 -b 2048 -ngl 99 -t 16 --n-cpu-moe 2,3,4,5,6,7,8,9,30 -fa on ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon RX 7900 GRE, gfx1100 (0x1100), VMM: no, Wave Size: 32 | model | size | params | backend | ngl | n_cpu_moe | threads | n_ubatch | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ------: | -------: | --------------: | -------------------: | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 2 | 16 | 2048 | pp512 | 247.95 ± 9.81 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 2 | 16 | 2048 | tg128 | 7.03 ± 0.18 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 3 | 16 | 2048 | pp512 | 243.92 ± 8.31 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 3 | 16 | 2048 | tg128 | 5.37 ± 0.19 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 4 | 16 | 2048 | pp512 | 339.53 ± 15.05 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 4 | 16 | 2048 | tg128 | 4.31 ± 0.09 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 5 | 16 | 2048 | pp512 | 322.23 ± 23.39 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 5 | 16 | 2048 | tg128 | 3.71 ± 0.15 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 6 | 16 | 2048 | pp512 | 389.06 ± 27.76 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 6 | 16 | 2048 | tg128 | 3.02 ± 0.16 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 7 | 16 | 2048 | pp512 | 385.10 ± 46.43 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 7 | 16 | 2048 | tg128 | 2.75 ± 0.08 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 8 | 16 | 2048 | pp512 | 374.84 ± 59.77 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;ROCm 6.4 ( which I build using latest llvm):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;PS C:\Users\dreadwing\.lmstudio\models\lmstudio-community\Qwen3-Coder-30B-A3B-Instruct-GGUF&amp;gt; llama-bench -m .\Qwen3-Coder-30B-A3B-Instruct-Q8_0.gguf -ub 2048 -b 2048 -ngl 99 -t 16 --n-cpu-moe 6,5,30 -fa on ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon RX 7900 GRE, gfx1100 (0x1100), VMM: no, Wave Size: 32 | model | size | params | backend | ngl | n_cpu_moe | threads | n_ubatch | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ------: | -------: | --------------: | -------------------: | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 6 | 16 | 2048 | pp512 | 229.92 ± 12.49 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 6 | 16 | 2048 | tg128 | 15.69 ± 0.10 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 5 | 16 | 2048 | pp512 | 338.65 ± 30.11 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 5 | 16 | 2048 | tg128 | 15.20 ± 0.04 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 30 | 16 | 2048 | pp512 | 206.16 ± 65.14 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 30 | 16 | 2048 | tg128 | 21.28 ± 0.07 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Can someone please explain why this is happening, (ROCm 7 is still in beta for windows, but thats my hard guess).&lt;/p&gt; &lt;p&gt;I am still figuring out TheRock build and vulkan build and will soon benchmark them as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CyBerDreadWing"&gt; /u/CyBerDreadWing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oro9ng/rocm64_using_latest_llvm_vs_rocm_7_lemonade_sdk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oro9ng/rocm64_using_latest_llvm_vs_rocm_7_lemonade_sdk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oro9ng/rocm64_using_latest_llvm_vs_rocm_7_lemonade_sdk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T12:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1orn6q4</id>
    <title>Handy : Free, Offline AI dictation app for PC, supports Whisper and Parakeet models</title>
    <updated>2025-11-08T11:32:08+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Handy is a trending GitHub repo which is a free alternate for Wispr Flow for AI dictation. The app size is quite small and it supports all Parakeet (nvidia) and Whisper model for speech to text. &lt;/p&gt; &lt;p&gt;GitHub : &lt;a href="https://github.com/cjpais/Handy"&gt;https://github.com/cjpais/Handy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo : &lt;a href="https://youtu.be/1QzXdhVeOkI?si=yli8cfejvOy3ERbo"&gt;https://youtu.be/1QzXdhVeOkI?si=yli8cfejvOy3ERbo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orn6q4/handy_free_offline_ai_dictation_app_for_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orn6q4/handy_free_offline_ai_dictation_app_for_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orn6q4/handy_free_offline_ai_dictation_app_for_pc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T11:32:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1orlqmh</id>
    <title>Honey we shrunk MiniMax M2</title>
    <updated>2025-11-08T10:04:06+00:00</updated>
    <author>
      <name>/u/arjunainfinity</name>
      <uri>https://old.reddit.com/user/arjunainfinity</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orlqmh/honey_we_shrunk_minimax_m2/"&gt; &lt;img alt="Honey we shrunk MiniMax M2" src="https://external-preview.redd.it/ehjUFeUe3VAS2s784qCwiz1JNp5TRqMKwWsfk9mVQNM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a556feed177eef7b3a5785bd56ea678c06d8ea4e" title="Honey we shrunk MiniMax M2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks, we pruned MiniMax M2 from 250B to 192B (~25%) with only ~5% loss in coding quality. We did this with $200 worth of 8XH200 compute. Our 50% pruned model is ETA 5 more days. Would love to hear your feedback and would you want a 50% pruned Kimi K2 Thinking?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arjunainfinity"&gt; /u/arjunainfinity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/VibeStudio/thrift"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orlqmh/honey_we_shrunk_minimax_m2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orlqmh/honey_we_shrunk_minimax_m2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T10:04:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ordgys</id>
    <title>We got this, we can do it! When is the REAP’d iQ_001_XXS GGUF dropping?</title>
    <updated>2025-11-08T02:15:28+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ordgys/we_got_this_we_can_do_it_when_is_the_reapd_iq_001/"&gt; &lt;img alt="We got this, we can do it! When is the REAP’d iQ_001_XXS GGUF dropping?" src="https://preview.redd.it/qfahc43zzxzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1975edd06526787672ea84d9ae1d9904b84715e1" title="We got this, we can do it! When is the REAP’d iQ_001_XXS GGUF dropping?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qfahc43zzxzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ordgys/we_got_this_we_can_do_it_when_is_the_reapd_iq_001/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ordgys/we_got_this_we_can_do_it_when_is_the_reapd_iq_001/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T02:15:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ormxoq</id>
    <title>Kimi K2 Thinking was trained with only $4.6 million</title>
    <updated>2025-11-08T11:16:59+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"&gt; &lt;img alt="Kimi K2 Thinking was trained with only $4.6 million" src="https://a.thumbs.redditmedia.com/L1ScXXMSwvWG0gMuGlVV23jh1shgAs9PlFDsIKJOn94.jpg" title="Kimi K2 Thinking was trained with only $4.6 million" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI: &amp;quot;We need government support to cover $1.4 trillion in chips and data centers.&amp;quot;&lt;/p&gt; &lt;p&gt;Kimi:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/03swwvpfo00g1.png?width=1199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f97e99e5a5aac5bb23a62ff58df048f5db678e09"&gt;https://preview.redd.it/03swwvpfo00g1.png?width=1199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f97e99e5a5aac5bb23a62ff58df048f5db678e09&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T11:16:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqy1k7</id>
    <title>AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)</title>
    <updated>2025-11-07T15:53:33+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt; &lt;img alt="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" src="https://preview.redd.it/8v2luf5owuzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9bc34ec8dddd94422397eaa91e0310250da5ba3" title="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8v2luf5owuzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:53:33+00:00</published>
  </entry>
</feed>
