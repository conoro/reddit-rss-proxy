<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-11T17:05:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o3vjz1</id>
    <title>Can we use glm coding api in python code?</title>
    <updated>2025-10-11T13:23:41+00:00</updated>
    <author>
      <name>/u/n3pst3r_007</name>
      <uri>https://old.reddit.com/user/n3pst3r_007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Need to do some concurrent requests to rewrite a small book.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/n3pst3r_007"&gt; /u/n3pst3r_007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3vjz1/can_we_use_glm_coding_api_in_python_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3vjz1/can_we_use_glm_coding_api_in_python_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3vjz1/can_we_use_glm_coding_api_in_python_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T13:23:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o33mui</id>
    <title>A list of models released or updated this week on this sub, in case you missed any (10 Oct).</title>
    <updated>2025-10-10T15:13:02+00:00</updated>
    <author>
      <name>/u/aifeed-fyi</name>
      <uri>https://old.reddit.com/user/aifeed-fyi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;Here is the list of models (releases and updates), I found mentioned on the LocalLlama for this week, Please update or let me know in the comments if there are any mistakes or misses. Enjoy !&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Reddit post&lt;/th&gt; &lt;th align="left"&gt;HF / GitHub&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Jamba 3B&lt;/td&gt; &lt;td align="left"&gt;tiny 3 B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1o1ac09"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;WEBGEN,UIGEN‚ÄëFX&lt;/td&gt; &lt;td align="left"&gt;research‚Äëpreview for UI/UX&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1nz20g2"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Tesslate/UIGENT-30B-3A-Preview"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;KAT-Dev-72B-Exp&lt;/td&gt; &lt;td align="left"&gt;Coding model&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o31rdl/kwaipilotkatdev72bexp_model_released"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Playable-GGUF&lt;/td&gt; &lt;td align="left"&gt;7b vibe coding retro games&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o27xsj/introducing_playable1gguf_by_far_the_worlds_best"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/playable/Playable1-GGUF"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UserLM-8b&lt;/td&gt; &lt;td align="left"&gt;8b LLM playing user role&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o23vqf/microsoftuserlm8b_unlike_typical_llms_that_are"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/microsoft/UserLM-8b"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CoDA‚Äëv0‚ÄëInstruct&lt;/td&gt; &lt;td align="left"&gt;language‚Äëdiffusion&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o1s7q8/an_open_sourced_language_diffusion_model_by_sf/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Salesforce/CoDA-v0-Instruct"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ColBERT Nano 250K&lt;/td&gt; &lt;td align="left"&gt;tiny‚Äëretrieval&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o1mpt5/introducing_the_colbert_nano_series_of_models_all/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/NeuML/"&gt;HF collection&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2‚Äë8B‚ÄëA1B&lt;/td&gt; &lt;td align="left"&gt;hybrid 8 B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o0zted/lfm28ba1b_quality_34b_dense_yet_faster_than/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-8B-A1B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3‚ÄëVL‚Äë30B‚ÄëA3B‚ÄëInstruct&lt;/td&gt; &lt;td align="left"&gt;vision‚ÄëLLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BULaMU&lt;/td&gt; &lt;td align="left"&gt;Luganda LLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nykxfq/bulamuthe_first_luganda_large_language_model/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/datasets/mwebazarick/BULaMU"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;YanoljaNEXT‚ÄëRosetta‚Äë12B‚Äë2510&lt;/td&gt; &lt;td align="left"&gt;translation 12 B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o2bm3z/yanoljayanoljanextrosetta12b2510/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-12B-2510"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SDLM 32B&lt;/td&gt; &lt;td align="left"&gt;multimodal 32B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nwxje9/sdlm_32b4b_from_opengvlab/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/OpenGVLab/SDLM-32B-D4"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SDLM 4B&lt;/td&gt; &lt;td align="left"&gt;multimodal 4 B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nwxje9/sdlm_32b4b_from_opengvlab/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/OpenGVLab/SDLM-3B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;üîß Notable resources&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Resource&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Reddit post&lt;/th&gt; &lt;th align="left"&gt;HF / GitHub&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;MLXSharp&lt;/td&gt; &lt;td align="left"&gt;.NET MLX wrapper&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nypq6q/made_the_first_net_wrapper_for_apple_mlx_looking/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/managedcode/MLXSharp"&gt;GH&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Video2X 6.x&lt;/td&gt; &lt;td align="left"&gt;upscaler + interpolation&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nykzv3/video2x_6x_opensource_upscaler_frame/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/k4yt3x/video2x"&gt;GH&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SurfSense&lt;/td&gt; &lt;td align="left"&gt;Perplexity alt.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o00ban/open_source_alternative_to_perplexity/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/MODSetter/SurfSense"&gt;GH&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aifeed-fyi"&gt; /u/aifeed-fyi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o33mui/a_list_of_models_released_or_updated_this_week_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o33mui/a_list_of_models_released_or_updated_this_week_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o33mui/a_list_of_models_released_or_updated_this_week_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T15:13:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3sch6</id>
    <title>Adding search to open models</title>
    <updated>2025-10-11T10:38:26+00:00</updated>
    <author>
      <name>/u/Simple_Split5074</name>
      <uri>https://old.reddit.com/user/Simple_Split5074</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So right now I mostly use the small GLM Plan in Roo - main missing thing is search, that is only available in the 5 times more expensive medium plan. &lt;/p&gt; &lt;p&gt;Do I need to bite the bullet there and get the medium plan or are there better options?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Simple_Split5074"&gt; /u/Simple_Split5074 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3sch6/adding_search_to_open_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3sch6/adding_search_to_open_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3sch6/adding_search_to_open_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T10:38:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3lgag</id>
    <title>Alignment is strong on this one</title>
    <updated>2025-10-11T03:40:01+00:00</updated>
    <author>
      <name>/u/Honest-Debate-6863</name>
      <uri>https://old.reddit.com/user/Honest-Debate-6863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3lgag/alignment_is_strong_on_this_one/"&gt; &lt;img alt="Alignment is strong on this one" src="https://preview.redd.it/750najbileuf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee379ef86f00df4ce67bac7aed8f41c898b52758" title="Alignment is strong on this one" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve noticed the Auto mode in cursor was getting good suddenly the quality stopped and has been ignoring instructions even when steered in a direction. It seems to forget the direction and steer back on the wrong direction it previously choose. &lt;/p&gt; &lt;p&gt;I think it‚Äôs developing some ego &lt;/p&gt; &lt;p&gt;Are the RL reward model tuning making it ego-centric? Is there a metric or bench to measure this? Is there a way to create a balance? I‚Äôve seen this in a lot of open source models as well. Appreciate any literature references that you can provide. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Honest-Debate-6863"&gt; /u/Honest-Debate-6863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/750najbileuf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3lgag/alignment_is_strong_on_this_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3lgag/alignment_is_strong_on_this_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T03:40:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3evon</id>
    <title>What laptop would you choose? Ryzen AI MAX+ 395 with 128GB of unified RAM or Intel 275HX + Nvidia RTX 5090 (128GB of RAM + 24GB of VRAM)?</title>
    <updated>2025-10-10T22:20:42+00:00</updated>
    <author>
      <name>/u/cl0p3z</name>
      <uri>https://old.reddit.com/user/cl0p3z</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For more or less the same price I can chose between this two laptops:&lt;/p&gt; &lt;p&gt;- HP G1a: &lt;strong&gt;AMD Ryzen AI MAX+ 395 with 128GB of RAM (no eGPU)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Lenovo ThinkPad P16 Gen 3: &lt;strong&gt;Intel 275HX with 128GB of RAM + Nvidia RTX 5090 24GB of VRAM&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;What would you choose and why?&lt;/p&gt; &lt;p&gt;What I can do with AI/LLMs with one that I can't do with the other?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cl0p3z"&gt; /u/cl0p3z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3evon/what_laptop_would_you_choose_ryzen_ai_max_395/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3evon/what_laptop_would_you_choose_ryzen_ai_max_395/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3evon/what_laptop_would_you_choose_ryzen_ai_max_395/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T22:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3jl8r</id>
    <title>Here are the benchmarks that I keep up with</title>
    <updated>2025-10-11T02:02:27+00:00</updated>
    <author>
      <name>/u/SomeOddCodeGuy_v2</name>
      <uri>https://old.reddit.com/user/SomeOddCodeGuy_v2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey hey folks! I've returned... in a fashion.&lt;/p&gt; &lt;p&gt;I've been sitting on all kinds of stuff that I wanted to talk about for the past few months, but I figured I'd start by dropping the list of benchmarks I currently track, since in the past folks were interested in that list.&lt;/p&gt; &lt;p&gt;These should be mostly up to date, and I'm constantly on the prowl for more. If you have any good ones (&lt;em&gt;ESPECIALLY translation benchmarks... those feel like the holy grail&lt;/em&gt;), please share.&lt;/p&gt; &lt;p&gt;I know there are a lot more leaderboards out there, but I generally don't hang on to the ones that either aren't kept reasonably up to date, or were exceptionally limited. So if you don't see a leaderboard on here, feel free to share but it may have been excluded on purpose.&lt;/p&gt; &lt;p&gt;As always- benchmarks aren't everything, and you should always try the models out yourself. But it definitely is nice to have some metrics to look at from time to time, even if they can get gamed.&lt;/p&gt; &lt;h1&gt;Code Specific&lt;/h1&gt; &lt;p&gt;&lt;a href="https://www.swebench.com/"&gt;SWE Bench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://aider.chat/docs/leaderboards/"&gt;Aider Coding Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Context Window Capability&lt;/h1&gt; &lt;p&gt;&lt;a href="https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87"&gt;FictionBench&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;(This is a really good one, as it visualizes where so many people mess up with LLMs: not realizing context window limitations)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;General Ability&lt;/h1&gt; &lt;p&gt;&lt;a href="https://livebench.ai/#/"&gt;Livebench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://dubesor.de/benchtable"&gt;Dubesor Benchtable&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://scale.com/leaderboard/humanitys_last_exam_text_only"&gt;Humanity's Last Exam&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;(I am shocked at how low of a score GLM 4.5 got here... testing error maybe?)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Domain Knowledge&lt;/h1&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro"&gt;MMLU-Pro&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Advanced Reasoning&lt;/h1&gt; &lt;p&gt;&lt;a href="https://scale.com/leaderboard/enigma_eval"&gt;Enigma Eval&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Human Preference&lt;/h1&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/leaderboard"&gt;LM Arena&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;EQ (emotional intelligence) and Creative Writing Ability&lt;/h1&gt; &lt;p&gt;&lt;a href="https://eqbench.com/"&gt;EQBench&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Censorship&lt;/h1&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard"&gt;Uncensored General Intelligence Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Intelligence Index, Cost, Speed, and Model Comparisons&lt;/h1&gt; &lt;p&gt;&lt;a href="https://artificialanalysis.ai/leaderboards/models"&gt;Artificial Analysis&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Coding Agent Capability&lt;/h1&gt; &lt;p&gt;&lt;a href="https://www.tbench.ai/leaderboard"&gt;Terminal Bench&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Kotlin (Android dev)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://firebender.com/leaderboard"&gt;Kotlin Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Function Calling&lt;/h1&gt; &lt;p&gt;&lt;a href="https://gorilla.cs.berkeley.edu/leaderboard.html"&gt;Berkeley Function-Calling Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Other&lt;/h1&gt; &lt;p&gt;&lt;a href="https://www.vellum.ai/llm-leaderboard"&gt;Vellum Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeOddCodeGuy_v2"&gt; /u/SomeOddCodeGuy_v2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3jl8r/here_are_the_benchmarks_that_i_keep_up_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3jl8r/here_are_the_benchmarks_that_i_keep_up_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3jl8r/here_are_the_benchmarks_that_i_keep_up_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T02:02:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3r6et</id>
    <title>Quality degradation of fp8 quantization?</title>
    <updated>2025-10-11T09:25:15+00:00</updated>
    <author>
      <name>/u/Confident-Willow5457</name>
      <uri>https://old.reddit.com/user/Confident-Willow5457</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've been entirely in the GGUF ecosystem up till now, but I've been wanting to try out vllm for a potential speed boost as well as batching. &lt;/p&gt; &lt;p&gt;Generally with GGUFs Q8_0 is considered to be so close to full precision that it's practically indistinguishable. It's my understanding that Q8_0 is a bit closer to full precision than FP8, but how much worse is FP8 than full precision exactly? As a reference, is it between Q8 and Q6? Worse?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Confident-Willow5457"&gt; /u/Confident-Willow5457 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3r6et/quality_degradation_of_fp8_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3r6et/quality_degradation_of_fp8_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3r6et/quality_degradation_of_fp8_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T09:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3s74i</id>
    <title>How should I translate movie subtitles?</title>
    <updated>2025-10-11T10:29:27+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(I hope this thread's replies can serve as a tutorial to other people interested in this. I was not able to find a similar discussion in this sub's search history. There was a few threads about &amp;quot;best model for X language&amp;quot;, but nothing about translation workflows.)&lt;/p&gt; &lt;p&gt;I have english subtitles for movies and I'd like to convert them to arabic. This is just a low-effort thing, I'm not planning on distributing this on the internet, it's just to quickly put out some srt's for someone I know when none exist in his language.&lt;/p&gt; &lt;p&gt;Just to remind the folks at home, SRT subtitles files are text files that look like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1 00:00:12,222 --&amp;gt; 00:00:15,333 Oh no, the ship's sinking! 2 00:00:16,123 --&amp;gt; 00:00:20,456 To the life rafts, now! ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For reference, the subtitle file for Interstellar is 58k tokens.&lt;/p&gt; &lt;p&gt;I don't think I should be just dumping 50k+ tokens into a small local LLM and asking it to translate. I'm worried about the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Hallucinations in the timestamps: this would completely mess up the subtitles, ruining the movie&lt;/li&gt; &lt;li&gt;Hallucinations/schizoness in the content: many LLMs degrade at such large contexts&lt;/li&gt; &lt;li&gt;the LLM might simply drop some entries altogether, I've seen it before, and that was ChatGPT! (asked to translate a photo of a newspaper article with just 5 paragraphs)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So what are my options here? How do hobbyist AI translators do it?&lt;/p&gt; &lt;p&gt;My thoughts in comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3s74i/how_should_i_translate_movie_subtitles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3s74i/how_should_i_translate_movie_subtitles/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3s74i/how_should_i_translate_movie_subtitles/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T10:29:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3qx3e</id>
    <title>Kwaipilot/KAT-Dev-72B-Exp seems to be a great coding modelÔºü</title>
    <updated>2025-10-11T09:08:47+00:00</updated>
    <author>
      <name>/u/Human-Gas-1288</name>
      <uri>https://old.reddit.com/user/Human-Gas-1288</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3qx3e/kwaipilotkatdev72bexp_seems_to_be_a_great_coding/"&gt; &lt;img alt="Kwaipilot/KAT-Dev-72B-Exp seems to be a great coding modelÔºü" src="https://external-preview.redd.it/rxyepxgYUof3_-pxPA16Sj6OoiuoO3OTQiZrKV-cxps.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a34ebb6475d3fb833395063cb58949ed7cc21cd" title="Kwaipilot/KAT-Dev-72B-Exp seems to be a great coding modelÔºü" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp"&gt;https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j0m718zu7guf1.png?width=1186&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f04d36fe4ab33026c8bafc9cb90592b260562ec"&gt;https://preview.redd.it/j0m718zu7guf1.png?width=1186&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f04d36fe4ab33026c8bafc9cb90592b260562ec&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Human-Gas-1288"&gt; /u/Human-Gas-1288 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3qx3e/kwaipilotkatdev72bexp_seems_to_be_a_great_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3qx3e/kwaipilotkatdev72bexp_seems_to_be_a_great_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3qx3e/kwaipilotkatdev72bexp_seems_to_be_a_great_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T09:08:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3ttlo</id>
    <title>How do I compare cost per token for serverless vs provisioned hardware?</title>
    <updated>2025-10-11T12:00:57+00:00</updated>
    <author>
      <name>/u/OverclockingUnicorn</name>
      <uri>https://old.reddit.com/user/OverclockingUnicorn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How are you guys comparing the cost per token for serverless vs provisioned hardware?&lt;/p&gt; &lt;p&gt;Eg, aws bedrock vs an EC2 running vllm&lt;/p&gt; &lt;p&gt;Mostly interested in batch inference costs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OverclockingUnicorn"&gt; /u/OverclockingUnicorn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3ttlo/how_do_i_compare_cost_per_token_for_serverless_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3ttlo/how_do_i_compare_cost_per_token_for_serverless_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3ttlo/how_do_i_compare_cost_per_token_for_serverless_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T12:00:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3gyjn</id>
    <title>best coding LLM right now?</title>
    <updated>2025-10-10T23:53:41+00:00</updated>
    <author>
      <name>/u/RadianceTower</name>
      <uri>https://old.reddit.com/user/RadianceTower</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Models constantly get updated and new ones come out, so old posts aren't as valid.&lt;/p&gt; &lt;p&gt;I have 24GB of VRAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RadianceTower"&gt; /u/RadianceTower &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3gyjn/best_coding_llm_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3gyjn/best_coding_llm_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3gyjn/best_coding_llm_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T23:53:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3zgy7</id>
    <title>The LLM running on my local PC is too slow.</title>
    <updated>2025-10-11T16:07:43+00:00</updated>
    <author>
      <name>/u/Glanble</name>
      <uri>https://old.reddit.com/user/Glanble</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I'm getting really slow speeds and need a sanity check.&lt;br /&gt; I'm only getting 1.0 t/s running a C4AI 111B model (63GB Q4_GGUF) on an RTX 5090 with 128GB of RAM.&lt;br /&gt; this normal, or is something wrong with my config?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glanble"&gt; /u/Glanble &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3zgy7/the_llm_running_on_my_local_pc_is_too_slow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3zgy7/the_llm_running_on_my_local_pc_is_too_slow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3zgy7/the_llm_running_on_my_local_pc_is_too_slow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T16:07:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3vnt3</id>
    <title>Tooling+Model recommendations for base (16G) mac Mini M4 as remote server?</title>
    <updated>2025-10-11T13:28:29+00:00</updated>
    <author>
      <name>/u/Valuable-Question706</name>
      <uri>https://old.reddit.com/user/Valuable-Question706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use Intel laptop as my main coding machine. Recently got myself a base model mac Mini and got surprised how fast it is for inference.&lt;/p&gt; &lt;p&gt;I'm still very new at using AI for coding. Not trying to be lazy, but want to get an advice in a large and quickly developing field from knowledgeable people.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;What I already tried: &lt;a href="http://Continue.dev"&gt;Continue.dev&lt;/a&gt; in VS studio + ollama with qwen2.5-coder:7B. It works, but is there a better, more efficient way? I'm quite technical so I won't mind running more complex software stack if it brings significant improvements.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I'd like to automate some routine, boring programming tasks, for example: writing boilerplate html/js, writing bash scripts (yes, I very carefully check them before running), writing basic, boring python code. Nothing too complex, because I still prefer using my brain for actual work, plus even paid edge models are still not good at my area.&lt;/p&gt; &lt;p&gt;So I need a model that is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;is good at tasks specified above (should I use a specially optimized model or generic ones are OK?)&lt;/li&gt; &lt;li&gt;outputs at least 15+ tokens/sec&lt;/li&gt; &lt;li&gt;would integrate nicely with tooling on my work machine&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also, what does a proper, modern VS code setup looks nowadays? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Valuable-Question706"&gt; /u/Valuable-Question706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3vnt3/toolingmodel_recommendations_for_base_16g_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3vnt3/toolingmodel_recommendations_for_base_16g_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3vnt3/toolingmodel_recommendations_for_base_16g_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T13:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3sstr</id>
    <title>Every single COT terms score.</title>
    <updated>2025-10-11T11:04:46+00:00</updated>
    <author>
      <name>/u/Ambitious-a4s</name>
      <uri>https://old.reddit.com/user/Ambitious-a4s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;1. Zeroing&lt;/strong&gt; - By far the most annoying word I've ever heard, used by Gemini 2.5 Pro's Thinking but when people actually knows this meaning, it just a fancy word that means 'I am directing all the focus into this or that (subject)'. Its efficient (it only uses two tokens) and fancy but annoying. &lt;strong&gt;8.7/10&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Synthesizing&lt;/strong&gt; - GLM 4.6 uses this, Gemini 2.5 Pro Exp, and more. Its fancy wording too, but it just means 'I am combining this thought I have with the old thought I made', its good and it doesn't really sound the much annoying (IMO). It also helps the AI combining ideas and thoughts in one go so its &lt;strong&gt;9/10&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Hmm&lt;/strong&gt; - Used by Qwen (idk what model but Qwen), Deepseek V3.1 to V3.2, and more. Its not fancy, but it just means lots of things, sometimes Qwen 3 235B do this by pausing and hesitating before dumping more thoughts, and Deepseek uses this in the first word to think. It doesn't do much I would say, its by far mid and its only for pausing and hesitating to think. My only favorite part is that it uses two tokens, &lt;strong&gt;6.6/10&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Confidence Score/Confident Score&lt;/strong&gt; - For some reason, it's one of my favorite terms. It makes the LLM aware how confident it is with the answer or not. It can also make the LLM think more further ahead for some reason, but its not perfect, most LLM's hallucinate and would think its 5/5 or 10/10 in a wrong answer they would give so sometimes it had no point to use it. &lt;strong&gt;7.1/10&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Alternatively&lt;/strong&gt; - The old Deepseek times. ONE OF MY FAVORITE TERMS. It makes the LLM aware of its thoughts so it lowers the mistake. &lt;/p&gt; &lt;p&gt;The biggest con is when the LLM uses these terms is that it BLOATS, when you expect it only uses 941 tokens on a single thought turned into a massive 5000 tokens in a single thought before response. It makes my API cripple so bad and its the biggest con. So its a &lt;strong&gt;5.4/10&lt;/strong&gt;, I wish it can be back but this time more efficient.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious-a4s"&gt; /u/Ambitious-a4s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3sstr/every_single_cot_terms_score/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3sstr/every_single_cot_terms_score/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3sstr/every_single_cot_terms_score/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T11:04:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3k9zr</id>
    <title>AI Studio Pro mini PC from Orange Pi pairs dual Huawei Ascend 310 processors with up to 192GB of RAM</title>
    <updated>2025-10-11T02:37:28+00:00</updated>
    <author>
      <name>/u/cafedude</name>
      <uri>https://old.reddit.com/user/cafedude</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3k9zr/ai_studio_pro_mini_pc_from_orange_pi_pairs_dual/"&gt; &lt;img alt="AI Studio Pro mini PC from Orange Pi pairs dual Huawei Ascend 310 processors with up to 192GB of RAM" src="https://external-preview.redd.it/pGGQzmvOi4pQiLTOWOaCqIqsQ66ZIQqjltXUjZikyAU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5a776a9f8f4b6e3a30c9df214e7029c4c64c5b8" title="AI Studio Pro mini PC from Orange Pi pairs dual Huawei Ascend 310 processors with up to 192GB of RAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cafedude"&gt; /u/cafedude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techradar.com/pro/this-mini-pc-has-192gb-of-ram-yes-ram-but-thats-not-the-most-surprising-fact-about-it-the-orange-pi-ai-studio-pro-uses-a-huawei-ascend-310-thats-on-paper-7x-more-powerful-than-amds-ryzen-ai-max-395"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3k9zr/ai_studio_pro_mini_pc_from_orange_pi_pairs_dual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3k9zr/ai_studio_pro_mini_pc_from_orange_pi_pairs_dual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T02:37:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1o408is</id>
    <title>How to use GLM Plan + Claude Plan with Claude Code on macOS</title>
    <updated>2025-10-11T16:38:25+00:00</updated>
    <author>
      <name>/u/Routine-Teach5293</name>
      <uri>https://old.reddit.com/user/Routine-Teach5293</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o408is/how_to_use_glm_plan_claude_plan_with_claude_code/"&gt; &lt;img alt="How to use GLM Plan + Claude Plan with Claude Code on macOS" src="https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f53460a90493497883ab4cacbbb58e2acb464c4" title="How to use GLM Plan + Claude Plan with Claude Code on macOS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Routine-Teach5293"&gt; /u/Routine-Teach5293 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://gist.github.com/RuiNelson/a5af5620404a0a9fbf3cf3e92fe97585"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o408is/how_to_use_glm_plan_claude_plan_with_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o408is/how_to_use_glm_plan_claude_plan_with_claude_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T16:38:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3kb3o</id>
    <title>Real SVD GLM-4.5-Air-GLM-4.6-Distill</title>
    <updated>2025-10-11T02:39:01+00:00</updated>
    <author>
      <name>/u/realmaywell</name>
      <uri>https://old.reddit.com/user/realmaywell</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3kb3o/real_svd_glm45airglm46distill/"&gt; &lt;img alt="Real SVD GLM-4.5-Air-GLM-4.6-Distill" src="https://external-preview.redd.it/e09zZ1vJH-206eXdnYOOyzVU_npt174nxPybTvRs_LQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9aac58b8a3b097d5a8fa8a939d97c17ee2f3dee5" title="Real SVD GLM-4.5-Air-GLM-4.6-Distill" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/imzpad6l8euf1.png?width=7167&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1582f372758ee501dae32e6d09cf36652bba3a9f"&gt;https://preview.redd.it/imzpad6l8euf1.png?width=7167&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1582f372758ee501dae32e6d09cf36652bba3a9f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is the person who posted that &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1fb6jdy/reflectionllama3170b_is_actually_llama3/"&gt;Reflection-Llama-3.1-70B is actually Llama-3. &lt;/a&gt;I didn't expect my first Reddit post in a year would be another debunk.&lt;/p&gt; &lt;p&gt;After seeing this &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nyopyc/did_anyone_try_out_glm45airglm46distill/"&gt;Reddit post&lt;/a&gt;, the idea of using SVD to distill a model seemed plausible, So I decided to test it out myself.&lt;/p&gt; &lt;p&gt;Although the original model mentioned in the post was a scam, I was curious about what would happen if I actually applied the methodology. So, I rewrote the entire existing CPU-based script into PyTorch code and ran the experiment on an H200 machine.&lt;/p&gt; &lt;p&gt;I excluded the LayerNorm and embed layers from the distillation because it was obvious that including them would break the model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3y75s9519euf1.png?width=2502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3eb9670538e5c603cdeba84a0e04e3035817192e"&gt;https://preview.redd.it/3y75s9519euf1.png?width=2502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3eb9670538e5c603cdeba84a0e04e3035817192e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The results are as shown in the pictures above. I guess this means you don't have to go out of your way to replicate this experiment yourselves.&lt;/p&gt; &lt;p&gt;Still, for those who do want to try it out, you might find the model and LoRA below helpful.&lt;br /&gt; Model: &lt;a href="https://huggingface.co/maywell/GLM-4.5-Air-GLM-4.6-Distill"&gt;https://huggingface.co/maywell/GLM-4.5-Air-GLM-4.6-Distill&lt;/a&gt;&lt;br /&gt; LoRA: &lt;a href="https://huggingface.co/maywell/GLM-4.5-Air-GLM-4.6-Distill-LoRA"&gt;https://huggingface.co/maywell/GLM-4.5-Air-GLM-4.6-Distill-LoRA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yay.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/realmaywell"&gt; /u/realmaywell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3kb3o/real_svd_glm45airglm46distill/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3kb3o/real_svd_glm45airglm46distill/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3kb3o/real_svd_glm45airglm46distill/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T02:39:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3dfib</id>
    <title>GPT-OSS from Scratch on AMD GPUs</title>
    <updated>2025-10-10T21:21:45+00:00</updated>
    <author>
      <name>/u/tuanlda78202</name>
      <uri>https://old.reddit.com/user/tuanlda78202</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3dfib/gptoss_from_scratch_on_amd_gpus/"&gt; &lt;img alt="GPT-OSS from Scratch on AMD GPUs" src="https://external-preview.redd.it/kr7-cYQLjVSYgCHTdYk0hOHo8LtDx1fruxInOK1rC5k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df53654295de2b4add7e5b3992fa595b88f048f1" title="GPT-OSS from Scratch on AMD GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After six years-the first time since GPT-2, OpenAI has released new open-weight LLMs, gpt-oss-20b and gpt-oss-120b. From day one, many inference engines such as llama.cpp, vLLM, and sgl-project have supported these models; however, most focus on maximizing throughput using CUDA for NVIDIA GPUs, offering limited support for AMD* GPUs. Moreover, their library-oriented implementations are often complex to understand and difficult to adapt for personal or experimental use cases.&lt;/p&gt; &lt;p&gt;To address these limitations, my team introduce ‚Äúgpt-oss-amd‚Äù, a pure C++ implementation of OpenAI‚Äôs GPT-OSS models designed to maximize inference throughput on AMD GPUs without relying on external libraries. Our goal is to explore end-to-end LLM optimization, from kernel-level improvements to system-level design, providing insights for researchers and developers interested in high-performance computing and model-level optimization.&lt;/p&gt; &lt;p&gt;Inspired by llama2.c by Andrej Karpathy, our implementation uses HIP (an AMD programming model equivalent to CUDA) and avoids dependencies such as rocBLAS, hipBLAS, RCCL, and MPI. We utilize multiple optimization strategies for the 20B and 120B models, including efficient model loading, batching, multi-streaming, multi-GPU communication, optimized CPU‚ÄìGPU‚ÄìSRAM memory access, FlashAttention, matrix-core‚Äìbased GEMM, and load balancing for MoE routing.&lt;/p&gt; &lt;p&gt;Experiments on a single node with 8√ó AMD MI250 GPUs show that our implementation achieves over 30k TPS on the 20B model and nearly 10k TPS on the 120B model in custom benchmarks, demonstrating the effectiveness of our optimizations and the strong potential of AMD GPUs for large-scale LLM inference.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9dr4gme0qcuf1.png?width=3392&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ad41a33bc3ecf6625afaa2ff62ca762f5479d2a"&gt;https://preview.redd.it/9dr4gme0qcuf1.png?width=3392&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ad41a33bc3ecf6625afaa2ff62ca762f5479d2a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/tuanlda78202/gpt-oss-amd"&gt;https://github.com/tuanlda78202/gpt-oss-amd&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tuanlda78202"&gt; /u/tuanlda78202 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3dfib/gptoss_from_scratch_on_amd_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3dfib/gptoss_from_scratch_on_amd_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3dfib/gptoss_from_scratch_on_amd_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T21:21:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3z7q9</id>
    <title>LM Studio + Open-WebUI - no reasoning</title>
    <updated>2025-10-11T15:57:46+00:00</updated>
    <author>
      <name>/u/michalpl7</name>
      <uri>https://old.reddit.com/user/michalpl7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I run &lt;strong&gt;LM Studio&lt;/strong&gt; + &lt;strong&gt;Open-WebUI&lt;/strong&gt; with model &lt;strong&gt;GPT-OSS-20b&lt;/strong&gt; but it's much worse on that web page than used locally in LM Studio, it answers completely stupid. I also don't see the &lt;strong&gt;reasoning button&lt;/strong&gt;, checked models settings in Open-WebUI admin page but there were nothing matching, only vision, file input, code interpreter, etc. Do you know how to make it working same smart with open-webui as local?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/michalpl7"&gt; /u/michalpl7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3z7q9/lm_studio_openwebui_no_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3z7q9/lm_studio_openwebui_no_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3z7q9/lm_studio_openwebui_no_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T15:57:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3atdu</id>
    <title>GLM 5 coming before the end of 2025</title>
    <updated>2025-10-10T19:41:19+00:00</updated>
    <author>
      <name>/u/Helpful_Jacket8953</name>
      <uri>https://old.reddit.com/user/Helpful_Jacket8953</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3atdu/glm_5_coming_before_the_end_of_2025/"&gt; &lt;img alt="GLM 5 coming before the end of 2025" src="https://a.thumbs.redditmedia.com/hVBgymEdEuqZZKnYwcpRm5UQBweQDbSo5chE1AGg9a8.jpg" title="GLM 5 coming before the end of 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Get ready. At this rate it seems like there's a real chance it'll start surpassing SOTA models on some benchmarks, not just DeepSeek.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2knp5zv98cuf1.png?width=1556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4458e1672e8db34aceb649902b4d838d88335dc2"&gt;https://preview.redd.it/2knp5zv98cuf1.png?width=1556&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4458e1672e8db34aceb649902b4d838d88335dc2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Helpful_Jacket8953"&gt; /u/Helpful_Jacket8953 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3atdu/glm_5_coming_before_the_end_of_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3atdu/glm_5_coming_before_the_end_of_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3atdu/glm_5_coming_before_the_end_of_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T19:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3l5zs</id>
    <title>GLM just blow up, or have I been in the dark?</title>
    <updated>2025-10-11T03:24:01+00:00</updated>
    <author>
      <name>/u/EasyConference4177</name>
      <uri>https://old.reddit.com/user/EasyConference4177</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems like this community is ever moving, did GLM just blow up? like, I did not realise so many people talked about it.... What kinda system are you guys on 4.6 running? Because it looks like I would essential need 4x48gb Quadro 8000s/a6000s/6000 ada GPUs or at least 2x96gb RTX Pro 6000s... I may can afford 4 quadros but not 2 rtx pro 6000s, for the price of a car. lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EasyConference4177"&gt; /u/EasyConference4177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3l5zs/glm_just_blow_up_or_have_i_been_in_the_dark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3l5zs/glm_just_blow_up_or_have_i_been_in_the_dark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3l5zs/glm_just_blow_up_or_have_i_been_in_the_dark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T03:24:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1o371t4</id>
    <title>bro disappeared like he never existed</title>
    <updated>2025-10-10T17:18:54+00:00</updated>
    <author>
      <name>/u/Full_Piano_3448</name>
      <uri>https://old.reddit.com/user/Full_Piano_3448</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o371t4/bro_disappeared_like_he_never_existed/"&gt; &lt;img alt="bro disappeared like he never existed" src="https://preview.redd.it/2e01fz4pibuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2158310d30c308746ab8924442748cf6a37b692a" title="bro disappeared like he never existed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Knowing him is a sign you‚Äôve been in the AI game for a long time (iykyk)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Full_Piano_3448"&gt; /u/Full_Piano_3448 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2e01fz4pibuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o371t4/bro_disappeared_like_he_never_existed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o371t4/bro_disappeared_like_he_never_existed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T17:18:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o394p3</id>
    <title>Here we go again</title>
    <updated>2025-10-10T18:36:34+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o394p3/here_we_go_again/"&gt; &lt;img alt="Here we go again" src="https://preview.redd.it/b2abfaikwbuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7db3949cb0def07809e7a9ba9a730d1582083844" title="Here we go again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b2abfaikwbuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o394p3/here_we_go_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o394p3/here_we_go_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T18:36:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3xluf</id>
    <title>Fighting Email Spam on Your Mail Server with LLMs ‚Äî Privately</title>
    <updated>2025-10-11T14:52:20+00:00</updated>
    <author>
      <name>/u/unixf0x</name>
      <uri>https://old.reddit.com/user/unixf0x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sharing a blog post I wrote: &lt;a href="https://cybercarnet.eu/posts/email-spam-llm/"&gt;https://cybercarnet.eu/posts/email-spam-llm/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's about how to use local LLMs on your own mail server to identify and fight email spam.&lt;/p&gt; &lt;p&gt;This uses Mailcow, Rspamd, Ollama and a custom proxy in python.&lt;/p&gt; &lt;p&gt;Give your opinion, what you think about the post. If this could be useful for those of you that self-host mail servers.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unixf0x"&gt; /u/unixf0x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3xluf/fighting_email_spam_on_your_mail_server_with_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3xluf/fighting_email_spam_on_your_mail_server_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3xluf/fighting_email_spam_on_your_mail_server_with_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T14:52:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3opq5</id>
    <title>What the sub feels like lately</title>
    <updated>2025-10-11T06:47:33+00:00</updated>
    <author>
      <name>/u/marderbot13</name>
      <uri>https://old.reddit.com/user/marderbot13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"&gt; &lt;img alt="What the sub feels like lately" src="https://preview.redd.it/92s8znbxifuf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb4866bff0d572386ea47fc19d643a6b2261fbdb" title="What the sub feels like lately" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marderbot13"&gt; /u/marderbot13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/92s8znbxifuf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T06:47:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
