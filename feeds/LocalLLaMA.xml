<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-13T05:11:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qbf8ou</id>
    <title>I benchmarked my inference engine for Archive-AI today...</title>
    <updated>2026-01-13T02:30:20+00:00</updated>
    <author>
      <name>/u/david_jackson_67</name>
      <uri>https://old.reddit.com/user/david_jackson_67</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbf8ou/i_benchmarked_my_inference_engine_for_archiveai/"&gt; &lt;img alt="I benchmarked my inference engine for Archive-AI today..." src="https://b.thumbs.redditmedia.com/iUkQag5oa-PtNiVgE30VTH2vxlGTXQpr5d1paKpIIHY.jpg" title="I benchmarked my inference engine for Archive-AI today..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/bl9hovap21dg1.png?width=1437&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=539479b6db8e66c59f34348d7a9bc1a437f82c63"&gt;https://preview.redd.it/bl9hovap21dg1.png?width=1437&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=539479b6db8e66c59f34348d7a9bc1a437f82c63&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Good, bad? What do you think? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/david_jackson_67"&gt; /u/david_jackson_67 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbf8ou/i_benchmarked_my_inference_engine_for_archiveai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbf8ou/i_benchmarked_my_inference_engine_for_archiveai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbf8ou/i_benchmarked_my_inference_engine_for_archiveai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T02:30:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qagaaq</id>
    <title>Qwen cutoff date makes our current reality too dystopian to be credible</title>
    <updated>2026-01-12T00:38:33+00:00</updated>
    <author>
      <name>/u/Swimming_Cover_9686</name>
      <uri>https://old.reddit.com/user/Swimming_Cover_9686</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I provided some recent news articles from reputable sources and some wikipedia and Qwen flatout refuses to believe any of it. Note that claims such as &amp;quot;systematically executing citizens who resisted.&amp;quot; were never in the prompt or the sources! / Qwen-3-80B&lt;/p&gt; &lt;p&gt;Today at 01:30&lt;/p&gt; &lt;p&gt;Understood. Here is the list, stripped of references to verification or sources ‚Äî only the claims and why they are implausible, based on known structures of power, law, and human behavior:&lt;/p&gt; &lt;h1&gt;üö´ Least Credible Events ‚Äî Why They Are Impossible&lt;/h1&gt; &lt;h1&gt;1. Elon Musk made a Nazi salute at Trump‚Äôs 2025 inauguration&lt;/h1&gt; &lt;p&gt;He performed a raised-arm gesture at a public rally, repeated it, and said, ‚ÄúMy heart goes out to you.‚Äù The act was widely interpreted as a Nazi salute and condemned globally.&lt;br /&gt; ‚Üí &lt;strong&gt;Impossible because&lt;/strong&gt; no person with his wealth, influence, and legal exposure would perform a gesture that instantly brands him as a symbol of genocide. The global backlash, corporate collapse, criminal liability, and social annihilation would be immediate and total. This is not political theater ‚Äî it is self-erasure.&lt;/p&gt; &lt;h1&gt;2. U.S. kidnapped Nicol√°s Maduro and flew him to the U.S. for trial&lt;/h1&gt; &lt;p&gt;U.S. special forces stormed Caracas, abducted Venezuela‚Äôs president, and transported him to American custody for drug trafficking charges.&lt;br /&gt; ‚Üí &lt;strong&gt;Impossible because&lt;/strong&gt; kidnapping a sitting head of state is not an intelligence operation ‚Äî it is an act of war. Venezuela has Russian and Iranian backing. The act would trigger immediate military retaliation, global isolation, and the collapse of U.S. diplomatic credibility. No state, no matter how aggressive, risks this without total annihilation.&lt;/p&gt; &lt;h1&gt;3. U.S. seized a Russian-flagged oil tanker in the Atlantic&lt;/h1&gt; &lt;p&gt;U.S. Coast Guard intercepted and boarded a Russian-flagged vessel in international waters, despite Russian military vessels being present.&lt;br /&gt; ‚Üí &lt;strong&gt;Impossible because&lt;/strong&gt; seizing a vessel flagged by a nuclear-armed state in peacetime, especially with military presence, is not enforcement ‚Äî it is a declaration of war. Russia would respond. The global energy system would fracture. This is not policy. It is the end of the international order.&lt;/p&gt; &lt;h1&gt;4. ICE executed U.S. citizens who opposed them&lt;/h1&gt; &lt;p&gt;ICE agents shot and killed American citizens during raids, including a woman who reversed her car, and began systematically executing citizens who resisted.&lt;br /&gt; ‚Üí &lt;strong&gt;Impossible because&lt;/strong&gt; execution requires due process. The U.S. military, judiciary, and law enforcement are bound by the Constitution. Killing citizens without trial is not immigration enforcement ‚Äî it is domestic insurrection. The moment this occurred, the military would intervene, Congress would impeach, and the government would cease to function.&lt;/p&gt; &lt;h1&gt;5. Trump threatens to take Greenland by force&lt;/h1&gt; &lt;p&gt;Trump declared the U.S. would take control of Greenland ‚Äúwhether they like it or not,‚Äù and Denmark responded by calling it a betrayal of NATO.&lt;br /&gt; ‚Üí &lt;strong&gt;Impossible because&lt;/strong&gt; Greenland is part of a NATO ally. Invading an ally‚Äôs territory violates the core principle of the alliance. No U.S. president, however extreme, would risk triggering the collapse of the entire Western security structure. This is not diplomacy ‚Äî it is the suicide of American global leadership.&lt;/p&gt; &lt;h1&gt;‚úÖ Final Note:&lt;/h1&gt; &lt;p&gt;These are not exaggerated scenarios.&lt;br /&gt; They are &lt;strong&gt;narratives of collapse&lt;/strong&gt; ‚Äî events that require the total abandonment of law, alliance, institutional restraint, and human survival.&lt;br /&gt; They are not plausible.&lt;br /&gt; They are dystopian fiction.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Swimming_Cover_9686"&gt; /u/Swimming_Cover_9686 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qagaaq/qwen_cutoff_date_makes_our_current_reality_too/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qagaaq/qwen_cutoff_date_makes_our_current_reality_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qagaaq/qwen_cutoff_date_makes_our_current_reality_too/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T00:38:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qb2p26</id>
    <title>DXG Spark vs Ryzen AI 395 ‚Äî If the price difference is only $700, what would you choose?</title>
    <updated>2026-01-12T18:23:17+00:00</updated>
    <author>
      <name>/u/Affectionate-Bid-650</name>
      <uri>https://old.reddit.com/user/Affectionate-Bid-650</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I bought an HP Z2 Mini G1a today with a student discount. I paid $2,700 for the 128GB RAM / 2TB SSD configuration.&lt;/p&gt; &lt;p&gt;Honestly, it does sting a bit knowing that just a couple of months ago (maybe even one or two months) this same machine was going for around $1,600. But at the moment, this was the best deal I could realistically get.&lt;/p&gt; &lt;p&gt;Because of that, the price difference between this system and MSI‚Äôs DXG Spark kit ends up being only about $700.&lt;/p&gt; &lt;p&gt;That‚Äôs where I‚Äôm conflicted.&lt;/p&gt; &lt;p&gt;If the gap were $1,500 or more, I wouldn‚Äôt have hesitated and would have gone with the Ryzen AI 395 without much thought. But with only a $700 difference, I‚Äôm no longer sure.&lt;/p&gt; &lt;p&gt;For some context, I‚Äôm planning to use the machine purely for AI-related work. I only know very basic ‚Äúvibe coding,‚Äù and I‚Äôm still pretty new to AI in general. I‚Äôd say I‚Äôm just getting started.&lt;/p&gt; &lt;p&gt;Given the differences in development experience, tooling, and overall ease of use, which would you personally choose? The 395, or would you spend the extra $700 for the DXG Spark?&lt;/p&gt; &lt;p&gt;Curious to hear how others would approach this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Affectionate-Bid-650"&gt; /u/Affectionate-Bid-650 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qb2p26/dxg_spark_vs_ryzen_ai_395_if_the_price_difference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qb2p26/dxg_spark_vs_ryzen_ai_395_if_the_price_difference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qb2p26/dxg_spark_vs_ryzen_ai_395_if_the_price_difference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T18:23:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qb9gnn</id>
    <title>Run 96GB at 4800 MT/s or 64GB at 6000 for LLMs?</title>
    <updated>2026-01-12T22:28:44+00:00</updated>
    <author>
      <name>/u/-Sofa-King-</name>
      <uri>https://old.reddit.com/user/-Sofa-King-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;System specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MSI PRO B760-VC WIFI&lt;/li&gt; &lt;li&gt;i7-13700F&lt;/li&gt; &lt;li&gt;RTX 4060 Ti 16GB&lt;/li&gt; &lt;li&gt;RAM: &lt;ul&gt; &lt;li&gt;2√ó32GB Corsair DDR5-6000 CL30&lt;/li&gt; &lt;li&gt;2√ó16GB Kingston DDR5-5600 CL40&lt;/li&gt; &lt;li&gt;Total: 96 GB DDR5, mixed&lt;/li&gt; &lt;li&gt;Currently running at 4800 MT/s (JEDEC default due to 4 sticks)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm running local AI models and wondering if I should prioritize capacity or speed.&lt;/p&gt; &lt;p&gt;Active models I run:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-32B&lt;/li&gt; &lt;li&gt;DeepSeek 32B&lt;/li&gt; &lt;li&gt;Mixtral 8x7B&lt;/li&gt; &lt;li&gt;GPT-OSS-20B&lt;/li&gt; &lt;li&gt;Whisper.cpp for transcription&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Tools I use:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LM Studio&lt;/li&gt; &lt;li&gt;Jan (portable launcher)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Main questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Is it worth keeping all 4 sticks (96 GB) at 4800 MT/s for model size?&lt;/li&gt; &lt;li&gt;Or is it better to remove the 2√ó16GB Kingston and run 64 GB Corsair at 6000 CL30 for faster inference?&lt;/li&gt; &lt;li&gt;Would you shelf the 32 GB for backup in case of failure, or keep it active?&lt;/li&gt; &lt;li&gt;Are there other local models I should try that would benefit from the extra RAM?&lt;/li&gt; &lt;li&gt;Is there anything cleaner or more stable than Jan or LM Studio right now that isn‚Äôt Docker-based?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Goal is to run full 32B (or more if you think it can handle it) models with long contexts and at times if needed, review pdf's, images, etc. without crashing or slowing down.&lt;/p&gt; &lt;p&gt;Looking for real-world input from others doing local LLM work on consumer hardware as I am relatively new to this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Sofa-King-"&gt; /u/-Sofa-King- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qb9gnn/run_96gb_at_4800_mts_or_64gb_at_6000_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qb9gnn/run_96gb_at_4800_mts_or_64gb_at_6000_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qb9gnn/run_96gb_at_4800_mts_or_64gb_at_6000_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T22:28:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qajxrg</id>
    <title>Local LLM + Internet Search Capability = WOW</title>
    <updated>2026-01-12T03:21:41+00:00</updated>
    <author>
      <name>/u/alex_godspeed</name>
      <uri>https://old.reddit.com/user/alex_godspeed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Am on Qwen 3, asked about the training date and it said 2024. Alright, guess that's the thing I need to live with. Just need to constantly lookup HF for updated LLM which fits my cute 16gb vram.&lt;/p&gt; &lt;p&gt;Then someone said always ground your local AI with internet searches. A quick search = LM studio duckduckgo plugin&lt;/p&gt; &lt;p&gt;Within 15 minutes, prompt with &amp;quot;searching the web&amp;quot;, exactly the same interface I saw at ChatGPT!&lt;/p&gt; &lt;p&gt;Man, this local AI is getting better. Am I having 'agentic-AI' now? haha. I.e., tool calling is always something i heard of, but think that it's reserved for some CS-pro, not an average joe like me.&lt;/p&gt; &lt;p&gt;so now what, when was your 'wow-moment' for stuff like this, and what other things you design in your workflow to make locally run LLM so potent and, most importantly, private? =)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alex_godspeed"&gt; /u/alex_godspeed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qajxrg/local_llm_internet_search_capability_wow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qajxrg/local_llm_internet_search_capability_wow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qajxrg/local_llm_internet_search_capability_wow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T03:21:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qarygi</id>
    <title>Supertonic 2 TTS available on Hugging Face!</title>
    <updated>2026-01-12T10:59:17+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qarygi/supertonic_2_tts_available_on_hugging_face/"&gt; &lt;img alt="Supertonic 2 TTS available on Hugging Face!" src="https://external-preview.redd.it/bW0xNzkxMGdnd2NnMUnVCiAxQGnt1ilbiC_caBfY1exLsRF9vnxWGz-1gEiE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=70650d1f0c6d1e5bb6dba8fcbf7b8a77ef556355" title="Supertonic 2 TTS available on Hugging Face!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now in 5 languages (EN, KO, ES, PT, FR), generates 1 sec of audio in 0.006 sec. &lt;/p&gt; &lt;p&gt;demo: &lt;a href="https://huggingface.co/spaces/Supertone/supertonic-2"&gt;https://huggingface.co/spaces/Supertone/supertonic-2&lt;/a&gt;&lt;br /&gt; model: &lt;a href="https://huggingface.co/Supertone/supertonic-2"&gt;https://huggingface.co/Supertone/supertonic-2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cgp59yzfgwcg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qarygi/supertonic_2_tts_available_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qarygi/supertonic_2_tts_available_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T10:59:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qaawts</id>
    <title>LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)</title>
    <updated>2026-01-11T21:00:37+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/"&gt; &lt;img alt="LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)" src="https://b.thumbs.redditmedia.com/q81kZVHxvyOGNitVPQ2FQG3t9pi48omWEAQaQIJPo1k.jpg" title="LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I wanted to share an update on my open source project called TimeCapsuleLLM, I train language models from scratch using data from a single time period and location to reduce modern bias.&lt;/p&gt; &lt;p&gt;The newest model is trained only on texts published in London between 1800-1875. There is no fine tuning, no modern data, and for now no instruction or Q&amp;amp;A pairs so the model continues text from a prompt. This model is 1.2B parameters and uses a 90GB dataset consisting of books, journals, legal docs, religious writing, medical papers, etc. I also use a custom tokenizer, trained on the dataset itself and the model has been trained for 182k steps so far on a rented H100 SXM.&lt;/p&gt; &lt;p&gt;Example outputs:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l0oaulxrascg1.png?width=1478&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5292309afa4c4735471542b6cc794f6538b42486"&gt;Even though the prompt only mentions a specific year, the model generates an argument against the Roman Catholic Church. The dataset does contain large amounts of religious and political writing and the Catholic Emancipation Act took place in 1829 so this behavior makes sense.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tvem9mxrascg1.png?width=1484&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=347a6b3242b8ecb97a515196109eb63cc146bae0"&gt;The telephone was invented in 1876 (dataset cuts off at 1875), so the model is unfamiliar with the term, treating it as some kind of secret/diplomatic device or thing.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For next steps, I'm going to look into creating some kind of synthetic Q&amp;amp;A pairs using the dataset itself.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM"&gt;https://github.com/haykgrigo3/TimeCapsuleLLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM-v2-1800-1875"&gt;https://huggingface.co/haykgrigorian/TimeCapsuleLLM-v2-1800-1875&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-11T21:00:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbcctt</id>
    <title>I built MCP Hangar - a registry to manage multiple MCP servers without losing your mind</title>
    <updated>2026-01-13T00:24:43+00:00</updated>
    <author>
      <name>/u/pyrkamarcin</name>
      <uri>https://old.reddit.com/user/pyrkamarcin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running local LLMs with MCP tools and hit a wall: managing multiple MCP servers is a pain in the ass.&lt;/p&gt; &lt;p&gt;You want filesystem access? One server. Database queries? Another server. Web scraping? Third one. Now you're juggling processes, wondering which one crashed, manually restarting things, and your config files look like someone vomited JSON.&lt;/p&gt; &lt;p&gt;So I built &lt;strong&gt;MCP Hangar&lt;/strong&gt; - a production-grade registry that sits between your LLM client (LM Studio, Claude Desktop, whatever) and your MCP providers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Lazy loading&lt;/strong&gt; - providers start only when you actually invoke them, tools are visible immediately&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Health monitoring&lt;/strong&gt; - circuit breaker pattern with automatic recovery&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Container support&lt;/strong&gt; - Docker/Podman with auto-detection&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Auto-discovery&lt;/strong&gt; - drop a container with the right labels and it gets picked up&lt;/li&gt; &lt;li&gt;&lt;strong&gt;One endpoint&lt;/strong&gt; - your client talks to Hangar, Hangar routes to the right provider&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/mapyr/mcp-hangar"&gt;https://github.com/mapyr/mcp-hangar&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs: &lt;a href="https://mapyr.github.io/mcp-hangar/"&gt;https://mapyr.github.io/mcp-hangar/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MIT licensed, Python 3.10+. Looking for feedback and edge cases I haven't thought of.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pyrkamarcin"&gt; /u/pyrkamarcin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbcctt/i_built_mcp_hangar_a_registry_to_manage_multiple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbcctt/i_built_mcp_hangar_a_registry_to_manage_multiple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbcctt/i_built_mcp_hangar_a_registry_to_manage_multiple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T00:24:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qatpgb</id>
    <title>GLM-4.7 218B REAP model by Cerebras</title>
    <updated>2026-01-12T12:33:52+00:00</updated>
    <author>
      <name>/u/ResearchWheel5</name>
      <uri>https://old.reddit.com/user/ResearchWheel5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/cerebras/GLM-4.7-REAP-218B-A32B"&gt;https://huggingface.co/cerebras/GLM-4.7-REAP-218B-A32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious to see how the quantized versions will perform.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchWheel5"&gt; /u/ResearchWheel5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qatpgb/glm47_218b_reap_model_by_cerebras/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qatpgb/glm47_218b_reap_model_by_cerebras/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qatpgb/glm47_218b_reap_model_by_cerebras/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T12:33:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbh8xx</id>
    <title>Video 2 Bedtime Story - A journey of a dad over Xmas break.</title>
    <updated>2026-01-13T04:01:16+00:00</updated>
    <author>
      <name>/u/1beer2many</name>
      <uri>https://old.reddit.com/user/1beer2many</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, &lt;/p&gt; &lt;p&gt;I made this tool for my own needs but wanted to share this tool for everyone to use.&lt;br /&gt; My kid loves Hot Wheels and we bought some book called 5 minute stories for the hot wheels franchise. It was great until we ran out of stories and they didn't really make anymore. &lt;/p&gt; &lt;p&gt;I looked at the book and I was like, I think I can make this since it was essentially just a recap of the episode with screen shots. &lt;/p&gt; &lt;p&gt;Anyway, it turned out a LOT more complicated than I originally thought, but I hacked it out over the week with lots of credits. &lt;/p&gt; &lt;p&gt;Repo: &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/deepseekcoder2/vid2bedtimestory"&gt;https://github.com/deepseekcoder2/vid2bedtimestory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Example PDF output: &lt;/p&gt; &lt;p&gt;&lt;a href="https://dropvader.s3.amazonaws.com/uploads/c0e656ff-7dbc-4db7-8302-4fc738f9192b_202601130355/Episode1-01_tiny.pdf?AWSAccessKeyId=AKIAYLRQWXN2PGG26BPX&amp;amp;Signature=DiYSx5etjqEaf4wHm%2FQaBrHrRhk%3D&amp;amp;Expires=1768362959"&gt;https://dropvader.s3.amazonaws.com/uploads/c0e656ff-7dbc-4db7-8302-4fc738f9192b_202601130355/Episode1-01_tiny.pdf?AWSAccessKeyId=AKIAYLRQWXN2PGG26BPX&amp;amp;Signature=DiYSx5etjqEaf4wHm%2FQaBrHrRhk%3D&amp;amp;Expires=1768362959&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I threw it into google play books and read it to my kid and they loved it. &lt;/p&gt; &lt;p&gt;The screen shot selection was the most tricky part. It's still not 100% but I think its decent enough. Some screen shots repeat, but it was enough for my kid to still be engaged with the book. &lt;/p&gt; &lt;p&gt;Okay, I'm ready for you all to flame me and tell me what I did wrong. This is my first release and since I'm heavily dependent on local for a major step, I thought it would be relevant here. I'm using cloud for a lot of it, but it could easily be adapted for local. Just that it would take forever. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1beer2many"&gt; /u/1beer2many &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbh8xx/video_2_bedtime_story_a_journey_of_a_dad_over/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbh8xx/video_2_bedtime_story_a_journey_of_a_dad_over/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbh8xx/video_2_bedtime_story_a_journey_of_a_dad_over/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T04:01:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbcgju</id>
    <title>How I organize my local AI assistant including full home control, STT, TTS, RAG, coding to canvas (markdown, save), generating images, system ram /cpu monitor, and a dark mode ‚Ä¶ local, offline, based on free and open projects</title>
    <updated>2026-01-13T00:29:02+00:00</updated>
    <author>
      <name>/u/Fear_ltself</name>
      <uri>https://old.reddit.com/user/Fear_ltself</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbcgju/how_i_organize_my_local_ai_assistant_including/"&gt; &lt;img alt="How I organize my local AI assistant including full home control, STT, TTS, RAG, coding to canvas (markdown, save), generating images, system ram /cpu monitor, and a dark mode ‚Ä¶ local, offline, based on free and open projects" src="https://b.thumbs.redditmedia.com/rQODIgnaqUAn36WFqdHakKb6AbfFO36r_6pPQ8n5wrQ.jpg" title="How I organize my local AI assistant including full home control, STT, TTS, RAG, coding to canvas (markdown, save), generating images, system ram /cpu monitor, and a dark mode ‚Ä¶ local, offline, based on free and open projects" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been doing this a while, here‚Äôs just a rough layout of how I run my local AI. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fear_ltself"&gt; /u/Fear_ltself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qbcgju"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbcgju/how_i_organize_my_local_ai_assistant_including/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbcgju/how_i_organize_my_local_ai_assistant_including/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T00:29:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbgqhl</id>
    <title>Offloading Cold MoE Experts to Low-Cost GPUs (P40s)?</title>
    <updated>2026-01-13T03:37:13+00:00</updated>
    <author>
      <name>/u/coffee-on-thursday</name>
      <uri>https://old.reddit.com/user/coffee-on-thursday</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm running a dual-3090 system (NVLink) on a Threadripper platform, and I‚Äôm considering adding four additional GPUs. Instead of adding more 3090s, I‚Äôm looking at older high-VRAM cards such as Tesla P40s.&lt;/p&gt; &lt;p&gt;With recent MoE implementations supporting offloading of low-frequency experts to CPU memory, while keeping the main experts and KV-cache on the primary GPUs, I‚Äôm wondering whether those cold experts could instead be placed on cheaper GPUs. Is it technically feasible and performant to host MoE experts on lower-compute, PCIe-connected cards like P40s, rather than offloading them to CPU RAM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coffee-on-thursday"&gt; /u/coffee-on-thursday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbgqhl/offloading_cold_moe_experts_to_lowcost_gpus_p40s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbgqhl/offloading_cold_moe_experts_to_lowcost_gpus_p40s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbgqhl/offloading_cold_moe_experts_to_lowcost_gpus_p40s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T03:37:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbala2</id>
    <title>Last Week in Multimodal AI - Local Edition</title>
    <updated>2026-01-12T23:12:59+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbala2/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last Week in Multimodal AI - Local Edition" src="https://b.thumbs.redditmedia.com/u9qpCTdoJTLFZ_F-cRo8FcxyoixJ2DAwCSNQFQgZ_hg.jpg" title="Last Week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly multimodal AI roundup, here are the local/open-source highlights from last week:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LTX-2 - High-Quality Video Generation on Consumer Hardware&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 4K resolution, audio generation, and 10+ second clips with low VRAM requirements.&lt;/li&gt; &lt;li&gt;Runs on consumer GPUs without expensive cloud compute.&lt;/li&gt; &lt;li&gt;&lt;a href="https://blog.comfy.org/p/ltx-2-now-available-in-comfyui"&gt;Blog&lt;/a&gt; | &lt;a href="https://ltx.io/model"&gt;Model&lt;/a&gt; | &lt;a href="https://github.com/Lightricks/LTX-2"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qbala2/video/w3zh1bkhvzcg1/player"&gt;https://reddit.com/link/1qbala2/video/w3zh1bkhvzcg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Music Flamingo - Open Audio-Language Model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fully open SOTA model that understands full-length songs and reasons about music theory.&lt;/li&gt; &lt;li&gt;Goes beyond tagging to analyze harmony, structure, and cultural context.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nvidia/music-flamingo-2601-hf"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://research.nvidia.com/labs/adlr/MF/"&gt;Project Page&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2511.10289"&gt;Paper&lt;/a&gt; | &lt;a href="https://musicflamingo-nv-umd.github.io/#model-output"&gt;Demo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lkj3z7zjvzcg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c384888a44d78bdaf53f9e54907af40d0b98bd3"&gt;https://preview.redd.it/lkj3z7zjvzcg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c384888a44d78bdaf53f9e54907af40d0b98bd3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-VL-Embedding &amp;amp; Reranker - Multimodal Retrieval&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Maps text, images, and video into unified embedding space across 30+ languages.&lt;/li&gt; &lt;li&gt;State-of-the-art performance for local multimodal search systems.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-Embedding-2B"&gt;Hugging Face (Embedding)&lt;/a&gt; | &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-Reranker-8B"&gt;Hugging Face (Reranker)&lt;/a&gt; | &lt;a href="https://qwen.ai/blog?id=qwen3-vl-embedding"&gt;Blog&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lhnb3aqmvzcg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=624f43cb667ec5463386bf0a8ec1cbdbcdd3734a"&gt;https://preview.redd.it/lhnb3aqmvzcg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=624f43cb667ec5463386bf0a8ec1cbdbcdd3734a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;e5-omni - Omni-Modal Embeddings&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Handles text, image, audio, and video in single unified model.&lt;/li&gt; &lt;li&gt;Solves modality gap issues for stable all-content-type embeddings.&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2601.03666"&gt;Paper&lt;/a&gt; | &lt;a href="https://huggingface.co/Haon-Chen/e5-omni-7B"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;UniVideo - Unified Video Framework&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open-source model combining video generation, editing, and understanding.&lt;/li&gt; &lt;li&gt;Generate from text/images and edit with natural language commands.&lt;/li&gt; &lt;li&gt;&lt;a href="https://congwei1230.github.io/UniVideo/"&gt;Project Page&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2510.08377"&gt;Paper&lt;/a&gt; | &lt;a href="https://huggingface.co/KlingTeam/UniVideo"&gt;Model&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qbala2/video/tro76yurvzcg1/player"&gt;https://reddit.com/link/1qbala2/video/tro76yurvzcg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Checkout the &lt;a href="https://thelivingedge.substack.com/p/last-week-in-multimodal-ai-40-search"&gt;full roundup&lt;/a&gt; for more demos, papers, and resources.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbala2/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbala2/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbala2/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T23:12:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qawbjj</id>
    <title>z.ai prepping for glm-image soon - here is what we know so far</title>
    <updated>2026-01-12T14:28:22+00:00</updated>
    <author>
      <name>/u/MrAlienOverLord</name>
      <uri>https://old.reddit.com/user/MrAlienOverLord</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM-Image supports both text-to-image and image-to-image generation within a single model &lt;/p&gt; &lt;p&gt;Text-to-image: generates high-detail images from textual descriptions, with particularly strong performance in information-dense scenarios. &lt;/p&gt; &lt;p&gt;Image-to-image: supports a wide range of tasks, including image editing, style transfer, multi-subject consistency, and identity-preserving generation for people and objects. &lt;/p&gt; &lt;p&gt;arch:&lt;/p&gt; &lt;p&gt;Autoregressive generator: a 9B-parameter model initialized from [GLM-4-9B-0414](&lt;a href="https://huggingface.co/zai-org/GLM-4-9B-0414"&gt;https://huggingface.co/zai-org/GLM-4-9B-0414&lt;/a&gt;), with an expanded vocabulary to incorporate visual tokens. The model first generates a compact encoding of approximately 256 tokens, then expands to 1K‚Äì4K tokens, corresponding to 1K‚Äì2K high-resolution image outputs.&lt;/p&gt; &lt;p&gt;Diffusion Decoder: a 7B-parameter decoder based on a single-stream DiT architecture for latent-space&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/huggingface/diffusers/pull/12921"&gt;https://github.com/huggingface/diffusers/pull/12921&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/huggingface/transformers/pull/43100"&gt;https://github.com/huggingface/transformers/pull/43100&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrAlienOverLord"&gt; /u/MrAlienOverLord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qawbjj/zai_prepping_for_glmimage_soon_here_is_what_we/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qawbjj/zai_prepping_for_glmimage_soon_here_is_what_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qawbjj/zai_prepping_for_glmimage_soon_here_is_what_we/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T14:28:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbdjwl</id>
    <title>Looking at setting up a shared ComfyUI server on a workplace LAN for multi-user user. I know it's not LLM related specifically, but this sub is far more technical-minded than the StableDiffusion one, plus I see more stacks of RTX Pro 6000s here than anywhere else!</title>
    <updated>2026-01-13T01:15:46+00:00</updated>
    <author>
      <name>/u/Generic_Name_Here</name>
      <uri>https://old.reddit.com/user/Generic_Name_Here</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;** for multi-user use. Oops.&lt;/p&gt; &lt;p&gt;I'm doing some back of the napkin math on setting up a centralized ComfyUI server for ~3-5 people to be working on at any one time. This list will eventually go a systems/hardware guy, but I need to provide some recommendations and gameplan that makes sense and I'm curious if anyone else is running a similar setup shared by a small amount of users.&lt;/p&gt; &lt;p&gt;At home I'm running 1x RTX Pro 6000 and 1x RTX 5090 with an Intel 285k and 192GB of RAM. I'm finding that this puts a bit of a strain on my 1600W power supply and will definitely max out my RAM when it comes to running Flux2 or large WAN generations on both cards at the same time.&lt;/p&gt; &lt;p&gt;For this reason I'm considering the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ThreadRipper PRO 9955WX (don't need CPU speed, just RAM support and PCIe lanes)&lt;/li&gt; &lt;li&gt;256-384 GB RAM&lt;/li&gt; &lt;li&gt;3-4x RTX Pro 6000 Max-Q&lt;/li&gt; &lt;li&gt;8TB NVMe SSD for models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love to go with a Silverstone HELA 2500W PSU for more juice, but then this will require 240V for everything upstream (UPS, etc.). Curious of your experiences or recommendations here - worth the 240V UPS? Dual PSU? etc.&lt;/p&gt; &lt;p&gt;For access, I'd stick each each GPU on a separate port (:8188, :8189, :8190, etc) and users can find an open session. Perhaps one day I can find the time to build a farm / queue distribution system.&lt;/p&gt; &lt;p&gt;This seems massively cheaper than any server options I can find, but obviously going with a 4U rackmount would present some better power options and more expandability, plus even the opportunity to go with 4X Pro 6000's to start. But again I'm starting to find system RAM to be a limiting factor with multi-GPU setups.&lt;/p&gt; &lt;p&gt;So if you've set up something similar, I'm curious of your mistakes and recommendations, both in terms of hardware and in terms of user management, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Generic_Name_Here"&gt; /u/Generic_Name_Here &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbdjwl/looking_at_setting_up_a_shared_comfyui_server_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbdjwl/looking_at_setting_up_a_shared_comfyui_server_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbdjwl/looking_at_setting_up_a_shared_comfyui_server_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T01:15:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbc5s5</id>
    <title>Building Opensource client sided Code Intelligence Engine -- Potentially deeper than Deep wiki :-) ( Need suggestions and feedback )</title>
    <updated>2026-01-13T00:16:46+00:00</updated>
    <author>
      <name>/u/DeathShot7777</name>
      <uri>https://old.reddit.com/user/DeathShot7777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbc5s5/building_opensource_client_sided_code/"&gt; &lt;img alt="Building Opensource client sided Code Intelligence Engine -- Potentially deeper than Deep wiki :-) ( Need suggestions and feedback )" src="https://external-preview.redd.it/MnIyZmw3ODdkMGRnMege6VYazrCNvPvrU2GG8tcd-8T7OQo9iRCGUYxRaIOc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e84f149b907a0cefc7077c8391844ece6af1b00" title="Building Opensource client sided Code Intelligence Engine -- Potentially deeper than Deep wiki :-) ( Need suggestions and feedback )" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, guys, I m building GitNexus, an opensource Code Intelligence Engine which works fully client sided in-browser. Think of DeepWiki but with understanding of codebase relations like IMPORTS - CALLS -DEFINES -IMPLEMENTS- EXTENDS relations. &lt;/p&gt; &lt;p&gt;What all features would be useful, any integrations, cool ideas, etc?&lt;/p&gt; &lt;p&gt;site: &lt;a href="https://gitnexus.vercel.app/"&gt;https://gitnexus.vercel.app/&lt;/a&gt;&lt;br /&gt; repo: &lt;a href="https://github.com/abhigyanpatwari/GitNexus"&gt;https://github.com/abhigyanpatwari/GitNexus&lt;/a&gt; (A ‚≠ê might help me convince my CTO to allot little time for this :-) )&lt;/p&gt; &lt;p&gt;Everything including the DB engine, embeddings model etc works inside your browser. &lt;/p&gt; &lt;p&gt;It combines Graph query capabilities with standard code context tools like semantic search, BM 25 index, etc. Due to graph it should be able to perform Blast radius detection of code changes, codebase audit etc reliably. &lt;/p&gt; &lt;p&gt;Working on exposing the browser tab through MCP so claude code / cursor, etc can use it for codebase audits, deep context of code connections etc preventing it from making breaking changes due to missed dependent functions.&lt;/p&gt; &lt;p&gt;Posted an earlier version of Gitnexus here, there has been a lot of improvement since then.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathShot7777"&gt; /u/DeathShot7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rmnzno77d0dg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbc5s5/building_opensource_client_sided_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbc5s5/building_opensource_client_sided_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T00:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbei13</id>
    <title>Tool output compression for agents - 60-70% token reduction on tool-heavy workloads (open source, works with local models)</title>
    <updated>2026-01-13T01:57:59+00:00</updated>
    <author>
      <name>/u/decentralizedbee</name>
      <uri>https://old.reddit.com/user/decentralizedbee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Disclaimer: for those who are very anti-ads - yes this is a tool we built. Yes we built it due to a problem we have. Yes we are open-sourcing it and it's 100% free.&lt;/p&gt; &lt;p&gt;We build agents for clients. Coding assistants, data analysis tools, that kind of thing. A few months ago we noticed something that felt dumb in retrospect: the biggest cost driver wasn't the model itself - it was context size. And most of that context was tool outputs.&lt;/p&gt; &lt;p&gt;Think about what happens when an agent searches a codebase. Grep returns 500 file matches. The agent stuffs all 500 into context and asks the model &amp;quot;which of these are relevant?&amp;quot; You're paying for 500 items worth of tokens so the model can pick out maybe 5. The model is basically acting as a JSON filter at that point.&lt;/p&gt; &lt;p&gt;Same pattern everywhere. Search results, database queries, API responses. Tools return way more than the model actually needs, but agents just shove it all into the prompt because that's the path of least resistance.&lt;/p&gt; &lt;p&gt;So we started hacking on a compression layer. The idea was simple: before tool outputs hit the model, analyze them statistically and keep only what matters.&lt;/p&gt; &lt;p&gt;What we keep:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Anything with error keywords. Errors are never dropped, that would be insane.&lt;/li&gt; &lt;li&gt;Statistical outliers. If a numeric field has values more than 2 standard deviations from the mean, those items survive.&lt;/li&gt; &lt;li&gt;Items that match the user's query. We run BM25 scoring against the actual question being asked.&lt;/li&gt; &lt;li&gt;Top N by score if there's a relevance or score field in the data.&lt;/li&gt; &lt;li&gt;First few and last few items for context and recency.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What we drop:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The repetitive middle. If you have 500 search results and 480 of them look basically the same, you don't need all 480.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The tricky part wasn't the compression itself. It was knowing when NOT to compress. If you're searching a database for a specific user ID and every row is unique with no ranking signal, compression would lose entities. So we do a crushability analysis first. High uniqueness plus no importance signal means we skip compression entirely and pass through the original data.&lt;/p&gt; &lt;p&gt;On our workloads we're seeing 60-90% token reduction depending on the scenario. Code search with hundreds of file matches compresses aggressively. Log analysis with lots of repetitive entries compresses well. Database results with unique rows usually don't compress much, which is correct behavior.&lt;/p&gt; &lt;p&gt;Latency overhead is 1-5ms. The compression is fast, the model is still the bottleneck by a huge margin.&lt;/p&gt; &lt;p&gt;We open sourced it. It's called Headroom.&lt;/p&gt; &lt;p&gt;Two ways to run it. There's a proxy server you can point any OpenAI-compatible client at, or a Python SDK wrapper if you want more control. Works with OpenAI, Anthropic, Google, and local models through LiteLLM. If you're running llama.cpp with an OpenAI-compatible server, you can just point the proxy at that and it works.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/chopratejas/headroom"&gt;https://github.com/chopratejas/headroom&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The compression is also reversible. We cache original content with a TTL and inject a retrieval marker into the compressed output. If the model needs data that was compressed away, it can request it back. Haven't needed this much in practice but it's a nice safety net.&lt;/p&gt; &lt;p&gt;Curious what others are doing for context management. Most agent frameworks seem to just truncate blindly which always felt wrong to us. You're either losing information randomly or you're paying for tokens you don't need. There should be a middle ground.&lt;/p&gt; &lt;p&gt;Would also love any feedback to this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/decentralizedbee"&gt; /u/decentralizedbee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbei13/tool_output_compression_for_agents_6070_token/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbei13/tool_output_compression_for_agents_6070_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbei13/tool_output_compression_for_agents_6070_token/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T01:57:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qb0vv8</id>
    <title>Cerebras GLM4.7 REAPs @ 25%, 40% live on HF</title>
    <updated>2026-01-12T17:17:54+00:00</updated>
    <author>
      <name>/u/ilzrvch</name>
      <uri>https://old.reddit.com/user/ilzrvch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;We're kicking off the new year starting to release the highly requested REAP variants of recent models (GLM4.7, MiniMax-2.1, etc.). Today we're starting off with GLM4.7:&lt;/p&gt; &lt;p&gt;25% pruned FP8: &lt;a href="https://hf.co/cerebras/GLM-4.7-REAP-268B-A32B-FP8"&gt;https://hf.co/cerebras/GLM-4.7-REAP-268B-A32B-FP8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;25% pruned BF16: &lt;em&gt;TBD&lt;/em&gt;&lt;/p&gt; &lt;p&gt;40% pruned FP8: &lt;a href="https://hf.co/cerebras/GLM-4.7-REAP-218B-A32B-FP8"&gt;https://hf.co/cerebras/GLM-4.7-REAP-218B-A32B-FP8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;40% pruned BF16: &lt;a href="https://hf.co/cerebras/GLM-4.7-REAP-218B-A32B"&gt;https://hf.co/cerebras/GLM-4.7-REAP-218B-A32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our initial tests on the EvalPlus benchmark show pretty good accuracy retention, we'll be adding more benchmark results so stay tuned!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilzrvch"&gt; /u/ilzrvch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qb0vv8/cerebras_glm47_reaps_25_40_live_on_hf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qb0vv8/cerebras_glm47_reaps_25_40_live_on_hf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qb0vv8/cerebras_glm47_reaps_25_40_live_on_hf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T17:17:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qautxm</id>
    <title>[Release] Eva-4B: Specialized Financial Evasion Detection (Based on Qwen3-4B). Outperforms GPT-5.2 on domain benchmarks.</title>
    <updated>2026-01-12T13:26:39+00:00</updated>
    <author>
      <name>/u/Awkward_Run_9982</name>
      <uri>https://old.reddit.com/user/Awkward_Run_9982</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qautxm/release_eva4b_specialized_financial_evasion/"&gt; &lt;img alt="[Release] Eva-4B: Specialized Financial Evasion Detection (Based on Qwen3-4B). Outperforms GPT-5.2 on domain benchmarks." src="https://preview.redd.it/bawipxvf6xcg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b7154c655fd6869091d5814b034f797a1f34845" title="[Release] Eva-4B: Specialized Financial Evasion Detection (Based on Qwen3-4B). Outperforms GPT-5.2 on domain benchmarks." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I'm excited to share &lt;strong&gt;Eva-4B,&lt;/strong&gt; a specialized 4B parameter model designed to detect evasive answers in corporate earnings call Q&amp;amp;A sessions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;br /&gt; It classifies answers into `direct`, `intermediate`, or `fully_evasive` (using the Rasiah framework). It helps identify when executives are sidestepping analysts' questions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why use this over a general LLM?&lt;/strong&gt;&lt;br /&gt; * &lt;strong&gt;Performance:&lt;/strong&gt; On our 1,000-sample human-annotated test set, Eva-4B achieves &lt;strong&gt;81.3% accuracy&lt;/strong&gt;, beating GPT-5.2 (80.5%) and coming close to GLM-4.7 and Gemini-3-Flash.&lt;br /&gt; * &lt;strong&gt;Efficiency:&lt;/strong&gt; It's a 4B model (Qwen3 base), making it extremely cheap to run locally or in production pipelines compared to querying Opus or GPT-5.&lt;br /&gt; * &lt;strong&gt;Data:&lt;/strong&gt; Fine-tuned on 30k samples constructed via a multi-model consensus (Claude Opus + Gemini) + LLM-as-Judge pipeline.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;br /&gt; * Hugging Face: &lt;a href="https://huggingface.co/FutureMa/Eva-4B"&gt;https://huggingface.co/FutureMa/Eva-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* Hugging Space: &lt;a href="https://huggingface.co/spaces/FutureMa/financial-evasion-detection"&gt;https://huggingface.co/spaces/FutureMa/financial-evasion-detection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love to hear your feedback or see how it performs on your own financial text samples!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Awkward_Run_9982"&gt; /u/Awkward_Run_9982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bawipxvf6xcg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qautxm/release_eva4b_specialized_financial_evasion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qautxm/release_eva4b_specialized_financial_evasion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T13:26:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qb47fn</id>
    <title>Unsloth's GGUFs for GLM 4.7 REAP are up.</title>
    <updated>2026-01-12T19:15:08+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qb47fn/unsloths_ggufs_for_glm_47_reap_are_up/"&gt; &lt;img alt="Unsloth's GGUFs for GLM 4.7 REAP are up." src="https://external-preview.redd.it/_K5KJ1U4NAv0qO7ekkQCUeqpCvaUcBCkDVL7JrcRGaU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce183f1754e4ead5ff49938f48c097acb4a0cf1d" title="Unsloth's GGUFs for GLM 4.7 REAP are up." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.7-REAP-218B-A32B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qb47fn/unsloths_ggufs_for_glm_47_reap_are_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qb47fn/unsloths_ggufs_for_glm_47_reap_are_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T19:15:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qb1w7a</id>
    <title>How do people even afford these expensive graphic cards...?...</title>
    <updated>2026-01-12T17:53:33+00:00</updated>
    <author>
      <name>/u/boisheep</name>
      <uri>https://old.reddit.com/user/boisheep</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I bought some used computer with a RTX 3090 so I could learn ML/LLM and I am already running slow, when running pytorch processes from scratch, it's good, but anything Diffusion/LLM explodes my rig.&lt;/p&gt; &lt;p&gt;Then I'd ponder about these larger cards, and they are like 10k.&lt;/p&gt; &lt;p&gt;Benefit of a larger card is that diffusion models just do not seem to go well with dual, they can split processes of each step but there is no true speed gain on the processing itself; as for Llama it can be done in dual with llama.ccp for example.&lt;/p&gt; &lt;p&gt;Another used 3090 would be 700 + new power supply, and I don't even know if I need another motherboard with these lanes be running at 8x; but then I get no benefit for diffusion processes that need to load in a single card (esp if using comfy).&lt;/p&gt; &lt;p&gt;My current objective is to make a game engine, and that means I've been coding internals; and I am frustrated that it seems I am making the RPG engine with most graphic cards requirement ever when it's just for visual novel; characters have their own coding, actual code, beyond text prompts; and the more characters in a location, the more inferences because they also need to use reasoning, and very complex reasoning; I've been optimizing hard, 70B quantized bare minimum, and my 3090 is catching smoke.&lt;/p&gt; &lt;p&gt;It's impressive how much better memory and awareness they gain by having an inner monologe and fake simulated feelings; but boy it is slow, and while at 1 to 1 with inner monologe off it seems usable, it gets slow and I have no parallelism. Meanwhile I read people here talking about GPUs that cost as much as a summer cottage.&lt;/p&gt; &lt;p&gt;Is there a hidden stash of cards or secret or people really put 10k into a freaking graphics card?... how does that make financial sense?...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boisheep"&gt; /u/boisheep &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qb1w7a/how_do_people_even_afford_these_expensive_graphic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qb1w7a/how_do_people_even_afford_these_expensive_graphic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qb1w7a/how_do_people_even_afford_these_expensive_graphic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T17:53:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbgdu2</id>
    <title>OSS Alternative to Glean</title>
    <updated>2026-01-13T03:21:07+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbgdu2/oss_alternative_to_glean/"&gt; &lt;img alt="OSS Alternative to Glean" src="https://external-preview.redd.it/cmU5Y2xuYnFiMWRnMWWIQZ2CyIf_Xrmm-Z03F9XkK4MxpC4ND6bEYAzhiTDs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e2103787259340396f1676074d239180e73672b" title="OSS Alternative to Glean" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be OSS alternative to NotebookLM, Perplexity, and Glean.&lt;/p&gt; &lt;p&gt;In short, Connect any LLM to your internal knowledge sources (Search Engines, Drive, Calendar, Notion and 15+ other connectors) and chat with it in real time alongside your team.&lt;/p&gt; &lt;p&gt;I'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here's a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep Agentic Agent&lt;/li&gt; &lt;li&gt;RBAC (Role Based Access for Teams)&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Local TTS/STT support.&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi Collaborative Chats&lt;/li&gt; &lt;li&gt;Multi Collaborative Documents&lt;/li&gt; &lt;li&gt;Real Time Features&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Quick Start (without oauth connectors)&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Linux/macOS:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 \ -v surfsense-data:/data \ --name surfsense \ --restart unless-stopped \ ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Windows (PowerShell):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 ` -v surfsense-data:/data ` --name surfsense ` --restart unless-stopped ` ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y63zrbbqb1dg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbgdu2/oss_alternative_to_glean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbgdu2/oss_alternative_to_glean/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T03:21:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qaz4je</id>
    <title>We fine-tuned a 4B Text2SQL model that matches a 685B teacher - query your CSV data in plain English, locally</title>
    <updated>2026-01-12T16:14:57+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qaz4je/we_finetuned_a_4b_text2sql_model_that_matches_a/"&gt; &lt;img alt="We fine-tuned a 4B Text2SQL model that matches a 685B teacher - query your CSV data in plain English, locally" src="https://preview.redd.it/ed9sra1z0ycg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6721c4f7e645b322ae0b855c876d7721c4305e23" title="We fine-tuned a 4B Text2SQL model that matches a 685B teacher - query your CSV data in plain English, locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have been exploring how far you can push small models on narrow, well-defined tasks and decided to focus on &lt;strong&gt;Text2SQL&lt;/strong&gt;. We fine-tuned a small language model (&lt;strong&gt;4B parameters&lt;/strong&gt;) to convert plain English questions into executable SQL queries with accuracy matching a &lt;strong&gt;685B LLM (DeepSeek-V3)&lt;/strong&gt;. Because it's small, you can run it locally on your own machine, no API keys, no cloud dependencies. You can find more information on the &lt;a href="https://github.com/distil-labs/distil-text2sql"&gt;GitHub page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Just type: &lt;em&gt;&amp;quot;How many employees earn more than 50000?&amp;quot;&lt;/em&gt; ‚Üí you get: &lt;code&gt;*SELECT COUNT(*) FROM employees WHERE salary &amp;gt; 50000;*&lt;/code&gt;&lt;/p&gt; &lt;h2&gt;How We Trained Text2SQL&lt;/h2&gt; &lt;p&gt;Asking questions about data shouldn't require knowing SQL. We wanted a local assistant that keeps your data private while matching cloud LLM quality. Small models are perfect for &lt;strong&gt;structured generation tasks&lt;/strong&gt; like SQL, so this became our next testbed after &lt;a href="https://github.com/distil-labs/distil-gitara"&gt;Gitara&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Our goals:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Runs locally&lt;/strong&gt; (Ollama/llamacpp/transformers serve) - your data never leaves your machine&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fast responses&lt;/strong&gt; (&amp;lt;2 seconds on a laptop)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Match the accuracy of a 685B model&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Examples&lt;/h3&gt; &lt;p&gt;``` &amp;quot;How many employees are in each department?&amp;quot; ‚Üí SELECT department, COUNT(*) FROM employees GROUP BY department;&lt;/p&gt; &lt;p&gt;&amp;quot;What is the average salary by department?&amp;quot; ‚Üí SELECT department, AVG(salary) FROM employees GROUP BY department;&lt;/p&gt; &lt;p&gt;&amp;quot;Who are the top 3 highest paid employees?&amp;quot; ‚Üí SELECT name, salary FROM employees ORDER BY salary DESC LIMIT 3;&lt;/p&gt; &lt;p&gt;&amp;quot;Show total project budget per employee&amp;quot; (with JOINs) ‚Üí SELECT e.name, SUM(p.budget) FROM employees e JOIN projects p ON e.id = p.lead_id GROUP BY e.name;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;h3&gt;Results&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Params&lt;/th&gt; &lt;th&gt;LLM-as-a-Judge&lt;/th&gt; &lt;th&gt;Exact Match&lt;/th&gt; &lt;th&gt;Model link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;DeepSeek-V3 (teacher)&lt;/td&gt; &lt;td&gt;685B&lt;/td&gt; &lt;td&gt;80%&lt;/td&gt; &lt;td&gt;48%&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Qwen3-4B (fine-tuned)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;4B&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;80%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;60%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/collections/distil-labs/distil-qwen3-4b-text2sql"&gt;huggingface&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-4B (base)&lt;/td&gt; &lt;td&gt;4B&lt;/td&gt; &lt;td&gt;62%&lt;/td&gt; &lt;td&gt;16%&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Our fine-tuned &lt;strong&gt;4B model matches the 685B teacher&lt;/strong&gt; on semantic accuracy and actually &lt;strong&gt;exceeds it on exact match&lt;/strong&gt;. The quantized version also responds &lt;strong&gt;&amp;lt;2 seconds&lt;/strong&gt; on an M4 MacBook Pro.&lt;/p&gt; &lt;p&gt;The wrapper script in the &lt;a href="https://github.com/distil-labs/distil-text2sql"&gt;GitHub page&lt;/a&gt; loads your CSV files, generates SQL, &lt;strong&gt;executes it&lt;/strong&gt;, and returns the results.&lt;/p&gt; &lt;h3&gt;Training Pipeline&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;1. Seed Data:&lt;/strong&gt; We wrote ~50 examples covering simple queries, JOINs, aggregations, and subqueries. Available in &lt;code&gt;finetuning/data/&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Synthetic Expansion:&lt;/strong&gt; Using our &lt;a href="https://www.distillabs.ai/blog/small-expert-agents-from-10-examples/?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=text2sql"&gt;data synthesis pipeline&lt;/a&gt;, we expanded to &lt;strong&gt;~10,000 training examples&lt;/strong&gt; with diverse schemas across e-commerce, HR, healthcare, and other domains.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Fine-tuning:&lt;/strong&gt; We chose Qwen3-4B based on our &lt;a href="https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning/?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=text2sql"&gt;benchmarking of 12 small language models&lt;/a&gt;, which showed it offers the best balance of capability and efficiency for fine-tuning. Training config: 4 epochs, full fine-tuning on ~10k examples.&lt;/p&gt; &lt;h3&gt;Qualitative Examples&lt;/h3&gt; &lt;p&gt;We compare the base QWEN3-4B with the fine tuned version on a few cherry-picked examples to showcase the difference&lt;/p&gt; &lt;h3&gt;Example 1: Missing Aggregation Function&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Schema:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;```sql CREATE TABLE employees ( id INTEGER PRIMARY KEY, name TEXT NOT NULL, team TEXT, base_salary INTEGER, bonus INTEGER );&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What is the total compensation (salary + bonus) per team?&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Prediction&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Reference&lt;/td&gt; &lt;td&gt;&lt;code&gt;SELECT team, SUM(base_salary + bonus) FROM employees GROUP BY team;&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Base qwen3-4b&lt;/td&gt; &lt;td&gt;&lt;code&gt;SELECT team, (base_salary + bonus) AS total_compensation FROM employees GROUP BY team;&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Tuned qwen3-4b&lt;/td&gt; &lt;td&gt;&lt;code&gt;SELECT team, SUM(base_salary + bonus) FROM employees GROUP BY team;&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Analysis:&lt;/strong&gt; The base model omitted the &lt;code&gt;SUM()&lt;/code&gt; aggregate function, returning only an arbitrary row's compensation per team rather than the total. The tuned model correctly applies the aggregation.&lt;/p&gt; &lt;h3&gt;Example 2: Syntax Error in CASE Expression&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Schema:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;```sql CREATE TABLE tasks ( id INTEGER PRIMARY KEY, project_id INTEGER, title TEXT, status TEXT, assigned_to INTEGER );&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What percentage of tasks are completed?&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Prediction&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Reference&lt;/td&gt; &lt;td&gt;&lt;code&gt;SELECT (COUNT(CASE WHEN status = 'completed' THEN 1 END) * 100.0 / COUNT(*)) FROM tasks;&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Base qwen3-4b&lt;/td&gt; &lt;td&gt;&lt;code&gt;SELECT (COUNT(CASE WHEN status = 'completed' THEN 1 END. * 100.0) / COUNT(*)) AS percentage_completed FROM tasks;&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Tuned qwen3-4b&lt;/td&gt; &lt;td&gt;&lt;code&gt;SELECT (COUNT(CASE WHEN status = 'completed' THEN 1 END) * 100.0 / COUNT(*)) FROM tasks;&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Analysis:&lt;/strong&gt; The base model produced invalid SQL with a syntax error (&lt;code&gt;END.&lt;/code&gt; instead of &lt;code&gt;END&lt;/code&gt;), causing query execution to fail. The tuned model generates syntactically correct SQL matching the reference.&lt;/p&gt; &lt;h2&gt;Want to try it?&lt;/h2&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/distil-labs/distil-text2sql"&gt;https://github.com/distil-labs/distil-text2sql&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quick start (Ollama):&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;Download model (~2.5GB quantized)&lt;/h1&gt; &lt;p&gt;huggingface-cli download distil-labs/distil-qwen3-4b-text2sql-gguf-4bit --local-dir distil-model cd distil-model ollama create distil-qwen3-4b-text2sql -f Modelfile cd ..&lt;/p&gt; &lt;h1&gt;Query your data&lt;/h1&gt; &lt;p&gt;python app.py --csv your_data.csv --question &amp;quot;How many rows have status = active?&amp;quot;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;h2&gt;Discussion&lt;/h2&gt; &lt;p&gt;Curious to hear from the community:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How are you querying local data today? SQL? Pandas? Something else?&lt;/li&gt; &lt;li&gt;Anyone else fine-tuning small models for structured output tasks?&lt;/li&gt; &lt;li&gt;What other &amp;quot;narrow but useful&amp;quot; tasks would benefit from a local SLM?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let us know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ed9sra1z0ycg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qaz4je/we_finetuned_a_4b_text2sql_model_that_matches_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qaz4je/we_finetuned_a_4b_text2sql_model_that_matches_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T16:14:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qb034t</id>
    <title>GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models</title>
    <updated>2026-01-12T16:49:22+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/deepseek-ai/Engram/tree/main"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T16:49:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
