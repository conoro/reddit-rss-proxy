<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-21T14:08:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oc6yqt</id>
    <title>How can I browse my own GGUF file in GPT4ALL and LMStudio</title>
    <updated>2025-10-21T07:57:56+00:00</updated>
    <author>
      <name>/u/FatFigFresh</name>
      <uri>https://old.reddit.com/user/FatFigFresh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These two apps demand you download the model from them, while i already have all models downloaded. I see some online posts that say you gotta copy your files to a specific folder for them to see, but I don‚Äôt want to do that. All my library for models has its own place and I can‚Äôt copy them all for sake of these apps. Is there any workaround?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FatFigFresh"&gt; /u/FatFigFresh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc6yqt/how_can_i_browse_my_own_gguf_file_in_gpt4all_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc6yqt/how_can_i_browse_my_own_gguf_file_in_gpt4all_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc6yqt/how_can_i_browse_my_own_gguf_file_in_gpt4all_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T07:57:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocdoc3</id>
    <title>Running on surface laptop 7</title>
    <updated>2025-10-21T13:56:54+00:00</updated>
    <author>
      <name>/u/Daveddus</name>
      <uri>https://old.reddit.com/user/Daveddus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, i have a surface laptop 7 that has a Snapdragon X Elite 12 core/16GB and 128MB GPU 1TB HDD.&lt;/p&gt; &lt;p&gt;Im needing to do some pretty straight forward text analysis on a few thousand records, extract and infer specific data.&lt;/p&gt; &lt;p&gt;Am I wishful thinking that I can run something locally? Im not worried too much about speed. Would be happy for it to run over night.&lt;/p&gt; &lt;p&gt;Any help, advice, recommendations would be great appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daveddus"&gt; /u/Daveddus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocdoc3/running_on_surface_laptop_7/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocdoc3/running_on_surface_laptop_7/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocdoc3/running_on_surface_laptop_7/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T13:56:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc1j9i</id>
    <title>dual radeon r9700 benchmarks</title>
    <updated>2025-10-21T02:45:40+00:00</updated>
    <author>
      <name>/u/luminarian721</name>
      <uri>https://old.reddit.com/user/luminarian721</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;code&gt;Just got my 2 radeon pro r9700 32gb cards delivered a couple of days ago.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;I can't seem to get anything other then gibberish with rocm 7.0.2 when using both cards no matter how i configured them or what i turn on or off in the cmake.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;So the benchmarks are single card only, and these cards are stuck on my e5-2697a v4 box until next year. so only pcie 3.0 ftm.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Any benchmark requests?&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gpt-oss 20B F16 | 12.83 GiB | 20.91 B | ROCm | 999 | ROCm1 | pp512 | 404.28 ¬± 1.07 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gpt-oss 20B F16 | 12.83 GiB | 20.91 B | ROCm | 999 | ROCm1 | tg128 | 86.12 ¬± 0.22 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | ROCm | 999 | ROCm1 | pp512 | 197.89 ¬± 0.62 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | ROCm | 999 | ROCm1 | tg128 | 81.94 ¬± 0.34 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| llama 8B Q4_K - Medium | 4.64 GiB | 8.03 B | ROCm | 999 | ROCm1 | pp512 | 332.95 ¬± 3.21 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| llama 8B Q4_K - Medium | 4.64 GiB | 8.03 B | ROCm | 999 | ROCm1 | tg128 | 71.74 ¬± 0.08 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gemma3 27B Q4_K - Medium | 15.66 GiB | 27.01 B | ROCm | 999 | ROCm1 | pp512 | 186.91 ¬± 0.79 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gemma3 27B Q4_K - Medium | 15.66 GiB | 27.01 B | ROCm | 999 | ROCm1 | tg128 | 24.47 ¬± 0.03 |&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/luminarian721"&gt; /u/luminarian721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc1j9i/dual_radeon_r9700_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc1j9i/dual_radeon_r9700_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc1j9i/dual_radeon_r9700_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T02:45:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc0jfl</id>
    <title>What would be the best budget GPU now?</title>
    <updated>2025-10-21T01:57:53+00:00</updated>
    <author>
      <name>/u/Suomi422</name>
      <uri>https://old.reddit.com/user/Suomi422</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got RTX 3050 OEM now and I'm building a new PC where I would like to have something more powerful for local LLMs - I'm also gaming but only really light stuffs like indie games. I'm planing to use Linux where AMD support works better at Wayland these days, but I also understand that AMD GPUs haven't good support for LLMs...&lt;/p&gt; &lt;p&gt;My budget would be something between Radeon RX 9060 XT 16GB and Nvidia RTX 5060Ti 16GB. Is there something better in this price category? * I was also thinking about Sparkle Intel Arc A770 Titan, but do not have any experience with Intel's GPUs yet...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suomi422"&gt; /u/Suomi422 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc0jfl/what_would_be_the_best_budget_gpu_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc0jfl/what_would_be_the_best_budget_gpu_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc0jfl/what_would_be_the_best_budget_gpu_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T01:57:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1obcm9r</id>
    <title>DeepSeek releases DeepSeek OCR</title>
    <updated>2025-10-20T06:26:26+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obcm9r/deepseek_releases_deepseek_ocr/"&gt; &lt;img alt="DeepSeek releases DeepSeek OCR" src="https://external-preview.redd.it/ddlXXAanndfx0k3ivMcCdrEJtDQlMZs1JyMP8q81Yms.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54c207b8079de2f72cbaafba0d28b87918c60e33" title="DeepSeek releases DeepSeek OCR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR"&gt;https://huggingface.co/deepseek-ai/DeepSeek-OCR&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/t4ji6agdn7wf1.png?width=2646&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f76bdf09e595fa18f0d701b98f9b0f3ed01ee5db"&gt;https://preview.redd.it/t4ji6agdn7wf1.png?width=2646&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f76bdf09e595fa18f0d701b98f9b0f3ed01ee5db&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obcm9r/deepseek_releases_deepseek_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obcm9r/deepseek_releases_deepseek_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obcm9r/deepseek_releases_deepseek_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T06:26:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1obo226</id>
    <title>whats up with the crazy amount of OCR models launching?</title>
    <updated>2025-10-20T17:19:16+00:00</updated>
    <author>
      <name>/u/ComplexType568</name>
      <uri>https://old.reddit.com/user/ComplexType568</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obo226/whats_up_with_the_crazy_amount_of_ocr_models/"&gt; &lt;img alt="whats up with the crazy amount of OCR models launching?" src="https://preview.redd.it/dfdpiv7fvawf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e0b9278cf631890d722235d0fa392c339e1208e" title="whats up with the crazy amount of OCR models launching?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;aside from these models, we got MinerU2.5 and some other models i forgot. im most interested by DeepSeek launching an OCR model of all things, weren't they into AGI? do you think its for more efficient document parsing for training data or something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexType568"&gt; /u/ComplexType568 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dfdpiv7fvawf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obo226/whats_up_with_the_crazy_amount_of_ocr_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obo226/whats_up_with_the_crazy_amount_of_ocr_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T17:19:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1occcel</id>
    <title>SmolVLM AWQ Text Quantization (4 GB ‚Üí 2GB with minimal quality loss on DocVQA)</title>
    <updated>2025-10-21T13:02:06+00:00</updated>
    <author>
      <name>/u/Ok_Employee_6418</name>
      <uri>https://old.reddit.com/user/Ok_Employee_6418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occcel/smolvlm_awq_text_quantization_4_gb_2gb_with/"&gt; &lt;img alt="SmolVLM AWQ Text Quantization (4 GB ‚Üí 2GB with minimal quality loss on DocVQA)" src="https://external-preview.redd.it/Kjegehdr73l6a0EStswJLB7yLrGnAt87gT0UjQYkxvk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e9752bb52ed840b99326e2e047a618fbe01dff3c" title="SmolVLM AWQ Text Quantization (4 GB ‚Üí 2GB with minimal quality loss on DocVQA)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing AWQ and GPTQ quantized versions of SmolVLM from Hugging Face. &lt;/p&gt; &lt;p&gt;These models only had their text models quantized, and had a 50% model size reduction (4GB~2GB) while keeping model degradation under 1% on the DocVQA benchmark. &lt;/p&gt; &lt;p&gt;#huggingface #smolvlm #smollm&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Employee_6418"&gt; /u/Ok_Employee_6418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ronantakizawa/SmolVLM-Instruct-awq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occcel/smolvlm_awq_text_quantization_4_gb_2gb_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1occcel/smolvlm_awq_text_quantization_4_gb_2gb_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T13:02:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1occepv</id>
    <title>Which vision language models are best?</title>
    <updated>2025-10-21T13:04:52+00:00</updated>
    <author>
      <name>/u/Much_Pack_2143</name>
      <uri>https://old.reddit.com/user/Much_Pack_2143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to use them in gastrology image interpretation to benchmark them, what models do u guys suggest would be good? (should be open access)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Much_Pack_2143"&gt; /u/Much_Pack_2143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occepv/which_vision_language_models_are_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occepv/which_vision_language_models_are_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1occepv/which_vision_language_models_are_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T13:04:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocbxhm</id>
    <title>We built ContextAgent ‚Äî a context-centric take on multi-agent systems (rethinking what an ‚Äúagent‚Äù is)</title>
    <updated>2025-10-21T12:44:01+00:00</updated>
    <author>
      <name>/u/TimeLover935</name>
      <uri>https://old.reddit.com/user/TimeLover935</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We think multi-agent frameworks have gotten too heavy.&lt;/p&gt; &lt;p&gt;So we tried something different ‚Äî &lt;strong&gt;ContextAgent&lt;/strong&gt; treats each ‚Äúagent‚Äù simply as an &lt;strong&gt;LLM with a different context&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Instead of managing tons of roles and message-passing, everything revolves around a &lt;strong&gt;central context object&lt;/strong&gt; that stores and updates shared state between agents.&lt;/p&gt; &lt;p&gt;That design makes it possible to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;run complex multi-agent workflows (like research or data analysis)&lt;/li&gt; &lt;li&gt;keep the whole system lightweight and minimal&lt;/li&gt; &lt;li&gt;extend with simple, modular components&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We already built two pipelines ‚Äî&lt;/p&gt; &lt;p&gt;üï∏Ô∏è &lt;em&gt;Web Research&lt;/em&gt; and üìà &lt;em&gt;Data Analysis (auto ML from a file)&lt;/em&gt; ‚Äî and plan to add more while staying minimal.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/context-machine-lab/contextagent"&gt;https://github.com/context-machine-lab/contextagent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear what others think about the agent system for context engineering.&lt;/p&gt; &lt;p&gt;Really appreciate &lt;a href="https://github.com/openai/openai-agents-python"&gt;OpenAI Agents SDK&lt;/a&gt;, &lt;a href="https://github.com/TencentCloudADP/youtu-agent"&gt;Youtu-Agent&lt;/a&gt; and &lt;a href="https://github.com/qx-labs/agents-deep-research"&gt;agents-deep-research&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TimeLover935"&gt; /u/TimeLover935 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocbxhm/we_built_contextagent_a_contextcentric_take_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocbxhm/we_built_contextagent_a_contextcentric_take_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocbxhm/we_built_contextagent_a_contextcentric_take_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T12:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc71s2</id>
    <title>Status of local OCR and python</title>
    <updated>2025-10-21T08:03:16+00:00</updated>
    <author>
      <name>/u/R_Duncan</name>
      <uri>https://old.reddit.com/user/R_Duncan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Needing to have a fully local pipeline to OCR some confidential documents full of tables, I couldn't use marker+gemini like some moths ago, so I tried everything, and I want to share my experience, as a Windows user. Many retries, breakage, packages not installing or not working as expected.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Marker : many issue if llm is local, VRAM used by suryaOCR, compatibility issues with OpenAI API format.&lt;/li&gt; &lt;li&gt;llamacpp : seems working with llama-server, however results are lackluster for granite-docling, nanonet and OlmOCR (this last seems to work on very little images but on a table of 16 rows never worked in 5 retries). Having only 8GB VRAM tried all combinations, starting from Q4+f16&lt;/li&gt; &lt;li&gt;Docstrange : asks for forced authentication at startup, not an option for confidential documents (sorry I can read and work with data inside, doc is not mine).&lt;/li&gt; &lt;li&gt;Docling : very bad, granite_docling almost always embed the image into a document, in some particular image resolution can produce a decent markdown (same model worked in WebGPU demo), didn't worked with pdf tables due header/footer.&lt;/li&gt; &lt;li&gt;Deepseek : only linux by design (vllm, windows version not compatible)&lt;/li&gt; &lt;li&gt;Paddle*** : paddlepaddle is awful to install, the rest seems to install, but inference never worked even from a clean venv. (windows issue?)&lt;/li&gt; &lt;li&gt;So I tried also the old excalibur-py, but it doesn't installs anymore due to pycrypto being obsolete, and binaries in shadow archives are only for python &amp;lt;3.8.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Then I tried &lt;strong&gt;nexa-sdk&lt;/strong&gt; (starting from win cmd, git bash is not the right terminal), Qwen3-VL-4B-Thinking-GGUF was doing something but inconclusive and hard to force, Qwen3-VL-4B-Instruct-GGUF is just working. So this is my post of appreciation.&lt;/p&gt; &lt;p&gt;After wasting 3 days for this, I think python registry needs some kind of rework and the number of dependencies and versions started to be an hell.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R_Duncan"&gt; /u/R_Duncan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc71s2/status_of_local_ocr_and_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc71s2/status_of_local_ocr_and_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc71s2/status_of_local_ocr_and_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T08:03:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1obsdm6</id>
    <title>ROCm 7.9 RC1 released. Supposedly this one supports Strix Halo. Finally, it's listed under supported hardware.</title>
    <updated>2025-10-20T20:12:35+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://rocm.docs.amd.com/en/docs-7.9.0/about/release-notes.html#supported-hardware-and-operating-systems"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obsdm6/rocm_79_rc1_released_supposedly_this_one_supports/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obsdm6/rocm_79_rc1_released_supposedly_this_one_supports/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T20:12:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocdrlg</id>
    <title>Selecting hardware for local LLM</title>
    <updated>2025-10-21T14:00:32+00:00</updated>
    <author>
      <name>/u/deadmoroz14</name>
      <uri>https://old.reddit.com/user/deadmoroz14</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I need an advice on selecting the hardware to run LLMs locally.&lt;/p&gt; &lt;p&gt;My tasks require coding and thinking LLMs. Inference speed is not that critical, but ability to perform taks correctly is (thus, I am leaning towards bigger models). I am not planning on training models, only inference.&lt;/p&gt; &lt;p&gt;What would be the best setup, considering the budget around 2-2.5k$?&lt;/p&gt; &lt;p&gt;As I see it, I have several options:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Get the regular PC with something akin RTX 3090 24GB and plenty of regular RAM. It will run smaller models fast, but I am not sure it will suffice for larger models (and getting better results). Since it is an nVidia, I expect less compatibility issues. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Get the mini PC on AMD Strix Halo with 128GB of unified RAM (Framework Destop or GMKtec EVO-X2). It will fit larger models, but will run slower, and is more problematic to use (selecting appropriate runtime for model, general lack of CUDA, setting the VRAM limit). But 96 GB of VRAM is tempting, and vulkan backend seems to work fine.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(I'd go for Mac Studio or nVidia DGX Spark, but they are too expensive)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Get the regular (or not so) PC without dGPU, but with lots of fast multichannel RAM (Xeons and Threadrippers). Haven't really looked into that much, but it could work (maybe?).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any other options that I don't know?&lt;/p&gt; &lt;p&gt;What will be the best choice?&lt;/p&gt; &lt;p&gt;Any way, I will be pleased with any suggestions.&lt;br /&gt; Thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/deadmoroz14"&gt; /u/deadmoroz14 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocdrlg/selecting_hardware_for_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocdrlg/selecting_hardware_for_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocdrlg/selecting_hardware_for_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T14:00:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc9vvl</id>
    <title>AMD iGPU + dGPU : llama.cpp tensor-split not working with Vulkan backend</title>
    <updated>2025-10-21T11:01:02+00:00</updated>
    <author>
      <name>/u/Sixbroam</name>
      <uri>https://old.reddit.com/user/Sixbroam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Edit : Picard12832 gave me the solution, using --device Vulkan0,Vulkan1 instead of passing GGML_VK_VISIBLE_DEVICES=0,1 did the trick.&lt;/p&gt; &lt;p&gt;Trying to run gpt-oss-120b with llama.cpp with Vulkan backend using my 780M iGPU (64GB shared) and Vega 64 (8GB VRAM) but tensor-split just doesn't work. Everything dumps onto the Vega and uses GTT while the iGPU does nothing.&lt;/p&gt; &lt;p&gt;Output says &amp;quot;using device Vulkan1&amp;quot; and all 59GB goes there.&lt;/p&gt; &lt;p&gt;Tried flipping device order, different ts values, --main-gpu 0, split-mode layer, bunch of env vars... always picks Vulkan1.&lt;/p&gt; &lt;p&gt;Does tensor-split even work with Vulkan? Works fine for CUDA apparently but can't find anyone doing multi-GPU with Vulkan.&lt;/p&gt; &lt;p&gt;The model barely overflows my RAM so I just need the Vega to handle that bit, not for compute. If the split worked it'd be perfect.&lt;/p&gt; &lt;p&gt;Any help would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sixbroam"&gt; /u/Sixbroam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc9vvl/amd_igpu_dgpu_llamacpp_tensorsplit_not_working/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc9vvl/amd_igpu_dgpu_llamacpp_tensorsplit_not_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc9vvl/amd_igpu_dgpu_llamacpp_tensorsplit_not_working/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T11:01:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1obrvab</id>
    <title>Support for Ling and Ring models (1000B/103B/16B) has finally been merged into llama.cpp</title>
    <updated>2025-10-20T19:54:08+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obrvab/support_for_ling_and_ring_models_1000b103b16b_has/"&gt; &lt;img alt="Support for Ling and Ring models (1000B/103B/16B) has finally been merged into llama.cpp" src="https://external-preview.redd.it/n2_CIH2NdPrPVJO7RzSAqCKKA-IjXoFSmGm_ZeORNmA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d670894b141c1fd2de6b21248ecab346ff0c897" title="Support for Ling and Ring models (1000B/103B/16B) has finally been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been following this PR for over a month because it adds support for some interesting MoE, the 103B size sounds cool&lt;/p&gt; &lt;p&gt;1T models:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T"&gt;https://huggingface.co/inclusionAI/Ring-1T&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-1T"&gt;https://huggingface.co/inclusionAI/Ling-1T&lt;/a&gt;&lt;/p&gt; &lt;p&gt;103B models&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-flash-2.0"&gt;https://huggingface.co/inclusionAI/Ling-flash-2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-flash-2.0"&gt;https://huggingface.co/inclusionAI/Ring-flash-2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;16B models&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-mini-2.0"&gt;https://huggingface.co/inclusionAI/Ring-mini-2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-mini-2.0"&gt;https://huggingface.co/inclusionAI/Ling-mini-2.0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16063"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obrvab/support_for_ling_and_ring_models_1000b103b16b_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obrvab/support_for_ling_and_ring_models_1000b103b16b_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:54:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1obrde8</id>
    <title>Cerebras REAP update: pruned checkpoints for GLM4.5-Air &amp; Qwen3-Coder-30B now of HF!</title>
    <updated>2025-10-20T19:35:48+00:00</updated>
    <author>
      <name>/u/ilzrvch</name>
      <uri>https://old.reddit.com/user/ilzrvch</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/"&gt; &lt;img alt="Cerebras REAP update: pruned checkpoints for GLM4.5-Air &amp;amp; Qwen3-Coder-30B now of HF!" src="https://b.thumbs.redditmedia.com/c9KTXS-jH2CuE3vbdqAs-d7zeKzxIjLJapF1oi1eETU.jpg" title="Cerebras REAP update: pruned checkpoints for GLM4.5-Air &amp;amp; Qwen3-Coder-30B now of HF!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have heard your feedback on our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"&gt;initial REAP post&lt;/a&gt; and are excited to released REAP-pruned checkpoints for more lightweight models, GLM4.5-Air and Qwen3-Coder-30B:&lt;/p&gt; &lt;p&gt;25% pruned GLM4.5-Air: &lt;a href="https://hf.co/cerebras/GLM-4.5-Air-REAP-82B-A12B"&gt;https://hf.co/cerebras/GLM-4.5-Air-REAP-82B-A12B&lt;/a&gt;&lt;br /&gt; 20% pruned Qwen3-Coder-30B: &lt;a href="https://huggingface.co/cerebras/Qwen3-Coder-REAP-25B-A3B"&gt;https://huggingface.co/cerebras/Qwen3-Coder-REAP-25B-A3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are releasing those in BF16 so more accurate low-bit quantized GGUFs can be created for streamlined local deployment.&lt;/p&gt; &lt;p&gt;TLDR on REAP: &lt;/p&gt; &lt;p&gt;We show that one-shot pruning of experts in large MoEs is better than expert merging when looking at realistic benchmarks, not just perplexity measures.&lt;/p&gt; &lt;p&gt;Using a saliency criterion that measures expected routed contribution of each expert (REAP), we pruned Qwen3-Coder-480B to 363B (25% pruning) and 246B (50% pruning), all in FP8. At 25%, accuracy degradation is minimal across a suite of benchmarks. More on arXiv: &lt;a href="https://arxiv.org/abs/2510.13999"&gt;https://arxiv.org/abs/2510.13999&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let us know which models we should prune next in the comments!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vuu82b8sjbwf1.png?width=6539&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc8a064e15281f6e830e724e70d86a1b46721dc3"&gt;https://preview.redd.it/vuu82b8sjbwf1.png?width=6539&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc8a064e15281f6e830e724e70d86a1b46721dc3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilzrvch"&gt; /u/ilzrvch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:35:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1occ8uv</id>
    <title>Confirmed: Junk social media data makes LLMs dumber</title>
    <updated>2025-10-21T12:58:04+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occ8uv/confirmed_junk_social_media_data_makes_llms_dumber/"&gt; &lt;img alt="Confirmed: Junk social media data makes LLMs dumber" src="https://a.thumbs.redditmedia.com/It6udXwo12VzdtUYx3sIoCiTZnQLhz-QLVIedLuaJN8.jpg" title="Confirmed: Junk social media data makes LLMs dumber" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new study from Texas A&amp;amp;M University and Purdue University proposes the &lt;em&gt;LLM Brain Rot Hypothesis&lt;/em&gt;: continual pretraining on ‚Äújunk‚Äù social-media text (short, viral, sensational content) causes lasting declines in reasoning, long-context and safety.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wq569rzfpgwf1.png?width=2772&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7a14a98cc9682cd209918c93fa23222d2df7b23"&gt;https://preview.redd.it/wq569rzfpgwf1.png?width=2772&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7a14a98cc9682cd209918c93fa23222d2df7b23&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ARC-Challenge with Chain Of Thoughts drops 74.9 ‚Üí 57.2 and RULER-CWE 84.4 ‚Üí 52.3 as junk ratio rises from 0% to 100%.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occ8uv/confirmed_junk_social_media_data_makes_llms_dumber/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occ8uv/confirmed_junk_social_media_data_makes_llms_dumber/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1occ8uv/confirmed_junk_social_media_data_makes_llms_dumber/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T12:58:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1obn0q7</id>
    <title>The Innovations in DeepSeek OCR</title>
    <updated>2025-10-20T16:29:30+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek just released a pretty shocking new paper. They really buried the lede here by referring to it simply as DeepSeek OCR. &lt;/p&gt; &lt;p&gt;While it‚Äôs a very strong OCR model, the purpose of it and the implications of their approach go far beyond what you‚Äôd expect of ‚Äúyet another OCR model.‚Äù&lt;/p&gt; &lt;p&gt;Traditionally, vision LLM tokens almost seemed like an afterthought or ‚Äúbolt on‚Äù to the LLM paradigm. And 10k words of English would take up far more space in a multimodal LLM when expressed as intelligible pixels than when expressed as tokens.&lt;/p&gt; &lt;p&gt;So those 10k words may have turned into 15k tokens, or 30k to 60k ‚Äúvisual tokens.‚Äù So vision tokens were way less efficient and really only made sense to use for data that couldn‚Äôt be effectively conveyed with words. &lt;/p&gt; &lt;p&gt;But that gets inverted now from the ideas in this paper. DeepSeek figured out how to get 10x better compression using vision tokens than with text tokens! So you could theoretically store those 10k words in just 1,500 of their special compressed visual tokens.&lt;/p&gt; &lt;p&gt;This might not be as unexpected as it sounds if you think of how your own mind works. After all, I know that when I‚Äôm looking for a part of a book that I‚Äôve already read, I imagine it visually and always remember which side of the book it was on and approximately where on the page it was, which suggests some kind of visual memory representation at work.&lt;/p&gt; &lt;p&gt;Now, it‚Äôs not clear how exactly this interacts with the other downstream cognitive functioning of an LLM; can the model reason as intelligently over those compressed visual tokens as it can using regular text tokens? Does it make the model less articulate by forcing it into a more vision-oriented modality? &lt;/p&gt; &lt;p&gt;But you can imagine that, depending on the exact tradeoffs, it could be a very exciting new axis to greatly expand effective context sizes. Especially when combined with DeepSeek‚Äôs other recent paper from a couple weeks ago about sparse attention.&lt;/p&gt; &lt;p&gt;For all we know, Google could have already figured out something like this, which could explain why Gemini has such a huge context size and is so good and fast at OCR tasks. If they did, they probably wouldn‚Äôt say because it would be viewed as an important trade secret.&lt;/p&gt; &lt;p&gt;But the nice thing about DeepSeek is that they‚Äôve made the entire thing open source and open weights and explained how they did it, so now everyone can try it out and explore.&lt;/p&gt; &lt;p&gt;Even if these tricks make attention more lossy, the potential of getting a frontier LLM with a 10 or 20 million token context window is pretty exciting. &lt;/p&gt; &lt;p&gt;You could basically cram all of a company‚Äôs key internal documents into a prompt preamble and cache this with OpenAI and then just add your specific query or prompt on top of that and not have to deal with search tools and still have it be fast and cost-effective. &lt;/p&gt; &lt;p&gt;Or put an entire code base into the context and cache it, and then just keep appending the equivalent of the git diffs as you make changes to the code. &lt;/p&gt; &lt;p&gt;If you‚Äôve ever read stories about the great physicist Hans Bethe, he was known for having vast amounts of random physical facts memorized (like the entire periodic table; boiling points of various substances, etc.) so that he could seamlessly think and compute without ever having to interrupt his flow to look something up in a reference table. &lt;/p&gt; &lt;p&gt;Having vast amounts of task-specific knowledge in your working memory is extremely useful. This seems like a very clever and additive approach to potentially expanding that memory bank by 10x or more.&lt;/p&gt; &lt;p&gt;source: &lt;a href="https://x.com/doodlestein/status/1980282222893535376"&gt;https://x.com/doodlestein/status/1980282222893535376&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obn0q7/the_innovations_in_deepseek_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obn0q7/the_innovations_in_deepseek_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obn0q7/the_innovations_in_deepseek_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T16:29:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc3f0i</id>
    <title>Qwen3 Omni interactive speech</title>
    <updated>2025-10-21T04:21:40+00:00</updated>
    <author>
      <name>/u/Powerful-Angel-301</name>
      <uri>https://old.reddit.com/user/Powerful-Angel-301</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 Omni is very interesting. They claim it supports real-time voice, but I couldn't find out how and there was no tutorial for this on their github. &lt;/p&gt; &lt;p&gt;Anyone having any experience with that? Basically continuously talk to the model and get voice responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Angel-301"&gt; /u/Powerful-Angel-301 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc3f0i/qwen3_omni_interactive_speech/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc3f0i/qwen3_omni_interactive_speech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc3f0i/qwen3_omni_interactive_speech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T04:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc8vb4</id>
    <title>Do you have any ideas for OCR on pages of documents with very very low contrast?</title>
    <updated>2025-10-21T10:01:29+00:00</updated>
    <author>
      <name>/u/suelzsuelz</name>
      <uri>https://old.reddit.com/user/suelzsuelz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc8vb4/do_you_have_any_ideas_for_ocr_on_pages_of/"&gt; &lt;img alt="Do you have any ideas for OCR on pages of documents with very very low contrast?" src="https://preview.redd.it/yhbgv2pztfwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ac61a3033e6e868cec48291c7feaa1ca30e5a51" title="Do you have any ideas for OCR on pages of documents with very very low contrast?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My use case is to locally extract pdf content into Markdown or JSON-structured data. The problem, as demonstrated by the example, is that the contrast between the text and background is very poor.&lt;/p&gt; &lt;p&gt;Has anyone ever processed similar documents?&lt;br /&gt; Which local models with how many parameters can do this reliably? &lt;/p&gt; &lt;p&gt;Newer cloud models don't seem to have any problems. We have already tested these:&lt;/p&gt; &lt;p&gt;- granite3.2-vision&lt;br /&gt; - minicpm-v2.6:8b&lt;br /&gt; - llama3.2-vision:11b&lt;br /&gt; - DeepSeek-OCR&lt;/p&gt; &lt;p&gt;Maybe they are just too small?&lt;/p&gt; &lt;p&gt;We are able to use a 4 x RTX 3090 Workstation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/suelzsuelz"&gt; /u/suelzsuelz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yhbgv2pztfwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc8vb4/do_you_have_any_ideas_for_ocr_on_pages_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc8vb4/do_you_have_any_ideas_for_ocr_on_pages_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T10:01:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc8dqx</id>
    <title>Vascura FRONT - Open Source (Apache 2.0), Bloat Free, Portable and Lightweight (288 kb) LLM Frontend.</title>
    <updated>2025-10-21T09:31:29+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc8dqx/vascura_front_open_source_apache_20_bloat_free/"&gt; &lt;img alt="Vascura FRONT - Open Source (Apache 2.0), Bloat Free, Portable and Lightweight (288 kb) LLM Frontend." src="https://external-preview.redd.it/Z24wYzZvdXdvZndmMTaXKbAxEnkCSlwbwmJDXf_lyDQzd483n4JJoFhjK3xD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae56ff1f5b7f8b918196f743f18a36cbbf044718" title="Vascura FRONT - Open Source (Apache 2.0), Bloat Free, Portable and Lightweight (288 kb) LLM Frontend." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4oaz6nuwofwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc8dqx/vascura_front_open_source_apache_20_bloat_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc8dqx/vascura_front_open_source_apache_20_bloat_free/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T09:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocbkry</id>
    <title>[By GLM Team] Glyph: Scaling Context Windows via Visual-Text Compression</title>
    <updated>2025-10-21T12:27:54+00:00</updated>
    <author>
      <name>/u/NeterOster</name>
      <uri>https://old.reddit.com/user/NeterOster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2510.17800"&gt;https://arxiv.org/abs/2510.17800&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling context windows to the million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. In this work, we take a different perspective-visual context scaling-to tackle this challenge. Instead of extending token-based sequences, we propose Glyph, a framework that renders long texts into images and processes them with vision-language models (VLMs). This approach substantially compresses textual input while preserving semantic information, and we further design an LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression. Through extensive experiments, we demonstrate that our method achieves 3-4x token compression while maintaining accuracy comparable to leading LLMs such as Qwen3-8B on various long-context benchmarks. This compression also leads to around 4x faster prefilling and decoding, and approximately 2x faster SFT training. Furthermore, under extreme compression, a 128K-context VLM could scale to handle 1M-token-level text tasks. In addition, the rendered text data benefits real-world multimodal tasks, such as document understanding. Our code and model are released at &lt;a href="https://github.com/thu-coai/Glyph"&gt;this https URL&lt;/a&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The model is not yet available at the moment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeterOster"&gt; /u/NeterOster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocbkry/by_glm_team_glyph_scaling_context_windows_via/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocbkry/by_glm_team_glyph_scaling_context_windows_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocbkry/by_glm_team_glyph_scaling_context_windows_via/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T12:27:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1occan8</id>
    <title>vLLM + OpenWebUI + Tailscale = private, portable AI</title>
    <updated>2025-10-21T13:00:13+00:00</updated>
    <author>
      <name>/u/zhambe</name>
      <uri>https://old.reddit.com/user/zhambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occan8/vllm_openwebui_tailscale_private_portable_ai/"&gt; &lt;img alt="vLLM + OpenWebUI + Tailscale = private, portable AI" src="https://b.thumbs.redditmedia.com/cRxS8E_CLjIUT07vC7k-8ob1jCAW2BZimcQfFKJicNg.jpg" title="vLLM + OpenWebUI + Tailscale = private, portable AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My mind is positively blown... My own AI?!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zhambe"&gt; /u/zhambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1occan8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occan8/vllm_openwebui_tailscale_private_portable_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1occan8/vllm_openwebui_tailscale_private_portable_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T13:00:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc7uio</id>
    <title>DeepSeek-OCR Playground ‚Äî Dockerized FastAPI + React workbench (5090-ready), image ‚Üí text/description, more to come</title>
    <updated>2025-10-21T08:56:32+00:00</updated>
    <author>
      <name>/u/Putrid_Passion_6916</name>
      <uri>https://old.reddit.com/user/Putrid_Passion_6916</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Repo: &lt;a href="https://github.com/rdumasia303/deepseek_ocr_app"&gt;https://github.com/rdumasia303/deepseek_ocr_app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TL;DR: A tiny web app to mess with the new DeepSeek-OCR locally. Upload an image, pick a mode (Plain OCR, Describe, Find/grounding, Freeform), and get results instantly. &lt;/p&gt; &lt;p&gt;It runs in Docker with GPU (tested on 5090/Blackwell), has a slick UI, and is ‚Äúgood enough‚Äù to ship &amp;amp; let the community break/fix/improve it. PRs welcome.&lt;/p&gt; &lt;p&gt;What‚Äôs inside&lt;/p&gt; &lt;p&gt;Frontend: React/Vite + glassy Tailwind UI (drag-drop, live preview, copy/download). Backend: FastAPI + Transformers, calls DeepSeek-OCR with eval_mode=True. GPU: Blackwell-friendly (bfloat16), designed to run on RTX 5090 (or any CUDA GPU).&lt;/p&gt; &lt;p&gt;Modes shipped now: Plain OCR (super strong) Describe (short freeform caption) Find (grounding) ‚Äî returns boxes for a term (e.g., ‚ÄúTotal Due‚Äù, ‚ÄúSignature‚Äù) Freeform ‚Äî your own instruction&lt;/p&gt; &lt;p&gt;There‚Äôs groundwork laid for more modes (Markdown, Tables‚ÜíCSV/MD, KV‚ÜíJSON, PII, Layout map). If you add one, make a PR!&lt;/p&gt; &lt;p&gt;Quick start&lt;/p&gt; &lt;h1&gt;clone&lt;/h1&gt; &lt;p&gt;git clone &lt;a href="https://github.com/rdumasia303/deepseek_ocr_app"&gt;https://github.com/rdumasia303/deepseek_ocr_app&lt;/a&gt; cd deepseek_ocr_app&lt;/p&gt; &lt;h1&gt;run&lt;/h1&gt; &lt;p&gt;docker compose up -d --build&lt;/p&gt; &lt;h1&gt;open&lt;/h1&gt; &lt;h1&gt;frontend: http://localhost:3000 (or whatever the repo says)&lt;/h1&gt; &lt;h1&gt;backend: http://localhost:8000/docs&lt;/h1&gt; &lt;p&gt;Heads-up: First model load downloads weights + custom code (trust_remote_code). If you want reproducibility, pin a specific HF revision in the backend.&lt;/p&gt; &lt;p&gt;Sample prompts (try these) Plain OCR: (no need to type anything ‚Äî just run the mode) Describe: ‚ÄúDescribe this image concisely in 2‚Äì3 sentences.‚Äù Find: set term to Total Due, Signature, Logo, etc. Freeform: ‚ÄúConvert the document to markdown.‚Äù ‚ÄúExtract every table and output CSV only.‚Äù ‚ÄúReturn strict JSON with fields {invoice_no, date, vendor, total:{amount,currency}}.‚Äù Known rough edges (be gentle, or better, fix them üòÖ)&lt;/p&gt; &lt;p&gt;Grounding (boxes) can be flaky; plain OCR and describe are rock-solid. Structured outputs (CSV/MD/JSON) need post-processing to be 100% reliable.&lt;/p&gt; &lt;p&gt;Roadmap / ideas (grab an issue &amp;amp; go wild)&lt;/p&gt; &lt;p&gt;Add Markdown / Tables / JSON / PII / Layout modes (OCR-first with deterministic fallbacks).&lt;/p&gt; &lt;p&gt;Proper box overlay scaling (processed size vs CSS pixels) ‚Äî coords should snap exactly.&lt;/p&gt; &lt;p&gt;PDF ingestion (pdf2image ‚Üí per-page OCR + merge).&lt;/p&gt; &lt;p&gt;Simple telemetry (mode counts, latency, GPU mem) for perf tuning.&lt;/p&gt; &lt;p&gt;One-click HuggingFace revision pin to avoid surprise code updates. If you try it, please drop feedback ) ‚Äî I‚Äôll iterate. If you make it better, I‚Äôll take your PRs ASAP. üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Putrid_Passion_6916"&gt; /u/Putrid_Passion_6916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc7uio/deepseekocr_playground_dockerized_fastapi_react/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc7uio/deepseekocr_playground_dockerized_fastapi_react/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc7uio/deepseekocr_playground_dockerized_fastapi_react/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T08:56:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1occyly</id>
    <title>Qwen3-Next 80B-A3B llama.cpp implementation with CUDA support half-working already (up to 40k context only), also Instruct GGUFs</title>
    <updated>2025-10-21T13:28:06+00:00</updated>
    <author>
      <name>/u/Ok_Top9254</name>
      <uri>https://old.reddit.com/user/Ok_Top9254</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occyly/qwen3next_80ba3b_llamacpp_implementation_with/"&gt; &lt;img alt="Qwen3-Next 80B-A3B llama.cpp implementation with CUDA support half-working already (up to 40k context only), also Instruct GGUFs" src="https://preview.redd.it/a21ouwhkvgwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=242dfbde4c1caaaa4f35057e48df50de5e9cc8f6" title="Qwen3-Next 80B-A3B llama.cpp implementation with CUDA support half-working already (up to 40k context only), also Instruct GGUFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095#issuecomment-3424224842"&gt;Llama.cpp pull request&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF"&gt;GGUFs for Instruct model&lt;/a&gt; (old news but info for the uninitiated)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Top9254"&gt; /u/Ok_Top9254 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a21ouwhkvgwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occyly/qwen3next_80ba3b_llamacpp_implementation_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1occyly/qwen3next_80ba3b_llamacpp_implementation_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T13:28:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocbggm</id>
    <title>Poll on thinking/no thinking for the next open-weights Google model</title>
    <updated>2025-10-21T12:22:21+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/osanseviero/status/1980553451261292628"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocbggm/poll_on_thinkingno_thinking_for_the_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocbggm/poll_on_thinkingno_thinking_for_the_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T12:22:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1obqkpe</id>
    <title>Best Local LLMs - October 2025</title>
    <updated>2025-10-20T19:06:06+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Welcome to the first monthly &amp;quot;Best Local LLMs&amp;quot; post!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;General&lt;/li&gt; &lt;li&gt;Agentic/Tool Use&lt;/li&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;Creative Writing/RP&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(look for the top level comments for each Application and please thread your responses under that)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:06:06+00:00</published>
  </entry>
</feed>
