<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-12T12:54:28+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pkmi5v</id>
    <title>Docling PDF Parsing with remote VLM</title>
    <updated>2025-12-12T08:05:43+00:00</updated>
    <author>
      <name>/u/Top-Fig1571</name>
      <uri>https://old.reddit.com/user/Top-Fig1571</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;currently i am using the Mineru Library to parse PDF to markdown which is great as it as well preserves images or text coordinates. However I might need to switch to a non-chinese solution so i planned to use docling.&lt;/p&gt; &lt;p&gt;I am not sure if granite-docling is strong enough to handle complex pdfs so my plan was to switch the VLM. But as docling is specialized with doctags I am not sure if it is reliably working with remote VLM (e.g. OlmOCR). Does anyone have a solid docling pipeline already for this?&lt;/p&gt; &lt;p&gt;Also what is in your opinion the best way to parse PDFs with images/tables nowadays? Are these the small, specializes OCR VLMs like granite-docling or OlmOCR or are big VLMs better? I need an Open Source solution.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Top-Fig1571"&gt; /u/Top-Fig1571 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkmi5v/docling_pdf_parsing_with_remote_vlm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkmi5v/docling_pdf_parsing_with_remote_vlm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkmi5v/docling_pdf_parsing_with_remote_vlm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T08:05:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk4e27</id>
    <title>Updates to official SWE-bench leaderboard: Kimi K2 Thinking top of open-source</title>
    <updated>2025-12-11T18:05:23+00:00</updated>
    <author>
      <name>/u/klieret</name>
      <uri>https://old.reddit.com/user/klieret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk4e27/updates_to_official_swebench_leaderboard_kimi_k2/"&gt; &lt;img alt="Updates to official SWE-bench leaderboard: Kimi K2 Thinking top of open-source" src="https://b.thumbs.redditmedia.com/kKZ3ZoCKjqFucLu9U3zcx9kw9f07ItjqFMxxpwplBXs.jpg" title="Updates to official SWE-bench leaderboard: Kimi K2 Thinking top of open-source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, thanks for your suggestions of what models to evaluate! Still working on some, but we've just added Kimi K2 thinking and the two new mistral models. Turns out Kimi K2 Thinking takes the top, surpassing minimax by 2.4%pts (that's 12 task instances). The devstral models fall in the middle, but they are currently freely available on the mistral API!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7d8p912z5m6g1.png?width=4071&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6688a0a1b7583b3c78097fbb75c31618cbe46b21"&gt;https://preview.redd.it/7d8p912z5m6g1.png?width=4071&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6688a0a1b7583b3c78097fbb75c31618cbe46b21&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All of these results are independently evaluated with the exact same (minimal) agent. So it is expected that the numbers are lower than what companies typically report.&lt;/p&gt; &lt;p&gt;Note the asterisk with the cost for Kimi K2 thinking, it is calculated based on the official API pricing information, but the actual cost that was billed seemed lower (but also the cost portal seemed buggy, so not sure what to trust here‚Äîfor now it's calculated based on the number of tokens same as all the other reported). Anyone know what could be causing any discrepancies?&lt;/p&gt; &lt;p&gt;Kimi K2 Thinking and the devstral models are the exact opposite in terms of steps: Kimi K2 takes the least steps to iterate of all models, devstral the most.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/37akv7ra6m6g1.png?width=2345&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ab53c4ba03c2f013f21fc9115a53e87e111db10"&gt;https://preview.redd.it/37akv7ra6m6g1.png?width=2345&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ab53c4ba03c2f013f21fc9115a53e87e111db10&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you're thinking about limiting runtimes to conserve costs/time, here's how performance scales with step limits (even with Kimi, you still want to run for 125-150 steps on hard problems).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6tdoe4zh6m6g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b3803a5c3567ebb0ffee73c5245b3ff92d02e7ec"&gt;https://preview.redd.it/6tdoe4zh6m6g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b3803a5c3567ebb0ffee73c5245b3ff92d02e7ec&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And this would translate in the following cost-performance plot (where deepseek is still hard to beat). We didn't put the mistral models in here because they're only free temporarily. Of course those are just your API costs, so if you're running on your own hardware, you can ignore this plot:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fd9gseql6m6g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f78011f256fa2019627b1b89962ec418593163d"&gt;https://preview.redd.it/fd9gseql6m6g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f78011f256fa2019627b1b89962ec418593163d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also have all the trajectories/logs updated if you're curious how each model solves things. They're available from the &amp;quot;Trajs&amp;quot; column on &lt;a href="http://swebench.com"&gt;swebench.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As always, you can reproduce our numbers using &lt;a href="https://github.com/SWE-agent/mini-swe-agent/"&gt;https://github.com/SWE-agent/mini-swe-agent/&lt;/a&gt; (there's a page in the tutorial).&lt;/p&gt; &lt;p&gt;Any new models we should add? (there's still some recommendations from last time that I didn't get to yet). Or any other information we should add ? (we've started collecting latency information as of recently).&lt;/p&gt; &lt;p&gt;Also curious if things like the number of steps a model takes etc. show up in your workflows. Depending on how closely users are in the loop behavior is probably quite different. Also would be interested if you have any qualitative observations about the model behaviors and how they differ (if there's interesting observations, we could see if we can add more information about them for the next releases based on all the agent trajectories we collect)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klieret"&gt; /u/klieret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk4e27/updates_to_official_swebench_leaderboard_kimi_k2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk4e27/updates_to_official_swebench_leaderboard_kimi_k2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk4e27/updates_to_official_swebench_leaderboard_kimi_k2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T18:05:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk3cky</id>
    <title>Shisa V2.1: Improved Japanese (JA/EN) Models (1.2B-70B)</title>
    <updated>2025-12-11T17:25:49+00:00</updated>
    <author>
      <name>/u/randomfoo2</name>
      <uri>https://old.reddit.com/user/randomfoo2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're celebrating the 2 year anniversary of our original &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/18cwh4n/shisa_7b_a_new_jaen_bilingual_model_based_on/"&gt;Shisa V1&lt;/a&gt; with an updated set of &lt;a href="https://huggingface.co/collections/shisa-ai/shisa-v21"&gt;Shisa V2.1&lt;/a&gt; JA/EN bilingual models.&lt;/p&gt; &lt;p&gt;Shisa V2.1 introduces new and improved 8B, 14B, and 70B dense models with a big performance bump to our previous &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jz2lll/shisa_v2_a_family_of_new_jaen_bilingual_models/"&gt;Shisa V2 releases&lt;/a&gt;, as well as new 1.2B (LFM2-based) and 3B (Llama 3.2-based) models. Each of these are class-leading in Japanese language capabilities for their size. Our new V2.1 14B beats the old V2 70B and the new V2.1 70B model gets very close to our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1l318di/shisa_v2_405b_the_strongest_model_ever_built_in/"&gt;Shisa V2 405B&lt;/a&gt;! These aren't reasoning or coding models, but if you're looking for an open model that is especially strong at natural/native Japanese, maybe give these a spin.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;License&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Parameters&lt;/th&gt; &lt;th align="left"&gt;Context Length&lt;/th&gt; &lt;th align="left"&gt;JA AVG&lt;/th&gt; &lt;th align="left"&gt;EN AVG&lt;/th&gt; &lt;th align="left"&gt;JA-MT Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/shisa-ai/shisa-v2.1-lfm2-1.2b"&gt;shisa-v2.1-lfm2-1.2b&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1.2B&lt;/td&gt; &lt;td align="left"&gt;32K&lt;/td&gt; &lt;td align="left"&gt;43.4&lt;/td&gt; &lt;td align="left"&gt;27.6&lt;/td&gt; &lt;td align="left"&gt;6.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.2&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/shisa-ai/shisa-v2.1-llama3.2-3b"&gt;shisa-v2.1-llama3.2-3b&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;57.9&lt;/td&gt; &lt;td align="left"&gt;43.2&lt;/td&gt; &lt;td align="left"&gt;7.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Apache 2.0&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/shisa-ai/shisa-v2.1-qwen3-8b"&gt;shisa-v2.1-qwen3-8b&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;32K/128K&lt;/td&gt; &lt;td align="left"&gt;67.8&lt;/td&gt; &lt;td align="left"&gt;57.8&lt;/td&gt; &lt;td align="left"&gt;8.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MIT&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/shisa-ai/shisa-v2.1-unphi4-14b"&gt;shisa-v2.1-unphi4-14b&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;14B&lt;/td&gt; &lt;td align="left"&gt;16K&lt;/td&gt; &lt;td align="left"&gt;72.6&lt;/td&gt; &lt;td align="left"&gt;57.7&lt;/td&gt; &lt;td align="left"&gt;9.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.3&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/shisa-ai/shisa-v2.1-llama3.3-70b"&gt;shisa-v2.1-llama3.3-70b&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;70B&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;73.1&lt;/td&gt; &lt;td align="left"&gt;66.0&lt;/td&gt; &lt;td align="left"&gt;9.26&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;For those that just want to kick the tires, we have &lt;a href="https://chat.shisa.ai/"&gt;https://chat.shisa.ai/&lt;/a&gt; up and running that lets you test and compare V2.1 14B, V2.1 70B, and V2 405B, you might be surprised at just how strong the smaller models are.&lt;/p&gt; &lt;p&gt;These models were all trained on an MI300X node provided by AMD via the &lt;a href="https://www.amd.com/en/developer/resources/cloud-access/amd-developer-cloud.html"&gt;AMD Developer Cloud&lt;/a&gt;. Thanks to all of our compute sponsors, we couldn't keep releasing open models without them. More details (including all sponsors and very detailed eval info) are available on the HF model cards or our &lt;a href="https://shisa.ai/posts/shisa-v2.1/"&gt;announcement post&lt;/a&gt; and mradermacher and others have made GGUFs over the past couple days already for all sizes.&lt;/p&gt; &lt;p&gt;I did want to pull out one interesting bit from the model card, since it's fairly new and unique:&lt;/p&gt; &lt;h3&gt;Cross-Lingual Token Leakage&lt;/h3&gt; &lt;p&gt;While reviewing eval results, we noticed that many models can score highly on Japanese language benchmarks but still output non-Japanese words or sub-words (tokens). Internally we refer to this as Cross-Lingual Token Leakage (CLTL). It has also been referred to more generally as &amp;quot;word-level language confusion&amp;quot; (Marchisio et al., &amp;quot;&lt;a href="https://arxiv.org/abs/2406.20052"&gt;Understanding and Mitigating Language Confusion in LLMs&lt;/a&gt;,&amp;quot; Cohere).&lt;/p&gt; &lt;p&gt;We see many strong multilingual models that exhibit language confusion behavior, but quantifying (and reliably identifying) this issue is harder than one might expect because not only do Japanese and Chinese share Unicode code-planes, but also many valid English words can commonly appear in Japanese text. (Think &amp;quot;AI&amp;quot;, &amp;quot;VR&amp;quot;, or common words and acronyms like &amp;quot;Google&amp;quot; or &amp;quot;NATO&amp;quot;). This is compounded by the fact that even frontier models suffer from ‚Äútoken blindness‚Äù - they are often unable to disentangle the meaning from the actual language of the tokens and often fail to recognize wrong-language tokens.&lt;/p&gt; &lt;p&gt;For Shisa V2.1, we have developed a brand-new class of Japanese evaluation benchmark specifically designed to identify CLTL, which can both measure and specifically identify wrong language tokens.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Base Model&lt;/th&gt; &lt;th&gt;Shisa V2.1 Model&lt;/th&gt; &lt;th align="right"&gt;Base Leak %&lt;/th&gt; &lt;th align="right"&gt;Shisa V2.1 Leak %&lt;/th&gt; &lt;th align="right"&gt;Leakage Improvement&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Llama-3.2-3B-Instruct&lt;/td&gt; &lt;td&gt;shisa-v2.1-llama3.2-3b&lt;/td&gt; &lt;td align="right"&gt;11.48%&lt;/td&gt; &lt;td align="right"&gt;0.24%&lt;/td&gt; &lt;td align="right"&gt;47.8√ó&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LFM2-1.2B&lt;/td&gt; &lt;td&gt;shisa-v2.1-lfm2-1.2b&lt;/td&gt; &lt;td align="right"&gt;4.32%&lt;/td&gt; &lt;td align="right"&gt;0.32%&lt;/td&gt; &lt;td align="right"&gt;13.5√ó&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-8B&lt;/td&gt; &lt;td&gt;shisa-v2.1-qwen3-8b&lt;/td&gt; &lt;td align="right"&gt;2.18%&lt;/td&gt; &lt;td align="right"&gt;0.44%&lt;/td&gt; &lt;td align="right"&gt;5.0√ó&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama-3.3-70B-Instruct&lt;/td&gt; &lt;td&gt;shisa-v2.1-llama3.3-70b&lt;/td&gt; &lt;td align="right"&gt;1.90%&lt;/td&gt; &lt;td align="right"&gt;0.36%&lt;/td&gt; &lt;td align="right"&gt;5.3√ó&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;phi-4&lt;/td&gt; &lt;td&gt;shisa-v2.1-unphi4-14b&lt;/td&gt; &lt;td align="right"&gt;0.12%&lt;/td&gt; &lt;td align="right"&gt;0.06%&lt;/td&gt; &lt;td align="right"&gt;2.0√ó&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;We believe eliminating both CLTL and language confusion in general is of the utmost importance for deploying LLMs for most Japanese-language production use cases (e.g., translation, customer service, or even basic writing tasks) and we plan to continue to both improve our detection heuristics and to integrate it into all our future evaluation grading, as well as use our better CLTL detection to further improve our training methods. We will be publishing more details in-depth in a future writeup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/randomfoo2"&gt; /u/randomfoo2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3cky/shisa_v21_improved_japanese_jaen_models_12b70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3cky/shisa_v21_improved_japanese_jaen_models_12b70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3cky/shisa_v21_improved_japanese_jaen_models_12b70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T17:25:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkpyno</id>
    <title>Running vLLM on ROCm using docker (dual RX 7900 XTX)</title>
    <updated>2025-12-12T11:50:34+00:00</updated>
    <author>
      <name>/u/StupidityCanFly</name>
      <uri>https://old.reddit.com/user/StupidityCanFly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found the command I used to run vLLM in docker. It appears to be working with the latest nightly.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run -it --rm --network=host \ --group-add=video --ipc=host --cap-add=SYS_PTRACE \ --security-opt seccomp=unconfined --device /dev/kfd \ --device /dev/dri \ -v ~/.cache/huggingface/hub:/app/models \ -e HF_HOME=&amp;quot;/app/models&amp;quot; \ -e HF_TOKEN=&amp;quot;&amp;lt;token_here&amp;gt;&amp;quot; \ -e NCCL_P2P_DISABLE=1 \ -e VLLM_CUSTOM_OPS=all \ -e VLLM_ROCM_USE_AITER=0 \ -e SAFETENSORS_FAST_GPU=1 \ -e PYTORCH_TUNABLEOP_ENABLED=1 rocm/vllm-dev:nightly &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This gets you in a shell. Then I use simple &lt;code&gt;vllm start&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;root@dev:/app# vllm serve Qwen/Qwen3-VL-8B-Thinking -tp 2 --max_model_len 64000 --enable-auto-tool-choice --tool-call-parser hermes --reasoning-parser qwen3 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;NOTE: I did not try any quants yet, that was problematic the last time.&lt;/p&gt; &lt;p&gt;Quick benchmark ran with this command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm bench serve \ --model Qwen/Qwen3-VL-8B-Thinking \ --endpoint /v1/completions \ --dataset-name sharegpt \ --dataset-path /app/models/datasets/ShareGPT_V3_unfiltered_cleaned_split.json \ --num-prompts 10 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 10 Failed requests: 0 Benchmark duration (s): 54.23 Total input tokens: 1374 Total generated tokens: 2534 Request throughput (req/s): 0.18 Output token throughput (tok/s): 46.73 Peak output token throughput (tok/s): 427.00 Peak concurrent requests: 10.00 Total token throughput (tok/s): 72.07 ---------------Time to First Token---------------- Mean TTFT (ms): 26055.59 Median TTFT (ms): 28947.21 P99 TTFT (ms): 28949.27 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 99.61 Median TPOT (ms): 75.77 P99 TPOT (ms): 325.06 ---------------Inter-token Latency---------------- Mean ITL (ms): 59.65 Median ITL (ms): 14.60 P99 ITL (ms): 16.06 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StupidityCanFly"&gt; /u/StupidityCanFly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpyno/running_vllm_on_rocm_using_docker_dual_rx_7900_xtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpyno/running_vllm_on_rocm_using_docker_dual_rx_7900_xtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpyno/running_vllm_on_rocm_using_docker_dual_rx_7900_xtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T11:50:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkqzmf</id>
    <title>I cooked MPOA abliterated Seed-OSS-36B-Instruct</title>
    <updated>2025-12-12T12:45:43+00:00</updated>
    <author>
      <name>/u/Perfect_Biscotti_476</name>
      <uri>https://old.reddit.com/user/Perfect_Biscotti_476</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi community,&lt;/p&gt; &lt;p&gt;I cooked up a new abliterated version of Seed-OSS-36B-Instruct using the norm-preserving biprojected abliteration technique.&lt;/p&gt; &lt;p&gt;Although I used to use the &amp;quot;Norm-Preserving Abliterated&amp;quot; tag, I am switching to the MPOA tag (Magnitude-Preserving Orthogonalized Ablation, a.k.a. norm-preserving biprojected abliteration) to stay consistent with grimjim, who proposed this technique.&lt;/p&gt; &lt;p&gt;Model card: &lt;a href="https://huggingface.co/YanLabs/Seed-OSS-36B-Instruct-MPOA"&gt;https://huggingface.co/YanLabs/Seed-OSS-36B-Instruct-MPOA&lt;/a&gt;&lt;br /&gt; Model: YanLabs/Seed-OSS-36B-Instruct-MPOA&lt;br /&gt; Technique: jim-plus/llm-abliteration&lt;br /&gt; Hardware: one A100 GPU via RunPod&lt;/p&gt; &lt;p&gt;GGUF files are now available at:&lt;br /&gt; &lt;a href="https://huggingface.co/YanLabs/Seed-OSS-36B-Instruct-MPOA-GGUF"&gt;https://huggingface.co/YanLabs/Seed-OSS-36B-Instruct-MPOA-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please give it a try ‚Äî any feedback is appreciated!&lt;/p&gt; &lt;p&gt;By the way, I also uploaded&lt;br /&gt; &lt;a href="https://huggingface.co/YanLabs/gemma-3-4b-it-abliterated-normpreserve"&gt;https://huggingface.co/YanLabs/gemma-3-4b-it-abliterated-normpreserve&lt;/a&gt;&lt;br /&gt; and the corresponding GGUF files&lt;br /&gt; (&lt;a href="https://huggingface.co/YanLabs/gemma-3-4b-it-abliterated-normpreserve-GGUF"&gt;https://huggingface.co/YanLabs/gemma-3-4b-it-abliterated-normpreserve-GGUF&lt;/a&gt;)&lt;br /&gt; to my HF repository. Since this is a smaller model, I‚Äôm saving myself some time by not making a dedicated release post.&lt;/p&gt; &lt;h1&gt;Disclaimer&lt;/h1&gt; &lt;p&gt;This model has safety guardrails removed. It is for research purposes only.&lt;br /&gt; Use responsibly and in compliance with applicable laws.&lt;/p&gt; &lt;h1&gt;About Me&lt;/h1&gt; &lt;p&gt;I'm an LLM enthusiast and practicing lawyer based in Shanghai.&lt;br /&gt; If your AI company needs legal services (domestic or international), feel free to reach out:&lt;/p&gt; &lt;p&gt;üìß [&lt;a href="mailto:ruiqingyan@outlook.com"&gt;ruiqingyan@outlook.com&lt;/a&gt;](mailto:&lt;a href="mailto:ruiqingyan@outlook.com"&gt;ruiqingyan@outlook.com&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Happy experimenting! üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Perfect_Biscotti_476"&gt; /u/Perfect_Biscotti_476 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkqzmf/i_cooked_mpoa_abliterated_seedoss36binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkqzmf/i_cooked_mpoa_abliterated_seedoss36binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkqzmf/i_cooked_mpoa_abliterated_seedoss36binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T12:45:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk3znw</id>
    <title>Microsoft analyzed 37.5 million AI conversations in 2025.</title>
    <updated>2025-12-11T17:50:31+00:00</updated>
    <author>
      <name>/u/Karam1234098</name>
      <uri>https://old.reddit.com/user/Karam1234098</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3znw/microsoft_analyzed_375_million_ai_conversations/"&gt; &lt;img alt="Microsoft analyzed 37.5 million AI conversations in 2025." src="https://b.thumbs.redditmedia.com/I0gHntPEl9oW4aCmQGgV1BpHx_Jt0xMYNUHzNOY1Pys.jpg" title="Microsoft analyzed 37.5 million AI conversations in 2025." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft just released their &amp;quot;Copilot Usage Report 2025,&amp;quot; analyzing de-identified data to see how people actually use AI in their daily lives. The results are surprisingly human. Here are the most interesting graphs and takeaways from the report:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The &amp;quot;Work Hard, Play Hard&amp;quot; Split&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;People have distinct modes for the week vs. the weekend.&lt;/p&gt; &lt;p&gt;View Graph: Programming vs. Gaming&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Insight: In August, there was a perfect crossover. &amp;quot;Programming&amp;quot; queries rise steadily from Monday to Friday, then tank on Saturday/Sunday. &amp;quot;Gaming&amp;quot; does the exact opposite, dominating the weekends.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;The 2 AM Philosophy Club&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The topics we talk about change drastically depending on the time of day.&lt;/p&gt; &lt;p&gt;View Graph: Topic by Hour of Day&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Insight: This radial chart shows that &amp;quot;Travel&amp;quot; queries peak during standard commuting hours. However, &amp;quot;Religion and Philosophy&amp;quot; sees a massive spike in the early morning hours. If you're asking AI about the nature of existence at 3 AM, you aren't alone.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;The Valentine's Day Panic&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;February data shows a very specific narrative arc.&lt;/p&gt; &lt;p&gt;View Graph: February Topic Trends&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Insight: &amp;quot;Personal Growth&amp;quot; topics peak in the days leading up to Valentine's Day (people trying to improve themselves?), while &amp;quot;Relationship&amp;quot; queries spike on the day itself (people needing immediate advice).&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;Health is King on Mobile&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;When we are on our phones, we are almost always worried about our health.&lt;/p&gt; &lt;p&gt;View Graph: Top Mobile Topics&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Insight: No matter the month, &amp;quot;Health&amp;quot; is consistently the #1 topic for mobile users, far outpacing entertainment or productivity. TL;DR: We use AI to code during the week, survive relationships in February, and serve as a therapist/philosopher late at night.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Source: &lt;a href="https://microsoft.ai/news/its-about-time-the-copilot-usage-report-2025/?utm_source=alphasignal&amp;amp;utm_campaign=2025-12-11&amp;amp;lid=bpzfIvhThUltNeQ9&amp;amp;hl=en-GB"&gt;Microsoft AI - The Copilot Usage Report 2025 &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Karam1234098"&gt; /u/Karam1234098 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pk3znw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3znw/microsoft_analyzed_375_million_ai_conversations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk3znw/microsoft_analyzed_375_million_ai_conversations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T17:50:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkg4iy</id>
    <title>Typical performance of gpt-oss-120b on consumer hardware?</title>
    <updated>2025-12-12T02:20:52+00:00</updated>
    <author>
      <name>/u/Diligent-Culture-432</name>
      <uri>https://old.reddit.com/user/Diligent-Culture-432</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is this typical performance, or are there ways to optimize tps even further?&lt;/p&gt; &lt;p&gt;11-12 tps on gpt-oss-120b on 32GB VRAM (2x5060Ti) &amp;amp; 128GB DDR4 RAM&lt;/p&gt; &lt;p&gt;- Intel i7-11700&lt;/p&gt; &lt;p&gt;- 1x 5060Ti 16gb on PCIe x16&lt;/p&gt; &lt;p&gt;- 1x 5060Ti 16gb on PCIe x4&lt;/p&gt; &lt;p&gt;- 4x 32 GB DDR4-3200 RAM (actually appears to be running at 2400 on checking task manager)&lt;/p&gt; &lt;p&gt;- Running on LM Studio&lt;/p&gt; &lt;p&gt;- 32k context&lt;/p&gt; &lt;p&gt;- experts offloaded to CPU&lt;/p&gt; &lt;p&gt;- 36/36 GPU offloaded&lt;/p&gt; &lt;p&gt;- flash attention enabled&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diligent-Culture-432"&gt; /u/Diligent-Culture-432 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkg4iy/typical_performance_of_gptoss120b_on_consumer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkg4iy/typical_performance_of_gptoss120b_on_consumer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkg4iy/typical_performance_of_gptoss120b_on_consumer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T02:20:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkhx0l</id>
    <title>whats everyones thoughts on devstral small 24b?</title>
    <updated>2025-12-12T03:46:24+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Idk if llamacpp is broken for it but my experience is not too great. &lt;/p&gt; &lt;p&gt;Tried creating a snake game and it failed to even start. Considered that maybe the model is more focused on solving problems so I gave it a hard leetcode problem that imo it shouldve been trained on but when it tried to solve it, failed...which gptoss 20b and qwen30b a3b both completed successfully. &lt;/p&gt; &lt;p&gt;lmk if theres a bug the quant I used was unsloth dynamic 4bit&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhx0l/whats_everyones_thoughts_on_devstral_small_24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhx0l/whats_everyones_thoughts_on_devstral_small_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhx0l/whats_everyones_thoughts_on_devstral_small_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T03:46:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pjvtgn</id>
    <title>Leaked footage from Meta's post-training strategy meeting.</title>
    <updated>2025-12-11T12:02:11+00:00</updated>
    <author>
      <name>/u/YouCanMake1t</name>
      <uri>https://old.reddit.com/user/YouCanMake1t</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjvtgn/leaked_footage_from_metas_posttraining_strategy/"&gt; &lt;img alt="Leaked footage from Meta's post-training strategy meeting." src="https://preview.redd.it/2cbgowoj0i6g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8274908702ea2b4e3ee76f7741b54aa24bef73d7" title="Leaked footage from Meta's post-training strategy meeting." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YouCanMake1t"&gt; /u/YouCanMake1t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2cbgowoj0i6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjvtgn/leaked_footage_from_metas_posttraining_strategy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pjvtgn/leaked_footage_from_metas_posttraining_strategy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T12:02:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pko44g</id>
    <title>Benchmark Fatigue - How do you evaluate new models for yourself?</title>
    <updated>2025-12-12T09:55:10+00:00</updated>
    <author>
      <name>/u/Funny-Clock1582</name>
      <uri>https://old.reddit.com/user/Funny-Clock1582</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am getting more and more the impression that the benchmark results published for new models are not even close to the experience i make with models.&lt;br /&gt; Maybe its time for me to create some standard questions for a first quick evaluation of new models just for myself.&lt;br /&gt; Do you guys do this and do you have prompts you feel are helpful in your experience?&lt;/p&gt; &lt;p&gt;Cheers Wolfram &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Funny-Clock1582"&gt; /u/Funny-Clock1582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pko44g/benchmark_fatigue_how_do_you_evaluate_new_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pko44g/benchmark_fatigue_how_do_you_evaluate_new_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pko44g/benchmark_fatigue_how_do_you_evaluate_new_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T09:55:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkphs3</id>
    <title>Undo for destructive shell commands used by AI agents (SafeShell)</title>
    <updated>2025-12-12T11:22:14+00:00</updated>
    <author>
      <name>/u/qhkmdev90</name>
      <uri>https://old.reddit.com/user/qhkmdev90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As local AI agents start running shell commands directly, we probably need a better way to protect the filesystem than sandboxes or confirmation prompts.&lt;/p&gt; &lt;p&gt;I built a small open source tool called SafeShell that makes destructive commands reversible (rm, mv, cp, chmod, chown).&lt;/p&gt; &lt;p&gt;It automatically checkpoints before a command runs, so if an agent deletes or mutates the wrong files, you can roll back instantly.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;rm -rf ./build safeshell rollback --last &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;No sandbox, VM, or root&lt;/p&gt; &lt;p&gt;Hard-link snapshots (minimal overhead)&lt;/p&gt; &lt;p&gt;Single Go binary (macOS + Linux)&lt;/p&gt; &lt;p&gt;MCP support&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/qhkm/safeshell"&gt;https://github.com/qhkm/safeshell&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious how others are handling filesystem safety for local agents.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qhkmdev90"&gt; /u/qhkmdev90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkphs3/undo_for_destructive_shell_commands_used_by_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkphs3/undo_for_destructive_shell_commands_used_by_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkphs3/undo_for_destructive_shell_commands_used_by_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T11:22:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkbwco</id>
    <title>EQ-Bench updates: Gpt-5.2, Opus 4.5, Mistral Large 3 and Nanbeige4-3B</title>
    <updated>2025-12-11T23:06:43+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkbwco/eqbench_updates_gpt52_opus_45_mistral_large_3_and/"&gt; &lt;img alt="EQ-Bench updates: Gpt-5.2, Opus 4.5, Mistral Large 3 and Nanbeige4-3B" src="https://b.thumbs.redditmedia.com/jSd0TbO_srnflwRt8ojBram2pVNjfZ3rVlBCwbCJcHQ.jpg" title="EQ-Bench updates: Gpt-5.2, Opus 4.5, Mistral Large 3 and Nanbeige4-3B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://eqbench.com"&gt;https://eqbench.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gpt-5.2 writing samples: &lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/gpt-5.2.html"&gt;https://eqbench.com/results/creative-writing-v3/gpt-5.2.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;opus-4.5 writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/claude-opus-4-5-20251101.html"&gt;https://eqbench.com/results/creative-writing-v3/claude-opus-4-5-20251101.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;mistral-large-3 writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/mistralai__Mistral-Large-3-675B-Instruct-2512.html"&gt;https://eqbench.com/results/creative-writing-v3/mistralai__Mistral-Large-3-675B-Instruct-2512.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;nanbeige4-3b writing samples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/Nanbeige__Nanbeige4-3B-Thinking-2511.html"&gt;https://eqbench.com/results/creative-writing-v3/Nanbeige__Nanbeige4-3B-Thinking-2511.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pkbwco"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkbwco/eqbench_updates_gpt52_opus_45_mistral_large_3_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkbwco/eqbench_updates_gpt52_opus_45_mistral_large_3_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T23:06:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pjw7rj</id>
    <title>Mistral‚Äôs Vibe CLI now supports a 200K token context window (previously 100K)</title>
    <updated>2025-12-11T12:23:44+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjw7rj/mistrals_vibe_cli_now_supports_a_200k_token/"&gt; &lt;img alt="Mistral‚Äôs Vibe CLI now supports a 200K token context window (previously 100K)" src="https://external-preview.redd.it/ZnNsb2d0dzFpazZnMZt0kKC274AvCvOpM9k0UQCIyB1BQvPjsN5T3o1kO8eQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=841cfe71df83eebc90bd5a8915c65e4a8693db6c" title="Mistral‚Äôs Vibe CLI now supports a 200K token context window (previously 100K)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4nxnq6w1ik6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pjw7rj/mistrals_vibe_cli_now_supports_a_200k_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pjw7rj/mistrals_vibe_cli_now_supports_a_200k_token/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T12:23:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkndwc</id>
    <title>Chat bots up to 24B</title>
    <updated>2025-12-12T09:06:00+00:00</updated>
    <author>
      <name>/u/PsychologicalMud210</name>
      <uri>https://old.reddit.com/user/PsychologicalMud210</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I like to chat about random subjects with AI. It serves more as an aid to thought and sometimes they are really helpful. Subjects may be sensitive, so I like to run local. &lt;/p&gt; &lt;p&gt;What are the best models up to about 24B that I can use? In your experience, what exactly this model does best?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PsychologicalMud210"&gt; /u/PsychologicalMud210 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkndwc/chat_bots_up_to_24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkndwc/chat_bots_up_to_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkndwc/chat_bots_up_to_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T09:06:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkjx5y</id>
    <title>Agentic coding with 32GB of VRAM.. is it doable?</title>
    <updated>2025-12-12T05:29:18+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Theres some solid models that run at this size, but for agentic coding I consider 60K context the bare minimum to get a good number of iterations in on a microservice.&lt;/p&gt; &lt;p&gt;Assuming I can tolerate Q8/Q8 kv cache quantization.. what's the best model I can run that'll fit 60K confidently?&lt;/p&gt; &lt;p&gt;Qwen3-VL-32B runs, but to hit 60K I need to drop down to iq4_xs, and that's introducing frequent errors that Q5 and Q6 don't encounter.&lt;/p&gt; &lt;p&gt;Qwen3-30B-Coder is in a somewhat similar spot only it's faster and works slightly worse with these tools.&lt;/p&gt; &lt;p&gt;Qwen3-Next works great but since I need CPU offloading to start with, prompt processing quickly becomes unacceptably slow.&lt;/p&gt; &lt;p&gt;Anything smaller I've tried fails to adhere to the lengthy 10k token system prompts or enters an infinite loop.&lt;/p&gt; &lt;p&gt;Any suggestions? Is it doable?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkjx5y/agentic_coding_with_32gb_of_vram_is_it_doable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkjx5y/agentic_coding_with_32gb_of_vram_is_it_doable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkjx5y/agentic_coding_with_32gb_of_vram_is_it_doable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T05:29:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pko16f</id>
    <title>7B MoE with 1B active</title>
    <updated>2025-12-12T09:49:48+00:00</updated>
    <author>
      <name>/u/lossless-compression</name>
      <uri>https://old.reddit.com/user/lossless-compression</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found that models in that range are relatively rare,I found some models such as (may not be exactly 7B and exactly 1B activated but in that range) are&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1- Granite-4-tiny&lt;/li&gt; &lt;li&gt;2- LFM2-8B-A1B&lt;/li&gt; &lt;li&gt;3- Trinity-nano 6B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Most of SLMs that are in that range are made of high amount of experts (tiny experts) where larger amount of experts gets activated but the overall parameters activated are ~1B so the model can specialize well.&lt;/p&gt; &lt;p&gt;I really wonder why that range isn't popular,I tried those models and Trinity nano is a very good researcher and it got a good character too and I asked a few general question it answered well,LFM feels like a RAG model even the standard one,it feels so robotic and answers are not the best,even the 350M can be coherent but it still feels like a RAG model, didn't test Granite 4 tiny yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lossless-compression"&gt; /u/lossless-compression &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pko16f/7b_moe_with_1b_active/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pko16f/7b_moe_with_1b_active/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pko16f/7b_moe_with_1b_active/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T09:49:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkpsee</id>
    <title>Training an LLM only on 1800s London texts - 90GB dataset</title>
    <updated>2025-12-12T11:40:11+00:00</updated>
    <author>
      <name>/u/Remarkable-Trick-177</name>
      <uri>https://old.reddit.com/user/Remarkable-Trick-177</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, you may have seen a few of my posts here a couple months ago. If not, hi. I‚Äôm working on an open source project called TimeCapsuleLLM, where I train LLMs from scratch using only 1800-1875 London texts.&lt;/p&gt; &lt;p&gt;Until recently most of my work has been done on a small scale but over the past 3 months I‚Äôve been working on a much larger dataset for the next model. My newest dataset is 90GB with 135,000 documents, it contains basically every usable document that I could find on the Internet Archive for that time period. &lt;/p&gt; &lt;p&gt;Before doing any training, I ran an inspection across every file and generated a bias report covering temporal bias, gender/pronoun bias and geographic bias. Given the time period it‚Äôs strongly biased, but it‚Äôs important to study this. You can find the report on my GitHub if anyone wants to take a look. I‚Äôve also trained a small evaluation model on a 15GB subset to evaluate the dataset before I scale up to all 90GB. It‚Äôs a LlaMA style model (300M parameters) trained to 10K steps. Example output: &lt;/p&gt; &lt;p&gt;Prompt: Who is Charles Dickens?&lt;/p&gt; &lt;p&gt;Output with fixed spacing: ‚ÄúWho is Charles Dickens? Does that work more of his excellent stirring, in his plays, in the Great Company's farm? What I have yet to quote from Jack Pickett? Do you not know that they were a species of galloping, or sawing of their breasts, or what was to be done about the time when Jackson was looking on the window? What is the success of an Englishman, and which his son has not been discovering to me, whereby to accomplish such a weight? Did you ever make a passage into the old roadway, or to an anchor-breeze at the foot of our boat, which you must leave us? The fact is, that whether the wind would rise up from the plain on Saturday night or noontide, or till the north, or otherwise, we shall be compelled to describe a formidable barrier, with the same effects as the present. In this situation, at least, it is not too much to say that we have left that room. I believe there are three copies in the 'Five Hundred-fold,' to be referred to, as the first number of our readers who wish to.‚Äù&lt;/p&gt; &lt;p&gt;This type of output is expected since 10,000 steps is very early and it‚Äôs not a QA model. The model has already learned long, winding sentence structures, but can‚Äôt connect ideas logically yet. The main goal here was to see how clean the output would be. &lt;/p&gt; &lt;p&gt;One issue that came up was with the tokenizer, it over-split the text, splitting words into individual characters and subparts. So the model by default gives output like this: &lt;/p&gt; &lt;p&gt;Original output: ‚ÄúW ho is Charles D ic ens ? D oes that work more of h ise x cell ent st ir ring , in his pl ays , int he G reat C omp any 's f arm ? What I have y et to qu ote from J ack P ick ett ?‚Äù&lt;/p&gt; &lt;p&gt;It doubled the tokens for the same amount of data, making learning harder. Next steps are training another eval model and then scaling to the full 90GB dataset for a 1.2B parameter model. The eval model is already on Hugging Face and you can find a run script for it on my GitHub. I‚Äôll upload the 15GB subset to Hugging Face once the tokenizer is corrected.&lt;/p&gt; &lt;p&gt;I also want to thank everyone in this subreddit. This is the only place I‚Äôve shared the project other than github, and a lot of the early guidance came directly from here. I really appreciate how generous people here have been with advice. More updates soon.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/haykgrigo3/TimeCapsuleLLM"&gt;haykgrigo3/TimeCapsuleLLM: A LLM trained only on data from certain time periods to reduce modern bias&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/haykgrigorian/v2mini-eval1"&gt;haykgrigorian/v2mini-eval1 ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable-Trick-177"&gt; /u/Remarkable-Trick-177 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpsee/training_an_llm_only_on_1800s_london_texts_90gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T11:40:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkflfw</id>
    <title>Run Mistral Devstral 2 locally Guide + Fixes! (25GB RAM) - Unsloth</title>
    <updated>2025-12-12T01:56:20+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkflfw/run_mistral_devstral_2_locally_guide_fixes_25gb/"&gt; &lt;img alt="Run Mistral Devstral 2 locally Guide + Fixes! (25GB RAM) - Unsloth" src="https://preview.redd.it/1f2wim2zgl6g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=884f4f99a6d0cca98f924c0f9b62f3ea56ac95cb" title="Run Mistral Devstral 2 locally Guide + Fixes! (25GB RAM) - Unsloth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1f2wim2zgl6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkflfw/run_mistral_devstral_2_locally_guide_fixes_25gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkflfw/run_mistral_devstral_2_locally_guide_fixes_25gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T01:56:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkhzf0</id>
    <title>Reverse-Engineering the RK3588 NPU: Hacking Memory Limits to run massive Vision Transformers</title>
    <updated>2025-12-12T03:49:27+00:00</updated>
    <author>
      <name>/u/one_does_not_just</name>
      <uri>https://old.reddit.com/user/one_does_not_just</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I worked on a &amp;quot;fun&amp;quot; project for my grad school class. I decided to write a blog post about it, maybe its useful to someone who is dealing with problems deploying vision transformers on edge devices&lt;/p&gt; &lt;p&gt;&lt;a href="https://amohan.dev/blog/2025/shard-optimizing-vision-transformers-edge-npu/"&gt;https://amohan.dev/blog/2025/shard-optimizing-vision-transformers-edge-npu/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Removed massive from title, but reddit won't let me change title, sorry about that&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/one_does_not_just"&gt; /u/one_does_not_just &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhzf0/reverseengineering_the_rk3588_npu_hacking_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhzf0/reverseengineering_the_rk3588_npu_hacking_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhzf0/reverseengineering_the_rk3588_npu_hacking_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T03:49:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkhudf</id>
    <title>US Administration Issues Executive Order Opposing State-Level Regulation of AI Industry</title>
    <updated>2025-12-12T03:42:45+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The EO:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.whitehouse.gov/presidential-actions/2025/12/eliminating-state-law-obstruction-of-national-artificial-intelligence-policy/"&gt;https://www.whitehouse.gov/presidential-actions/2025/12/eliminating-state-law-obstruction-of-national-artificial-intelligence-policy/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My take: The EO orders the US AG to set up a task force to sue states which have legislated their own AI industry regulations, orders other agencies to prepare a report on how states might be denied federal funds, and orders that a set of recommendations be made to Congress to draft and pass new laws.&lt;/p&gt; &lt;p&gt;It seems like Christmas came early for commercial inference services, this year.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhudf/us_administration_issues_executive_order_opposing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhudf/us_administration_issues_executive_order_opposing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkhudf/us_administration_issues_executive_order_opposing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T03:42:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pk0ubn</id>
    <title>New in llama.cpp: Live Model Switching</title>
    <updated>2025-12-11T15:49:43+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk0ubn/new_in_llamacpp_live_model_switching/"&gt; &lt;img alt="New in llama.cpp: Live Model Switching" src="https://external-preview.redd.it/8Hy799ws5wvJKYaRb__KN0TGXYxiPxKG6PuG-1SlIWg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2a43f5804fb810225237c9c37046b91c9bbb6451" title="New in llama.cpp: Live Model Switching" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/ggml-org/model-management-in-llamacpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pk0ubn/new_in_llamacpp_live_model_switching/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pk0ubn/new_in_llamacpp_live_model_switching/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-11T15:49:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkdkjo</id>
    <title>Agentic Local AI on CPU = Mistral Vibe + Granite-4-h-1b</title>
    <updated>2025-12-12T00:22:10+00:00</updated>
    <author>
      <name>/u/PotentialFunny7143</name>
      <uri>https://old.reddit.com/user/PotentialFunny7143</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkdkjo/agentic_local_ai_on_cpu_mistral_vibe_granite4h1b/"&gt; &lt;img alt="Agentic Local AI on CPU = Mistral Vibe + Granite-4-h-1b" src="https://external-preview.redd.it/NDYwbGgydmYybzZnMf2LvdJmBzIyNzEDfN0eOt2yDrF46dRxJq4WcX4O0NUM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4363f44505584728345cc48958c232a7ab91036f" title="Agentic Local AI on CPU = Mistral Vibe + Granite-4-h-1b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A a3b LLM is all you need :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PotentialFunny7143"&gt; /u/PotentialFunny7143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vewmcluf2o6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkdkjo/agentic_local_ai_on_cpu_mistral_vibe_granite4h1b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkdkjo/agentic_local_ai_on_cpu_mistral_vibe_granite4h1b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T00:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkidf6</id>
    <title>What is the smartest uncensored nsfw LLM you can run with 12GB VRAM and 32GB RAM?</title>
    <updated>2025-12-12T04:08:01+00:00</updated>
    <author>
      <name>/u/Dex921</name>
      <uri>https://old.reddit.com/user/Dex921</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't know if it's allowed, but I am asking about ALL available LLMs including ones that are closed source and cannot be run locally (like chatgpt or gemini, and in that case obviously the ram limit doesn't apply)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dex921"&gt; /u/Dex921 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkidf6/what_is_the_smartest_uncensored_nsfw_llm_you_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkidf6/what_is_the_smartest_uncensored_nsfw_llm_you_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkidf6/what_is_the_smartest_uncensored_nsfw_llm_you_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T04:08:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pkpxss</id>
    <title>Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face</title>
    <updated>2025-12-12T11:49:10+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/"&gt; &lt;img alt="Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face" src="https://preview.redd.it/7r3bnj5ugr6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c3d5909063dd5ce912e8ebc203168db53b765be" title="Someone from NVIDIA made a big mistake and uploaded the parent folder of their upcoming model on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Xeophon on ùïè: &lt;a href="https://x.com/xeophon_/status/1999394570967089630"&gt;https://x.com/xeophon_/status/1999394570967089630&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7r3bnj5ugr6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-12T11:49:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
