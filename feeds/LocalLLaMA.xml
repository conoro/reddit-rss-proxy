<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-13T20:34:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ng15en</id>
    <title>Where can I find training data for intent classification (chat-to-SQL bot)?</title>
    <updated>2025-09-13T15:59:53+00:00</updated>
    <author>
      <name>/u/Small-Inevitable6185</name>
      <uri>https://old.reddit.com/user/Small-Inevitable6185</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m building a &lt;strong&gt;chat-to-SQL system&lt;/strong&gt; (read-only, no inserts/updates/deletes). I want to train a &lt;strong&gt;DistilBERT-based intent classifier&lt;/strong&gt; that categorizes user queries into three classes:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Description type answer&lt;/strong&gt; → user asks about schema (e.g., “What columns are in the customers table?”)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SQL-based query filter answer&lt;/strong&gt; → user asks for data retrieval (e.g., “Show me all customers from New York.”)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Both&lt;/strong&gt; → user wants explanation + query together (e.g., “Which column stores customer age, and show me all customers older than 30?”)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My problem: I’m not sure where to get a &lt;strong&gt;dataset&lt;/strong&gt; to train this classifier. Most datasets I’ve found (ATIS, Spider, WikiSQL) are great for text-to-SQL mapping, but they don’t label queries into “description / query / both.”&lt;/p&gt; &lt;p&gt;Should I:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Try adapting text-to-SQL datasets (Spider/WikiSQL) by manually labeling a subset into my categories?&lt;/li&gt; &lt;li&gt;Or are there existing intent classification datasets closer to this use case that I might be missing?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any guidance or pointers to datasets/resources would be super helpful&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Small-Inevitable6185"&gt; /u/Small-Inevitable6185 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng15en/where_can_i_find_training_data_for_intent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng15en/where_can_i_find_training_data_for_intent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ng15en/where_can_i_find_training_data_for_intent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T15:59:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng4nlb</id>
    <title>Codestral 22B-V01</title>
    <updated>2025-09-13T18:16:43+00:00</updated>
    <author>
      <name>/u/StringInter630</name>
      <uri>https://old.reddit.com/user/StringInter630</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running this on llama.cpp both 8 and 6 Quant's. Runs at 50tk/s on RTX 5090 but very hot, peaking regularly at 99% utilization and 590-600+ watts for basic python file analysis and response. I'm afraid of this thing. I feel like it's going to set the house on fire. I don't have this problem with gemma-27b or even llama-70b ggufs.How do I tamp this thing down? I don't need 50tk/sec. Would be happy with half of that. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StringInter630"&gt; /u/StringInter630 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng4nlb/codestral_22bv01/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng4nlb/codestral_22bv01/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ng4nlb/codestral_22bv01/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T18:16:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf9x9m</id>
    <title>Apple stumbled into succes with MLX</title>
    <updated>2025-09-12T17:50:47+00:00</updated>
    <author>
      <name>/u/Alarming-Ad8154</name>
      <uri>https://old.reddit.com/user/Alarming-Ad8154</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-next 80b-a3b is out in mlx on hugging face, MLX already supports it. Open source contributors got this done within 24 hrs. Doing things apple itself couldn’t ever do quickly, simply because the call to support, or not support, specific Chinese AI companies, who’s parent company may or may not be under specific US sanctions would take months if it had the apple brand anywhere near it If apple hadn’t let MLX sort of evolve in its research arm while they tried, and failed, to manage “apple intelligence”, and pulled it into the company, closed it, centralized it, they would be nowhere now. It’s really quite a story arc and I feel with their new M5 chip design having matmul cores (faster prompt processing) they’re actually leaning into it! Apple is never the choice for sort of “go at it on your own” tinkerers, but now it actually is…&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarming-Ad8154"&gt; /u/Alarming-Ad8154 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf9x9m/apple_stumbled_into_succes_with_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf9x9m/apple_stumbled_into_succes_with_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf9x9m/apple_stumbled_into_succes_with_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T17:50:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng6d4t</id>
    <title>Is it possible to recreate a dnd party with local ai similar to what dougdoug does?</title>
    <updated>2025-09-13T19:24:26+00:00</updated>
    <author>
      <name>/u/No_Strawberry_8719</name>
      <uri>https://old.reddit.com/user/No_Strawberry_8719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng6d4t/is_it_possible_to_recreate_a_dnd_party_with_local/"&gt; &lt;img alt="Is it possible to recreate a dnd party with local ai similar to what dougdoug does?" src="https://preview.redd.it/jpel2ck1gzof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b885e3443e6af1029686acadbe11356371867c79" title="Is it possible to recreate a dnd party with local ai similar to what dougdoug does?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just curious if its possible to use local ai to play dnd with or some other game? How might i achieve such results kinda like how dougdoug plays.&lt;/p&gt; &lt;p&gt;What would you suggest or advise?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Strawberry_8719"&gt; /u/No_Strawberry_8719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jpel2ck1gzof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng6d4t/is_it_possible_to_recreate_a_dnd_party_with_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ng6d4t/is_it_possible_to_recreate_a_dnd_party_with_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T19:24:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng7543</id>
    <title>LM Studio can't detect RTX 5090 after system wake from suspend - Ubuntu Linux</title>
    <updated>2025-09-13T19:57:02+00:00</updated>
    <author>
      <name>/u/OldEffective9726</name>
      <uri>https://old.reddit.com/user/OldEffective9726</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng7543/lm_studio_cant_detect_rtx_5090_after_system_wake/"&gt; &lt;img alt="LM Studio can't detect RTX 5090 after system wake from suspend - Ubuntu Linux" src="https://b.thumbs.redditmedia.com/g2eN5YZ-RS9P_1GRK7zT-ldjXZguoB0DCTg9rg2ZbYw.jpg" title="LM Studio can't detect RTX 5090 after system wake from suspend - Ubuntu Linux" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else experiencing this issue? Here are the details:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RTX 5090 32GB (Zotac)&lt;/li&gt; &lt;li&gt;Ubuntu Linux&lt;/li&gt; &lt;li&gt;NVIDIA driver 580 (also tried 575)&lt;/li&gt; &lt;li&gt;LM Studio&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; After my system goes into suspend mode, LM Studio loses detection of the GPU when I wake it up. This happens even after properly closing the AI model and quitting LM Studio before suspend.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I've tried:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Logging out and back in (doesn't work)&lt;/li&gt; &lt;li&gt;Only fix is a full system restart each time&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Additional info:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU shows no warning lights and appears healthy&lt;/li&gt; &lt;li&gt;nvidia -smi works no problem&lt;/li&gt; &lt;li&gt;Never had this issue with my previous RX 7900XT 20GB&lt;/li&gt; &lt;li&gt;Problem is consistent and reproducible&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has anyone found a solution that doesn't require restarting? Maybe a command to reinitialize the GPU or restart specific services?&lt;/p&gt; &lt;p&gt;Thanks for any help!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mapyjxhpmzof1.png?width=2627&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=faaa2865bd0179242670e6617730515bbabc89d6"&gt;https://preview.redd.it/mapyjxhpmzof1.png?width=2627&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=faaa2865bd0179242670e6617730515bbabc89d6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OldEffective9726"&gt; /u/OldEffective9726 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng7543/lm_studio_cant_detect_rtx_5090_after_system_wake/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng7543/lm_studio_cant_detect_rtx_5090_after_system_wake/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ng7543/lm_studio_cant_detect_rtx_5090_after_system_wake/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T19:57:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng7lid</id>
    <title>Run Qwen3-Next-80B-A3B-Instruct-8bit in a single line of code on Mac with mlx-lm - 45 tokens/s!</title>
    <updated>2025-09-13T20:16:10+00:00</updated>
    <author>
      <name>/u/DomeGIS</name>
      <uri>https://old.reddit.com/user/DomeGIS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you're on a Mac, you can run Qwen's latest Qwen3-Next-80B-A3B-Instruct-8bit in a single line of code! The script lives on my github as a gist and is then chained to uv (my favorite package manager by far), so you don't even need to create a persistent env!&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl -sL https://gist.githubusercontent.com/do-me/34516f7f4d8cc701da823089b09a3359/raw/5f3b7e92d3e5199fd1d4f21f817a7de4a8af0aec/prompt.py | uv run --with git+https://github.com/ml-explore/mlx-lm.git python - --prompt &amp;quot;What is the meaning of life?&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you rerun the script the model will be cached on your disk (like in this video). I usually get 45-50 tokens-per-sec which is pretty much on par with ChatGPT. But all privately on your device!&lt;/p&gt; &lt;p&gt;Note that this is the full version and depending on your VRAM you might want to go with a smaller version. I cut out some seconds of initial load (like 20 secs) in the video but the generation speed is 1:1. So if downloaded, it takes something like 48s in total with this cold start on an M3 Max. Didn't test a new prompt yet when the model is already loaded.&lt;/p&gt; &lt;p&gt;Disclaimer: You should never run remote code like this from random folks on the internet. Check out the gist for a safer 2-line solution: &lt;a href="https://gist.github.com/do-me/34516f7f4d8cc701da823089b09a3359"&gt;https://gist.github.com/do-me/34516f7f4d8cc701da823089b09a3359&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ng7lid/video/r9zda34lozof1/player"&gt;https://reddit.com/link/1ng7lid/video/r9zda34lozof1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DomeGIS"&gt; /u/DomeGIS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng7lid/run_qwen3next80ba3binstruct8bit_in_a_single_line/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng7lid/run_qwen3next80ba3binstruct8bit_in_a_single_line/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ng7lid/run_qwen3next80ba3binstruct8bit_in_a_single_line/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T20:16:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfhbzv</id>
    <title>Ring-mini-2.0 16B 1.4b MoE</title>
    <updated>2025-09-12T22:46:15+00:00</updated>
    <author>
      <name>/u/HilLiedTroopsDied</name>
      <uri>https://old.reddit.com/user/HilLiedTroopsDied</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfhbzv/ringmini20_16b_14b_moe/"&gt; &lt;img alt="Ring-mini-2.0 16B 1.4b MoE" src="https://external-preview.redd.it/k_1ZiAjClo_PWHpZM0iAYeW3wPAsQ_ZQE2cc_xW7-3o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e8a42ffff0966a52b0cd60044e86cd61473738b" title="Ring-mini-2.0 16B 1.4b MoE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HilLiedTroopsDied"&gt; /u/HilLiedTroopsDied &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-mini-2.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfhbzv/ringmini20_16b_14b_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfhbzv/ringmini20_16b_14b_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T22:46:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng0fmv</id>
    <title>PSA/RFC: KV Cache quantization forces excess processing onto CPU in llama.cpp</title>
    <updated>2025-09-13T15:30:51+00:00</updated>
    <author>
      <name>/u/MutantEggroll</name>
      <uri>https://old.reddit.com/user/MutantEggroll</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for additional comments/suggestions for optimization, since I have a very small sample size and have only been playing with GPT-OSS-120B.&lt;/p&gt; &lt;p&gt;I was struggling with GPT-OSS-120B despite my relatively high-spec hardware, only getting ~90tk/s prompt and ~10tk/s inference at 10k context. Turns out this was because quantizing the KV cache in llama.cpp seems to force the CPU to take on much more responsibility than the GPU. After only removing the KV cache quantization options, I'm now getting ~1200tk/s prompt and ~35tk/s inference at 50k context. System specs/llama.cpp commands below for reference:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;System:&lt;/strong&gt;&lt;br /&gt; CPU: Intel i9-13900K (Hyper-Threading disabled)&lt;br /&gt; RAM: 64GB DDR5-6000 (OC'd from DDR5-5400)&lt;br /&gt; GPU: NVIDIA RTX 5090 (undervolted to 890mV, driver 581.15)&lt;br /&gt; OS: Windows 11 Pro 24H2 (Build 26100.6584)&lt;br /&gt; llama.cpp Release: CUDA-12 B6318&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Initial Command (90tk/s prompt, 10tk/s inference @ 10k context):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server --threads 8 --cpu-range 0-7 --cpu-strict 1 --prio 2 --flash-attn --n-gpu-layers 999 --offline --model &amp;quot;\path\to\unsloth\gpt-oss-120b-GGUF\gpt-oss-120b-F16.gguf&amp;quot; --no-mmap --n-cpu-moe 22 --ctx-size 65536 --cache-type-k q4_0 --cache-type-v q4_0 --batch-size 2048 --ubatch-size 2048 --jinja &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Improved Command (1200tk/s prompt, 35tk/s inference @ 50k context):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server --threads 8 --cpu-range 0-7 --cpu-strict 1 --prio 2 --flash-attn --n-gpu-layers 999 --offline --model &amp;quot;\path\to\unsloth\gpt-oss-120b-GGUF\gpt-oss-120b-F16.gguf&amp;quot; --no-mmap --n-cpu-moe 22 --ctx-size 65536 --batch-size 2048 --ubatch-size 2048 --jinja &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hope this helps someone eke out a few more tk/s!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MutantEggroll"&gt; /u/MutantEggroll &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng0fmv/psarfc_kv_cache_quantization_forces_excess/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng0fmv/psarfc_kv_cache_quantization_forces_excess/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ng0fmv/psarfc_kv_cache_quantization_forces_excess/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T15:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfzgge</id>
    <title>Anyone put together an “oversight agent” on top of Roo Code?</title>
    <updated>2025-09-13T14:52:13+00:00</updated>
    <author>
      <name>/u/Sluggerjt44</name>
      <uri>https://old.reddit.com/user/Sluggerjt44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just came across the idea of agentic swarms and it sounds amazing. The way I understand it, you give a high-level goal and the agents keep working (coding, testing, fixing) until the thing is done.&lt;/p&gt; &lt;p&gt;Right now, I’m using Roo Code with Gemini inside VS Code and it’s pretty great, but I feel like I’m acting as the oversight layer. I have to keep nudging it step by step, almost like being the manager. What I’d love is something that's one level higher like a lightweight “boss agent” that just watches Roo, retries/re-prompts when things fail, and keeps pushing toward the end goal until the small project or app is finished.&lt;/p&gt; &lt;p&gt;From my limited understanding at this point, I'm not looking for a full LangChain/CrewAI setup, just something glue-code simple that could give me that extra hierarchy layer. Has anyone here already built something like this, or is everyone still handling oversight manually?&lt;/p&gt; &lt;p&gt;Would be very help for the little apps I’m trying to build instead of having to watch it constantly for the next step. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sluggerjt44"&gt; /u/Sluggerjt44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfzgge/anyone_put_together_an_oversight_agent_on_top_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfzgge/anyone_put_together_an_oversight_agent_on_top_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfzgge/anyone_put_together_an_oversight_agent_on_top_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T14:52:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nflbh4</id>
    <title>Qwen3max feels like a manager that had to attend sensitivity training</title>
    <updated>2025-09-13T01:54:45+00:00</updated>
    <author>
      <name>/u/Coldaine</name>
      <uri>https://old.reddit.com/user/Coldaine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nflbh4/qwen3max_feels_like_a_manager_that_had_to_attend/"&gt; &lt;img alt="Qwen3max feels like a manager that had to attend sensitivity training" src="https://preview.redd.it/371xjrd89uof1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d1fc0fb0a562a6ddb6d774061259f8ebdfe2969e" title="Qwen3max feels like a manager that had to attend sensitivity training" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really did have someone like this in real life. He was definitely a little bit on the spectrum and didn't get humor at all. People told him to lighten up, and it somehow got even worse when he was trying to be funny. &lt;/p&gt; &lt;p&gt;The rest of my code review did not go as well as the first line, but at least qwen was able to find one good thing about my code. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Coldaine"&gt; /u/Coldaine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/371xjrd89uof1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nflbh4/qwen3max_feels_like_a_manager_that_had_to_attend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nflbh4/qwen3max_feels_like_a_manager_that_had_to_attend/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T01:54:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf7zhq</id>
    <title>Meta released MobileLLM-R1 on Hugging Face</title>
    <updated>2025-09-12T16:35:23+00:00</updated>
    <author>
      <name>/u/Illustrious_Row_9971</name>
      <uri>https://old.reddit.com/user/Illustrious_Row_9971</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf7zhq/meta_released_mobilellmr1_on_hugging_face/"&gt; &lt;img alt="Meta released MobileLLM-R1 on Hugging Face" src="https://preview.redd.it/huchm6bahrof1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f091dea3c1b3bd8cc946d3ae61d24b3e9a2e3a3b" title="Meta released MobileLLM-R1 on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;model: &lt;a href="https://huggingface.co/facebook/MobileLLM-R1-950M"&gt;https://huggingface.co/facebook/MobileLLM-R1-950M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;app (vibe coded): &lt;a href="https://huggingface.co/spaces/akhaliq/MobileLLM-R1-950M"&gt;https://huggingface.co/spaces/akhaliq/MobileLLM-R1-950M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;app was made in: &lt;a href="https://huggingface.co/spaces/akhaliq/anycoder"&gt;https://huggingface.co/spaces/akhaliq/anycoder&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious_Row_9971"&gt; /u/Illustrious_Row_9971 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/huchm6bahrof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf7zhq/meta_released_mobilellmr1_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf7zhq/meta_released_mobilellmr1_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T16:35:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfieif</id>
    <title>vLLM Now Supports Qwen3-Next: Hybrid Architecture with Extreme Efficiency</title>
    <updated>2025-09-12T23:33:53+00:00</updated>
    <author>
      <name>/u/chisleu</name>
      <uri>https://old.reddit.com/user/chisleu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfieif/vllm_now_supports_qwen3next_hybrid_architecture/"&gt; &lt;img alt="vLLM Now Supports Qwen3-Next: Hybrid Architecture with Extreme Efficiency" src="https://external-preview.redd.it/K3rGlpkjbDPCdSyb_xOk55T-rqiVrUIviv6vZoP3TV0.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16235ad425456d8b12ef7e3e92930529dc005885" title="vLLM Now Supports Qwen3-Next: Hybrid Architecture with Extreme Efficiency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let's fire it up!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chisleu"&gt; /u/chisleu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.vllm.ai/2025/09/11/qwen3-next.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfieif/vllm_now_supports_qwen3next_hybrid_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfieif/vllm_now_supports_qwen3next_hybrid_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T23:33:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfqr69</id>
    <title>WarLlama: 2x MI50 LLM MicroATX Server</title>
    <updated>2025-09-13T06:55:54+00:00</updated>
    <author>
      <name>/u/__E8__</name>
      <uri>https://old.reddit.com/user/__E8__</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqr69/warllama_2x_mi50_llm_microatx_server/"&gt; &lt;img alt="WarLlama: 2x MI50 LLM MicroATX Server" src="https://b.thumbs.redditmedia.com/h9qMX2YlXQIKQi4YG23oXTimROWCT0LUz66l7zJkwKg.jpg" title="WarLlama: 2x MI50 LLM MicroATX Server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some ppl on this sub have Ahab-class dreadnoughts rocking a DeepSeek/Kimi high quant. Other have a warhorse w a giant gpu or six (or 16x?). This is my sleek lil &lt;em&gt;warllama&lt;/em&gt;. &lt;/p&gt; &lt;p&gt;It's is not abt the bling-bling; it's abt the ching-ching: how little money I spend building a little power house. It came out comely, but it was meant to be minimalist-- a pure headless Linux box running llama.cpp + rocm (which needs freq reboots from lots of llm usage) w a comfy 64gb vram. Cost of main parts: &lt;span class="md-spoiler-text"&gt;$730&lt;/span&gt;. The bells &amp;amp; whistles prob costs another $200+ nowadays but I bought most of it bf the recent (hyper)inflation/tariff BS. YMMV. &lt;/p&gt; &lt;p&gt;WARNING: I flout every sensible guideline in the &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j09qk1/dell_t640_for_a_4x_3090_build/"&gt;LocalLlama build guidebook&lt;/a&gt;: super tight case, ancient desktop mobo, weird gpus, buggy drivers, even buggier vbioxen, cramped airflow. You'll prob be eaten by a Grue.&lt;/p&gt; &lt;h2&gt;Write-Up Sections:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;PC Parts &amp;amp; Costs&lt;/li&gt; &lt;li&gt;Benchmarks &amp;amp; Temperatures&lt;/li&gt; &lt;li&gt;Notes&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;PC HW/SW Parts &amp;amp; Costs&lt;/h1&gt; &lt;h2&gt;HW&lt;/h2&gt; &lt;p&gt;It's all abt the models, then the gpus. The main computer is an afterthought.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Price&lt;/th&gt; &lt;th align="left"&gt;Part&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;$400&lt;/td&gt; &lt;td align="left"&gt;2x mi50 32gb&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$130&lt;/td&gt; &lt;td align="left"&gt;Asus Maximus VIII Gene + 32gb ddr4 + i5-6600k&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$35&lt;/td&gt; &lt;td align="left"&gt;Powertrain X100 PC case&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$60&lt;/td&gt; &lt;td align="left"&gt;ESGaming 750w modular PSU&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$50&lt;/td&gt; &lt;td align="left"&gt;1tb nvme&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$17&lt;/td&gt; &lt;td align="left"&gt;ARGB CPU fan&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$8&lt;/td&gt; &lt;td align="left"&gt;2x delta fans&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;?&lt;/td&gt; &lt;td align="left"&gt;various 3D printer parts: fan shroud, i/o shield, gpu stand, psu mount&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$4&lt;/td&gt; &lt;td align="left"&gt;18pin ribbon cable for extending mobo front panels pins around mi50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;TOTAL: $731&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Bells &amp;amp; Whistles (no idea what these cost nowadays)&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Razer Chroma ARGB controller (6ch, perfect openrgb ctrl)&lt;/li&gt; &lt;li&gt;lcd 2004 + i2c adap&lt;/li&gt; &lt;li&gt;ch341: usb to i2c/gpio&lt;/li&gt; &lt;li&gt;ARGB 120mm case fan&lt;/li&gt; &lt;li&gt;usb cables/adap for internal usb devs&lt;/li&gt; &lt;li&gt;2x ARGB magnetic led strips&lt;/li&gt; &lt;li&gt;2x pcie Y-splitter for gpus&lt;/li&gt; &lt;li&gt;vga/hdmi car-rearview monitor&lt;/li&gt; &lt;li&gt;ezOutlet5 (poor man's bmc)&lt;/li&gt; &lt;li&gt;keyboard&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Smaller than a 24pack of soda. Heavy like a chonky cat.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dim: 349 x 185 x 295mm (19L, I think)&lt;/li&gt; &lt;li&gt;Total Weight: 19.3lb (8.68kg)&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;SW&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Ubuntu 22.04 + 6.8 hwe kernel &lt;/li&gt; &lt;li&gt;rocm 6.4.1 (6.4.4 ripped out mi50 supp!)&lt;/li&gt; &lt;li&gt;llama.cpp -&amp;gt; build_rocm&lt;/li&gt; &lt;li&gt;vbios: 113-D1631700-111 (orig hacky vbios that shipped w mi50).&lt;/li&gt; &lt;li&gt;bios: v0402 (mobo had first oem bios bf update)&lt;/li&gt; &lt;li&gt;openrgb (for python argb ctrl)&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/frank-zago/ch341-i2c-spi-gpio"&gt;ch341 linux driver&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Benchmarks &amp;amp; Temperatures&lt;/h1&gt; &lt;p&gt;Put into comment below&lt;/p&gt; &lt;h1&gt;Notes&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m389gi/32gb_mi50_but_llamacpp_vulkan_sees_only_16gb/"&gt;mi50 vbios misadventures&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j09qk1/dell_t640_for_a_4x_3090_build/"&gt;Building a chonker multi-gpu rig considerations&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mo92ou/rtx_5090_vs_rtx_4090_48gb_or_rtx_6000/"&gt;How much HW do I rly need??? Vram Eaters vs the Gpu Cartel&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;you cant dress trash until you spend a lotta money. building smthg like this can only be done w v clear sw req assessment and a whole lotta hw expertise. multi-gpu compat on old hw is v arcane; esp w mi50s.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;target model: qwen family. v versatile, hq, instructable. v lil refusal bs.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;usecases: filing cooking recipes, modernizing Rolodex, doing arithmetic on dozens (!) of tabular cells. Or how abt: erp, dank memes, navigation calcs (dont wanna fly thru a star when i hit lightspeed)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;mobo is 10yro but is one of the slickest boards i've ever owned&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;its miraculous i was able to fit everything into case. the gpus, the fans &amp;amp; mounts. the normal atx cable lengths. the long (160mm) full sized atx psu. sff builds take more parts bc need to get evryhting to fit. either custom 3d printed plastic or workarounds like ribbon cables&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;similarly there's enough airflow thru such smol spaces to keep things undr 70C during llama-bench&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;i needed to ext the pin headers on the bottom edge of the mobo. 2.54mm pitch ribbon cables to the rescue. still needed to grind a few edges, but it works&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;i pray my nvme will last forevaaaaaah bc id need to tear the whole thing apart to swap drives.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;econ of cheap hw are terrible outside of hobbyests. for viable business, a comp builder would need to make thousands per box. but nobody is gonna pay that for less than multi-gpu behemoths. DIY or DIE.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;the mi50 appears to be the second coming of the P40 due to software advances from gents like these. thanks guys! &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15884"&gt;Flash attn for mi50&lt;/a&gt;. &lt;a href="https://github.com/iacopPBK/llama.cpp-gfx906"&gt;Part2&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;a 4x mi50 rig would be excellent, but exps w 2x tell me sorting out the pcie rsrc alloc issues would be more work than usual for multi-gpu. and still too smol for deepseek&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__E8__"&gt; /u/__E8__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nfqr69"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqr69/warllama_2x_mi50_llm_microatx_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqr69/warllama_2x_mi50_llm_microatx_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T06:55:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng3ztb</id>
    <title>baidu/ERNIE-4.5-21B-A3B Models</title>
    <updated>2025-09-13T17:51:29+00:00</updated>
    <author>
      <name>/u/Holiday_Purpose_3166</name>
      <uri>https://old.reddit.com/user/Holiday_Purpose_3166</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did anyone used this model, and does it live to its expectations?&lt;/p&gt; &lt;p&gt;There's so many downloads on HF that I'm genuinely curious, if there's actually that much use, there might be some feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Purpose_3166"&gt; /u/Holiday_Purpose_3166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng3ztb/baiduernie4521ba3b_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng3ztb/baiduernie4521ba3b_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ng3ztb/baiduernie4521ba3b_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T17:51:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nftdeo</id>
    <title>appreciation post for qwen3 0.6b llm model</title>
    <updated>2025-09-13T09:42:11+00:00</updated>
    <author>
      <name>/u/iamzooook</name>
      <uri>https://old.reddit.com/user/iamzooook</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, For the last few days I was trying out all the low param llm models which would run on cpu.&lt;/p&gt; &lt;p&gt;I have tested from openai oss 20b, gemma 270m, 1b, 4b, deepseek 1.5b, qwen3 0.6b, 1.7b, 4b, 8b, granite 2b, and many more.&lt;/p&gt; &lt;p&gt;the performance and the reliability of qwen3 0.6b is unmatched to any other models. gemma isn't reliable at all even its 4b model. at the same time qwen3 4b beats oss 20b easily. granite 2b is good backup.&lt;/p&gt; &lt;p&gt;I got rid of all the models and just kept qwen3 0.6b, 4b and granite 2b. this would be my doomsday llm models running on cpu.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamzooook"&gt; /u/iamzooook &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nftdeo/appreciation_post_for_qwen3_06b_llm_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nftdeo/appreciation_post_for_qwen3_06b_llm_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nftdeo/appreciation_post_for_qwen3_06b_llm_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T09:42:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfyefq</id>
    <title>Built an OpenWebUI Mobile Companion (Conduit): Alternative to Commercial Chat Apps</title>
    <updated>2025-09-13T14:07:55+00:00</updated>
    <author>
      <name>/u/cogwheel0</name>
      <uri>https://old.reddit.com/user/cogwheel0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfyefq/built_an_openwebui_mobile_companion_conduit/"&gt; &lt;img alt="Built an OpenWebUI Mobile Companion (Conduit): Alternative to Commercial Chat Apps" src="https://external-preview.redd.it/bmM2eHJndWN1eG9mMYOZirCPOS6YBPWlrGphmOW5xVKSUfZTzrO9LgRxJEFF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16d62db6f900a25a3678e55cff44f818a3b54ee1" title="Built an OpenWebUI Mobile Companion (Conduit): Alternative to Commercial Chat Apps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I have been building this for the past month. &lt;a href="https://www.reddit.com/r/selfhosted/comments/1mo9w3t/built_a_native_openwebui_client_for_ios_android/"&gt;After announcing it on different sub&lt;/a&gt; and receiving incredible feedback, I have been iterating. It's currently quite stable for daily use, even for non savvy users. This remains a primary goal with this project as it's difficult to move family off of commercial chat apps like ChatGPT, Gemini, etc without a viable alternative.&lt;/p&gt; &lt;p&gt;It's fully opensource and private: &lt;a href="https://github.com/cogwheel0/conduit"&gt;https://github.com/cogwheel0/conduit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please try it out if you're already selfhosting OpenWebUI and open an issue on GitHub for any problems!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cogwheel0"&gt; /u/cogwheel0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6eh7mfucuxof1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfyefq/built_an_openwebui_mobile_companion_conduit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfyefq/built_an_openwebui_mobile_companion_conduit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T14:07:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng6xnd</id>
    <title>gemma-3-27b and gpt-oss-120b</title>
    <updated>2025-09-13T19:48:15+00:00</updated>
    <author>
      <name>/u/s-i-e-v-e</name>
      <uri>https://old.reddit.com/user/s-i-e-v-e</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using local models for creative writing, translation, summarizing text and similar workloads for more than a year. I am partial to gemma-3-27b ever since it was released and tried gpt-oss-120b soon after it was released.&lt;/p&gt; &lt;p&gt;While both gemma-3-27b and gpt-oss-120b are better than almost anything else I have run locally for these tasks, I find gemma-3-27b to be superior to gpt-oss-120b as far as coherence is concerned. While gpt-oss does know more things and might produce better/realistic prose, it gets lost badly all the time. The details are off within contexts as small as 8-16K tokens.&lt;/p&gt; &lt;p&gt;Yes, it is a MOE model and only 5B params are active at any given time, but I expected more of it. DeepSeek V3 with its 671B params with 37B active ones blows almost everything else that you could host locally away.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/s-i-e-v-e"&gt; /u/s-i-e-v-e &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng6xnd/gemma327b_and_gptoss120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng6xnd/gemma327b_and_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ng6xnd/gemma327b_and_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T19:48:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng5kfb</id>
    <title>Guide: running Qwen3 Next on Windows using vLLM + Docker+ WSL2</title>
    <updated>2025-09-13T18:52:17+00:00</updated>
    <author>
      <name>/u/prusswan</name>
      <uri>https://old.reddit.com/user/prusswan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Below is a batch script I used to pull a pre-built nightly image of vLLM to run a AWQ-4bit version of Qwen3 Next 80B. You can paste the whole block into a file named run.bat etc. Some things to note:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Docker Desktop + WSL2 is needed. If your C drive has less than 100GB free space, you might want to move the default storage location of vhdx (check Docker Desktop settings) to another drive as vLLM image is rather large&lt;/li&gt; &lt;li&gt;original Qwen3 Next is 160GB in size, you can try that if you have all that in VRAM. Otherwise AWQ 4-bit version is around 48GB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; REM Define variables SET MODEL_DIR=E:\vllm_models SET PORT=18000 REM move or make space later: %LOCALAPPDATA%\Docker\wsl\data\ext4.vhdx REM official image from vllm-ci process, yet to test REM SET VLLM_COMMIT=15b8fef453b373b84406207a947005a4d9d68acc REM docker pull public.ecr.aws/q9t5s3a7/vllm-ci-postmerge-repo:%VLLM_COMMIT% REM docker pull public.ecr.aws/q9t5s3a7/vllm-ci-postmerge-repo:latest REM SET VLLM_IMAGE=vllm/vllm-openai:latest # this is not nightly SET VLLM_IMAGE=lmcache/vllm-openai:nightly-2025-09-12 REM SET MODEL_NAME=meta-llama/Llama-2-7b-hf REM SET MODEL_NAME=Qwen/Qwen3-Next-80B-A3B-Instruct SET MODEL_NAME=cpatonn/Qwen3-Next-80B-A3B-Thinking-AWQ-4bit REM Ensure Docker is running docker info &amp;gt;nul 2&amp;gt;&amp;amp;1 if %errorlevel% neq 0 ( echo Docker Desktop is not running. Please start it and try again. pause exit /b 1 ) REM sanity test for gpu in container REM docker run --rm --gpus &amp;quot;device=1&amp;quot; --runtime=nvidia nvidia/cuda:13.0.1-base-ubuntu24.04 nvidia-smi REM Pull the vLLM Docker image if not already present docker pull %VLLM_IMAGE% REM Run the vLLM container docker run --rm -it --runtime=nvidia --gpus &amp;quot;device=1&amp;quot; ^ -v &amp;quot;%MODEL_DIR%:/models&amp;quot; ^ -p %PORT%:8000 ^ -e CUDA_DEVICE_ORDER=PCI_BUS_ID ^ -e CUDA_VISIBLE_DEVICES=1 ^ --ipc=host ^ %VLLM_IMAGE% ^ --model=%MODEL_NAME% ^ --download-dir /models REM --entrypoint bash ^ REM --tensor-parallel-size 4 echo &amp;quot;vLLM container started. Access the OpenAI-compatible API at http://localhost:%PORT%&amp;quot; pause &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prusswan"&gt; /u/prusswan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng5kfb/guide_running_qwen3_next_on_windows_using_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng5kfb/guide_running_qwen3_next_on_windows_using_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ng5kfb/guide_running_qwen3_next_on_windows_using_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T18:52:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfktdg</id>
    <title>To The Qwen Team, Kindly Contribute to Qwen3-Next GGUF Support!</title>
    <updated>2025-09-13T01:29:35+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfktdg/to_the_qwen_team_kindly_contribute_to_qwen3next/"&gt; &lt;img alt="To The Qwen Team, Kindly Contribute to Qwen3-Next GGUF Support!" src="https://b.thumbs.redditmedia.com/FW-gGORXDRiCTDczp9dCooqVX1C79rwrf1Z5V7TuZEM.jpg" title="To The Qwen Team, Kindly Contribute to Qwen3-Next GGUF Support!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you haven't noticed already, Qwen3-Next hasn't yet been supported in llama.cpp, and that's because it comes with a custom SSM archiecture. Without the support of the Qwen team, this amazing model might not be supported for weeks or even months. By now, I strongly believe that llama.cpp day one support is an absolute must.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uxcnskn54uof1.png?width=938&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d39e54d6738531f99913794f3f2917609ff47b3"&gt;https://preview.redd.it/uxcnskn54uof1.png?width=938&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d39e54d6738531f99913794f3f2917609ff47b3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfktdg/to_the_qwen_team_kindly_contribute_to_qwen3next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfktdg/to_the_qwen_team_kindly_contribute_to_qwen3next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfktdg/to_the_qwen_team_kindly_contribute_to_qwen3next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T01:29:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfqe2c</id>
    <title>What's with the obsession with reasoning models?</title>
    <updated>2025-09-13T06:33:35+00:00</updated>
    <author>
      <name>/u/HadesThrowaway</name>
      <uri>https://old.reddit.com/user/HadesThrowaway</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is just a mini rant so I apologize beforehand. Why are practically all AI model releases in the last few months all reasoning models? Even those that aren't are now &amp;quot;hybrid thinking&amp;quot; models. It's like every AI corpo is obsessed with reasoning models currently. &lt;/p&gt; &lt;p&gt;I personally dislike reasoning models, it feels like their only purpose is to help answer tricky riddles at the cost of a huge waste of tokens. &lt;/p&gt; &lt;p&gt;It also feels like everything is getting increasingly benchmaxxed. Models are overfit on puzzles and coding at the cost of creative writing and general intelligence. I think a good example is Deepseek v3.1 which, although technically benchmarking better than v3-0324, feels like a worse model in many ways.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HadesThrowaway"&gt; /u/HadesThrowaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqe2c/whats_with_the_obsession_with_reasoning_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqe2c/whats_with_the_obsession_with_reasoning_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqe2c/whats_with_the_obsession_with_reasoning_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T06:33:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng6fg2</id>
    <title>Qwen-Image-Edit is the real deal! Case + simple guide</title>
    <updated>2025-09-13T19:27:07+00:00</updated>
    <author>
      <name>/u/Antique_Savings7249</name>
      <uri>https://old.reddit.com/user/Antique_Savings7249</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Girlfriend tried using GPT-5 to repair a precious photo with writing on it.&lt;/li&gt; &lt;li&gt;GPT-5s imagegen, because its not really an editing model, failed miserably.&lt;/li&gt; &lt;li&gt;I then tried a local Qwen-Image-Edit (4bit version), just &amp;quot;Remove the blue text&amp;quot;. (RTX 3090 + 48Gb system RAM)&lt;/li&gt; &lt;li&gt;It succeeded amazingly, despite the 4bit quant: All facial features of the subject intact, everything looking clean and natural. No need to send the image to Silicon Valley or China. Girlfriend was very impressed.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;Yes - I could have used Google's image editing for even better results&lt;/em&gt;, but the point for me here was to get a hold of a local tool that could do the type of stuff I usually have used Gimp and Photoshop for. I knew that would be super useful. Although the 4bit does make mistakes, it usually delivers with some tweaks.&lt;/p&gt; &lt;p&gt;Below is the slightly modified &amp;quot;standard Python code&amp;quot; that you will find on huggingface. (my mod makes new indices per run so you dont overwrite previous runs).&lt;/p&gt; &lt;p&gt;All you need outside of this, is the 4bit model &lt;a href="https://huggingface.co/ovedrive/qwen-image-edit-4bit/"&gt;https://huggingface.co/ovedrive/qwen-image-edit-4bit/&lt;/a&gt; , the lora optimized weights (in the same directory): &lt;a href="https://huggingface.co/lightx2v/Qwen-Image-Lightning"&gt;https://huggingface.co/lightx2v/Qwen-Image-Lightning&lt;/a&gt;&lt;br /&gt; .. and the necessary Python libraries, see the import statements. Use LLM assistance if you get run errors and you should be up and running in notime.&lt;/p&gt; &lt;p&gt;In terms of resource use, it will spend around 12Gb of your VRAM and 20Gb of system RAM and run a couple of minutes, mostly on GPU.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import torch from pathlib import Path from transformers import BitsAndBytesConfig as TransformersBitsAndBytesConfig from transformers import Qwen2_5_VLForConditionalGeneration from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig from diffusers import QwenImageEditPipeline, QwenImageTransformer2DModel from diffusers.utils import load_image # from https://huggingface.co/Qwen/Qwen-Image-Edit/discussions/6 model_id = r&amp;quot;G:\Data\AI\Qwen-Image-Edit&amp;quot; fname = &amp;quot;tiko2&amp;quot; prompt = &amp;quot;Remove the blue text from this image&amp;quot; torch_dtype = torch.bfloat16 device = &amp;quot;cuda&amp;quot; quantization_config = DiffusersBitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=&amp;quot;nf4&amp;quot;, bnb_4bit_compute_dtype=torch.bfloat16, llm_int8_skip_modules=[&amp;quot;transformer_blocks.0.img_mod&amp;quot;], ) transformer = QwenImageTransformer2DModel.from_pretrained( model_id, subfolder=&amp;quot;transformer&amp;quot;, quantization_config=quantization_config, torch_dtype=torch_dtype, ) transformer = transformer.to(&amp;quot;cpu&amp;quot;) quantization_config = TransformersBitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=&amp;quot;nf4&amp;quot;, bnb_4bit_compute_dtype=torch.bfloat16, ) text_encoder = Qwen2_5_VLForConditionalGeneration.from_pretrained( model_id, subfolder=&amp;quot;text_encoder&amp;quot;, quantization_config=quantization_config, torch_dtype=torch_dtype, ) text_encoder = text_encoder.to(&amp;quot;cpu&amp;quot;) pipe = QwenImageEditPipeline.from_pretrained( model_id, transformer=transformer, text_encoder=text_encoder, torch_dtype=torch_dtype ) # optionally load LoRA weights to speed up inference pipe.load_lora_weights(model_id + r&amp;quot;\Qwen-Image-Lightning&amp;quot;, weight_name=&amp;quot;Qwen-Image-Edit-Lightning-8steps-V1.0-bf16.safetensors&amp;quot;) # pipe.load_lora_weights( # &amp;quot;lightx2v/Qwen-Image-Lightning&amp;quot;, weight_name=&amp;quot;Qwen-Image-Lightning-4steps-V1.0-bf16.safetensors&amp;quot; # ) pipe.enable_model_cpu_offload() generator = torch.Generator(device=&amp;quot;cuda&amp;quot;).manual_seed(42) image = load_image(model_id + &amp;quot;\\&amp;quot; + fname + &amp;quot;.png&amp;quot;).convert(&amp;quot;RGB&amp;quot;) # change steps to 8 or 4 if you used the lighting loras image = pipe(image, prompt, num_inference_steps=8).images[0] prefix = Path(model_id) / f&amp;quot;{fname}_out&amp;quot; i = 2 # &amp;lt;- replace hardcoded 2 here (starting index) out = Path(f&amp;quot;{prefix}{i}.png&amp;quot;) while out.exists(): i += 1 out = Path(f&amp;quot;{prefix}{i}.png&amp;quot;) image.save(out) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Antique_Savings7249"&gt; /u/Antique_Savings7249 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng6fg2/qwenimageedit_is_the_real_deal_case_simple_guide/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng6fg2/qwenimageedit_is_the_real_deal_case_simple_guide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ng6fg2/qwenimageedit_is_the_real_deal_case_simple_guide/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T19:27:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng1fa5</id>
    <title>New Qwen 3 Next 80B A3B</title>
    <updated>2025-09-13T16:10:18+00:00</updated>
    <author>
      <name>/u/Haruki_090</name>
      <uri>https://old.reddit.com/user/Haruki_090</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng1fa5/new_qwen_3_next_80b_a3b/"&gt; &lt;img alt="New Qwen 3 Next 80B A3B" src="https://b.thumbs.redditmedia.com/H83OV_9-rVuIMILDJ4WSO054RM2o8R-_wKJXYnVERiQ.jpg" title="New Qwen 3 Next 80B A3B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Benchmarks&lt;/p&gt; &lt;p&gt;Model Card: &lt;a href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Instruct Model Card: &lt;a href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source of benchmarks: &lt;a href="https://artificialanalysis.ai"&gt;https://artificialanalysis.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Haruki_090"&gt; /u/Haruki_090 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ng1fa5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng1fa5/new_qwen_3_next_80b_a3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ng1fa5/new_qwen_3_next_80b_a3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T16:10:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng4jas</id>
    <title>I built a private AI that runs Google's Gemma + a full RAG pipeline 100% in your browser. No Docker, no Python, just WebAssembly.</title>
    <updated>2025-09-13T18:12:09+00:00</updated>
    <author>
      <name>/u/Weird_Shoulder_2730</name>
      <uri>https://old.reddit.com/user/Weird_Shoulder_2730</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;For a while now, I've been fascinated by the idea of running powerful AI models entirely on the client-side. I wanted to see if I could build a truly private, serverless AI workspace that didn't require any complex setup with Docker, Python environments, or command-line tools.&lt;/p&gt; &lt;p&gt;The result is &lt;strong&gt;Gemma Web&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It's a fully private, browser-based AI workspace that runs Google's Gemma models directly on your device. Your data never leaves your machine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;100% Browser-Based:&lt;/strong&gt; Everything from the model inference to document embedding happens on the client-side.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero-Setup &amp;amp; Offline:&lt;/strong&gt; No dependencies. After the first load, it can work completely offline, making it a true local-first application.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full RAG Pipeline:&lt;/strong&gt; This was the biggest challenge. You can upload your own documents (PDFs, TXT) and have context-aware conversations, with all the processing happening locally in a Web Worker.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Private by Design:&lt;/strong&gt; No data is ever sent to a server. Incognito mode is available for ephemeral chats.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This was made possible by running Gemma via WebAssembly using the MediaPipe LLM Task API. The RAG embeddings are handled by TensorFlow.js (Universal Sentence Encoder), and everything is stored locally in IndexedDB.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Live Demo:&lt;/strong&gt;&lt;a href="https://gemma-web-ai.vercel.app/"&gt;https://gemma-web-ai.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would love to get your feedback, answer any technical questions, and hear any suggestions you might have. Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weird_Shoulder_2730"&gt; /u/Weird_Shoulder_2730 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng4jas/i_built_a_private_ai_that_runs_googles_gemma_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng4jas/i_built_a_private_ai_that_runs_googles_gemma_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ng4jas/i_built_a_private_ai_that_runs_googles_gemma_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T18:12:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfy5pv</id>
    <title>WEBGEN-OSS Web Design Model - a model that runs on a laptop and generates clean responsive websites from a single prompt</title>
    <updated>2025-09-13T13:57:18+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfy5pv/webgenoss_web_design_model_a_model_that_runs_on_a/"&gt; &lt;img alt="WEBGEN-OSS Web Design Model - a model that runs on a laptop and generates clean responsive websites from a single prompt" src="https://external-preview.redd.it/MmZ0eTk4bzlyeG9mMVpKx18zu2Al60haJHCIipoecy1_uH38KnrawZp01IuI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=27f7dd2294fdcf4c328eea19464490c2b249a9e9" title="WEBGEN-OSS Web Design Model - a model that runs on a laptop and generates clean responsive websites from a single prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/WEBGEN-OSS-20B"&gt;https://huggingface.co/Tesslate/WEBGEN-OSS-20B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm excited to share WEBGEN-OSS-20B, a new 20B open-weight model focused exclusively on generating responsive websites. It’s small enough to run locally for fast iteration and is fine-tuned to produce modern HTML/CSS with Tailwind.&lt;/p&gt; &lt;p&gt;It prefers semantic HTML, sane spacing, and modern component blocks (hero sections, pricing tables, FAQs, etc.). Released under the Apache 2.0 license.&lt;/p&gt; &lt;p&gt;This is a research preview. Use it as you wish but we will be improving the model series greatly in the coming days. (Its very opinionated).&lt;/p&gt; &lt;p&gt;Key Links:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hugging Face Model: &lt;a href="https://huggingface.co/Tesslate/WEBGEN-OSS-20B"&gt;Tesslate/WEBGEN-OSS-20B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Example Outputs: &lt;a href="https://uigenoutput.tesslate.com/"&gt;uigenoutput.tesslate.com&lt;/a&gt; (will be updated within 24 hours)&lt;/li&gt; &lt;li&gt;Join the Tesslate Community to talk about AI and vote for upcoming models: &lt;a href="https://discord.com/invite/EcCpcTv93U"&gt;Discord&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r8vjb8o9rxof1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfy5pv/webgenoss_web_design_model_a_model_that_runs_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfy5pv/webgenoss_web_design_model_a_model_that_runs_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T13:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng0nia</id>
    <title>4x 3090 local ai workstation</title>
    <updated>2025-09-13T15:39:55+00:00</updated>
    <author>
      <name>/u/monoidconcat</name>
      <uri>https://old.reddit.com/user/monoidconcat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng0nia/4x_3090_local_ai_workstation/"&gt; &lt;img alt="4x 3090 local ai workstation" src="https://preview.redd.it/0ug26v2gcyof1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6919babc61209c9648169dac26dd08693e7e02e2" title="4x 3090 local ai workstation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;4x RTX 3090($2500) 2x evga 1600w PSU($200) WRX80E + 3955wx($900) 8x 64gb RAM($500) 1x 2tb nvme($200)&lt;/p&gt; &lt;p&gt;All bought from used market, in total $4300, and I got 96gb of VRAM in total.&lt;/p&gt; &lt;p&gt;Currently considering to acquire two more 3090s and maybe one 5090, but I think the price of 3090s right now is a great deal to build a local AI workstation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/monoidconcat"&gt; /u/monoidconcat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0ug26v2gcyof1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng0nia/4x_3090_local_ai_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ng0nia/4x_3090_local_ai_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T15:39:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nct0z8</id>
    <title>Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)</title>
    <updated>2025-09-09T19:47:12+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt; &lt;img alt="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" src="https://preview.redd.it/7vh8enuu07of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0f499252613d5bcf478fb1b75c594bb50f43436" title="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7vh8enuu07of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T19:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndjxdt</id>
    <title>AMA with the Unsloth team</title>
    <updated>2025-09-10T17:02:18+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;, I'm Daniel from &lt;a href="https://docs.unsloth.ai/"&gt;Unsloth&lt;/a&gt;! You might know us from our RL &amp;amp; fine-tuning open-source framework, our GGUFs, kernels or bug fixes. We’re super excited to answer all your questions!! 🦥 Our GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To celebrate the AMA, we’re releasing Aider Polyglot benchmarks comparing our DeepSeek-V3.1 Dynamic GGUFs to other models and quants. We also made a Localllama post here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our participants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Daniel, u/danielhanchen&lt;/li&gt; &lt;li&gt;Michael, u/yoracale&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 10AM – 1PM PST, with the Unsloth team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thanks so much!🥰&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T17:02:18+00:00</published>
  </entry>
</feed>
