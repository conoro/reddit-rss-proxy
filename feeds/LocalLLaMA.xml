<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-26T17:49:02+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ogq54x</id>
    <title>Ryzen AI Max+ 395 vs RTX 4000 ada SFF</title>
    <updated>2025-10-26T17:15:17+00:00</updated>
    <author>
      <name>/u/dougmaitelli</name>
      <uri>https://old.reddit.com/user/dougmaitelli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Quick question to you all.&lt;/p&gt; &lt;p&gt;Context: I have a RTX 4000 ada that was just sitting in a drawer here. Also had a unused machine with a 10th gen i7 and 64gb of ram collecting dust. I decided to put them together and try to run ollama on Ubuntu.&lt;/p&gt; &lt;p&gt;I am getting about 31 tokens per second with Gemma3:12b.&lt;/p&gt; &lt;p&gt;However, the system is too big and I want something compact, so I bought a GMKtec with the Ryzen AI Max+ 395 and 64gb of shared memory.&lt;/p&gt; &lt;p&gt;The GMKtec is doing 24 tokens per second on the same model on windows ollama.&lt;/p&gt; &lt;p&gt;I saw some people here having like 40 tokens per second with the Ryzen AI Max+ 395 with models of like 37b parameters.&lt;/p&gt; &lt;p&gt;So, what am I missing here? Is my expectation that the Ryzen should be faster for llm wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dougmaitelli"&gt; /u/dougmaitelli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogq54x/ryzen_ai_max_395_vs_rtx_4000_ada_sff/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogq54x/ryzen_ai_max_395_vs_rtx_4000_ada_sff/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogq54x/ryzen_ai_max_395_vs_rtx_4000_ada_sff/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T17:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1og2k8e</id>
    <title>Who is using Granite 4? What's your use case?</title>
    <updated>2025-10-25T21:01:40+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been about 3 weeks since Granite 4 was released with base and instruct versions. If you're using it, what are you using it for? What made you choose it over (or alongside) others?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; this is great and extremely interesting. These use-cases are actually motivating me to consider Granite for a research-paper-parsing project I've been thinking about trying. &lt;/p&gt; &lt;p&gt;The basic idea: I read research papers, and increasingly I talk with LLMs about various bits of different papers. It's annoying to manually process chunks of a paper to pass into an LLM, so I've been thinking about making an agent or few to price a paper into markdown and summarize certain topics and parts automatically for me. &lt;em&gt;And&lt;/em&gt;, of course, I just recall that docling is already integrated with a granite model for basic processing..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og2k8e/who_is_using_granite_4_whats_your_use_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og2k8e/who_is_using_granite_4_whats_your_use_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og2k8e/who_is_using_granite_4_whats_your_use_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T21:01:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1og8jil</id>
    <title>My LLM-powered text adventure needed a dynamic soundtrack, so I'm training a MIDI generation model to compose it on the fly. Here's a video of its progress so far.</title>
    <updated>2025-10-26T01:45:03+00:00</updated>
    <author>
      <name>/u/orblabs</name>
      <uri>https://old.reddit.com/user/orblabs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og8jil/my_llmpowered_text_adventure_needed_a_dynamic/"&gt; &lt;img alt="My LLM-powered text adventure needed a dynamic soundtrack, so I'm training a MIDI generation model to compose it on the fly. Here's a video of its progress so far." src="https://external-preview.redd.it/dXp6ZXRxaTQxZHhmMSaFlwyBbCxWLebwmipVQ7r7-6zFGQDSVaPWFx0zsp7A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c97f76e0095051f80ca25d22b1c21f41a02c24e2" title="My LLM-powered text adventure needed a dynamic soundtrack, so I'm training a MIDI generation model to compose it on the fly. Here's a video of its progress so far." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share a component of a larger project I'm working on called &lt;strong&gt;Synthasia&lt;/strong&gt;. It's a text adventure game, but the core idea is to have multiple LLMs working in synergy to create a deeply dynamic and open-ended world. During development, I hit a predictable wall: because the game can go in any direction, pre-made music is basically impossible, and I found that total silence gets boring fast. Sure, most users will play their own music if they really want to, but I felt like it needed something by default. So...&lt;/p&gt; &lt;p&gt;I decided to tackle this by training a MIDI generation model from scratch to act as the game's dynamic composer. Because... why not choose the most complex and interesting solution? :)&lt;/p&gt; &lt;p&gt;After a lot of research, failed attempts, walls hit, desperation, tears, punches against my poor desk (and... ehm... not proud of it, but some LLM verbal abuse, a lot of it...) I settled on using a 5-stage curriculum training approach. The idea is to build a strong, unconditional composer first before fine-tuning it to follow text prompts (hence why you will see &amp;quot;unconditional&amp;quot; in the video a lot).&lt;/p&gt; &lt;p&gt;The video I linked covers the first 3 of these 5 planned stages. I'm currently in the middle of training Stage 4, which is where I'm introducing an encoder to tie the generation to natural language prompts (that another LLM will generate in my game based on the situation). So this is very much a work-in-progress, and it could very well still fail spectacularly.&lt;/p&gt; &lt;p&gt;Be warned: a lot of what you will hear sucks... badly. In some cases, especially during Stage 3, the sucking is actually good, as the underlying musical structure shows progress even if it doesn't sound like it. &amp;quot;Trust the process&amp;quot; and all... I've had to learn to live by that motto.&lt;/p&gt; &lt;p&gt;You can literally watch its evolution:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Stage 1:&lt;/strong&gt; It starts with classic mode collapse (just one repeating note) before eventually figuring out how to build simple melodies and harmonies.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stage 2:&lt;/strong&gt; It learns the &amp;quot;full vocabulary,&amp;quot; discovering velocity (how hard a note is played) and rests. Its style gets way more expressive and splits into distinct &amp;quot;jazzy&amp;quot; and &amp;quot;lyrical&amp;quot; phases.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stage 3:&lt;/strong&gt; It gets introduced to a huge dataset with multiple instruments. The initial output is a chaotic but fascinating &amp;quot;instrument salad,&amp;quot; which slowly resolves as it starts to understand orchestration and counterpoint.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To help me visualize all this, I put together a Python script to generate the video—and I have to give a huge shout-out to Gemini 2.5 Pro for doing most of the job on it. The music in the video is generated from the validation samples I create every few epochs to evaluate progress and keep an eye out for bugs and weirdness.&lt;/p&gt; &lt;p&gt;I have been overseeing every step of its learning, with dozens of custom loss functions tested and tweaked, so many hours i lost count of, tears and joy, so to me it is super interesting while I am sure to most of you it will be boring as fuck, but thought that maybe someone here will appreciate observing the learning steps and progress in such detail.&lt;/p&gt; &lt;p&gt;Btw, the model doesn't have a name yet. I've been kicking around a couple of cheesy puns: &lt;strong&gt;AI.da&lt;/strong&gt; (like the opera) or &lt;strong&gt;viv-AI-ldi&lt;/strong&gt;. Curious to hear which one lands better, or if you have any other ideas&lt;/p&gt; &lt;p&gt;Edit... forgot to mention that the goal is to have the smallest, working, model possible so that it can run locally within my game and together with other small models for other tasks (like TTS etc). The current design is at 20 mil total parameters and 140mb full precision (i hope to gain something by converting it to fp16 ONNX for actual use in game)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/orblabs"&gt; /u/orblabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fdy23li41dxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og8jil/my_llmpowered_text_adventure_needed_a_dynamic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og8jil/my_llmpowered_text_adventure_needed_a_dynamic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T01:45:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogql0m</id>
    <title>Have access to the LLM but don't know what to do with it ....</title>
    <updated>2025-10-26T17:32:30+00:00</updated>
    <author>
      <name>/u/GTHell</name>
      <uri>https://old.reddit.com/user/GTHell</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 5080 and a 4070, used to have a 3090, subscription to GLM 4.6 that allow 500 calls every 5 hours, Codex CLI enterprise, MiniMax Free till November, Nano Banana credit, 80$ left in Openrouter credit, and more. And yet, I don't know what to do with the LLM.&lt;/p&gt; &lt;p&gt;I think my access to LLM is considering infinite now for my case. I feel truly stuck with the ideas right now. Is there anyone else also like this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GTHell"&gt; /u/GTHell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogql0m/have_access_to_the_llm_but_dont_know_what_to_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogql0m/have_access_to_the_llm_but_dont_know_what_to_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogql0m/have_access_to_the_llm_but_dont_know_what_to_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T17:32:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogqv0p</id>
    <title>Anyone have experience with Local Motion Capture models?</title>
    <updated>2025-10-26T17:43:10+00:00</updated>
    <author>
      <name>/u/onil34</name>
      <uri>https://old.reddit.com/user/onil34</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can only find datasets on hugging face but not the models. if anyone has any ideas. that would be appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil34"&gt; /u/onil34 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogqv0p/anyone_have_experience_with_local_motion_capture/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogqv0p/anyone_have_experience_with_local_motion_capture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogqv0p/anyone_have_experience_with_local_motion_capture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T17:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogqzqj</id>
    <title>Looking for a simple real-time local speech transcription API for Windows</title>
    <updated>2025-10-26T17:48:13+00:00</updated>
    <author>
      <name>/u/martinerous</name>
      <uri>https://old.reddit.com/user/martinerous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd like to experiment with something that could help my immobile relative control his computer with voice. He's been using Windows 10 Speech Recognition for years, but it does not support his language (Latvian). Now he's upgraded to Windows 11 with Voice Access, but that one is buggy and worse.&lt;/p&gt; &lt;p&gt;Now we have better voice recognition out there. I know that Whisper supports Latvian and have tested whisper-fast on my ComfyUI installation. &lt;/p&gt; &lt;p&gt;I will implement the mouse, keyboard and system commands myself - should be easy, I've programmed desktop apps in C#.&lt;/p&gt; &lt;p&gt;All I need is to have some kind of a small background server that receives audio from a microphone and has a simple HTTP or TCP API that I could poll for accumulated transcribed text, and ideally, with some kind of timestamps or relative time since the last detected word, so that I could distinguish separate voice commands by pauses when needed. Ideally, it should also have a simple option to select the correct microphone and also maybe to increase gain for preprocessing the audio, because his voice is quite weak, and default mic settings even at 100% might be too low. Although Windows 10 SR worked fine, so, hopefully, Whisper won't be worse.&lt;/p&gt; &lt;p&gt;I have briefly browsed a few GitHub projects implementing faster-whisper but there are too many unknowns about every project. Some seem to not support Windows at all. Some need Docker (which I wouldn't want to install to every end-user's machine, if my project ends up useful for more people). Some might work only with a latest generation GPU (I'm ready to buy him a 3060 if the solution in general turns out to be useful). Some might not support real-time microphone transcription. It might take me weeks to test them all and fail many times until I find something usable.&lt;/p&gt; &lt;p&gt;I hoped that someone else has already found such a simple real-time transcription tool that could easily be set up on a computer of someone who does not have any development tools installed at all. Wouldn't want it suddenly fail because it cannot build a Python wheel, which some GitHub projects attempt to do. Something that runs with embedded Python would be ok - then I could set up everything on my computer and copy everything to his machine when its ready.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martinerous"&gt; /u/martinerous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogqzqj/looking_for_a_simple_realtime_local_speech/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogqzqj/looking_for_a_simple_realtime_local_speech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogqzqj/looking_for_a_simple_realtime_local_speech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T17:48:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogn3a6</id>
    <title>Built a lightweight Trust &amp; Compliance layer for AI. Am curious if it’s useful for local / self-hosted setups</title>
    <updated>2025-10-26T15:12:47+00:00</updated>
    <author>
      <name>/u/Capable-Property-539</name>
      <uri>https://old.reddit.com/user/Capable-Property-539</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all!&lt;/p&gt; &lt;p&gt;I’ve been building something with a policy expert who works on early drafts of the &lt;strong&gt;EU AI Act&lt;/strong&gt; and &lt;strong&gt;ISO 42001&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Together we made &lt;a href="https://intilium.ai/"&gt;&lt;strong&gt;Intilium&lt;/strong&gt;&lt;/a&gt;. A small &lt;strong&gt;Trust &amp;amp; Compliance layer&lt;/strong&gt; that sits in front of your AI stack.&lt;/p&gt; &lt;p&gt;It’s basically an &lt;strong&gt;API gateway&lt;/strong&gt; that: &lt;/p&gt; &lt;p&gt;Enforces model and region policies (e.g. EU-only, provider allow-lists)&lt;/p&gt; &lt;p&gt;Detects and masks PII before requests go out&lt;/p&gt; &lt;p&gt;Keeps a full audit trail of every LLM call&lt;/p&gt; &lt;p&gt;Works with OpenAI, Anthropic, Google, Mistral and could extend to &lt;strong&gt;local models&lt;/strong&gt; too&lt;/p&gt; &lt;p&gt;The idea is to help teams (or solo builders) &lt;strong&gt;prove compliance automatically&lt;/strong&gt;, especially with new EU rules coming in.&lt;/p&gt; &lt;p&gt;Right now it’s &lt;strong&gt;live and free to test&lt;/strong&gt; in a sandbox environment.&lt;/p&gt; &lt;p&gt;I’d love feedback from anyone running &lt;strong&gt;local inference or self-hosted LLMs&lt;/strong&gt; - what kind of compliance or logging would actually be &lt;em&gt;useful&lt;/em&gt; in that context?&lt;/p&gt; &lt;p&gt;&lt;a href="https://intilium.ai"&gt;https://intilium.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would really appreciate your thoughts on how something like this could integrate into local LLM pipelines (Ollama, LM Studio, custom APIs, etc.).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Capable-Property-539"&gt; /u/Capable-Property-539 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogn3a6/built_a_lightweight_trust_compliance_layer_for_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogn3a6/built_a_lightweight_trust_compliance_layer_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogn3a6/built_a_lightweight_trust_compliance_layer_for_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T15:12:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogezrx</id>
    <title>GPT-OSS DPO/RL fine-tuning, anyone?</title>
    <updated>2025-10-26T08:04:48+00:00</updated>
    <author>
      <name>/u/Few_Art_4147</name>
      <uri>https://old.reddit.com/user/Few_Art_4147</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am quite surprised that I can't find a single example of GPT-OSS fine-tuning with DPO or RL. Anyone tried? I wanted to see some benchmarks before putting time into it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Art_4147"&gt; /u/Few_Art_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogezrx/gptoss_dporl_finetuning_anyone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogezrx/gptoss_dporl_finetuning_anyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogezrx/gptoss_dporl_finetuning_anyone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T08:04:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogo4vt</id>
    <title>Using my Mac Mini M4 as an LLM server—Looking for recommendations</title>
    <updated>2025-10-26T15:55:00+00:00</updated>
    <author>
      <name>/u/cockpit_dandruff</name>
      <uri>https://old.reddit.com/user/cockpit_dandruff</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m looking to set up my Mac Mini M4 (24 GB RAM) as an LLM server. It’s my main desktop, but I want to also use it to run language models locally. I’ve been playing around with the OpenAI API, and ideally I want something that:&lt;/p&gt; &lt;p&gt;• Uses the OpenAI API endpoint (so it’s compatible with existing OpenAI API calls and can act as a drop-in replacement)&lt;/p&gt; &lt;p&gt;• Supports API key authentication. Even though everything will run on my local network, I want API keys to make sure I’m implementing projects correctly.&lt;/p&gt; &lt;p&gt;• Is easy to use or has excellent documentation.&lt;/p&gt; &lt;p&gt;• Can start at boot, so the service is always accessible.&lt;/p&gt; &lt;p&gt;I have been looking into LocalAI but documentation is poor and i simply couldn’t get it to run .&lt;/p&gt; &lt;p&gt;I’d appreciate any pointers, recommendations, or examples of setups people are using on macOS for this.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cockpit_dandruff"&gt; /u/cockpit_dandruff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogo4vt/using_my_mac_mini_m4_as_an_llm_serverlooking_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogo4vt/using_my_mac_mini_m4_as_an_llm_serverlooking_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogo4vt/using_my_mac_mini_m4_as_an_llm_serverlooking_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T15:55:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogl5hl</id>
    <title>Tool Calling with TabbyAPI and Exllamav3</title>
    <updated>2025-10-26T13:51:56+00:00</updated>
    <author>
      <name>/u/Flashy_Management962</name>
      <uri>https://old.reddit.com/user/Flashy_Management962</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did anybody get this to work? I attempted to use exllamav3 with qwen code, the model loads but no tool calls do not work. Im surely doing something wrong. I use the chat template specified by unsloth for tool calling. I dont know what Im doing wrong, but certainly something is wrong. Help would be appreciated&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flashy_Management962"&gt; /u/Flashy_Management962 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogl5hl/tool_calling_with_tabbyapi_and_exllamav3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogl5hl/tool_calling_with_tabbyapi_and_exllamav3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogl5hl/tool_calling_with_tabbyapi_and_exllamav3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T13:51:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogm384</id>
    <title>A highly adaptable toolkit to build APIs and agents, with friendly interfaces for streaming and multimodality</title>
    <updated>2025-10-26T14:31:35+00:00</updated>
    <author>
      <name>/u/apnkv</name>
      <uri>https://old.reddit.com/user/apnkv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I've been working for quite a while on a toolkit/framework to build APIs and agents easily, in a way friendly to developers that would not hide complexity behind abstractions, but that would also be in step with modern requirements and capabilities: stateful, async execution, streaming, multimodality, persistence, etc. &lt;/p&gt; &lt;p&gt;I thought this community would be a perfect place to get feedback, and also that the library itself can be genuinely useful here, so feedback is very welcome!&lt;/p&gt; &lt;p&gt;Landing page with a few nice demos: &lt;a href="https://actionengine.dev/"&gt;https://actionengine.dev/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code examples in Python, TypeScript, C++: &lt;a href="https://github.com/google-deepmind/actionengine/tree/main/examples"&gt;https://github.com/google-deepmind/actionengine/tree/main/examples&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To get an overall grasp, check out the stateful ollama chat sessions example: &lt;a href="https://actionengine.dev/gemini?q=ollama"&gt;demo&lt;/a&gt;, &lt;a href="https://github.com/google-deepmind/actionengine/blob/main/examples/007-python-generative-media/actions/gemini.py"&gt;backend handlers&lt;/a&gt;, &lt;a href="https://github.com/google-deepmind/actionengine/blob/main/examples/007-python-generative-media/server.py"&gt;server&lt;/a&gt;, &lt;a href="https://github.com/google-deepmind/actionengine/blob/main/web/app/gemini/page.tsx"&gt;chat page frontend code&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Why another framework?&lt;/h1&gt; &lt;p&gt;I don't really like the word, but it's hard to find anything better and still have people understand what the project is about. IMO, the problem of &amp;quot;agentic frameworks&amp;quot; is that they give excessively rigid abstractions. The novel challenge is not to &amp;quot;define&amp;quot; &amp;quot;agents&amp;quot;. They are just chains of calls in some distributed context. The actual novel challenge is to build tools and cultivate a common language to express highly dynamic, highly experimental interactions performantly (and safely!) in very different kinds of applications and environments. In other words, the challenge is to acknowledge and enable the diversity of applications and contexts code runs from.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;That means that the framework itself should allow experimentation and adapt to applications, not have applications adapt to it.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I work at Google DeepMind (hence releasing Action Engine under the org), and the intention for me and co-authors/internal supporters is to validate some shifts we think the agent landscape is experiencing, have a quick-feedback way to navigate that, including checking very non-mainstream approaches. Some examples for me are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;developers don't seem to really need &amp;quot;loop runner&amp;quot; type frameworks with tight abstractions, but rather a set of thin layers they can combine to: &lt;ul&gt; &lt;li&gt;relieve &amp;quot;daily&amp;quot;, &amp;quot;boring&amp;quot; issues (e.g. serialisation of custom types, chaining tasks),&lt;/li&gt; &lt;li&gt;have consistent, similar ways to store and transmit state and express agentic behaviour across backend peers, browser clients, model servers etc. (maybe edge devices even),&lt;/li&gt; &lt;li&gt;&amp;quot;productionise&amp;quot;: serve, scale, authorise, discover,&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;it is important to design such tools and frameworks at the full stack to enable builders of all types of apps: web/native, client orchestration or a worker group in a cluster, etc.,&lt;/li&gt; &lt;li&gt;data representation, storage and transport matter much more than the runtime/execution context.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm strongly convinced that such a framework should be absolutely flexible to runtimes, and should accommodate different &amp;quot;wire&amp;quot; protocols and different storage backends to be useful for the general public. Therefore interactions with those layers are extensible:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;for &amp;quot;wire&amp;quot; connections, there are websockets and WebRTC (and Stubby internally at Google), and this can be extended,&lt;/li&gt; &lt;li&gt;for &amp;quot;store&amp;quot;, there is an in-memory implementation and one over Redis streams (also can be extended!) &lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What the library is, exactly&lt;/h1&gt; &lt;p&gt;Action Engine is built as a kit of optional components, for different needs of different applications. IMO that makes it stand out from other frameworks: they lock you in the whole set of abstractions, which you might not need.&lt;/p&gt; &lt;p&gt;The core concepts are &lt;em&gt;action&lt;/em&gt; and &lt;em&gt;async node&lt;/em&gt;. &amp;quot;Action&amp;quot; is simple: it's just executable code with a name and i/o schema assigned, and some well-defined behaviour to prepare and clean up. Async node is a logical &amp;quot;stream&amp;quot; of data: a channel-like interface that one party (or parties!) can write into, and another can read with a &amp;quot;block with timeout&amp;quot; semantics.&lt;/p&gt; &lt;p&gt;These core concepts are easy to understand. Unlike with loaded terms like &amp;quot;agent&amp;quot;, &amp;quot;context&amp;quot; or &amp;quot;graph executor&amp;quot;, you won't make any huge mistake thinking about &lt;em&gt;actions&lt;/em&gt; as about &lt;em&gt;functions&lt;/em&gt;, and about &lt;em&gt;async nodes&lt;/em&gt; as about &lt;em&gt;channels&lt;/em&gt; or &lt;em&gt;queues&lt;/em&gt; that go as inputs and outputs to those functions.&lt;/p&gt; &lt;p&gt;The rest of the library simply cares about building context to run or call actions, and lets you do that yourself—there are implementations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;for particular-backend &lt;em&gt;wire streams&lt;/em&gt;, &lt;/li&gt; &lt;li&gt;for &lt;em&gt;sessions&lt;/em&gt; that share a data context between action runs, &lt;/li&gt; &lt;li&gt;for &lt;em&gt;services&lt;/em&gt; that hold multiple sessions and route wire connections into them, &lt;/li&gt; &lt;li&gt;for &lt;em&gt;servers&lt;/em&gt; that listen to connections / do access control / etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;...but it's not a package offering. No layer is obligatory, and in your particular project, you may end up having a nicer integration and less complexity than if you used ADK, for example. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Flexibility to integrate any use case, model or API, and flexibility to run in different infrastructure are first-class concerns here, and so is avoiding large cognitive footprint.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Anyway, I'd be grateful for feedback! Have a look, try it out—the project is WIP and the level of documentation is definitely less than needed, but I'll be happy to answer any questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/apnkv"&gt; /u/apnkv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogm384/a_highly_adaptable_toolkit_to_build_apis_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogm384/a_highly_adaptable_toolkit_to_build_apis_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogm384/a_highly_adaptable_toolkit_to_build_apis_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T14:31:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1og9ije</id>
    <title>How good is Ling-1T?</title>
    <updated>2025-10-26T02:35:28+00:00</updated>
    <author>
      <name>/u/Aware_Magician7958</name>
      <uri>https://old.reddit.com/user/Aware_Magician7958</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og9ije/how_good_is_ling1t/"&gt; &lt;img alt="How good is Ling-1T?" src="https://preview.redd.it/cs7bb6igbdxf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be874b0392daf9b87a138381fb588243291b71a3" title="How good is Ling-1T?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apparently there's been a new model by Ant Group (InclusionAI) that is an open-weight non-thinking model with 1000B parameters. According to their article their performance is better than paid models. Has anyone run this yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aware_Magician7958"&gt; /u/Aware_Magician7958 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cs7bb6igbdxf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og9ije/how_good_is_ling1t/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og9ije/how_good_is_ling1t/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T02:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogfmt4</id>
    <title>GLM 4.5 air for coding</title>
    <updated>2025-10-26T08:46:12+00:00</updated>
    <author>
      <name>/u/Magnus114</name>
      <uri>https://old.reddit.com/user/Magnus114</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You who use a local glm 4.5 air for coding, can you please share your software setup?&lt;/p&gt; &lt;p&gt;I have had some success with unsloth q4_k_m on llama.cpp with opencode. To get the tool usage to work I had to use a jinja template from a pull request, and still the tool calling fails occasionally. Tried unsloth jinja template from glm 4.6, but no success. Also experimented with claude code with open router with a similar result. Considering to trying to write my own template and also trying with vllm.&lt;/p&gt; &lt;p&gt;Would love to hear how others are using glm 4.5 air. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Magnus114"&gt; /u/Magnus114 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogfmt4/glm_45_air_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogfmt4/glm_45_air_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogfmt4/glm_45_air_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T08:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1og3cnt</id>
    <title>Llama.cpp model conversion guide</title>
    <updated>2025-10-25T21:35:28+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og3cnt/llamacpp_model_conversion_guide/"&gt; &lt;img alt="Llama.cpp model conversion guide" src="https://external-preview.redd.it/9H8ID2ho6Hmpcg_iEkUBl0dzrALfa1J8gFxkkoi6ojc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=960cfd58e78583a22cdad3922567cd461d36ac4b" title="Llama.cpp model conversion guide" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the open source community always benefits by having more people do stuff, I figured I would capitalize on my experiences with a few architectures I've done and add a guide for people who, like me, would like to gain practical experience by porting a model architecture.&lt;/p&gt; &lt;p&gt;Feel free to propose any topics / clarifications and ask any questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16770"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og3cnt/llamacpp_model_conversion_guide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og3cnt/llamacpp_model_conversion_guide/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T21:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1og87pd</id>
    <title>If you had $4k, would you invest in a DGX Spark?</title>
    <updated>2025-10-26T01:28:00+00:00</updated>
    <author>
      <name>/u/Excellent_Koala769</name>
      <uri>https://old.reddit.com/user/Excellent_Koala769</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Guys, I am very curious what everyone's opinion is regarding the DGX Spark. &lt;/p&gt; &lt;p&gt;If you had $4k and you needed to use that money to start building out your own personal AI data center, would you buy a DGX Spark... or go a different direction?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Koala769"&gt; /u/Excellent_Koala769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og87pd/if_you_had_4k_would_you_invest_in_a_dgx_spark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og87pd/if_you_had_4k_would_you_invest_in_a_dgx_spark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og87pd/if_you_had_4k_would_you_invest_in_a_dgx_spark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T01:28:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogaohi</id>
    <title>MiniMax: MiniMax M2 seems to VERY, VERY good</title>
    <updated>2025-10-26T03:39:02+00:00</updated>
    <author>
      <name>/u/klippers</name>
      <uri>https://old.reddit.com/user/klippers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Generally use GLM4.6 , been at a few problems most of the week, today threw these at MiniMax: MiniMax M2 and it sorted them with no fuss......Very impressed!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klippers"&gt; /u/klippers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogaohi/minimax_minimax_m2_seems_to_very_very_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogaohi/minimax_minimax_m2_seems_to_very_very_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogaohi/minimax_minimax_m2_seems_to_very_very_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T03:39:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogjkfj</id>
    <title>Cheaper &amp; faster LLM stack in 2025: Kimi/Qwen vs OpenAI</title>
    <updated>2025-10-26T12:37:45+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjkfj/cheaper_faster_llm_stack_in_2025_kimiqwen_vs/"&gt; &lt;img alt="Cheaper &amp;amp; faster LLM stack in 2025: Kimi/Qwen vs OpenAI" src="https://b.thumbs.redditmedia.com/v4DjQ3ybawpfEVteS_F8SfFp6OReWcTch-9YAwPpLfs.jpg" title="Cheaper &amp;amp; faster LLM stack in 2025: Kimi/Qwen vs OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/v0ddm42g9gxf1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f75f7809cead99b006dc49dc76a53f453f06a8f"&gt;Chamath Palihapitiya&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dmbx1rcl9gxf1.png?width=1196&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=154810c46f400e52c2ef4cef6c6a44c79fab9fef"&gt;https://preview.redd.it/dmbx1rcl9gxf1.png?width=1196&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=154810c46f400e52c2ef4cef6c6a44c79fab9fef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The valley is built on open-source models?&lt;/p&gt; &lt;p&gt;On the All-In podcast, Chamath Palihapitiya says his team redirected a ton of workloads to &lt;strong&gt;Kimi K2&lt;/strong&gt; because it was “&lt;strong&gt;way more performant&lt;/strong&gt;” and “&lt;strong&gt;a ton cheaper&lt;/strong&gt;” than OpenAI and Anthropic.&lt;/p&gt; &lt;p&gt;Airbnb CEO Brian Chesky says they’re relying a lot on Alibaba’s &lt;strong&gt;Qwen&lt;/strong&gt; in production because it’s “&lt;strong&gt;fast and cheap.&lt;/strong&gt;” They still use OpenAI’s latest models, but “typically don’t use them that much in production” due to &lt;strong&gt;faster/cheaper&lt;/strong&gt; options.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjkfj/cheaper_faster_llm_stack_in_2025_kimiqwen_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjkfj/cheaper_faster_llm_stack_in_2025_kimiqwen_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjkfj/cheaper_faster_llm_stack_in_2025_kimiqwen_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T12:37:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1oga7um</id>
    <title>All the models seem to love using the same names.</title>
    <updated>2025-10-26T03:13:40+00:00</updated>
    <author>
      <name>/u/AI_Renaissance</name>
      <uri>https://old.reddit.com/user/AI_Renaissance</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In particular thorn and vance when doing horror or science fiction, for a woman its almost always elara vance, and if there is a male doctor or scientist, usually thomas thorn. Has anyone else experienced this?&lt;/p&gt; &lt;p&gt;Right now I mostly use Cydonia which is a pretty good local model, but this even happens on the perchance ai website. It's funny, but annoying. I think maybe the training data eating itself with merges.&lt;/p&gt; &lt;p&gt;For example, try a prompt like &amp;quot;write a story about a mad scientist that creates a monster&amp;quot;. The name of the scientist will most likely be something like Dr. Aris or Thomas Thorne. Its not a that big of a deal if you come up with your own names for characters. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AI_Renaissance"&gt; /u/AI_Renaissance &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oga7um/all_the_models_seem_to_love_using_the_same_names/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oga7um/all_the_models_seem_to_love_using_the_same_names/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oga7um/all_the_models_seem_to_love_using_the_same_names/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T03:13:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogo2jv</id>
    <title>I made a 1B model to generate 3d files (barely)</title>
    <updated>2025-10-26T15:52:22+00:00</updated>
    <author>
      <name>/u/ThomasPhilli</name>
      <uri>https://old.reddit.com/user/ThomasPhilli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;2 weeks ago, I finetuned Gemma3 1B on Synthetic 3D file data. I called the model K-1B.&lt;/p&gt; &lt;p&gt;Yesterday I packaged it into an app, hosting the model on Modal.&lt;/p&gt; &lt;p&gt;I would appreciate any feedback as this is a hobby project that I will keep on training the model etc.&lt;/p&gt; &lt;p&gt;Thanks :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasPhilli"&gt; /u/ThomasPhilli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://cadmonkey.web.app"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogo2jv/i_made_a_1b_model_to_generate_3d_files_barely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogo2jv/i_made_a_1b_model_to_generate_3d_files_barely/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T15:52:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogh8ec</id>
    <title>Using GLM 4.6 to understand it's limitations</title>
    <updated>2025-10-26T10:27:40+00:00</updated>
    <author>
      <name>/u/Vozer_bros</name>
      <uri>https://old.reddit.com/user/Vozer_bros</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogh8ec/using_glm_46_to_understand_its_limitations/"&gt; &lt;img alt="Using GLM 4.6 to understand it's limitations" src="https://a.thumbs.redditmedia.com/E-b1AYjpK-0wB4j8lY7BJQb9A8ucue0uF7crpypRrQ0.jpg" title="Using GLM 4.6 to understand it's limitations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/gq9yiommnfxf1.png?width=1994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=04be61d0c1fe988448c06878ea77b577ddd6aee1"&gt;https://preview.redd.it/gq9yiommnfxf1.png?width=1994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=04be61d0c1fe988448c06878ea77b577ddd6aee1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The actual loosing point will start at 30% less than the number in the table. For example, tool calling actually starting to fail randomly at 70k context.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vozer_bros"&gt; /u/Vozer_bros &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogh8ec/using_glm_46_to_understand_its_limitations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogh8ec/using_glm_46_to_understand_its_limitations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogh8ec/using_glm_46_to_understand_its_limitations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T10:27:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofu15a</id>
    <title>I rebuilt DeepSeek’s OCR model in Rust so anyone can run it locally (no Python!)</title>
    <updated>2025-10-25T15:13:52+00:00</updated>
    <author>
      <name>/u/Outrageous-Voice</name>
      <uri>https://old.reddit.com/user/Outrageous-Voice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! After wrestling with the original DeepSeek-OCR release (Python + Transformers, tons of dependencies, zero UX), I decided to port the whole inference stack to Rust. The repo is deepseek-ocr.rs (&lt;a href="https://github.com/TimmyOVO/deepseek-ocr.rs"&gt;https://github.com/TimmyOVO/deepseek-ocr.rs&lt;/a&gt;) and it ships both a CLI and an OpenAI-compatible server so you can drop it straight into existing clients like Open WebUI.&lt;/p&gt; &lt;h1&gt;Why bother?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;No Python, no conda—just a single Rust binary.&lt;/li&gt; &lt;li&gt;Works offline and keeps documents private.&lt;/li&gt; &lt;li&gt;Fully OpenAI-compatible, so existing SDKs/ChatGPT-style UIs “just work”.&lt;/li&gt; &lt;li&gt;Apple Silicon support with optional Metal acceleration (FP16).&lt;/li&gt; &lt;li&gt;Built-in Hugging Face downloader: config/tokenizer/weights (≈6.3 GB) fetch automatically; needs about 13 GB RAM to run.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What’s inside the Rust port?&lt;/h1&gt; &lt;p&gt;- Candle-based reimplementation of the language model (DeepSeek-V2) with KV caches + optional FlashAttention.&lt;/p&gt; &lt;p&gt;- Full SAM + CLIP vision pipeline, image tiling, projector, and tokenizer alignment identical to the PyTorch release.&lt;/p&gt; &lt;p&gt;- Rocket server that exposes /v1/responses and /v1/chat/completions (OpenAI-compatible streaming included).&lt;/p&gt; &lt;p&gt;- Single-turn prompt compaction so OCR doesn’t get poisoned by multi-turn history.&lt;/p&gt; &lt;p&gt;- Debug hooks to compare intermediate tensors against the official model (parity is already very close).&lt;/p&gt; &lt;h1&gt;Getting started&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;You can download prebuilt archives (macOS with Metal, Windows) from the latest successful run of the repo’s GitHub Actions “build-binaries (&lt;a href="https://github.com/TimmyOVO/deepseek-ocr.rs/actions/workflows/build-binaries.yml"&gt;https://github.com/TimmyOVO/deepseek-ocr.rs/actions/workflows/build-binaries.yml)”&lt;/a&gt;”) workflow—no local build required.&lt;/li&gt; &lt;li&gt;Prefer compiling? git clone &lt;a href="https://github.com/TimmyOVO/deepseek-ocr.rs"&gt;https://github.com/TimmyOVO/deepseek-ocr.rs&lt;/a&gt; → cargo fetch&lt;/li&gt; &lt;li&gt;CLI: cargo run -p deepseek-ocr-cli -- --prompt &amp;quot;&amp;lt;image&amp;gt;...&amp;quot; --image mydoc.png&lt;/li&gt; &lt;li&gt;Server: cargo run -p deepseek-ocr-server -- --host &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; --port 8000&lt;/li&gt; &lt;li&gt;On macOS, add --features metal plus --device metal --dtype f16 for GPU acceleration.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Use cases&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Batch document conversion (receipts → markdown, contracts → summaries, etc.).&lt;/li&gt; &lt;li&gt;Plugging into Open WebUI (looks/feels like ChatGPT but runs YOUR OCR model).&lt;/li&gt; &lt;li&gt;Building document QA bots that need faithful extraction.&lt;strong&gt;If you try it, I’d love to hear your feedback—feature requests, edge cases, performance reports, all welcome. And if it saves you from Python dependency hell, toss the repo a ⭐️.&lt;/strong&gt;Cheers!&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Voice"&gt; /u/Outrageous-Voice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofu15a/i_rebuilt_deepseeks_ocr_model_in_rust_so_anyone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofu15a/i_rebuilt_deepseeks_ocr_model_in_rust_so_anyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofu15a/i_rebuilt_deepseeks_ocr_model_in_rust_so_anyone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T15:13:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogigyu</id>
    <title>Is SSM dead now?</title>
    <updated>2025-10-26T11:39:31+00:00</updated>
    <author>
      <name>/u/Spapoxl</name>
      <uri>https://old.reddit.com/user/Spapoxl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried researching about it and found almost all of the news and information is 1 years ago. Is it discontinued? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spapoxl"&gt; /u/Spapoxl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogigyu/is_ssm_dead_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogigyu/is_ssm_dead_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogigyu/is_ssm_dead_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T11:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogjtkn</id>
    <title>Poor GPU Club : Good Worthy Pruned models?</title>
    <updated>2025-10-26T12:50:38+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to explore more on this after seeing recent threads( &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oefu29/cerebras_reapd_glm46_25_30_40_pruned_fp8/"&gt;3&lt;/a&gt; , &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/"&gt;2&lt;/a&gt; , &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"&gt;1&lt;/a&gt; ) from Cerebras. They already pruned few MOE models such as Qwen3-Coder-30B, Qwen3-Coder-480B, GLM-4.5-Air, GLM-4.6. I'm just waiting for few small MOE models from them, hope they do soon or later.&lt;/p&gt; &lt;p&gt;Meanwhile &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1octe2s/pruned_moe_reap_quants_for_testing/"&gt;one other person pruned few other MOE models&lt;/a&gt;(Qwen3-30B, Qwen3-30B-Instruct, Qwen3-Coder-30B, GPT-OSS-20B, GPT-OSS-120B) using same Reap by Cerebras.&lt;/p&gt; &lt;p&gt;I'll be trying those small pruned models for sure since I have only 8GB VRAM(and 32GB RAM).&lt;/p&gt; &lt;p&gt;I'm sure some of you might have tried few pruned models before. HuggingFace has 100s of pruned models. Below are links to pruned models with different tags. Of course there must be some more pruned models without below tags. &lt;a href="https://huggingface.co/models?other=pruned"&gt;Pruned&lt;/a&gt; , &lt;a href="https://huggingface.co/models?other=prune"&gt;Prune&lt;/a&gt; , &lt;a href="https://huggingface.co/models?other=pruning"&gt;Pruning&lt;/a&gt; , &lt;a href="https://huggingface.co/models?other=pruned-model"&gt;pruned-model&lt;/a&gt; , &lt;a href="https://huggingface.co/models?other=expert-pruning"&gt;expert-pruning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1]&lt;/strong&gt; Please recommend good worthy pruned models particularly small ones under 50B&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2]&lt;/strong&gt; Cerebras Reap method is only for MOE models. Does anyone came across anything for Dense models? Recently I posted a thread about Q3/Q2 quants of Dense models since I couldn't run those models with high quants like Q4 &amp;amp; above. &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o3zz30/poor_gpu_club_anyone_use_q3q2_quants_of_2040b/"&gt;Anyone use Q3/Q2 quants of 20-40B Dense models? How's it?&lt;/a&gt; Unfortunately I couldn't run even Q3 with bearable t/s.&lt;/p&gt; &lt;p&gt;Currently I'm looking for Pruned models of below ones:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Seed-OSS-36B-Instruct&lt;/li&gt; &lt;li&gt;Devstral-Small-2507&lt;/li&gt; &lt;li&gt;Magistral-Small-2509&lt;/li&gt; &lt;li&gt;Mistral-Small-3.2-24B-Instruct-2506&lt;/li&gt; &lt;li&gt;reka-flash-3.1&lt;/li&gt; &lt;li&gt;Gemma-3-27B-it&lt;/li&gt; &lt;li&gt;Qwen3-32B&lt;/li&gt; &lt;li&gt;GLM-4-32B-0414&lt;/li&gt; &lt;li&gt;And lot of 20B+ finetunes from sources like TheDrummer, SicariusSicariiStuff, etc.,&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It would be great if someone shrink those dense models to 50%(at least 25-35%) so I could use Q4 with decent/bearable t/s with my 8GB VRAM(and 32GB RAM).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjtkn/poor_gpu_club_good_worthy_pruned_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjtkn/poor_gpu_club_good_worthy_pruned_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjtkn/poor_gpu_club_good_worthy_pruned_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T12:50:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogoyza</id>
    <title>Qwen3-VL-32B is really good. Quick test vs several other local models I keep on my workstation (details in comments)</title>
    <updated>2025-10-26T16:28:19+00:00</updated>
    <author>
      <name>/u/EmPips</name>
      <uri>https://old.reddit.com/user/EmPips</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogoyza/qwen3vl32b_is_really_good_quick_test_vs_several/"&gt; &lt;img alt="Qwen3-VL-32B is really good. Quick test vs several other local models I keep on my workstation (details in comments)" src="https://preview.redd.it/8a00jiy4ghxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d362439263cafe886a82048ec21177d435463df4" title="Qwen3-VL-32B is really good. Quick test vs several other local models I keep on my workstation (details in comments)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmPips"&gt; /u/EmPips &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8a00jiy4ghxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogoyza/qwen3vl32b_is_really_good_quick_test_vs_several/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogoyza/qwen3vl32b_is_really_good_quick_test_vs_several/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T16:28:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogg6sz</id>
    <title>Why didn't LoRA catch on with LLMs?</title>
    <updated>2025-10-26T09:22:15+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;Explanation of LoRA for the folks at home&lt;/h2&gt; &lt;p&gt;(skip to next section if you already know what Lora is)&lt;/p&gt; &lt;p&gt;I only know it from the image generation Stable Diffusion world, and I only tried that briefly, so this won't be 100% exact.&lt;/p&gt; &lt;p&gt;Let's say your image generation model is Stable Diffusion 1.5, which came out a few years ago. It can't know the artstyle of a new artist that came up in the past year, let's say his name his Bobsolete.&lt;/p&gt; &lt;p&gt;What lora creators did is create a small dataset of Bobsolete's art, and use it to train SD 1.5 for like 1-2 days. This outputs a small lora file (the SD 1.5 model is 8GB, a lora is like 20MB). Users can download this lora, and when loading SD 1.5, say &amp;quot;also attach Bobsolete.lora to the model&amp;quot;. Now the user is interacting with SD 1.5 that has been augmented with knowledge of Bobsolete. The user can specify &amp;quot;drawn in the style of Bobsolete&amp;quot; and it will work.&lt;/p&gt; &lt;p&gt;Loras are used to add new styles to a model, new unique characters, and so on.&lt;/p&gt; &lt;h2&gt;Back to LLMs&lt;/h2&gt; &lt;p&gt;LLMs apparently support loras, but no one seems to use them. I've never ever seen them discussed on this sub in my 2 years of casual browsing, although I see they exist in the search results.&lt;/p&gt; &lt;p&gt;I was wondering why this hasn't caught on. People could add little bodies of knowledge to an already-released model. For example, you take a solid general model like Gemma 3 27B. Someone could release a lora trained on all scifi books, another based on all major movie scripts, etc. You could then &amp;quot;./llama.cpp -m models/gemma3.gguf --lora models/scifi-books-rev6.lora --lora models/movie-scripts.lora&amp;quot; and try to get Gemma 3 to help you write a modern scifi movie script. You could even focus even more on specific authors, cormac-mccarthy.lora etc.&lt;/p&gt; &lt;p&gt;A more useful/legal example would be attaching current-events-2025.lora to a model whose cutoff date was December 2024. &lt;/p&gt; &lt;p&gt;So why didn't this catch on the way it did in the image world? Is this technology inherently more limited on LLMs? Why does it seem like companies interested in integrating their doc with AI are more focused on RAG than training a Lora on their internal docs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogg6sz/why_didnt_lora_catch_on_with_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogg6sz/why_didnt_lora_catch_on_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogg6sz/why_didnt_lora_catch_on_with_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T09:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1obqkpe</id>
    <title>Best Local LLMs - October 2025</title>
    <updated>2025-10-20T19:06:06+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Welcome to the first monthly &amp;quot;Best Local LLMs&amp;quot; post!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;General&lt;/li&gt; &lt;li&gt;Agentic/Tool Use&lt;/li&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;Creative Writing/RP&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(look for the top level comments for each Application and please thread your responses under that)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:06:06+00:00</published>
  </entry>
</feed>
