<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-03T11:34:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1nw60fj</id>
    <title>Open source speech foundation model that runs locally on CPU in real-time</title>
    <updated>2025-10-02T14:54:43+00:00</updated>
    <author>
      <name>/u/TeamNeuphonic</name>
      <uri>https://old.reddit.com/user/TeamNeuphonic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw60fj/open_source_speech_foundation_model_that_runs/"&gt; &lt;img alt="Open source speech foundation model that runs locally on CPU in real-time" src="https://external-preview.redd.it/3w13BgLMXQ4-v0J3QSPqnnHAcC8U3HjNheDu4QFAWrk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bfeb812f14c0b82e510265b807d168b2af385bc" title="Open source speech foundation model that runs locally on CPU in real-time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1nw60fj/video/3kh334ujppsf1/player"&gt;https://reddit.com/link/1nw60fj/video/3kh334ujppsf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’ve just released Neuphonic TTS Air, a lightweight open-source speech foundation model under Apache 2.0.&lt;/p&gt; &lt;p&gt;The main idea: frontier-quality text-to-speech, but small enough to run in realtime on CPU. No GPUs, no cloud APIs, no rate limits.&lt;/p&gt; &lt;p&gt;Why we built this: - Most speech models today live behind paid APIs → privacy tradeoffs, recurring costs, and external dependencies. - With Air, you get full control, privacy, and zero marginal cost. - It enables new use cases where running speech models on-device matters (edge compute, accessibility tools, offline apps).&lt;/p&gt; &lt;p&gt;Git Repo: &lt;a href="https://github.com/neuphonic/neutts-air"&gt;https://github.com/neuphonic/neutts-air&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF: &lt;a href="https://huggingface.co/neuphonic/neutts-air"&gt;https://huggingface.co/neuphonic/neutts-air&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from on performance, applications, and contributions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TeamNeuphonic"&gt; /u/TeamNeuphonic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw60fj/open_source_speech_foundation_model_that_runs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw60fj/open_source_speech_foundation_model_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw60fj/open_source_speech_foundation_model_that_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T14:54:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwql4a</id>
    <title>DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder (Delivers 14.8× faster inference than the base model)</title>
    <updated>2025-10-03T05:24:33+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This also seems to work with image diffusion models. Could it be used for LLM diffusion models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hanlab.mit.edu/projects/dc-videogen"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwql4a/dcvideogen_efficient_video_generation_with_deep/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwql4a/dcvideogen_efficient_video_generation_with_deep/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T05:24:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwc1oc</id>
    <title>Apertus model implementation has been merged into llama.cpp</title>
    <updated>2025-10-02T18:37:32+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwc1oc/apertus_model_implementation_has_been_merged_into/"&gt; &lt;img alt="Apertus model implementation has been merged into llama.cpp" src="https://external-preview.redd.it/WBeE9GPvyJdOEySnojQ_o2A9ys0na0K0XH7uI9iyd_o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=23de1c1602b1561cace347dd342baae689fd7c5c" title="Apertus model implementation has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think Piotr can now fully focus on Qwen Next ;)&lt;/p&gt; &lt;p&gt;model description:&lt;/p&gt; &lt;p&gt;Apertus is a 70B and 8B parameter language model designed to push the boundaries of fully-open multilingual and transparent models. The model supports over 1000 languages and long context, it uses only fully compliant and open training data, and achieves comparable performance to models trained behind closed doors.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/swiss-ai/Apertus-70B-Instruct-2509"&gt;https://huggingface.co/swiss-ai/Apertus-70B-Instruct-2509&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/swiss-ai/Apertus-8B-Instruct-2509"&gt;https://huggingface.co/swiss-ai/Apertus-8B-Instruct-2509&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15852"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwc1oc/apertus_model_implementation_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwc1oc/apertus_model_implementation_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T18:37:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvzeuh</id>
    <title>Jan now auto-optimizes llama.cpp settings based on your hardware for more efficient performance</title>
    <updated>2025-10-02T09:47:49+00:00</updated>
    <author>
      <name>/u/ShinobuYuuki</name>
      <uri>https://old.reddit.com/user/ShinobuYuuki</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzeuh/jan_now_autooptimizes_llamacpp_settings_based_on/"&gt; &lt;img alt="Jan now auto-optimizes llama.cpp settings based on your hardware for more efficient performance" src="https://external-preview.redd.it/NzRlNXJuc3A2b3NmMe-uhlatbqnQI0WkANIEyFuJlq6CEOqVOtkO0hhCMPfO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=152efa7692053a33bdf74c0824752b907d2becc8" title="Jan now auto-optimizes llama.cpp settings based on your hardware for more efficient performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I'm Yuuki from the Jan team.&lt;/p&gt; &lt;p&gt;We’ve been working on some updates for a while. We released Jan v0.7.0. I'd like to quickly share what's new:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;llama.cpp improvements&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan now automatically optimizes llama.cpp settings (e.g. context size, gpu layers) based on your hardware. So your models run more efficiently. It's an experimental feature&lt;/li&gt; &lt;li&gt;You can now see some stats (how much context is used, etc.) when the model runs&lt;/li&gt; &lt;li&gt;Projects is live now. You can organize your chats using it - it's pretty similar to ChatGPT&lt;/li&gt; &lt;li&gt;You can rename your models in Settings&lt;/li&gt; &lt;li&gt;Plus, we're also improving Jan's cloud capabilities: Model names update automatically - so no need to manually add cloud models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you haven't seen it yet: Jan is an open-source ChatGPT alternative. It runs AI models locally and lets you add agentic capabilities &lt;a href="https://www.jan.ai/docs/desktop/mcp#configure-and-use-mcps-within-jan"&gt;through MCPs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Website: &lt;a href="https://www.jan.ai/"&gt;https://www.jan.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/menloresearch/jan"&gt;https://github.com/menloresearch/jan&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShinobuYuuki"&gt; /u/ShinobuYuuki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/49h5xlsp6osf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzeuh/jan_now_autooptimizes_llamacpp_settings_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzeuh/jan_now_autooptimizes_llamacpp_settings_based_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T09:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwpxxx</id>
    <title>Performance wise what is the best backend right now?</title>
    <updated>2025-10-03T04:47:32+00:00</updated>
    <author>
      <name>/u/ResponsibleTruck4717</name>
      <uri>https://old.reddit.com/user/ResponsibleTruck4717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently I'm using mostly ollama and sometimes the transformers library, ollama is really nice allowing me to focus on the code instead of configure model and manager memory and gpu load, while transformers takes more work.&lt;/p&gt; &lt;p&gt;Any other frameworks I should test, specially one that offer more performance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleTruck4717"&gt; /u/ResponsibleTruck4717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwpxxx/performance_wise_what_is_the_best_backend_right/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwpxxx/performance_wise_what_is_the_best_backend_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwpxxx/performance_wise_what_is_the_best_backend_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T04:47:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwogkl</id>
    <title>Sloppiest model!?</title>
    <updated>2025-10-03T03:27:03+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Odd request, but can anyone share the sloppiest models they have tried? I'm trying to generate data with as much AI slop (it's not this–its that / shivers-down-spines / emojis / bulleted lists / testaments &amp;amp; tapestries /etc) as possible.&lt;/p&gt; &lt;p&gt;EDIT: Thanks for the input guys! I think I found the model (Original versions of Qwen3 14B / 30BA3B with /no_think seems to do a great job :D)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwogkl/sloppiest_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwogkl/sloppiest_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwogkl/sloppiest_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T03:27:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nww6jy</id>
    <title>vllm setup for nvidia (can use llama)</title>
    <updated>2025-10-03T11:12:06+00:00</updated>
    <author>
      <name>/u/Superb-Security-578</name>
      <uri>https://old.reddit.com/user/Superb-Security-578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nww6jy/vllm_setup_for_nvidia_can_use_llama/"&gt; &lt;img alt="vllm setup for nvidia (can use llama)" src="https://external-preview.redd.it/g-dAVuhM7CACDLn6hnyYgfALNmbnOxqfo8XgA0k50eE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50cc15d083f6554b68138a570e8dd46add3e9da4" title="vllm setup for nvidia (can use llama)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Having recently nabbed 2x 3090 second hand and playing around with ollama, I wanted to make better use of both cards. I created this setup (based on a few blog posts) for prepping Ubuntu 24.04 and then running vllm with single or multiple GPU.&lt;/p&gt; &lt;p&gt;I thought it might make it easier for those with less technically ability. Note that I am still learning all this myself (Quantization, Context size), but it works!&lt;/p&gt; &lt;p&gt;On a clean machine this worked perfectly to then get up and running.&lt;/p&gt; &lt;p&gt;You can provide other models via flags or edit the api_server.py to change my defaults (&amp;quot;model&amp;quot;: &amp;quot;RedHatAI/gemma-3-27b-it-quantized.w4a16&amp;quot;).&lt;/p&gt; &lt;p&gt;I then use roocode in vscode to access the openAI compatible API, but other plugins should work.&lt;/p&gt; &lt;p&gt;Now back to playing!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Superb-Security-578"&gt; /u/Superb-Security-578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/aloonj/vllm-nvidia"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nww6jy/vllm_setup_for_nvidia_can_use_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nww6jy/vllm_setup_for_nvidia_can_use_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T11:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw8jbn</id>
    <title>Ring Flash 2.0 104B A6B with Linear Attention released a few days ago</title>
    <updated>2025-10-02T16:28:11+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8jbn/ring_flash_20_104b_a6b_with_linear_attention/"&gt; &lt;img alt="Ring Flash 2.0 104B A6B with Linear Attention released a few days ago" src="https://external-preview.redd.it/arTReyF0GyVAVaEDNDlfVvJyFYJ0q7EWSfcCybSgWt0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af2d40f335ae44a32d6d45dc30487a7c16511fa4" title="Ring Flash 2.0 104B A6B with Linear Attention released a few days ago" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-flash-linear-2.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8jbn/ring_flash_20_104b_a6b_with_linear_attention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8jbn/ring_flash_20_104b_a6b_with_linear_attention/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T16:28:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nww2m7</id>
    <title>Deep dive: Optimizing LLM inference for speed &amp; efficiency — lessons learned from real-world experiments</title>
    <updated>2025-10-03T11:06:15+00:00</updated>
    <author>
      <name>/u/tony_silkworm</name>
      <uri>https://old.reddit.com/user/tony_silkworm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="http://trungtranthanh.medium.com/the-art-of-llm-inference-fast-fit-and-free-c9faf1190d78"&gt;trungtranthanh.medium.com/the-art-of-llm-inference-fast-fit-and-free-c9faf1190d78&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tony_silkworm"&gt; /u/tony_silkworm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nww2m7/deep_dive_optimizing_llm_inference_for_speed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nww2m7/deep_dive_optimizing_llm_inference_for_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nww2m7/deep_dive_optimizing_llm_inference_for_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T11:06:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwpshs</id>
    <title>Granite 4 H Tiny Q8 in RTX 3090, It's a context king.</title>
    <updated>2025-10-03T04:39:05+00:00</updated>
    <author>
      <name>/u/Plotozoario</name>
      <uri>https://old.reddit.com/user/Plotozoario</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm testing the Granite 4 H Tiny Q8 in the LM Studio, and holy moly, you can set the context window up to 1M and keep solid 50-60 tokens/s using a single RTX 3090 24Gb + 48GB RAM DDR4 3200mhz with Flash attention enabled. How far we come!! &lt;/p&gt; &lt;p&gt;Unfortunately i didn't tested yet the degradation of the model after the 100k tokens.&lt;/p&gt; &lt;p&gt;What is your vision about this new model and its new context management?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plotozoario"&gt; /u/Plotozoario &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwpshs/granite_4_h_tiny_q8_in_rtx_3090_its_a_context_king/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwpshs/granite_4_h_tiny_q8_in_rtx_3090_its_a_context_king/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwpshs/granite_4_h_tiny_q8_in_rtx_3090_its_a_context_king/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T04:39:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw2ghd</id>
    <title>GLM 4.6 is nice</title>
    <updated>2025-10-02T12:31:10+00:00</updated>
    <author>
      <name>/u/theodordiaconu</name>
      <uri>https://old.reddit.com/user/theodordiaconu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I bit the bullet and sacrificed 3$ (lol) for a &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt; subscription as I can't run this behemoth locally. And because I'm a very generous dude I wanted them to keep the full margin instead of going through routers.&lt;/p&gt; &lt;p&gt;For convenience, I created a simple 'glm' bash script that starts claude with env variables (that point to z.ai). I type glm and I'm locked in.&lt;/p&gt; &lt;p&gt;Previously I experimented a lot with OW models with GPT-OSS-120B, GLM 4.5, KIMI K2 0905, Qwen3 Coder 480B (and their latest variant included which is only through 'qwen' I think) honestly they were making silly mistakes on the project or had trouble using agentic tools (many failed edits) and abandoned their use quickly in favor of the king: gpt-5-high. I couldn't even work with Sonnet 4 unless it was frontend.&lt;/p&gt; &lt;p&gt;This specific project I tested it on is an open-source framework I'm working on, and it's not very trivial to work on a framework that wants to adhere to 100% code coverage for every change, every little addition/change has impacts on tests, on documentation on lots of stuff. Before starting any task I have to feed the whole documentation.&lt;/p&gt; &lt;p&gt;GLM 4.6 is in another class for OW models. I felt like it's an equal to GPT-5-high and Claude 4.5 Sonnet. Ofcourse this is an early vibe-based assessment, so take it with a grain of sea salt.&lt;/p&gt; &lt;p&gt;Today I challenged them (Sonnet 4.5, GLM 4.6) to refactor a class that had 600+ lines. And I usually have bad experiences when asking for refactors with all models.&lt;/p&gt; &lt;p&gt;Sonnet 4.5 could not make it reach 100% on its own after refactor, started modifying existing tests and sort-of found a silly excuse for not reaching 100% it stopped at 99.87% and said that it's the testing's fault (lmao).&lt;/p&gt; &lt;p&gt;Now on the other hand, GLM 4.6, it worked for 10 mins I think?, ended up with a perfect result. It understood the assessment. They both had interestingly similar solutions to refactoring, so planning wise, both were good and looked like they really understood the task. I never leave an agent run without reading its plan first.&lt;/p&gt; &lt;p&gt;I'm not saying it's better than Sonnet 4.5 or GPT-5-High, I just tried it today, all I can say for a fact is that it's a different league for open weight, perceived on this particular project.&lt;/p&gt; &lt;p&gt;Congrats &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt;&lt;br /&gt; What OW models do you use for coding?&lt;/p&gt; &lt;p&gt;LATER_EDIT: the 'bash' script since a few asked in ~/.local/bin on Mac: &lt;a href="https://pastebin.com/g9a4rtXn"&gt;https://pastebin.com/g9a4rtXn&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theodordiaconu"&gt; /u/theodordiaconu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2ghd/glm_46_is_nice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2ghd/glm_46_is_nice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2ghd/glm_46_is_nice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T12:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwgma3</id>
    <title>A Summary of Key AI Events from September 2025</title>
    <updated>2025-10-02T21:27:57+00:00</updated>
    <author>
      <name>/u/nh_local</name>
      <uri>https://old.reddit.com/user/nh_local</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;ByteDance released &lt;strong&gt;Seedream 4.0&lt;/strong&gt;, a next-generation image model unifying high-quality text-to-image generation and natural-language image editing.&lt;/li&gt; &lt;li&gt;An advanced Gemini variant, reported as &lt;strong&gt;Gemini 2.5 - Deep Think&lt;/strong&gt;, achieved gold-medal-level performance at the ICPC World Finals programming contest.&lt;/li&gt; &lt;li&gt;OpenAI reported a reasoning and code model achieved a perfect score (12/12) in ICPC testing.&lt;/li&gt; &lt;li&gt;Suno released &lt;strong&gt;Suno v5&lt;/strong&gt;, an upgrade in music generation with studio-grade fidelity and more natural-sounding vocals.&lt;/li&gt; &lt;li&gt;Alibaba unveiled &lt;strong&gt;Qwen-3-Max&lt;/strong&gt;, its flagship model with over a trillion parameters, focusing on long context and agent capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Wan 2.5&lt;/strong&gt; was released, a generative video model focused on multi-shot consistency and character animation.&lt;/li&gt; &lt;li&gt;Anthropic announced &lt;strong&gt;Claude Sonnet 4.5&lt;/strong&gt;, a model optimized for coding, agent construction, and improved reasoning.&lt;/li&gt; &lt;li&gt;OpenAI released &lt;strong&gt;Sora 2&lt;/strong&gt;, a flagship video and audio generation model with improved physical modeling and synchronized sound.&lt;/li&gt; &lt;li&gt;DeepSeek released &lt;strong&gt;DeepSeek-V3.2-Exp&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;OpenAI and NVIDIA announced a strategic partnership for NVIDIA to supply at least &lt;strong&gt;10 gigawatts&lt;/strong&gt; of AI systems for OpenAI's infrastructure.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nh_local"&gt; /u/nh_local &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwgma3/a_summary_of_key_ai_events_from_september_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwgma3/a_summary_of_key_ai_events_from_september_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwgma3/a_summary_of_key_ai_events_from_september_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T21:27:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwuslg</id>
    <title>Qwen2.5 VL for OCR</title>
    <updated>2025-10-03T09:52:37+00:00</updated>
    <author>
      <name>/u/Jastibute</name>
      <uri>https://old.reddit.com/user/Jastibute</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been living in the dark ages up until today. I've asked ChatGPT maybe 50 questions over the years but overall I've not used AI past this. But today I discovered Qwen for OCR which sounds very interesting to me because I've had the need to scan thousands of pages of various books for a number of years now and I think finally this is becoming a possibility cheaply. I was initially looking at Tesseract and I might yet go down this route because it means not needing to buy expensive hardware or paying cloud services and it might be good enough for my needs but I would like to entertain the idea of Qwen. I would like to self host it. The only problem is video cards. I can justify one new 16GB or maybe a 20GB video card but that's it. Don't want to go into video card farming. Once I finish scanning a dozen or so books, I don't see a need for AI for me for the foreseeable future. Will continue living in the dark ages unless another use case surfaces for me.&lt;/p&gt; &lt;p&gt;Q is: I don't care about speed. I don't know how AI works but if it needs to offload to RAM and move slowly, I don't care as long as the quality is the same and it gets there eventually. I've currently got an 8GB video card. Is this capable of running say Qwen3-VL albeit slowly or does this model have a minimum requirement? I'm taking about this in the context of OCR with good quality images.&lt;/p&gt; &lt;p&gt;I have 2.5 in the heading, but found that 3 is out already while typing this up and forgot to change the heading.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jastibute"&gt; /u/Jastibute &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwuslg/qwen25_vl_for_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwuslg/qwen25_vl_for_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwuslg/qwen25_vl_for_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T09:52:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwibsb</id>
    <title>Ming V2 is out</title>
    <updated>2025-10-02T22:37:08+00:00</updated>
    <author>
      <name>/u/Chance_Camp3720</name>
      <uri>https://old.reddit.com/user/Chance_Camp3720</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ming V2 is already out&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/inclusionAI/ming-v2-68ddea4954413c128d706630"&gt;https://huggingface.co/collections/inclusionAI/ming-v2-68ddea4954413c128d706630&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chance_Camp3720"&gt; /u/Chance_Camp3720 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwibsb/ming_v2_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwibsb/ming_v2_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwibsb/ming_v2_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T22:37:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw5kkc</id>
    <title>It's been a long time since Google released a new Gemma model.</title>
    <updated>2025-10-02T14:37:55+00:00</updated>
    <author>
      <name>/u/ArcherAdditional2478</name>
      <uri>https://old.reddit.com/user/ArcherAdditional2478</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was here using Gemma 3 4B, a model that I can confidently say has so far been the best of its size, something truly usable: it’s super coherent in Portuguese (not just in English and Chinese) and even gives me solid image recognition. It allowed me to process personal stuff without having to throw it into some obscure cloud. After seeing so many amazing releases, but with little focus on being multilingual, I deeply missed seeing Google release a new Gemma. And judging by the pace of AI evolution, it’s been about 35 years since Google last released a new Gemma, let’s be honest.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArcherAdditional2478"&gt; /u/ArcherAdditional2478 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw5kkc/its_been_a_long_time_since_google_released_a_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw5kkc/its_been_a_long_time_since_google_released_a_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw5kkc/its_been_a_long_time_since_google_released_a_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T14:37:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwimej</id>
    <title>GLM 4.6 Local Gaming Rig Performance</title>
    <updated>2025-10-02T22:50:00+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwimej/glm_46_local_gaming_rig_performance/"&gt; &lt;img alt="GLM 4.6 Local Gaming Rig Performance" src="https://preview.redd.it/0peifhs11ssf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c59d73d412700f4d4389356e00f3193eb466bc1" title="GLM 4.6 Local Gaming Rig Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sad there is no GLM-4.6-Air (seems unlikely it will be released, but who knows). So instead I cooked the &lt;code&gt;ubergarm/GLM-4.6-GGUF&lt;/code&gt; &lt;code&gt;smol-IQ2_KS&lt;/code&gt; 97.990 GiB (2.359 BPW) quant which is just a little bigger than full Q8_0 Air.&lt;/p&gt; &lt;p&gt;It is running well on my local gaming rig with 96GB RAM + 24 GB VRAM. I can get up to 32k context, or can do some trade-offs between PP and TG speeds and context length. &lt;/p&gt; &lt;p&gt;The graph is &lt;code&gt;llama-sweep-bench&lt;/code&gt; showing how quantizing kv-cache gives a steeper drop off on TG for this architecture which I observed similarly in the older GLM-4.5.&lt;/p&gt; &lt;p&gt;Have fun running quants of these big models at home on your gaming rig! The huggingface repo has some metrics comparing quality vs size trade-offs and folks over on AI Beavers Discord have a lot of KLD metrics comparing various available quants from different quant cookers so pick the right size for your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0peifhs11ssf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwimej/glm_46_local_gaming_rig_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwimej/glm_46_local_gaming_rig_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T22:50:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw8c6y</id>
    <title>Granite 4.0 Micro (3.4B) running 100% locally in your browser w/ WebGPU acceleration</title>
    <updated>2025-10-02T16:20:56+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8c6y/granite_40_micro_34b_running_100_locally_in_your/"&gt; &lt;img alt="Granite 4.0 Micro (3.4B) running 100% locally in your browser w/ WebGPU acceleration" src="https://external-preview.redd.it/aG1yZ2k0M3Y0cXNmMTjBkk0zpHe1cUKuUpjTdKuc-czjYGWzckCtqtrm-IdD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2fd6b6a33eda5421f6a81ae5b65f2f068b49e13c" title="Granite 4.0 Micro (3.4B) running 100% locally in your browser w/ WebGPU acceleration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/14cmif4v4qsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8c6y/granite_40_micro_34b_running_100_locally_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8c6y/granite_40_micro_34b_running_100_locally_in_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T16:20:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw52ad</id>
    <title>Introducing Onyx - a fully open source chat UI with RAG, web search, deep research, and MCP</title>
    <updated>2025-10-02T14:18:05+00:00</updated>
    <author>
      <name>/u/Weves11</name>
      <uri>https://old.reddit.com/user/Weves11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw52ad/introducing_onyx_a_fully_open_source_chat_ui_with/"&gt; &lt;img alt="Introducing Onyx - a fully open source chat UI with RAG, web search, deep research, and MCP" src="https://external-preview.redd.it/ODh3bjRsOWJpcHNmMcggpjsEMzF-IE1l8vJahmQmeeToARZwc_P-uEOcis7p.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=491d98e181b37d6e6d0003c442bfc14ebbed0594" title="Introducing Onyx - a fully open source chat UI with RAG, web search, deep research, and MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weves11"&gt; /u/Weves11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vklzqk9bipsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw52ad/introducing_onyx_a_fully_open_source_chat_ui_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw52ad/introducing_onyx_a_fully_open_source_chat_ui_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T14:18:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw2wd6</id>
    <title>Granite 4.0 Language Models - a ibm-granite Collection</title>
    <updated>2025-10-02T12:51:10+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection/"&gt; &lt;img alt="Granite 4.0 Language Models - a ibm-granite Collection" src="https://external-preview.redd.it/dG6nrEEPIkS2YfUpzm-ii0PPK1xkTA3ZMcynqcTCXQc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=374e83fed526e7600d653259e65b30be13801c21" title="Granite 4.0 Language Models - a ibm-granite Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Granite 4, &lt;strong&gt;32B-A9B, 7B-A1B, and 3B&lt;/strong&gt; dense models available.&lt;/p&gt; &lt;p&gt;GGUF's are in the same repo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-quantized-models-67f944eddd16ff8e057f115c"&gt;https://huggingface.co/collections/ibm-granite/granite-quantized-models-67f944eddd16ff8e057f115c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-40-language-models-6811a18b820ef362d9e5a82c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T12:51:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwnlp8</id>
    <title>How has everyone been liking Granite 4?</title>
    <updated>2025-10-03T02:43:24+00:00</updated>
    <author>
      <name>/u/SpicyWangz</name>
      <uri>https://old.reddit.com/user/SpicyWangz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How does it compare to similar models for you?&lt;/p&gt; &lt;p&gt;So far I've been testing out the 7b model and it's been performing really well on my benchmarks for a model of that size. I think I've found a new go-to model for that class.&lt;/p&gt; &lt;p&gt;The output looks fairly plaintext without much formatting or markdown. I'd probably like to see a little more structure and variation from it, but I prefer plain to the table hell that I've gotten from gpt-oss-20b.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpicyWangz"&gt; /u/SpicyWangz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwnlp8/how_has_everyone_been_liking_granite_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwnlp8/how_has_everyone_been_liking_granite_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwnlp8/how_has_everyone_been_liking_granite_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T02:43:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwovv8</id>
    <title>Granite-4.0-H-Tiny vs. OLMoE: Rapid AI improvements</title>
    <updated>2025-10-03T03:49:33+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwovv8/granite40htiny_vs_olmoe_rapid_ai_improvements/"&gt; &lt;img alt="Granite-4.0-H-Tiny vs. OLMoE: Rapid AI improvements" src="https://preview.redd.it/q7lat3zxjtsf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db27566f8b03ab0a4d8599a0ebfc454ee0ea0790" title="Granite-4.0-H-Tiny vs. OLMoE: Rapid AI improvements" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, just looking at some of the new model releases and wanted to share a quick comparison I made that really shows how fast things are moving in the world of open-source LLMs.&lt;/p&gt; &lt;p&gt;I've been tracking and comparing a couple of Mixture of Experts models that have a similar dense and active parameters, in this case a 7B total parameter count with 1B active parameters. With today's Granite release we can compare OLMoE, which came out in January, and the new Granite-4.0-H-Tiny model that just dropped today.&lt;/p&gt; &lt;p&gt;The side-by-side results are pretty wild for just a 10-month difference. The new Granite model is straight-up better on every single metric we can compare. It's not just a small improvement, either. We're talking huge jumps in areas like math, coding, and general knowledge.&lt;/p&gt; &lt;p&gt;Things are advancing really fast, just to give a little more perspective, the new Granite-4.0-H-Tiny has a similar MMLU score to Llama 2 70B that came out on January 2024 but the granite model can run at reasonable speeds even on a potato PC with CPU inference, I still remember the old days when people were happy that Llama 2 70B could run at 2tk/s on their machines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q7lat3zxjtsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwovv8/granite40htiny_vs_olmoe_rapid_ai_improvements/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwovv8/granite40htiny_vs_olmoe_rapid_ai_improvements/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T03:49:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwr6sb</id>
    <title>How's granite 4 small 32B going for you?</title>
    <updated>2025-10-03T06:01:18+00:00</updated>
    <author>
      <name>/u/mr_zerolith</name>
      <uri>https://old.reddit.com/user/mr_zerolith</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I notice that it's almost twice as fast as my current favorite, SEED OSS 36B. 79 tokens/sec starting from a blank context, but this speed doesn't seem to degrade as you fill up the context. &lt;/p&gt; &lt;p&gt;Accuracy on some hard questions is a little challenging ( less smart than SEED OSS ) but it does good with clarifications.&lt;br /&gt; Output length is short and to the point, doesn't spam you with emojis, fancy formatting or tables ( i like this ) &lt;/p&gt; &lt;p&gt;Memory consumption is extremely low per K of context, I don't understand how i can jack the context up to 512k and run it on a 5090. Memory usage doesn't seem to climb as i fill up the context either.&lt;/p&gt; &lt;p&gt;First impressions are good. There may be something special here. Let me know what your experiences look like.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_zerolith"&gt; /u/mr_zerolith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwr6sb/hows_granite_4_small_32b_going_for_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwr6sb/hows_granite_4_small_32b_going_for_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwr6sb/hows_granite_4_small_32b_going_for_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T06:01:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwkzq7</id>
    <title>Huawei Develop New LLM Quantization Method (SINQ) that's 30x Faster than AWQ and Beats Calibrated Methods Without Needing Any Calibration Data</title>
    <updated>2025-10-03T00:36:48+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwkzq7/huawei_develop_new_llm_quantization_method_sinq/"&gt; &lt;img alt="Huawei Develop New LLM Quantization Method (SINQ) that's 30x Faster than AWQ and Beats Calibrated Methods Without Needing Any Calibration Data" src="https://external-preview.redd.it/nBFUIJw0Ejvd09O6shC9aA8_DA1taNSIvE_cak2wtlo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac886e4b9ca714a3746fa6d670ba959d7721d3a2" title="Huawei Develop New LLM Quantization Method (SINQ) that's 30x Faster than AWQ and Beats Calibrated Methods Without Needing Any Calibration Data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2509.22944"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwkzq7/huawei_develop_new_llm_quantization_method_sinq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwkzq7/huawei_develop_new_llm_quantization_method_sinq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T00:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwukd5</id>
    <title>Granite4 -1M context window, and no one even noticed?</title>
    <updated>2025-10-03T09:38:28+00:00</updated>
    <author>
      <name>/u/Western_Courage_6563</name>
      <uri>https://old.reddit.com/user/Western_Courage_6563</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How is it, when IBM drops a model, no one notice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Western_Courage_6563"&gt; /u/Western_Courage_6563 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwukd5/granite4_1m_context_window_and_no_one_even_noticed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwukd5/granite4_1m_context_window_and_no_one_even_noticed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwukd5/granite4_1m_context_window_and_no_one_even_noticed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T09:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwv4q0</id>
    <title>A list of models released or udpated last week on this sub, in case you missed any (3rd Oct)</title>
    <updated>2025-10-03T10:12:59+00:00</updated>
    <author>
      <name>/u/aifeed-fyi</name>
      <uri>https://old.reddit.com/user/aifeed-fyi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We had an interesting week in releases this week (Open &amp;amp; Closed).&lt;/p&gt; &lt;p&gt;Here is the weekly list of models, I found discussed on LocalLlama &lt;em&gt;this week.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Please update or let me know in the comments if there are any mistakes or misses. Good Friday!&lt;/p&gt; &lt;h1&gt;Model Releases &amp;amp; Updates&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Reddit&lt;/th&gt; &lt;th align="left"&gt;HF / GH&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu75l3/glm_46_is_out_and_its_going_against_claude_45/"&gt;GLM-4.6&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;LLM 200k ctx&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu75l3/glm_46_is_out_and_its_going_against_claude_45/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.6"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"&gt;DeepSeek-V3.2-Exp&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;LLM exp/base&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection"&gt;Granite 4.0&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;IBM LLM collection&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-quantized-models-67f944eddd16ff8e057f115c"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nwibsb/ming_v2_is_out/"&gt;Ming V2&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Multimodal collection&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nwibsb/ming_v2_is_out/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/inclusionAI/ming-v2-68ddea4954413c128d706630"&gt;HF Collection&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1nvltym"&gt;LFM2-Audio-1.5&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Audio&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1nvltym"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-Audio-1.5B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.liquid.ai/blog/introducing-liquid-nanos-frontier-grade-performance-on-everyday-devices"&gt;LiquidAI&lt;/a&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuyjp9/liquidai_bet_on_small_but_mighty_model/"&gt; &lt;/a&gt;nanos&lt;/td&gt; &lt;td align="left"&gt;Small task LLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuyjp9/liquidai_bet_on_small_but_mighty_model/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/LiquidAI/liquid-nanos-68b98d898414dd94d4d5f99a"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nt2l57/qwen3_omni_awq_released/"&gt;Qwen3 Omni AWQ&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;30B 4bit AWQ&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nt2l57/qwen3_omni_awq_released/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/cpatonn/Qwen3-Omni-30B-A3B-Instruct-AWQ-4bit"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu44n4/1t_open_source_reasoning_model_with_50b_activation/"&gt;Ring-1T-preview&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1T reasoning 50B Active&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu44n4/1t_open_source_reasoning_model_with_50b_activation/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T-preview"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw8jbn/ring_flash_20_104b_a6b_with_linear_attention"&gt;RingFlash linea r 2&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;LLM 104B MOE&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw8jbn/ring_flash_20_104b_a6b_with_linear_attention"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-flash-linear-2.0"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu1rul/lingmini20_finally_almost_here_lets_push_context/"&gt;Ling-mini-2.0&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;16B LLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu1rul/lingmini20_finally_almost_here_lets_push_context/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-mini-2.0-GGUF"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrvo9g/finally_internvl3_5_flash_versions_coming/"&gt;InternVL3_5 Flash&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Vision-language&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrvo9g/finally_internvl3_5_flash_versions_coming/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-8B-Flash"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrhr13/k2think_32b_reasoning_model_from_uae/"&gt;K2-Think 32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B reasoning&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrhr13/k2think_32b_reasoning_model_from_uae/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LLM360/K2-Think-32B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nv5uw8/dont_sleep_on_apriel1515bthinker_and_snowpiercer/"&gt;Apriel-1.5-15b-Thinker&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;15B multimodal&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nv5uw8/dont_sleep_on_apriel1515bthinker_and_snowpiercer/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-1.5-15b-Thinker"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuxdd4/release_finally_a_working_8bit_quantized/"&gt;VibeVoice 1.8.0 (8-bit)&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;8-bit speech&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuxdd4/release_finally_a_working_8bit_quantized/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/FabioSarracino/VibeVoice-Large-Q8"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;🧰 Resources &amp;amp; Tools&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Name&lt;/th&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Reddit&lt;/th&gt; &lt;th align="left"&gt;Link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://v.redd.it/vklzqk9bipsf1"&gt;Onyx&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Open-source Chat UI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://v.redd.it/vklzqk9bipsf1"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;–&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ntiua9/we_just_opensourced_kroko_asr_a_fast_streaming/"&gt;Kroko ASR&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Speech recognition&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ntiua9/we_just_opensourced_kroko_asr_a_fast_streaming/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://kroko.ai"&gt;kroko.ai&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu3slg/an_opensource_omni_chatbot_for_long_speech_and/"&gt;MGM-Omni&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Omni chatbot&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu3slg/an_opensource_omni_chatbot_for_long_speech_and/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/dvlab-research/MGM-Omni"&gt;GitHub&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrrgoy/monkesearch_technical_report_out_now/"&gt;monkeSearch Report&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Research/benchmark&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrrgoy/monkesearch_technical_report_out_now/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://monkesearch.github.io/"&gt;monkesearch.github.io&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aifeed-fyi"&gt; /u/aifeed-fyi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwv4q0/a_list_of_models_released_or_udpated_last_week_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwv4q0/a_list_of_models_released_or_udpated_last_week_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwv4q0/a_list_of_models_released_or_udpated_last_week_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T10:12:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect — Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect — Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I’m Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM – 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
