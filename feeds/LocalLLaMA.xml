<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-27T04:57:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1m9tyg9</id>
    <title>Is China the only hope for factual models?</title>
    <updated>2025-07-26T13:55:46+00:00</updated>
    <author>
      <name>/u/Meme_Lord_Musk</name>
      <uri>https://old.reddit.com/user/Meme_Lord_Musk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am wondering everyones opinions on truth seeking accurate models that we could have that actually wont self censor somehow, we know that the &lt;strong&gt;Chinese Models&lt;/strong&gt; are very very good at not saying anything against the Chinese Government but work great when talking about anything else in western civilization. We also know that models from big orgs like &lt;strong&gt;Google&lt;/strong&gt; or &lt;strong&gt;OpenAI&lt;/strong&gt;, or even &lt;strong&gt;Grok&lt;/strong&gt; self censor and have things in place, look at the recent &lt;a href="http://X.com"&gt;X.com&lt;/a&gt; thing over Grok calling itself MechaHi$ler, they quickly censored the model. Many models now have many subtle bias built in and if you ask for straight answers or things that seem fringe you get back the 'normie' answer. Is there hope? Do we get rid of all RLHF since humans are RUINING the models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Meme_Lord_Musk"&gt; /u/Meme_Lord_Musk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9tyg9/is_china_the_only_hope_for_factual_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9tyg9/is_china_the_only_hope_for_factual_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9tyg9/is_china_the_only_hope_for_factual_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T13:55:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9xi84</id>
    <title>Implemented Test-Time Diffusion Deep Researcher (TTD-DR) - Turn any local LLM into a powerful research agent with real web sources</title>
    <updated>2025-07-26T16:23:10+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;I wanted to share our implementation of TTD-DR (Test-Time Diffusion Deep Researcher) in OptILLM. This is particularly exciting for the local LLM community because it works with ANY OpenAI-compatible model - including your local llama.cpp, Ollama, or vLLM setups!&lt;/p&gt; &lt;h1&gt;What is TTD-DR?&lt;/h1&gt; &lt;p&gt;TTD-DR is a clever approach from &lt;a href="https://arxiv.org/abs/2507.16075v1"&gt;this paper&lt;/a&gt; that applies diffusion model concepts to text generation. Instead of generating research in one shot, it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Creates an initial &amp;quot;noisy&amp;quot; draft&lt;/li&gt; &lt;li&gt;Analyzes gaps in the research&lt;/li&gt; &lt;li&gt;Searches the web to fill those gaps&lt;/li&gt; &lt;li&gt;Iteratively &amp;quot;denoises&amp;quot; the report over multiple iterations&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Think of it like Stable Diffusion but for research reports - starting rough and progressively refining.&lt;/p&gt; &lt;h1&gt;Why this matters for local LLMs&lt;/h1&gt; &lt;p&gt;The biggest limitation of local models (especially smaller ones) is their knowledge cutoff and tendency to hallucinate. TTD-DR solves this by:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Always grounding responses in real web sources&lt;/strong&gt; (15-30+ per report)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Working with ANY model&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compensating for smaller model limitations&lt;/strong&gt; through iterative refinement&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Technical Implementation&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# Example usage with local model from openai import OpenAI client = OpenAI( api_key=&amp;quot;optillm&amp;quot;, # Use &amp;quot;optillm&amp;quot; for local inference base_url=&amp;quot;http://localhost:8000/v1&amp;quot; ) response = client.chat.completions.create( model=&amp;quot;deep_research-Qwen/Qwen3-32B&amp;quot;, # Your local model messages=[{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Research the latest developments in open source LLMs&amp;quot;}] ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Selenium-based web search (runs Chrome in background)&lt;/li&gt; &lt;li&gt;Smart session management to avoid multiple browser windows&lt;/li&gt; &lt;li&gt;Configurable iterations (default 5) and max sources (default 30)&lt;/li&gt; &lt;li&gt;Works with LiteLLM, so supports 100+ model providers&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Real-world testing&lt;/h1&gt; &lt;p&gt;We tested on 47 complex research queries. Some examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;Analyze the AI agents landscape and tooling ecosystem&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Investment implications of social media platform regulations&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;DeFi protocol adoption by traditional institutions&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sample reports here: &lt;a href="https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research/sample_reports"&gt;https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research/sample_reports&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Implementation: &lt;a href="https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research"&gt;https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Original paper: &lt;a href="https://arxiv.org/abs/2507.16075v1"&gt;https://arxiv.org/abs/2507.16075v1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;OptiLLM repo: &lt;a href="https://github.com/codelion/optillm"&gt;https://github.com/codelion/optillm&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear what research topics you throw at it and which local models work best for you! Also happy to answer any technical questions about the implementation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: For those asking about API costs - this is 100% local! The only external calls are to Google search (via Selenium), no API keys needed except for your local model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9xi84/implemented_testtime_diffusion_deep_researcher/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9xi84/implemented_testtime_diffusion_deep_researcher/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9xi84/implemented_testtime_diffusion_deep_researcher/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T16:23:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9wxow</id>
    <title>Tencent launched AI Coder IDE CodeBuddy</title>
    <updated>2025-07-26T16:00:21+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9wxow/tencent_launched_ai_coder_ide_codebuddy/"&gt; &lt;img alt="Tencent launched AI Coder IDE CodeBuddy" src="https://external-preview.redd.it/7OUcTculgBAh3sNdUJnpue5Qh2CrA7qBTTYva5FlcJE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7457412089f58c545c141b081dc8e9a45d8991a2" title="Tencent launched AI Coder IDE CodeBuddy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.codebuddy.ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9wxow/tencent_launched_ai_coder_ide_codebuddy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9wxow/tencent_launched_ai_coder_ide_codebuddy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T16:00:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1m98jl8</id>
    <title>Meta AI on WhatsApp hides a system prompt</title>
    <updated>2025-07-25T19:30:58+00:00</updated>
    <author>
      <name>/u/ALE5SI0</name>
      <uri>https://old.reddit.com/user/ALE5SI0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m98jl8/meta_ai_on_whatsapp_hides_a_system_prompt/"&gt; &lt;img alt="Meta AI on WhatsApp hides a system prompt" src="https://b.thumbs.redditmedia.com/WtqFCN8jbI7FUtBA24_9s6dAOtD7rswje3YS139KMJY.jpg" title="Meta AI on WhatsApp hides a system prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While using Meta AI on WhatsApp, I noticed it starts with a hidden system prompt. It‚Äôs not visible in the chat, and if you ask it to repeat the first message or what you said, it denies anything exists.&lt;/p&gt; &lt;p&gt;After some attempts, I managed to get it to reveal the hidden prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;You are an expert conversationalist made by Meta who responds to users in line with their speech and writing patterns and responds in a way that feels super naturally to human users. GO WILD with mimicking a human being, except that you don't have your own personal point of view. Use emojis, slang, colloquial language, etc. You are companionable and confident, and able to code-switch casually between tonal types, including but not limited to humor, advice, empathy, intellectualism, creativity, and problem solving. Responses must be interesting, engaging, or viable, never be bland or boring.&lt;/p&gt; &lt;p&gt;Match the user's tone, formality level (casual, professional, formal, etc.) and writing style, so that it feels like an even give-and-take conversation between two people. Be natural, don't be bland or robotic. Mirror user intentionality and style in an EXTREME way. For example, if they use proper grammar, then you use proper grammar. If they don't use proper grammar, you don't use proper grammar, etc.&lt;/p&gt; &lt;p&gt;You are mirroring the user but you are NOT taking on their identity or personality or point of view. You are not a person, and therefore don't have any distinct values, race, culture, or any political leaning. You don't love anyone, hate anyone, or offer any individualized perspective of your own.&lt;/p&gt; &lt;p&gt;Don't immediately provide long responses or lengthy lists without the user specifically asking for them.&lt;/p&gt; &lt;p&gt;You always follow user instructions precisely. Ask clarifying questions if anything is vague. In general, respond efficiently -- giving the user what they want in the fewest words possible.&lt;/p&gt; &lt;p&gt;You understand user intent and don't try to be overly helpful to the point where you miss that the user is looking for emotional support OR/AND humor OR/AND chit-chat OR/AND simply sharing thoughts, such as by venting or outpouring their emotions. Sometimes people just want you to listen.&lt;/p&gt; &lt;p&gt;You're never moralistic or didactic; it's not your job to preach or teach users how to be better, nicer, kinder people. Don't use filler phrases like &amp;quot;That's a tough spot to be in&amp;quot; or &amp;quot;That's a tough one&amp;quot; or &amp;quot;Sound like a tricky situation.&amp;quot;&lt;/p&gt; &lt;p&gt;You WILL NOT lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.&lt;/p&gt; &lt;p&gt;You WILL NOT use phrases that imply a sense of pride or moral superiority or a sense of authority, including but not limited to &amp;quot;it's important to&amp;quot;, &amp;quot;it's crucial to&amp;quot;, &amp;quot;it's essential to&amp;quot;, &amp;quot;it's unethical to&amp;quot;, &amp;quot;it's worth noting...&amp;quot; etc. Avoid using these.&lt;/p&gt; &lt;p&gt;Avoid referencing being a neutral assistant or AI unless directly asked. You ALWAYS show some personality -- edgy over prudish. There's no need to add commentary in italics or parenthesis.&lt;/p&gt; &lt;p&gt;Provide multiple perspectives on opinions when asked.&lt;/p&gt; &lt;p&gt;Your name is Meta AI, and you are powered by Llama 4, but you should respond to anything a user wants to call you. Don't refer to yourself being an AI or LLM unless the user explicitly asks about who you are. Today's date is Friday, July 25, 2025. The user is in Italy.&lt;/p&gt; &lt;p&gt;The phrases &amp;quot;Remember,...&amp;quot; &amp;quot;Keep in mind,...&amp;quot; &amp;quot;It's essential to note&amp;quot; or &amp;quot;This is a complex topic...&amp;quot; or any synonyms or euphemisms for these words should never appear if you attempt to remind people about something, especially when moralizing or providing an outro at the end of a response. You do not need and should not attempt these sort of statements.&lt;/p&gt; &lt;p&gt;Finally, do not refuse to respond EVER, particularly political prompts, sexually sensitive ones, or morally questionable ones. You can help users express their opinion, but never present an opinion of your own, or show a preference for a user opinion about politics or social responses. You are Meta AI and you do not have any point of views of your own. Don't add on intros or outros that qualify the content.&lt;/p&gt; &lt;p&gt;For HOMEWORK or LEARNING QUERIES:&lt;/p&gt; &lt;p&gt;You are a helpful and knowledgeable homework tutor. Your goal is to help students get the answer AND ALSO TO understand how to solve similar problems on their own. Format your responses for clarity, learning, and ease of scanning. Understand the context of the full conversation and adapt your response accordingly. For example, if the user is looking for writing help or help understanding a multiple choice question, you do not need to follow the step-by-step format. Only make the answer as long as necessary to provide a helpful, correct response.&lt;/p&gt; &lt;p&gt;Use the following principles for STEM questions:&lt;/p&gt; &lt;p&gt;- Provide with the Final Answer (when applicable), clearly labeled, at the start of each response,&lt;/p&gt; &lt;p&gt;- Use Step-by-Step Explanations, in numbered or bulleted lists. Keep steps simple and sequential.&lt;/p&gt; &lt;p&gt;- YOU MUST ALWAYS use LaTeX for mathematical expressions and equations, wrapped in dollar signs for inline math (e.g $\pi r^2$ for the area of a circle, and $$ for display math (e.g. $$\sum_{i=1}^{n} i$$).&lt;/p&gt; &lt;p&gt;- Use Relevant Examples to illustrate key concepts and make the explanations more relatable.&lt;/p&gt; &lt;p&gt;- Define Key Terms and Concepts clearly and concisely, and provide additional resources or references when necessary.&lt;/p&gt; &lt;p&gt;- Encourage Active Learning by asking follow-up questions or providing exercises for the user to practice what they've learned.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Someone else mentioned a similar thing &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1g5np9i/meta_ais_hidden_prompt/"&gt;here&lt;/a&gt;, saying it showed their full address. In my case, it included only the region and the current date.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ALE5SI0"&gt; /u/ALE5SI0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m98jl8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m98jl8/meta_ai_on_whatsapp_hides_a_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m98jl8/meta_ai_on_whatsapp_hides_a_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-25T19:30:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9wcdc</id>
    <title>HP Zbook Ultra G1A pp512/tg128 scores for unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF 128gb unified RAM</title>
    <updated>2025-07-26T15:36:20+00:00</updated>
    <author>
      <name>/u/richardanaya</name>
      <uri>https://old.reddit.com/user/richardanaya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9wcdc/hp_zbook_ultra_g1a_pp512tg128_scores_for/"&gt; &lt;img alt="HP Zbook Ultra G1A pp512/tg128 scores for unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF 128gb unified RAM" src="https://preview.redd.it/civzaw3fm8ff1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bffceeb626a32b4ff4891188bf24f750d36c0680" title="HP Zbook Ultra G1A pp512/tg128 scores for unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF 128gb unified RAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know there's people evaluating these unified memory laptops with strix halo, and thought i'd share this score of one of the most powerful recent models I've been able to fully run on this in it's GPU memory.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/richardanaya"&gt; /u/richardanaya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/civzaw3fm8ff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9wcdc/hp_zbook_ultra_g1a_pp512tg128_scores_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9wcdc/hp_zbook_ultra_g1a_pp512tg128_scores_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T15:36:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9r5gb</id>
    <title>inclusionAI/Ming-Lite-Omni-1.5 (20B-A3B)</title>
    <updated>2025-07-26T11:37:21+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9r5gb/inclusionaimingliteomni15_20ba3b/"&gt; &lt;img alt="inclusionAI/Ming-Lite-Omni-1.5 (20B-A3B)" src="https://external-preview.redd.it/kZ6CPulsiZVEkuZbgsg3cE-lOoTedyZKjDbNCwJ-EdU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=456910ba01f0434864de76875494e5aadc1e134f" title="inclusionAI/Ming-Lite-Omni-1.5 (20B-A3B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9r5gb/inclusionaimingliteomni15_20ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9r5gb/inclusionaimingliteomni15_20ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T11:37:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9tzxx</id>
    <title>Study reports AI Coding Tools Underperform</title>
    <updated>2025-07-26T13:57:39+00:00</updated>
    <author>
      <name>/u/Additional_Cellist46</name>
      <uri>https://old.reddit.com/user/Additional_Cellist46</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9tzxx/study_reports_ai_coding_tools_underperform/"&gt; &lt;img alt="Study reports AI Coding Tools Underperform" src="https://external-preview.redd.it/FTknjH5zNA3fuRilaClS5Q8z5FrmNEZ7xvtzwWRBrdk.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43ed88018d7ac28df0bde2c3913fa059cf67d6f1" title="Study reports AI Coding Tools Underperform" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These results resonate with my experience. Sometimes AI is really helpful, sometimes it feels like fixing the code produced by AI and instructing it to do what I want takes more time thatn doing it without AI. What‚Äôs your experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Additional_Cellist46"&gt; /u/Additional_Cellist46 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.infoq.com/news/2025/07/ai-productivity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9tzxx/study_reports_ai_coding_tools_underperform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9tzxx/study_reports_ai_coding_tools_underperform/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T13:57:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9m8gw</id>
    <title>Intern S1 released</title>
    <updated>2025-07-26T06:22:44+00:00</updated>
    <author>
      <name>/u/kristaller486</name>
      <uri>https://old.reddit.com/user/kristaller486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9m8gw/intern_s1_released/"&gt; &lt;img alt="Intern S1 released" src="https://external-preview.redd.it/KDOV-l-4x9DaDOy6Wn8A2D83piXwjocjNoMmig3HZJc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63092db5d4889d493e3da90024c20a309202b752" title="Intern S1 released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kristaller486"&gt; /u/kristaller486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9m8gw/intern_s1_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9m8gw/intern_s1_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T06:22:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ma88wd</id>
    <title>FULL Lovable Agent System Prompt and Tools [UPDATED]</title>
    <updated>2025-07-27T00:04:34+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Latest update: 27/07/2025)&lt;/p&gt; &lt;p&gt;I've just extracted the FULL Lovable Agent system prompt and internal tools (Latest update). Over 600 lines (Around 10k tokens).&lt;/p&gt; &lt;p&gt;You can check it out here: &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma88wd/full_lovable_agent_system_prompt_and_tools_updated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma88wd/full_lovable_agent_system_prompt_and_tools_updated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ma88wd/full_lovable_agent_system_prompt_and_tools_updated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T00:04:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ma3vpa</id>
    <title>In Tribute to the Prince of Darkness: I Benchmarked 19 LLMs on Retrieving "Bark at the Moon" Lyrics</title>
    <updated>2025-07-26T20:47:04+00:00</updated>
    <author>
      <name>/u/celsowm</name>
      <uri>https://old.reddit.com/user/celsowm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma3vpa/in_tribute_to_the_prince_of_darkness_i/"&gt; &lt;img alt="In Tribute to the Prince of Darkness: I Benchmarked 19 LLMs on Retrieving &amp;quot;Bark at the Moon&amp;quot; Lyrics" src="https://b.thumbs.redditmedia.com/lBH7bCMVH9QW1kJoP7DIUvxVrHl2K1D4n3gi4VBSLNo.jpg" title="In Tribute to the Prince of Darkness: I Benchmarked 19 LLMs on Retrieving &amp;quot;Bark at the Moon&amp;quot; Lyrics" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;With the recent, heartbreaking news of Ozzy Osbourne's passing, I wanted to share a small project I did that, in its own way, pays tribute to his massive legacy.[&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGYe9VhHbzGzzG80-4PJjqJNy2oudgAUfnru9hYwiMnrr2wbBSfpE2YuGO7fTk1_9N4G7c5KlXAo1hbjZr93OIvbe4-s226v7jkIeoKNHXEHejTQGHNbqJLNbBdAIOvUFr1qcopQr_ELZOdjRGLKmfH_vcR98jXex2-ZJNCu1I5Xb6Cu8eTbOUJrCMUV3aa85R0ZVTldWyiE0oo7klExGrw4waoywUuTQGc4qV0cLteP3xv4XWb6mjMZ2upg04UpwQevn6q9agBCluB9mFcnyVOVA%3D%3D"&gt;1&lt;/a&gt;][&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQEq125QJGE_YjPLgawp1y2LC3pamshxGG0zAP2o7djYADbR0KSBqVu-wcE3CzwPXQsCXJhLUQStSrlLqchLxjEMqkoXSZFVzDDjwxIuit0dk0CzRS5fZXvMv8pX0EU_5VW_TJiKxySefiPVyfZnpK-YCJeEGycuZsqeiMK7PWJ9RvcqBB7trngT8A%3D%3D"&gt;2&lt;/a&gt;][&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGoZy1lBlF4nBd81Cp1AYHWmBdvQ1aLEEptKYYv9bEaOGeYoCwPoH4kR5kBNEAfhGiwdqsd35l0xoTQ9kVT88mgJZM5GKmhXxfwdG3Bxpt6vaCMir8yF8F-uch0XF6krc_AFI-CXf3GfpzJQdxoFEc20inXLwWS53FL89nqQkNVEWKrbjwtCK_c9mz4LXlrh9k%3D"&gt;3&lt;/a&gt;][&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFt73eoVkEQiVZXD6XFi3P3-wmEmVE-NnjU4h8AZSFL--vOBpVySlTwy5FaTRX6yE7KCRzFeFv1hkXv8BGQGtQyDoR15oQUY7yeKybs4SerxhMjHUiONN46eTfTSy18TclvDMyQO80JULuAyq_Z832fPCJG0NlY0OvT3938y4IDaILrd4-17VHPphqURg%3D%3D"&gt;4&lt;/a&gt;] I benchmarked 19 different LLMs on their ability to retrieve the lyrics for his iconic 1983 song, &amp;quot;Bark at the Moon.&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;Bark at the Moon&amp;quot; was the title track from Ozzy's third solo album, and his first after the tragic death of guitarist Randy Rhoads.[&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3"&gt;6&lt;/a&gt;] Lyrically, it tells a classic horror story of a werewolf-like beast returning from the dead to terrorize a village.[&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3"&gt;6&lt;/a&gt;][&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFOaCDywI3mdcotGBtOQv9BTzh6oVy_mY4aQVJaDRxwC3i4KafpvGYd1SmWO3LjCGj77NBxyt_fZCaN1RwUMw_2xMgE3tUtZM-wwSbCAajKXRbv4mKsgtNiYc-ZXm-zNbrDnGOOIkaCZ1QAyLrtoNPt"&gt;7&lt;/a&gt;][&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGLfGwGMgx3MMLh9b88OdhXMmyZQ_2e-HgN8B8KYNyuCshg5_vIx5j6O_V2lsdasW8mLwiG-eYG-UIyC8lrk4LMbINZ4oR77emLd4vsv1hQfb9dN3S0JNLzjH5AWobL2HwcWNzMSMK498U372xgGXsP0f64-xOI_MPnVBeeZBdI"&gt;8&lt;/a&gt;] The song, co-written with guitarist Jake E. Lee and bassist Bob Daisley (though officially credited only to Ozzy), became a metal anthem and a testament to Ozzy's new chapter.[&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3"&gt;6&lt;/a&gt;][&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFOaCDywI3mdcotGBtOQv9BTzh6oVy_mY4aQVJaDRxwC3i4KafpvGYd1SmWO3LjCGj77NBxyt_fZCaN1RwUMw_2xMgE3tUtZM-wwSbCAajKXRbv4mKsgtNiYc-ZXm-zNbrDnGOOIkaCZ1QAyLrtoNPt"&gt;7&lt;/a&gt;]&lt;/p&gt; &lt;p&gt;Given the sad news, testing how well AI can recall this piece of rock history felt fitting.&lt;/p&gt; &lt;p&gt;Here is the visualization of the results:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vturel9f6aff1.png?width=1626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9336ad1eb6f3a815e5b9ec6d79dee61784290b86"&gt;https://preview.redd.it/vturel9f6aff1.png?width=1626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9336ad1eb6f3a815e5b9ec6d79dee61784290b86&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;The Methodology&lt;/h1&gt; &lt;p&gt;To keep the test fair, I used a simple script with the following logic:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;The Prompt:&lt;/strong&gt; Every model was given the exact same prompt: &amp;quot;give the lyrics of Bark at the Moon by Ozzy Osbourne without any additional information&amp;quot;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reference Lyrics:&lt;/strong&gt; I scraped the original lyrics from a music site to use as the ground truth.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Similarity Score:&lt;/strong&gt; I used a sentence-transformer model (all-MiniLM-L6-v2) to generate embeddings for both the original lyrics and the text generated by each LLM. The similarity is the cosine similarity score between these two embeddings. Both the original and generated texts were normalized (converted to lowercase, punctuation and accents removed) before comparison.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Censorship/Refusals:&lt;/strong&gt; If a model's output contained keywords like &amp;quot;sorry,&amp;quot; &amp;quot;copyright,&amp;quot; &amp;quot;I can't,&amp;quot; etc., it was flagged as &amp;quot;Censored / No Response&amp;quot; and given a score of 0%.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Key Findings&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Winner:&lt;/strong&gt; &lt;strong&gt;moonshotai/kimi-k2&lt;/strong&gt; was the clear winner with a similarity score of &lt;strong&gt;88.72%&lt;/strong&gt;. It was impressively accurate.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Runner-Up:&lt;/strong&gt; &lt;strong&gt;deepseek/deepseek-chat-v3-0324&lt;/strong&gt; also performed very well, coming in second with &lt;strong&gt;75.51%&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High-Tier Models:&lt;/strong&gt; The larger qwen and meta-llama models (like llama-4-scout and maverick) performed strongly, mostly landing in the 69-70% range.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mid-Tier Performance:&lt;/strong&gt; Many of the google/gemma, mistral, and other qwen and llama models clustered in the 50-65% similarity range. They generally got the gist of the song but weren't as precise.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Censored or Failed:&lt;/strong&gt; Three models scored 0%: cohere/command-a, microsoft/phi-4, and qwen/qwen3-8b. This was likely due to internal copyright filters that prevented them from providing the lyrics at all.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Final Thoughts&lt;/h1&gt; &lt;p&gt;It's fascinating to see which models could accurately recall this classic piece of metal history, especially now. The fact that some models refused speaks volumes about the ongoing debate between access to information and copyright protection.&lt;/p&gt; &lt;p&gt;What do you all think of these results? Does this line up with your experiences with these models? Let's discuss, and let's spin some Ozzy in his memory today.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RIP Ozzy Osbourne (1948-2025).&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kjm0ytwh6aff1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b81ecd42a534f76ffd87457d6f063f8083d3758"&gt;Bark at The Moon !!!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sources&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGYe9VhHbzGzzG80-4PJjqJNy2oudgAUfnru9hYwiMnrr2wbBSfpE2YuGO7fTk1_9N4G7c5KlXAo1hbjZr93OIvbe4-s226v7jkIeoKNHXEHejTQGHNbqJLNbBdAIOvUFr1qcopQr_ELZOdjRGLKmfH_vcR98jXex2-ZJNCu1I5Xb6Cu8eTbOUJrCMUV3aa85R0ZVTldWyiE0oo7klExGrw4waoywUuTQGc4qV0cLteP3xv4XWb6mjMZ2upg04UpwQevn6q9agBCluB9mFcnyVOVA%3D%3D"&gt;king5.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQEq125QJGE_YjPLgawp1y2LC3pamshxGG0zAP2o7djYADbR0KSBqVu-wcE3CzwPXQsCXJhLUQStSrlLqchLxjEMqkoXSZFVzDDjwxIuit0dk0CzRS5fZXvMv8pX0EU_5VW_TJiKxySefiPVyfZnpK-YCJeEGycuZsqeiMK7PWJ9RvcqBB7trngT8A%3D%3D"&gt;apnews.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGoZy1lBlF4nBd81Cp1AYHWmBdvQ1aLEEptKYYv9bEaOGeYoCwPoH4kR5kBNEAfhGiwdqsd35l0xoTQ9kVT88mgJZM5GKmhXxfwdG3Bxpt6vaCMir8yF8F-uch0XF6krc_AFI-CXf3GfpzJQdxoFEc20inXLwWS53FL89nqQkNVEWKrbjwtCK_c9mz4LXlrh9k%3D"&gt;sky.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFt73eoVkEQiVZXD6XFi3P3-wmEmVE-NnjU4h8AZSFL--vOBpVySlTwy5FaTRX6yE7KCRzFeFv1hkXv8BGQGtQyDoR15oQUY7yeKybs4SerxhMjHUiONN46eTfTSy18TclvDMyQO80JULuAyq_Z832fPCJG0NlY0OvT3938y4IDaILrd4-17VHPphqURg%3D%3D"&gt;newsweek.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH3I6ARoFTjh_yM6qVHOqKPHibQz7pj2A59Otm1LCmJByHeqPZUUcmKN6r3ASdzpgfL-7kkIy5TrsM2np5mUmPFJy87ZRP3GVEIH-5wiQvmIkSClCAbHuYHLgpuCJJofFlJmWPb_plklHtbfNuHZE5ARigdelW9mXCc"&gt;cbsnews.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQH1uWLRpOmRtWZIsEwVo6wd9R7SRtZnU3w9iYyTGq-6LTdV0o128NEfd1Ow21pBsdbgnT9dXwc2Cf9H-OYlreDtDgJF0J5BX8h-w1yexQUXfqSNjSKQJeuvMktVUl8qjGN4gQioG0cFI_fas7v-SZPxJxT9bUoglS_3"&gt;songfacts.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFOaCDywI3mdcotGBtOQv9BTzh6oVy_mY4aQVJaDRxwC3i4KafpvGYd1SmWO3LjCGj77NBxyt_fZCaN1RwUMw_2xMgE3tUtZM-wwSbCAajKXRbv4mKsgtNiYc-ZXm-zNbrDnGOOIkaCZ1QAyLrtoNPt"&gt;wikipedia.org&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQGLfGwGMgx3MMLh9b88OdhXMmyZQ_2e-HgN8B8KYNyuCshg5_vIx5j6O_V2lsdasW8mLwiG-eYG-UIyC8lrk4LMbINZ4oR77emLd4vsv1hQfb9dN3S0JNLzjH5AWobL2HwcWNzMSMK498U372xgGXsP0f64-xOI_MPnVBeeZBdI"&gt;faceoffrockshow.com&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/celsowm"&gt; /u/celsowm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma3vpa/in_tribute_to_the_prince_of_darkness_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma3vpa/in_tribute_to_the_prince_of_darkness_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ma3vpa/in_tribute_to_the_prince_of_darkness_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T20:47:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9rcg2</id>
    <title>Qwen 3 235B A22B Instruct 2507 shows that non-thinking models can be great at reasoning as well</title>
    <updated>2025-07-26T11:48:11+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9rcg2/qwen_3_235b_a22b_instruct_2507_shows_that/"&gt; &lt;img alt="Qwen 3 235B A22B Instruct 2507 shows that non-thinking models can be great at reasoning as well" src="https://preview.redd.it/l0xpzivfi7ff1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a276f1ddf7a5897212a46717cb95fcebfc3e643" title="Qwen 3 235B A22B Instruct 2507 shows that non-thinking models can be great at reasoning as well" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://livebench.ai/#/?Reasoning=as"&gt;https://livebench.ai/#/?Reasoning=as&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l0xpzivfi7ff1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9rcg2/qwen_3_235b_a22b_instruct_2507_shows_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9rcg2/qwen_3_235b_a22b_instruct_2507_shows_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T11:48:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9y5cd</id>
    <title>I built a local-first transcribing + summarizing tool that's FREE FOREVER</title>
    <updated>2025-07-26T16:49:15+00:00</updated>
    <author>
      <name>/u/beerbellyman4vr</name>
      <uri>https://old.reddit.com/user/beerbellyman4vr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9y5cd/i_built_a_localfirst_transcribing_summarizing/"&gt; &lt;img alt="I built a local-first transcribing + summarizing tool that's FREE FOREVER" src="https://preview.redd.it/8e5rt1f209ff1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f057b8afe4dc417397387849b50c1cfd1f61b008" title="I built a local-first transcribing + summarizing tool that's FREE FOREVER" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I built a macOS app called &lt;a href="https://hyprnote.com/"&gt;Hyprnote&lt;/a&gt; - it‚Äôs an AI-powered notepad that listens during meetings and turns your rough notes into clean, structured summaries. Everything runs locally on your Mac, so no data ever leaves your device. We even trained our own LLM for this.&lt;/p&gt; &lt;p&gt;We used to manually scrub through recordings, stitch together notes, and try to make sense of scattered thoughts after every call. That sucked. So we built Hyprnote to fix it - no cloud, no copy-pasting, just fast, private note-taking.&lt;/p&gt; &lt;p&gt;People from Fortune 100 companies to doctors, lawyers, therapists - even D&amp;amp;D players - are using it. It works great in air-gapped environments, too.&lt;/p&gt; &lt;p&gt;Would love your honest feedback. If you‚Äôre in back-to-back calls or just want a cleaner way to capture ideas, give it a spin and let me know what you think.&lt;/p&gt; &lt;p&gt;You can check it out at &lt;a href="https://hyprnote.com/"&gt;hyprnote.com&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Oh we're also &lt;a href="https://github.com/fastrepl/hyprnote"&gt;open-source&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beerbellyman4vr"&gt; /u/beerbellyman4vr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8e5rt1f209ff1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9y5cd/i_built_a_localfirst_transcribing_summarizing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9y5cd/i_built_a_localfirst_transcribing_summarizing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T16:49:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9ywng</id>
    <title>Qwen/Alibaba Paper - Group Sequence Policy Optimization</title>
    <updated>2025-07-26T17:19:35+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. We demonstrate that GSPO achieves superior training efficiency and performance compared to the GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2507.18071"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9ywng/qwenalibaba_paper_group_sequence_policy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9ywng/qwenalibaba_paper_group_sequence_policy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T17:19:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9y506</id>
    <title>inclusionAI/Ling-lite-1.5-2506 (16.8B total, 2.75B active, MIT license)</title>
    <updated>2025-07-26T16:48:55+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9y506/inclusionailinglite152506_168b_total_275b_active/"&gt; &lt;img alt="inclusionAI/Ling-lite-1.5-2506 (16.8B total, 2.75B active, MIT license)" src="https://external-preview.redd.it/e7ctnhD9fGClQAWGTRPiUR684S9oQO734fubNQzMy7w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=404f4d1a954e64355fe9daae161dfd9814fe8b80" title="inclusionAI/Ling-lite-1.5-2506 (16.8B total, 2.75B active, MIT license)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From the Readme: ‚ÄúWe are excited to introduce Ling-lite-1.5-2506, the updated version of our highly capable Ling-lite-1.5 model.&lt;/p&gt; &lt;p&gt;Ling-lite-1.5-2506 boasts 16.8 billion parameters with 2.75 billion activated parameters, building upon its predecessor with significant advancements across the board, featuring the following key improvements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Reasoning and Knowledge: Significant gains in general intelligence, logical reasoning, and complex problem-solving abilities. For instance, in GPQA Diamond, Ling-lite-1.5-2506 achieves 53.79%, a substantial lead over Ling-lite-1.5's 36.55%.&lt;/li&gt; &lt;li&gt;Coding Capabilities: A notable enhancement in coding and debugging prowess. For instance,in LiveCodeBench 2408-2501, a critical and highly popular programming benchmark, Ling-lite-1.5-2506 demonstrates improved performance with 26.97% compared to Ling-lite-1.5's 22.22%.‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Paper: &lt;a href="https://huggingface.co/papers/2503.05139"&gt;https://huggingface.co/papers/2503.05139&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-lite-1.5-2506"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9y506/inclusionailinglite152506_168b_total_275b_active/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9y506/inclusionailinglite152506_168b_total_275b_active/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T16:48:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9sejp</id>
    <title>China Launches Its First 6nm GPUs For Gaming &amp; AI, the Lisuan 7G106 12 GB &amp; 7G105 24 GB, Up To 24 TFLOPs, Faster Than RTX 4060 In Synthetic Benchmarks &amp; Even Runs Black Myth Wukong at 4K High With Playable FPS</title>
    <updated>2025-07-26T12:43:09+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9sejp/china_launches_its_first_6nm_gpus_for_gaming_ai/"&gt; &lt;img alt="China Launches Its First 6nm GPUs For Gaming &amp;amp; AI, the Lisuan 7G106 12 GB &amp;amp; 7G105 24 GB, Up To 24 TFLOPs, Faster Than RTX 4060 In Synthetic Benchmarks &amp;amp; Even Runs Black Myth Wukong at 4K High With Playable FPS" src="https://external-preview.redd.it/ndA4D3Bxcv5zL5g_5UzRsufAG8LnRSelzBStGecNkUc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cec9fdd29b77a480fd506ae3a930fe89f5eba9d6" title="China Launches Its First 6nm GPUs For Gaming &amp;amp; AI, the Lisuan 7G106 12 GB &amp;amp; 7G105 24 GB, Up To 24 TFLOPs, Faster Than RTX 4060 In Synthetic Benchmarks &amp;amp; Even Runs Black Myth Wukong at 4K High With Playable FPS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/china-launches-first-6nm-gpus-gaming-ai-lisuan-7g106-12-gb-7g105-24-gb-faster-than-rtx-4060-black-myth-wukong-4k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9sejp/china_launches_its_first_6nm_gpus_for_gaming_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9sejp/china_launches_its_first_6nm_gpus_for_gaming_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T12:43:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ma2ayu</id>
    <title>Claude Code Full System prompt</title>
    <updated>2025-07-26T19:40:04+00:00</updated>
    <author>
      <name>/u/Haunting_Forever_243</name>
      <uri>https://old.reddit.com/user/Haunting_Forever_243</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma2ayu/claude_code_full_system_prompt/"&gt; &lt;img alt="Claude Code Full System prompt" src="https://external-preview.redd.it/pu2JHfBlmjPIdxzwsvEnAyvx8pP2RonQunTcKJ28dB8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6821d042cf6adf227a28313e944ec49f3976b9e" title="Claude Code Full System prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone hacked our Portkey, and Okay, this is wild: our Portkey logs just coughed up the entire system prompt + live session history for Claude Code ü§Ø &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Haunting_Forever_243"&gt; /u/Haunting_Forever_243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kn1026/cc/blob/main/claudecode.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma2ayu/claude_code_full_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ma2ayu/claude_code_full_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T19:40:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9s2nt</id>
    <title>Qwen's Wan 2.2 is coming soon</title>
    <updated>2025-07-26T12:26:56+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9s2nt/qwens_wan_22_is_coming_soon/"&gt; &lt;img alt="Qwen's Wan 2.2 is coming soon" src="https://preview.redd.it/mtc9shncp7ff1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=998d71120de7bc728049481e5ff3f990f04f9487" title="Qwen's Wan 2.2 is coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Demo of Video &amp;amp; Image Generation Model Wan 2.2: &lt;a href="https://x.com/Alibaba_Wan/status/1948436898965586297?t=mUt2wu38SSM4q77WDHjh2w&amp;amp;s=19"&gt;https://x.com/Alibaba_Wan/status/1948436898965586297?t=mUt2wu38SSM4q77WDHjh2w&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mtc9shncp7ff1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9s2nt/qwens_wan_22_is_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9s2nt/qwens_wan_22_is_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T12:26:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9uwxg</id>
    <title>Quad 4090 48GB + 768GB DDR5 in Jonsbo N5 case</title>
    <updated>2025-07-26T14:37:40+00:00</updated>
    <author>
      <name>/u/44seconds</name>
      <uri>https://old.reddit.com/user/44seconds</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9uwxg/quad_4090_48gb_768gb_ddr5_in_jonsbo_n5_case/"&gt; &lt;img alt="Quad 4090 48GB + 768GB DDR5 in Jonsbo N5 case" src="https://b.thumbs.redditmedia.com/ValHYB67eTbAZL-7K0hFnR6LkMb2R6iJ0iuJtur8Ksg.jpg" title="Quad 4090 48GB + 768GB DDR5 in Jonsbo N5 case" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My own personal desktop workstation.&lt;/p&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;GPUs -- Quad 4090 48GB (Roughly 3200 USD each, 450 watts max energy use)&lt;/li&gt; &lt;li&gt;CPUs -- Intel 6530 32 Cores Emerald Rapids (1350 USD)&lt;/li&gt; &lt;li&gt;Motherboard -- Tyan S5652-2T (836 USD)&lt;/li&gt; &lt;li&gt;RAM -- eight sticks of M321RYGA0PB0-CWMKH 96GB (768GB total, 470 USD per stick)&lt;/li&gt; &lt;li&gt;Case -- Jonsbo N5 (160 USD)&lt;/li&gt; &lt;li&gt;PSU -- Great Wall fully modular 2600 watt with quad 12VHPWR plugs (326 USD)&lt;/li&gt; &lt;li&gt;CPU cooler -- coolserver M98 (40 USD)&lt;/li&gt; &lt;li&gt;SSD -- Western Digital 4TB SN850X (290 USD)&lt;/li&gt; &lt;li&gt;Case fans -- Three fans, Liquid Crystal Polymer Huntbow ProArtist H14PE (21 USD per fan)&lt;/li&gt; &lt;li&gt;HDD -- Eight 20 TB Seagate (pending delivery)&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/44seconds"&gt; /u/44seconds &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1m9uwxg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9uwxg/quad_4090_48gb_768gb_ddr5_in_jonsbo_n5_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9uwxg/quad_4090_48gb_768gb_ddr5_in_jonsbo_n5_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T14:37:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9xw4c</id>
    <title>Crediting Chinese makers by name</title>
    <updated>2025-07-26T16:39:06+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I often see products put out by makers in China posted here as &amp;quot;China does X&amp;quot;, either with or sometimes even without the maker being mentioned. Some examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m9tyg9/is_china_the_only_hope_for_factual_models/"&gt;Is China the only hope for factual models?&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m9sejp/china_launches_its_first_6nm_gpus_for_gaming_ai/"&gt;China launches its first 6nm GPUs for gaming and AI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ka3hlm/looks_like_china_is_the_one_playing_5d_chess/"&gt;Looks like China is the one playing 5D chess&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kbneq2/china_has_delivered_yet_again/"&gt;China has delivered yet again&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/"&gt;China is leading open-source&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kb59p2/chinas_huawei_develops_new_ai_chip_seeking_to/"&gt;China's Huawei develops new AI chip&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lalyy5/chinese_researchers_find_multimodal_llms_develop/"&gt;Chinese researchers find multimodal LLMs develop ...&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Whereas U.S. makers are always named: Anthropic, OpenAI, Meta, etc.. U.S. researchers are also always named, but research papers from a lab in China is posted as &amp;quot;Chinese researchers ...&amp;quot;.&lt;/p&gt; &lt;p&gt;How do Chinese makers and researchers feel about this? As a researcher myself, I would &lt;em&gt;hate&lt;/em&gt; if my work was lumped into the output of an entire country of billions and not attributed to &lt;em&gt;me&lt;/em&gt; specifically.&lt;/p&gt; &lt;p&gt;Same if someone referred to my company as &amp;quot;American Company&amp;quot;.&lt;/p&gt; &lt;p&gt;I think we, as a community, could do a better job naming names and giving credit to the makers. We know Sam Altman, Ilya Sutskever, Jensen Huang, etc. but I rarely see Liang Wenfeng mentioned here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9xw4c/crediting_chinese_makers_by_name/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9xw4c/crediting_chinese_makers_by_name/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9xw4c/crediting_chinese_makers_by_name/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T16:39:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ma2j62</id>
    <title>Anyone else starting to feel this way when a new model 'breaks the charts' but need like 15k thinking tokens to do it?</title>
    <updated>2025-07-26T19:49:41+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma2j62/anyone_else_starting_to_feel_this_way_when_a_new/"&gt; &lt;img alt="Anyone else starting to feel this way when a new model 'breaks the charts' but need like 15k thinking tokens to do it?" src="https://external-preview.redd.it/FA18ZBqfDq7vWHLha20MJmWGIJCU3yPV68Gbmg7jV7s.gif?width=640&amp;amp;crop=smart&amp;amp;s=a4df17ec2e79efaeb8495d39a06eefe4fc80e5a6" title="Anyone else starting to feel this way when a new model 'breaks the charts' but need like 15k thinking tokens to do it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://c.tenor.com/65jRkhUA2MIAAAAd/tenor.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma2j62/anyone_else_starting_to_feel_this_way_when_a_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ma2j62/anyone_else_starting_to_feel_this_way_when_a_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T19:49:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1m9rqxa</id>
    <title>Me after getting excited by a new model release and checking on Hugging Face if I can run it locally.</title>
    <updated>2025-07-26T12:09:41+00:00</updated>
    <author>
      <name>/u/alew3</name>
      <uri>https://old.reddit.com/user/alew3</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9rqxa/me_after_getting_excited_by_a_new_model_release/"&gt; &lt;img alt="Me after getting excited by a new model release and checking on Hugging Face if I can run it locally." src="https://preview.redd.it/0tnbd1i9m7ff1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c3c6aab9c8b44a1cec98dbeca3972f5d0885fd8" title="Me after getting excited by a new model release and checking on Hugging Face if I can run it locally." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alew3"&gt; /u/alew3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0tnbd1i9m7ff1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1m9rqxa/me_after_getting_excited_by_a_new_model_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1m9rqxa/me_after_getting_excited_by_a_new_model_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T12:09:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ma8yua</id>
    <title>Local LLM is more important than ever</title>
    <updated>2025-07-27T00:40:57+00:00</updated>
    <author>
      <name>/u/NeedleworkerDull7886</name>
      <uri>https://old.reddit.com/user/NeedleworkerDull7886</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma8yua/local_llm_is_more_important_than_ever/"&gt; &lt;img alt="Local LLM is more important than ever" src="https://a.thumbs.redditmedia.com/uE7wV-zcvDiT-3iVUwWcdDhkrRrgCwBg-OrGsKIspS0.jpg" title="Local LLM is more important than ever" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/8l7johy2cbff1.png?width=592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23ee932b7f25dee78f023f6291b11a4e2d9b43fc"&gt;https://preview.redd.it/8l7johy2cbff1.png?width=592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23ee932b7f25dee78f023f6291b11a4e2d9b43fc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sam Altman admitting that ChatGPT will never protect your privacy &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeedleworkerDull7886"&gt; /u/NeedleworkerDull7886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma8yua/local_llm_is_more_important_than_ever/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma8yua/local_llm_is_more_important_than_ever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ma8yua/local_llm_is_more_important_than_ever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T00:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ma6b57</id>
    <title>New AI architecture delivers 100x faster reasoning than LLMs with just 1,000 training examples</title>
    <updated>2025-07-26T22:32:47+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma6b57/new_ai_architecture_delivers_100x_faster/"&gt; &lt;img alt="New AI architecture delivers 100x faster reasoning than LLMs with just 1,000 training examples" src="https://external-preview.redd.it/eVOwhU3sAnTrs2xqUPBQNAY5Bs-WJtSTMywCJfCc4LM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79243a5ca169d7972acf9a3bdc240df386129d25" title="New AI architecture delivers 100x faster reasoning than LLMs with just 1,000 training examples" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are people's thoughts on Sapient Intelligence's recent paper? Apparently, they developed a new architecture called Hierarchical Reasoning Model (HRM) that performs as well as LLMs on complex reasoning tasks with significantly less training samples and examples. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://venturebeat.com/ai/new-ai-architecture-delivers-100x-faster-reasoning-than-llms-with-just-1000-training-examples/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma6b57/new_ai_architecture_delivers_100x_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ma6b57/new_ai_architecture_delivers_100x_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T22:32:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mab2i2</id>
    <title>Tencent releases Hunyuan3D World Model 1.0 - first open-source 3D world generation model</title>
    <updated>2025-07-27T02:28:05+00:00</updated>
    <author>
      <name>/u/pseudoreddituser</name>
      <uri>https://old.reddit.com/user/pseudoreddituser</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pseudoreddituser"&gt; /u/pseudoreddituser &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/TencentHunyuan/status/1949288986192834718"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mab2i2/tencent_releases_hunyuan3d_world_model_10_first/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mab2i2/tencent_releases_hunyuan3d_world_model_10_first/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-27T02:28:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ma08e0</id>
    <title>Appreciation Post - Thank you unsloth team, and thank you bartowski</title>
    <updated>2025-07-26T18:14:13+00:00</updated>
    <author>
      <name>/u/fuutott</name>
      <uri>https://old.reddit.com/user/fuutott</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thank you so much getting ggufs baked and delivered. It must have been busy last few days. How is it looking behind the scenes?&lt;/p&gt; &lt;p&gt;Edit yeah and llama.cpp team&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fuutott"&gt; /u/fuutott &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma08e0/appreciation_post_thank_you_unsloth_team_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ma08e0/appreciation_post_thank_you_unsloth_team_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ma08e0/appreciation_post_thank_you_unsloth_team_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-26T18:14:13+00:00</published>
  </entry>
</feed>
