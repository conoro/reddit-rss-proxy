<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-09T22:39:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q8grqi</id>
    <title>I built a Inference Architecture (Early exit inspired) for LLaMA-3.1 (Base) that saves ~20% Compute using SLERP &amp; Dynamic RoPE.</title>
    <updated>2026-01-09T18:51:08+00:00</updated>
    <author>
      <name>/u/Hopeful-Sherbet-3100</name>
      <uri>https://old.reddit.com/user/Hopeful-Sherbet-3100</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8grqi/i_built_a_inference_architecture_early_exit/"&gt; &lt;img alt="I built a Inference Architecture (Early exit inspired) for LLaMA-3.1 (Base) that saves ~20% Compute using SLERP &amp;amp; Dynamic RoPE." src="https://b.thumbs.redditmedia.com/RgsuNYvA_Qp2Cm0rGRtcMAj3XNbKWsFLKqVzakLDGfQ.jpg" title="I built a Inference Architecture (Early exit inspired) for LLaMA-3.1 (Base) that saves ~20% Compute using SLERP &amp;amp; Dynamic RoPE." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Long time lurker. I‚Äôve been working on a way to speed up inference without quantization or distillation.&lt;/p&gt; &lt;p&gt;I call it &lt;strong&gt;&amp;quot;Cerebellum&amp;quot;&lt;/strong&gt; It‚Äôs a parasitic architecture (hooks-based) that attaches to a frozen LLaMA-3.1-8B and forces it to &amp;quot;teleport&amp;quot; hidden states from Layer 8 directly to Layer 32 when the token is semantic/syntactic glue (e.g., &amp;quot;the&amp;quot;, &amp;quot;and&amp;quot;, or common phrases).&lt;/p&gt; &lt;p&gt;It also works on a lot models without any tweaking currently I've tested Qwen, LLama and Mistral. Gemma can work but with constrained training since they start doing some shenanigans with attention in Gemma 3.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt;&lt;br /&gt; Most early-exit implementations fail because skipping layers breaks the KV Cache coherence. The model gets amnesia or hallucinates because the attention mechanism sees a &amp;quot;gap&amp;quot; in the history.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Fix (How I hacked it):&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Deep State Projection:&lt;/strong&gt; Instead of a classifier, I trained an MLP to predict the trajectory of the final hidden state from Layer 8.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SLERP (Spherical Linear Interpolation):&lt;/strong&gt; I use SLERP to reconstruct the missing intermediate states on the hypersphere surface. This keeps the vector magnitude consistent so the Attention Heads don't see &amp;quot;faded&amp;quot; ghosts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Check:&lt;/strong&gt; I trained a tiny MLP (Linear Layer with L1 Loss) to predict model uncertainty. This replaces running the massive 500M+ param LM Head for confidence checks, making the gating cost negligible.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Exit Rate:&lt;/strong&gt; ~25-30% (mostly on Layer 8).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quality:&lt;/strong&gt; Zero observed semantic drift on 400+ token narratives.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Setup:&lt;/strong&gt; LLaMA-3.1-8B Base on L4 GPU.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vpsm24uxddcg1.png?width=1170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3358361c36e6e843bd229ccdf87e7349a8c423d7"&gt;Green = Early Exit (L8). White = Full Compute (L32).&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôve filed a provisional patent on the architecture, but I‚Äôm looking for feedback on the approach. Has anyone else tried using SLERP for cache reconstruction?&lt;/p&gt; &lt;p&gt;Happy to answer questions about the implementation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hopeful-Sherbet-3100"&gt; /u/Hopeful-Sherbet-3100 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8grqi/i_built_a_inference_architecture_early_exit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8grqi/i_built_a_inference_architecture_early_exit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8grqi/i_built_a_inference_architecture_early_exit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T18:51:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8hfsd</id>
    <title>Idea of Cluster of Strix Halo and eGPU</title>
    <updated>2026-01-09T19:16:07+00:00</updated>
    <author>
      <name>/u/lets7512</name>
      <uri>https://old.reddit.com/user/lets7512</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys,&lt;br /&gt; I wanted to ask for your opinion about the idea of having eGPU that handles prefill and prompt processing and a strix halo (one or more in a cluster) that handle the model loading (Decoding stage)&lt;br /&gt; Similar to the Exo lab setup of a DGX and a cluster of MAC studios. It's not a fair comparison as the mac studio has 4x the memory bandwidth of strix halo but I think it's worth investigating.&lt;/p&gt; &lt;p&gt;What do you think of this idea?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lets7512"&gt; /u/lets7512 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8hfsd/idea_of_cluster_of_strix_halo_and_egpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8hfsd/idea_of_cluster_of_strix_halo_and_egpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8hfsd/idea_of_cluster_of_strix_halo_and_egpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T19:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7d8bj</id>
    <title>Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt</title>
    <updated>2026-01-08T14:29:47+00:00</updated>
    <author>
      <name>/u/Prior-Arm-6705</name>
      <uri>https://old.reddit.com/user/Prior-Arm-6705</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/"&gt; &lt;img alt="Jensen Huang saying &amp;quot;AI&amp;quot; 121 times during the NVIDIA CES keynote - cut with one prompt" src="https://external-preview.redd.it/M2cyNzBqaHB4NGNnMeuNas4_kS8fQc08s_eqp1ss4JB4szq45v23OyPEbFog.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a660094eff764f4c2c968c5de87ba1bafcb35b9" title="Jensen Huang saying &amp;quot;AI&amp;quot; 121 times during the NVIDIA CES keynote - cut with one prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone had to count it. Turns out Jensen said &amp;quot;AI&amp;quot; exactly 121 times in the CES 2025 keynote.&lt;/p&gt; &lt;p&gt;I used &lt;a href="https://github.com/OpenAgentPlatform/Dive"&gt;https://github.com/OpenAgentPlatform/Dive&lt;/a&gt; (open-source MCP client) + two MCPs I made:&lt;/p&gt; &lt;p&gt;- &lt;a href="https://github.com/kevinwatt/yt-dlp-mcp"&gt;https://github.com/kevinwatt/yt-dlp-mcp&lt;/a&gt; - YouTube download&lt;br /&gt; - &lt;a href="https://github.com/kevinwatt/ffmpeg-mcp-lite"&gt;https://github.com/kevinwatt/ffmpeg-mcp-lite&lt;/a&gt; - video editing&lt;/p&gt; &lt;p&gt;&lt;strong&gt;One prompt:&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Task: Create a compilation video of every exact moment Jensen Huang says &amp;quot;AI&amp;quot;.&lt;br /&gt; Video source: &lt;a href="https://www.youtube.com/watch?v=0NBILspM4c4"&gt;https://www.youtube.com/watch?v=0NBILspM4c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Instructions:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Download video in 720p + subtitles in JSON3 format (word-level timestamps)&lt;/p&gt; &lt;p&gt;Parse JSON3 to find every &amp;quot;AI&amp;quot; instance with precise start/end times&lt;/p&gt; &lt;p&gt;Use ffmpeg to cut clips (~50-100ms padding for natural sound)&lt;/p&gt; &lt;p&gt;Concatenate all clips chronologically&lt;/p&gt; &lt;p&gt;Output: Jensen_CES_AI.mp4&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Dive chained the two MCPs together - download ‚Üí parse timestamps ‚Üí cut 121 clips ‚Üí merge. All local, no cloud.&lt;/p&gt; &lt;p&gt;If you want to see how it runs: &lt;a href="https://www.youtube.com/watch?v=u_7OtyYAX74"&gt;https://www.youtube.com/watch?v=u_7OtyYAX74&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The result is... hypnotic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prior-Arm-6705"&gt; /u/Prior-Arm-6705 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hein55gpx4cg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T14:29:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7zywf</id>
    <title>Devstral Small 2 (Q4_K_M) on 5060 Ti 16GB and Zed Agent is amazing!</title>
    <updated>2026-01-09T05:37:33+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: Here's my setup&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PC: RTX 5060 Ti 16GB, 32GB DDR5-6000 (just flexing, no RAM offloading needed here)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lmstudio-community/Devstral-Small-2-24B-Instruct-2512-GGUF"&gt;Devstral-Small-2-24B-Instruct-2512-GGUF&lt;/a&gt;, Q4_K_M, 24k context length (the lmstudio-community version was slightly faster than the one from mistral)&lt;/li&gt; &lt;li&gt;Zed editor (with Zed Agent)&lt;/li&gt; &lt;li&gt;Performance: tg 9-11 tok/s, pp ~648tok/s&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;After many failed attempts (Qwen3 Coder 30B A3B was too big for a meaningful tg speed on my card, anything smaller than 14B was trash,...) I almost gave up on the dream of having a local AI coding setup.&lt;/p&gt; &lt;p&gt;Tonight, while scrolling through &lt;a href="https://swe-rebench.com/"&gt;swe-rebench&lt;/a&gt;, I noticed that Devstral Small 2 was actually ranked above Minimax M2, and just below Kimi K2 and Minimax M2.1, I decided to give it a try.&lt;/p&gt; &lt;p&gt;I was skeptical about a dense 24B model at first, but turned out, the key is to fit everything in the GPU's 16GB VRAM, so it won't offload anything to the RAM, maintaining a good tg speed. For my case, with a 24k context, that's about 15.2GB on the card.&lt;/p&gt; &lt;p&gt;The model works great in both Claude Code and Zed Editor, by great I mean the ability to produce a thinking, then chain of tool calls to explore the codebase, read multiple files, making edits, run commands to build/test.&lt;/p&gt; &lt;p&gt;I find that using Zed Agent was slightly faster than Claude Code because the system prompt was much shorter, so I still have plently of context window for the actual project's code.&lt;/p&gt; &lt;p&gt;For the code quality, it's a mix, I let it work on a few examples using my custom Rust framework. &lt;/p&gt; &lt;p&gt;For the first attempt, I tried with a very short instruction (just like what I usually do with... Opus 4.5), something like &amp;quot;build a multi agent example using this framework&amp;quot;. Devstral generated the code but ran into some cloning issues, then it went on to modify the framework to make the code work (a classical LLM's hack).&lt;/p&gt; &lt;p&gt;When I retried with a more detailed instruction, including a clear plan and some reference code, the model was able to generate the code, run build commands to test, takes a few rounds and a few rewrites but in the end, it completed the task without me having to intervene or clarify anything else.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/9wMI57W.png"&gt;screenshot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The performance was great too, prompt processing was around ~600-650 tok/s, token gen was around 9-11 tok/s, the GPU never ran above 45C, the fans weren't too loud. And I haven't run into looping issue like other posts in this sub mentioned.&lt;/p&gt; &lt;p&gt;So I guess I can postpone the plan to sell my kidney for a 2nd GPU or a Claude Max plan now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7zywf/devstral_small_2_q4_k_m_on_5060_ti_16gb_and_zed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7zywf/devstral_small_2_q4_k_m_on_5060_ti_16gb_and_zed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7zywf/devstral_small_2_q4_k_m_on_5060_ti_16gb_and_zed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T05:37:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8853g</id>
    <title>I've seen way too many people struggling with Arabic document extraction for RAG so here's the 5-stage pipeline that actually worked for me (especially for tabular data)</title>
    <updated>2026-01-09T13:23:59+00:00</updated>
    <author>
      <name>/u/MiserableBug140</name>
      <uri>https://old.reddit.com/user/MiserableBug140</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been lurking here for a while and noticed a ton of posts about Arabic OCR/document extraction failing spectacularly. Figured I'd share what's been working for us after months of pain.&lt;/p&gt; &lt;p&gt;Most platform assume Arabic is just &amp;quot;English but right-to-left&amp;quot; which is... optimistic at best.&lt;/p&gt; &lt;p&gt;You see the problem with arabic is text flows RTL, but numbers in Arabic text flow LTR. So you extract policy #8742 as #2478. I've literally seen insurance claims get paid to the wrong accounts because of this. actual money sent to wrong people....&lt;/p&gt; &lt;p&gt;Letters change shape based on position. Take ÿ® (the letter &amp;quot;ba&amp;quot;):&lt;/p&gt; &lt;p&gt;ÿ® when isolated&lt;/p&gt; &lt;p&gt;ÿ®ŸÄ at word start&lt;/p&gt; &lt;p&gt;ŸÄÿ®ŸÄ in the middle&lt;/p&gt; &lt;p&gt;ŸÄÿ® at the end&lt;/p&gt; &lt;p&gt;Same letter. Four completely different visual forms. Your Latin-trained model sees these as four different characters. Now multiply this by 28 Arabic letters.&lt;/p&gt; &lt;p&gt;Diacritical marks completely change meaning. Same base letters, different tiny marks above/below:&lt;/p&gt; &lt;p&gt;ŸÉŸéÿ™Ÿéÿ®Ÿé = &amp;quot;he wrote&amp;quot; (active)&lt;/p&gt; &lt;p&gt;ŸÉŸèÿ™Ÿêÿ®Ÿé = &amp;quot;it was written&amp;quot; (passive)&lt;/p&gt; &lt;p&gt;ŸÉŸèÿ™Ÿèÿ® = &amp;quot;books&amp;quot; (noun)&lt;/p&gt; &lt;p&gt;This is a big issue for liability in companies who process these types of docs&lt;/p&gt; &lt;p&gt;anyway since everyone is probably reading this for the solution here's all the details :&lt;/p&gt; &lt;p&gt;Stage 1: Visual understanding before OCR&lt;/p&gt; &lt;p&gt;Use vision transformers (ViT) to analyze document structure BEFORE reading any text. This classifies the doc type (insurance policy vs claim form vs treaty - they all have different layouts), segments the page into regions (headers, paragraphs, tables, signatures), and maps table structure using graph neural networks.&lt;/p&gt; &lt;p&gt;Why graphs? Because real-world Arabic tables have merged cells, irregular spacing, multi-line content. Traditional grid-based approaches fail hard. Graph representation treats cells as nodes and spatial relationships as edges.&lt;/p&gt; &lt;p&gt;Output: &amp;quot;Moroccan vehicle insurance policy. Three tables detected at coordinates X,Y,Z with internal structure mapped.&amp;quot;&lt;/p&gt; &lt;p&gt;Stage 2: Arabic-optimized OCR with confidence scoring&lt;/p&gt; &lt;p&gt;Transformer-based OCR that processes bidirectionally. Treats entire words/phrases as atomic units instead of trying to segment Arabic letters (impossible given their connected nature).&lt;/p&gt; &lt;p&gt;Fine-tuned on insurance vocabulary so when scan quality is poor, the language model biases toward domain terms like ÿ™ÿ£ŸÖŸäŸÜ (insurance), ŸÇÿ≥ÿ∑ (premium), ŸÖÿ∑ÿßŸÑÿ®ÿ© (claim).&lt;/p&gt; &lt;p&gt;Critical part: confidence scores for every extraction. &amp;quot;94% confident this is POL-2024-7891, but 6% chance the 7 is a 1.&amp;quot; This uncertainty propagates through your whole pipeline. For RAG, this means you're not polluting your vector DB with potentially wrong data.&lt;/p&gt; &lt;p&gt;Stage 3: Spatial reasoning for table reconstruction&lt;/p&gt; &lt;p&gt;Graph neural networks again, but now for cell relationships. The GNN learns to classify: is_left_of, is_above, is_in_same_row, is_in_same_column.&lt;/p&gt; &lt;p&gt;Arabic-specific learning: column headers at top of columns (despite RTL reading), but row headers typically on the RIGHT side of rows. Merged cells spanning columns represent summary categories.&lt;/p&gt; &lt;p&gt;Then semantic role labeling. Patterns like &amp;quot;ÿ±ŸÇŸÖ-Ÿ§digits-Ÿ§digits&amp;quot; ‚Üí policy numbers. Currency amounts in specific columns ‚Üí premiums/limits. This gives you:&lt;/p&gt; &lt;p&gt;Row 1: [Header] ŸÜŸàÿπ ÿßŸÑÿ™ÿ£ŸÖŸäŸÜ | ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿä | ÿßŸÑÿ¥ÿßŸÖŸÑ | ÿ∂ÿØ ÿßŸÑÿ∫Ÿäÿ± &lt;/p&gt; &lt;p&gt;Row 2: [Data] ÿßŸÑŸÇÿ≥ÿ∑ ÿßŸÑÿ≥ŸÜŸàŸä | Ÿ°Ÿ¢Ÿ†Ÿ† ÿ±ŸäÿßŸÑ | Ÿ£Ÿ•Ÿ†Ÿ† ÿ±ŸäÿßŸÑ | Ÿ®Ÿ†Ÿ† ÿ±ŸäÿßŸÑ&lt;/p&gt; &lt;p&gt;With semantic labels: coverage_type, basic_premium, comprehensive_premium, third_party_premium.&lt;/p&gt; &lt;p&gt;Stage 4: Agentic validation (this is the game-changer)&lt;/p&gt; &lt;p&gt;AI agents that continuously check and self-correct. Instead of treating first-pass extraction as truth, the system validates:&lt;/p&gt; &lt;p&gt;Consistency: Do totals match line items? Do currencies align with locations?&lt;/p&gt; &lt;p&gt;Structure: Does this car policy have vehicle details? Health policy have member info?&lt;/p&gt; &lt;p&gt;Cross-reference: Policy number appears 5 times in the doc - do they all match?&lt;/p&gt; &lt;p&gt;Context: Is this premium unrealistically low for this coverage type?&lt;/p&gt; &lt;p&gt;When it finds issues, it doesn't just flag them. It goes back to the original PDF, re-reads that specific region with better image processing or specialized models, then re-validates.&lt;/p&gt; &lt;p&gt;Creates a feedback loop: extract ‚Üí validate ‚Üí re-extract ‚Üí improve. After a few passes, you converge on the most accurate version with remaining uncertainties clearly marked.&lt;/p&gt; &lt;p&gt;Stage 5: RAG integration with hybrid storage&lt;/p&gt; &lt;p&gt;Don't just throw everything into a vector DB. Use hybrid architecture:&lt;/p&gt; &lt;p&gt;Vector store: semantic similarity search for queries like &amp;quot;what's covered for surgical procedures?&amp;quot;&lt;/p&gt; &lt;p&gt;Graph database: relationship traversal for &amp;quot;show all policies for vehicles owned by Ahmad Ali&amp;quot;&lt;/p&gt; &lt;p&gt;Structured tables: preserved for numerical queries and aggregations&lt;/p&gt; &lt;p&gt;Linguistic chunking that respects Arabic phrase boundaries. A coverage clause with its exclusion must stay together - splitting it destroys meaning. Each chunk embedded with context (source table, section header, policy type).&lt;/p&gt; &lt;p&gt;Confidence-weighted retrieval:&lt;/p&gt; &lt;p&gt;High confidence: &amp;quot;Your coverage limit is 500,000 SAR&amp;quot;&lt;/p&gt; &lt;p&gt;Low confidence: &amp;quot;Appears to be 500,000 SAR - recommend verifying with your policy&amp;quot;&lt;/p&gt; &lt;p&gt;Very low: &amp;quot;Don't have clear info on this - let me help you locate it&amp;quot;&lt;/p&gt; &lt;p&gt;This prevents confidently stating wrong information, which matters a lot when errors have legal/financial consequences.&lt;/p&gt; &lt;p&gt;A few advices for testing this properly:&lt;/p&gt; &lt;p&gt;Don't just test on clean, professionally-typed documents. That's not production. Test on:&lt;/p&gt; &lt;p&gt;Mixed Arabic/English in same document&lt;/p&gt; &lt;p&gt;Poor quality scans or phone photos&lt;/p&gt; &lt;p&gt;Handwritten Arabic sections&lt;/p&gt; &lt;p&gt;Tables with mixed-language headers&lt;/p&gt; &lt;p&gt;Regional dialect variations&lt;/p&gt; &lt;p&gt;Test with questions that require connecting info across multiple sections, understanding how they interact. If it can't do this, it's just translation with fancy branding.&lt;/p&gt; &lt;p&gt;Wrote this up in way more detail in an article if anyone wants it(shameless plug, link in comments).&lt;/p&gt; &lt;p&gt;But genuinely hope this helps someone. Arabic document extraction is hard and most resources handwave the actual problems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MiserableBug140"&gt; /u/MiserableBug140 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8853g/ive_seen_way_too_many_people_struggling_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8853g/ive_seen_way_too_many_people_struggling_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8853g/ive_seen_way_too_many_people_struggling_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T13:23:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7ysj2</id>
    <title>We benchmarked every 4-bit quantization method in vLLM üëÄ</title>
    <updated>2026-01-09T04:38:29+00:00</updated>
    <author>
      <name>/u/LayerHot</name>
      <uri>https://old.reddit.com/user/LayerHot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7ysj2/we_benchmarked_every_4bit_quantization_method_in/"&gt; &lt;img alt="We benchmarked every 4-bit quantization method in vLLM üëÄ" src="https://b.thumbs.redditmedia.com/m8O7xkrgA45EVdPp2UmUufoulHAEZRrosao1Uv_SFws.jpg" title="We benchmarked every 4-bit quantization method in vLLM üëÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just published a deep dive on vLLM quantization. Tested AWQ, GPTQ, Marlin, GGUF, and BitsandBytes on Qwen2.5-32B using an H200.&lt;/p&gt; &lt;p&gt;Stuff we found:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Marlin hits 712 tok/s, baseline FP16 does 461. Quantized and faster.&lt;/li&gt; &lt;li&gt;GPTQ without Marlin kernel is actually slower than FP16 (276 tok/s)&lt;/li&gt; &lt;li&gt;BitsandBytes had the smallest quality drop and doesn't need pre-quantized weights&lt;/li&gt; &lt;li&gt;GGUF had the worst perplexity but best HumanEval score among quantized methods&lt;/li&gt; &lt;li&gt;AWQ was weirdly slow in vLLM (67 tok/s)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Blog covers how each technique actually works under the hood if you want the details.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/t4212ygj59cg1.png?width=3169&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=97eff0fcb212924355a7feb7262b25895de5603a"&gt;https://preview.redd.it/t4212ygj59cg1.png?width=3169&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=97eff0fcb212924355a7feb7262b25895de5603a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://docs.jarvislabs.ai/blog/vllm-quantization-complete-guide-benchmarks"&gt;https://docs.jarvislabs.ai/blog/vllm-quantization-complete-guide-benchmarks&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LayerHot"&gt; /u/LayerHot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7ysj2/we_benchmarked_every_4bit_quantization_method_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7ysj2/we_benchmarked_every_4bit_quantization_method_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7ysj2/we_benchmarked_every_4bit_quantization_method_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T04:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q82l7m</id>
    <title>Show us your llama.cpp command line arguments</title>
    <updated>2026-01-09T08:09:15+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And mention your hardware.&lt;/p&gt; &lt;p&gt;Recently I switched to llama.cpp and I have to say the hardest part was to optimise the arguments. Please share yours and if you are running it within a service or just a script, share it as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q82l7m/show_us_your_llamacpp_command_line_arguments/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q82l7m/show_us_your_llamacpp_command_line_arguments/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q82l7m/show_us_your_llamacpp_command_line_arguments/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T08:09:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q84j0r</id>
    <title>Is it just me or has CES really not delivered anything exciting for local LLM setups?</title>
    <updated>2026-01-09T10:10:02+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;CES this year has been strangely quiet imho. There's no big banger announcement. There's Phison with their AiDaptiv+ solution that supposedly extends VRAM to some SSD setup, but that's been talked about at Computex already and if I'm not mistaken a year ago, but nothing about availability. What do you think is the reason for this being so quiet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q84j0r/is_it_just_me_or_has_ces_really_not_delivered/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q84j0r/is_it_just_me_or_has_ces_really_not_delivered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q84j0r/is_it_just_me_or_has_ces_really_not_delivered/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T10:10:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8m89n</id>
    <title>Hermit-AI: Chat with 100GB+ of Wikipedia/Docs offline using a Multi-Joint RAG pipeline</title>
    <updated>2026-01-09T22:20:02+00:00</updated>
    <author>
      <name>/u/Smart-Competition200</name>
      <uri>https://old.reddit.com/user/Smart-Competition200</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8m89n/hermitai_chat_with_100gb_of_wikipediadocs_offline/"&gt; &lt;img alt="Hermit-AI: Chat with 100GB+ of Wikipedia/Docs offline using a Multi-Joint RAG pipeline" src="https://preview.redd.it/2l4v4kpedecg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6da0d5b9083950377ca03f5636085a070944dde2" title="Hermit-AI: Chat with 100GB+ of Wikipedia/Docs offline using a Multi-Joint RAG pipeline" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hermit-AI&lt;/strong&gt; because I was frustrated with the state of offline RAG.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Headache:&lt;/strong&gt; I wanted to use Local AI along side my collection of ZIM files (Wikipedia, StackExchange, etc.) entirely offline. But every tool I tried had the same issues:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Needle in a Haystack&amp;quot;&lt;/strong&gt;: Traditional vector search kept retrieving irrelevant chunks when the dataset was this huge.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hallucinations&lt;/strong&gt;: The AI would confidently agree with false premises just to be helpful.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;So I built a &amp;quot;Multi-Joint&amp;quot; Reasoning Pipeline.&lt;/strong&gt; Instead of just doing one big search and hoping for the best, Hermit breaks the process down. while not perfect i am happy with the results. I can only imagine it getting better as the efficiency and intelligence of local models improve over time. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Joint 1 (Extraction)&lt;/strong&gt;: It stops to ask &amp;quot;Who/What specifically is this user asking about?&amp;quot; before touching the database.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Joint 2 (JIT Indexing)&lt;/strong&gt;: It builds a tiny, ephemeral search index &lt;em&gt;just for that query&lt;/em&gt; on the fly. This keeps it fast and accurate without needing 64GB of RAM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Joint 3 (Verification)&lt;/strong&gt;: This is the cool part. It has a specific &amp;quot;Fact-Check&amp;quot; stage that reads the retrieved text and effectively says, &amp;quot;Wait, does this text actually support what the user is claiming?&amp;quot; If not, it corrects you.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Who is this for?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data hoarders (like me) with terabytes of ZIMs.&lt;/li&gt; &lt;li&gt;Researchers working in air-gapped environments.&lt;/li&gt; &lt;li&gt;Privacy advocates who want zero data leakage.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pure Python + &lt;code&gt;llama-cpp-python&lt;/code&gt; (GGUF models)&lt;/li&gt; &lt;li&gt;Native ZIM file support (no conversion needed)&lt;/li&gt; &lt;li&gt;FAISS for the JIT indexing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've also included a tool called &lt;strong&gt;&amp;quot;Forge&amp;quot;&lt;/strong&gt; so you can turn your own PDF/Markdown folders into ZIM files and treat them like Wikipedia.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/0nspaceshipearth/Hermit-AI"&gt;https://github.com/0nspaceshipearth/Hermit-AI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love to hear if anyone else has hit these &amp;quot;needle in a haystack&amp;quot; limits with local RAG and how you solved them!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Smart-Competition200"&gt; /u/Smart-Competition200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2l4v4kpedecg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8m89n/hermitai_chat_with_100gb_of_wikipediadocs_offline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8m89n/hermitai_chat_with_100gb_of_wikipediadocs_offline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T22:20:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8gsde</id>
    <title>Strix Halo 128GB not using more than 62.54GB??</title>
    <updated>2026-01-09T18:51:46+00:00</updated>
    <author>
      <name>/u/sputnik13net</name>
      <uri>https://old.reddit.com/user/sputnik13net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm at wits end right now and hoping someone's run in to this. I'm on unbuntu 24.04, rocm 7.1.1, below is my grub config&lt;/p&gt; &lt;p&gt;&lt;code&gt;GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;ttm.pages_limit=30408704 ttm.page_pool_size=30408704 amdgpu.gttsize=118784 iommu=pt &amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;when I load some really large workflows in comfyui (qwen image 2512 bf16 + lightning4) or try to run a diffusion model while I have gpt-oss-120b loaded via llama.cpp, I keep getting OOM indicating I'm out of memory with a max of 62.54GB allowed.&lt;/p&gt; &lt;p&gt;At minimum I'd expect it to OOM and say I have a max of 116GB.&lt;/p&gt; &lt;p&gt;Individually gpt-oss-120b works perfectly and comfyui with qwen image 2512 works perfectly.&lt;/p&gt; &lt;p&gt;When I look at rocm smi/info I see 116GB is the max GTT.&lt;/p&gt; &lt;p&gt;Anyone had similar issues?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sputnik13net"&gt; /u/sputnik13net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8gsde/strix_halo_128gb_not_using_more_than_6254gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8gsde/strix_halo_128gb_not_using_more_than_6254gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8gsde/strix_halo_128gb_not_using_more_than_6254gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T18:51:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8c6x1</id>
    <title>Real-world DGX Spark experiences after 1-2 months? Fine-tuning, stability, hidden pitfalls?</title>
    <updated>2026-01-09T16:04:22+00:00</updated>
    <author>
      <name>/u/PromptAndHope</name>
      <uri>https://old.reddit.com/user/PromptAndHope</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôd like to hear from those who have been using the DGX Spark for 1-2 months now. What‚Äôs your experience so far?&lt;/p&gt; &lt;p&gt;I‚Äôm particularly interested in fine-tuning capabilities, and I find both the NVIDIA software stack and the possibilities offered by the 128 GB of memory very appealing. I‚Äôm currently practicing on an RTX 5060 Ti 16GB, so in terms of raw performance this would be roughly comparable. The main appeal for me is the ability to work with larger models without having to build a multi-GPU rig from used cards or rely on different cloud providers.&lt;/p&gt; &lt;p&gt;Cost ( and speed) is secondary for me, because if it supports learning and skill development, I see it as a good investment.&lt;/p&gt; &lt;p&gt;What I‚Äôm more interested in hearing about are the technical downsides or challenges: setup complexity, software limitations, stability issues, bottlenecks in fine-tuning workflows, or anything else that might not be obvious at first.&lt;/p&gt; &lt;p&gt;Has anyone run into technical issues that made them regret the purchase?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PromptAndHope"&gt; /u/PromptAndHope &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8c6x1/realworld_dgx_spark_experiences_after_12_months/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8c6x1/realworld_dgx_spark_experiences_after_12_months/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8c6x1/realworld_dgx_spark_experiences_after_12_months/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T16:04:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7uuxo</id>
    <title>OK I get it, now I love llama.cpp</title>
    <updated>2026-01-09T01:39:13+00:00</updated>
    <author>
      <name>/u/vulcan4d</name>
      <uri>https://old.reddit.com/user/vulcan4d</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just made the switch from Ollama to llama.cpp. Ollama is fantastic for the beginner because it lets you super easily run LLMs and switch between them all. Once you realize what you truly want to run, llama.cpp is really the way to go.&lt;/p&gt; &lt;p&gt;My hardware ain't great, I have a single 3060 12GB GPU and three P102-100 GPUs for a total of 42GB. My system ram is 96GB along with an Intel i7-9800x. It blows my mind that with some tuning what difference it can make. You really need to understand each of the commands for llama.cpp to get the most out of it especially with uneven vram like mine. I used Chatgpt, Perplexity and suprisingly only Google AI studio could optimize my settings while teaching me along the way.&lt;/p&gt; &lt;p&gt;Crazy how these two commands both fill up the ram but one is twice as fast as the other. Chatgpt helped me with the first one, Google AI with the other ;). Now I'm happy running local lol.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;11t/s:&lt;/strong&gt;&lt;br /&gt; sudo pkill -f llama-server; sudo nvidia-smi --gpu-reset -i 0,1,2,3 || true; sleep 5; sudo CUDA_VISIBLE_DEVICES=0,1,2,3 ./llama-server --model /home/llm/llama.cpp/models/gpt-oss-120b/Q4_K_M/gpt-oss-120b-Q4_K_M-00001-of-00002.gguf --n-gpu-layers 21 --main-gpu 0 --flash-attn off --cache-type-k q8_0 --cache-type-v f16 --ctx-size 30000 --port 8080 --host &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; --mmap --numa distribute --batch-size 384 --ubatch-size 256 --jinja --threads $(nproc) --parallel 2 --tensor-split 12,10,10,10 --mlock&lt;/p&gt; &lt;p&gt;&lt;strong&gt;21t/s&lt;/strong&gt;&lt;br /&gt; sudo pkill -f llama-server; sudo nvidia-smi --gpu-reset -i 0,1,2,3 || true; sleep 5; sudo GGML_CUDA_ENABLE_UNIFIED_MEMORY=0 CUDA_VISIBLE_DEVICES=0,1,2,3 ./llama-server --model /home/llm/llama.cpp/models/gpt-oss-120b/Q4_K_M/gpt-oss-120b-Q4_K_M-00001-of-00002.gguf --n-gpu-layers 99 --main-gpu 0 --split-mode layer --tensor-split 5,5,6,20 -ot &amp;quot;blk\.(2[1-9]|[3-9][0-9])\.ffn_.*_exps\.weight=CPU&amp;quot; --ctx-size 30000 --port 8080 --host &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; --batch-size 512 --ubatch-size 256 --threads 8 --parallel 1 --mlock&lt;/p&gt; &lt;p&gt;Nothing here is worth copying and pasting as it is unique to my config but the moral of the story is, if you tune llama.cpp this thing will FLY!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vulcan4d"&gt; /u/vulcan4d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7uuxo/ok_i_get_it_now_i_love_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7uuxo/ok_i_get_it_now_i_love_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7uuxo/ok_i_get_it_now_i_love_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T01:39:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7qcux</id>
    <title>The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.</title>
    <updated>2026-01-08T22:33:33+00:00</updated>
    <author>
      <name>/u/PostEasy7183</name>
      <uri>https://old.reddit.com/user/PostEasy7183</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, ‚ÄãI‚Äôve been reading the text of the &amp;quot;NO FAKES Act&amp;quot; currently in Congress, and it‚Äôs worse than I thought. ‚ÄãThe Tldr: It creates a &amp;quot;digital replica right&amp;quot; for voices/likenesses. That sounds fine for stopping deepfake porn, but the liability language is a trap. It targets anyone who &amp;quot;makes available&amp;quot; a tool that is primarily used for replicas.&lt;br /&gt; ‚ÄãThe Problem: If you release a TTS model or a voice-conversion RVC model on HuggingFace, and someone else uses it to fake a celebrity, you (the dev) can be liable for statutory damages ($5k-$25k per violation). ‚ÄãThere is no Section 230 protection here. This effectively makes hosting open weights for audio models a legal s*icide mission unless you are OpenAI or Google.&lt;/p&gt; &lt;p&gt;What I did: I contacted my reps email to flag this as an &amp;quot;innovation killer.&amp;quot; If you run a repo or care about open weights, you might want to do the same. We need them to add a &amp;quot;Safe Harbor&amp;quot; for tool devs.&lt;/p&gt; &lt;p&gt;S.1367 - 119th Congress (2025-2026): NO FAKES Act of 2025 | Congress.gov | Library of Congress &lt;a href="https://share.google/u6dpy7ZQDvZWUrlfc"&gt;https://share.google/u6dpy7ZQDvZWUrlfc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;UPDATE: ACTION ITEMS (How to actually stop this) ‚ÄãIf you don't want to go to jail for hosting a repo, you need to make noise now. ‚Äã1. The &amp;quot;Lazy&amp;quot; Email (Takes 30 seconds): Go to Democracy.io or your Senator‚Äôs contact page. ‚ÄãSubject: Opposition to NO FAKES Act (H.R. 2794 / S. 1367) - Open Source Liability ‚ÄãMessage: &amp;quot;I am a constituent and software engineer. I oppose the NO FAKES Act unless it includes a specific Safe Harbor for Open Source Code Repositories. The current 'Digital Fingerprinting' requirement (Section 3) is technically impossible for raw model weights to comply with. This bill effectively bans open-source AI hosting in the US and hands a monopoly to Big Tech. Please amend it to protect tool developers.&amp;quot; ‚Äã2. The &amp;quot;Nuclear&amp;quot; Option (Call them): ‚ÄãCall the Capitol Switchboard: (202) 224-3121 ‚ÄãAsk for Senators Wyden (D) or Massie (R) if you want to thank them for being tech-literate, or call your own Senator to complain. ‚ÄãScript: &amp;quot;The NO FAKES Act kills open-source innovation. We need a Safe Harbor for developers who write code, separate from the bad actors who use it.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PostEasy7183"&gt; /u/PostEasy7183 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T22:33:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1q82zdm</id>
    <title>Minimax also live on Hong Kong Stock Exchange</title>
    <updated>2026-01-09T08:33:27+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q82zdm/minimax_also_live_on_hong_kong_stock_exchange/"&gt; &lt;img alt="Minimax also live on Hong Kong Stock Exchange" src="https://preview.redd.it/999goi9xbacg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5d20235c10219672401efcf3df3bcdf3da53b9a5" title="Minimax also live on Hong Kong Stock Exchange" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/999goi9xbacg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q82zdm/minimax_also_live_on_hong_kong_stock_exchange/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q82zdm/minimax_also_live_on_hong_kong_stock_exchange/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T08:33:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8aypi</id>
    <title>Tested GLM 4.7 vs MiniMax M2.1 - impressed with the performance of both</title>
    <updated>2026-01-09T15:17:53+00:00</updated>
    <author>
      <name>/u/alokin_09</name>
      <uri>https://old.reddit.com/user/alokin_09</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8aypi/tested_glm_47_vs_minimax_m21_impressed_with_the/"&gt; &lt;img alt="Tested GLM 4.7 vs MiniMax M2.1 - impressed with the performance of both" src="https://b.thumbs.redditmedia.com/_Ob25yWKAikztI0zBuNqnB-jnl-5yva82tvIgausepQ.jpg" title="Tested GLM 4.7 vs MiniMax M2.1 - impressed with the performance of both" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full transparency, I work closely with the Kilo Code team, so take this with appropriate context. That said, I think the results are genuinely interesting for anyone running local/open-weight models.&lt;/p&gt; &lt;p&gt;We ran GLM 4.7 and MiniMax M2.1 through a real coding benchmark, building a CLI task runner with 20 features (dependency management, parallel execution, caching, YAML parsing, etc.). The kind of task that would take a senior dev a day or two.&lt;/p&gt; &lt;p&gt;How it was actually tested:&lt;/p&gt; &lt;p&gt;- Phase 1: Architecture planning (Architect mode)&lt;/p&gt; &lt;p&gt;- Phase 2: Full implementation (Code mode)&lt;/p&gt; &lt;p&gt;- Both models ran uninterrupted with zero human intervention&lt;/p&gt; &lt;p&gt;Overall performance summary &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c636beit7ccg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e175e42659bcbee51d9f66d5d29ec79958a2b00"&gt;https://preview.redd.it/c636beit7ccg1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0e175e42659bcbee51d9f66d5d29ec79958a2b00&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Phase 1 results&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;GLM 4.7:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;- 741-line architecture doc with 3 Mermaid diagrams&lt;/p&gt; &lt;p&gt;- Nested structure: 18 files across 8 directories &lt;/p&gt; &lt;p&gt;- Kahn's algorithm with pseudocode, security notes, 26-step roadmap&lt;/p&gt; &lt;p&gt;&lt;em&gt;MiniMax M2.1:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;- 284-line plan, 2 diagrams - leaner but covered everything&lt;/p&gt; &lt;p&gt;- Flat structure: 9 files&lt;/p&gt; &lt;p&gt;- Used Commander.js (smart library choice vs rolling your own) &lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Plan Scoring&lt;/em&gt;&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cw1fvloq9ccg1.png?width=1014&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af5febf64d3d28f170bf693d58257c386865c814"&gt;https://preview.redd.it/cw1fvloq9ccg1.png?width=1014&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af5febf64d3d28f170bf693d58257c386865c814&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Phase 2 Results: Implementation&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Both models successfully implemented all 20 requirements. The code compiles, runs, and handles the test cases correctly without any major issues or errors.&lt;/p&gt; &lt;p&gt;Implementations include:&lt;/p&gt; &lt;p&gt;- Working topological sort with cycle detection&lt;/p&gt; &lt;p&gt;- Parallel execution with concurrency limits&lt;/p&gt; &lt;p&gt;GLM 4.7‚Äôs is more responsive to individual task completion. MiniMax M2.1‚Äôs is simpler to understand.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Implementation Scoring&lt;/em&gt;&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a1g7d8ul9ccg1.png?width=1426&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7891b07de8642aac887a1acb44a432e02c5b2c58"&gt;https://preview.redd.it/a1g7d8ul9ccg1.png?width=1426&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7891b07de8642aac887a1acb44a432e02c5b2c58&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Code Quality Differences&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;While both implementations are functional, they differ in structure and style.&lt;/p&gt; &lt;p&gt;For example, for the architecture test, GLM 4.7 created a deeply modular structure, while MiniMax M2.1 created a flat structure. &lt;/p&gt; &lt;p&gt;For error handling, GLM 4.7 created custom error classes. On the other hand, MiniMax M2.1 used standard Error objects with descriptive messages:&lt;/p&gt; &lt;p&gt;&lt;a href="https://substackcdn.com/image/fetch/$s_!9AeR!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F155ec0e4-5b77-4398-a7aa-87af0f2395e6_1629x652.png"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Regarding CLI Parsing, GLM 4.7 implemented argument parsing manually, &lt;a href="https://substackcdn.com/image/fetch/$s_!J5xk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a945a88-dfa1-4f9a-b264-070994e52806_1629x600.png"&gt;&lt;/a&gt;MiniMax M2.1 used commander.js:&lt;/p&gt; &lt;p&gt;&lt;a href="https://substackcdn.com/image/fetch/$s_!v0un!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d599b7-4ff0-48a9-8a6e-12701c009262_1629x276.png"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM 4.7‚Äôs approach has no external dependency. MiniMax M2.1‚Äôs approach is more maintainable and handles edge cases automatically.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;GLM 4.7 generated a 363-line README.md with installation instructions, configuration reference, CLI options, multiple examples, and exit code documentation.&lt;/p&gt; &lt;p&gt;Both models demonstrated genuine agentic behavior. After finishing the implementation, each model tested its own work by running the CLI with Bash and verified the output.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Cost Analysis&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://substackcdn.com/image/fetch/$s_!VUYs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa32c27b-b49d-4704-b8be-6332d4875217_794x386.png"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9pesc5s0bccg1.png?width=794&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=980ef4aacd34f33d1aa9917126a2745fde950acd"&gt;https://preview.redd.it/9pesc5s0bccg1.png?width=794&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=980ef4aacd34f33d1aa9917126a2745fde950acd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tradeoffs&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Based on our testing, GLM 4.7 is better if you want comprehensive documentation and modular architecture out of the box. It generated a full README, detailed error classes, and organized code across 18 well-separated files. The tradeoff is higher cost and some arguably over-engineered patterns like manual CLI parsing when a library would do.&lt;/p&gt; &lt;p&gt;MiniMax M2.1 is better if you prefer simpler code and lower cost. Its 9-file structure is easier to navigate, and it used established libraries like Commander.js instead of rolling its own. The tradeoff is no documentation. You‚Äôll need to add a README and inline comments yourself. &lt;/p&gt; &lt;p&gt;If you want the full breakdown with code snippets and deeper analysis, you can read it here: &lt;a href="https://blog.kilo.ai/p/open-weight-models-are-getting-serious"&gt;https://blog.kilo.ai/p/open-weight-models-are-getting-serious&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alokin_09"&gt; /u/alokin_09 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8aypi/tested_glm_47_vs_minimax_m21_impressed_with_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8aypi/tested_glm_47_vs_minimax_m21_impressed_with_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8aypi/tested_glm_47_vs_minimax_m21_impressed_with_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T15:17:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8b82f</id>
    <title>Ministral-3-14B-Reasoning: High Intelligence on Low VRAM ‚Äì A Benchmark-Comparison</title>
    <updated>2026-01-09T15:27:49+00:00</updated>
    <author>
      <name>/u/Snail_Inference</name>
      <uri>https://old.reddit.com/user/Snail_Inference</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Below you‚Äôll find a benchmark comparison of Ministral-3-14B-Reasoning-2512 against 10 other large language models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LiveCodeBench:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;LiveCodeBench&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM-4.5-Air&lt;/td&gt; &lt;td align="left"&gt;70.7%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemini 2.5 Pro Preview&lt;/td&gt; &lt;td align="left"&gt;69.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.1 Nemotron Ultra&lt;/td&gt; &lt;td align="left"&gt;66.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 32B&lt;/td&gt; &lt;td align="left"&gt;65.7%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MiniMax M1 80K&lt;/td&gt; &lt;td align="left"&gt;65.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Ministral 3 (14B Reasoning)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;64.6%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;QwQ-32B&lt;/td&gt; &lt;td align="left"&gt;63.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B A3B&lt;/td&gt; &lt;td align="left"&gt;62.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MiniMax M1 40K&lt;/td&gt; &lt;td align="left"&gt;62.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ministral 3 (8B Reasoning)&lt;/td&gt; &lt;td align="left"&gt;61.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek R1 Distill Llama&lt;/td&gt; &lt;td align="left"&gt;57.5%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;GPQA:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;GPQA&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;o1-preview&lt;/td&gt; &lt;td align="left"&gt;73.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 VL 32B Thinking&lt;/td&gt; &lt;td align="left"&gt;73.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Claude Haiku 4.5&lt;/td&gt; &lt;td align="left"&gt;73.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Next-80B-A3B-Instruct&lt;/td&gt; &lt;td align="left"&gt;72.9%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT OSS 20B&lt;/td&gt; &lt;td align="left"&gt;71.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Ministral 3 (14B Reasoning)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;71.2%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5 nano&lt;/td&gt; &lt;td align="left"&gt;71.2%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Magistral Medium&lt;/td&gt; &lt;td align="left"&gt;70.8%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 VL 30B A3B Instruct&lt;/td&gt; &lt;td align="left"&gt;70.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-4o&lt;/td&gt; &lt;td align="left"&gt;70.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MiniMax M1 80K&lt;/td&gt; &lt;td align="left"&gt;70.0%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;AIME 2024:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;AIME 2024&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Grok-3&lt;/td&gt; &lt;td align="left"&gt;93.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemini 2.5 Pro&lt;/td&gt; &lt;td align="left"&gt;92.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;o3&lt;/td&gt; &lt;td align="left"&gt;91.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1-0528&lt;/td&gt; &lt;td align="left"&gt;91.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM-4.5&lt;/td&gt; &lt;td align="left"&gt;91.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Ministral 3 (14B Reasoning 2512)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;89.8%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM-4.5-Air&lt;/td&gt; &lt;td align="left"&gt;89.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemini 2.5 Flash&lt;/td&gt; &lt;td align="left"&gt;88.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;o3-mini&lt;/td&gt; &lt;td align="left"&gt;87.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek R1 Zero&lt;/td&gt; &lt;td align="left"&gt;86.7%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek R1 Distill Llama 70B&lt;/td&gt; &lt;td align="left"&gt;86.7%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;AIME 2025:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;AIME 2025&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Next-80B-A3B-Thinking&lt;/td&gt; &lt;td align="left"&gt;87.8%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1-0528&lt;/td&gt; &lt;td align="left"&gt;87.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Claude Sonnet 4.5&lt;/td&gt; &lt;td align="left"&gt;87.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;o3&lt;/td&gt; &lt;td align="left"&gt;86.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-5 nano&lt;/td&gt; &lt;td align="left"&gt;85.2%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Ministral 3 (14B Reasoning 2512)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;85.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 VL 32B Thinking&lt;/td&gt; &lt;td align="left"&gt;83.7%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 VL 30B A3B Thinking&lt;/td&gt; &lt;td align="left"&gt;83.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemini 2.5 Pro&lt;/td&gt; &lt;td align="left"&gt;83.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 Max&lt;/td&gt; &lt;td align="left"&gt;81.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B A22B&lt;/td&gt; &lt;td align="left"&gt;81.5%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;All benchmark results are sourced from this page: &lt;a href="https://llm-stats.com/benchmarks/llm-leaderboard-full"&gt;https://llm-stats.com/benchmarks/llm-leaderboard-full&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snail_Inference"&gt; /u/Snail_Inference &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8b82f/ministral314breasoning_high_intelligence_on_low/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8b82f/ministral314breasoning_high_intelligence_on_low/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8b82f/ministral314breasoning_high_intelligence_on_low/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T15:27:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8lt9c</id>
    <title>PSA: HF seems to be removing grandfathered limits on private storage and billing people on it.</title>
    <updated>2026-01-09T22:03:39+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HF is twisting the screw on their storage billing. I believe than when they announced changes, they grandfathered in storage limits for people who were over a 1 TB limit. I got 1.34TB limit.&lt;/p&gt; &lt;p&gt;Well, now this is over and I got billed additional $25 for keeping my files as is - anything over the first 1TB is counted as another 1TB bought, at $25/TB rate. I uploaded just around 20GB since November 30th, and I wasn't billed for that 1.34TB earlier.&lt;/p&gt; &lt;p&gt;Watch out for surprise bills!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8lt9c/psa_hf_seems_to_be_removing_grandfathered_limits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8lt9c/psa_hf_seems_to_be_removing_grandfathered_limits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8lt9c/psa_hf_seems_to_be_removing_grandfathered_limits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T22:03:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8f242</id>
    <title>After 8 years building cloud infrastructure, I'm betting on local-first AI</title>
    <updated>2026-01-09T17:48:38+00:00</updated>
    <author>
      <name>/u/PandaAvailable2504</name>
      <uri>https://old.reddit.com/user/PandaAvailable2504</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sold my Saas company last year and we used to process everything in the cloud. Now, after a few realisations, I'm doing the opposite. As I watch the AI space evolve, I can‚Äôt help but wonder how there‚Äôs a growing sentiment of wanting capable models that run on hardware they control. More people seem to be moving towards local inference: whether for privacy, cost, latency, or just independence from API rate limits. &lt;/p&gt; &lt;p&gt;Curious if anyone else is thinking about this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PandaAvailable2504"&gt; /u/PandaAvailable2504 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8f242/after_8_years_building_cloud_infrastructure_im/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8f242/after_8_years_building_cloud_infrastructure_im/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8f242/after_8_years_building_cloud_infrastructure_im/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T17:48:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1q84u82</id>
    <title>Big tech companies, now "DRAM beggars," are staying in Pangyo and Pyeongtaek, demanding "give us some supplies."</title>
    <updated>2026-01-09T10:28:56+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q84u82/big_tech_companies_now_dram_beggars_are_staying/"&gt; &lt;img alt="Big tech companies, now &amp;quot;DRAM beggars,&amp;quot; are staying in Pangyo and Pyeongtaek, demanding &amp;quot;give us some supplies.&amp;quot;" src="https://external-preview.redd.it/bSPrxxtNL1oMDlmeG0HktX0ZjOAtRM_In15JbYuAojA.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81d28b6e2751adbaced23bb325679e9970b53426" title="Big tech companies, now &amp;quot;DRAM beggars,&amp;quot; are staying in Pangyo and Pyeongtaek, demanding &amp;quot;give us some supplies.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not a Korean speaker. Came across this in another sub. The TLDR is that everyone is scrambling to buy as much as they can as soon as they can, because &amp;quot;demanding a 50-60% increase in server DRAM supply prices from the previous quarter during their first-quarter negotiations with customers&amp;quot;.&lt;/p&gt; &lt;p&gt;Per the article, DDR4 prices went up from $1.40 last January to $9.30 in December (my interpretation is $/GB). If they're increasing by another 50%, that's almost $14/GB!!! So, 1TB of DDR4-3200 will cost north of $14k by Q2 if this is true ü§Ø&lt;/p&gt; &lt;p&gt;In case anyone thought things weren't already bad, it's going to get much much worse this year.&lt;/p&gt; &lt;p&gt;Here's the full Google translate of the article:&lt;/p&gt; &lt;p&gt;DRAM, a type of memory semiconductor, was the key driver behind Samsung Electronics' first-quarter operating profit surpassing 20 trillion won. DRAM products, including high-bandwidth memory (HBM), are a core component of the computing infrastructure supporting the artificial intelligence (AI) era. The semiconductor industry predicts that the DRAM shortage, which began in earnest in the second half of last year, will continue until the end of this year, with prices also expected to continue rising.&lt;/p&gt; &lt;p&gt;Samsung Electronics and SK Hynix, major suppliers of DRAM, are reportedly demanding a 50-60% increase in server DRAM supply prices from the previous quarter during their first-quarter negotiations with customers. A semiconductor industry insider reported, &amp;quot;Even with significantly higher prices, the prevailing sentiment is 'let's buy as much as we can before it gets more expensive.'&amp;quot; Recently, semiconductor purchasing managers from Silicon Valley tech companies, nicknamed &amp;quot;DRAM Beggars,&amp;quot; have been reportedly competing fiercely to secure remaining DRAM inventory at hotels in the Pangyo and Pyeongtaek areas.&lt;/p&gt; &lt;p&gt;The semiconductor industry analyzes that &amp;quot;the demand that was initially focused on HBM in the early days of the AI ‚Äã‚Äãcraze is now spreading to server DRAM, creating an unprecedented semiconductor boom.&amp;quot; DRAM is a semiconductor that manages a computer's &amp;quot;short-term memory.&amp;quot; It stores and quickly transmits necessary data when the central processing unit (CPU), the brain, performs tasks. HBM is specialized for seamlessly delivering the massive data required for AI by increasing the data transmission path (bandwidth) dozens of times compared to conventional DRAM. However, HBM is extremely expensive and has limitations in increasing capacity. This explains why big tech companies are scrambling to secure server DRAM products to store more data.&lt;/p&gt; &lt;p&gt;The average contract price of DRAM soared from $1.40 (based on 8GB DDR4) in January last year to $9.30 in December. This marks the first time in seven years and four months that DRAM prices have surpassed the $9 threshold. Kim Dong-won, head of the research center at KB Securities, said, &amp;quot;Due to this price increase, the operating profit margin (the ratio of operating profit to sales) of some general-purpose memories (widely used standard memories) is expected to reach 70%, and DDR5 may even surpass the margin of HBM3E. This year, semiconductor companies' performance is expected to be determined by general-purpose memories.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.chosun.com/economy/tech_it/2026/01/09/MZNIFPCMTZGHHPV5757NJC5QW4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q84u82/big_tech_companies_now_dram_beggars_are_staying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q84u82/big_tech_companies_now_dram_beggars_are_staying/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T10:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8fagh</id>
    <title>RTX Blackwell Pro 6000 wholesale pricing has dropped by $150-200</title>
    <updated>2026-01-09T17:57:11+00:00</updated>
    <author>
      <name>/u/TastesLikeOwlbear</name>
      <uri>https://old.reddit.com/user/TastesLikeOwlbear</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Obviously the RTX Blackwell Pro 6000 cards are of great interest to the people here. I see them come up a lot. And we all ooh and ahh over the people that have 8 of them lined up in a nice row.&lt;/p&gt; &lt;p&gt;It also seems to me like the market is suffering from lack of transparency on these.&lt;/p&gt; &lt;p&gt;My employer buys these cards wholesale, and I can see current pricing and stock in our distributors' systems. (And I &lt;strong&gt;may have&lt;/strong&gt; slipped in an order for one for myself...) It's eye-opening.&lt;/p&gt; &lt;p&gt;I'm probably not supposed to disclose the exact price we buy these at. But I wanted people to know that unlike everything else with RAM in it, the wholesale price of these has &lt;strong&gt;dropped&lt;/strong&gt; by about ~$150-200 from December to January.&lt;/p&gt; &lt;p&gt;I will also say that the wholesale price for the 6000 Pro is only about $600 higher than the wholesale price for the new 72GiB 5000 Pro. So, for the love of god, please don't buy that!&lt;/p&gt; &lt;p&gt;(And no, this is &lt;strong&gt;not&lt;/strong&gt; marketing or an ad; I &lt;strong&gt;cannot&lt;/strong&gt; sell &lt;strong&gt;anyone&lt;/strong&gt; these cards at &lt;strong&gt;any&lt;/strong&gt; price. I would be fired immediately. I just want people to have the best available information when they're looking to buy something this expensive.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TastesLikeOwlbear"&gt; /u/TastesLikeOwlbear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8fagh/rtx_blackwell_pro_6000_wholesale_pricing_has/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8fagh/rtx_blackwell_pro_6000_wholesale_pricing_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8fagh/rtx_blackwell_pro_6000_wholesale_pricing_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T17:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1q89g1i</id>
    <title>DeepSeek V4 Coming</title>
    <updated>2026-01-09T14:18:56+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to two people with direct knowledge, DeepSeek is expected to roll out a next‚Äëgeneration flagship AI model in the coming weeks that focuses on strong code‚Äëgeneration capabilities.&lt;/p&gt; &lt;p&gt;The two sources said the model, codenamed V4, is an iteration of the V3 model DeepSeek released in December 2024. Preliminary internal benchmark tests conducted by DeepSeek employees indicate the model outperforms existing mainstream models in code generation, including Anthropic‚Äôs Claude and the OpenAI GPT family.&lt;/p&gt; &lt;p&gt;The sources said the V4 model achieves a technical breakthrough in handling and parsing very long code prompts, a significant practical advantage for engineers working on complex software projects. They also said the model‚Äôs ability to understand data patterns across the full training pipeline has been improved and that no degradation in performance has been observed.&lt;/p&gt; &lt;p&gt;One of the insiders said users may find that V4‚Äôs outputs are more logically rigorous and clear, a trait that indicates the model has stronger reasoning ability and will be much more reliable when performing complex tasks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.theinformation.com/articles/deepseek-release-next-flagship-ai-model-strong-coding-ability"&gt;https://www.theinformation.com/articles/deepseek-release-next-flagship-ai-model-strong-coding-ability&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T14:18:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1q88hdc</id>
    <title>(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability</title>
    <updated>2026-01-09T13:39:02+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/"&gt; &lt;img alt="(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability" src="https://a.thumbs.redditmedia.com/V0cq5JgXRMYN60IWmZOgeWuSilWb4Gxub72PzqtA_08.jpg" title="(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(paywall): &lt;a href="https://www.theinformation.com/articles/deepseek-release-next-flagship-ai-model-strong-coding-ability"&gt;https://www.theinformation.com/articles/deepseek-release-next-flagship-ai-model-strong-coding-ability&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q88hdc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T13:39:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8hqgd</id>
    <title>I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work</title>
    <updated>2026-01-09T19:27:29+00:00</updated>
    <author>
      <name>/u/Ok-Pomegranate1314</name>
      <uri>https://old.reddit.com/user/Ok-Pomegranate1314</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/"&gt; &lt;img alt="I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work" src="https://preview.redd.it/dban4j25kdcg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4a2061b8c9510f486ad4475d7a5f9b8d3a666f7" title="I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA officially supports clustering &lt;em&gt;two&lt;/em&gt; DGX Sparks together. I wanted three.&lt;/p&gt; &lt;p&gt;The problem: each Spark has two 100Gbps ConnectX-7 ports. In a 3-node triangle mesh, each link ends up on a different subnet. NCCL's built-in networking assumes all peers are reachable from a single NIC. It just... doesn't work.&lt;/p&gt; &lt;p&gt;So I wrote a custom NCCL network plugin from scratch.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Subnet-aware NIC selection (picks the right NIC for each peer)&lt;/li&gt; &lt;li&gt;Raw RDMA verbs implementation (QP state machines, memory registration, completion queues)&lt;/li&gt; &lt;li&gt;Custom TCP handshake protocol to avoid deadlocks&lt;/li&gt; &lt;li&gt;~1500 lines of C&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The result:&lt;/strong&gt; Distributed inference across all 3 nodes at 8+ GB/s over RDMA. &lt;strong&gt;The NVIDIA support tier I'm currently on:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ Supported configs ‚úì ‚îú‚îÄ‚îÄ &amp;quot;Should work&amp;quot; configs ‚îú‚îÄ‚îÄ &amp;quot;You're on your own&amp;quot; configs ‚îú‚îÄ‚îÄ &amp;quot;Please don't call us&amp;quot; configs ‚îú‚îÄ‚îÄ &amp;quot;How did you even...&amp;quot; configs ‚îî‚îÄ‚îÄ You are here ‚Üí &amp;quot;Writing custom NCCL plugins to cluster standalone workstations over a hand-wired RDMA mesh&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GitHub link: &lt;a href="https://github.com/autoscriptlabs/nccl-mesh-plugin"&gt;https://github.com/autoscriptlabs/nccl-mesh-plugin&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the implementation. This was a mass of low-level debugging (segfaults, RDMA state machine issues, GID table problems) but it works.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Pomegranate1314"&gt; /u/Ok-Pomegranate1314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dban4j25kdcg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T19:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q8ckz0</id>
    <title>The reason why RAM has become so expensive</title>
    <updated>2026-01-09T16:18:22+00:00</updated>
    <author>
      <name>/u/InvadersMustLive</name>
      <uri>https://old.reddit.com/user/InvadersMustLive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/"&gt; &lt;img alt="The reason why RAM has become so expensive" src="https://preview.redd.it/sgbhubsomccg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57d847c1b9a2d5786b0a888b5d0d25fe5ede9e12" title="The reason why RAM has become so expensive" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InvadersMustLive"&gt; /u/InvadersMustLive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sgbhubsomccg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T16:18:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
