<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-02T14:06:10+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qtovkl</id>
    <title>A concise list of CLI coding tools similar to Claude Code</title>
    <updated>2026-02-02T07:48:29+00:00</updated>
    <author>
      <name>/u/omarous</name>
      <uri>https://old.reddit.com/user/omarous</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtovkl/a_concise_list_of_cli_coding_tools_similar_to/"&gt; &lt;img alt="A concise list of CLI coding tools similar to Claude Code" src="https://external-preview.redd.it/e8LeKEXnh1rNGS7w2PM5NtzNbfKL_22okWgABQrx7Uk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9cf530f72352769d5a4ab2fe06a5927ff3a9f1e7" title="A concise list of CLI coding tools similar to Claude Code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omarous"&gt; /u/omarous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/omarabid/cli-llm-coding"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtovkl/a_concise_list_of_cli_coding_tools_similar_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtovkl/a_concise_list_of_cli_coding_tools_similar_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T07:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtf8hk</id>
    <title>AniMUL-v1 a 30B model trained to do species classification from audio files</title>
    <updated>2026-02-02T00:01:13+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not my project, sharing this for a friend since they don't have a reddit account. Thought this was cool and wanted to share it since they put in a lot of effort (none of this is my work, so all credits to them).&lt;/p&gt; &lt;p&gt;This is a fine tune of &lt;a href="https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Instruct"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/a&gt; using Earth Species Project's &lt;a href="https://huggingface.co/datasets/EarthSpeciesProject/NatureLM-audio-training"&gt;NatureLM-audio-training&lt;/a&gt; dataset of 26 million audio-text pairs, trained on &lt;strong&gt;8x B200 GPUs for roughly 912~ hours&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Check it out in these links below!&lt;br /&gt; HF: &lt;a href="https://huggingface.co/deepcrayon/AniMUL-v1"&gt;https://huggingface.co/deepcrayon/AniMUL-v1&lt;/a&gt;&lt;br /&gt; Git Repo: &lt;a href="https://spacecruft.org/deepcrayon/AniMUL"&gt;https://spacecruft.org/deepcrayon/AniMUL&lt;/a&gt;&lt;br /&gt; Demo (try it here!): &lt;a href="https://animul.ai/"&gt;https://animul.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EDIT - They are now having quantized formats made targeting various sizes, using autoround for higher accuracy, so people with less VRAM can run this model. Look forward to these!&lt;/p&gt; &lt;p&gt;Here's how it performs compared to the base model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;================================================================================ MODEL COMPARISON REPORT AniMUL-v1 vs Qwen3-Omni Base Model ================================================================================ ================================================================================ SUMMARY STATISTICS ================================================================================ Total samples: 100 AniMUL-v1 Checkpoint (Fine-tuned): Exact matches: 75/100 (75.0%) Contains matches: 76/100 (76.0%) Average similarity: 88.23% Qwen3-Omni Base Model (Not fine-tuned): Exact matches: 14/100 (14.0%) Contains matches: 18/100 (18.0%) Average similarity: 28.80% -------------------------------------------------------------------------------- COMPARISON (AniMUL vs Qwen3-Omni): -------------------------------------------------------------------------------- ‚úì AniMUL has 61 MORE exact matches (+61.0%) ‚úì AniMUL has 58 MORE contains matches (+58.0%) ‚úì AniMUL has 59.43% HIGHER average similarity üèÜ WINNER: AniMUL-v1 (fine-tuned model performs better) ================================================================================ &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtf8hk/animulv1_a_30b_model_trained_to_do_species/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtf8hk/animulv1_a_30b_model_trained_to_do_species/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtf8hk/animulv1_a_30b_model_trained_to_do_species/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T00:01:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtnz9s</id>
    <title>Best Local Model for Openclaw</title>
    <updated>2026-02-02T06:55:41+00:00</updated>
    <author>
      <name>/u/FeiX7</name>
      <uri>https://old.reddit.com/user/FeiX7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have recently tried gpt-oss 20b for openclaw and it performed awfully...&lt;/p&gt; &lt;p&gt;openclaw requires so much context and small models intelligence degrades with such amount of context.&lt;/p&gt; &lt;p&gt;any thoughts about it and any ideas how to make the local models to perform better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeiX7"&gt; /u/FeiX7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtnz9s/best_local_model_for_openclaw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtnz9s/best_local_model_for_openclaw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtnz9s/best_local_model_for_openclaw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T06:55:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsxowq</id>
    <title>OLMO 3.5 Is Around The Corner</title>
    <updated>2026-02-01T12:52:34+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsxowq/olmo_35_is_around_the_corner/"&gt; &lt;img alt="OLMO 3.5 Is Around The Corner" src="https://preview.redd.it/bfhk9qzqpvgg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63c2040c8dfb4a24d40bb5ca076c537bef194d77" title="OLMO 3.5 Is Around The Corner" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The OLMO series is seriously under-appreciated. Yes they may not perform the best compared to other openweight models, but OLMO models are fully open sourced, from their datasets to training recipes. So it's nice to see them experiment with more niche techniques.&lt;/p&gt; &lt;p&gt;It seems like for 3.5, they'll be using some of the techniques that Qwen3-Next introduced, so long context tasks should take less memory.&lt;/p&gt; &lt;p&gt;Though this series seems to be a set of Dense models, with the smallest being a 1B model.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;OLMo 3.5 Hybrid is a hybrid architecture model from Ai2 that combines standard transformer attention layers with linear attention layers using the Gated Deltanet. This hybrid approach aims to improve efficiency while maintaining model quality by interleaving full attention layers with linear attention layers.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bfhk9qzqpvgg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsxowq/olmo_35_is_around_the_corner/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsxowq/olmo_35_is_around_the_corner/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T12:52:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtv57o</id>
    <title>Kalynt ‚Äì Privacy-first AI IDE with local LLMs , serverless P2P and more...</title>
    <updated>2026-02-02T13:33:16+00:00</updated>
    <author>
      <name>/u/FixHour8452</name>
      <uri>https://old.reddit.com/user/FixHour8452</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtv57o/kalynt_privacyfirst_ai_ide_with_local_llms/"&gt; &lt;img alt="Kalynt ‚Äì Privacy-first AI IDE with local LLMs , serverless P2P and more..." src="https://external-preview.redd.it/b2xtMGdpNjIzM2hnMXpAWw2892folmnE1MbIKakWf9HmYjVjqNZI6RoFYdwG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=98b34f0e01c0cafbc18502daca4e12e7f07ed0df" title="Kalynt ‚Äì Privacy-first AI IDE with local LLMs , serverless P2P and more..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I've been working on &lt;strong&gt;Kalynt&lt;/strong&gt;, an open-core AI IDE that prioritizes local inference and privacy. After lurking here and learning from your optimization discussions, I wanted to share what I built.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem I'm Solving:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Tools like Cursor and GitHub Copilot require constant cloud connectivity and send your code to external servers. I wanted an IDE where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Code never leaves your machine unless you explicitly choose&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLMs run locally via node-llama-cpp&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Collaboration happens P2P without servers&lt;/li&gt; &lt;li&gt;Everything works offline&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Technical Architecture:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;AIME (Artificial Intelligence Memory Engine)&lt;/strong&gt; handles the heavy lifting:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Smart context windowing to fit models in constrained memory&lt;/li&gt; &lt;li&gt;Token caching for repeated contexts&lt;/li&gt; &lt;li&gt;Optimized for 8GB machines (I built this on a Lenovo laptop)&lt;/li&gt; &lt;li&gt;Works with GGUF models through node-llama-cpp&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Currently supported models in the UI:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen models (various sizes)&lt;/li&gt; &lt;li&gt;Devstral 24B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Backend supports additional models, but UI integration is still in progress. I focused on getting Qwen working well first since it has strong coding capabilities.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real-time collaboration&lt;/strong&gt; uses CRDTs (yjs) + WebRTC for serverless sync with optional E2E encryption. Important: I don't run any signaling servers ‚Äì it uses public open signals that are fully encrypted. Your code never touches my infrastructure.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance Reality Check:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Running Qwen on 8GB RAM with acceptable response times for coding tasks. Devstral 24B is pushing the limits but usable for those with more RAM. It's not as fast as cloud APIs, but the privacy tradeoff is worth it for my use case.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Known Issues (Beta Quality):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Being completely transparent here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Build/Debug features&lt;/strong&gt; may not work consistently across all devices, particularly on Windows and macOS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent system&lt;/strong&gt; can be unreliable ‚Äì sometimes fails to complete tasks properly&lt;/li&gt; &lt;li&gt;&lt;strong&gt;P2P connection&lt;/strong&gt; occasionally fails to establish or drops unexpectedly&lt;/li&gt; &lt;li&gt;Cross-platform testing is limited (built primarily on Windows)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is genuinely beta software. I'm a solo dev who shipped fast to get feedback, not a polished product.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open-Core Model:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Core components (editor, sync, code execution, filesystem) are AGPL-3.0. Advanced agentic features are proprietary but run 100% locally. You can audit the entire sync/networking stack.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current State:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;v1.0-beta released Feb 1&lt;/li&gt; &lt;li&gt;44k+ lines of TypeScript (Electron + React)&lt;/li&gt; &lt;li&gt;Monorepo with u/ kalynt/crdt, u/ kalynt/networking, u/ kalynt/shared&lt;/li&gt; &lt;li&gt;Built in one month as a solo project&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I'm Looking For:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Feedback on AIME architecture ‚Äì is there a better approach for context management?&lt;/li&gt; &lt;li&gt;Which models should I prioritize adding to the UI next?&lt;/li&gt; &lt;li&gt;Help debugging Windows/macOS issues (I developed on Linux)&lt;/li&gt; &lt;li&gt;Performance optimization tips for local inference on consumer hardware&lt;/li&gt; &lt;li&gt;Early testers who care about privacy + local-first and can handle rough edges&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="http://github.com/Hermes-Lekkas/Kalynt"&gt;github.com/Hermes-Lekkas/Kalynt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm not here to oversell this ‚Äì expect bugs, expect things to break. But if you've been looking for a local-first alternative to cloud IDEs and want to help shape where this goes, I'd appreciate your thoughts.&lt;/p&gt; &lt;p&gt;Happy to answer technical questions about the CRDT implementation, WebRTC signaling, or how AIME manages memory.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FixHour8452"&gt; /u/FixHour8452 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y6br5u5233hg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtv57o/kalynt_privacyfirst_ai_ide_with_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtv57o/kalynt_privacyfirst_ai_ide_with_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T13:33:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qti7jk</id>
    <title>Chonkers and thermals (dual 3090)</title>
    <updated>2026-02-02T02:11:30+00:00</updated>
    <author>
      <name>/u/BetStack</name>
      <uri>https://old.reddit.com/user/BetStack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qti7jk/chonkers_and_thermals_dual_3090/"&gt; &lt;img alt="Chonkers and thermals (dual 3090)" src="https://preview.redd.it/7mmkgvsnpzgg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2427362c838d13391fa8ede8f15a2fbfcf98bf12" title="Chonkers and thermals (dual 3090)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Repurposed old hardware into start trying local. Not enthused about the spacing. Can‚Äôt vertical mount the second card and sitting here thinking. Do I stand a chance? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BetStack"&gt; /u/BetStack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7mmkgvsnpzgg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qti7jk/chonkers_and_thermals_dual_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qti7jk/chonkers_and_thermals_dual_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T02:11:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtg3sm</id>
    <title>Mistral Vibe vs Claude Code vs OpenAI Codex vs Opencode/others? Best coding model for 92GB?</title>
    <updated>2026-02-02T00:38:18+00:00</updated>
    <author>
      <name>/u/Consumerbot37427</name>
      <uri>https://old.reddit.com/user/Consumerbot37427</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've dipped my toe in the water with Mistral Vibe, using LM Studio and Devstral Small for inference. I've had pretty good success refactoring a small python project, and a few other small tasks.&lt;/p&gt; &lt;p&gt;Overall, it seems to work well on my MacBook w/ 92GB RAM, although I've encountered issues when it gets near or above 100k tokens of context. Sometimes it stops working entirely with no errors indicated in LM Studio logs, just notice the model isn't loaded anymore. Aggressively compacting the context to stay under ~80k helps.&lt;/p&gt; &lt;p&gt;I've tried plugging other models in via the config.toml, and haven't had much luck. They &amp;quot;work&amp;quot;, but not well. Lots of tool call failures, syntax errors. (I was especially excited about GLM 4.7 Air, but keep running into looping issues, no matter what inference settings I try, GGUF or MLX models, even at Q8)&lt;/p&gt; &lt;p&gt;I'm curious what my best option is at this point, or if I'm already using it. I'm open to trying anything I can run on this machine--it runs GPT-OSS-120B beautifully, but it just doesn't seem to play well with Vibe (as described above).&lt;/p&gt; &lt;p&gt;I don't really have the time or inclination to install every different CLI to see which one works best. I've heard good things about Claude Code, but I'm guessing that's only with paid cloud inference. Prefer open source anyway.&lt;/p&gt; &lt;p&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt76qs/mistral_vibe_20/o314ydx/"&gt;This comment&lt;/a&gt; on a Mistral Vibe thread says I might be best served using the tool that goes with each model, but I'm loathe to spend the time installing and experimenting.&lt;/p&gt; &lt;p&gt;Is there another proven combination of CLI coding interface and model that works as well/better than Mistral Vibe with Devstral Small? Ideally, I could run &amp;gt;100k context, and get a bit more speed with an MoE model. I did try Qwen Coder, but experienced the issues I described above with failed tool calls and poor code quality.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Consumerbot37427"&gt; /u/Consumerbot37427 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtg3sm/mistral_vibe_vs_claude_code_vs_openai_codex_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtg3sm/mistral_vibe_vs_claude_code_vs_openai_codex_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtg3sm/mistral_vibe_vs_claude_code_vs_openai_codex_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T00:38:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsx51z</id>
    <title>Falcon-H1-Tiny (90M) is out - specialized micro-models that actually work</title>
    <updated>2026-02-01T12:25:04+00:00</updated>
    <author>
      <name>/u/United-Manner-7</name>
      <uri>https://old.reddit.com/user/United-Manner-7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TII just dropped Falcon-H1-Tiny - a series of sub-100M models that quietly challenge the scaling dogma. We've all suspected that narrow, specialized smal models tend to hallucinate less than giant generalists. After all, a 90M parameter model has far less internal &amp;quot;room&amp;quot; to drift off-topic or invent facts outside its training scope. But this release &lt;em&gt;proves&lt;/em&gt; it with numbers - and flips the script on how we think about capability at tiny scales.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's actually new&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Anti-curriculum training&lt;/strong&gt;: Instead of pretraining on web junk then fine-tuning, they inject target-domain data (SFT, reasoning traces, tool calls) from token #1. For 90M models with ~5 GT memorization windows, this works - no overfitting even after 100+ epochs on high-quality data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid Mamba+Attention blocks&lt;/strong&gt; inherited from Falcon-H1, plus Learnable Multipliers + Muon optimizer (up to 20% relative gain over AdamW).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Specialized variants that punch above weight&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;90M tool-caller hits 94.44% relevance detection (knows &lt;em&gt;when&lt;/em&gt; to call a function) matches 270M Function Gemma globally despite weaker AST accuracy&lt;/li&gt; &lt;li&gt;600M reasoning model (R-0.6B) post-GRPO solves 75% of AIME24 problems pass@1 - competitive with 7B-class models when scaled at inference&lt;/li&gt; &lt;li&gt;90M coder with native FIM support runs autocomplete inside VS Code via Continue plugin&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why this matters for local deployment&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Models this size (~90 MB quantized Q8_0) run on any modern phone or Raspberry Pi without breaking a sweat. They're not trying to replace your 7B daily driver they're purpose-built for constrained environments where footprint and latency dominate. And if you scaled these designs to ~1B parameters (11√ó), the'd likely cover 90% of everyday local use cases: chat, tool calling, light coding, reasoning traces - all while staying under 500 MB even quantized.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Base 90M instruct model: &lt;a href="https://huggingface.co/tiiuae/Falcon-H1-Tiny-R-90M"&gt;https://huggingface.co/tiiuae/Falcon-H1-Tiny-R-90M&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Full model collection: &lt;a href="https://huggingface.co/tiiuae/models"&gt;https://huggingface.co/tiiuae/models&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Technical blogpost with experiments: &lt;a href="https://huggingface.co/spaces/tiiuae/tiny-h1-blogpost"&gt;https://huggingface.co/spaces/tiiuae/tiny-h1-blogpost&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Manner-7"&gt; /u/United-Manner-7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsx51z/falconh1tiny_90m_is_out_specialized_micromodels/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsx51z/falconh1tiny_90m_is_out_specialized_micromodels/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsx51z/falconh1tiny_90m_is_out_specialized_micromodels/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T12:25:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtj039</id>
    <title>What's your dream in 2026?</title>
    <updated>2026-02-02T02:46:40+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hope that guys from Wall Street would make price of RAM/SSD back to normal, by whatever means.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtj039/whats_your_dream_in_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtj039/whats_your_dream_in_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtj039/whats_your_dream_in_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T02:46:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt9gyf</id>
    <title>I built a pentesting platform that lets AI control 400+ hacking tools</title>
    <updated>2026-02-01T20:17:14+00:00</updated>
    <author>
      <name>/u/Justachillguypeace</name>
      <uri>https://old.reddit.com/user/Justachillguypeace</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt9gyf/i_built_a_pentesting_platform_that_lets_ai/"&gt; &lt;img alt="I built a pentesting platform that lets AI control 400+ hacking tools" src="https://external-preview.redd.it/MmhocXdobTl5eGdnMS7Ny9qzMAmuinIQRg---a-6I7vN05-3-TDw6Gj1XVF3.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e0dd22cb6593dff42a1b197f8e3eb8049aa617e" title="I built a pentesting platform that lets AI control 400+ hacking tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been working on this project for the past month as a side project (I'm a pentester).&lt;/p&gt; &lt;p&gt;The idea: give your AI agent a full pentesting environment. Claude can execute tools directly in a Docker container, chain attacks based on what it finds, and document everything automatically.&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;p&gt;- AI agent connects via MCP to an Exegol container (400+ security tools)&lt;/p&gt; &lt;p&gt;- Executes nmap, sqlmap, nuclei, ffuf, etc. directly&lt;/p&gt; &lt;p&gt;- Tracks findings in a web dashboard&lt;/p&gt; &lt;p&gt;- Maintains full context across the entire assessment&lt;/p&gt; &lt;p&gt;No more copy-pasting commands back and forth between Claude and your terminal :)&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Vasco0x4/AIDA"&gt;https://github.com/Vasco0x4/AIDA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://www.youtube.com/watch?v=yz6ac-y4g08"&gt;https://www.youtube.com/watch?v=yz6ac-y4g08&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is my first big open source project, so I'm waiting for honest reviews and feedback. Not trying to monetize it, just sharing with the community.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Justachillguypeace"&gt; /u/Justachillguypeace &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/sfk44fm9yxgg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt9gyf/i_built_a_pentesting_platform_that_lets_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qt9gyf/i_built_a_pentesting_platform_that_lets_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T20:17:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtvo4r</id>
    <title>128GB devices have a new local LLM king: Step-3.5-Flash-int4</title>
    <updated>2026-02-02T13:55:00+00:00</updated>
    <author>
      <name>/u/tarruda</name>
      <uri>https://old.reddit.com/user/tarruda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's the HF Repo: &lt;a href="http://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4"&gt;http://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4&lt;/a&gt; (this is a GGUF repo)&lt;/p&gt; &lt;p&gt;I've been running this LLM for about an hour and it has handled all coding tests I've thrown at it in chat mode. IMO this is as good if not better than GLM 4.7, Minimax 2.1 while being much more efficient. Later I will try some agentic coding to see how it performs, but I already have high hopes for it.&lt;/p&gt; &lt;p&gt;I use a 128GB M1 ultra mac studio and can run it at full context (256k). Not only it is fast, but also super efficient in RAM usage.&lt;/p&gt; &lt;p&gt;You need to build a llama.cpp fork to run it, instructions at the HF repo. Though this model is so good that I believe it will soon be supported by llama.cpp upstream.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarruda"&gt; /u/tarruda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T13:55:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtlxnz</id>
    <title>Why is RVC still the king of STS after 2 years of silence? Is there a technical plateau?</title>
    <updated>2026-02-02T05:04:32+00:00</updated>
    <author>
      <name>/u/lnkhey</name>
      <uri>https://old.reddit.com/user/lnkhey</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I have been thinking about where Speech to Speech (STS) is heading for music use. RVC has not seen a major update in ages and I find it strange that we are still stuck with it. Even with the best forks like Applio or Mangio, those annoying artifacts and other issues are still present in almost every render.&lt;/p&gt; &lt;p&gt;Is it because the research has shifted towards Text to Speech (TTS) or Zero-shot models because they are more commercially viable? Or is it a bottleneck with current vocoders that just can not handle complex singing perfectly?&lt;/p&gt; &lt;p&gt;I also wonder if the industry is prioritizing real-time performance (low latency) over actual studio quality. Are there any diffusion-based models that are actually usable for singing without having all these artifacts ??&lt;/p&gt; &lt;p&gt;It feels like we are on a plateau while every other AI field is exploding. What am I missing here? Is there a &amp;quot;RVC killer&amp;quot; in the works or are we just repurposing old tech forever?&lt;/p&gt; &lt;p&gt;Thanks for your insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lnkhey"&gt; /u/lnkhey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtlxnz/why_is_rvc_still_the_king_of_sts_after_2_years_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtlxnz/why_is_rvc_still_the_king_of_sts_after_2_years_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtlxnz/why_is_rvc_still_the_king_of_sts_after_2_years_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T05:04:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtj87p</id>
    <title>What's the most complicated project you've built with AI?</title>
    <updated>2026-02-02T02:56:39+00:00</updated>
    <author>
      <name>/u/jazir555</name>
      <uri>https://old.reddit.com/user/jazir555</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bonus points if its complex and purely vibe coded&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jazir555"&gt; /u/jazir555 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtj87p/whats_the_most_complicated_project_youve_built/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtj87p/whats_the_most_complicated_project_youve_built/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtj87p/whats_the_most_complicated_project_youve_built/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T02:56:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtuwe7</id>
    <title>Local model fully replacing subscription service</title>
    <updated>2026-02-02T13:22:41+00:00</updated>
    <author>
      <name>/u/Icy_Distribution_361</name>
      <uri>https://old.reddit.com/user/Icy_Distribution_361</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm really impressed with local models on a Macbook Pro M4 Pro with 24GB memory. For my usecase, I don't really see the need anymore for a subscription model. While I'm a pretty heavy user of ChatGPT, I don't really ask complicated questions usually. It's mostly &amp;quot;what does the research say about this&amp;quot;, &amp;quot;who is that&amp;quot;, &amp;quot;how does X work&amp;quot;, &amp;quot;what's the etymology of ...&amp;quot; and so on. I don't really do much extensive writing together with it, or much coding (a little bit sometimes). I just hadn't expected Ollama + GPT-OSS:20b to be as high quality and fast as it is. And yes, I know about all the other local models out there, but I actually like GPT-OSS... I know it gets a lot of crap.&lt;/p&gt; &lt;p&gt;Anyone else considering, or has already, cancelling subscriptions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Distribution_361"&gt; /u/Icy_Distribution_361 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtuwe7/local_model_fully_replacing_subscription_service/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtuwe7/local_model_fully_replacing_subscription_service/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtuwe7/local_model_fully_replacing_subscription_service/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T13:22:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtoukf</id>
    <title>CISA acting director reportedly uploaded sensitive documents to ChatGPT</title>
    <updated>2026-02-02T07:46:50+00:00</updated>
    <author>
      <name>/u/EchoOfOppenheimer</name>
      <uri>https://old.reddit.com/user/EchoOfOppenheimer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Acting Director of CISA, the top cybersecurity agency in the US, was just caught uploading sensitive government documents to the PUBLIC version of ChatGPT. He reportedly bypassed his own agency's security blocks to do it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EchoOfOppenheimer"&gt; /u/EchoOfOppenheimer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.scworld.com/brief/cisa-acting-director-reportedly-uploaded-sensitive-documents-to-chatgpt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtoukf/cisa_acting_director_reportedly_uploaded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtoukf/cisa_acting_director_reportedly_uploaded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T07:46:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt76qs</id>
    <title>Mistral Vibe 2.0</title>
    <updated>2026-02-01T18:56:50+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt76qs/mistral_vibe_20/"&gt; &lt;img alt="Mistral Vibe 2.0" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Mistral Vibe 2.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like I missed Mistral Vibe 2.0 being announced because I‚Äôve been busy with OpenCode.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/mistral-vibe-2-0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt76qs/mistral_vibe_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qt76qs/mistral_vibe_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T18:56:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtisy5</id>
    <title>Step 3.5 Flash 200B</title>
    <updated>2026-02-02T02:37:59+00:00</updated>
    <author>
      <name>/u/limoce</name>
      <uri>https://old.reddit.com/user/limoce</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Huggingface: &lt;a href="https://huggingface.co/stepfun-ai/Step-3.5-Flash"&gt;https://huggingface.co/stepfun-ai/Step-3.5-Flash&lt;/a&gt;&lt;br /&gt; News: &lt;a href="https://static.stepfun.com/blog/step-3.5-flash/"&gt;https://static.stepfun.com/blog/step-3.5-flash/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: 196B A11B&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/limoce"&gt; /u/limoce &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtisy5/step_35_flash_200b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtisy5/step_35_flash_200b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtisy5/step_35_flash_200b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T02:37:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtqy6f</id>
    <title>Playing Civilization VI with a Computer-Use agent</title>
    <updated>2026-02-02T09:56:52+00:00</updated>
    <author>
      <name>/u/Working_Original9624</name>
      <uri>https://old.reddit.com/user/Working_Original9624</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtqy6f/playing_civilization_vi_with_a_computeruse_agent/"&gt; &lt;img alt="Playing Civilization VI with a Computer-Use agent" src="https://external-preview.redd.it/aG01OHh4ZzUwMmhnMSU9p-WqwsrPdmT3GD6YCmDr1IKgEI58rOR3KY0kqV6w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a0af494b21ef8e01bbfc861df9a131b193dcc03" title="Playing Civilization VI with a Computer-Use agent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With recent advances in VLMs, Computer-Use‚ÄîAI directly operating a real computer‚Äîhas gained a lot of attention.&lt;br /&gt; That said, most demos still rely on clean, API-controlled environments.&lt;/p&gt; &lt;p&gt;To push beyond that, I‚Äôm using Civilization VI, a complex turn-based strategy game, as the testbed.&lt;/p&gt; &lt;p&gt;The agent doesn‚Äôt receive structured game state via MCP alone.&lt;br /&gt; Instead, it reads the screen, interprets the UI, combines that with game data to plan, and controls the game via keyboard and mouse‚Äîlike a human player.&lt;/p&gt; &lt;p&gt;Civ VI involves long-horizon, non-structured decision making across science, culture, diplomacy, and warfare.&lt;br /&gt; Making all of this work using only vision + input actions is a fairly challenging setup.&lt;/p&gt; &lt;p&gt;After one week of experiments, the agent has started to understand the game interface and perform its first meaningful actions.&lt;/p&gt; &lt;p&gt;Can a Computer-Use agent autonomously lead a civilization all the way to prosperity‚Äîand victory?&lt;br /&gt; We‚Äôll see. üëÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Working_Original9624"&gt; /u/Working_Original9624 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pxraikg502hg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtqy6f/playing_civilization_vi_with_a_computeruse_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtqy6f/playing_civilization_vi_with_a_computeruse_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T09:56:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtvp74</id>
    <title>GLM-5 Coming in February! It's confirmed.</title>
    <updated>2026-02-02T13:56:14+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvp74/glm5_coming_in_february_its_confirmed/"&gt; &lt;img alt="GLM-5 Coming in February! It's confirmed." src="https://preview.redd.it/rq0meza173hg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71bbd7ed37e31d92af89abf19ffb4ef0e1d8925a" title="GLM-5 Coming in February! It's confirmed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Twitter Link: &lt;a href="https://x.com/jietang/status/2018246490775498791?s=20"&gt;https://x.com/jietang/status/2018246490775498791?s=20&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rq0meza173hg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvp74/glm5_coming_in_february_its_confirmed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvp74/glm5_coming_in_february_its_confirmed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T13:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qttq5w</id>
    <title>devstral small is faster and better than glm 4.7 flash for local agentic coding.</title>
    <updated>2026-02-02T12:28:47+00:00</updated>
    <author>
      <name>/u/theghost3172</name>
      <uri>https://old.reddit.com/user/theghost3172</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i just realised token per second is not the only thing that matters in agentic coding. glm 4.7 flash is almlst 3x faster but it keeps thinking for way more than 3 times the total tokens it generates so yes at the end devstral small finishes the task slighter faster than glm 4.7 flash. while obiously being much much better at agentic coding.&lt;/p&gt; &lt;p&gt;token efficiency of devstral small has to be discussed more often. its incredble.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theghost3172"&gt; /u/theghost3172 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qttq5w/devstral_small_is_faster_and_better_than_glm_47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qttq5w/devstral_small_is_faster_and_better_than_glm_47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qttq5w/devstral_small_is_faster_and_better_than_glm_47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T12:28:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtjhc8</id>
    <title>Step-3.5-Flash (196b/A11b) outperforms GLM-4.7 and DeepSeek v3.2</title>
    <updated>2026-02-02T03:07:42+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtjhc8/step35flash_196ba11b_outperforms_glm47_and/"&gt; &lt;img alt="Step-3.5-Flash (196b/A11b) outperforms GLM-4.7 and DeepSeek v3.2" src="https://b.thumbs.redditmedia.com/sBia_JVk3vzY7mBVsXJhMax7j8mOpxC8QNyJrUyazbc.jpg" title="Step-3.5-Flash (196b/A11b) outperforms GLM-4.7 and DeepSeek v3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The newly released Stepfun model Step-3.5-Flash outperforms DeepSeek v3.2 on multiple coding and agentic benchmarks, despite using far fewer parameters.&lt;/p&gt; &lt;p&gt;Step-3.5-Flash: 196B total / 11B active parameters&lt;/p&gt; &lt;p&gt;DeepSeek v3.2: 671B total / 37B active parameters&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/stepfun-ai/Step-3.5-Flash"&gt;https://huggingface.co/stepfun-ai/Step-3.5-Flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qtjhc8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtjhc8/step35flash_196ba11b_outperforms_glm47_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtjhc8/step35flash_196ba11b_outperforms_glm47_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T03:07:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtqspu</id>
    <title>1 Day Left Until ACE-Step 1.5 ‚Äî Open-Source Music Gen That Runs on &lt;4GB VRAM Open suno alternative (and yes, i made this frontend)</title>
    <updated>2026-02-02T09:47:32+00:00</updated>
    <author>
      <name>/u/ExcellentTrust4433</name>
      <uri>https://old.reddit.com/user/ExcellentTrust4433</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtqspu/1_day_left_until_acestep_15_opensource_music_gen/"&gt; &lt;img alt="1 Day Left Until ACE-Step 1.5 ‚Äî Open-Source Music Gen That Runs on &amp;lt;4GB VRAM Open suno alternative (and yes, i made this frontend)" src="https://external-preview.redd.it/dXBiYXJlb295MWhnMYGTlVfp4XddQFbQ7RXlmhemkMaIRdSQh0Jy7FObZ7qD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=34c74a399f0ef7e36cb52af6dda02f6967165407" title="1 Day Left Until ACE-Step 1.5 ‚Äî Open-Source Music Gen That Runs on &amp;lt;4GB VRAM Open suno alternative (and yes, i made this frontend)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An open-source model with quality approaching Suno v4.5/v5... running locally on a potato GPU. No subscriptions. No API limits. Just you and your creativity. &lt;/p&gt; &lt;p&gt;We're so lucky to be in this era of open-source AI. A year ago this was unthinkable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExcellentTrust4433"&gt; /u/ExcellentTrust4433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2geqqfooy1hg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtqspu/1_day_left_until_acestep_15_opensource_music_gen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtqspu/1_day_left_until_acestep_15_opensource_music_gen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T09:47:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtu8x1</id>
    <title>GLM 5 Coming Soon</title>
    <updated>2026-02-02T12:54:04+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtu8x1/glm_5_coming_soon/"&gt; &lt;img alt="GLM 5 Coming Soon" src="https://a.thumbs.redditmedia.com/TZGydN7rCAg4Jr3KMn3FCkLWjDYSFSPsrvNKU23imL4.jpg" title="GLM 5 Coming Soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/3i8wkkp8w2hg1.png?width=635&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd400f6ceedc90114cc90feedd2126e2bad951dc"&gt;https://preview.redd.it/3i8wkkp8w2hg1.png?width=635&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd400f6ceedc90114cc90feedd2126e2bad951dc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/jietang/status/2018246490775498791"&gt;https://x.com/jietang/status/2018246490775498791&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtu8x1/glm_5_coming_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtu8x1/glm_5_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtu8x1/glm_5_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T12:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
