<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-26T16:07:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nr4xed</id>
    <title>I am new, can anyone tell me any Image to video model (quantized) which is compatible with 2GB vram? I know its lame but my resources are limited</title>
    <updated>2025-09-26T15:50:13+00:00</updated>
    <author>
      <name>/u/Obvious_Ad8471</name>
      <uri>https://old.reddit.com/user/Obvious_Ad8471</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very fresh to all this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Obvious_Ad8471"&gt; /u/Obvious_Ad8471 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr4xed/i_am_new_can_anyone_tell_me_any_image_to_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr4xed/i_am_new_can_anyone_tell_me_any_image_to_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr4xed/i_am_new_can_anyone_tell_me_any_image_to_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T15:50:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqyi1x</id>
    <title>embedding with llama.cpp server</title>
    <updated>2025-09-26T11:10:14+00:00</updated>
    <author>
      <name>/u/DobobR</name>
      <uri>https://old.reddit.com/user/DobobR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a working app that uses ollama and snowflake-arctic-embed2 for embedding and rag with chromadb.&lt;/p&gt; &lt;p&gt;I want to switch to llama.cpp but i am not able to setup the embedding server correctly. The chromadb query function works well with ollama but not at all with llama.cpp. I think it has something todo with pooling or normalization. i tried a lot but i was not able to get it running.&lt;/p&gt; &lt;p&gt;i would appreciate anything that points me in the right direction!&lt;/p&gt; &lt;p&gt;thanks a lot! &lt;/p&gt; &lt;p&gt;my last try was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--model /models/snowflake-arctic-embed-l-v2.0-q5_k_m.gguf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--embeddings&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--ubatch-size 2048&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--batch-size 2028&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--ctx-size 8192&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--pooling mean&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--rope-scaling yarn&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--rope-freq-scale 0.75&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ngl 99&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--parallel 4&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DobobR"&gt; /u/DobobR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqyi1x/embedding_with_llamacpp_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqyi1x/embedding_with_llamacpp_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqyi1x/embedding_with_llamacpp_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T11:10:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nquiff</id>
    <title>Local Qwen-Code rig recommendations (~€15–20k)?</title>
    <updated>2025-09-26T06:57:48+00:00</updated>
    <author>
      <name>/u/logTom</name>
      <uri>https://old.reddit.com/user/logTom</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re in the EU, need GDPR compliance, and want to build a local AI rig mainly for coding (Qwen-Code). Budget is ~€15–20k. Timeline: decision within this year.&lt;/p&gt; &lt;p&gt;Any hardware/vendor recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/logTom"&gt; /u/logTom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nquiff/local_qwencode_rig_recommendations_1520k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nquiff/local_qwencode_rig_recommendations_1520k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nquiff/local_qwencode_rig_recommendations_1520k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T06:57:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqvbds</id>
    <title>Hands-on with Qwen3 Omni and read some community evaluations.</title>
    <updated>2025-09-26T07:50:19+00:00</updated>
    <author>
      <name>/u/Hairy-Librarian3796</name>
      <uri>https://old.reddit.com/user/Hairy-Librarian3796</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 Omni's positioning is that of a lightweight, full-modality model. It's fast, has decent image recognition accuracy, and is quite usable for everyday OCR and general visual scenarios. It works well as a multimodal recognition model that balances capability with resource consumption.However, there's a significant gap between Omni and Qwen3 Max in both understanding precision and reasoning ability. Max can decipher text that's barely legible to the human eye and comprehend the relationships between different text elements in an image. Omni, on the other hand, struggles with very small text and has a more superficial understanding of the image; it tends to describe what it sees literally without grasping the deeper context or connections.I also tested it on some math problems, and the results were inconsistent. It sometimes hallucinates answers. So, it's not yet reliable for tasks requiring rigorous &lt;a href="http://reasoning.In"&gt;reasoning.In&lt;/a&gt; terms of overall capability, Qwen3 Max is indeed more robust intellectually (though its response style could use improvement: the interface is cluttered with emojis and overly complex Markdown, and the writing style feels a bit unnatural and lacks nuance).That said, I believe the real value of this Qwen3 release isn't just about pushing benchmark scores up a few points. Instead, it lies in offering a comprehensive, developer-friendly, full-modality solution.For reference, here are some official resources:&lt;br /&gt; &lt;a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/assets/Qwen3_Omni.pdf"&gt;https://github.com/QwenLM/Qwen3-Omni/blob/main/assets/Qwen3_Omni.pdf&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb"&gt;https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hairy-Librarian3796"&gt; /u/Hairy-Librarian3796 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqvbds/handson_with_qwen3_omni_and_read_some_community/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqvbds/handson_with_qwen3_omni_and_read_some_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqvbds/handson_with_qwen3_omni_and_read_some_community/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T07:50:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr2w1u</id>
    <title>llama-server Is there a way to offload just context to another gpu?</title>
    <updated>2025-09-26T14:30:58+00:00</updated>
    <author>
      <name>/u/kylesk42</name>
      <uri>https://old.reddit.com/user/kylesk42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been messing with the params and i cant find a good way to do it. I have 3x 3090s on here. &lt;/p&gt; &lt;p&gt;GPU 2 is used for stable diffusion. &lt;/p&gt; &lt;p&gt;GPU 1 is running another llm uses nkvo so that the memory usage is constant. 12 gigs of vram free.&lt;/p&gt; &lt;p&gt;The model i want to run on GPU 0 uses pretty much all of the vram. I know i can split tensors, but it is faster when i keep the whole model on 1 gpu. I can do nkvo, but that goes to system memory. Def dont want that. A command similar to nkvo, but send the ram to a gpu is what i am hoping to find.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kylesk42"&gt; /u/kylesk42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr2w1u/llamaserver_is_there_a_way_to_offload_just/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr2w1u/llamaserver_is_there_a_way_to_offload_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr2w1u/llamaserver_is_there_a_way_to_offload_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T14:30:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqo0oo</id>
    <title>The current state of LLM benchmarks is so polluted</title>
    <updated>2025-09-26T01:03:46+00:00</updated>
    <author>
      <name>/u/Odd_Tumbleweed574</name>
      <uri>https://old.reddit.com/user/Odd_Tumbleweed574</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says. &lt;/p&gt; &lt;p&gt;Since the beginning of the LLM craze, every lab has been publishing and cherry picking their results, and there's a lack of transparency from the AI labs. This only affects the consumers.&lt;/p&gt; &lt;p&gt;There are multiple issues that exist today and haven't been solved:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Labs are reporting only the benchmarks where their models look good, they cherry pick results.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Some labs are training on the very same benchmarks they evaluate, maybe not on purpose, but contamination is there.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Most published benchmarks are not actually useful at all, they are usually weird academic cases where the models fail, instead of real-world use patterns of these models.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Every lab uses their own testing methodology, their own parameters and prompts, and they seem to tune things until they appear better than the previous release.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Everyone is implementing their own benchmarks in their own way and never release the code to reproduce.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The APIs fluctuate in quality and some providers are selling quantized versions instead of the original model, thus, we see regressions. Nobody is tracking this.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Is there anyone working on these issues? I'd love to talk if so. We just started working on independent benchmarking and plan to build a standard so anyone can build and publish their own benchmark easily, for any use case. All open source, open data.&lt;/p&gt; &lt;p&gt;Imagine a place that test new releases and report API regressions, in favor of the consumers. Not with academic contaminated benchmarks but with actual real world performance benchmarks.&lt;/p&gt; &lt;p&gt;There's already great websites out there doing an effort, but what I envision is a place where you can find hundreds of community built benchmarks of all kinds (legal, healthcare, roleplay, instruction following, asr, etc). And a way to monitor the real quality of the models out there.&lt;/p&gt; &lt;p&gt;Is this something anyone else shares? or is it just me becoming crazy due to no good existing solution?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Tumbleweed574"&gt; /u/Odd_Tumbleweed574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqo0oo/the_current_state_of_llm_benchmarks_is_so_polluted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqo0oo/the_current_state_of_llm_benchmarks_is_so_polluted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqo0oo/the_current_state_of_llm_benchmarks_is_so_polluted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T01:03:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr3101</id>
    <title>Why isn't there a thinking qwen3-max?</title>
    <updated>2025-09-26T14:36:28+00:00</updated>
    <author>
      <name>/u/elephant_ua</name>
      <uri>https://old.reddit.com/user/elephant_ua</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like the model, but when the task requires even a modicum of thinking and iterating/reflecting, it fails spectacularly. &lt;/p&gt; &lt;p&gt;Is this the issue limited to web-interface of qwen, or their api can't think for this version as well? Why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elephant_ua"&gt; /u/elephant_ua &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr3101/why_isnt_there_a_thinking_qwen3max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr3101/why_isnt_there_a_thinking_qwen3max/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr3101/why_isnt_there_a_thinking_qwen3max/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T14:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqzdit</id>
    <title>OrKa quickstart: run a traceable multi agent workflow in under 2 minutes</title>
    <updated>2025-09-26T11:57:03+00:00</updated>
    <author>
      <name>/u/marcosomma-OrKA</name>
      <uri>https://old.reddit.com/user/marcosomma-OrKA</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqzdit/orka_quickstart_run_a_traceable_multi_agent/"&gt; &lt;img alt="OrKa quickstart: run a traceable multi agent workflow in under 2 minutes" src="https://external-preview.redd.it/Y2k4MWdndHZ6aHJmMVpKx18zu2Al60haJHCIipoecy1_uH38KnrawZp01IuI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d13d23c31281b6f10ff2ca693b11f87b02adbc14" title="OrKa quickstart: run a traceable multi agent workflow in under 2 minutes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recorded a fast walkthrough showing how to spin up OrKA-reasoning and execute a workflow with full traceability.&lt;br /&gt; (No OpenAI key needed if you use local models.)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What OrKa is&lt;/strong&gt;&lt;br /&gt; A YAML defined cognition graph.&lt;br /&gt; You wire agents, routers, memory and services, then watch the full execution trace.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to run it like in the video&lt;/strong&gt;&lt;br /&gt; Pip&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install -U orka-reasoning orka-start orka memory watch orka run path/to/workflow.yaml &amp;quot;&amp;lt;your input as string&amp;gt;&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;What you will see in the result&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Live trace with timestamps for every step&lt;/li&gt; &lt;li&gt;Forks that execute agents in parallel and a join that merges results&lt;/li&gt; &lt;li&gt;Per agent metrics: latency, tokens, model and provider&lt;/li&gt; &lt;li&gt;Memory reads and writes visible in the timeline&lt;/li&gt; &lt;li&gt;Agreement score that shows the level of consensus&lt;/li&gt; &lt;li&gt;Final synthesized answer plus each agent’s raw output, grouped and inspectable&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why this matters&lt;/strong&gt;&lt;br /&gt; You can replay the entire run, audit decisions, and compare branches. It turns multi agent reasoning into something you can debug, not just hope for.&lt;/p&gt; &lt;p&gt;If you try it, tell me which model stack you used and how long your first run took. I will share optimized starter graphs in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marcosomma-OrKA"&gt; /u/marcosomma-OrKA &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wi8c8ftvzhrf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqzdit/orka_quickstart_run_a_traceable_multi_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqzdit/orka_quickstart_run_a_traceable_multi_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T11:57:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq182d</id>
    <title>Alibaba just unveiled their Qwen roadmap. The ambition is staggering!</title>
    <updated>2025-09-25T08:24:45+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq182d/alibaba_just_unveiled_their_qwen_roadmap_the/"&gt; &lt;img alt="Alibaba just unveiled their Qwen roadmap. The ambition is staggering!" src="https://preview.redd.it/5tm4p90rt9rf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6724dd3297826b1a060f45ea0c5e1fd9e366f5ab" title="Alibaba just unveiled their Qwen roadmap. The ambition is staggering!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two big bets: unified multi-modal models and extreme scaling across every dimension.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Context length: 1M → 100M tokens&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Parameters: trillion → ten trillion scale&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Test-time compute: 64k → 1M scaling&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Data: 10 trillion → 100 trillion tokens&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;They're also pushing synthetic data generation &amp;quot;without scale limits&amp;quot; and expanding agent capabilities across complexity, interaction, and learning modes.&lt;/p&gt; &lt;p&gt;The &amp;quot;scaling is all you need&amp;quot; mantra is becoming China's AI gospel.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5tm4p90rt9rf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq182d/alibaba_just_unveiled_their_qwen_roadmap_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nq182d/alibaba_just_unveiled_their_qwen_roadmap_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T08:24:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr3kl3</id>
    <title>Isn't there a TTS model just slightly better than Kokoro?</title>
    <updated>2025-09-26T14:58:01+00:00</updated>
    <author>
      <name>/u/TarkanV</name>
      <uri>https://old.reddit.com/user/TarkanV</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like its consistency and speed, but I mean, I might sound nitpicky but, it seems like it can fail easily on some relatively common words or names of non-English origin like &amp;quot;Los Angeles&amp;quot;, &amp;quot;Huawei&amp;quot;.&lt;br /&gt; I really wish there was an in-between model or even something that had just a little bit more more parameters than Kokoro.&lt;br /&gt; But to be fair, even ChatGPT Voice Mode seems to fail with names like Siobhan even though Kokoro gets it right...&lt;br /&gt; Otherwise, I'm fine if it's English only and preferably something smaller and faster than Zonos. My main use would be making audiobooks. My build is basically a laptop with a 3060 6GB and and 16gb of ram.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TarkanV"&gt; /u/TarkanV &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr3kl3/isnt_there_a_tts_model_just_slightly_better_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr3kl3/isnt_there_a_tts_model_just_slightly_better_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr3kl3/isnt_there_a_tts_model_just_slightly_better_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T14:58:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqhlyw</id>
    <title>I'm testing the progress on GitHub. Qwen Next gguf. Fingers crossed.</title>
    <updated>2025-09-25T20:25:45+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqhlyw/im_testing_the_progress_on_github_qwen_next_gguf/"&gt; &lt;img alt="I'm testing the progress on GitHub. Qwen Next gguf. Fingers crossed." src="https://external-preview.redd.it/EKetf3gBiUdNtrEdMnvehl50wpsTXtAb_rcC73bjkFY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8b0037a07998e56cdb65b5e64502a4aceb6ef48" title="I'm testing the progress on GitHub. Qwen Next gguf. Fingers crossed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4zwiyxqyddrf1.png?width=1163&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ab1462bb56937da0853af07c994da164ffdeb090"&gt;qwen next&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Can't wait to test the final build. &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095"&gt;https://github.com/ggml-org/llama.cpp/pull/16095&lt;/a&gt; . Thx for your hard work &lt;a href="https://github.com/pwilkin"&gt;&lt;strong&gt;pwilkin&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqhlyw/im_testing_the_progress_on_github_qwen_next_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqhlyw/im_testing_the_progress_on_github_qwen_next_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqhlyw/im_testing_the_progress_on_github_qwen_next_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T20:25:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr0lxs</id>
    <title>Anyone else run into LiteLLM breaking down under load?</title>
    <updated>2025-09-26T12:55:30+00:00</updated>
    <author>
      <name>/u/Fabulous_Ad993</name>
      <uri>https://old.reddit.com/user/Fabulous_Ad993</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been load testing different LLM gateways for a project where throughput matters. Setup was 1K → 5K RPS with mixed request sizes, tracked using Prometheus/Grafana.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.litellm.ai/"&gt;LiteLLM&lt;/a&gt;: stable up to ~300K RPS, but after that I started seeing latency spikes, retries piling up, and 5xx errors.&lt;/li&gt; &lt;li&gt;&lt;a href="https://portkey.ai/"&gt;Portkey&lt;/a&gt;: handled concurrency a bit better, though I noticed overhead rising at higher loads.&lt;/li&gt; &lt;li&gt;&lt;a href="https://getmax.im/bifr0st"&gt;Bifrost&lt;/a&gt;: didn’t break in the same way under the same tests. Overhead stayed low in my runs, and it comes with decent metrics/monitoring.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has anyone here benchmarked these (TGI, vLLM gateways, custom reverse proxies, etc.) at higher RPS? Also would love to know if anyone has tried &lt;strong&gt;Bifrost&lt;/strong&gt; (found it mentioned on some threads) since it’s relatively new compared to the others; would love to hear your insights.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Ad993"&gt; /u/Fabulous_Ad993 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr0lxs/anyone_else_run_into_litellm_breaking_down_under/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr0lxs/anyone_else_run_into_litellm_breaking_down_under/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr0lxs/anyone_else_run_into_litellm_breaking_down_under/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T12:55:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqaiaz</id>
    <title>Tencent is teasing the world’s most powerful open-source text-to-image model, Hunyuan Image 3.0 Drops Sept 28</title>
    <updated>2025-09-25T15:54:29+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqaiaz/tencent_is_teasing_the_worlds_most_powerful/"&gt; &lt;img alt="Tencent is teasing the world’s most powerful open-source text-to-image model, Hunyuan Image 3.0 Drops Sept 28" src="https://preview.redd.it/t8w84ihz1crf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35af183180dd8d2fd35f6be46e55d9a4dc75e26d" title="Tencent is teasing the world’s most powerful open-source text-to-image model, Hunyuan Image 3.0 Drops Sept 28" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t8w84ihz1crf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqaiaz/tencent_is_teasing_the_worlds_most_powerful/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqaiaz/tencent_is_teasing_the_worlds_most_powerful/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T15:54:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr020h</id>
    <title>InfiniteTalk — open-source sparse-frame video dubbing (lip + head/body sync)</title>
    <updated>2025-09-26T12:30:00+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Found a fun open-source project: &lt;strong&gt;InfiniteTalk&lt;/strong&gt;. It does “sparse-frame” video dubbing—so the &lt;strong&gt;lips, head, posture, and expressions&lt;/strong&gt; all track the audio, not just the mouth. It’s built for &lt;strong&gt;infinite-length&lt;/strong&gt; runs and claims fewer hand/body glitches with tighter lip sync than MultiTalk. Also works as &lt;strong&gt;image + audio → talking video&lt;/strong&gt;.&lt;br /&gt; Repo: &lt;a href="https://github.com/MeiGen-AI/InfiniteTalk"&gt;https://github.com/MeiGen-AI/InfiniteTalk&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr020h/infinitetalk_opensource_sparseframe_video_dubbing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr020h/infinitetalk_opensource_sparseframe_video_dubbing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr020h/infinitetalk_opensource_sparseframe_video_dubbing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T12:30:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr06en</id>
    <title>I built llamactl - Unified management and routing for llama.cpp, MLX and vLLM models with web dashboard.</title>
    <updated>2025-09-26T12:35:37+00:00</updated>
    <author>
      <name>/u/RealLordMathis</name>
      <uri>https://old.reddit.com/user/RealLordMathis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got tired of SSH-ing into servers to manually start/stop different model instances, so I built a control layer that sits on top of llama.cpp, MLX, and vLLM. Great for running multiple models at once or switching models on demand. &lt;/p&gt; &lt;p&gt;I first posted about this almost two months ago and have added a bunch of useful features since. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Main features:&lt;/strong&gt;&lt;br /&gt; - &lt;strong&gt;Multiple backend support&lt;/strong&gt;: Native integration with llama.cpp, MLX, and vLLM&lt;br /&gt; - &lt;strong&gt;On-demand instances&lt;/strong&gt;: Automatically start model instances when API requests come in&lt;br /&gt; - &lt;strong&gt;OpenAI-compatible API&lt;/strong&gt;: Drop-in replacement - route by using instance name as model name&lt;br /&gt; - &lt;strong&gt;API key authentication&lt;/strong&gt;: Separate keys for management operations vs inference API access&lt;br /&gt; - &lt;strong&gt;Web dashboard&lt;/strong&gt;: Modern UI for managing instances without CLI&lt;br /&gt; - &lt;strong&gt;Docker support&lt;/strong&gt;: Run backends in isolated containers&lt;br /&gt; - &lt;strong&gt;Smart resource management&lt;/strong&gt;: Configurable instance limits, idle timeout, and LRU eviction &lt;/p&gt; &lt;p&gt;The API lets you route requests to specific model instances by using the instance name as the model name in standard OpenAI requests, so existing tools work without modification. Instance state persists across server restarts, and failed instances get automatically restarted. &lt;/p&gt; &lt;p&gt;Documentation and installation guide: &lt;a href="https://llamactl.org/stable/"&gt;https://llamactl.org/stable/&lt;/a&gt; GitHub: &lt;a href="https://github.com/lordmathis/llamactl"&gt;https://github.com/lordmathis/llamactl&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MIT licensed. Feedback and contributions welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RealLordMathis"&gt; /u/RealLordMathis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr06en/i_built_llamactl_unified_management_and_routing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr06en/i_built_llamactl_unified_management_and_routing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr06en/i_built_llamactl_unified_management_and_routing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T12:35:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqunp9</id>
    <title>Can a 64GB Mac run Qwen3-Next-80B?</title>
    <updated>2025-09-26T07:06:48+00:00</updated>
    <author>
      <name>/u/xieyutong</name>
      <uri>https://old.reddit.com/user/xieyutong</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen comments suggesting that it's tight even on a 48GB Mac, but I'm hoping 64GB might be enough with proper quantization.I've also gathered some important caveats from the community that I'd like to confirm:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Quantization Pitfalls: Many community-shared quantized versions (like the FP8 ones) seem to have issues. A common problem mentioned is that the tokenizer_config.json might be missing the chat_template, which breaks function calling. The suggested fix is to replace it with the original tokenizer_config from the official model repo.&lt;/li&gt; &lt;li&gt;SGLang vs. Memory: Could frameworks like SGLang offer significant memory savings for this model compared to standard vLLM or llama.cpp? However, I saw reports that SGLang might have compatibility issues, particularly with some FP8 quantized versions, causing errors.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My Goal: I'm planning to compareQwen3-Next-80B (with Claude Code for coding tasks) against GPT-OSS-120B (with Codex) to see if the Qwen combo can be a viable local alternative.Any insights, especially from those who have tried running Qwen3-Next-80B on similar hardware, would be greatly appreciated! Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xieyutong"&gt; /u/xieyutong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqunp9/can_a_64gb_mac_run_qwen3next80b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqunp9/can_a_64gb_mac_run_qwen3next80b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqunp9/can_a_64gb_mac_run_qwen3next80b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T07:06:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqb3p3</id>
    <title>What? Running Qwen-32B on a 32GB GPU (5090).</title>
    <updated>2025-09-25T16:16:51+00:00</updated>
    <author>
      <name>/u/curiousily_</name>
      <uri>https://old.reddit.com/user/curiousily_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqb3p3/what_running_qwen32b_on_a_32gb_gpu_5090/"&gt; &lt;img alt="What? Running Qwen-32B on a 32GB GPU (5090)." src="https://external-preview.redd.it/eGQxejJvNXo1Y3JmMc7C-li4AFXa_Q-5qATmlwGRne0zNSJFPFjYVcktZ0y0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b1e06d42c9067d85e9ec551328755bfdad37895" title="What? Running Qwen-32B on a 32GB GPU (5090)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/curiousily_"&gt; /u/curiousily_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/01adz6it5crf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqb3p3/what_running_qwen32b_on_a_32gb_gpu_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqb3p3/what_running_qwen32b_on_a_32gb_gpu_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T16:16:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqr5lp</id>
    <title>Kwaipilot/KAT-Dev</title>
    <updated>2025-09-26T03:41:01+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqr5lp/kwaipilotkatdev/"&gt; &lt;img alt="Kwaipilot/KAT-Dev" src="https://external-preview.redd.it/RFhQxPOdxc2Em-bjhotgIyTCAALVqXONnbv5f8ro8uY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24d680deeef40cd14e1c0a13e00f25c88680f997" title="Kwaipilot/KAT-Dev" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;KAT-Dev-32B&lt;/strong&gt; is an open-source 32B-parameter model for software engineering tasks.&lt;/p&gt; &lt;p&gt;On SWE-Bench Verified, &lt;strong&gt;KAT-Dev-32B&lt;/strong&gt; achieves comparable performance with &lt;strong&gt;62.4%&lt;/strong&gt; resolved and ranks &lt;strong&gt;5th&lt;/strong&gt; among all open-source models with different scales.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-Dev"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqr5lp/kwaipilotkatdev/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqr5lp/kwaipilotkatdev/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T03:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr1zen</id>
    <title>€5,000 AI server for LLM</title>
    <updated>2025-09-26T13:54:31+00:00</updated>
    <author>
      <name>/u/Slakish</name>
      <uri>https://old.reddit.com/user/Slakish</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;We are looking for a solution to run LLMs for our developers. The budget is currently €5000. The setup should be as fast as possible, but also be able to process parallel requests. I was thinking, for example, of a dual RTX 3090TI system with the option of expansion (AMD EPYC platform). I have done a lot of research, but it is difficult to find exact builds. What would be your idea?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slakish"&gt; /u/Slakish &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr1zen/5000_ai_server_for_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr1zen/5000_ai_server_for_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr1zen/5000_ai_server_for_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T13:54:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr4v7e</id>
    <title>Gpt-oss Reinforcement Learning - Fastest inference now in Unsloth! (&lt;15GB VRAM)</title>
    <updated>2025-09-26T15:47:52+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr4v7e/gptoss_reinforcement_learning_fastest_inference/"&gt; &lt;img alt="Gpt-oss Reinforcement Learning - Fastest inference now in Unsloth! (&amp;lt;15GB VRAM)" src="https://preview.redd.it/pq6ej7up5jrf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=121b14e94d54780f5a2a7ae8625d9bd2f60d60f6" title="Gpt-oss Reinforcement Learning - Fastest inference now in Unsloth! (&amp;lt;15GB VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys we've got lots of updates for Reinforcement Learning (RL)! We’re excited to introduce gpt-oss, Vision, and even better RL in Unsloth. Our new gpt-oss RL inference also achieves the fastest token/s vs. any other implementation. Our GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Inference is crucial in RL training. Since gpt-oss RL isn’t vLLM compatible, we rewrote Transformers inference for 3× faster speeds (~21 tok/s). For BF16, Unsloth also delivers the fastest inference (~30 tok/s), especially relative to VRAM use vs. any other implementation.&lt;/li&gt; &lt;li&gt;We made a free &amp;amp; completely new custom notebook showing how RL can automatically create faster matrix multiplication kernels: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B"&gt;gpt-oss-20b GSPO Colab&lt;/a&gt;-GRPO.ipynb). We also show you how to counteract reward-hacking which is one of RL's biggest challenges.&lt;/li&gt; &lt;li&gt;Unsloth also uses the least VRAM (50% less) and supports the most context length (8x more). gpt-oss-20b RL fits in 15GB VRAM.&lt;/li&gt; &lt;li&gt;As usual, there is no accuracy degradation.&lt;/li&gt; &lt;li&gt;We released &lt;a href="https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl"&gt;Vision RL&lt;/a&gt;, allowing you to train Gemma 3, Qwen2.5-VL with GRPO free in our Colab notebooks.&lt;/li&gt; &lt;li&gt;We also previously introduced more &lt;a href="https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/memory-efficient-rl"&gt;memory efficient RL&lt;/a&gt; with Standby and extra kernels and algorithms. Unsloth RL now uses 90% less VRAM, and enables 16× longer context lengths than any setup.&lt;/li&gt; &lt;li&gt; ⚠️ Reminder to NOT use Flash Attention 3 for gpt-oss as it'll make your training loss wrong.&lt;/li&gt; &lt;li&gt;We released &lt;a href="https://docs.unsloth.ai/models/deepseek-v3.1-how-to-run-locally"&gt;DeepSeek-V3.1-Terminus&lt;/a&gt; Dynamic GGUFs. We showcased how 3-bit V3.1 scores 75.6% on Aider Polyglot, beating Claude-4-Opus (thinking).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For our new gpt-oss RL release, would recommend you guys to read our blog/guide which details our entire findings and bugs etc.: &lt;a href="https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning"&gt;https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks guys for reading and hope you all have a lovely Friday and weekend! 🦥&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pq6ej7up5jrf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr4v7e/gptoss_reinforcement_learning_fastest_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr4v7e/gptoss_reinforcement_learning_fastest_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T15:47:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqkx7o</id>
    <title>Apparently all third party providers downgrade, none of them provide a max quality model</title>
    <updated>2025-09-25T22:41:03+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqkx7o/apparently_all_third_party_providers_downgrade/"&gt; &lt;img alt="Apparently all third party providers downgrade, none of them provide a max quality model" src="https://preview.redd.it/k5on2q9i2erf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44c1c2fb9cea2d9deb0e87434f884c9dc83258dd" title="Apparently all third party providers downgrade, none of them provide a max quality model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k5on2q9i2erf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqkx7o/apparently_all_third_party_providers_downgrade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqkx7o/apparently_all_third_party_providers_downgrade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T22:41:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqkayx</id>
    <title>I trained an LLM from scratch AMA!</title>
    <updated>2025-09-25T22:14:44+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been a few months and I have posted a few times but I am finished!&lt;/p&gt; &lt;p&gt;I used Claude to write my training scripts, and I trained a 960M model on public domain data. It was not fast or easy, but it only cost $500 ( I received free credits from Amazon). It took 3 attempts to get it right. Happy to go into detail&lt;/p&gt; &lt;p&gt;It's a LLama 3 architecture with a 3:1 GQA, flash attention 2, and sink tokens. I have not began post-training yet, so it is NOT VERY USABLE!!!&lt;/p&gt; &lt;p&gt;I am hoping that post turns it into something useful, I have used 1B base models and they all kind of suck.&lt;/p&gt; &lt;p&gt;Post training will be TRL with DPO and the ultrafeedbck dataset. The mdoel is released under the CC0 license, do as you will with it.&lt;/p&gt; &lt;p&gt;Project website: &lt;a href="https://www.libremodel.xyz/"&gt;The LibreModel Project&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face : &lt;a href="https://huggingface.co/jerrimu/libremodel"&gt;jerrimu/libremodel · Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github ( GGUF here): &lt;a href="https://github.com/openconstruct/libremodel/releases"&gt;Releases · openconstruct/libremodel&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would like to train more open source models, and am seeking donations for hardware: If you would like to support this cause you may donate here : &lt;a href="https://github.com/sponsors/openconstruct"&gt;Sponsor @openconstruct on GitHub Sponsors&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqkayx/i_trained_an_llm_from_scratch_ama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqkayx/i_trained_an_llm_from_scratch_ama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqkayx/i_trained_an_llm_from_scratch_ama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T22:14:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr0jnz</id>
    <title>ROCM vs Vulkan on IGPU</title>
    <updated>2025-09-26T12:52:35+00:00</updated>
    <author>
      <name>/u/Eden1506</name>
      <uri>https://old.reddit.com/user/Eden1506</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr0jnz/rocm_vs_vulkan_on_igpu/"&gt; &lt;img alt="ROCM vs Vulkan on IGPU" src="https://b.thumbs.redditmedia.com/oezoSTnfC24x09rIDD1CuCmBAKqbu-BhvixwCYutkyo.jpg" title="ROCM vs Vulkan on IGPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While around the same for text generation vulkan is ahead for prompt processing by a fair margin on the new igpus from AMD now.&lt;/p&gt; &lt;p&gt;Curious considering that it was the other way around before.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eden1506"&gt; /u/Eden1506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nr0jnz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr0jnz/rocm_vs_vulkan_on_igpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr0jnz/rocm_vs_vulkan_on_igpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T12:52:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr3n2r</id>
    <title>How am I supposed to know which third party provider can be trusted not to completely lobotomize a model?</title>
    <updated>2025-09-26T15:00:41+00:00</updated>
    <author>
      <name>/u/Striking_Wedding_461</name>
      <uri>https://old.reddit.com/user/Striking_Wedding_461</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr3n2r/how_am_i_supposed_to_know_which_third_party/"&gt; &lt;img alt="How am I supposed to know which third party provider can be trusted not to completely lobotomize a model?" src="https://preview.redd.it/kabtcb5twirf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2cfbae1d53a3abc93a95be9789c678d6280c6d58" title="How am I supposed to know which third party provider can be trusted not to completely lobotomize a model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this is mostly open-weights and open-source discussion and all that jazz but let's be real, unless your name is Achmed Al-Jibani from Qatar or you pi*ss gold you're not getting the SOTA performance with open-weight models like Kimi K2 or DeepSeek because you have to quantize it, your options as an average-wage pleb are either:&lt;/p&gt; &lt;p&gt;a) third party providers&lt;br /&gt; b) running it yourself but quantized to hell&lt;br /&gt; c) spinning up a pod and using a third party providers GPU (expensive) to run your model&lt;/p&gt; &lt;p&gt;I opted for a) most of the time and a recent evaluation done on the accuracy of the Kimi K2 0905 models provided by third party providers has me doubting this decision.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking_Wedding_461"&gt; /u/Striking_Wedding_461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kabtcb5twirf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr3n2r/how_am_i_supposed_to_know_which_third_party/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr3n2r/how_am_i_supposed_to_know_which_third_party/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T15:00:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqwcsf</id>
    <title>A list of models released or udpated last week on this sub, in case you missed any - (26th Sep)</title>
    <updated>2025-09-26T08:59:24+00:00</updated>
    <author>
      <name>/u/aifeed-fyi</name>
      <uri>https://old.reddit.com/user/aifeed-fyi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks&lt;/p&gt; &lt;p&gt;So many models for this week specially from the &lt;em&gt;Qwen&lt;/em&gt; team who have been super active lately. Please double check my list and update in the comments in case I missed anything worth mentioned this week.&lt;/p&gt; &lt;p&gt;Enjoy :)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Reddit Link&lt;/th&gt; &lt;th align="left"&gt;HF/GH Link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Max&lt;/td&gt; &lt;td align="left"&gt;LLM (1TB)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nor65d/qwen_3_max_released/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://qwen.ai/blog?id=241398b9cd6353de490b0f82806c7848c5d2777d&amp;amp;from=research.latest-advancements-list"&gt;Qwen blog&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Code World Model (CWM) 32B&lt;/td&gt; &lt;td align="left"&gt;Code LLM 32B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1npp8xi/new_model_from_meta_fair_code_world_model_cwm_32b/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/facebook/cwm"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen-Image-Edit-2509&lt;/td&gt; &lt;td align="left"&gt;Image edit&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nnt539/qwenimageedit2509_has_been_released/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2509"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Omni 30B (A3B variants)&lt;/td&gt; &lt;td align="left"&gt;Omni-modal 30B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nnt1bw/3_qwen3omni_models_have_been_released/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Captioner"&gt;Captioner&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Thinking"&gt;Thinking&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-V3.1-Terminus&lt;/td&gt; &lt;td align="left"&gt;Update 685B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://i.redd.it/729mf2l1xpqf1.jpeg"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qianfan-VL (70B/8B/3B)&lt;/td&gt; &lt;td align="left"&gt;Vision LLMs&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nncyvv/baidu_releases_qianfanvl_70b8b3b/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/baidu/Qianfan-VL-70B"&gt;HF 70B&lt;/a&gt;, &lt;a href="https://huggingface.co/baidu/Qianfan-VL-8B"&gt;HF 8B&lt;/a&gt;, &lt;a href="https://huggingface.co/baidu/Qianfan-VL-3B"&gt;HF 3B&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hunyuan Image 3.0&lt;/td&gt; &lt;td align="left"&gt;T2I model (TB released)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nqaiaz/tencent_is_teasing_the_worlds_most_powerful"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;–&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Stockmark-2-100B-Instruct&lt;/td&gt; &lt;td align="left"&gt;Japanese LLM 100B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nq4xs9/stockmark_2_100b_instruct/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;–&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-VL-235B A22B (Thinking/Instruct)&lt;/td&gt; &lt;td align="left"&gt;Vision LLM 235B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1not4up/qwen3vl235ba22bthinking_and/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking"&gt;Thinking&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct"&gt;Instruct&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LongCat-Flash-Thinking&lt;/td&gt; &lt;td align="left"&gt;Reasoning MoE 18–31B active&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nmzio1/longcatflashthinking"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Thinking"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-4B Function Calling&lt;/td&gt; &lt;td align="left"&gt;LLM 4B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nmkswn/just_dropped_qwen34b_function_calling_on_just_6gb/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Manojb/Qwen3-4B-FunctionCalling"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Isaac 0.1&lt;/td&gt; &lt;td align="left"&gt;Perception LLM 2B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1nmiqjh"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/PerceptronAI/Isaac-0.1"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Magistral 1.2&lt;/td&gt; &lt;td align="left"&gt;Multi-Modal&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nmii5y/magistral_12_is_incredible_wife_prefers_it_over/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/unsloth/Magistral-Small-2509-GGUF"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ring-flash-2.0&lt;/td&gt; &lt;td align="left"&gt;Thinking MoE&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nl97i5/inclusionairingflash20/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-flash-2.0"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kokoro-82M-FP16-OpenVINO&lt;/td&gt; &lt;td align="left"&gt;TTS 82M&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nn45cx/kokoro82mfp16openvino/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Echo9Zulu/Kokoro-82M-FP16-OpenVINO"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Wan2.2-Animate-14B&lt;/td&gt; &lt;td align="left"&gt;Video animate 14B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nmnmqh/wan_22_animate_opensourced_model_for_character/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Wan-AI/Wan2.2-Animate-14B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MiniModel-200M-Base&lt;/td&gt; &lt;td align="left"&gt;Tiny LLM 200M&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://i.redd.it/clbzeq0i82rf1.png"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/xTimeCrystal/MiniModel-200M-Base"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Other notable mentions&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;K2 Vendor Verifier&lt;/strong&gt; – Open-source tool-call validator for LLM providers (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nq6hdq/kimi_infra_team_releases_k2_vendor_verifier_an/"&gt;Reddit&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;quelmap + Lightning-4b&lt;/strong&gt; – Local data analysis assistant + LLM (&lt;a href="https://quelmap.com"&gt;quelmap.com&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;llama.ui&lt;/strong&gt; – Updated privacy-focused LLM web UI (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nlufzx/llamaui_new_updates/"&gt;Reddit&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aifeed-fyi"&gt; /u/aifeed-fyi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqwcsf/a_list_of_models_released_or_udpated_last_week_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqwcsf/a_list_of_models_released_or_udpated_last_week_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqwcsf/a_list_of_models_released_or_udpated_last_week_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T08:59:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
