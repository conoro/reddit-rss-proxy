<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-13T01:14:34+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r2slnz</id>
    <title>Switching back to local. I am done</title>
    <updated>2026-02-12T12:51:33+00:00</updated>
    <author>
      <name>/u/SkyNetLive</name>
      <uri>https://old.reddit.com/user/SkyNetLive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2slnz/switching_back_to_local_i_am_done/"&gt; &lt;img alt="Switching back to local. I am done" src="https://external-preview.redd.it/c3luc2Vid244MmpnMbGdfNZqaswdb4rlQku0rSvAiCJJUY-RFtBWh5lemTEV.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=801bb84b7f0b591fed562344fd294890c7731975" title="Switching back to local. I am done" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i tried to report and got banned from the sub. this isnt a one off problem. it happens frequently.&lt;/p&gt; &lt;p&gt;I dont mind using openrouter again or setting up something that could fit on a 24GB VRAM. i just need it for coding tasks.&lt;br /&gt; I lurk this sub but i need some guidance. Is Qwen3-coder acceptable?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SkyNetLive"&gt; /u/SkyNetLive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2icufxvn82jg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2slnz/switching_back_to_local_i_am_done/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2slnz/switching_back_to_local_i_am_done/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T12:51:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r32wcz</id>
    <title>Fully opensource NPU for LLM inference (this runs gpt2 in simulation)</title>
    <updated>2026-02-12T19:28:13+00:00</updated>
    <author>
      <name>/u/Altruistic-Tea-5612</name>
      <uri>https://old.reddit.com/user/Altruistic-Tea-5612</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tiny-npu is a minimal, fully synthesizable neural processing unit in SystemVerilog, optimized for learning about how NPUs work from the ground up.&lt;/p&gt; &lt;p&gt;It supports two execution modes: LLM Mode for running real transformer models (GPT-2, LLaMA, Mistral, Qwen2) with a 128-bit microcode ISA, and Graph Mode for running ONNX models (MLP, CNN) with a dedicated graph ISA and tensor descriptor table. Both modes share the same compute engines (systolic array, softmax, etc.) and on-chip SRAM.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/harishsg993010/tiny-NPU"&gt;https://github.com/harishsg993010/tiny-NPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This has instructions can for anyone can download this and run this locally&lt;/p&gt; &lt;p&gt;This is weekend and experiment project built from scratch so this might have bugs&lt;/p&gt; &lt;p&gt;Currently this support only INT8 quantisation&lt;/p&gt; &lt;p&gt;I am working along with couple of others friends to add support for FP32 etc&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altruistic-Tea-5612"&gt; /u/Altruistic-Tea-5612 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r32wcz/fully_opensource_npu_for_llm_inference_this_runs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r32wcz/fully_opensource_npu_for_llm_inference_this_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r32wcz/fully_opensource_npu_for_llm_inference_this_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T19:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2i4lw</id>
    <title>Unsloth just unleashed Glm 5! GGUF NOW!</title>
    <updated>2026-02-12T03:01:37+00:00</updated>
    <author>
      <name>/u/RickyRickC137</name>
      <uri>https://old.reddit.com/user/RickyRickC137</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2i4lw/unsloth_just_unleashed_glm_5_gguf_now/"&gt; &lt;img alt="Unsloth just unleashed Glm 5! GGUF NOW!" src="https://preview.redd.it/nl19fknpbzig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58a7828b3f7d7a5547e90651d41e767976c7aa49" title="Unsloth just unleashed Glm 5! GGUF NOW!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/unsloth/GLM-5-GGUF"&gt;https://huggingface.co/unsloth/GLM-5-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RickyRickC137"&gt; /u/RickyRickC137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nl19fknpbzig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2i4lw/unsloth_just_unleashed_glm_5_gguf_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2i4lw/unsloth_just_unleashed_glm_5_gguf_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T03:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3a0b2</id>
    <title>Is Titans (and MIRAS) heading for the same graveyard as Infini-attention?</title>
    <updated>2026-02-13T00:07:27+00:00</updated>
    <author>
      <name>/u/_WindFall_</name>
      <uri>https://old.reddit.com/user/_WindFall_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, Iâ€™ve been following the AI evolution since 2020, focusing mainly on LLMs. Iâ€™m particularly interested in memory augmentation theory, so much so that I wrote my bachelor's thesis on a linked subject. A while ago, I tried to implement Infini-attention, but I eventually gave up after several months because the &amp;quot;memory&amp;quot; turned out to be far too &amp;quot;lossy&amp;quot; to be practically useful.&lt;/p&gt; &lt;p&gt;When the Titans paper was released by Google (the same team behind Infini-Gemma and the original Transformer), I followed it closely, hoping for new models or implementations. If you search Google or Reddit today, you still find posts from a year ago asking for models, with comments saying, &amp;quot;Itâ€™s only been a few months, give them time to train and refine.&amp;quot;&lt;/p&gt; &lt;p&gt;Fast forward more than a year, and we still have nothing, not even a small 300M open-source model. Recently, an update was released (Titans + MIRAS) which claims better results, but implementation is a nightmare. Unlike &amp;quot;Attention is All You Need,&amp;quot; these papers focus almost entirely on mathematical theory and provide next to no practical implementation advice. Iâ€™ve checked GitHub extensively, but I can't find anything that actually works.&lt;/p&gt; &lt;p&gt;So, I have to ask: Is Titans dead like Infini-attention? Has it been proven that the generation quality is too low to justify a release? It feels strange that after a year of development, there isn't a single working checkpoint available. Iâ€™d really like to know if this architecture is a dead end before I sink another few months into developing something that might be fundamentally flawed.&lt;/p&gt; &lt;p&gt;Has anyone found a working implementation or heard updates from the researchers?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_WindFall_"&gt; /u/_WindFall_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3a0b2/is_titans_and_miras_heading_for_the_same/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3a0b2/is_titans_and_miras_heading_for_the_same/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3a0b2/is_titans_and_miras_heading_for_the_same/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T00:07:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2uama</id>
    <title>Bots on the sub are a real issue</title>
    <updated>2026-02-12T14:05:22+00:00</updated>
    <author>
      <name>/u/perfect-finetune</name>
      <uri>https://old.reddit.com/user/perfect-finetune</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed that some bots over here are very advanced (they score 2-3% on AI detectors, they are perfect rage baiters too?) sometimes they are actually undetectable unless they make a very obvious mistake,how to catch those? Or at least not get rage baited by them? |:&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/perfect-finetune"&gt; /u/perfect-finetune &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2uama/bots_on_the_sub_are_a_real_issue/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2uama/bots_on_the_sub_are_a_real_issue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2uama/bots_on_the_sub_are_a_real_issue/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T14:05:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2teo4</id>
    <title>Zhipu (GLM) Not planning to release a small model for now.</title>
    <updated>2026-02-12T13:27:52+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2teo4/zhipu_glm_not_planning_to_release_a_small_model/"&gt; &lt;img alt="Zhipu (GLM) Not planning to release a small model for now." src="https://preview.redd.it/95fnwbtef2jg1.png?width=140&amp;amp;height=43&amp;amp;auto=webp&amp;amp;s=4eb81f46de34a4235031deb2c2cc9151878ace73" title="Zhipu (GLM) Not planning to release a small model for now." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/95fnwbtef2jg1.png?width=757&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0a4743db9252fdf3a413f2a28b467fff3e7ca07"&gt;https://preview.redd.it/95fnwbtef2jg1.png?width=757&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0a4743db9252fdf3a413f2a28b467fff3e7ca07&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source from discord &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2teo4/zhipu_glm_not_planning_to_release_a_small_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2teo4/zhipu_glm_not_planning_to_release_a_small_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2teo4/zhipu_glm_not_planning_to_release_a_small_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T13:27:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r26zsg</id>
    <title>Z.ai said they are GPU starved, openly.</title>
    <updated>2026-02-11T19:28:16+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r26zsg/zai_said_they_are_gpu_starved_openly/"&gt; &lt;img alt="Z.ai said they are GPU starved, openly." src="https://preview.redd.it/kjy1wqzt2xig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e573128364215e6c6e3a97ac576d0f84213ac948" title="Z.ai said they are GPU starved, openly." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kjy1wqzt2xig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r26zsg/zai_said_they_are_gpu_starved_openly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r26zsg/zai_said_they_are_gpu_starved_openly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T19:28:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2rqmg</id>
    <title>New Ovis2.6-30B-A3B, a lil better than Qwen3-VL-30B-A3B</title>
    <updated>2026-02-12T12:08:29+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2rqmg/new_ovis2630ba3b_a_lil_better_than_qwen3vl30ba3b/"&gt; &lt;img alt="New Ovis2.6-30B-A3B, a lil better than Qwen3-VL-30B-A3B" src="https://external-preview.redd.it/vk5y-X5rKzjDV5x114-FEAIUKduZcw4xVjgoneWCF5o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ab419c3d3048e954f79446457c90976e858a8a2" title="New Ovis2.6-30B-A3B, a lil better than Qwen3-VL-30B-A3B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ovis2.6-30B-A3B, the latest advancement in the Ovis series of Multimodal Large Language Models (MLLMs). Building on the strong foundation of Ovis2.5, Ovis2.6 upgrades the LLM backbone to a Mixture-of-Experts (MoE) architecture, delivering superior multimodal performance at a fraction of the serving cost. It also brings major improvements in long-context and high-resolution understanding, visual reasoning with active image analysis, and information-dense document comprehension.&lt;/p&gt; &lt;p&gt;It would be great if we had comparisons against GLM 4.7 Flash but I doubt it's better at coding than GLM, rather it seems this one is now the new best vision model at the 30B-A3B size. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AIDC-AI/Ovis2.6-30B-A3B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2rqmg/new_ovis2630ba3b_a_lil_better_than_qwen3vl30ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2rqmg/new_ovis2630ba3b_a_lil_better_than_qwen3vl30ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T12:08:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2e8mp</id>
    <title>#SaveLocalLLaMA</title>
    <updated>2026-02-12T00:07:52+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2e8mp/savelocalllama/"&gt; &lt;img alt="#SaveLocalLLaMA" src="https://preview.redd.it/0memizzegyig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91831d8533b440fb49b752bf597176bc5758ec99" title="#SaveLocalLLaMA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0memizzegyig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2e8mp/savelocalllama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2e8mp/savelocalllama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T00:07:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3aod7</id>
    <title>Qwen3 Coder Next : Loop Fix</title>
    <updated>2026-02-13T00:36:48+00:00</updated>
    <author>
      <name>/u/TBG______</name>
      <uri>https://old.reddit.com/user/TBG______</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;My Optimal llama.cpp Settings for Qwen3-Coder-Next After 1 Day of Testing&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As many of you have noted, the new Qwen3 Next models tend to get stuck in repetitive loops quite frequently. Additionally, both the coder and instruct variants with standard temperature settings can be overly creative - often initiating new tasks without being asked. For example, when you request &amp;quot;change the this in A,&amp;quot; it might decide to change multiple other Leters as well, which isn't always what we need.&lt;/p&gt; &lt;p&gt;After a full day of testing, I've found these settings work best for Qwen3-Coder-Next with llama.cpp to prevent loops and reduce unwanted creativity:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# This is the Loop Fix --temp 0.8 # default 1 was to creative for me --top-p 0.95 --min-p 0.01 --top-k 40 --presence-penalty 1.10 --dry-multiplier 0.5 --dry-allowed-length 5 --frequency_penalty 0.5&amp;quot; # This is for my system and Qwen3-Coder-Next-MXFP4_MOE so it fits all in my 2 GPUs with ctx 256k --cache-type-k q8_0 --cache-type-v q8_0 --threads 64 --threads-batch 64 --n-gpu-layers 999 ( you can just use --fit on) --n-cpu-moe 0 ( you can just use --fit on) --batch-size 2048 --ubatch-size 512&amp;quot; --parallel 1 # And the rest --model %MODEL% --alias %ALIAS% --host 0.0.0.0 --port 8080 --ctx-size %CTX% --jinja --flash-attn on --context-shift --cache-ram -1 (optional unlimited ram for cache ) Select ctx-size: 1) 32768 (32k) 2) 65536 (64k) 3) 98304 (96k) 4) 131072 (128k) 5) 180224 (180k) 6) 196608 (196K) 7) 202752 (200k) 8) 262144 (256k) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These parameters help keep the model focused on the actual task without going off on tangents or getting stuck repeating itself.&lt;/p&gt; &lt;p&gt;Stats: promt 1400 t/s | gen 30-38 t/s Windows WSL (way faster in wsl than in windos native 24 to 28 t/s) 3090RTX +5090RTX&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TBG______"&gt; /u/TBG______ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3aod7/qwen3_coder_next_loop_fix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3aod7/qwen3_coder_next_loop_fix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3aod7/qwen3_coder_next_loop_fix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T00:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2yxpw</id>
    <title>GLM-5 compared with more relevant models</title>
    <updated>2026-02-12T17:03:34+00:00</updated>
    <author>
      <name>/u/cloverasx</name>
      <uri>https://old.reddit.com/user/cloverasx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2yxpw/glm5_compared_with_more_relevant_models/"&gt; &lt;img alt="GLM-5 compared with more relevant models" src="https://preview.redd.it/vzxxad7eh3jg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cd62c388f788319d6c12c3ce5406e9addf9d171d" title="GLM-5 compared with more relevant models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not to discredit or trivialize the accomplishment, but opus 4.6 and gpt 5.3 codex are the more appropriate models to compare this against since they're direct replacements/improvements on their previous models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cloverasx"&gt; /u/cloverasx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vzxxad7eh3jg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2yxpw/glm5_compared_with_more_relevant_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2yxpw/glm5_compared_with_more_relevant_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T17:03:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1r397hi</id>
    <title>Step 3.5 Flash is a beast?</title>
    <updated>2026-02-12T23:33:47+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have not used it on serious tasks until today.&lt;/p&gt; &lt;p&gt;I gave it a complex task of merging, it worked through it and stayed completely sane even at 90k context and successfully finished the task. It felt so gut, I double checked that I am not running a closed source frontier model like claude 4.6.&lt;/p&gt; &lt;p&gt;I mean, for agentic tasks, this is definitely better than Gemini 3.0 Preview. And it's so fast.&lt;/p&gt; &lt;p&gt;I tested it on opencode and claude code (I don't use it, just wanted to see how flexible it is, and also found out setting up non anthropic model is a pain in the ass) and it did great in both.&lt;/p&gt; &lt;p&gt;What is your experience? Do we have open weight model that is in real world tasks better than gemini 3.0 pro?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r397hi/step_35_flash_is_a_beast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r397hi/step_35_flash_is_a_beast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r397hi/step_35_flash_is_a_beast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T23:33:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1r30e3y</id>
    <title>Hibiki-Zero, real-time speech translation model by Kyutai Labs</title>
    <updated>2026-02-12T17:57:12+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r30e3y/hibikizero_realtime_speech_translation_model_by/"&gt; &lt;img alt="Hibiki-Zero, real-time speech translation model by Kyutai Labs" src="https://external-preview.redd.it/eWp0b2k3c2JyM2pnMXyf8-rvm1C__Q4bDL3gJBkjO_bjkyMUPsobX80FiZpA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=910b5f94278a58d986c5730c51938aee29bf0b52" title="Hibiki-Zero, real-time speech translation model by Kyutai Labs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like another banger from Kyutai!&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/kyutai/hibiki-zero-3b-pytorch-bf16"&gt;https://huggingface.co/kyutai/hibiki-zero-3b-pytorch-bf16&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://kyutai.org/blog/2026-02-12-hibiki-zero"&gt;https://kyutai.org/blog/2026-02-12-hibiki-zero&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More samples: &lt;a href="https://huggingface.co/spaces/kyutai/hibiki-zero-samples"&gt;https://huggingface.co/spaces/kyutai/hibiki-zero-samples&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gm1dy5sbr3jg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r30e3y/hibikizero_realtime_speech_translation_model_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r30e3y/hibikizero_realtime_speech_translation_model_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T17:57:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2wcbr</id>
    <title>NeuTTS Nano Multilingual Collection: 120M Params on-device TTS in German, French, and Spanish</title>
    <updated>2026-02-12T15:25:57+00:00</updated>
    <author>
      <name>/u/TeamNeuphonic</name>
      <uri>https://old.reddit.com/user/TeamNeuphonic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2wcbr/neutts_nano_multilingual_collection_120m_params/"&gt; &lt;img alt="NeuTTS Nano Multilingual Collection: 120M Params on-device TTS in German, French, and Spanish" src="https://external-preview.redd.it/b2JtcjE2dGUwM2pnMeVwwjyNKdPH51Be4sQFZ3EXv8ZdpH_FAux6dp67XSVh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71cc49b609d4edb7b1971eb68c5f04b379e1333b" title="NeuTTS Nano Multilingual Collection: 120M Params on-device TTS in German, French, and Spanish" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, we're the team behind NeuTTS (Neuphonic). Some of you may have seen our previous releases of NeuTTS Air and NeuTTS Nano.&lt;/p&gt; &lt;p&gt;The most requested feature by far has been multilingual support, so today we're releasing three new language-specific Nano models: German, French, and Spanish.&lt;/p&gt; &lt;p&gt;Quick specs:&lt;/p&gt; &lt;p&gt;120M active parameters (same as Nano English)&lt;/p&gt; &lt;p&gt;Real-time inference on CPU via llama.cpp / llama-cpp-python&lt;/p&gt; &lt;p&gt;GGUF format (Q4 and Q8 quantizations available)&lt;/p&gt; &lt;p&gt;Zero-shot voice cloning from ~3 seconds of reference audio, works across all supported languages&lt;/p&gt; &lt;p&gt;Runs on laptops, phones, Raspberry Pi, Jetson&lt;/p&gt; &lt;p&gt;Fully local, nothing leaves the device&lt;/p&gt; &lt;p&gt;Architecture: Same as Nano English. Compact LM backbone + NeuCodec (our open-source neural audio codec, single codebook, 50hz). Each language has its own dedicated model for best quality.&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;ðŸ‡©ðŸ‡ª German: &lt;a href="https://huggingface.co/neuphonic/neutts-nano-german"&gt;https://huggingface.co/neuphonic/neutts-nano-german&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ðŸ‡«ðŸ‡· French: &lt;a href="https://huggingface.co/neuphonic/neutts-nano-french"&gt;https://huggingface.co/neuphonic/neutts-nano-french&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ðŸ‡ªðŸ‡¸ Spanish: &lt;a href="https://huggingface.co/neuphonic/neutts-nano-spanish"&gt;https://huggingface.co/neuphonic/neutts-nano-spanish&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF Spaces: &lt;a href="https://huggingface.co/spaces/neuphonic/neutts-nano-multilingual-collection"&gt;https://huggingface.co/spaces/neuphonic/neutts-nano-multilingual-collection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/neuphonic/neutts"&gt;https://github.com/neuphonic/neutts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Each model is a separate HF repo. Same install process as the English Nano, just swap the backbone repo path.&lt;/p&gt; &lt;p&gt;We're working on more languages. If there's a specific one you'd like to see next, let us know. Happy to answer any questions about the architecture, benchmarks, or deployment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TeamNeuphonic"&gt; /u/TeamNeuphonic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ey2c253c03jg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2wcbr/neutts_nano_multilingual_collection_120m_params/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2wcbr/neutts_nano_multilingual_collection_120m_params/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T15:25:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2wgzx</id>
    <title>Alibaba Open-Sources Zvec</title>
    <updated>2026-02-12T15:30:47+00:00</updated>
    <author>
      <name>/u/techlatest_net</name>
      <uri>https://old.reddit.com/user/techlatest_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Alibaba Open-Sources Zvec: An Embedded Vector Database Bringing SQLite-like Simplicity and High-Performance On-Device RAG to Edge Applications&lt;/h1&gt; &lt;p&gt;Link: &lt;a href="https://github.com/alibaba/zvec"&gt;https://github.com/alibaba/zvec&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techlatest_net"&gt; /u/techlatest_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2wgzx/alibaba_opensources_zvec/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2wgzx/alibaba_opensources_zvec/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2wgzx/alibaba_opensources_zvec/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T15:30:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2zzp3</id>
    <title>''The MiniMax M2.5 model weights will be open-sourced on HuggingFace'' - from the official MiniMax account on X</title>
    <updated>2026-02-12T17:42:39+00:00</updated>
    <author>
      <name>/u/Bestlife73</name>
      <uri>https://old.reddit.com/user/Bestlife73</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2zzp3/the_minimax_m25_model_weights_will_be_opensourced/"&gt; &lt;img alt="''The MiniMax M2.5 model weights will be open-sourced on HuggingFace'' - from the official MiniMax account on X" src="https://preview.redd.it/z51pi23wo3jg1.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=1596dabe8ec059a61cf08d2afbd3c143c780427e" title="''The MiniMax M2.5 model weights will be open-sourced on HuggingFace'' - from the official MiniMax account on X" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open source release confirmed.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/MiniMax_AI/status/2022001452131221872"&gt;MiniMax (official) on X: &amp;quot;MiniMax M2.5: Faster. Stronger. Smarter. Built for Real-World Productivity.&amp;quot; / X&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z51pi23wo3jg1.png?width=942&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30dd0075f7f3ddafccf30cf06e3ec35ad2401729"&gt;https://preview.redd.it/z51pi23wo3jg1.png?width=942&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=30dd0075f7f3ddafccf30cf06e3ec35ad2401729&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bestlife73"&gt; /u/Bestlife73 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2zzp3/the_minimax_m25_model_weights_will_be_opensourced/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2zzp3/the_minimax_m25_model_weights_will_be_opensourced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2zzp3/the_minimax_m25_model_weights_will_be_opensourced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T17:42:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2vs3s</id>
    <title>Is this true? GLM 5 was trained solely using huawei hardware and their mindspore framework</title>
    <updated>2026-02-12T15:04:43+00:00</updated>
    <author>
      <name>/u/Acceptable_Home_</name>
      <uri>https://old.reddit.com/user/Acceptable_Home_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2vs3s/is_this_true_glm_5_was_trained_solely_using/"&gt; &lt;img alt="Is this true? GLM 5 was trained solely using huawei hardware and their mindspore framework" src="https://preview.redd.it/7q0za97mw2jg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87dc6fe48350fd9ca4d5b62b500e942dea9fd500" title="Is this true? GLM 5 was trained solely using huawei hardware and their mindspore framework" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only confirmed model to be 100% trained on huawei cards before GLM 5 was GLM image, solely trained on huawei hardware and mindspore infrastructure as of &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt; official statements &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.trendingtopics.eu/glm-5-the-worlds-strongest-open-source-llm-solely-trained-on-chinese-huawei-chips/"&gt;https://www.trendingtopics.eu/glm-5-the-worlds-strongest-open-source-llm-solely-trained-on-chinese-huawei-chips/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I find it kind of astonishing, impressed af, note it that formal technical paper has been released by Z.ai for glm 5 So.. we still don't know if it's 100% true or not but the article says so They said it was solely trained on huawei ascend using their own mindspore framework (complete pipeline training to inference) This is so big because glm 5 has literally beaten gemini 3 pro, opus 4.5 and gpt 5.2, on the third spot behind by both opus 4.6 variants and gpt 5.2 xhigh&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable_Home_"&gt; /u/Acceptable_Home_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7q0za97mw2jg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2vs3s/is_this_true_glm_5_was_trained_solely_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2vs3s/is_this_true_glm_5_was_trained_solely_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T15:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1r35326</id>
    <title>I'm playing telephone pictionary with LLMs, VLMs, SDs, and Kokoro on my Strix Halo</title>
    <updated>2026-02-12T20:51:36+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35326/im_playing_telephone_pictionary_with_llms_vlms/"&gt; &lt;img alt="I'm playing telephone pictionary with LLMs, VLMs, SDs, and Kokoro on my Strix Halo" src="https://external-preview.redd.it/YmJmc3hpcWVtNGpnMZDShp7-xGpOcgsVOxorkEUrrQwTSNVbCBVhROxXE8sP.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6045e2eeee4aab8952d17984a247e68ed71ffb61" title="I'm playing telephone pictionary with LLMs, VLMs, SDs, and Kokoro on my Strix Halo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/txyz48qem4jg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35326/im_playing_telephone_pictionary_with_llms_vlms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r35326/im_playing_telephone_pictionary_with_llms_vlms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T20:51:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r35ceo</id>
    <title>GLM-5 and Minimax-2.5 on Fiction.liveBench</title>
    <updated>2026-02-12T21:01:32+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35ceo/glm5_and_minimax25_on_fictionlivebench/"&gt; &lt;img alt="GLM-5 and Minimax-2.5 on Fiction.liveBench" src="https://preview.redd.it/4390rts4o4jg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72aeaba258795dc87fe96ebe0ff21b86947a9bfd" title="GLM-5 and Minimax-2.5 on Fiction.liveBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4390rts4o4jg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35ceo/glm5_and_minimax25_on_fictionlivebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r35ceo/glm5_and_minimax25_on_fictionlivebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T21:01:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2xjwp</id>
    <title>Ring-1T-2.5 released by inclusionAI</title>
    <updated>2026-02-12T16:12:00+00:00</updated>
    <author>
      <name>/u/Bestlife73</name>
      <uri>https://old.reddit.com/user/Bestlife73</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2xjwp/ring1t25_released_by_inclusionai/"&gt; &lt;img alt="Ring-1T-2.5 released by inclusionAI" src="https://external-preview.redd.it/tM5lqBklywlEgOf58rJ0Tjpu5co9UMoZ2A7rXbdJMJU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=315ed6672b4c954402baf403f2d93d36865cb7ca" title="Ring-1T-2.5 released by inclusionAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SOTA performance on deep thinking&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bestlife73"&gt; /u/Bestlife73 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T-2.5-FP8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2xjwp/ring1t25_released_by_inclusionai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2xjwp/ring1t25_released_by_inclusionai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T16:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r30bgz</id>
    <title>Ming-flash-omni-2.0: 100B MoE (6B active) omni-modal model - unified speech/SFX/music generation</title>
    <updated>2026-02-12T17:54:35+00:00</updated>
    <author>
      <name>/u/bobeeeeeeeee8964</name>
      <uri>https://old.reddit.com/user/bobeeeeeeeee8964</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r30bgz/mingflashomni20_100b_moe_6b_active_omnimodal/"&gt; &lt;img alt="Ming-flash-omni-2.0: 100B MoE (6B active) omni-modal model - unified speech/SFX/music generation" src="https://external-preview.redd.it/_l8FEwEfj_HhNLZzTpSlQTuaBTUdY25FimxgyYeDN_Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59d3dea77c3801617c49eea3f48b88a3ec1ddde3" title="Ming-flash-omni-2.0: 100B MoE (6B active) omni-modal model - unified speech/SFX/music generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ant Group just open-sourced Ming-flash-omni-2.0, a true (omni-modal) model: image + text + video + audio input â†’ image + text + audio output, all in one unified architecture. Looks realy interesting. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobeeeeeeeee8964"&gt; /u/bobeeeeeeeee8964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-flash-omni-2.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r30bgz/mingflashomni20_100b_moe_6b_active_omnimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r30bgz/mingflashomni20_100b_moe_6b_active_omnimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T17:54:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2ygac</id>
    <title>Why do we allow "un-local" content</title>
    <updated>2026-02-12T16:45:57+00:00</updated>
    <author>
      <name>/u/JacketHistorical2321</name>
      <uri>https://old.reddit.com/user/JacketHistorical2321</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title somewhat says it all. I get that it's related but if links to new models are being discussed shouldn't it be a requirement that there be a &amp;quot;local&amp;quot; component?&lt;/p&gt; &lt;p&gt;Edit: since this is starting to get some traction I want to be a little more specific with what I'm talking about. &lt;/p&gt; &lt;p&gt;In the past 2 to 3 days we've seen multiple posts related to new models being released. They include links to API resources prior to weights being released. &lt;/p&gt; &lt;p&gt;I believe that if a post includes a link to API serving hosts then it should be requirement that a hugging face link is also included. If both of these requirements cannot be met for any reason (ex. Weights will probably be released but have not been released yet) the post should be taken down. &lt;/p&gt; &lt;p&gt;This would at least put some guardrails in place that would make sure posts are closer to the true nature of this sub as opposed to being low-key marketing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JacketHistorical2321"&gt; /u/JacketHistorical2321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2ygac/why_do_we_allow_unlocal_content/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2ygac/why_do_we_allow_unlocal_content/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2ygac/why_do_we_allow_unlocal_content/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T16:45:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2xotu</id>
    <title>Minimax M2.5 Officially Out</title>
    <updated>2026-02-12T16:17:13+00:00</updated>
    <author>
      <name>/u/Which_Slice1600</name>
      <uri>https://old.reddit.com/user/Which_Slice1600</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2xotu/minimax_m25_officially_out/"&gt; &lt;img alt="Minimax M2.5 Officially Out" src="https://preview.redd.it/75rjx62d93jg1.png?width=140&amp;amp;height=67&amp;amp;auto=webp&amp;amp;s=bc1913b92a211d48c5d2979574442a87148c17cf" title="Minimax M2.5 Officially Out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only official webpages released now. But the bench looks very promising:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SWE-Bench Verified 80.2%&lt;/li&gt; &lt;li&gt;Multi-SWE-Bench 51.3%&lt;/li&gt; &lt;li&gt;BrowseComp 76.3%&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Edit: replaced with the en page:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.minimax.io/news/minimax-m25"&gt;https://www.minimax.io/news/minimax-m25&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Which_Slice1600"&gt; /u/Which_Slice1600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r2xotu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2xotu/minimax_m25_officially_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2xotu/minimax_m25_officially_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T16:17:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r35d2x</id>
    <title>MiniMaxAI MiniMax-M2.5 has 230b parameters and 10b active parameters</title>
    <updated>2026-02-12T21:02:15+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35d2x/minimaxai_minimaxm25_has_230b_parameters_and_10b/"&gt; &lt;img alt="MiniMaxAI MiniMax-M2.5 has 230b parameters and 10b active parameters" src="https://external-preview.redd.it/_kcNQarR05LXfQqSjI9sCiHSj5IycOpRZaI00SHW4k8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96ababa53bad9198147827e5856fa3e99fbda827" title="MiniMaxAI MiniMax-M2.5 has 230b parameters and 10b active parameters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenHands reveals the model size in their announcement.&lt;/p&gt; &lt;p&gt;Still waiting for the model to appear on HF.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openhands.dev/blog/minimax-m2-5-open-weights-models-catch-up-to-claude"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35d2x/minimaxai_minimaxm25_has_230b_parameters_and_10b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r35d2x/minimaxai_minimaxm25_has_230b_parameters_and_10b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T21:02:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
