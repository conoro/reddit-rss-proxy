<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-07T11:48:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1na3xkd</id>
    <title>Llama-3.3-Nemotron-Super-49B-v1.5 is very good model to summarized long text into formatted markdown (Nvidia also provided free unlimited API call with rate limit)</title>
    <updated>2025-09-06T16:29:19+00:00</updated>
    <author>
      <name>/u/dheetoo</name>
      <uri>https://old.reddit.com/user/dheetoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a project to convert medical lesson data from websites into markdown format for a RAG application. Tested several popular models including Qwen3 235B, Gemma 3 27B, and GPT-oss-120 they all performed well technically, but as someone with a medical background, the output style just didn't click with me (totally subjective, I know).&lt;/p&gt; &lt;p&gt;So I decided to experiment with some models on NVIDIA's API platform and stumbled upon &lt;strong&gt;Llama-3.3-Nemotron-Super-49B-v1.5&lt;/strong&gt; This thing is surprisingly solid for my use case. I'd tried it before in an agent setup where it didn't perform great on evals, so I had to stick with the bigger models. But for this specific summarization task, it's been excellent.&lt;/p&gt; &lt;p&gt;The output is well-written, requires minimal proofreading, and the markdown formatting is clean right out of the box. Plus it's free through NVIDIA's API (40 requests/minute limit), which is perfect for my workflow since I manually review everything anyway.&lt;/p&gt; &lt;p&gt;Definitely worth trying if you're doing similar work with medical or technical content, write a good prompt still the key though.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dheetoo"&gt; /u/dheetoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3xkd/llama33nemotronsuper49bv15_is_very_good_model_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3xkd/llama33nemotronsuper49bv15_is_very_good_model_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na3xkd/llama33nemotronsuper49bv15_is_very_good_model_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T16:29:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nairnx</id>
    <title>Did you notice the VibeVoice model card privacy policy?</title>
    <updated>2025-09-07T03:23:52+00:00</updated>
    <author>
      <name>/u/pilkyton</name>
      <uri>https://old.reddit.com/user/pilkyton</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quoting Microsoft's repo and HuggingFace model card. This text &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nairnx/comment/ncvhtde/"&gt;was in their repo from the start&lt;/a&gt;, 14 days ago. You &lt;strong&gt;can still see it in the oldest commit from day 1&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I wonder if any of this is true for their released local-machine source code; or if it's only true for output generated by some specific website?&lt;/p&gt; &lt;p&gt;If their source code repo contains spyware code, or if it's hidden in a requirements.txt dependency, or if the model itself contains pickled Python spyware bytecode, then we should know about it.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;To mitigate the risks of VibeVoice misuse, we have:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Embedded an audible disclaimer (e.g. ‚ÄúThis segment was generated by AI‚Äù) automatically into every synthesized audio file.&lt;/li&gt; &lt;li&gt;Added an imperceptible watermark to generated audio so third parties can verify VibeVoice provenance. Please see contact information at the end of this model card.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Logged inference requests (hashed) for abuse pattern detection and publishing aggregated statistics quarterly.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Users are responsible for sourcing their datasets legally and ethically. This may include securing appropriate rights and/or anonymizing data prior to use with VibeVoice.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Users are reminded to be mindful of data privacy concerns.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pilkyton"&gt; /u/pilkyton &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nairnx/did_you_notice_the_vibevoice_model_card_privacy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nairnx/did_you_notice_the_vibevoice_model_card_privacy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nairnx/did_you_notice_the_vibevoice_model_card_privacy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T03:23:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1naiy7o</id>
    <title>What do yall use your agents for?</title>
    <updated>2025-09-07T03:33:17+00:00</updated>
    <author>
      <name>/u/ChiefMalone</name>
      <uri>https://old.reddit.com/user/ChiefMalone</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My rig is 2x 3090 and 64gb vram. Currently working with 1 card to host Gemma 3 which does image recognition and text prompt stuff to basically act as my Jarvis (work in progress still, been playing with possibilities for about 2 months now and finally feel comfortable with embeddings, rag retrieval, agent tooling etc) and the other card runs some of the agents tools like image/video gen and TTS (vibe voice). Eventually want to give my agent a body like a little rc car it can drive around or something lol. Just curious what others are doing with their local setups. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChiefMalone"&gt; /u/ChiefMalone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naiy7o/what_do_yall_use_your_agents_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naiy7o/what_do_yall_use_your_agents_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naiy7o/what_do_yall_use_your_agents_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T03:33:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1naolrp</id>
    <title>Best app and quant type for hosting LLM on modern android phone, with an endpoint for Sillytavern?</title>
    <updated>2025-09-07T09:12:18+00:00</updated>
    <author>
      <name>/u/CanineAssBandit</name>
      <uri>https://old.reddit.com/user/CanineAssBandit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean actually running the model on the device, not accessing a remote API elsewhere. The use case is run a small RP fine tune on the phone in places without internet access. &lt;/p&gt; &lt;p&gt;Phone is S23 Ultra with a Snapdragon 8 Gen 2 with 8GB ram, 4B and small quant 8Bs fit but small quant 8Bs seem unusably slow (with Layla at least).&lt;/p&gt; &lt;p&gt;-I don't know what quant type is best for ARM SOCs,&lt;/p&gt; &lt;p&gt;-or what apps will run what kinds of quants,&lt;/p&gt; &lt;p&gt;-or which apps/quant types utilize the Qualcomm Hexagon AI accelerator chip thing embedded into the Snapdragon 8 Gen 2,&lt;/p&gt; &lt;p&gt;-or if I should even care about utilizing the Hexagon AI accelerator vs the standard compute on the main cpu in the SOC,&lt;/p&gt; &lt;p&gt;-or which apps (if any) will give me an actual IP address port or other type of end point that Sillytavern can use (Sillytavern works through Termux on android).&lt;/p&gt; &lt;p&gt;Layla crashes all the time and does not seem to be able to give an endpoint for Sillytavern. It also gives terrible outputs compared to the same models on PC, and from what I can tell my sampler settings are the same.&lt;/p&gt; &lt;p&gt;I've used search but all the posts about this are either very outdated or not quite relevant (no mention of how to set up a link between Sillytavern as the frontend on the phone, and some type of host on the phone). I'd appreciate any guidance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CanineAssBandit"&gt; /u/CanineAssBandit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naolrp/best_app_and_quant_type_for_hosting_llm_on_modern/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naolrp/best_app_and_quant_type_for_hosting_llm_on_modern/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naolrp/best_app_and_quant_type_for_hosting_llm_on_modern/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T09:12:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1n9x1ho</id>
    <title>So I tried Qwen 3 Max skills for programming</title>
    <updated>2025-09-06T11:19:58+00:00</updated>
    <author>
      <name>/u/TruckUseful4423</name>
      <uri>https://old.reddit.com/user/TruckUseful4423</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9x1ho/so_i_tried_qwen_3_max_skills_for_programming/"&gt; &lt;img alt="So I tried Qwen 3 Max skills for programming" src="https://external-preview.redd.it/_Qhoi5tM5uwwG8h9pFbHe7_wEttk4KG4M_-539ZjdPE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d6c7a702febc40739857dbd7082314de38697a5" title="So I tried Qwen 3 Max skills for programming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;So I Tried Qwen 3 Max for Programming ‚Äî Project VMP (Visualized Music Player)&lt;/h1&gt; &lt;p&gt;I wanted to see how far Qwen 3 Max could go when tasked with building a full project from a very detailed specification. The result: VMP ‚Äî Visualized Music Player, a cyberpunk-style music player with FFT-based visualizations, crossfade playback, threading, and even a web terminal.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Tech Stack &amp;amp; Dependencies&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Python 3.11&lt;/li&gt; &lt;li&gt;pygame, numpy, mutagen, pydub, websockets&lt;/li&gt; &lt;li&gt;Requires FFmpeg in PATH&lt;/li&gt; &lt;li&gt;Runs with a simple BAT file on Windows&lt;/li&gt; &lt;li&gt;SDL hints set for Windows: &lt;ul&gt; &lt;li&gt;SDL_RENDER_DRIVER=direct3d&lt;/li&gt; &lt;li&gt;SDL_HINT_RENDER_SCALE_QUALITY=1&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Core Features&lt;/h1&gt; &lt;h1&gt;Configuration&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;AudioCfg, VisualCfg, UiCfg dataclasses with sane defaults&lt;/li&gt; &lt;li&gt;Global instances: AUDIO, VIS, UI&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Logging&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Custom logger vmp with console + rotating file handler&lt;/li&gt; &lt;li&gt;Optional WebTermHandler streams logs to connected websocket clients&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;FFmpeg Integration&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Automatic FFmpeg availability check&lt;/li&gt; &lt;li&gt;On-demand decode with ffmpeg -ss ... -t ... into raw PCM&lt;/li&gt; &lt;li&gt;Reliable seeking via decoded segments&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Music Library&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Recursive scan for .mp3, .wav, .flac, .ogg, .m4a&lt;/li&gt; &lt;li&gt;Metadata via mutagen (fallback to smart filename guessing)&lt;/li&gt; &lt;li&gt;Sortable, with directory ignore list&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;DSP &amp;amp; Analysis&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Stereo EQ (low shelf, peaking, high shelf) + softclip limiter&lt;/li&gt; &lt;li&gt;FFT analysis with Hann windows, band mapping, adaptive beat detection&lt;/li&gt; &lt;li&gt;Analysis LRU cache (capacity 64) for performance&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Visualization&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Cyberpunk ring with dotted ticks, glow halos, progress arc&lt;/li&gt; &lt;li&gt;Outward 64-band bars + central vocal pulse disc&lt;/li&gt; &lt;li&gt;Smooth envelopes, beat halos, ~60% transparent overlays&lt;/li&gt; &lt;li&gt;Fonts: cyberpunk.ttf if present, otherwise Segoe/Arial&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Playback Model&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;pygame.mixer at 44.1 kHz stereo&lt;/li&gt; &lt;li&gt;Dual-channel system for precise seeking and crossfade overlap&lt;/li&gt; &lt;li&gt;Smooth cosine crossfade without freezing visuals&lt;/li&gt; &lt;li&gt;Modes: &lt;ul&gt; &lt;li&gt;Music = standard streaming&lt;/li&gt; &lt;li&gt;Channel = decoded segment playback (reliable seek)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Window &amp;amp; UI&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Resizable window, optional fake fullscreen&lt;/li&gt; &lt;li&gt;Backgrounds with dark overlay, cache per resolution&lt;/li&gt; &lt;li&gt;Topmost toggle, drag-window mode (Windows)&lt;/li&gt; &lt;li&gt;Presets for HUD/FPS/TIME/TITLE (keys 1‚Äì5, V, F2)&lt;/li&gt; &lt;li&gt;Help overlay (H) shows all controls&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Controls&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Playback: Space pause/resume, N/P next/prev, S shuffle, R repeat-all&lt;/li&gt; &lt;li&gt;Seek: ‚Üê/‚Üí ‚àí5s / +5s&lt;/li&gt; &lt;li&gt;Window/UI: F fake fullscreen, T topmost, B toggle backgrounds, [/] prev/next BG&lt;/li&gt; &lt;li&gt;Volume: Mouse wheel; volume display fades quickly&lt;/li&gt; &lt;li&gt;Quit: Esc / Q&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Web Terminal&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Optional --webterm flag&lt;/li&gt; &lt;li&gt;Websocket server on ws://localhost:3030&lt;/li&gt; &lt;li&gt;Streams logs + accepts remote commands (n, p, space, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Performance&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Low-CPU visualization mode (--viz-lowcpu)&lt;/li&gt; &lt;li&gt;Heavy operations skipped while paused&lt;/li&gt; &lt;li&gt;Preallocated NumPy buffers &amp;amp; surface caches&lt;/li&gt; &lt;li&gt;Threaded FFT + loader workers, priority queue for analysis&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;CLI Options&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;--music-dir Path to your music library --backgrounds Path to background images --debug Verbose logging --shuffle Enable shuffle mode --repeat-all Repeat entire playlist --no-fft Disable FFT --viz-lowcpu Low CPU visualization --ext File extensions to include --ignore Ignore directories --no-tags Skip metadata tags --webterm Enable websocket terminal &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Crossfade works seamlessly, with no visual freeze&lt;/li&gt; &lt;li&gt;Seek is reliable thanks to FFmpeg segment decoding&lt;/li&gt; &lt;li&gt;Visualizations scale cleanly across windowed and fake-fullscreen modes&lt;/li&gt; &lt;li&gt;Handles unknown tags gracefully by guessing titles from filenames&lt;/li&gt; &lt;li&gt;Everything runs as a single script, no external modules beyond listed deps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üëâ Full repo: &lt;a href="https://github.com/feckom/vmp"&gt;github.com/feckom/vmp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Results&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wixd9wdhzinf1.jpg?width=1282&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6b1a18941410cb3a7f4b0da54f36003298180dca"&gt;https://preview.redd.it/wixd9wdhzinf1.jpg?width=1282&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6b1a18941410cb3a7f4b0da54f36003298180dca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m6chuvdhzinf1.jpg?width=1282&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0c0df79e54b59b2ab064e4f7c791bb7984297a8b"&gt;https://preview.redd.it/m6chuvdhzinf1.jpg?width=1282&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0c0df79e54b59b2ab064e4f7c791bb7984297a8b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bma8vwdhzinf1.jpg?width=1282&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=bfe32593e27d63fd9e533c6202979bc9da6d8330"&gt;https://preview.redd.it/bma8vwdhzinf1.jpg?width=1282&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=bfe32593e27d63fd9e533c6202979bc9da6d8330&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TruckUseful4423"&gt; /u/TruckUseful4423 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9x1ho/so_i_tried_qwen_3_max_skills_for_programming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n9x1ho/so_i_tried_qwen_3_max_skills_for_programming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n9x1ho/so_i_tried_qwen_3_max_skills_for_programming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T11:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nae1zj</id>
    <title>Effecient hot-swappable LoRA variant supported in llama.cpp</title>
    <updated>2025-09-06T23:28:27+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nae1zj/effecient_hotswappable_lora_variant_supported_in/"&gt; &lt;img alt="Effecient hot-swappable LoRA variant supported in llama.cpp" src="https://external-preview.redd.it/j9pZxlhUy5RV0Ryi-2-5BCQ4nmGeUpe7px39264mgOc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9f29e94704fe7ecbbc414657d711e7dcf339469" title="Effecient hot-swappable LoRA variant supported in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Activated LoRA: Fine-tuned LLMs for Intrinsics - &lt;a href="https://arxiv.org/abs/2504.12397"&gt;https://arxiv.org/abs/2504.12397&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Despite the promise of highly customized behaviors and capabilities, switching between relevant LoRAs in a multiturn setting is inefficient, as the key-value (KV) cache of the entire turn history must be recomputed with the LoRA weights before generation can begin. To address this problem, we propose Activated LoRA (aLoRA), an adapter architecture which modifies the LoRA framework to only adapt weights for the tokens in the sequence after the aLoRA is invoked. This change crucially allows aLoRA to accept the base model's KV cache of the input string, meaning that aLoRA can be instantly activated whenever needed in a chain without recomputing the cache&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I don't think any other model besides granite is supported yet. This has some merit for hybrid and cpu inference, especially if they can figure out alora extraction. If we are changing the model especially the strength/influence that can give better results than just an appended prompt alone. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15327"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nae1zj/effecient_hotswappable_lora_variant_supported_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nae1zj/effecient_hotswappable_lora_variant_supported_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T23:28:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1na0mw3</id>
    <title>Qwen3 30B A3B Hits 13 token/s on 4x Raspberry Pi 5</title>
    <updated>2025-09-06T14:15:15+00:00</updated>
    <author>
      <name>/u/vibjelo</name>
      <uri>https://old.reddit.com/user/vibjelo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na0mw3/qwen3_30b_a3b_hits_13_tokens_on_4x_raspberry_pi_5/"&gt; &lt;img alt="Qwen3 30B A3B Hits 13 token/s on 4x Raspberry Pi 5" src="https://external-preview.redd.it/KUWKhlT5OZYpzmuPdkrY6FyowQ4PaYe23RiUvraDVrQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4733d277f6f6e759fe47794087d1da790f8d36b7" title="Qwen3 30B A3B Hits 13 token/s on 4x Raspberry Pi 5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibjelo"&gt; /u/vibjelo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/b4rtaz/distributed-llama/discussions/255"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na0mw3/qwen3_30b_a3b_hits_13_tokens_on_4x_raspberry_pi_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na0mw3/qwen3_30b_a3b_hits_13_tokens_on_4x_raspberry_pi_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T14:15:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nalgur</id>
    <title>Create a shared alternative to OpenRouter Together</title>
    <updated>2025-09-07T05:56:06+00:00</updated>
    <author>
      <name>/u/Far-Incident822</name>
      <uri>https://old.reddit.com/user/Far-Incident822</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I had this idea after reading the latest paper by Nvidia on making large models more efficient for long context through modification of the model. &lt;/p&gt; &lt;p&gt;I did some calculations on OpenRouter margins for models like Qwen 3 Coder 480B parameter, and the charges for running the model is quite high on OpenRouter, especially when compared to running the model on a 8xB200 GPU system that can be rented for about 22 to 29 dollars an hour from DataCrunch.io. Without any model optimization and assuming fairly large input tokens of around 10k+ tokens input average, it‚Äôs about three to five times more expensive than it costs to run on a 8xB200 system. However if we use an optimized model, using the latest Nvidia paper, it‚Äôs about 5-10 times cheaper to run than the price listed assuming at least 75% average utilization of the system throughout the day. It costs quite a lot to optimize a model, even if we‚Äôre only use some of the optimizations in the paper. &lt;/p&gt; &lt;p&gt;My original thought was to create an inference provider on OpenRouter using the low hanging fruit optimizations from the paper to make a good profit, but I‚Äôm not that interested in making another business right now or making more money. However I figure if we pool our knowledge together, and our financial and GPU resources, we can do a light pass series of optimizations on the most common models, and offer inference to each other at a close to at cost rate, basically saving a large amount from the cost of OpenRouter. &lt;/p&gt; &lt;p&gt;What are your thoughts? &lt;/p&gt; &lt;p&gt;Here‚Äôs the paper for those that asked: &lt;a href="https://arxiv.org/pdf/2508.15884v1"&gt;https://arxiv.org/pdf/2508.15884v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Incident822"&gt; /u/Far-Incident822 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nalgur/create_a_shared_alternative_to_openrouter_together/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nalgur/create_a_shared_alternative_to_openrouter_together/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nalgur/create_a_shared_alternative_to_openrouter_together/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T05:56:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1na7zg7</id>
    <title>Silly-v0.2 is my new favorite model</title>
    <updated>2025-09-06T19:09:54+00:00</updated>
    <author>
      <name>/u/RandumbRedditor1000</name>
      <uri>https://old.reddit.com/user/RandumbRedditor1000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's not my model, I just feel like it's very underrated. it's the most fun I've had talking to an LLM, and it feels a lot like character AI. it has almost no GPT-isms and is very humanlike, and it's only 12b parameters, so it's insanely fast. it seems to work really well with character cards as well. I've been looking for a model like this for ages, and i'm glad we finally have one like it that's open-source&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandumbRedditor1000"&gt; /u/RandumbRedditor1000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na7zg7/sillyv02_is_my_new_favorite_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na7zg7/sillyv02_is_my_new_favorite_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na7zg7/sillyv02_is_my_new_favorite_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T19:09:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1napgwx</id>
    <title>Imagine an AI Coding Assistant CLI with Domain Expertise like Tech Leads and Vector Code Search like Crusor</title>
    <updated>2025-09-07T10:08:33+00:00</updated>
    <author>
      <name>/u/prabhjots665</name>
      <uri>https://old.reddit.com/user/prabhjots665</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1napgwx/imagine_an_ai_coding_assistant_cli_with_domain/"&gt; &lt;img alt="Imagine an AI Coding Assistant CLI with Domain Expertise like Tech Leads and Vector Code Search like Crusor" src="https://preview.redd.it/822tkwtuvpnf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d57de564e1e533aa28806e728440b57bef0c8bf2" title="Imagine an AI Coding Assistant CLI with Domain Expertise like Tech Leads and Vector Code Search like Crusor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever wished your AI coding assistant actually understood your team's domain knowledge and architectural decisions?&lt;/p&gt; &lt;p&gt;Just shipped &lt;strong&gt;Terra Code CLI&lt;/strong&gt; - the first AI assistant that learns your organization's patterns and works like a senior developer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What makes it different:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Interactive KT Sessions&lt;/strong&gt; - Senior devs teach Terra through structured knowledge transfer&lt;br /&gt; ‚Ä¢ &lt;strong&gt;Semantic Code Search&lt;/strong&gt; - Lightning-fast indexing of entire codebases for analysis&lt;br /&gt; ‚Ä¢ &lt;strong&gt;Persistent Memory&lt;/strong&gt; - Remembers team standards across all projects&lt;br /&gt; ‚Ä¢ &lt;strong&gt;Domain Expertise&lt;/strong&gt; - Upload architecture docs, API specs (.txt, .md, .docx, .pdf)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Built on Qwen's foundation&lt;/strong&gt; (thanks to the Qwen team!) + Gemini CLI framework.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try it free during beta:&lt;/strong&gt; &lt;code&gt;bash npm install -g @terra-code/terra-code@latest terra &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Which feature would most improve your coding workflow?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full domain knowledge integration&lt;/li&gt; &lt;li&gt;Semantic code search capabilities&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Persistent team memory&lt;/li&gt; &lt;li&gt;Interactive knowledge transfer&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Beta ending soon&lt;/strong&gt; - perfect time to onboard your team's knowledge!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question for the community:&lt;/strong&gt; Have you faced challenges with AI coding assistants lacking domain understanding? How did it impact your development process?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/TerraAGI/terra-code-cli"&gt;Star us on GitHub&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Website:&lt;/strong&gt; &lt;a href="https://terra-agi.com/"&gt;Visit our website&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;Built with ‚ù§Ô∏è by the TerraAGI team&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prabhjots665"&gt; /u/prabhjots665 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/822tkwtuvpnf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1napgwx/imagine_an_ai_coding_assistant_cli_with_domain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1napgwx/imagine_an_ai_coding_assistant_cli_with_domain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T10:08:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1na3u8w</id>
    <title>Built QWEN3-0.6B mini inference engine in CUDA from scratch</title>
    <updated>2025-09-06T16:25:42+00:00</updated>
    <author>
      <name>/u/yassa9</name>
      <uri>https://old.reddit.com/user/yassa9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3u8w/built_qwen306b_mini_inference_engine_in_cuda_from/"&gt; &lt;img alt="Built QWEN3-0.6B mini inference engine in CUDA from scratch" src="https://external-preview.redd.it/ZXpiNW9wenhra25mMbxpIYt75C5r2fT6cxhEXwgg3zD3iMqrP6b9J5eZI0o1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e8266ac29eb4dbc6dde7ee5d798cb3890af08d38" title="Built QWEN3-0.6B mini inference engine in CUDA from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm into CUDA and GPGPU programming much, didn't get into LLMs or NLP at all, so tried build that side project as as a hands-on way to learn about LLMs while practicing my CUDA programming.&lt;/p&gt; &lt;p&gt;chose that cute tiny model of qwen3-600m &lt;/p&gt; &lt;p&gt;Static configured, with suckless philosophy in code as much as possible, no deps to build beyond cuBLAS, CUB, std IO libs&lt;/p&gt; &lt;p&gt;I know that im missing smth but in benchmarking with greedy sampling (temp=0) on my RTX 3050, I get 3x speed of hf with flash-attn inference and extremely comparable speed with llama.cpp&lt;/p&gt; &lt;p&gt;My guess is the slight edge over llama.cpp comes from being hyper-specialized for just one model, allowing for more compile-time optimizations with no runtime branching.&lt;/p&gt; &lt;p&gt;feel free to check github if you want: &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/yassa9/qwen600"&gt;https://github.com/yassa9/qwen600&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yassa9"&gt; /u/yassa9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xh5qjozxkknf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3u8w/built_qwen306b_mini_inference_engine_in_cuda_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na3u8w/built_qwen306b_mini_inference_engine_in_cuda_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T16:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nake20</id>
    <title>Best for Coding</title>
    <updated>2025-09-07T04:53:16+00:00</updated>
    <author>
      <name>/u/johanna_75</name>
      <uri>https://old.reddit.com/user/johanna_75</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was reading the discussion about the pros and cons of K2 ‚Äì 0905, GLM 4.5, deepseek etc. I have used all of these although not extensively then I tried Qwen3-coder which seems so superior for any type of coding work. And yet I seldom see Qwen3-coder discussed or commented, is there some reason it is not popular?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johanna_75"&gt; /u/johanna_75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nake20/best_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nake20/best_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nake20/best_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T04:53:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1na9crp</id>
    <title>Nemotron Nano V2 models are remarkably good for agentic coding</title>
    <updated>2025-09-06T20:05:00+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use Nvidia Nemotron Nano 9B V2 and 12B v2 with Roo Coder served by both LM Studio (Mac) and Llama.cpp (Ubuntu). These models are small, fast, smart, follow instructions well, and are good at tool calling. Just an FYI for anyone with a smaller vram GPU - I can load up Q8 quants with 300k context and VRAM use is less than 24Gb. Great little models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na9crp/nemotron_nano_v2_models_are_remarkably_good_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na9crp/nemotron_nano_v2_models_are_remarkably_good_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na9crp/nemotron_nano_v2_models_are_remarkably_good_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T20:05:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nam7i1</id>
    <title>Best 100B class model/framework to run on 16 P100s (256GB of VRAM)?</title>
    <updated>2025-09-07T06:40:22+00:00</updated>
    <author>
      <name>/u/TooManyPascals</name>
      <uri>https://old.reddit.com/user/TooManyPascals</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve got 16√ó Tesla P100s (256 GB VRAM) and I‚Äôm trying to explore and find how to run 100B+ models with max context on Pascal cards. &lt;/p&gt; &lt;p&gt;See the machine: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ktiq99/i_accidentally_too_many_p100/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ktiq99/i_accidentally_too_many_p100/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;At the time, I had a rough time trying to get Qwen3 MoE models to work with Pascal, but maybe things have improved. &lt;/p&gt; &lt;p&gt;The two models at the top of my list are gpt-oss-120B and GLM-4.5-Air. For extended context I‚Äôd love to get one of the 235B Qwen3 models to work too. &lt;/p&gt; &lt;p&gt;I‚Äôve tried llama.cpp, Ollama, ExLlamaV2, and vllm-pascal. But none have handled MoE properly on this setup. So, if anyone has been able to run MoE models on P100s, I'd love to have some pointers. I‚Äôm open to anything. I‚Äôll report back with configs and numbers if I get something working.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TooManyPascals"&gt; /u/TooManyPascals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nam7i1/best_100b_class_modelframework_to_run_on_16_p100s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nam7i1/best_100b_class_modelframework_to_run_on_16_p100s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nam7i1/best_100b_class_modelframework_to_run_on_16_p100s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T06:40:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1naf93r</id>
    <title>2x MI50 32GB Quant Speed Comparison (Mistral 3.2 24B, llama.cpp, Vulkan)</title>
    <updated>2025-09-07T00:24:56+00:00</updated>
    <author>
      <name>/u/OUT_OF_HOST_MEMORY</name>
      <uri>https://old.reddit.com/user/OUT_OF_HOST_MEMORY</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naf93r/2x_mi50_32gb_quant_speed_comparison_mistral_32/"&gt; &lt;img alt="2x MI50 32GB Quant Speed Comparison (Mistral 3.2 24B, llama.cpp, Vulkan)" src="https://b.thumbs.redditmedia.com/FV3rVrLKjx3h5viB13673innMBOKrcScYojGFAgGn5g.jpg" title="2x MI50 32GB Quant Speed Comparison (Mistral 3.2 24B, llama.cpp, Vulkan)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All tests were run on the same system with 2x MI50 32GB from AliExpress, with a fixed VBios found on this subreddit. Llama.cpp was compiled with vulkan support as that is what I use for all of my GPUs regardless of vendor.&lt;/p&gt; &lt;p&gt;Quants for Mistral 3.2 Small 2506 24B were sourced from both Bartowski and Unsloth, when there were quants provided by both the values were averaged as I found that there was negligible difference in speed and size between the providers.&lt;/p&gt; &lt;p&gt;Every quant was run through 8 tests using llama-bench, with the variables in play being Flash Attention On/Off, Depth of either 0 or 32768, and the test type PP512 or TG128. Testing took approximately 62 hours to complete.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n2b2e0xvwmnf1.png?width=1255&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4a3d7a2ff32cbcca43de514b1a88a25fc3751fe"&gt;Chart 1: Prompt Processing in Tokens Per Second&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r0tltrr9xmnf1.png?width=1255&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9011470110b826a17a7e4b4e10d5f37c61bb2295"&gt;Chart 2: Token Generation in Tokens Per Second&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xmrwqghbxmnf1.png?width=1255&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ce8c383a27c8fd05e356a97851b49179b4e3703"&gt;Chart 3: Prompt Processing in GB x Tokens Per Second&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/apls9iqdxmnf1.png?width=1255&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14de7a426d9413cc331b35b6fdaf16fa6a76b320"&gt;Chart 4: Token Generation in GB x Tokens Per Second&lt;/a&gt;&lt;/p&gt; &lt;p&gt;An explanation of the charts:&lt;/p&gt; &lt;p&gt;Chart 1 and 2 are quite straight forward, they show the raw scores from the PP512 and TG128 test respectively, it clearly shows that there is a massive spike in prompt processing for Q4_0, Q4_1, Q8_0, UD-Q8_K_XL, and BF16 at low depths, which gradually equalizes once flash attention is enabled and as depth increases. On the other hand the Token generation graph shows a massive plummet for IQ4_XS.&lt;/p&gt; &lt;p&gt;Chart 3 and 4 are simply taking the values used for chart 1 and 2 and multiplying by the reported model size in llama-bench during the run. I only really ran this test since I have been slowly losing faith in quantization all together and am shifting towards using Q8_0 and BF16 models wherever possible and wanted to confirm my own biases with cherry picked statistics. The results are the same as before Q4_0, Q4_1, Q8_0, UD-Q8_K_XL and BF16 are the only real standouts.&lt;/p&gt; &lt;p&gt;TLDR - Q4_0, Q4_1, Q8_0, Q8_K_XL, BF16&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OUT_OF_HOST_MEMORY"&gt; /u/OUT_OF_HOST_MEMORY &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naf93r/2x_mi50_32gb_quant_speed_comparison_mistral_32/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naf93r/2x_mi50_32gb_quant_speed_comparison_mistral_32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naf93r/2x_mi50_32gb_quant_speed_comparison_mistral_32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T00:24:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nadq9u</id>
    <title>[Tool] Speakr v0.5.5: Self-hosted audio transcription app with LocalLLM support + new semantic search &amp; full internationalization</title>
    <updated>2025-09-06T23:13:24+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nadq9u/tool_speakr_v055_selfhosted_audio_transcription/"&gt; &lt;img alt="[Tool] Speakr v0.5.5: Self-hosted audio transcription app with LocalLLM support + new semantic search &amp;amp; full internationalization" src="https://a.thumbs.redditmedia.com/HKdME730ruEeKNuss1IdypHSurnN7BTEP90tbny6kM8.jpg" title="[Tool] Speakr v0.5.5: Self-hosted audio transcription app with LocalLLM support + new semantic search &amp;amp; full internationalization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Speakr v0.5.5 is out - a self-hosted app that connects to your local STT and LLM instances for transcription with speaker diarization and semantic search.&lt;/p&gt; &lt;p&gt;Inquire Mode (still experimental) uses the all-MiniLM-L6-v2 embedding model to allow semantic search over recordings. It works on CPU, creates 384d vectors, and synthesizes answers from your complete library, not just returning search hits. Ask &amp;quot;What deliverables were assigned to me in the TPS meeting?&amp;quot; and get actual narrative answers with citations. &lt;a href="https://murtaza-nasir.github.io/speakr/screenshots"&gt;Have a look at some screenshots&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Works with any OpenAI-compatible API (vLLM, LocalAI, LM Studio). Works with any Whisper endpoint, with an recommended ASR companion container for speaker diarization. Tag-based prompt customization allows you to customize summaries by domain - medical recordings get medical summaries, technical meetings get technical summaries.&lt;/p&gt; &lt;p&gt;What's new in this release: five-language UI support, automatic audio format conversion where necessary, air-gapped environment support, and rewritten documentation.&lt;/p&gt; &lt;p&gt;Everything stays local. No external API calls for embeddings or processing, unless you want to use external APIs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/murtaza-nasir/speakr"&gt;GitHub&lt;/a&gt; | &lt;a href="https://hub.docker.com/r/learnedmachine/speakr"&gt;Docker Hub&lt;/a&gt; | &lt;a href="https://murtaza-nasir.github.io/speakr/screenshots"&gt;Screenshots&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking for feedback on Inquire Mode. What features would improve your workflow?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nadq9u"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nadq9u/tool_speakr_v055_selfhosted_audio_transcription/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nadq9u/tool_speakr_v055_selfhosted_audio_transcription/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T23:13:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1naiud3</id>
    <title>Do local LLMs do almost as well with code generation as the big boys?</title>
    <updated>2025-09-07T03:27:39+00:00</updated>
    <author>
      <name>/u/Middle_Reception286</name>
      <uri>https://old.reddit.com/user/Middle_Reception286</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;Sort of a &amp;quot;startup&amp;quot; wears all hats person like many are these days with AI/LLM tools at our disposal.&lt;/p&gt; &lt;p&gt;I pay for the $200 month Anthropic plan because CC (cli mode) did quite well on some tasks, and I was always running out of context with the $20 plan and even the $100 plan. However, as many are starting to say on a few llm channels, it seems like it has gotten worse. Not sure how accurate that is or not. BUT.. that, the likely growing costs, and experimenting with taking the output of CC as input to ChatGPT5 and Gemini 2.5 Pro (using some credits I have left from playing with KiloCode before I switched to CC Max).. I have been seeing that what CC puts out is often a bunch of fluff. It says all these great things like &amp;quot;It's 100% working, its the best ever&amp;quot; and then I try to use my code and find out its mostly mock, fake or CC generated the values instead of actually ran some code and got results from the code running.&lt;/p&gt; &lt;p&gt;It got me thinking. The monthly costs to use 2 or 3 of these things starts to add up for those of us not lucky enough to be employed and/or a company paying for it. Myself, I am unemployed for almost 2 years now and decided I want to try to build my dream passion project that I have vetted with several colleagues and they are all agreeing it is much needed and could very well be very valuable. So I figure.. use AI + my experience/knowledge. I can't afford to hire a team, and frankly my buddy in India who runs a company to farm out works was looking at $5K a month per developer.. so yah.. that's like 6+ months of multiple AIs cost.. figured not worth it for one developer month of a likely &amp;quot;meh&amp;quot; coder that would require many months or more to build what I am now working on with AI.&lt;/p&gt; &lt;p&gt;SO.. per my subject (sorry had to add some context).. my thought is.. would it benefit me to run a local LLM like DeepSeek or Meta or Qwen 3.. but buying the hardware.. in this case it seems like the Mac M3 Studio Ultra (hoping they announce an M4 Studio Ultra in a few days) with 512GB RAM or even the lower cpu/256GB ram would be a good way to go. Before anyone says &amp;quot;Dude.. thats $6K to $10K depending on configuration.. that's a LOT of cloud AI you can afford&amp;quot;. My argument is that it seems like using Claude + ChatGPT + Gemini.. to bounce results between them is at least getting me a bit better code out of CC than CC is on its own. I have a few uses for running a local LLM for my products that I am working on, but I am wondering if running the larger models + much larger context windows will be a LOT better than using LM Studio on my desktop with 16GB of gpu VRAM. Is the results from these larger models + more context window going to be that much better? OR is it a matter of a few percentage points better? I read for example the FP16 is not any better than Q8 in terms of quality.. like literally about .1% or less better and not all the time. Given that open source models are getting better all the time, free to download/use, I am really curious if they could be coerced with the right prompting to put code out as good as claude code or ChatGPT 5 or Gemini 2.5Pro if I had a larger 200GB to 400GB model and 1mil+ context window. &lt;/p&gt; &lt;p&gt;I've seen some bits of info on this topic.. that yes they can be every bit as good or they are not as good because the big 3 (or so) have TBs of model size and massive amounts of hardware ($billions).. so of course a $5K to $10K Studio + OS large model may not be as good.. but is it good enough that you could rely on it to do initial ideas/draft code, then feed that code to Claude, ChatGPT, Gemini. &lt;/p&gt; &lt;p&gt;But the bigger ask is.. do you basically get really good overall quality code if you use multiple models against each other.. or.. working together. Like giving the prompt to local LLM. Generate a bunch of code. Then feed the project to ChatGPT. Have it come back with some response. Then tell Claude (this is what ChatGPT and my DeepSeek said.. what do you think..) and so on. My hope is some sort of &amp;quot;cross response&amp;quot; between them results in one of them (ideally local would be great to avoid cloud costs) coming up with great quality code that mostly works.&lt;/p&gt; &lt;p&gt;I do realize I have to review/test code.. I am not relying on the generated stuff 100%. However, I am working in a few languages two of which I know jack shit about, three of which I know a little bit of and 2 I know very well. So I am sort of relying on the knowledge of AI for most of this stuff and applying my experience/knowledge to try to re-prompt to get better results. &lt;/p&gt; &lt;p&gt;Maybe it's all wishful thinking. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Middle_Reception286"&gt; /u/Middle_Reception286 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naiud3/do_local_llms_do_almost_as_well_with_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naiud3/do_local_llms_do_almost_as_well_with_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naiud3/do_local_llms_do_almost_as_well_with_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T03:27:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1naqln5</id>
    <title>How is qwen3 4b this good?</title>
    <updated>2025-09-07T11:18:38+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naqln5/how_is_qwen3_4b_this_good/"&gt; &lt;img alt="How is qwen3 4b this good?" src="https://b.thumbs.redditmedia.com/iayFtcVrbsCZAlrIPv-683BX53HPUAlfD1bIlFDeLGo.jpg" title="How is qwen3 4b this good?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This model is on a different level. The only models which can beat it are 6 to 8 times larger. I am very impressed. It even Beats all models in the &amp;quot;small&amp;quot; range in Maths (AIME 2025).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1naqln5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naqln5/how_is_qwen3_4b_this_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naqln5/how_is_qwen3_4b_this_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T11:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1napq0m</id>
    <title>check https://huggingface.co/papers/2509.01363</title>
    <updated>2025-09-07T10:24:40+00:00</updated>
    <author>
      <name>/u/LowChance4561</name>
      <uri>https://old.reddit.com/user/LowChance4561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The paper shows that reasoning ability can be extracted as a vector from RL-trained models and added to others via simple arithmetic to boost reasoning without retraining&lt;br /&gt; would appreciate an upvote &lt;a href="https://huggingface.co/papers/2509.01363"&gt;https://huggingface.co/papers/2509.01363&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LowChance4561"&gt; /u/LowChance4561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1napq0m/check_httpshuggingfacecopapers250901363/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1napq0m/check_httpshuggingfacecopapers250901363/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1napq0m/check_httpshuggingfacecopapers250901363/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T10:24:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nabcek</id>
    <title>Anyone actully try to run gpt-oss-120b (or 20b) on a Ryzen AI Max+ 395?</title>
    <updated>2025-09-06T21:28:33+00:00</updated>
    <author>
      <name>/u/PM_ME_YOUR_PROOFS</name>
      <uri>https://old.reddit.com/user/PM_ME_YOUR_PROOFS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AMD is understandably&lt;a href="https://www.amd.com/en/blogs/2025/how-to-run-openai-gpt-oss-20b-120b-models-on-amd-ryzen-ai-radeon.html"&gt; trying to tout this&lt;/a&gt; and and there's this from a month a go claiming &amp;quot;30 tokens per second&amp;quot; (not clear if 120b or 20b). I can't tell if the flops are int8 flops of bf16 or fp16 on the 395. In theory if we assume the 395 has 50 tops of bf16 on its NPU and we trust their &lt;a href="https://www.amd.com/en/products/processors/laptop/ryzen/ai-300-series/amd-ryzen-ai-max-plus-395.html"&gt;&amp;quot;overall TOPS&amp;quot;&lt;/a&gt; its potentially pushing into 3090 territory under ideal conditions. It has *waaay* more memory which is super useful for getting things to run at all but it also has a lot less memory bandwidth about 1/4th as much. I guess a more fair comparison would be on 20b. I'd strong anticipate the 3090 getting better tokens per second on 20b.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/ryzen/comments/1lzr7yq/yolov8_multimachine_benchmark_rtx_3090_vs_ryzen/"&gt;this post &lt;/a&gt;suggests that actually under common configs a lot of times the 395 can beat the 3090...this is very surprising to me. Curious if anyone has actually tried 20b on both and can compare. Also curious what actual tokens per second people are getting with 120b.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PM_ME_YOUR_PROOFS"&gt; /u/PM_ME_YOUR_PROOFS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nabcek/anyone_actully_try_to_run_gptoss120b_or_20b_on_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nabcek/anyone_actully_try_to_run_gptoss120b_or_20b_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nabcek/anyone_actully_try_to_run_gptoss120b_or_20b_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T21:28:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nan5az</id>
    <title>I managed to compile and run Llama 3B Q4_K_M on llama.cpp with Termux on ARMv7a, using only 2 GB.</title>
    <updated>2025-09-07T07:38:21+00:00</updated>
    <author>
      <name>/u/arbolito_mr</name>
      <uri>https://old.reddit.com/user/arbolito_mr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nan5az/i_managed_to_compile_and_run_llama_3b_q4_k_m_on/"&gt; &lt;img alt="I managed to compile and run Llama 3B Q4_K_M on llama.cpp with Termux on ARMv7a, using only 2 GB." src="https://b.thumbs.redditmedia.com/sv68yAoEG_zuCAcmLHS29J0tj-6x4vmOQwMWqwEnnMQ.jpg" title="I managed to compile and run Llama 3B Q4_K_M on llama.cpp with Termux on ARMv7a, using only 2 GB." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used to think running a reasonably coherent model on Android ARMv7a was impossible, but a few days ago I decided to put it to the test with llama.cpp, and I was genuinely impressed with how well it works. It's not something you can demand too much from, but being local and, of course, offline, it can get you out of tricky situations more than once. The model weighs around 2 GB and occupies roughly the same amount in RAM, although with certain flags it can be optimized to reduce consumption by up to 1 GB. It can also be integrated into personal Android projects thanks to its server functionality and the endpoints it provides for sending requests.&lt;/p&gt; &lt;p&gt;If anyone thinks this could be useful, let me know; as soon as I can, I‚Äôll prepare a complete step-by-step guide, especially aimed at those who don‚Äôt have a powerful enough device to run large models or rely on a 32-bit processor.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arbolito_mr"&gt; /u/arbolito_mr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nan5az"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nan5az/i_managed_to_compile_and_run_llama_3b_q4_k_m_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nan5az/i_managed_to_compile_and_run_llama_3b_q4_k_m_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T07:38:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1na7c1b</id>
    <title>OpenAI: Why Language Models Hallucinate</title>
    <updated>2025-09-06T18:44:12+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In short: LLMs hallucinate because we've inadvertently designed the training and evaluation process to reward confident, even if incorrect, answers, rather than honest admissions of uncertainty. Fixing this requires a shift in how we grade these systems to steer them towards more trustworthy behavior. &lt;/p&gt; &lt;p&gt;The Solution:&lt;/p&gt; &lt;p&gt;Explicitly stating &amp;quot;confidence targets&amp;quot; in evaluation instructions, where mistakes are penalized and admitting uncertainty (IDK) might receive 0 points, but guessing incorrectly receives a negative score. This encourages &amp;quot;behavioral calibration,&amp;quot; where the model only answers if it's sufficiently confident.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://share.google/9SKn7X0YThlmnkZ9m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na7c1b/openai_why_language_models_hallucinate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na7c1b/openai_why_language_models_hallucinate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T18:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nai7rf</id>
    <title>Why isn't there a local tool server that replicates most of the tools avaliable on ChatGPT?</title>
    <updated>2025-09-07T02:53:58+00:00</updated>
    <author>
      <name>/u/gigaflops_</name>
      <uri>https://old.reddit.com/user/gigaflops_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've made it to the point where mid-sized local LLMs can rival some cloud models in some use cases, but it feels like the local tool ecosystem is still years behind. It's a shame because models like gpt-oss-120b are pretty competent at &lt;em&gt;using&lt;/em&gt; tools that it is given access to.&lt;/p&gt; &lt;p&gt;A small, but not-insignificant fraction of all LLM prompts in most domains &lt;em&gt;need&lt;/em&gt; tools. Web search for up to date information, python interpreter for data analysis and moderately complex calculations, date and time access, and the ability to leverage an image-gen model all &amp;quot;just work&amp;quot; on ChatGPT. Even if I could run the GPT-5 model locally on my PC, it could never be usable for me without the tools.&lt;/p&gt; &lt;p&gt;In the local space, a quick search for MCP tool servers yields a fragmented ecosystem servers that do &lt;em&gt;one&lt;/em&gt; thing, often highly specialized, like analyze a github codebase or read your google calendar. You can't come close to replicating the &lt;em&gt;basic&lt;/em&gt; functionality of ChatGPT like web search and calculator without downloading 5+ servers using the command line or github (RIP beginners) and learning how to use docker or writing some master server to proxys them all into one.&lt;/p&gt; &lt;p&gt;Maybe I'm not looking in the right places, but it seems like people are only interested in using cloud tool servers (often with an API cost) with their local LLM, something that defeats the purpose imo. Even the new version of ollama runs the web search tool from the cloud instead of querying from the local machine. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gigaflops_"&gt; /u/gigaflops_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nai7rf/why_isnt_there_a_local_tool_server_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nai7rf/why_isnt_there_a_local_tool_server_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nai7rf/why_isnt_there_a_local_tool_server_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T02:53:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1na3f1s</id>
    <title>Renting GPUs is hilariously cheap</title>
    <updated>2025-09-06T16:08:44+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3f1s/renting_gpus_is_hilariously_cheap/"&gt; &lt;img alt="Renting GPUs is hilariously cheap" src="https://preview.redd.it/dhtzimf7jknf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1bca94832d9e6b8fb7b8faf80d61387d12889d7f" title="Renting GPUs is hilariously cheap" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A 140 GB monster GPU that costs $30k to buy, plus the rest of the system, plus electricity, plus maintenance, plus a multi-Gbps uplink, for a little over 2 bucks per hour.&lt;/p&gt; &lt;p&gt;If you use it for 5 hours per day, 7 days per week, and factor in auxiliary costs and interest rates, buying that GPU today vs. renting it when you need it will only pay off in 2035 or later. That‚Äôs a tough sell.&lt;/p&gt; &lt;p&gt;Owning a GPU is great for privacy and control, and obviously, many people who have such GPUs run them nearly around the clock, but for quick experiments, renting is often the best option.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dhtzimf7jknf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3f1s/renting_gpus_is_hilariously_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na3f1s/renting_gpus_is_hilariously_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T16:08:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1namz1q</id>
    <title>HF releases 3T tokens dataset sourced entirely from PDFs.</title>
    <updated>2025-09-07T07:26:55+00:00</updated>
    <author>
      <name>/u/Other_Housing8453</name>
      <uri>https://old.reddit.com/user/Other_Housing8453</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guy, something we have teased a bit during our AMA is finally out: &lt;/p&gt; &lt;p&gt;üìÑ FinePDFs, the largest PDF dataset ever released, spanning over half a billion documents!&lt;/p&gt; &lt;p&gt;- Long context: Documents are 2x longer than web text&lt;/p&gt; &lt;p&gt;- 3T tokens from high-demand domains like legal and science.&lt;/p&gt; &lt;p&gt;- Heavily improves over SoTA when mixed with FW-EDU&amp;amp;DCLM web copora üìà.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Other_Housing8453"&gt; /u/Other_Housing8453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1namz1q/hf_releases_3t_tokens_dataset_sourced_entirely/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1namz1q/hf_releases_3t_tokens_dataset_sourced_entirely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1namz1q/hf_releases_3t_tokens_dataset_sourced_entirely/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T07:26:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7j5z2</id>
    <title>Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)</title>
    <updated>2025-09-03T16:14:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt; &lt;img alt="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" src="https://preview.redd.it/wdx4ivdw3zmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=876855c03867ead70389d15b60f24b91d478f835" title="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wdx4ivdw3zmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T16:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8c3l2</id>
    <title>AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more.</title>
    <updated>2025-09-04T14:43:01+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt; &lt;img alt="AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more." src="https://external-preview.redd.it/y8IJElEOEd_2568MHNUZQsP7_aRTCAzyzXUKpDJwl1Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e377887ea8d7eae841499cc497b90b82aa97816" title="AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're super excited to do this AMA. Come ask your questions to the researchers behind &lt;strong&gt;SmolLM, SmolVLM, FineWeb&lt;/strong&gt;, and more. You can learn more about our work at &lt;a href="http://hf.co/science"&gt;hf.co/science&lt;/a&gt; ü§ó&lt;/p&gt; &lt;p&gt;If you want to get started in ML, a good place is &lt;a href="https://hf.co/learn"&gt;https://hf.co/learn&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To celebrate the AMA, we release a new &lt;strong&gt;FineVision&lt;/strong&gt; dataset, check it out! &lt;a href="https://huggingface.co/datasets/HuggingFaceM4/FineVision"&gt;https://huggingface.co/datasets/HuggingFaceM4/FineVision&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our participants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/eliebak"&gt;Elie Bakouch&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/eliebakk"&gt;u/eliebakk&lt;/a&gt; (SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/loubnabnl"&gt;Loubna Ben Allal&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/loubnabnl"&gt;u/loubnabnl&lt;/a&gt; (SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nouamanetazi"&gt;Nouamane Tazi&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/Norlax"&gt;u/Norlax&lt;/a&gt;_42 (Nanotron/SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lvwerra"&gt;Leandro von Werra&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/lvwerra"&gt;u/lvwerra&lt;/a&gt; (Head of Research)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/edbeeching"&gt;Edward Beeching&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/edbeeching"&gt;u/edbeeching&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/cmpatino"&gt;Carlos Miguel Pati√±o&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/cmpatino"&gt;u/cmpatino&lt;/a&gt;_ (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kashif"&gt;Kashif Rasul&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/krasul"&gt;u/krasul&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lewtun"&gt;Lewis Tunstall&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/lewtun"&gt;u/lewtun&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/qgallouedec"&gt;Quentin Gallou√©dec&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/qgallouedec"&gt;u/qgallouedec&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/clefourrier"&gt;Cl√©mentine Fourrier&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/clefourrier"&gt;u/clefourrier&lt;/a&gt; (Eval)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/SaylorTwift"&gt;Nathan Habib&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/HauntingMoment"&gt;u/HauntingMoment&lt;/a&gt; (Eval)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lusxvr"&gt;Luis Wiedmann&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/luswd"&gt;u/luswd&lt;/a&gt; (Multimodal)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/andito"&gt;Andres Marafioti&lt;/a&gt;, &lt;a href="/u/futterneid"&gt;u/futterneid&lt;/a&gt; (Multimodal)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/guipenedo"&gt;Guilherme Penedo&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/PhilipsNostrum"&gt;u/PhilipsNostrum&lt;/a&gt; (Data)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/hynky"&gt;Hynek Kydl√≠ƒçek&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/Other"&gt;u/Other&lt;/a&gt;_Housing8453 (Data)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/reach-vb"&gt;Vaibhav Srivastav,&lt;/a&gt; &lt;a href="/u/vaibhavs10"&gt;u/vaibhavs10&lt;/a&gt; (Head of Developer Experience and Community)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/BrigitteTousi"&gt;Brigitte Tousignant&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/BriggieSmalls1992"&gt;u/BriggieSmalls1992&lt;/a&gt; (Comms)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Xenova"&gt;Xenova&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/xenovatech"&gt;u/xenovatech&lt;/a&gt; (Transformers.js)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/craffel"&gt;Colin Raffel&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/craffel"&gt;u/craffel&lt;/a&gt; (Research)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ngxson"&gt;Xuan Son Nguyen&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/MediocreProgrammer99"&gt;u/MediocreProgrammer99&lt;/a&gt; (llama.cpp)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you are passionate about open source and open science like us, apply at &lt;a href="https://hf.co/jobs"&gt;https://hf.co/jobs&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Hugging Face team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o6moshv0u5nf1.png?width=2013&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee6a9392c3da8651e8a1425264ed855a51b69135"&gt;https://preview.redd.it/o6moshv0u5nf1.png?width=2013&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee6a9392c3da8651e8a1425264ed855a51b69135&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended but we will still answer question async for the next 24h. Follow our &lt;a href="https://hf.co/science"&gt;Hugging Face Science Org&lt;/a&gt; to be aware of our latest release! ü§ó&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T14:43:01+00:00</published>
  </entry>
</feed>
