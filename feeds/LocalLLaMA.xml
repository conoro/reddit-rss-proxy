<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-02T13:24:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1olsliw</id>
    <title>MiniMax-M2-exl3 - now with CatBench‚Ñ¢</title>
    <updated>2025-11-01T16:45:44+00:00</updated>
    <author>
      <name>/u/Unstable_Llama</name>
      <uri>https://old.reddit.com/user/Unstable_Llama</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olsliw/minimaxm2exl3_now_with_catbench/"&gt; &lt;img alt="MiniMax-M2-exl3 - now with CatBench‚Ñ¢" src="https://b.thumbs.redditmedia.com/hZFUwp417AAuD17RUalBhIzjpyePeIINpbOg8MJu3Xo.jpg" title="MiniMax-M2-exl3 - now with CatBench‚Ñ¢" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/turboderp/MiniMax-M2-exl3"&gt;https://huggingface.co/turboderp/MiniMax-M2-exl3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è Requires ExLlamaV3 v0.0.12&lt;/p&gt; &lt;p&gt;Use the optimized quants if you can fit them!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3hxgebenboyf1.jpg?width=836&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=339786475037e8109c89de298db8c14dfd6bbb45"&gt;https://preview.redd.it/3hxgebenboyf1.jpg?width=836&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=339786475037e8109c89de298db8c14dfd6bbb45&lt;/a&gt;&lt;/p&gt; &lt;p&gt;True AGI will make the best cat memes. You'll see it here first ;)&lt;/p&gt; &lt;p&gt;Exllama discord: &lt;a href="https://discord.gg/GJmQsU7T"&gt;https://discord.gg/GJmQsU7T&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unstable_Llama"&gt; /u/Unstable_Llama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olsliw/minimaxm2exl3_now_with_catbench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olsliw/minimaxm2exl3_now_with_catbench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olsliw/minimaxm2exl3_now_with_catbench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T16:45:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1omfalb</id>
    <title>Setup for fine-tuning for a 65k budget</title>
    <updated>2025-11-02T11:51:00+00:00</updated>
    <author>
      <name>/u/oh_my_right_leg</name>
      <uri>https://old.reddit.com/user/oh_my_right_leg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, my previous company is expecting to receive around $65k with the purpose of buying some AI infrastructure. I promised I'll help them with this, and after some searching, I found two candidates for the GPUs: the RTX 6000 Pro Blackwell and the H200. If they are planning to do fine-tuning(14-32B models dense or higher if sparse) and inference (for general purpose agents and agentic coding, less than 10 Concurrent users), what would be the better option between 4x 6000 Pro (did their price drop recently? Then maybe 5x?) or 1x H200 (maybe 2x, but due to price, that's unlikely) for that use case? Thanks for any recommendations&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oh_my_right_leg"&gt; /u/oh_my_right_leg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omfalb/setup_for_finetuning_for_a_65k_budget/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omfalb/setup_for_finetuning_for_a_65k_budget/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omfalb/setup_for_finetuning_for_a_65k_budget/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T11:51:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1om7ccz</id>
    <title>If I want to train, fine tune, and do image gen then... DGX Spark?</title>
    <updated>2025-11-02T03:37:51+00:00</updated>
    <author>
      <name>/u/MontageKapalua6302</name>
      <uri>https://old.reddit.com/user/MontageKapalua6302</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I want to train, fine tune, and do image gen, then do those reasons make the DGX Spark and clones worthwhile?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;From what I've heard on the positive:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Diffusion performance is strong.&lt;/p&gt; &lt;p&gt;MXFP4 performance is strong and doesn't make much of a quality hit.&lt;/p&gt; &lt;p&gt;Training performance is strong compared to the Strix Halo.&lt;/p&gt; &lt;p&gt;I can put two together to get 256 GB of memory and get significantly better performance as well as fit larger models or, more importantly, train larger models than I could with, say, Strix Halo or a 6000 Pro. Even if it's too slow or memory constrained for a larger model, I can proof of concept it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;More specifically what I want to do (in order of importance):&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Fine tune (or train?) a model for niche text editing, using &amp;lt;5 GB of training data. Too much to fit into context by far. Start with a single machine and a smaller model. If that works well enough, buy another or rent time on a big machine, though I'm loathe to put my life's work on somebody else's computer. Then run that model on the DGX or another machine, depending on performance. Hopefully have enough space &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Image generation and editing for fun without annoying censorship. I keep asking for innocuous things, and I keep getting denied by online generators.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Play around with drone AI training.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I don't want to game, use Windows, or do anything else with the box. Except for the above needs, I don't care if it's on the CUDA stack. I own NVIDIA, AMD, and Apple hardware. I am agnostic towards these companies.&lt;/p&gt; &lt;p&gt;I can also wait for the M5 Ultra, but that could be more than a year away.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MontageKapalua6302"&gt; /u/MontageKapalua6302 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om7ccz/if_i_want_to_train_fine_tune_and_do_image_gen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om7ccz/if_i_want_to_train_fine_tune_and_do_image_gen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om7ccz/if_i_want_to_train_fine_tune_and_do_image_gen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T03:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1om7s1y</id>
    <title>What am I doing wrong with GPT-OSS 120b on 2x 7900 XT w/ 128GB DDR5?</title>
    <updated>2025-11-02T04:02:44+00:00</updated>
    <author>
      <name>/u/InfinityApproach</name>
      <uri>https://old.reddit.com/user/InfinityApproach</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've often run across numbers like the attached on GPT-OSS 120b. Despite me having 40GB of VRAM, I cannot get any faster than 350 t/s pp and 30 t/s tg. Yet a system with only 12GB of VRAM is getting 25 tg! What am I doing wrong?&lt;/p&gt; &lt;p&gt;Here's the best settings I've found: &lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-bench -m &amp;quot;F:\LLMs\unsloth\gpt-oss-120b-GGUF\gpt-oss-120b-Q4_K_S-00001-of-00002.gguf&amp;quot; -fa 1 -ngl 999 -ncmoe 16 -ub 4096 -mmp 0 -mg 0 -ts &amp;quot;0.65;0.35&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;-ncmoe 16&amp;quot; is the sweet spot for offloading moe layers to my two GPUs&lt;/li&gt; &lt;li&gt;I'm doing a tensor split of 0.65;0.35 to account for my primary GPU having less usable VRAM because of the Windows desktop. Both GPUs are loaded to just under 20GB.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Win 11&lt;/li&gt; &lt;li&gt;Ryzen 7900x&lt;/li&gt; &lt;li&gt;128 GB DDR5 @ 6000, two sticks of 64GB&lt;/li&gt; &lt;li&gt;2x Radeon 7900xt GPUs, 20GB each&lt;/li&gt; &lt;li&gt;Latest Radeon PRO drivers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here's the best I can muster after lots of tinkering:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;ggml_vulkan: Found 2 Vulkan devices:&lt;/p&gt; &lt;p&gt;ggml_vulkan: 0 = AMD Radeon RX 7900 XT (AMD proprietary driver) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 32768 | int dot: 1 | matrix cores: KHR_coopmat&lt;/p&gt; &lt;p&gt;ggml_vulkan: 1 = AMD Radeon RX 7900 XT (AMD proprietary driver) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 32768 | int dot: 1 | matrix cores: KHR_coopmat&lt;/p&gt; &lt;p&gt;| model | size | params | backend | ngl | n_ubatch | fa | ts | mmap | test | t/s |&lt;/p&gt; &lt;p&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | ------------ | ---: | --------------: | -------------------: |&lt;/p&gt; &lt;p&gt;| gpt-oss 120B Q4_K - Small | 58.44 GiB | 116.83 B | Vulkan | 999 | 4096 | 1 | 0.65/0.35 | 0 | pp512 | 346.71 ¬± 3.42 |&lt;/p&gt; &lt;p&gt;| gpt-oss 120B Q4_K - Small | 58.44 GiB | 116.83 B | Vulkan | 999 | 4096 | 1 | 0.65/0.35 | 0 | tg128 | 29.98 ¬± 0.49 |&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Other details:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I've found that Vulkan is better than ROCM on my system&lt;/li&gt; &lt;li&gt;When I use a single GPU with 12 layers (maximizing 20GB VRAM), the best I can get is 12 t/s tg. That's compared to a single 4070 TI getting 25 tg.&lt;/li&gt; &lt;li&gt;On LM Studio, which doesn't allow me to tensor-split or offload 16 moe layers, the best I can do is load 20 layers and get 19 t/s tg.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Am I right that these numbers are low for my hardware? What settings should I change to speed it up?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InfinityApproach"&gt; /u/InfinityApproach &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalAIServers/comments/1oj6j9q/gptoss120b_2x_mi50_32gb_update_now_optimized_on/nmi37cn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om7s1y/what_am_i_doing_wrong_with_gptoss_120b_on_2x_7900/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om7s1y/what_am_i_doing_wrong_with_gptoss_120b_on_2x_7900/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T04:02:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1omggeh</id>
    <title>Are AI Agents just another tech trend or the next logical step in computing?</title>
    <updated>2025-11-02T12:50:55+00:00</updated>
    <author>
      <name>/u/purellmagents</name>
      <uri>https://old.reddit.com/user/purellmagents</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some days ago, I shared a post here about building AI Agents from scratch. It got a lot of attention, but I noticed something in the comments:&lt;/p&gt; &lt;p&gt;Many people still think ‚Äúagents‚Äù are just another temporary LLM gimmick. I wrote a short essay explaining why I believe AI Agents are not a passing fad, but the next logical evolution in the history of computing, an idea that started long before LLMs.&lt;/p&gt; &lt;p&gt;Since Alan Turing asked in 1950 whether machines can think, the form of those machines has changed constantly - but the underlying idea hasn‚Äôt. Turing‚Äôs famous ‚ÄúImitation Game‚Äù wasn‚Äôt just a test of deception; it was the first description of an intelligent system acting toward a goal. In modern terms, it was the first definition of an agent: something that perceives, decides, and acts.&lt;/p&gt; &lt;p&gt;Every generation of artificial intelligence has built on this same foundation:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In the 1950s, symbolic logic systems tried to reproduce reasoning.&lt;/li&gt; &lt;li&gt;In the 1980s, robotics introduced perception and action.&lt;/li&gt; &lt;li&gt;In the 2010s, deep learning made learning from data scalable.&lt;/li&gt; &lt;li&gt;In the 2020s, LLMs added language and flexible reasoning.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Agents now combine all of these. They don‚Äôt just respond, they act. They can perceive through APIs, decide through reasoning, and perform through tools. They are not tied to one technology or model; they are the structure that organizes intelligence itself.&lt;/p&gt; &lt;p&gt;Large Language Models are one layer in this progression. They give today‚Äôs agents a powerful form of perception and reasoning, but the agent idea existed long before them and will outlive them too. If LLMs fade, new architectures will replace them and agents will simply adapt, because their purpose remains the same: systems that pursue goals autonomously.&lt;/p&gt; &lt;p&gt;This is why I believe AI Agents are not a trend. They represent a shift from models that answer questions to systems that take action, a shift from computation to behavior. The agent concept isn‚Äôt hype; it‚Äôs the operating system of machine intelligence.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purellmagents"&gt; /u/purellmagents &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omggeh/are_ai_agents_just_another_tech_trend_or_the_next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omggeh/are_ai_agents_just_another_tech_trend_or_the_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omggeh/are_ai_agents_just_another_tech_trend_or_the_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T12:50:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1olkx65</id>
    <title>Gaming PC converted to AI Workstation</title>
    <updated>2025-11-01T10:56:09+00:00</updated>
    <author>
      <name>/u/highdefw</name>
      <uri>https://old.reddit.com/user/highdefw</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olkx65/gaming_pc_converted_to_ai_workstation/"&gt; &lt;img alt="Gaming PC converted to AI Workstation" src="https://preview.redd.it/z55xgdghmmyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58f471128678bd5884598fa5d1393664c1759fe5" title="Gaming PC converted to AI Workstation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RTX Pro 5000 and 4000 just arrived. NVME expansion slot on the bottom. 5950x with 128gb ram. Future upgrade will be a cpu upgrade.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/highdefw"&gt; /u/highdefw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z55xgdghmmyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olkx65/gaming_pc_converted_to_ai_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olkx65/gaming_pc_converted_to_ai_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T10:56:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1omgnvs</id>
    <title>Hlp please to find a good LLM</title>
    <updated>2025-11-02T13:00:41+00:00</updated>
    <author>
      <name>/u/korino11</name>
      <uri>https://old.reddit.com/user/korino11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt; I have tried Claude and M2+ GLM 4.6. I am disappointing because ALWAYS M2 in rust code implement placeholders but not real functions, it always trying to avoid execution and searching every method how to simplify tasks. Even when prompts have a strong a clear rules that it doesnt allow to do! he always ruin the code. I have a project in a high end math and physics,,, it always lieng like cloude.. very similar behavior. M2 and Claude always making simplifying and placeholders...and doesnt wanna resolve code and write full implementations/ My project about quantum simulations. I have got a clear concept with formulas and just need to imlement it correct! GPT5 doesnt wanna do this also, becouse he have some filters.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/korino11"&gt; /u/korino11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omgnvs/hlp_please_to_find_a_good_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omgnvs/hlp_please_to_find_a_good_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omgnvs/hlp_please_to_find_a_good_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T13:00:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1omd8pc</id>
    <title>Help me decide: EPYC 7532 128GB + 2 x 3080 20GB vs GMtec EVO-X2</title>
    <updated>2025-11-02T09:44:10+00:00</updated>
    <author>
      <name>/u/fukisan</name>
      <uri>https://old.reddit.com/user/fukisan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;I'd really appreciate some advice please.&lt;/p&gt; &lt;p&gt;I'm looking to do a bit more than my 6800xt + 5900x 32GB build can handle, and have been thinking of selling two 3900x machines I've been using as Linux servers (can probably get at least $250 for each machine).&lt;/p&gt; &lt;p&gt;I'd like to be able to run larger models and do some faster video + image generation via comfyui. I know RTX 3090 is recommended, but around me they usually sell for $900, and supply is short.&lt;/p&gt; &lt;p&gt;After doing sums it looks like I have the following options for under $2,300:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option 1: Server build = $2250&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;HUANANZHI H12D 8D&lt;/p&gt; &lt;p&gt;EPYC 7532&lt;/p&gt; &lt;p&gt;4 x 32GB 3200 SK Hynix&lt;/p&gt; &lt;p&gt;RTX 3080 20GB x 2&lt;/p&gt; &lt;p&gt;Cooler + PSU + 2TB nvme&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option 2: GMtec EVO-X2 = $2050&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;128GB RAM and 2TB storage&lt;/p&gt; &lt;p&gt;Pros with option 1 are I can sell the 3900x machines (making it cheaper overall) and have more room to expand RAM and VRAM in future if I need, plus I can turn this into a proper server (e.g. proxmox). Cons are higher power bills, more time to setup and debug, needs to be stored in the server closet, probably will be louder than existing devices in closet, and there's the potential for issues given used parts and modifications to 3080.&lt;/p&gt; &lt;p&gt;Pros with option 2 are lower upfront cost, less time setting up and debugging, can be out in the living room hooked up to the TV, and lower power costs. Cons are potential for slower performance, no upgrade path, and probably need to retain 3900x servers.&lt;/p&gt; &lt;p&gt;I have no idea how these compare inference performance wise - perhaps image and video generation will be quicker on option 1, but the GPT-OSS-120b, Qwen3 (32B VL, Coder and normal) and Seed-OSS-36B models I'd be looking to run seem like they'd perform much the same?&lt;/p&gt; &lt;p&gt;What would you recommend I do?&lt;/p&gt; &lt;p&gt;Thanks for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fukisan"&gt; /u/fukisan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omd8pc/help_me_decide_epyc_7532_128gb_2_x_3080_20gb_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omd8pc/help_me_decide_epyc_7532_128gb_2_x_3080_20gb_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omd8pc/help_me_decide_epyc_7532_128gb_2_x_3080_20gb_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T09:44:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1om2nqy</id>
    <title>[P] Training Better LLMs with 30% Less Data ‚Äì Entropy-Based Data Distillation</title>
    <updated>2025-11-01T23:43:09+00:00</updated>
    <author>
      <name>/u/Jolly-Act9349</name>
      <uri>https://old.reddit.com/user/Jolly-Act9349</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om2nqy/p_training_better_llms_with_30_less_data/"&gt; &lt;img alt="[P] Training Better LLMs with 30% Less Data ‚Äì Entropy-Based Data Distillation" src="https://a.thumbs.redditmedia.com/FvGLkMq1HBa1qbDFZdnwVjHyXma7vwQ3uhUXFySyDr4.jpg" title="[P] Training Better LLMs with 30% Less Data ‚Äì Entropy-Based Data Distillation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with data-efficient LLM training as part of a project I'm calling &lt;strong&gt;Oren&lt;/strong&gt;, focused on entropy-based dataset filtering.&lt;/p&gt; &lt;p&gt;The philosophy behind this emerged from knowledge distillation pipelines, where student models basically inherit the same limitations of intelligence as the teacher models have. Thus, the goal of Oren is to change LLM training completely ‚Äì from the current frontier approach of rapidly upscaling in compute costs and GPU hours to a new strategy: optimizing training datasets for smaller, smarter models.&lt;/p&gt; &lt;p&gt;The experimentation setup: two identical 100M-parameter language models. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model A:&lt;/strong&gt; trained on 700M raw tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model B:&lt;/strong&gt; trained on the top 70% of samples (500M tokens) selected via entropy-based filtering&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt; Model B matched Model A in performance, while using 30% less data, time, and compute. No architecture or hyperparameter changes.&lt;/p&gt; &lt;p&gt;Open-source models:&lt;/p&gt; &lt;p&gt;ü§ó &lt;a href="https://huggingface.co/vitalune/nanochat-d10-raw-700m"&gt;Model A - Raw (700M tokens)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ó &lt;a href="https://huggingface.co/vitalune/nanochat-d10-filtered-500m"&gt;Model B - Filtered (500M tokens)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd love feedback, especially on how to generalize this into a reusable pipeline that can be directly applied onto LLMs before training and/or fine-tuning. Would love feedback from anyone here who has tried entropy or loss-based filtering and possibly even scaled it&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2ywguh21eqyf1.png?width=4461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad3684760ca19910e37a7594255b8e935a20c7ac"&gt;https://preview.redd.it/2ywguh21eqyf1.png?width=4461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ad3684760ca19910e37a7594255b8e935a20c7ac&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jolly-Act9349"&gt; /u/Jolly-Act9349 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om2nqy/p_training_better_llms_with_30_less_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om2nqy/p_training_better_llms_with_30_less_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om2nqy/p_training_better_llms_with_30_less_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T23:43:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ome1la</id>
    <title>When Five Dumb AIs Beat One Smart AI: The Case for Multi-Agent Systems</title>
    <updated>2025-11-02T10:35:30+00:00</updated>
    <author>
      <name>/u/SuspiciousFile9845</name>
      <uri>https://old.reddit.com/user/SuspiciousFile9845</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://medium.com/@ksramalakshmi/when-five-dumb-ais-beat-one-smart-ai-the-case-for-multi-agent-systems-47b72ac5d7da"&gt;https://medium.com/@ksramalakshmi/when-five-dumb-ais-beat-one-smart-ai-the-case-for-multi-agent-systems-47b72ac5d7da&lt;/a&gt;&lt;br /&gt; What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuspiciousFile9845"&gt; /u/SuspiciousFile9845 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ome1la/when_five_dumb_ais_beat_one_smart_ai_the_case_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ome1la/when_five_dumb_ais_beat_one_smart_ai_the_case_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ome1la/when_five_dumb_ais_beat_one_smart_ai_the_case_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T10:35:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1om26g2</id>
    <title>Why don‚Äôt more apps run AI locally?</title>
    <updated>2025-11-01T23:21:27+00:00</updated>
    <author>
      <name>/u/elinaembedl</name>
      <uri>https://old.reddit.com/user/elinaembedl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been seeing more talk about running small LLMs locally on phones.&lt;/p&gt; &lt;p&gt;Almost every new phone ships with dedicated AI hardware (NPU,GPU, etc). Still, very few apps seem to use them to run models on-device.&lt;/p&gt; &lt;p&gt;What‚Äôs holding local inference back on mobile in your experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elinaembedl"&gt; /u/elinaembedl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om26g2/why_dont_more_apps_run_ai_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om26g2/why_dont_more_apps_run_ai_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om26g2/why_dont_more_apps_run_ai_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T23:21:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1olqjxj</id>
    <title>Official GGUFs in Qwen3-VL Collection - 235B/32B/30B/8B/4B/2B</title>
    <updated>2025-11-01T15:23:56+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olqjxj/official_ggufs_in_qwen3vl_collection/"&gt; &lt;img alt="Official GGUFs in Qwen3-VL Collection - 235B/32B/30B/8B/4B/2B" src="https://external-preview.redd.it/_7BYCEiuSe8H_fldVM7chLfCb5j0ciz_pk_F5HpmBuY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de9f337f24ae40e158287e6812e9408e53add8ae" title="Official GGUFs in Qwen3-VL Collection - 235B/32B/30B/8B/4B/2B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olqjxj/official_ggufs_in_qwen3vl_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olqjxj/official_ggufs_in_qwen3vl_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:23:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1om03mi</id>
    <title>AMD EPYC 4565P is a beast</title>
    <updated>2025-11-01T21:50:40+00:00</updated>
    <author>
      <name>/u/coding9</name>
      <uri>https://old.reddit.com/user/coding9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Haven‚Äôt seen too much coverage on these CPUs but I got a system with it. I can get over 15t/s on gpt-oss 20b with cpu only on 5600mhz ecc ram. &lt;/p&gt; &lt;p&gt;Pretty surprised it‚Äôs this good with the avx 512 instruction set. &lt;/p&gt; &lt;p&gt;Anyone else using these or have any thoughts?&lt;/p&gt; &lt;p&gt;Edit: this wasn‚Äôt purchased for inference so I‚Äôm just excited it can do some basic stuff with it as well&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coding9"&gt; /u/coding9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om03mi/amd_epyc_4565p_is_a_beast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om03mi/amd_epyc_4565p_is_a_beast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om03mi/amd_epyc_4565p_is_a_beast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T21:50:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1oma5ws</id>
    <title>OCR models: HF demos vs local performance</title>
    <updated>2025-11-02T06:20:38+00:00</updated>
    <author>
      <name>/u/SubstantialSock8002</name>
      <uri>https://old.reddit.com/user/SubstantialSock8002</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oma5ws/ocr_models_hf_demos_vs_local_performance/"&gt; &lt;img alt="OCR models: HF demos vs local performance" src="https://b.thumbs.redditmedia.com/mIhQZi1GFRBjVDMNiNavyg3R-VaEfLzvZJo7nl-gqMc.jpg" title="OCR models: HF demos vs local performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The last few days, I've been testing every OCR model under the sun to compare performance. I'd get amazing results on the HuggingFace Space demos, but when running locally, the models would hallucinate or output garbage.&lt;/p&gt; &lt;p&gt;The latest model I tried running locally was MinerU 2.5, and it had the same issue, even with the exact gradio demo provided in the repo as the hosted version. However, I then switched from the default pipeline backend to vlm-transformers, and it performed as well as the hosted version.&lt;/p&gt; &lt;p&gt;Has anyone else experienced similar issues? I haven't found a fix for others, but so far I've tried docling granite, deepseek ocr, paddleocr vl, and olmocr, with the same common theme: hosted works, local fails.&lt;/p&gt; &lt;p&gt;Here's an example image I used, along with the outputs for MinerU with both backends.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1e78yfvkdsyf1.jpg?width=370&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c80636d3d1b6b063014c7e14cd6f748247c8edc9"&gt;https://preview.redd.it/1e78yfvkdsyf1.jpg?width=370&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c80636d3d1b6b063014c7e14cd6f748247c8edc9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pipeline output:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;# The Daily&lt;/p&gt; &lt;p&gt;# Martians invade earth&lt;/p&gt; &lt;p&gt;Incredible as it may seem, headed towards the North Ren it has been confimed that Pole and Santa Claus was foll a lat ge martian invasion taken hostage by the imp tonight. invaders.&lt;/p&gt; &lt;p&gt;Afterwards they split apart First vessels were sighted in order to approach most over Great Britain, major cities around the Denmark and Norway earth. The streets filled as already in the late evening thousands fled their from where, as further homes, many only wearing reports indicate, the fleet their pajamas...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;vlm-transformers output:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;# The Daily&lt;/p&gt; &lt;p&gt;Sunday, August 30, 2006&lt;/p&gt; &lt;p&gt;# Martians invade earth&lt;/p&gt; &lt;p&gt;Incredible as it may seem, it has been confirmed that a large martian invasion fleet has landed on earth tonight.&lt;/p&gt; &lt;p&gt;First vessels were sighted over Great Britain, Denmark and Norway already in the late evening from where, as further reports indicate, the fleet&lt;/p&gt; &lt;p&gt;headed towards the North Pole and Santa Claus was taken hostage by the invaders.&lt;/p&gt; &lt;p&gt;Afterwards they split apart in order to approach most major cities around the earth. The streets filled as thousands fled their homes, many only wearing their pajamas...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SubstantialSock8002"&gt; /u/SubstantialSock8002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oma5ws/ocr_models_hf_demos_vs_local_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oma5ws/ocr_models_hf_demos_vs_local_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oma5ws/ocr_models_hf_demos_vs_local_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T06:20:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1olmj9z</id>
    <title>Bought MI50 32 Gb from Alibaba. Did I get scammed?</title>
    <updated>2025-11-01T12:26:47+00:00</updated>
    <author>
      <name>/u/Moist_Toto</name>
      <uri>https://old.reddit.com/user/Moist_Toto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olmj9z/bought_mi50_32_gb_from_alibaba_did_i_get_scammed/"&gt; &lt;img alt="Bought MI50 32 Gb from Alibaba. Did I get scammed?" src="https://preview.redd.it/v3w8clon2nyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ef938653e71635a81d9e3e2eaf625cfbf73033e" title="Bought MI50 32 Gb from Alibaba. Did I get scammed?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I bought 8 MI50 32Gb units from someone on Alibaba.&lt;/p&gt; &lt;p&gt;After spending some time to figure out Linux and the software stack, I entered the 'amd-smi static' command in the terminal.&lt;/p&gt; &lt;p&gt;The result is quite frightening, here it is: &lt;/p&gt; &lt;p&gt;especially the bottom part product name saying &amp;quot;16GB&amp;quot;, my heart skipped a beat. Is this something driver related or am I screwed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Moist_Toto"&gt; /u/Moist_Toto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v3w8clon2nyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olmj9z/bought_mi50_32_gb_from_alibaba_did_i_get_scammed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olmj9z/bought_mi50_32_gb_from_alibaba_did_i_get_scammed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T12:26:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1olouiw</id>
    <title>TIL: For long-lived LLM sessions, swapping KV Cache to RAM is ~10x faster than recalculating it. Why isn't this a standard feature?</title>
    <updated>2025-11-01T14:12:57+00:00</updated>
    <author>
      <name>/u/Shoddy-Tutor9563</name>
      <uri>https://old.reddit.com/user/Shoddy-Tutor9563</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I was diving into how vLLM and similar inference servers work and had a thought about optimizing memory for long-lived but inactive chat sessions. The standard approach seems to be either keeping the KV Cache in precious VRAM or evicting it and recalculating from scratch when the user returns. I think there might be a better way.&lt;/p&gt; &lt;p&gt;Here's the core idea: Implement a swapping mechanism for the KV Cache of inactive sessions, moving it from VRAM to system RAM (and back), instead of deleting it.&lt;/p&gt; &lt;p&gt;We always focus on the high cost of moving data between CPU and GPU, but we often forget the cost of recalculating that data. Let's do a quick back-of-the-napkin comparison for a Qwen3-4B-like model with a 16k token context:&lt;/p&gt; &lt;p&gt;Scenario: A user's session becomes inactive. Their 16k-token KV Cache is evicted. Later, they return. We need to restore their context.&lt;/p&gt; &lt;p&gt;¬∑ Option A: Recalculate the KV Cache (Standard Approach) ¬∑ This requires a full &amp;quot;prefill&amp;quot; pass over the entire 16k token prompt. ¬∑ Estimated Time: ~1.5 to 3 seconds on a modern GPU. ¬∑ Option B: Swapping (Proposed Approach) ¬∑ We simply copy the ~4 GB of KV Cache data from system RAM back to VRAM over PCIe. ¬∑ Estimated Time: ~200-400 ms (on PCIe 4.0).&lt;/p&gt; &lt;p&gt;The math is pretty compelling. Swapping is roughly 7-15x faster than a full recalculation. For a user, waiting 200ms for their chat history to &amp;quot;wake up&amp;quot; is a much better experience than waiting 2+ seconds.&lt;/p&gt; &lt;p&gt;This wouldn't be for high-throughput, always-online inference, but specifically for managing many long-lived sessions (e.g., support chatbots, document analysis with breaks, multi-user systems with intermittent activity). It's a classic space-time tradeoff, but in this case, using slightly more &amp;quot;space&amp;quot; (system RAM) saves a huge amount of &amp;quot;time&amp;quot; (latency on reactivation).&lt;/p&gt; &lt;p&gt;So, I have two main questions for the community:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Did I mess up my calculations or reasoning anywhere? Are there hidden costs or architectural limitations (e.g., in vLLM, PyTorch, or CUDA) that make this swapping idea less practical than it seems on paper?&lt;/li&gt; &lt;li&gt;Has anyone seen or heard of implementations doing this? I know vLLM's PagedAttention is genius for VRAM management, but I haven't found anything about spilling over to CPU RAM. Are there any forks, research papers, or other inference engines exploring this?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Keen to hear your thoughts and correct any misunderstandings I might have!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shoddy-Tutor9563"&gt; /u/Shoddy-Tutor9563 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olouiw/til_for_longlived_llm_sessions_swapping_kv_cache/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olouiw/til_for_longlived_llm_sessions_swapping_kv_cache/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olouiw/til_for_longlived_llm_sessions_swapping_kv_cache/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T14:12:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1omgi3f</id>
    <title>Why does Image Recognition work in llama-server but not through Open WebUI?</title>
    <updated>2025-11-02T12:53:06+00:00</updated>
    <author>
      <name>/u/pixelterpy</name>
      <uri>https://old.reddit.com/user/pixelterpy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omgi3f/why_does_image_recognition_work_in_llamaserver/"&gt; &lt;img alt="Why does Image Recognition work in llama-server but not through Open WebUI?" src="https://preview.redd.it/8fjfilvybuyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2447588056ef1b6e5514cbfcc3152a09667d1816" title="Why does Image Recognition work in llama-server but not through Open WebUI?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pixelterpy"&gt; /u/pixelterpy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8fjfilvybuyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omgi3f/why_does_image_recognition_work_in_llamaserver/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omgi3f/why_does_image_recognition_work_in_llamaserver/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T12:53:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1om8wdf</id>
    <title>Do you have any "AI toy projects"?</title>
    <updated>2025-11-02T05:04:35+00:00</updated>
    <author>
      <name>/u/MustBeSomethingThere</name>
      <uri>https://old.reddit.com/user/MustBeSomethingThere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om8wdf/do_you_have_any_ai_toy_projects/"&gt; &lt;img alt="Do you have any &amp;quot;AI toy projects&amp;quot;?" src="https://external-preview.redd.it/ZW1iMzQxdDB4cnlmMeDjq5qHZ2-J4399WQRf0LNpnjuXZ3nb5V768lbHJFZZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=904f40fa717f5b1bf416cec3465963053f8e879b" title="Do you have any &amp;quot;AI toy projects&amp;quot;?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I share my toy project as an example: &lt;a href="https://github.com/PasiKoodaa/TextTube"&gt;https://github.com/PasiKoodaa/TextTube&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Maybe in 10-15 years most streaming services will be replaced by local AI content creators.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MustBeSomethingThere"&gt; /u/MustBeSomethingThere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/iy7yl0u0xryf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om8wdf/do_you_have_any_ai_toy_projects/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om8wdf/do_you_have_any_ai_toy_projects/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T05:04:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1omf6p7</id>
    <title>Next evolution of agentic memory</title>
    <updated>2025-11-02T11:44:42+00:00</updated>
    <author>
      <name>/u/Any-Cockroach-3233</name>
      <uri>https://old.reddit.com/user/Any-Cockroach-3233</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every new AI startup says they've &amp;quot;solved memory&amp;quot;&lt;/p&gt; &lt;p&gt;99% of them just dump text into a vector DB&lt;/p&gt; &lt;p&gt;I wrote about why that approach is broken, and how agents can build human-like memory instead&lt;/p&gt; &lt;p&gt;Link in the comments&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Cockroach-3233"&gt; /u/Any-Cockroach-3233 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omf6p7/next_evolution_of_agentic_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omf6p7/next_evolution_of_agentic_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omf6p7/next_evolution_of_agentic_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T11:44:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ome16n</id>
    <title>LEAP: Ifm2-2.6b running locally on my RM11 Pro+</title>
    <updated>2025-11-02T10:34:48+00:00</updated>
    <author>
      <name>/u/ANG3LBEATZ</name>
      <uri>https://old.reddit.com/user/ANG3LBEATZ</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ome16n/leap_ifm226b_running_locally_on_my_rm11_pro/"&gt; &lt;img alt="LEAP: Ifm2-2.6b running locally on my RM11 Pro+" src="https://external-preview.redd.it/dWk5bXFrd2pudHlmMS2vRIL-FSqGaAZYDE4hFOYMDU1BKOfLu6Jj8jaoH7vM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06ec81f2aecb737f1ef77aba433757f6df108957" title="LEAP: Ifm2-2.6b running locally on my RM11 Pro+" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;uploading this by the request&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ANG3LBEATZ"&gt; /u/ANG3LBEATZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qeszvwvjntyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ome16n/leap_ifm226b_running_locally_on_my_rm11_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ome16n/leap_ifm226b_running_locally_on_my_rm11_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T10:34:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1om81j1</id>
    <title>glm-proxy - A Proxy Server I Built to Fix GLM 4.5 Air's Tool Call Issues</title>
    <updated>2025-11-02T04:17:09+00:00</updated>
    <author>
      <name>/u/akirose1004</name>
      <uri>https://old.reddit.com/user/akirose1004</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"&gt; &lt;img alt="glm-proxy - A Proxy Server I Built to Fix GLM 4.5 Air's Tool Call Issues" src="https://b.thumbs.redditmedia.com/7K256kODiuLD_3gm4UtRLndAWItovQwEv3qnrZaI3FI.jpg" title="glm-proxy - A Proxy Server I Built to Fix GLM 4.5 Air's Tool Call Issues" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/mdu32bj6kryf1.png?width=476&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6c99630db5ed36a9a2340d77a8bae86f2d708cc"&gt;https://preview.redd.it/mdu32bj6kryf1.png?width=476&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6c99630db5ed36a9a2340d77a8bae86f2d708cc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was running GLM 4.5 Air on my MacBook M4 Max with LM Studio, but &lt;strong&gt;tool calls weren't working properly&lt;/strong&gt;, which meant I couldn't use qwen-code CLI. I wanted to use an OpenAI-compatible interface, and this constant friction frustrated me enough to build a solution.&lt;/p&gt; &lt;p&gt;A proxy server that &lt;strong&gt;automatically converts GLM's XML-formatted tool calls to OpenAI-compatible format&lt;/strong&gt;. Now you can use any OpenAI-compatible client (like qwen-code) with GLM seamlessly!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full OpenAI API compatibility&lt;/li&gt; &lt;li&gt;Automatic conversion of GLM's XML &lt;code&gt;&amp;lt;tool_call&amp;gt;&lt;/code&gt; format to OpenAI JSON format&lt;/li&gt; &lt;li&gt;Streaming support&lt;/li&gt; &lt;li&gt;Multiple tool calls and complex JSON argument parsing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Point any OpenAI-compatible client (qwen-code, LangChain, etc.) to this address and use GLM 4.5 Air as if it were OpenAI!&lt;/p&gt; &lt;h1&gt;üîó GitHub&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/akirose/glm-proxy"&gt;https://github.com/akirose/glm-proxy&lt;/a&gt; (MIT License)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you're using GLM 4.5 with LM Studio, no more tool call headaches!&lt;/strong&gt; üòä&lt;/p&gt; &lt;p&gt;Feedback and suggestions welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akirose1004"&gt; /u/akirose1004 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1om81j1/glmproxy_a_proxy_server_i_built_to_fix_glm_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T04:17:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1omaa4i</id>
    <title>OCR Testing Tool maybe Open Source it?</title>
    <updated>2025-11-02T06:28:09+00:00</updated>
    <author>
      <name>/u/No-Fig-8614</name>
      <uri>https://old.reddit.com/user/No-Fig-8614</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created a quick OCR tool, what it does is you choose a file then a OCR model to use. Its free to use on this test site. What it does is upload the document -&amp;gt; turns to base64-&amp;gt; OCR Model -&amp;gt; extraction model. The extraction model is a larger model (In this case GLM4.6) to create key value extractions, then format it into json output. Eventually could add API's and user management. &lt;a href="https://parasail-ocr-pipeline.azurewebsites.net/"&gt;https://parasail-ocr-pipeline.azurewebsites.net/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For PDF's I put a pre-processing library that will cut the pdf into pages/images then send it to the OCR model then combine it after.&lt;/p&gt; &lt;p&gt;The status bar needs work because it will produce the OCR output first but then takes another minute for the auto schema (key/value) creation, then modify the JSON).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Any feedback on it would be great on it!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: There is no user segregation so any document uploaded anyone else can see.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Fig-8614"&gt; /u/No-Fig-8614 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omaa4i/ocr_testing_tool_maybe_open_source_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omaa4i/ocr_testing_tool_maybe_open_source_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omaa4i/ocr_testing_tool_maybe_open_source_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T06:28:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1olytpd</id>
    <title>Qwen3-VL is impressive!</title>
    <updated>2025-11-01T20:56:56+00:00</updated>
    <author>
      <name>/u/KraiiFox</name>
      <uri>https://old.reddit.com/user/KraiiFox</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olytpd/qwen3vl_is_impressive/"&gt; &lt;img alt="Qwen3-VL is impressive!" src="https://external-preview.redd.it/d2xhbXRjcGxscHlmMUowvrHmMIpZo4AiauGE1Mcv4FXKd8bkFKJe4QU1BrJL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=46a71b7402921f97028babf1e571a097b79a162c" title="Qwen3-VL is impressive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KraiiFox"&gt; /u/KraiiFox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/sfcu47ollpyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olytpd/qwen3vl_is_impressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olytpd/qwen3vl_is_impressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T20:56:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1omcjct</id>
    <title>Running Local LLM's Fascinates me - But I'm Absolutely LOST</title>
    <updated>2025-11-02T08:57:28+00:00</updated>
    <author>
      <name>/u/WhatsGoingOnERE</name>
      <uri>https://old.reddit.com/user/WhatsGoingOnERE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I watched PewDiePie‚Äôs new video and now I‚Äôm obsessed with the idea of running models locally. He had a ‚Äúcouncil‚Äù of AIs talking to each other, then voting on the best answer. You can also fine tune and customise stuff, which sounds unreal.&lt;/p&gt; &lt;p&gt;Here‚Äôs my deal. I already pay for GPT-5 Pro and Claude Max and they are great. I want to know if I would actually see better performance by doing this locally, or if it‚Äôs just a fun rabbit hole.&lt;/p&gt; &lt;p&gt;Basically want to know if using these local models gets better results for anyone vs the best models available online, and if not, what are the other benefits?&lt;/p&gt; &lt;p&gt;I know privacy is a big one for some people, but lets ignore that for this case.&lt;/p&gt; &lt;p&gt;My main use cases are for business (SEO, SaaS, general marketing, business idea ideation, etc), and coding.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WhatsGoingOnERE"&gt; /u/WhatsGoingOnERE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omcjct/running_local_llms_fascinates_me_but_im/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omcjct/running_local_llms_fascinates_me_but_im/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omcjct/running_local_llms_fascinates_me_but_im/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T08:57:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1olxijp</id>
    <title>List of interesting open-source models released this month.</title>
    <updated>2025-11-01T20:03:07+00:00</updated>
    <author>
      <name>/u/Acrobatic-Tomato4862</name>
      <uri>https://old.reddit.com/user/Acrobatic-Tomato4862</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I've been tracking the latest AI model releases and wanted to share a curated list of AI models released this month.&lt;/p&gt; &lt;p&gt;Credit to &lt;a href="/u/duarteeeeee"&gt;u/duarteeeeee&lt;/a&gt; for finding all these models.&lt;/p&gt; &lt;p&gt;Here's a chronological breakdown of some of the most interesting open models released around October 1st - 31st, 2025:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;October 1st:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/Liquid4All/liquid-audio"&gt;&lt;strong&gt;LFM2-Audio-1.5B&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Low-latency, end-to-end audio foundation model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-370m"&gt;&lt;strong&gt;KaniTTS-370M&lt;/strong&gt;&lt;/a&gt; (NineNineSix): Fast, open-source TTS for real-time applications.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 2nd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models"&gt;&lt;strong&gt;Granite 4.0&lt;/strong&gt;&lt;/a&gt; (IBM): Hyper-efficient, hybrid models for enterprise use.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/neuphonic/neutts-air"&gt;&lt;strong&gt;NeuTTS Air&lt;/strong&gt;&lt;/a&gt; (Neuphonic Speech): On-device TTS with instant voice cloning.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 3rd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://simular.ai/articles/agent-s3"&gt;&lt;strong&gt;Agent S3&lt;/strong&gt;&lt;/a&gt; (Simular): Open framework for human-like computer use.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-UniVision-16B-A3B"&gt;&lt;strong&gt;Ming-UniVision-16B-A3B&lt;/strong&gt;&lt;/a&gt; (Ant Group): Unified vision understanding, generation, editing model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/character-ai/Ovi"&gt;&lt;strong&gt;Ovi (TTV/ITV)&lt;/strong&gt;&lt;/a&gt; (Character.AI / Yale): Open-source framework for offline talking avatars.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Salesforce/CoDA-v0-Instruct"&gt;&lt;strong&gt;CoDA-v0-Instruct&lt;/strong&gt;&lt;/a&gt; (Salesforce AI Research): Bidirectional diffusion model for code generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 4th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct"&gt;&lt;strong&gt;Qwen3-VL-30B-A3B-Instruct&lt;/strong&gt;&lt;/a&gt; (Alibaba): Powerful vision-language model for agentic tasks.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/DecartAI/Decart-XR"&gt;&lt;strong&gt;DecartXR&lt;/strong&gt;&lt;/a&gt; (Decart AI): Open-source Quest app for realtime video-FX.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 7th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-8B-A1B"&gt;&lt;strong&gt;LFM2-8B-A1B&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Efficient on-device mixture-of-experts model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Tencent-Hunyuan/HunyuanVision"&gt;&lt;strong&gt;Hunyuan-Vision-1.5-Thinking&lt;/strong&gt;&lt;/a&gt; (Tencent): Multimodal &amp;quot;thinking on images&amp;quot; reasoning model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/bageldotcom/paris"&gt;&lt;strong&gt;Paris&lt;/strong&gt;&lt;/a&gt; (Bagel Network): Decentralized-trained open-weight diffusion model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/cumulo-autumn/StreamDiffusion"&gt;&lt;strong&gt;StreamDiffusionV2&lt;/strong&gt;&lt;/a&gt; (UC Berkeley, MIT, et al.): Open-source pipeline for real-time video streaming.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 8th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ai21labs/Jamba-v0.1"&gt;&lt;strong&gt;Jamba Reasoning 3B&lt;/strong&gt;&lt;/a&gt; (AI21 Labs): Small hybrid model for on-device reasoning.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T"&gt;&lt;strong&gt;Ling-1T / Ring-1T&lt;/strong&gt;&lt;/a&gt; (Ant Group): Trillion-parameter thinking/non-thinking open models.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/TingtingLiao/mimix"&gt;&lt;strong&gt;Mimix&lt;/strong&gt;&lt;/a&gt; (Research): Framework for multi-character video generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 9th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/microsoft/UserLM-8b"&gt;&lt;strong&gt;UserLM-8b&lt;/strong&gt;&lt;/a&gt; (Microsoft): Open-weight model simulating a &amp;quot;user&amp;quot; role.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/RadicalNumerics/RND1"&gt;&lt;strong&gt;RND1-Base-0910&lt;/strong&gt;&lt;/a&gt; (Radical Numerics): Experimental diffusion language model (30B MoE).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 10th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp"&gt;&lt;strong&gt;KAT-Dev-72B-Exp&lt;/strong&gt;&lt;/a&gt; (Kwaipilot): Open-source experimental model for agentic coding.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 12th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://pbihao.github.io/projects/DreamOmni2/"&gt;&lt;strong&gt;DreamOmni2&lt;/strong&gt;&lt;/a&gt; (ByteDance): Multimodal instruction-based image editing/generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 13th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/mit-han-lab/streaming-vlm"&gt;&lt;strong&gt;StreamingVLM&lt;/strong&gt;&lt;/a&gt; (MIT Han Lab): Real-time understanding for infinite video streams.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 14th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL"&gt;&lt;strong&gt;Qwen3-VL-4B / 8B&lt;/strong&gt;&lt;/a&gt; (Alibaba): Efficient, open vision-language models for edge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 16th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PaddlePaddle/PaddleOCR-VL"&gt;&lt;strong&gt;PaddleOCR-VL&lt;/strong&gt;&lt;/a&gt; (Baidu): Lightweight 109-language document parsing model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/facebook/MobileLLM-Pro"&gt;&lt;strong&gt;MobileLLM-Pro&lt;/strong&gt;&lt;/a&gt; (Meta): 1B parameter on-device model (128k context).&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0"&gt;&lt;strong&gt;FlashWorld&lt;/strong&gt;&lt;/a&gt; (Tencent): Fast (5-10 sec) 3D scene generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 17th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-flash-preview"&gt;&lt;strong&gt;LLaDA2.0-flash-preview&lt;/strong&gt;&lt;/a&gt; (Ant Group): 100B MoE diffusion model for reasoning/code.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 20th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR"&gt;&lt;strong&gt;DeepSeek-OCR&lt;/strong&gt;&lt;/a&gt; (DeepseekAI): Open-source model for optical context-compression.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/krea-ai/realtime-video"&gt;&lt;strong&gt;Krea Realtime 14B&lt;/strong&gt;&lt;/a&gt; (Krea AI): 14B open-weight real-time video generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 21st:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct"&gt;&lt;strong&gt;Qwen3-VL-2B / 32B&lt;/strong&gt;&lt;/a&gt; (Alibaba): Open, dense VLMs for edge and cloud.&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2510.14876"&gt;&lt;strong&gt;BADAS-Open&lt;/strong&gt;&lt;/a&gt; (Nexar): Ego-centric collision prediction model for ADAS.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 22nd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-VL-3B"&gt;&lt;strong&gt;LFM2-VL-3B&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Efficient vision-language model for edge deployment.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/tencent/HunyuanWorld-1"&gt;&lt;strong&gt;HunyuanWorld-1.1&lt;/strong&gt;&lt;/a&gt; (Tencent): 3D world generation from multi-view/video.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PokeeAI/pokee_research_7b"&gt;&lt;strong&gt;PokeeResearch-7B&lt;/strong&gt;&lt;/a&gt; (Pokee AI): Open 7B deep-research agent (search/synthesis).&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/allenai/olmOCR-2-7B-1025"&gt;&lt;strong&gt;olmOCR-2-7B-1025&lt;/strong&gt;&lt;/a&gt; (Allen Institute for AI): Open-source, single-pass PDF-to-structured-text model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 23rd:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://ltx.video/"&gt;&lt;strong&gt;LTX 2&lt;/strong&gt;&lt;/a&gt; (Lightricks): Open-source 4K video engine for consumer GPUs.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lightonai/LightOnOCR-1B-1025"&gt;&lt;strong&gt;LightOnOCR-1B&lt;/strong&gt;&lt;/a&gt; (LightOn): Fast, 1B-parameter open-source OCR VLM.&lt;/li&gt; &lt;li&gt;&lt;a href="https://holo-cine.github.io/"&gt;&lt;strong&gt;HoloCine&lt;/strong&gt;&lt;/a&gt; (Research): Model for holistic, multi-shot cinematic narratives.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 24th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/tahoebio/tahoe-x1"&gt;&lt;strong&gt;Tahoe-x1&lt;/strong&gt;&lt;/a&gt; (Tahoe Therapeutics): 3B open-source single-cell biology model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PRIME-RL/P1-30B-A3B"&gt;&lt;strong&gt;P1&lt;/strong&gt;&lt;/a&gt; (PRIME-RL): Model mastering Physics Olympiads with RL.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 25th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Video"&gt;&lt;strong&gt;LongCat-Video&lt;/strong&gt;&lt;/a&gt; (Meituan): 13.6B open model for long video generation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/html/2510.19944v1"&gt;&lt;strong&gt;Seed 3D 1.0&lt;/strong&gt;&lt;/a&gt; (ByteDance): Generates simulation-grade 3D assets from images.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 27th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.minimax.io/news/minimax-m2"&gt;&lt;strong&gt;Minimax M2&lt;/strong&gt;&lt;/a&gt; (Minimax): Open-sourced intelligence engine for agentic workflows.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-flash-omni-Preview"&gt;&lt;strong&gt;Ming-flash-omni-Preview&lt;/strong&gt;&lt;/a&gt; (Ant Group): 100B MoE omni-modal model for perception.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-mini-preview"&gt;&lt;strong&gt;LLaDA2.0-mini-preview&lt;/strong&gt;&lt;/a&gt; (Ant Group): 16B MoE diffusion model for language.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 28th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-ColBERT-350M"&gt;&lt;strong&gt;LFM2-ColBERT-350M&lt;/strong&gt;&lt;/a&gt; (Liquid AI): Multilingual &amp;quot;late interaction&amp;quot; RAG retriever model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-4.0-350m"&gt;&lt;strong&gt;Granite 4.0 Nano (1B / 350M)&lt;/strong&gt;&lt;/a&gt; (IBM): Smallest open models for on-device use.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/HKUDS/ViMax"&gt;&lt;strong&gt;ViMax&lt;/strong&gt;&lt;/a&gt; (HKUDS): Agentic framework for end-to-end video creation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://build.nvidia.com/nvidia/nemotron-nano-12b-v2-vl/modelcard"&gt;&lt;strong&gt;Nemotron Nano v2 VL&lt;/strong&gt;&lt;/a&gt; (NVIDIA): 12B open model for multi-image/video understanding.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 29th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://openai.com/index/introducing-gpt-oss-safeguard/"&gt;&lt;strong&gt;gpt-oss-safeguard&lt;/strong&gt;&lt;/a&gt; (OpenAI): Open-weight reasoning models for safety classification.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/morphicfilms/frames-to-video"&gt;&lt;strong&gt;Frames to Video&lt;/strong&gt;&lt;/a&gt; (Morphic): Open-source model for keyframe video interpolation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Bria-AI/FIBO"&gt;&lt;strong&gt;Fibo&lt;/strong&gt;&lt;/a&gt; (Bria AI): SOTA open-source model (trained on licensed data).&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ByteDance/Ouro-2.6B-Thinking"&gt;&lt;strong&gt;Bytedance Ouro 2.6b thinking and non thinking&lt;/strong&gt;&lt;/a&gt;: Small language models that punch above their weight. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;October 30th:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/baaivision/Emu3.5"&gt;&lt;strong&gt;Emu3.5&lt;/strong&gt;&lt;/a&gt; (BAAI): Native multimodal model as a world learner.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct"&gt;&lt;strong&gt;Kimi-Linear-48B-A3B&lt;/strong&gt;&lt;/a&gt; (Moonshot AI): Long-context model using a linear-attention mechanism.&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.modelscope.cn/studios/BlinkDL/RWKV-CHN-2/summary"&gt;&lt;strong&gt;RWKV-7 G0a3 7.2B&lt;/strong&gt;&lt;/a&gt; (BlinkDL): A multilingual RNN-based large language model.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/alibaba/UI-Ins"&gt;&lt;strong&gt;UI-Ins-32B / 7B&lt;/strong&gt;&lt;/a&gt; (Alibaba): GUI grounding agent.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please correct me if I have misclassified/mislinked any of the above models. This is my first post, so I am expecting there might be some mistakes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acrobatic-Tomato4862"&gt; /u/Acrobatic-Tomato4862 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olxijp/list_of_interesting_opensource_models_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olxijp/list_of_interesting_opensource_models_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olxijp/list_of_interesting_opensource_models_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T20:03:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
</feed>
