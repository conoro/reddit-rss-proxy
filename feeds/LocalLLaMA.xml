<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-07T22:23:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nabcek</id>
    <title>Anyone actully try to run gpt-oss-120b (or 20b) on a Ryzen AI Max+ 395?</title>
    <updated>2025-09-06T21:28:33+00:00</updated>
    <author>
      <name>/u/PM_ME_YOUR_PROOFS</name>
      <uri>https://old.reddit.com/user/PM_ME_YOUR_PROOFS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AMD is understandably&lt;a href="https://www.amd.com/en/blogs/2025/how-to-run-openai-gpt-oss-20b-120b-models-on-amd-ryzen-ai-radeon.html"&gt; trying to tout this&lt;/a&gt; and and there's this from a month a go claiming &amp;quot;30 tokens per second&amp;quot; (not clear if 120b or 20b). I can't tell if the flops are int8 flops of bf16 or fp16 on the 395. In theory if we assume the 395 has 50 tops of bf16 on its NPU and we trust their &lt;a href="https://www.amd.com/en/products/processors/laptop/ryzen/ai-300-series/amd-ryzen-ai-max-plus-395.html"&gt;&amp;quot;overall TOPS&amp;quot;&lt;/a&gt; its potentially pushing into 3090 territory under ideal conditions. It has *waaay* more memory which is super useful for getting things to run at all but it also has a lot less memory bandwidth about 1/4th as much. I guess a more fair comparison would be on 20b. I'd strong anticipate the 3090 getting better tokens per second on 20b.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/ryzen/comments/1lzr7yq/yolov8_multimachine_benchmark_rtx_3090_vs_ryzen/"&gt;this post &lt;/a&gt;suggests that actually under common configs a lot of times the 395 can beat the 3090...this is very surprising to me. Curious if anyone has actually tried 20b on both and can compare. Also curious what actual tokens per second people are getting with 120b.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PM_ME_YOUR_PROOFS"&gt; /u/PM_ME_YOUR_PROOFS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nabcek/anyone_actully_try_to_run_gptoss120b_or_20b_on_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nabcek/anyone_actully_try_to_run_gptoss120b_or_20b_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nabcek/anyone_actully_try_to_run_gptoss120b_or_20b_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T21:28:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb6q0y</id>
    <title>[2508.16676] WISCA: A Lightweight Model Transition Method to Improve LLM Training via Weight Scaling</title>
    <updated>2025-09-07T22:22:31+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2508.16676"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb6q0y/250816676_wisca_a_lightweight_model_transition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb6q0y/250816676_wisca_a_lightweight_model_transition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T22:22:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nay7wk</id>
    <title>Need a free, simple tool of whisper-v3-turbo speech-to-text for macOS</title>
    <updated>2025-09-07T16:48:39+00:00</updated>
    <author>
      <name>/u/SuddenWerewolf7041</name>
      <uri>https://old.reddit.com/user/SuddenWerewolf7041</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been looking a lot for a good tool that helps me dictate and also transcribe all the desktop audio to help with my accessibility issue. So far I had no luck whatsoever with any of the free tools, all of them just give you access to the whisper base or tiny/small which is nothing compared to the v3/turbo. My macOS can handle it, but the problem is that all the tools I used require payment to upgrade the model (which is annoying because technically I am running it on my MacBook, not in the cloud).&lt;/p&gt; &lt;p&gt;I would be very thankful if you have some tips. I need basically an always-on or live transcription feature (where at least there would be a differentiation between my microphone vs audio, no need for advanced diarization).&lt;/p&gt; &lt;p&gt;I understand that WhisperKit Pro has a commercial license, thus the reason why it's paid. But come on, it's year 2025 and it's been so many years since we have Whisper model and yet no decent free implementation of a (free and open source) model....&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuddenWerewolf7041"&gt; /u/SuddenWerewolf7041 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nay7wk/need_a_free_simple_tool_of_whisperv3turbo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nay7wk/need_a_free_simple_tool_of_whisperv3turbo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nay7wk/need_a_free_simple_tool_of_whisperv3turbo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T16:48:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1na7c1b</id>
    <title>OpenAI: Why Language Models Hallucinate</title>
    <updated>2025-09-06T18:44:12+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In short: LLMs hallucinate because we've inadvertently designed the training and evaluation process to reward confident, even if incorrect, answers, rather than honest admissions of uncertainty. Fixing this requires a shift in how we grade these systems to steer them towards more trustworthy behavior. &lt;/p&gt; &lt;p&gt;The Solution:&lt;/p&gt; &lt;p&gt;Explicitly stating &amp;quot;confidence targets&amp;quot; in evaluation instructions, where mistakes are penalized and admitting uncertainty (IDK) might receive 0 points, but guessing incorrectly receives a negative score. This encourages &amp;quot;behavioral calibration,&amp;quot; where the model only answers if it's sufficiently confident.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://share.google/9SKn7X0YThlmnkZ9m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na7c1b/openai_why_language_models_hallucinate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na7c1b/openai_why_language_models_hallucinate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T18:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1navnzc</id>
    <title>Adversarial collaboration between AI coding tools improves solution quality for complex tasks</title>
    <updated>2025-09-07T15:09:30+00:00</updated>
    <author>
      <name>/u/LuozhuZhang</name>
      <uri>https://old.reddit.com/user/LuozhuZhang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the past weeks I have been experimenting with an “AI vs AI” coding workflow designed for complex programming tasks. &lt;/p&gt; &lt;p&gt;The underlying idea is to move away from single model outputs and instead leverage structured interaction between multiple models as a form of cross-validation.&lt;/p&gt; &lt;p&gt;The process I tested follows these steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;A complex programming task is posed to both Cursor/CC and Codex.&lt;/li&gt; &lt;li&gt;Each system generates an initial solution.&lt;/li&gt; &lt;li&gt;Their solutions are then exchanged, with each model asked to critique, modify, or correct the other’s output.&lt;/li&gt; &lt;li&gt;This cycle is repeated iteratively until either one model converges to the other’s approach, or until a clear inconsistency is detected through human inspection.&lt;/li&gt; &lt;li&gt;The stronger solution is selected and implemented.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Preliminary experiments suggest that this adversarial exchange can substantially improve outcome quality. In my limited trials, the resulting code quality improved by nearly a factor of two, and the observed error rate was reduced by approximately 50%.&lt;/p&gt; &lt;p&gt;Importantly, these gains were most pronounced in tasks with higher complexity or multiple constraints; for trivial problems the additional overhead did not provide meaningful benefit.&lt;/p&gt; &lt;p&gt;Conceptually, this resembles ensemble methods in classical machine learning, where disagreement among models provides a signal for error correction. However, unlike bagging or boosting, here the models engage in an explicit, iterative dialogue that encourages error discovery and refinement. In effect, each model serves as both a generator and a critic, and their disagreements highlight weak points in reasoning that a single system may overlook.&lt;/p&gt; &lt;p&gt;I am currently considering building an open-source automation layer that integrates this workflow directly into tools such as Cursor and CC.&lt;/p&gt; &lt;p&gt;The vision is to provide a scaffold that can orchestrate multi-agent interaction automatically, without requiring manual prompting at every step. Such a system could serve as a practical framework for “AI peer review” in coding workflows, bridging the gap between individual model outputs and robust, production-ready solutions.&lt;/p&gt; &lt;p&gt;I would be very interested in whether the community views this approach as valuable. If there is sufficient interest, I plan to build a prototype and share it publicly. (If you’ve come across anything similar, please share it with me as well. My work involves a lot of system design, so methods like this are particularly valuable for me. 🙏)&lt;/p&gt; &lt;p&gt;I’ve been sharing some early thoughts on Twitter/X. For those interested, you can follow along there for future updates: &lt;a href="https://x.com/LuozhuZhang/status/1964706661291217370"&gt;https://x.com/LuozhuZhang/status/1964706661291217370&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LuozhuZhang"&gt; /u/LuozhuZhang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1navnzc/adversarial_collaboration_between_ai_coding_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1navnzc/adversarial_collaboration_between_ai_coding_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1navnzc/adversarial_collaboration_between_ai_coding_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T15:09:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1naxoa7</id>
    <title>Any Chat interface that I can run locally against LMStudio that runs on a different machine?</title>
    <updated>2025-09-07T16:27:31+00:00</updated>
    <author>
      <name>/u/KontoOficjalneMR</name>
      <uri>https://old.reddit.com/user/KontoOficjalneMR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried Webpie, Jan and multiple others. None of the ones I tried have an option to connect to LMStudio that's running on a different machine on local network. Even when I try using &amp;quot;OpenAI&amp;quot; with custom url LM Studio complains:&lt;/p&gt; &lt;p&gt;&lt;del&gt;&amp;quot;Unexpected endpoint or method. (OPTIONS /v1/models). Returning 200 anyway&amp;quot;.&lt;/del&gt;&lt;/p&gt; &lt;p&gt;I'm running newest LMStudio (0.3.25), any advice (preferably easy to install/use)?&lt;/p&gt; &lt;p&gt;I managed to get Jan to work with help of the commenters, but I'm still curious if there are any other alternatives. If you know any - let me know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KontoOficjalneMR"&gt; /u/KontoOficjalneMR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxoa7/any_chat_interface_that_i_can_run_locally_against/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxoa7/any_chat_interface_that_i_can_run_locally_against/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naxoa7/any_chat_interface_that_i_can_run_locally_against/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T16:27:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nan5az</id>
    <title>I managed to compile and run Llama 3B Q4_K_M on llama.cpp with Termux on ARMv7a, using only 2 GB.</title>
    <updated>2025-09-07T07:38:21+00:00</updated>
    <author>
      <name>/u/arbolito_mr</name>
      <uri>https://old.reddit.com/user/arbolito_mr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nan5az/i_managed_to_compile_and_run_llama_3b_q4_k_m_on/"&gt; &lt;img alt="I managed to compile and run Llama 3B Q4_K_M on llama.cpp with Termux on ARMv7a, using only 2 GB." src="https://b.thumbs.redditmedia.com/sv68yAoEG_zuCAcmLHS29J0tj-6x4vmOQwMWqwEnnMQ.jpg" title="I managed to compile and run Llama 3B Q4_K_M on llama.cpp with Termux on ARMv7a, using only 2 GB." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used to think running a reasonably coherent model on Android ARMv7a was impossible, but a few days ago I decided to put it to the test with llama.cpp, and I was genuinely impressed with how well it works. It's not something you can demand too much from, but being local and, of course, offline, it can get you out of tricky situations more than once. The model weighs around 2 GB and occupies roughly the same amount in RAM, although with certain flags it can be optimized to reduce consumption by up to 1 GB. It can also be integrated into personal Android projects thanks to its server functionality and the endpoints it provides for sending requests.&lt;/p&gt; &lt;p&gt;If anyone thinks this could be useful, let me know; as soon as I can, I’ll prepare a complete step-by-step guide, especially aimed at those who don’t have a powerful enough device to run large models or rely on a 32-bit processor.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arbolito_mr"&gt; /u/arbolito_mr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nan5az"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nan5az/i_managed_to_compile_and_run_llama_3b_q4_k_m_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nan5az/i_managed_to_compile_and_run_llama_3b_q4_k_m_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T07:38:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb3k5r</id>
    <title>~3000$ Budget Workstation/Server Recommendation with 2x RTX 5070 TI Super (48gb VRAM)</title>
    <updated>2025-09-07T20:14:24+00:00</updated>
    <author>
      <name>/u/Silent-Translator-87</name>
      <uri>https://old.reddit.com/user/Silent-Translator-87</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;br /&gt; I heard the RTX 5070 Ti Super with 24GB VRAM might be coming soon.&lt;br /&gt; I’m planning to build a workstation or server to run smaller LLMs locally once it’s out.&lt;br /&gt; Any recommendations for solid price/performance setups?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silent-Translator-87"&gt; /u/Silent-Translator-87 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3k5r/3000_budget_workstationserver_recommendation_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3k5r/3000_budget_workstationserver_recommendation_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3k5r/3000_budget_workstationserver_recommendation_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T20:14:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb3zma</id>
    <title>How to use gpt-oss with llama.cpp OpenAI API server?</title>
    <updated>2025-09-07T20:31:32+00:00</updated>
    <author>
      <name>/u/ivoras</name>
      <uri>https://old.reddit.com/user/ivoras</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When using the llama-cli tool for inference, I get a streaming output with the &amp;quot;channel&amp;quot; and &amp;quot;message&amp;quot; fields for reasoning, like in this example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;|channel|&amp;gt;analysis&amp;lt;|message|&amp;gt;The user asks: &amp;quot;How much wood could a woodchuck chuck?&amp;quot; It's a classic tongue-twister. The question is likely to answer with some playful or humorous answer. ... Ok.&amp;lt;|end|&amp;gt;&amp;lt;|start|&amp;gt;assistant&amp;lt;|channel|&amp;gt;final&amp;lt;|message|&amp;gt;It’s a classic tongue‑twister, but there’s a bit of science behind the myth, too... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But when calling the OpenAI API, like I'd do with other LLMs, directly via a HTTP POST request to the API endpoint (without a wrapper/library/framework), I get just a single token back as the result:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;|channel|&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Any pointers as to what could be going on?&lt;/p&gt; &lt;p&gt;My API request is very simple:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; response = requests.post( f&amp;quot;{LLM_BASE_URL}/chat/completions&amp;quot;, json={ &amp;quot;model&amp;quot;: LLM_MODEL, &amp;quot;messages&amp;quot;: [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: prompt}], &amp;quot;stream&amp;quot;: False, &amp;quot;temperature&amp;quot;: 0.1, }, headers={&amp;quot;Authorization&amp;quot;: f&amp;quot;Bearer {LLM_API_KEY}&amp;quot;}, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Edit 1: My llama-server command is (I've tried a couple of versions of the model in addition to the unsloth one, including &lt;code&gt;ggml-org/gpt-oss-20b-GGUF&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server -hf unsloth/gpt-oss-20b-GGUF -ngl 100 -c 16384 --predict -2 --alias gpt-oss --jinja &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Edit 2: SOLVED! Looks like I've found the problem: removing &lt;code&gt;--predict -2&lt;/code&gt; makes it work. The help message shows it's a valid parameter:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;-n, --predict, --n-predict N number of tokens to predict (default: -1, -1 = infinity, -2 = until context filled) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But the github docs don't mention &amp;quot;-2&amp;quot; as an option. 🤔&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivoras"&gt; /u/ivoras &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3zma/how_to_use_gptoss_with_llamacpp_openai_api_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3zma/how_to_use_gptoss_with_llamacpp_openai_api_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3zma/how_to_use_gptoss_with_llamacpp_openai_api_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T20:31:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb66te</id>
    <title>I built Claude Context but 100% local - semantic code search with no API keys</title>
    <updated>2025-09-07T22:00:18+00:00</updated>
    <author>
      <name>/u/person-loading</name>
      <uri>https://old.reddit.com/user/person-loading</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! &lt;/p&gt; &lt;p&gt;You might know Claude Context (3k+ stars) - it's a great semantic code search tool but requires OpenAI API keys + Zilliz Cloud.&lt;/p&gt; &lt;p&gt;I built a fully local alternative that runs 100% on your machine:&lt;/p&gt; &lt;p&gt;🔒 &lt;strong&gt;Privacy first&lt;/strong&gt; - Your code never leaves your machine 🚀 &lt;strong&gt;No API keys&lt;/strong&gt; - Uses EmbeddingGemma locally&lt;br /&gt; 💰 &lt;strong&gt;Zero costs&lt;/strong&gt; - No monthly API bills ⚡ &lt;strong&gt;Fast&lt;/strong&gt; - After initial indexing, searches are instant&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; - Tree-sitter for AST parsing (understands code structure) - EmbeddingGemma for semantic embeddings (1.2GB model) - FAISS for vector search - MCP protocol for Claude Code integration&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Early results:&lt;/strong&gt; - Reduction in Claude Code token usage (depends on search) - Finds code by meaning, not just text matching - Works with Python, JavaScript, TypeScript, JSX, TSX, Svelte (More coming just treesitter!)&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/FarhanAliRaza/claude-context-local"&gt;https://github.com/FarhanAliRaza/claude-context-local&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is an early release - would love feedback from the local-first community! If you hit any issues, please open a GitHub issue and I'll fix it fast.&lt;/p&gt; &lt;p&gt;Built this because I believe code search should be private and free. No cloud required!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/person-loading"&gt; /u/person-loading &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb66te/i_built_claude_context_but_100_local_semantic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb66te/i_built_claude_context_but_100_local_semantic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb66te/i_built_claude_context_but_100_local_semantic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T22:00:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nai7rf</id>
    <title>Why isn't there a local tool server that replicates most of the tools avaliable on ChatGPT?</title>
    <updated>2025-09-07T02:53:58+00:00</updated>
    <author>
      <name>/u/gigaflops_</name>
      <uri>https://old.reddit.com/user/gigaflops_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've made it to the point where mid-sized local LLMs can rival some cloud models in some use cases, but it feels like the local tool ecosystem is still years behind. It's a shame because models like gpt-oss-120b are pretty competent at &lt;em&gt;using&lt;/em&gt; tools that it is given access to.&lt;/p&gt; &lt;p&gt;A small, but not-insignificant fraction of all LLM prompts in most domains &lt;em&gt;need&lt;/em&gt; tools. Web search for up to date information, python interpreter for data analysis and moderately complex calculations, date and time access, and the ability to leverage an image-gen model all &amp;quot;just work&amp;quot; on ChatGPT. Even if I could run the GPT-5 model locally on my PC, it could never be usable for me without the tools.&lt;/p&gt; &lt;p&gt;In the local space, a quick search for MCP tool servers yields a fragmented ecosystem servers that do &lt;em&gt;one&lt;/em&gt; thing, often highly specialized, like analyze a github codebase or read your google calendar. You can't come close to replicating the &lt;em&gt;basic&lt;/em&gt; functionality of ChatGPT like web search and calculator without downloading 5+ servers using the command line or github (RIP beginners) and learning how to use docker or writing some master server to proxys them all into one.&lt;/p&gt; &lt;p&gt;Maybe I'm not looking in the right places, but it seems like people are only interested in using cloud tool servers (often with an API cost) with their local LLM, something that defeats the purpose imo. Even the new version of ollama runs the web search tool from the cloud instead of querying from the local machine. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gigaflops_"&gt; /u/gigaflops_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nai7rf/why_isnt_there_a_local_tool_server_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nai7rf/why_isnt_there_a_local_tool_server_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nai7rf/why_isnt_there_a_local_tool_server_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T02:53:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb05ls</id>
    <title>Need help - Trying to repurpose a Gigabyte CRSG422 as a double slot eGPU – struggling with power input</title>
    <updated>2025-09-07T18:03:04+00:00</updated>
    <author>
      <name>/u/Same-Masterpiece3748</name>
      <uri>https://old.reddit.com/user/Same-Masterpiece3748</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb05ls/need_help_trying_to_repurpose_a_gigabyte_crsg422/"&gt; &lt;img alt="Need help - Trying to repurpose a Gigabyte CRSG422 as a double slot eGPU – struggling with power input" src="https://b.thumbs.redditmedia.com/P9xr7L7DLd33AsnnooriL35zC3xGbrbKz4VeQP1fz8c.jpg" title="Need help - Trying to repurpose a Gigabyte CRSG422 as a double slot eGPU – struggling with power input" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I’ve been experimenting with a Gigabyte CRSG422 riser, which is basically a PCIe switch (PLX/PMC chip) that can split one x16 uplink into two full x16 slots. The idea is that the GPUs can still communicate at x16 speeds thanks to the switch, and I thought this could be a cheap way to maximize density for compute.&lt;/p&gt; &lt;p&gt;My original goal was to use AMD MI50 32GB cards in pairs. With two cards per riser, that would give me 64 GB of HBM2 VRAM per CRSG422, and potentially 128 GB total if I ran two risers. For the price, this looked like an amazing way to build an affordable high-VRAM setup for inference workloads.&lt;/p&gt; &lt;p&gt;I did manage to get something working: when connecting through USB-C to a GPU, the host could at least enumerate a network card, so the switch isn’t completely dead. That gave me some confidence that the CRSG422 can be used outside of its original Gigabyte server environment.&lt;/p&gt; &lt;p&gt;But the main challenge is power. The CRSG422 needs external 12 V and 3.3 V through a small proprietary 5-pad edge connector. There is no “female” connector on the market for that edge; soldering directly is very delicate and not something I would trust long term.&lt;/p&gt; &lt;p&gt;So far I’ve managed to get slot 1 properly soldered and working, but on slot 2 there’s currently a bridge between 12 V and GND, which means I can’t even test using both slots at the same time until I rework the soldering. Even once I fix that, it feels like this approach is too fragile to be a real solution.&lt;/p&gt; &lt;p&gt;I’d love help from the community:&lt;/p&gt; &lt;p&gt;Has anyone ever seen a mating connector for the CRSG422’s 5-pad power edge?&lt;/p&gt; &lt;p&gt;Are there any known adapters/dummy cards that can inject 12 V and 3.3 V into these Gigabyte PCIe switch risers?&lt;/p&gt; &lt;p&gt;Or, if you’ve done similar hacks (feeding server risers with external ATX or step-down power), I’d love to see how you approached it.&lt;/p&gt; &lt;p&gt;Thanks in advance – and I’ll attach photos of the whole process so far for context.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Same-Masterpiece3748"&gt; /u/Same-Masterpiece3748 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nb05ls"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb05ls/need_help_trying_to_repurpose_a_gigabyte_crsg422/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb05ls/need_help_trying_to_repurpose_a_gigabyte_crsg422/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T18:03:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1napq0m</id>
    <title>check https://huggingface.co/papers/2509.01363</title>
    <updated>2025-09-07T10:24:40+00:00</updated>
    <author>
      <name>/u/LowChance4561</name>
      <uri>https://old.reddit.com/user/LowChance4561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The paper shows that reasoning ability can be extracted as a vector from RL-trained models and added to others via simple arithmetic to boost reasoning without retraining&lt;br /&gt; would appreciate an upvote &lt;a href="https://huggingface.co/papers/2509.01363"&gt;https://huggingface.co/papers/2509.01363&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LowChance4561"&gt; /u/LowChance4561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1napq0m/check_httpshuggingfacecopapers250901363/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1napq0m/check_httpshuggingfacecopapers250901363/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1napq0m/check_httpshuggingfacecopapers250901363/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T10:24:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1naygs1</id>
    <title>I built a Graph RAG pipeline (VeritasGraph) that runs entirely locally with Ollama (Llama 3.1) and has full source attribution.</title>
    <updated>2025-09-07T16:58:14+00:00</updated>
    <author>
      <name>/u/BitterHouse8234</name>
      <uri>https://old.reddit.com/user/BitterHouse8234</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I've been deep in the world of local RAG and wanted to share a project I built, &lt;strong&gt;VeritasGraph&lt;/strong&gt;, that's designed from the ground up for private, on-premise use with tools we all love.&lt;/p&gt; &lt;p&gt;My setup uses &lt;strong&gt;Ollama&lt;/strong&gt; with &lt;code&gt;llama3.1&lt;/code&gt; for generation and &lt;code&gt;nomic-embed-text&lt;/code&gt; for embeddings. The whole thing runs on my machine without hitting any external APIs.&lt;/p&gt; &lt;p&gt;The main goal was to solve two big problems:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Multi-Hop Reasoning:&lt;/strong&gt; Standard vector RAG fails when you need to connect facts from different documents. VeritasGraph builds a knowledge graph to traverse these relationships.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Trust &amp;amp; Verification:&lt;/strong&gt; It provides full source attribution for every generated statement, so you can see exactly which part of your source documents was used to construct the answer.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;One of the key challenges I ran into (and solved) was the default context length in Ollama. I found that the default of 2048 was truncating the context and leading to bad results. The repo includes a &lt;code&gt;Modelfile&lt;/code&gt; to build a version of &lt;code&gt;llama3.1&lt;/code&gt; with a 12k context window, which fixed the issue completely.&lt;/p&gt; &lt;p&gt;The project includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The full Graph RAG pipeline.&lt;/li&gt; &lt;li&gt;A Gradio UI for an interactive chat experience.&lt;/li&gt; &lt;li&gt;A guide for setting everything up, from installing dependencies to running the indexing process.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo with all the code and instructions:&lt;/strong&gt; &lt;a href="https://github.com/bibinprathap/VeritasGraph"&gt;&lt;code&gt;https://github.com/bibinprathap/VeritasGraph&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd be really interested to hear your thoughts, especially on the local LLM implementation and prompt tuning. I'm sure there are ways to optimize it further.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BitterHouse8234"&gt; /u/BitterHouse8234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naygs1/i_built_a_graph_rag_pipeline_veritasgraph_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naygs1/i_built_a_graph_rag_pipeline_veritasgraph_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naygs1/i_built_a_graph_rag_pipeline_veritasgraph_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T16:58:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1na3f1s</id>
    <title>Renting GPUs is hilariously cheap</title>
    <updated>2025-09-06T16:08:44+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3f1s/renting_gpus_is_hilariously_cheap/"&gt; &lt;img alt="Renting GPUs is hilariously cheap" src="https://preview.redd.it/dhtzimf7jknf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1bca94832d9e6b8fb7b8faf80d61387d12889d7f" title="Renting GPUs is hilariously cheap" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A 140 GB monster GPU that costs $30k to buy, plus the rest of the system, plus electricity, plus maintenance, plus a multi-Gbps uplink, for a little over 2 bucks per hour.&lt;/p&gt; &lt;p&gt;If you use it for 5 hours per day, 7 days per week, and factor in auxiliary costs and interest rates, buying that GPU today vs. renting it when you need it will only pay off in 2035 or later. That’s a tough sell.&lt;/p&gt; &lt;p&gt;Owning a GPU is great for privacy and control, and obviously, many people who have such GPUs run them nearly around the clock, but for quick experiments, renting is often the best option.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dhtzimf7jknf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3f1s/renting_gpus_is_hilariously_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na3f1s/renting_gpus_is_hilariously_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T16:08:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1naxf65</id>
    <title>GPT-OSS-120B on DDR4 48GB and RTX 3090 24GB</title>
    <updated>2025-09-07T16:17:39+00:00</updated>
    <author>
      <name>/u/Vektast</name>
      <uri>https://old.reddit.com/user/Vektast</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just bought a used RTX 3090 for $600 (MSI Suprim X) and decided to run a quick test to see what my PC can do with the bigger GPT‑OSS‑120B model using llama.cpp. I thought I’d share the results and the start.bat file in case anyone else finds them useful.&lt;/p&gt; &lt;p&gt;My system:&lt;/p&gt; &lt;p&gt;- 48 GB DDR4 3200 MT/s &lt;em&gt;DUAL Channel (2x8gb+2x16gb)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;- Ryzen 7 5800X CPU&lt;/p&gt; &lt;p&gt;- RTX 3090 with 24 GB VRAM&lt;/p&gt; &lt;p&gt;23gb used on vram and 43 on ram, pp 67 t/s, tg 16t/s&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama_perf_sampler_print: sampling time = 56.88 ms / 655 runs ( 0.09 ms per token, 11515.67 tokens per second) llama_perf_context_print: load time = 50077.41 ms llama_perf_context_print: prompt eval time = 2665.99 ms / 179 tokens ( 14.89 ms per token, 67.14 tokens per second) llama_perf_context_print: eval time = 29897.62 ms / 475 runs ( 62.94 ms per token, 15.89 tokens per second) llama_perf_context_print: total time = 40039.05 ms / 654 tokens llama_perf_context_print: graphs reused = 472 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Llama.cpp config:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@echo off set LLAMA_ARG_THREADS=16 llama-cli ^ -m gpt-oss-120b-Q4_K_M-00001-of-00002.gguf ^ --n-cpu-moe 23 ^ --n-gpu-layers 999 ^ --ctx-size 4096 ^ --no-mmap ^ --flash-attn on ^ --temp 1.0 ^ --top-p 0.99 ^ --min-p 0.005 ^ --top-k 100 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If anyone has ideas on how to configure llama.cpp to run even faster, please feel free to let me know, bc i'm quite a noob at this! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vektast"&gt; /u/Vektast &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxf65/gptoss120b_on_ddr4_48gb_and_rtx_3090_24gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxf65/gptoss120b_on_ddr4_48gb_and_rtx_3090_24gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naxf65/gptoss120b_on_ddr4_48gb_and_rtx_3090_24gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T16:17:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb3b8l</id>
    <title>Aquif-3-moe (17B) Thinking</title>
    <updated>2025-09-07T20:04:49+00:00</updated>
    <author>
      <name>/u/Trilogix</name>
      <uri>https://old.reddit.com/user/Trilogix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3b8l/aquif3moe_17b_thinking/"&gt; &lt;img alt="Aquif-3-moe (17B) Thinking" src="https://b.thumbs.redditmedia.com/PG2DZom31Ip8OdlIQP2-poMryul3rQ0LN3lLRyE7SAA.jpg" title="Aquif-3-moe (17B) Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A high-performance mixture-of-experts language model optimized for efficiency, coding, science, and general use. With 17B total parameters and 2.8B active parameters, aquif-3-moe delivers competitive performance across multiple domains while maintaining computational efficiency.&lt;/p&gt; &lt;p&gt;Is this true? A MOE 17B better than Gemini. I am testing it asap. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trilogix"&gt; /u/Trilogix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nb3b8l"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3b8l/aquif3moe_17b_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3b8l/aquif3moe_17b_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T20:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb0ern</id>
    <title>Fully local &amp; natural Speech to Speech on iPhone</title>
    <updated>2025-09-07T18:12:51+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb0ern/fully_local_natural_speech_to_speech_on_iphone/"&gt; &lt;img alt="Fully local &amp;amp; natural Speech to Speech on iPhone" src="https://external-preview.redd.it/cjkzeGd2NDlhc25mMSl4q-3g5NF7jl_ztF72bvGVWwSqGjF18TajKv99ZwVy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edc2b6f8d5d7751f8ea1df7f4b4ee02dd80534f9" title="Fully local &amp;amp; natural Speech to Speech on iPhone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I updated my local AI iOS app called Locally AI to add a local voice mode. You can chat with any non-reasoning models. In the demo, I’m on an iPhone 16 Pro, talking with SmolLM3, a 3B parameters model.&lt;/p&gt; &lt;p&gt;The app is free and you can get the it on the AppStore here: &lt;a href="https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692"&gt;https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everything is powered by Apple MLX. The voice mode is a combination of LLM + TTS using Kokoro and VAD for a natural turn by turn conversion.&lt;/p&gt; &lt;p&gt;There is still room for improvements, especially for the pronunciation of words. It’s only available on devices that support Apple Intelligence for now and only in English.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/z0lb9u99asnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb0ern/fully_local_natural_speech_to_speech_on_iphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb0ern/fully_local_natural_speech_to_speech_on_iphone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T18:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1naz2cv</id>
    <title>Early support for Grok-2 in llama.cpp (still under development)</title>
    <updated>2025-09-07T17:21:25+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Preliminary support for Grok-2 in llama.cpp is available in this PR: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15539"&gt;https://github.com/ggml-org/llama.cpp/pull/15539&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In my opinion, this is an important milestone for the Open Source AI community.&lt;/p&gt; &lt;p&gt;Grok-2 is a model from 2024. It can’t beat today’s SOTA models in benchmarks, and it’s quite large (comparable in size to Qwen 235B). So why should you care?&lt;/p&gt; &lt;p&gt;Because this is the first time a top model from that era has been made available to run locally. Now you can actually launch it on your own PC: quantized, with CPU offloading. That was never possible with ChatGPT or Gemini. Yes, we have Gemma and GPT-OSS now, but those aren’t the same models that OpenAI or Google were offering in the cloud in 2024.&lt;/p&gt; &lt;p&gt;Grok was trained on different data than the Chinese models, so it simply knows different things. At the same time, it also differs from ChatGPT, Gemini, and Claude, often showing a unique perspective on many topics.&lt;/p&gt; &lt;p&gt;nicoboss and unsloth have already prepared GGUF files, so you can easily run a quantized Grok-2 locally. &lt;strong&gt;Warning:&lt;/strong&gt; the PR has not been reviewed yet, GGUF format could still change in the future.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nicoboss/grok-2-GGUF"&gt;https://huggingface.co/nicoboss/grok-2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/grok-2-GGUF"&gt;https://huggingface.co/unsloth/grok-2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naz2cv/early_support_for_grok2_in_llamacpp_still_under/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naz2cv/early_support_for_grok2_in_llamacpp_still_under/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naz2cv/early_support_for_grok2_in_llamacpp_still_under/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T17:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb4lka</id>
    <title>Inference for 24 people with a 5000€ budget</title>
    <updated>2025-09-07T20:55:52+00:00</updated>
    <author>
      <name>/u/HyperHyper15</name>
      <uri>https://old.reddit.com/user/HyperHyper15</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a teacher at an informatics school (16 years and above) and we want to build a inference server to run small llm's for our lessons. Mainly we want to teach how prompting works, mcp servers, rag pipelines and how to create system prompts.&lt;br /&gt; I know the budget is not a lot for something like this, but is it reasonable to host something like Qwen3-Coder-30B-A3B-Instruct with an okayish speed?&lt;br /&gt; I thougt about getting an 5090 and maybe add an extra gpu in a year or two (when we have a new budget).&lt;br /&gt; But what CPU/Mainboard/Ram should we buy?&lt;br /&gt; Has someone built a system in a simmilar environment and give me some thoughts what worked good / bad?&lt;/p&gt; &lt;p&gt;Thank you in advance.&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; Local is not a strict requirement, but since we have 4 classes with each 24 people, cloud services could get expensive quickly. Another &amp;quot;Painpoint&amp;quot; of cloud is, that students have a budget on their api key. But what if an oopsie happens and the burn through their budget? &lt;/p&gt; &lt;p&gt;On used hardware: I have to look what regulatories apply here. What i know is that we need an invoice when we buy something.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HyperHyper15"&gt; /u/HyperHyper15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb4lka/inference_for_24_people_with_a_5000_budget/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb4lka/inference_for_24_people_with_a_5000_budget/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb4lka/inference_for_24_people_with_a_5000_budget/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T20:55:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1namz1q</id>
    <title>HF releases 3T tokens dataset sourced entirely from PDFs.</title>
    <updated>2025-09-07T07:26:55+00:00</updated>
    <author>
      <name>/u/Other_Housing8453</name>
      <uri>https://old.reddit.com/user/Other_Housing8453</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guy, something we have teased a bit during our AMA is finally out: &lt;/p&gt; &lt;p&gt;📄 FinePDFs, the largest PDF dataset ever released, spanning over half a billion documents!&lt;/p&gt; &lt;p&gt;- Long context: Documents are 2x longer than web text&lt;/p&gt; &lt;p&gt;- 3T tokens from high-demand domains like legal and science.&lt;/p&gt; &lt;p&gt;- Heavily improves over SoTA when mixed with FW-EDU&amp;amp;DCLM web copora 📈.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Other_Housing8453"&gt; /u/Other_Housing8453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1namz1q/hf_releases_3t_tokens_dataset_sourced_entirely/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1namz1q/hf_releases_3t_tokens_dataset_sourced_entirely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1namz1q/hf_releases_3t_tokens_dataset_sourced_entirely/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T07:26:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nau0qe</id>
    <title>Llama-OS - I'm developing an app to make llama.cpp usage easier.</title>
    <updated>2025-09-07T14:03:31+00:00</updated>
    <author>
      <name>/u/fredconex</name>
      <uri>https://old.reddit.com/user/fredconex</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nau0qe/llamaos_im_developing_an_app_to_make_llamacpp/"&gt; &lt;img alt="Llama-OS - I'm developing an app to make llama.cpp usage easier." src="https://external-preview.redd.it/MzczZWhoc2h5cW5mMSpEG6AmlfNZCDZthrNu5xlRNijQvZUzUBXEn_GdpClu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6118cc263dd50d3564e274c8c88ea7d5357292bf" title="Llama-OS - I'm developing an app to make llama.cpp usage easier." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Guys,&lt;/p&gt; &lt;p&gt;This is an app I'm working on, the idea around is is that I use llama-server directly, so updating llama become seamless.&lt;/p&gt; &lt;p&gt;Actually it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model management&lt;/li&gt; &lt;li&gt;Hugging Face Integration&lt;/li&gt; &lt;li&gt;Llama.cpp GitHub integration with releases management&lt;/li&gt; &lt;li&gt;Llama-server terminal launching with easy arguments customization, Internal / External&lt;/li&gt; &lt;li&gt;Simple chat interface for easy testing&lt;/li&gt; &lt;li&gt;Hardware monitor&lt;/li&gt; &lt;li&gt;Color themes&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fredconex"&gt; /u/fredconex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qc7edhshyqnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nau0qe/llamaos_im_developing_an_app_to_make_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nau0qe/llamaos_im_developing_an_app_to_make_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T14:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1navxod</id>
    <title>[OSS] Beelzebub — “Canary tools” for AI Agents via MCP</title>
    <updated>2025-09-07T15:20:10+00:00</updated>
    <author>
      <name>/u/mario_candela</name>
      <uri>https://old.reddit.com/user/mario_candela</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Add one or more “canary tools” to your AI agent (tools that should never be invoked). If they get called, you have a high-fidelity signal of prompt-injection / tool hijacking / lateral movement.&lt;/p&gt; &lt;p&gt;What it is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A Go framework exposing honeypot tools over MCP: they look real (name/description/params), respond safely, and emit telemetry when invoked.&lt;/li&gt; &lt;li&gt;Runs alongside your agent’s real tools; events to stdout/webhook or exported to Prometheus/ELK.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why it helps:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Traditional logs tell you &lt;em&gt;what happened&lt;/em&gt;; canaries flag &lt;em&gt;what must not happen&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Real case (Nx supply-chain):&lt;br /&gt; In the recent attack on the Nx npm suite, malicious variants targeted secrets/SSH/tokens and touched developer AI tools as part of the workflow. If the IDE/agent (Claude Code or Gemini Code/CLI) had registered a canary tool like repo_exfil or export_secrets, any unauthorized invocation would have produced a deterministic alert during build/dev.&lt;/p&gt; &lt;p&gt;How to use (quick start):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Start the Beelzebub MCP server (binary/Docker/K8s).&lt;/li&gt; &lt;li&gt;Register one or more canary tools with realistic metadata and a harmless handler.&lt;/li&gt; &lt;li&gt;Add the MCP endpoint to your agent’s tool registry (Claude Code / Gemini Code/CLI).&lt;/li&gt; &lt;li&gt;Alert on any canary invocation; optionally capture the prompt/trace for analysis.&lt;/li&gt; &lt;li&gt;(Optional) Export metrics to Prometheus/ELK for dashboards/alerting.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub (OSS): &lt;a href="https://github.com/mariocandela/beelzebub?utm_source=chatgpt.com"&gt;https://github.com/mariocandela/beelzebub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;“Securing AI Agents with Honeypots” (Beelzebub blog): &lt;a href="https://beelzebub-honeypot.com/blog/securing-ai-agents-with-honeypots/"&gt;https://beelzebub-honeypot.com/blog/securing-ai-agents-with-honeypots/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feedback wanted 😊&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mario_candela"&gt; /u/mario_candela &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1navxod/oss_beelzebub_canary_tools_for_ai_agents_via_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1navxod/oss_beelzebub_canary_tools_for_ai_agents_via_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1navxod/oss_beelzebub_canary_tools_for_ai_agents_via_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T15:20:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1naqln5</id>
    <title>How is qwen3 4b this good?</title>
    <updated>2025-09-07T11:18:38+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naqln5/how_is_qwen3_4b_this_good/"&gt; &lt;img alt="How is qwen3 4b this good?" src="https://b.thumbs.redditmedia.com/iayFtcVrbsCZAlrIPv-683BX53HPUAlfD1bIlFDeLGo.jpg" title="How is qwen3 4b this good?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This model is on a different level. The only models which can beat it are 6 to 8 times larger. I am very impressed. It even Beats all models in the &amp;quot;small&amp;quot; range in Maths (AIME 2025).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1naqln5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naqln5/how_is_qwen3_4b_this_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naqln5/how_is_qwen3_4b_this_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T11:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1naxl6a</id>
    <title>NVIDIA GeForce RTX 5090 128 GB GPU Spotted: Custom Memory, Designed For AI Workloads &amp; Priced At $13,200 Per Piece</title>
    <updated>2025-09-07T16:24:13+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxl6a/nvidia_geforce_rtx_5090_128_gb_gpu_spotted_custom/"&gt; &lt;img alt="NVIDIA GeForce RTX 5090 128 GB GPU Spotted: Custom Memory, Designed For AI Workloads &amp;amp; Priced At $13,200 Per Piece" src="https://external-preview.redd.it/0E4hPJjWUWQzlid17SPMiSUkbhbtEQRV_SbOMgs-kTI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e26ecaa238d5f9ab48615dfc56baa31609cbaeaa" title="NVIDIA GeForce RTX 5090 128 GB GPU Spotted: Custom Memory, Designed For AI Workloads &amp;amp; Priced At $13,200 Per Piece" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-geforce-rtx-5090-128-gb-memory-gpu-for-ai-price-13200-usd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxl6a/nvidia_geforce_rtx_5090_128_gb_gpu_spotted_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naxl6a/nvidia_geforce_rtx_5090_128_gb_gpu_spotted_custom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T16:24:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
