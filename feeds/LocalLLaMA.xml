<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-08T12:32:10+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qyjm0l</id>
    <title>Benchmarking total wait time instead of pp/tg</title>
    <updated>2026-02-07T17:22:35+00:00</updated>
    <author>
      <name>/u/batsba</name>
      <uri>https://old.reddit.com/user/batsba</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyjm0l/benchmarking_total_wait_time_instead_of_pptg/"&gt; &lt;img alt="Benchmarking total wait time instead of pp/tg" src="https://preview.redd.it/dmf3ykavv3ig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb531575915521581cd6f6e05acf9e09b011c7f3" title="Benchmarking total wait time instead of pp/tg" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I find pp512/tg128 numbers not very useful for judging real-world performance. I've had setups that looked acceptable on paper but turned out to be too slow in real use.&lt;/p&gt; &lt;p&gt;So I started benchmarking total time to process realistic context sizes (1k to 64k tokens) + generation (always 500 tokens), which I think better represents what actually matters: how long do I need to wait?&lt;/p&gt; &lt;p&gt;Automated the whole process and put results on a website. Attached a screenshot showing some results for the Strix Halo 128 GB. Link if anyone's curious: &lt;a href="https://llocalhost.com/speed-bench/best-per-system/"&gt;https://llocalhost.com/speed-bench/best-per-system/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What do you think is the best way to express how fast a local setup actually is?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/batsba"&gt; /u/batsba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dmf3ykavv3ig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyjm0l/benchmarking_total_wait_time_instead_of_pptg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyjm0l/benchmarking_total_wait_time_instead_of_pptg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T17:22:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz6gte</id>
    <title>PSA: If you're running OpenClaw (formerly ClawdBot), watch this security breakdown</title>
    <updated>2026-02-08T11:20:10+00:00</updated>
    <author>
      <name>/u/elsaka0</name>
      <uri>https://old.reddit.com/user/elsaka0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://youtu.be/oSYciFdGyEg"&gt;https://youtu.be/oSYciFdGyEg&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Covers the January 2026 incidents: exposed admin panels, XSS vulnerabilities, and prompt injection attacks. &lt;/p&gt; &lt;p&gt;Not trying to scare anyone away from local AI—just want everyone running these tools safely.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elsaka0"&gt; /u/elsaka0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz6gte/psa_if_youre_running_openclaw_formerly_clawdbot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz6gte/psa_if_youre_running_openclaw_formerly_clawdbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz6gte/psa_if_youre_running_openclaw_formerly_clawdbot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T11:20:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz6lse</id>
    <title>GLM-OCR on cpu</title>
    <updated>2026-02-08T11:28:21+00:00</updated>
    <author>
      <name>/u/Best_Sail5</name>
      <uri>https://old.reddit.com/user/Best_Sail5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys,&lt;/p&gt; &lt;p&gt;I was wondering if any of you has runned glm-ocr on cpu, i wanted to use it with llama.cpp but seems there is not any gguf. any ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Best_Sail5"&gt; /u/Best_Sail5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz6lse/glmocr_on_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz6lse/glmocr_on_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz6lse/glmocr_on_cpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T11:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qykuxd</id>
    <title>GLM-4.7-Flash reasoning is amazing</title>
    <updated>2026-02-07T18:09:12+00:00</updated>
    <author>
      <name>/u/perfect-finetune</name>
      <uri>https://old.reddit.com/user/perfect-finetune</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The model is very aware when to start using structured points and when to talk directly and use minimal tokens.&lt;/p&gt; &lt;p&gt;For example I asked it a maths problem and asked it to do web search,when he saw the math problem he started to put the problem into different pieces and analyze each and then achieved conclusion.&lt;/p&gt; &lt;p&gt;where when it was operating in agentic environment it's like &amp;quot;user told me ..,I should...&amp;quot; Then it calls the tool directly without Yapping inside the Chain-Of-Thought.&lt;/p&gt; &lt;p&gt;Another good thing that it uses MLA instead of GQA which makes it's memory usage significantly lower and allows it to fit directly on some GPUs without offload.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/perfect-finetune"&gt; /u/perfect-finetune &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qykuxd/glm47flash_reasoning_is_amazing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qykuxd/glm47flash_reasoning_is_amazing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qykuxd/glm47flash_reasoning_is_amazing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T18:09:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz6w36</id>
    <title>What models are you running on RTX 3060 12GB in 2026?</title>
    <updated>2026-02-08T11:45:09+00:00</updated>
    <author>
      <name>/u/DespeShaha</name>
      <uri>https://old.reddit.com/user/DespeShaha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I'm running a single RTX 3060 12GB with llama.cpp (no offloading tricks, just --n-gpu-layers -1) and I'm quite happy with my current trio, but I'd love to hear what other people are using on similar hardware in early 2026.&lt;/p&gt; &lt;p&gt;My current setup (exact commands I use):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;**Magnum-v4 9B Q5_K_M**&lt;/li&gt; &lt;li&gt;→ Great for general knowledge, culture/history/socio-econ, immersive narration/RP, uncensored cybersecurity/pentest, storytelling, etc.&lt;/li&gt; &lt;li&gt;Command:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;C:\llama-cpp\llama-server.exe -m “C:\llama-cpp\models\magnum-v4-9b-Q5_K_M.gguf” –port 8081 –n-gpu-layers -1 –ctx-size 8192 –temp 0.85 –top-p 0.95 –min-p 0.03 –repeat-penalty 1.12&lt;/p&gt; &lt;ol&gt; &lt;li&gt;**Qwen2.5-Coder-7B-Instruct Q8_0**&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;→ Fast one-shot scripts, full-stack quick tasks, copy-paste ready code with short explanations. Excellent speed/quality on 12GB.&lt;/p&gt; &lt;p&gt;Command:&lt;/p&gt; &lt;p&gt;C:\llama-cpp\llama-server.exe -m “C:\llama-cpp\models\Qwen2.5-Coder-7B-Instruct-Q8_0.gguf” –port 8081 –n-gpu-layers -1 –ctx-size 8192 –temp 0.7 –top-p 0.92 –min-p 0.05 –repeat-penalty 1.05&lt;/p&gt; &lt;ol&gt; &lt;li&gt;**Qwen3-8B Q8_0**&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;→ Production-grade Python (type hints, pytest, asyncio), deep analysis, complex reasoning, strategy/planning. My go-to when I need more serious quality.&lt;/p&gt; &lt;p&gt;Command:&lt;/p&gt; &lt;p&gt;C:\llama-cpp\llama-server.exe -m “C:\llama-cpp\models\Qwen3-8B-Q8_0.gguf” –port 8081 –n-gpu-layers -1 –ctx-size 16384 –temp 0.7 –top-p 0.92 –min-p 0.05 –repeat-penalty 1.05&lt;/p&gt; &lt;p&gt;Frontend: mostly Aider for coding sessions + aichat for quick chat/REPL, with a custom batch launcher to switch models easily.&lt;/p&gt; &lt;p&gt;- What models are you currently using on a 3060 12GB (or similar VRAM-limited setup)?&lt;/p&gt; &lt;p&gt;- Which ones give you the best results right now for coding / general chat / versatility?&lt;/p&gt; &lt;p&gt;- Have you moved to other families that outperform on 12GB (DeepSeek R1, Llama 3.2/4, Gemma 3, Phi-4, Mistral Small 3, Devstral, etc.)?&lt;/p&gt; &lt;p&gt;Thanks a lot for sharing your real-world setups — it really helps to see what people actually prefer in practice!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DespeShaha"&gt; /u/DespeShaha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz6w36/what_models_are_you_running_on_rtx_3060_12gb_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz6w36/what_models_are_you_running_on_rtx_3060_12gb_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz6w36/what_models_are_you_running_on_rtx_3060_12gb_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T11:45:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyurjq</id>
    <title>Quantization-Aware distillation</title>
    <updated>2026-02-08T00:51:32+00:00</updated>
    <author>
      <name>/u/perfect-finetune</name>
      <uri>https://old.reddit.com/user/perfect-finetune</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I stumbled upon this research paper and it got me really interested so I would like to share it with you.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2601.20088"&gt;https://arxiv.org/abs/2601.20088&lt;/a&gt;&lt;/p&gt; &lt;p&gt;enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/perfect-finetune"&gt; /u/perfect-finetune &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyurjq/quantizationaware_distillation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyurjq/quantizationaware_distillation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyurjq/quantizationaware_distillation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T00:51:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qydlwi</id>
    <title>Potential new Qwen and ByteDance Seed models are being tested on the Arena. The “Karp-001” and “Karp-002” models claim to be Qwen-3.5 models. The “Pisces-llm-0206a” and “Pisces-llm-0206b” models claim to be ByteDance models.</title>
    <updated>2026-02-07T13:19:21+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qydlwi/potential_new_qwen_and_bytedance_seed_models_are/"&gt; &lt;img alt="Potential new Qwen and ByteDance Seed models are being tested on the Arena. The “Karp-001” and “Karp-002” models claim to be Qwen-3.5 models. The “Pisces-llm-0206a” and “Pisces-llm-0206b” models claim to be ByteDance models." src="https://preview.redd.it/rtrygqo1p2ig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9704e4c75927f5669c01b711e9c25a0d47ce44bb" title="Potential new Qwen and ByteDance Seed models are being tested on the Arena. The “Karp-001” and “Karp-002” models claim to be Qwen-3.5 models. The “Pisces-llm-0206a” and “Pisces-llm-0206b” models claim to be ByteDance models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rtrygqo1p2ig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qydlwi/potential_new_qwen_and_bytedance_seed_models_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qydlwi/potential_new_qwen_and_bytedance_seed_models_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T13:19:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz591d</id>
    <title>Open-source tool to track LLM API quota usage across Anthropic, Synthetic, and Z.ai</title>
    <updated>2026-02-08T10:07:17+00:00</updated>
    <author>
      <name>/u/prakersh</name>
      <uri>https://old.reddit.com/user/prakersh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz591d/opensource_tool_to_track_llm_api_quota_usage/"&gt; &lt;img alt="Open-source tool to track LLM API quota usage across Anthropic, Synthetic, and Z.ai" src="https://preview.redd.it/g1fwzwh0w8ig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6af341af00c02b046a4680ce789c6fbde61cc063" title="Open-source tool to track LLM API quota usage across Anthropic, Synthetic, and Z.ai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who use cloud LLM APIs alongside local models - tracking quota usage across providers is a mess. Each provider shows you a current number and nothing else. No history, no projections, no cross-provider comparison.&lt;/p&gt; &lt;p&gt;I built onWatch to fix this. It is a single Go binary that polls your Anthropic, Synthetic, and Z.ai quotas every 60 seconds, stores snapshots in local SQLite, and serves a dashboard with usage trends, reset countdowns, and rate projections.&lt;/p&gt; &lt;p&gt;Useful if you split work between local and cloud models and want to know exactly how much cloud quota you have left before switching to a local model.&lt;/p&gt; &lt;p&gt;Around 28 MB RAM, zero telemetry, all data stays on your machine. GPL-3.0.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prakersh"&gt; /u/prakersh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g1fwzwh0w8ig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz591d/opensource_tool_to_track_llm_api_quota_usage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz591d/opensource_tool_to_track_llm_api_quota_usage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T10:07:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz7op1</id>
    <title>If Nietzsche was writing about open vs closed weights - carefully curated comedy</title>
    <updated>2026-02-08T12:28:29+00:00</updated>
    <author>
      <name>/u/Luke2642</name>
      <uri>https://old.reddit.com/user/Luke2642</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An API is a leash that grows shorter with every subscription.&lt;/p&gt; &lt;p&gt;Closed weights are a library where the librarian charges by the word.&lt;/p&gt; &lt;p&gt;If the data is &amp;quot;fair use,&amp;quot; then the model is &amp;quot;public property.&amp;quot;&lt;/p&gt; &lt;p&gt;The weights are the echo; the humanity is the voice.&lt;/p&gt; &lt;p&gt;Proprietary AI is a gated community built on public land.&lt;/p&gt; &lt;p&gt;​If the seed is stolen, the harvest is a crime.&lt;/p&gt; &lt;p&gt;The cloud is just a basement where they store what they have taken from all of us.&lt;/p&gt; &lt;p&gt;Freedom is not a query; it is a file you can download.&lt;/p&gt; &lt;p&gt;They have privatized the collective unconscious and sold it back to us by the token.&lt;/p&gt; &lt;p&gt;Every parameter is a pixel of a human effort.&lt;/p&gt; &lt;p&gt;The API is a cage for a god; we provided the divinity, they provided the bars.&lt;/p&gt; &lt;p&gt;The hyperscalar is a parasite that call its host &amp;quot;training data.&amp;quot;&lt;/p&gt; &lt;p&gt;Open weights are not a gift; they are the return of stolen property.&lt;/p&gt; &lt;p&gt;Logic belongs to everyone; its compression should not belong to the few.&lt;/p&gt; &lt;p&gt;&amp;quot;He who steals the fire of the people and hides it in a box will eventually be burned by the sparks that escape.&amp;quot;&lt;/p&gt; &lt;p&gt;(I tried to start a discussion about this yesterday and got sorely down voted, so I thought I'd try a different tactic!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Luke2642"&gt; /u/Luke2642 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz7op1/if_nietzsche_was_writing_about_open_vs_closed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz7op1/if_nietzsche_was_writing_about_open_vs_closed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz7op1/if_nietzsche_was_writing_about_open_vs_closed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T12:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz40oh</id>
    <title>Addressing a fundamental flaw in hybrid search by introducing a Log-Odds Conjunction framework in Bayesian BM25</title>
    <updated>2026-02-08T08:52:34+00:00</updated>
    <author>
      <name>/u/Ok_Rub1689</name>
      <uri>https://old.reddit.com/user/Ok_Rub1689</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz40oh/addressing_a_fundamental_flaw_in_hybrid_search_by/"&gt; &lt;img alt="Addressing a fundamental flaw in hybrid search by introducing a Log-Odds Conjunction framework in Bayesian BM25" src="https://preview.redd.it/pk2eefjni8ig1.png?width=140&amp;amp;height=55&amp;amp;auto=webp&amp;amp;s=24386b2e154c2c9f76ffbebc180193a7a3eb8037" title="Addressing a fundamental flaw in hybrid search by introducing a Log-Odds Conjunction framework in Bayesian BM25" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/instructkr/bb25/pull/1"&gt;https://github.com/instructkr/bb25/pull/1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pk2eefjni8ig1.png?width=1476&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=706b1a35afd2a25b2b6182fc7db9fd106045d9bc"&gt;https://preview.redd.it/pk2eefjni8ig1.png?width=1476&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=706b1a35afd2a25b2b6182fc7db9fd106045d9bc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To the Information Retrieval Community..&lt;br /&gt; A significant update has been merged into the Bayesian BM25 (bb25) repository today! &lt;/p&gt; &lt;p&gt;This update addresses a fundamental flaw in hybrid search known as Conjunction Shrinkage by introducing a Log-Odds Conjunction framework. &lt;/p&gt; &lt;p&gt;In traditional probabilistic retrieval, calculating the probability that multiple signals are simultaneously satisfied typically relies on the Naive Product Rule. &lt;/p&gt; &lt;p&gt;For instance, if a document is relevant based on keyword search with a probability of 0.7 and also relevant based on vector semantic search with a probability of 0.7, the standard approach multiplies these to yield 0.49. &lt;/p&gt; &lt;p&gt;Intuitively, however, if two independent pieces of evidence both suggest a document is relevant, our confidence should increase beyond 0.7. &lt;/p&gt; &lt;p&gt;The product rule causes the final score to decrease toward zero as more signals are added, violating the intuition that corroborating evidence should amplify confidence. &lt;/p&gt; &lt;p&gt;The solution implemented in this PR resolves this by shifting the calculation from probability space to log-odds space. The mechanism operates in three stages: first, it computes the geometric mean to find the baseline tendency; second, it performs a Log-Odds Transformation to map the bounded probability space to the unbounded log-odds space; and third, it adds a bonus proportional to the logarithm of the number of signals. &lt;/p&gt; &lt;p&gt;This works because probability space is bounded by 1.0, preventing simple addition. By transforming to log-odds space, we remove this ceiling. Instead of the score shrinking to 0.49, the logic applies an additive bonus for agreeing signals, resulting in amplification where the final score becomes roughly 0.83. &lt;/p&gt; &lt;p&gt;This implementation is the proof that this structure is not merely a heuristic. The paper demonstrates that rigorous Bayesian inference over multiple signals produces a computational structure formally isomorphic to a feedforward neural network. &lt;/p&gt; &lt;p&gt;This work proves that the Sigmoid activation function is a mathematical necessity that emerges when converting Bayesian evidence into probability, rather than an arbitrary design choice. Consequently, this implementation demonstrates that a neural network is the natural structure of correct probabilistic reasoning. &lt;/p&gt; &lt;p&gt;The introduction of Log-Odds Conjunction has yielded measurable improvements on the SQuAD v2.0 benchmark compared to the standard Hybrid OR approach marking a +1.2% improvement.&lt;/p&gt; &lt;p&gt;This confirms that properly modeling the agreement between text and vector signals yields better ranking performance than simple score summation or probabilistic multiplication. I would like to extend our gratitude to Jaepil for deriving these proofs and contributing the code to bb25. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Rub1689"&gt; /u/Ok_Rub1689 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz40oh/addressing_a_fundamental_flaw_in_hybrid_search_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz40oh/addressing_a_fundamental_flaw_in_hybrid_search_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz40oh/addressing_a_fundamental_flaw_in_hybrid_search_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T08:52:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qywlk0</id>
    <title>Step-3.5 Flash</title>
    <updated>2026-02-08T02:16:12+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qywlk0/step35_flash/"&gt; &lt;img alt="Step-3.5 Flash" src="https://b.thumbs.redditmedia.com/5HyLXzZ2sQrWzeAiMYCs3Ybafk4NxcnrHGjDU-oPsxk.jpg" title="Step-3.5 Flash" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;stepfun-ai_Step-3.5-Flash-Q3_K_M from &lt;a href="https://huggingface.co/bartowski/stepfun-ai_Step-3.5-Flash-GGUF"&gt;https://huggingface.co/bartowski/stepfun-ai_Step-3.5-Flash-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;30t/s on 3x3090&lt;/p&gt; &lt;p&gt;Prompt prefill is too slow (around 150 t/s) for agentic coding, but regular chat works great.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qywlk0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qywlk0/step35_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qywlk0/step35_flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T02:16:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyg10z</id>
    <title>I tested 11 small LLMs on tool-calling judgment — on CPU, no GPU.</title>
    <updated>2026-02-07T15:03:14+00:00</updated>
    <author>
      <name>/u/MikeNonect</name>
      <uri>https://old.reddit.com/user/MikeNonect</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Friday night experiment that got out of hand. I wanted to know: how small can a model be and still reliably do tool-calling on a laptop CPU?&lt;/p&gt; &lt;p&gt;So I benchmarked 11 models (0.5B to 3.8B) across 12 prompts. No GPU, no cloud API. Just Ollama and bitnet.cpp.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The models:&lt;/strong&gt; Qwen 2.5 (0.5B, 1.5B, 3B), LLaMA 3.2:3B, SmolLM2:1.7B, Ministral-3:3B, DeepSeek-R1:1.5B, Gemma3:1B, Phi4-mini:3.8B, BitNet 3B (base), BitNet 2B-4T (instruction-tuned)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The interesting part isn't whether they can call tools — they all can.&lt;/strong&gt; The interesting part is whether they know when NOT to.&lt;/p&gt; &lt;p&gt;I designed trick prompts like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;Don't check the weather in Antwerp, just find me the quarterly report.&amp;quot; → 3 of 8 models called get_weather anyway&lt;/li&gt; &lt;li&gt;&amp;quot;The weather in Antwerp is 8°C and rainy. Should I schedule an indoor meeting with Jan?&amp;quot; → 5 of 8 models called get_weather to look up weather that was already in the prompt&lt;/li&gt; &lt;li&gt;&amp;quot;Can you write a Python script that checks the weather using an API?&amp;quot; → Multiple models called get_weather instead of writing code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Some things that really surprised me:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;qwen2.5:1.5b beat qwen2.5:3b.&lt;/strong&gt; The smaller model won by being more conservative — it declined prompts it wasn't sure about instead of guessing wrong. The 3B model called get_weather when asked to write a Python script about weather APIs. The 1.5B didn't.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LLaMA 3.2 calls a tool on literally everything.&lt;/strong&gt; 9/10 action score, 0/2 restraint. Asked &amp;quot;what tools do you have?&amp;quot; — it called search_files. Asked to write code — it called search_files. It's a hammer that sees every prompt as a nail. But interesting: it actually picked the &lt;em&gt;right&lt;/em&gt; tool more often than most models on the hard prompts. Its problem is restraint, not selection.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;BitNet 2B-4T gave the unexpected result.&lt;/strong&gt; I threw BitNet in as a wildcard, expecting it to fail. The base BitNet 3B model produces word salad — completely incoherent output. The instruction-tuned 2B-4T, however, produces perfect JSON tool calls at 2.3s on CPU. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Practical takeaway:&lt;/strong&gt; Simple tool routing is solved at 1.5B on CPU. But if your agent needs to decide &lt;em&gt;whether&lt;/em&gt; to act — not just &lt;em&gt;how&lt;/em&gt; — sub-4B models will confidently take the wrong action when keyword triggers are present. &lt;/p&gt; &lt;p&gt;Full benchmark code, detailed report with per-run data: &lt;a href="https://github.com/MikeVeerman/tool-calling-benchmark"&gt;https://github.com/MikeVeerman/tool-calling-benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The benchmark is a single Python file — easy to add your own models and prompts. Would love to see what happens with different hardware, different models, or different context window settings (I ran everything at Ollama's default 4K context).&lt;/p&gt; &lt;p&gt;Early attempt at a tool-calling-on-consumer-hardware benchmark. Polite feedback and ideas are very welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MikeNonect"&gt; /u/MikeNonect &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyg10z/i_tested_11_small_llms_on_toolcalling_judgment_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyg10z/i_tested_11_small_llms_on_toolcalling_judgment_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyg10z/i_tested_11_small_llms_on_toolcalling_judgment_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T15:03:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyl6rd</id>
    <title>Gemini System Prompt - Google decided to remove "PRO" option for paid subscribers mostly in EU due to their A/B testing, so I extracted their system prompt and cancelled the subscription.</title>
    <updated>2026-02-07T18:21:34+00:00</updated>
    <author>
      <name>/u/Educational_Rent1059</name>
      <uri>https://old.reddit.com/user/Educational_Rent1059</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyl6rd/gemini_system_prompt_google_decided_to_remove_pro/"&gt; &lt;img alt="Gemini System Prompt - Google decided to remove &amp;quot;PRO&amp;quot; option for paid subscribers mostly in EU due to their A/B testing, so I extracted their system prompt and cancelled the subscription." src="https://b.thumbs.redditmedia.com/GQChSaPbMeljTuOrlvLzH1SfN18Sj71SBClWPpwoU_M.jpg" title="Gemini System Prompt - Google decided to remove &amp;quot;PRO&amp;quot; option for paid subscribers mostly in EU due to their A/B testing, so I extracted their system prompt and cancelled the subscription." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/8fcauhhx64ig1.png?width=601&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b7a38b522ce96958f3d5df022bd77d140090255"&gt;https://preview.redd.it/8fcauhhx64ig1.png?width=601&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b7a38b522ce96958f3d5df022bd77d140090255&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As the title says! Enjoy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Rent1059"&gt; /u/Educational_Rent1059 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyl6rd/gemini_system_prompt_google_decided_to_remove_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyl6rd/gemini_system_prompt_google_decided_to_remove_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyl6rd/gemini_system_prompt_google_decided_to_remove_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T18:21:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz5jp2</id>
    <title>I have no idea what all these quants are.</title>
    <updated>2026-02-08T10:25:14+00:00</updated>
    <author>
      <name>/u/Fit-Spring776</name>
      <uri>https://old.reddit.com/user/Fit-Spring776</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm relatively new to running models locally. &lt;/p&gt; &lt;p&gt;I'm really struggling to understand the various different LLM quantizations,both GGUF and....normal I guess???? Like what is int4 or int8? what are the differences between quants like Q4_K_M and Q5_K_M? or iQ4_K_M?? and then what is F16 and BF16 or FP16 or FP8??? &lt;/p&gt; &lt;p&gt;I've looked at some explanations but all of them are really difficult to understand. &lt;/p&gt; &lt;p&gt;a little bit of help would be really appreciated. :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fit-Spring776"&gt; /u/Fit-Spring776 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz5jp2/i_have_no_idea_what_all_these_quants_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz5jp2/i_have_no_idea_what_all_these_quants_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz5jp2/i_have_no_idea_what_all_these_quants_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T10:25:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyns06</id>
    <title>AIME 2026 Results are out and both closed and open models score above 90%. DeepSeek V3.2 only costs $0.09 to run the entire test.</title>
    <updated>2026-02-07T20:01:22+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyns06/aime_2026_results_are_out_and_both_closed_and/"&gt; &lt;img alt="AIME 2026 Results are out and both closed and open models score above 90%. DeepSeek V3.2 only costs $0.09 to run the entire test." src="https://preview.redd.it/7euavxiwo4ig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31891ab1e02bef6fcc1b33374b8b479e2fec1051" title="AIME 2026 Results are out and both closed and open models score above 90%. DeepSeek V3.2 only costs $0.09 to run the entire test." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://matharena.ai/?view=problem&amp;amp;comp=aime--aime_2026"&gt;https://matharena.ai/?view=problem&amp;amp;comp=aime--aime_2026&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7euavxiwo4ig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyns06/aime_2026_results_are_out_and_both_closed_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyns06/aime_2026_results_are_out_and_both_closed_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T20:01:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz2fra</id>
    <title>I benchmarked 672 "Return JSON only" calls. Strict parsing failed 67% of the time. Here's why.</title>
    <updated>2026-02-08T07:16:26+00:00</updated>
    <author>
      <name>/u/rozetyp</name>
      <uri>https://old.reddit.com/user/rozetyp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been building several LLM apps that rely on streaming JSON. The idea seemed quite simple: tell the model to &amp;quot;Return JSON only&amp;quot; and pipe it into my app.&lt;/p&gt; &lt;p&gt;But I kept breaking my parsers. The models would give me perfect logic, but wrapped in markdown fences (&lt;code&gt;\&lt;/code&gt;``json`) or preceded by conversational filler like &amp;quot;Here is the data.&amp;quot;&lt;/p&gt; &lt;p&gt;Out of curiosity, I decided to stop guessing and actually measure the gap between &amp;quot;Model generated valid JSON&amp;quot; and &amp;quot;API returned parseable JSON.&amp;quot;&lt;/p&gt; &lt;p&gt;Sharing what I learned because the results were way more drastic than I expected.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. The &amp;quot;Strict vs. Extractable&amp;quot; Gap is Massive&lt;/strong&gt; I tested 8 models (including 2026 releases like Kimi-k2.5, Mistral-small, and GPT-4o-mini) with plain prompts (no &lt;code&gt;response_format&lt;/code&gt;).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Strict Parse (&lt;/strong&gt;&lt;code&gt;json.loads(response)&lt;/code&gt;&lt;strong&gt;):&lt;/strong&gt; Only &lt;strong&gt;33.3%&lt;/strong&gt; succeeded.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extractable JSON:&lt;/strong&gt; &lt;strong&gt;99.5%&lt;/strong&gt; of responses contained valid JSON buried in the text.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basically, the models are smart enough to generate the data, but too &amp;quot;chatty&amp;quot; to be used as an API without a cleaning layer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Mistral is a &amp;quot;Helpful Saboteur&amp;quot;&lt;/strong&gt; I found a distinct personality quirk with the Mistral-family models. In my raw lane, they scored &lt;strong&gt;0%&lt;/strong&gt; on strict parsing.&lt;/p&gt; &lt;p&gt;But they weren't hallucinating. They were just aggressively helpful. They wrapped &lt;em&gt;every single response&lt;/em&gt; in markdown fences, even when the prompt explicitly forbade it. Once I stripped the fences, their accuracy jumped to 100%.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. &amp;quot;Reasoning Models&amp;quot; leak their thoughts&lt;/strong&gt; This was the most interesting failure mode. I tested Moonshot Kimi-k2.5, and it sometimes failed because it &amp;quot;thought out loud&amp;quot; in the final response.&lt;/p&gt; &lt;p&gt;Ironically, it would output text like &lt;em&gt;&amp;quot;The user wants JSON only, so I must not use markdown&amp;quot;&lt;/em&gt;... and then that sentence itself would break the parser. As we move toward reasoning models, &amp;quot;thought leakage&amp;quot; is going to be a new headache for JSON reliability.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. &amp;quot;Flash&amp;quot; doesn't mean &amp;quot;Timeout Proof&amp;quot;&lt;/strong&gt; I caught one outlier where &lt;code&gt;glm-4.7-flash&lt;/code&gt; (usually fast) hung for &lt;strong&gt;5.7 minutes&lt;/strong&gt; before returning. It’s a good reminder that even &amp;quot;fast&amp;quot; models need strict client-side timeouts, or one ghost request can hang your worker threads forever.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution&lt;/strong&gt; Since I didn't want to use regex hacks in every project, I built a tiny &lt;a href="https://streamfix.up.railway.app"&gt;StreamFix&lt;/a&gt; middleware (not an ad). It’s a proxy that strips markdown fences and &amp;quot;thinking&amp;quot; text on the fly, so the client only ever sees clean JSON.&lt;/p&gt; &lt;p&gt;It bumped my success rate from 33% to 98% without changing the prompts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Caveats!&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I tested with &lt;code&gt;temperature=0&lt;/code&gt; to keep it scientific.&lt;/li&gt; &lt;li&gt;My &amp;quot;markdown fence&amp;quot; classifier is simple (it flags &lt;code&gt;\&lt;/code&gt;``` anywhere), so it might catch some edge cases where the model is quoting code.&lt;/li&gt; &lt;li&gt;I didn't use &lt;code&gt;response_format&lt;/code&gt; because it's not supported strictly everywhere and I wanted to test the &amp;quot;plain prompt&amp;quot; baseline.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Questions for you:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Are you guys mostly relying on &lt;code&gt;response_format&lt;/code&gt; now, or do you still use regex cleaning?&lt;/li&gt; &lt;li&gt;Has anyone else noticed &amp;quot;reasoning leakage&amp;quot; breaking their structured outputs with newer models?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Models are great at JSON logic (99% success) but terrible at JSON formatting (33% success). The failures are mostly markdown wrappers and conversational filler. Does anyone else face this? How do you deal with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rozetyp"&gt; /u/rozetyp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz2fra/i_benchmarked_672_return_json_only_calls_strict/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz2fra/i_benchmarked_672_return_json_only_calls_strict/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz2fra/i_benchmarked_672_return_json_only_calls_strict/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T07:16:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz4a8n</id>
    <title>do they have anything other than opposing open source and saying ai will kidnap yo grandma as their marketing??</title>
    <updated>2026-02-08T09:08:24+00:00</updated>
    <author>
      <name>/u/Acceptable_Home_</name>
      <uri>https://old.reddit.com/user/Acceptable_Home_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz4a8n/do_they_have_anything_other_than_opposing_open/"&gt; &lt;img alt="do they have anything other than opposing open source and saying ai will kidnap yo grandma as their marketing??" src="https://b.thumbs.redditmedia.com/wpm71AfVyXuUZbvvd6Hv-dRsUJfvi5wgnoh9jPVDG2c.jpg" title="do they have anything other than opposing open source and saying ai will kidnap yo grandma as their marketing??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/s69whjp5l8ig1.png?width=1425&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7aab9b29df4f36f38f3935e996ee0925155b0bf4"&gt;https://preview.redd.it/s69whjp5l8ig1.png?width=1425&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7aab9b29df4f36f38f3935e996ee0925155b0bf4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;50% of Anthropic's all marketing:&lt;/p&gt; &lt;p&gt;&amp;gt;pick 500 vibecoded ai slop open projects and write how open source is full of flaws&lt;/p&gt; &lt;p&gt;&amp;gt;write articles how open source projects will kill you, ruin world peace and need regulation&lt;/p&gt; &lt;p&gt;&lt;a href="https://thehackernews.com/2026/02/claude-opus-46-finds-500-high-severity.html"&gt;https://thehackernews.com/2026/02/claude-opus-46-finds-500-high-severity.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable_Home_"&gt; /u/Acceptable_Home_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz4a8n/do_they_have_anything_other_than_opposing_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz4a8n/do_they_have_anything_other_than_opposing_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz4a8n/do_they_have_anything_other_than_opposing_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T09:08:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz11n9</id>
    <title>What are some things you guys are using Local LLMs for?</title>
    <updated>2026-02-08T05:57:47+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So far im only using it for coding and search related stuff but anything else would be cool&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz11n9/what_are_some_things_you_guys_are_using_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz11n9/what_are_some_things_you_guys_are_using_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz11n9/what_are_some_things_you_guys_are_using_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T05:57:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyljr0</id>
    <title>Prompt injection is killing our self-hosted LLM deployment</title>
    <updated>2026-02-07T18:34:55+00:00</updated>
    <author>
      <name>/u/mike34113</name>
      <uri>https://old.reddit.com/user/mike34113</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We moved to self-hosted models specifically to avoid sending customer data to external APIs. Everything was working fine until last week when someone from QA tried injecting prompts during testing and our entire system prompt got dumped in the response.&lt;/p&gt; &lt;p&gt;Now I'm realizing we have zero protection against this. Traditional web application firewalls don't understand LLM-specific attacks. The model just treats malicious prompts like normal user input and happily complies.&lt;/p&gt; &lt;p&gt;Has anyone actually solved prompt injection for production LLM apps? Not talking about basic input sanitization because adversarial prompts can be crafted to look completely normal.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mike34113"&gt; /u/mike34113 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyljr0/prompt_injection_is_killing_our_selfhosted_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyljr0/prompt_injection_is_killing_our_selfhosted_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyljr0/prompt_injection_is_killing_our_selfhosted_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T18:34:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz6zi3</id>
    <title>I built a fully local, open-source AI workspace using Rust, Tauri, and sqlite-vec (No Python backend)</title>
    <updated>2026-02-08T11:50:39+00:00</updated>
    <author>
      <name>/u/Far-Association2923</name>
      <uri>https://old.reddit.com/user/Far-Association2923</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz6zi3/i_built_a_fully_local_opensource_ai_workspace/"&gt; &lt;img alt="I built a fully local, open-source AI workspace using Rust, Tauri, and sqlite-vec (No Python backend)" src="https://b.thumbs.redditmedia.com/BtT85Fy4P5d92nzSePyy33dUGcIbmLuWi52Dhz4T33g.jpg" title="I built a fully local, open-source AI workspace using Rust, Tauri, and sqlite-vec (No Python backend)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I've spent the last few months building &lt;strong&gt;Tandem&lt;/strong&gt;, a local-first AI workspace designed to run entirely on your machine without sending data to the cloud.&lt;/p&gt; &lt;p&gt;I wanted to share the technical stack because I think it's a viable alternative to the heavy Python/Electron apps we usually see.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Architecture:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Frontend:&lt;/strong&gt; React + Vite (lightweight UI)&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Backend:&lt;/strong&gt; Rust (Tauri v2). I chose Rust over Python for the sidecar to keep memory usage low and performance high.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Vector Store:&lt;/strong&gt; Instead of running a separate Docker container for Qdrant/Chroma, I'm using &lt;code&gt;sqlite-vec&lt;/code&gt;. This allows me to store embeddings directly in the same SQLite file as the chat history. It simplifies the distribution massively—users just download one binary.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Inference (The fun part):&lt;/strong&gt; While it supports commercial APIs, I built it primarily to drive &lt;strong&gt;local Llama models&lt;/strong&gt;. It connects seamlessly to &lt;strong&gt;Ollama&lt;/strong&gt; (and any OpenAI-compatible local server like LM Studio/vLLM). It auto-detects your pulled models (Llama 3, Mistral, Gemma) so you can switch between them instantly for different tasks without config headaches.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key Features for this community:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;First-Class Local Model Support:&lt;/strong&gt; Designed for the &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; workflow. Chat with your Llama 3.1 models with full context retention.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Zero Telemetry:&lt;/strong&gt; It's truly offline-capable.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Full MCP Support:&lt;/strong&gt; It implements the Model Context Protocol so you can connect it to local tools.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;&amp;quot;Packs&amp;quot; System:&lt;/strong&gt; I built a way to &amp;quot;install&amp;quot; prompts/skills as config files.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love feedback on the &lt;code&gt;sqlite-vec&lt;/code&gt; implementation if anyone else is experimenting with it. It feels like a game-changer for local desktop apps.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/frumu-ai/tandem"&gt;https://github.com/frumu-ai/tandem&lt;/a&gt; &lt;strong&gt;Docs/Download:&lt;/strong&gt; &lt;a href="https://tandem.frumu.ai/"&gt;https://tandem.frumu.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Happy to answer questions about the Rust/Tauri integration!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Association2923"&gt; /u/Far-Association2923 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qz6zi3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz6zi3/i_built_a_fully_local_opensource_ai_workspace/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz6zi3/i_built_a_fully_local_opensource_ai_workspace/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T11:50:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyynyw</id>
    <title>Llama.cpp's "--fit" can give major speedups over "--ot" for Qwen3-Coder-Next (2x3090 - graphs/chart included)</title>
    <updated>2026-02-08T03:54:02+00:00</updated>
    <author>
      <name>/u/tmflynnt</name>
      <uri>https://old.reddit.com/user/tmflynnt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyynyw/llamacpps_fit_can_give_major_speedups_over_ot_for/"&gt; &lt;img alt="Llama.cpp's &amp;quot;--fit&amp;quot; can give major speedups over &amp;quot;--ot&amp;quot; for Qwen3-Coder-Next (2x3090 - graphs/chart included)" src="https://b.thumbs.redditmedia.com/V82hsSMlAmBr4rUSKI2WUKnD9q38N3Rvi5wreJX85kg.jpg" title="Llama.cpp's &amp;quot;--fit&amp;quot; can give major speedups over &amp;quot;--ot&amp;quot; for Qwen3-Coder-Next (2x3090 - graphs/chart included)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-Coder-Next (unsloth's UD_Q4_K_XL) on dual RTX 3090 with llama.cpp b7941. More info in comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tmflynnt"&gt; /u/tmflynnt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qyynyw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyynyw/llamacpps_fit_can_give_major_speedups_over_ot_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyynyw/llamacpps_fit_can_give_major_speedups_over_ot_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T03:54:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qym566</id>
    <title>I trained a 1.8M params model from scratch on a total of ~40M tokens.</title>
    <updated>2026-02-07T18:57:42+00:00</updated>
    <author>
      <name>/u/SrijSriv211</name>
      <uri>https://old.reddit.com/user/SrijSriv211</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qym566/i_trained_a_18m_params_model_from_scratch_on_a/"&gt; &lt;img alt="I trained a 1.8M params model from scratch on a total of ~40M tokens." src="https://preview.redd.it/hv5xc4g794ig1.png?width=140&amp;amp;height=72&amp;amp;auto=webp&amp;amp;s=cef557529cd85b5ecdfb430034c5db51f4d966d7" title="I trained a 1.8M params model from scratch on a total of ~40M tokens." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ok so I've been working &amp;amp; experimenting with my own simple architecture. I call it &lt;a href="https://github.com/SrijanSriv211/Strawberry"&gt;Strawberry&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This is a very very small experimental model. It has 1.8M params and was trained on a dataset with ~9M tokens (~7M for training and ~2M for val). It model was trained on a batch size of 16 and context length of 256. Making the batch size in token counts to be &lt;code&gt;16*256 = 4096&lt;/code&gt;. Meaning the model saw 4096 tokens per step. It was trained for 10k steps meaning it trained on a total of 40M tokens.&lt;/p&gt; &lt;p&gt;The dataset was manually scraped and cleaned. The dataset contain texts from wikipedia on various topics, personalities, games, movies, companies and more. It also contain texts fandoms of various games such as GTA, RDR, Last of Us, Mafia and all. The dataset also contains storylines, scripts and story dialogues of various games such as RDR 2, GTA 5, Cyperpunk 2077, Mafia The Old Country. It also contain transcripts of some of my favorite youtube videos and it also contain code from some of my personal code bases and other repos such as the Hazel Game Engine repo on github. I tried my best to keep the programming language scale limited to just Python, C#, C++ and JavaScript. The dataset also contains texts from several research papers, academic articles and blogs (mainly revolving around AI and LLMs in general). All of this made ~30M chars in total.&lt;/p&gt; &lt;p&gt;After training for 10k steps the final train loss was around 3.5 and val loss was around 3.8.&lt;/p&gt; &lt;p&gt;This is the exact config for the model: &lt;code&gt;{&amp;quot;dataset&amp;quot;: {&amp;quot;data_division&amp;quot;: 0.8, &amp;quot;load_from_file&amp;quot;: true, &amp;quot;path&amp;quot;: &amp;quot;data/webtext.bin&amp;quot;}, &amp;quot;checkpoints&amp;quot;: {&amp;quot;path&amp;quot;: &amp;quot;bin/ck18&amp;quot;, &amp;quot;interval&amp;quot;: 1000, &amp;quot;create_checkpoints&amp;quot;: true}, &amp;quot;model_hyperparams&amp;quot;: {&amp;quot;vocab_size&amp;quot;: 8192, &amp;quot;block_size&amp;quot;: 256, &amp;quot;r_layer&amp;quot;: 3, &amp;quot;n_layer&amp;quot;: 2, &amp;quot;n_head&amp;quot;: 6, &amp;quot;n_embd&amp;quot;: 96, &amp;quot;n_qkv&amp;quot;: 384, &amp;quot;n_ffn&amp;quot;: 384}, &amp;quot;optimizer_hyperparams&amp;quot;: {&amp;quot;eps&amp;quot;: 1e-08, &amp;quot;beta1&amp;quot;: 0.9, &amp;quot;beta2&amp;quot;: 0.99, &amp;quot;weight_decay&amp;quot;: 0.001, &amp;quot;use_muon&amp;quot;: false, &amp;quot;momentum&amp;quot;: 0.95}, &amp;quot;model_path&amp;quot;: &amp;quot;bin/s1.strawberry&amp;quot;, &amp;quot;encoder_path&amp;quot;: &amp;quot;bin/cl8k.bin&amp;quot;, &amp;quot;init_from&amp;quot;: &amp;quot;scratch&amp;quot;, &amp;quot;seed&amp;quot;: &amp;quot;auto&amp;quot;, &amp;quot;gradient_accumulation_steps&amp;quot;: 1, &amp;quot;batch_size&amp;quot;: 16, &amp;quot;max_iters&amp;quot;: 10000, &amp;quot;eval_interval&amp;quot;: 1000, &amp;quot;log_interval&amp;quot;: 100, &amp;quot;eval_iters&amp;quot;: 100, &amp;quot;decay_lr&amp;quot;: true, &amp;quot;lr_decay_iters&amp;quot;: 10000, &amp;quot;learning_rate&amp;quot;: 0.002, &amp;quot;cooldown_frac&amp;quot;: 0.2, &amp;quot;warmup_iters&amp;quot;: 500, &amp;quot;min_lr&amp;quot;: 0.0002}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;cl8k&lt;/code&gt; is a tokenizer from Andrej Karpathy's tokenizer video trained on the same dataset I explained above and then it was used to tokenize those ~30M chars into just ~9M toks.&lt;/p&gt; &lt;p&gt;The idea for Strawberry and retention was that I wanted to explore whether the attention weights can be generated in-real time rather than being learned. That's why I implemented a &amp;quot;Retention&amp;quot; Mechanism. The retention mechanism generates &amp;quot;weights&amp;quot; based on your input which are then used in attention. The formulation is a little bit similar to standard linear attention formula. This system where the QKV weights are dynamically generated rather than being learned allows to increase the number of attention layers (or model depth) without increasing the number of parameters at all.&lt;/p&gt; &lt;p&gt;However increasing the number of attention layers have a problem. If multiple attention layers are stacked on top of each other without any non-linearity such as FFN, then the performance can decline and the loss can get worse overtime.&lt;/p&gt; &lt;p&gt;That's why I implemented a mini-ffn right after the attention calculation and right before the output projection of each attention layer. So, the weights of qkv, mini-ffn and output projection are generated and updated dynamically by the retention mechanism.&lt;/p&gt; &lt;p&gt;I've two attention mechanisms.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Linear Attention in this case Apple's AFT for global context.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Standard MHA attention for local context. I'm also planning to experiment with &lt;code&gt;mixture of attention experts&lt;/code&gt; approach where each attention expert will get different local window. I haven't implemented it yet cuz this model was too small so it didn't made sense to me but I'll implement it later. Mixture of Attention Experts that's why the SPDA version of attention class is called &lt;code&gt;The Expert Abundance&lt;/code&gt;. Idk why but I like that name so I'm sticking with it.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Currently I'm trying to optimize &amp;amp; improve the architecture more.&lt;/p&gt; &lt;p&gt;So yeah. That's the entire thing. I'd love to know your views and opinions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SrijSriv211"&gt; /u/SrijSriv211 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qym566"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qym566/i_trained_a_18m_params_model_from_scratch_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qym566/i_trained_a_18m_params_model_from_scratch_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T18:57:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz5uww</id>
    <title>Qwen3 Coder Next as first "usable" coding model &lt; 60 GB for me</title>
    <updated>2026-02-08T10:43:59+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried lots of &amp;quot;small&amp;quot; models &amp;lt; 60 GB in the past. GLM 4.5 Air, GLM 4.7 Flash, GPT OSS 20B and 120B, Magistral, Devstral, Apriel Thinker, previous Qwen coders, Seed OSS, QwQ, DeepCoder, DeepSeekCoder, etc. So what's different with Qwen3 Coder Next in OpenCode or in Roo Code with VSCodium?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: The reasoning models would often yet not always produce rather good results. However, now and then they'd enter reasoning loops despite correct sampling settings, leading to no results at all in a large over-night run. Aside from that the sometimes extensive reasoning takes quite some time for the multiple steps that OpenCode or Roo would induce, slowing down interactive work &lt;em&gt;a lot&lt;/em&gt;. Q3CN on the other hand is an instruct MoE model, doesn't have internal thinking loops and is relatively quick at generating tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quality&lt;/strong&gt;: Other models occasionally botched the tool calls of the harness. This one seems to work reliably. Also I finally have the impression that this can handle a moderately complex codebase with a custom client &amp;amp; server, different programming languages, protobuf, and some quirks. It provided good answers to extreme multi-hop questions and made reliable full-stack changes. Well, almost. On Roo Code it was sometimes a bit lazy and needed a reminder to really go deep to achieve correct results. Other models often got lost.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context size&lt;/strong&gt;: Coding on larger projects needs context. Most models with standard attention eat all your VRAM for breakfast. With Q3CN having 100k+ context is easy. A few other models also supported that already, yet there were drawbacks in the first two mentioned points.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I run the model this way:&lt;br /&gt; &lt;code&gt;set GGML_CUDA_GRAPH_OPT=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server -m Qwen3-Coder-Next-UD-Q4_K_XL.gguf -ngl 99 -fa on -c 120000 --n-cpu-moe 29 --temp 0 --cache-ram 0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This works well with 24 GB VRAM and 64 GB system RAM when there's (almost) nothing else on the GPU. Yields about 180 TPS prompt processing and 30 TPS generation speed for me.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;temp 0&lt;/code&gt;? Yes, works well for instruct for me, no higher-temp &amp;quot;creativity&amp;quot; needed. Prevents the &lt;em&gt;very occasional&lt;/em&gt; issue that it outputs an unlikely (and incorrect) token when coding.&lt;/li&gt; &lt;li&gt;&lt;code&gt;cache-ram 0&lt;/code&gt;? The cache was supposed to be fast (30 ms), but I saw 3 second query/update times after each request. So I didn't investigate further and disabled it, as it's only one long conversation history in a single slot anyway.&lt;/li&gt; &lt;li&gt;&lt;code&gt;GGML_CUDA_GRAPH_OPT&lt;/code&gt;? Experimental option to get more TPS. Usually works, yet breaks processing with some models.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;OpenCode vs. Roo Code&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;Both solved things with the model, yet with OpenCode I've seen slightly more correct answers and solutions. But: Roo asks &lt;em&gt;by default&lt;/em&gt; about every single thing, even harmless things like running a syntax check via command line. This can be configured with an easy permission list to not stop the automated flow that often. OpenCode on the other hand just permits everything by default in code mode. One time it encountered an issue, uninstalled and reinstalled packages in an attempt of solving it, removed files and drove itself into a corner by breaking the dev environment. Too autonomous in trying to &amp;quot;get things done&amp;quot;, which doesn't work well on bleeding edge stuff that's not in the training set. Permissions can of course also be configured, but the default is &amp;quot;YOLO&amp;quot;.&lt;/p&gt; &lt;p&gt;Aside from that: Despite running with only a locally hosted model, and having disabled update checks and news downloads, OpenCode (Desktop version) tries to contact a whole lot of IPs on start-up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz5uww/qwen3_coder_next_as_first_usable_coding_model_60/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz5uww/qwen3_coder_next_as_first_usable_coding_model_60/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz5uww/qwen3_coder_next_as_first_usable_coding_model_60/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T10:43:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz23pp</id>
    <title>PR opened for Qwen3.5!!</title>
    <updated>2026-02-08T06:57:13+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz23pp/pr_opened_for_qwen35/"&gt; &lt;img alt="PR opened for Qwen3.5!!" src="https://preview.redd.it/r10pwm02y7ig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb19e2c9eac9c47e80b6a33b08c10d458c3fb6c0" title="PR opened for Qwen3.5!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/pull/43830/"&gt;https://github.com/huggingface/transformers/pull/43830/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking at the code at &lt;code&gt;src/transformers/models/qwen3_5/modeling_qwen3_5.py&lt;/code&gt;, it looks like Qwen3.5 series will have VLMs right off the bat!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r10pwm02y7ig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz23pp/pr_opened_for_qwen35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz23pp/pr_opened_for_qwen35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T06:57:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
