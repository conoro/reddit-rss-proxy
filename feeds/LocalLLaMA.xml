<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-11T09:38:14+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lwwuwq</id>
    <title>Help me find the best Android app for running LLMs locally</title>
    <updated>2025-07-11T03:42:32+00:00</updated>
    <author>
      <name>/u/DanielD2724</name>
      <uri>https://old.reddit.com/user/DanielD2724</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for both a good app and an availability of a good and capable LLM. Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanielD2724"&gt; /u/DanielD2724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwwuwq/help_me_find_the_best_android_app_for_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwwuwq/help_me_find_the_best_android_app_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwwuwq/help_me_find_the_best_android_app_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T03:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1lww2w9</id>
    <title>Open Source Claude Coder alternative?</title>
    <updated>2025-07-11T03:02:14+00:00</updated>
    <author>
      <name>/u/maxwell321</name>
      <uri>https://old.reddit.com/user/maxwell321</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I all, I've been using Claude Coder at work for a while now and LOVE it. Is there any high quality alternatives where you can use local models / openAI endpoint(s)? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maxwell321"&gt; /u/maxwell321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lww2w9/open_source_claude_coder_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lww2w9/open_source_claude_coder_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lww2w9/open_source_claude_coder_alternative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T03:02:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwrd38</id>
    <title>Best large open-source LLM for health/medical data analytics (RTX 6000 Pro, $10k budget)</title>
    <updated>2025-07-10T23:16:29+00:00</updated>
    <author>
      <name>/u/LeastExperience1579</name>
      <uri>https://old.reddit.com/user/LeastExperience1579</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, Weâ€™re a hospital building an on-prem system for health and medical data analytics using LLMs. Our setup includes an RTX 6000 Pro and a 5090, and weâ€™re working with a $10~$19k budget.&lt;/p&gt; &lt;p&gt;I have already tried Gemma3 on 5090 but canâ€™t unleash the 96gb vram capabilities.&lt;/p&gt; &lt;p&gt;Weâ€™re looking to: â€¢ Run a large open-source LLM locally (currently putting eyes in llama4) â€¢ Do fine-tuning (LoRA or full) on structured clinical data and unstructured medical notes â€¢ Use the model for summarization, Q&amp;amp;A, and EHR-related tasks&lt;/p&gt; &lt;p&gt;Weâ€™d love recommendations on: 1. The best large open-source LLM to use in this context 2. How much CPU matters for performance (inference + fine-tuning) alongside these GPUs&lt;/p&gt; &lt;p&gt;Would really appreciate any suggestions based on real-world setupsâ€”especially if youâ€™ve done similar work in the health/biomed space.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeastExperience1579"&gt; /u/LeastExperience1579 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T23:16:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwlw1j</id>
    <title>VS Code June 2025 (version 1.102)</title>
    <updated>2025-07-10T19:33:09+00:00</updated>
    <author>
      <name>/u/isidor_n</name>
      <uri>https://old.reddit.com/user/isidor_n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwlw1j/vs_code_june_2025_version_1102/"&gt; &lt;img alt="VS Code June 2025 (version 1.102)" src="https://external-preview.redd.it/U1REkZbjoWpzn7VEVeFMpzt04omcgqHBQRP2UuKsAZE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fea22c6187bb94fe796b6c79160a51ef50be7f7d" title="VS Code June 2025 (version 1.102)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Explore and contribute to the open sourced GitHub Copilot Chat extension (&lt;a href="https://code.visualstudio.com/blogs/2025/06/30/openSourceAIEditorFirstMilestone"&gt;Read our blog post&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Generate custom instructions that reflect your project's conventions (&lt;a href="https://code.visualstudio.com/updates/v1_102#_generate-custom-instructions"&gt;Show more&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Use custom modes to tailor chat for tasks like planning or research (&lt;a href="https://code.visualstudio.com/updates/v1_102#_chat-mode-improvements"&gt;Show more&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Automatically approve selected terminal commands (&lt;a href="https://code.visualstudio.com/updates/v1_102#_terminal-auto-approval-experimental"&gt;Show more&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Edit and resubmit previous chat requests (&lt;a href="https://code.visualstudio.com/updates/v1_102#_edit-previous-requests-experimental"&gt;Show more&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MCP&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;MCP support is now generally available in VS Code (&lt;a href="https://code.visualstudio.com/updates/v1_102#_mcp-support-in-vs-code-is-generally-available"&gt;Show more&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Easily install and manage MCP servers with the MCP view and gallery (&lt;a href="https://code.visualstudio.com/updates/v1_102#_mcp-server-discovery-and-installation"&gt;Show more&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;MCP servers as first-class resources in profiles and Settings Sync (&lt;a href="https://code.visualstudio.com/updates/v1_102#_mcp-servers-as-first-class-resources"&gt;Show more&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Editor experience&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Delegate tasks to Copilot coding agent and let it handle them in the background (&lt;a href="https://code.visualstudio.com/updates/v1_102#_start-a-coding-agent-session-preview"&gt;Show more&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Scroll the editor on middle click (&lt;a href="https://code.visualstudio.com/updates/v1_102#_scroll-on-middle-click"&gt;Show more&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;VS Code pm here in case there are any questions I am happy to answer.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/isidor_n"&gt; /u/isidor_n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://code.visualstudio.com/updates/v1_102"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwlw1j/vs_code_june_2025_version_1102/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwlw1j/vs_code_june_2025_version_1102/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T19:33:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx0b5w</id>
    <title>Manage multiple MCP servers for Ollama + OpenWebUI as Docker service</title>
    <updated>2025-07-11T07:04:33+00:00</updated>
    <author>
      <name>/u/trevorstr</name>
      <uri>https://old.reddit.com/user/trevorstr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm running Ollama &amp;amp; OpenWebUI on a headless Linux server, as Docker (with Compose) containers, with an NVIDIA GPU. This setup works great, but I want to add MCP servers to my environment, to improve the results from Ollama invocations.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://docs.openwebui.com/openapi-servers/mcp/"&gt;documentation for OpenWebUI&lt;/a&gt; suggests running a single container per MCP server. However, that will get unwieldy quickly.&lt;/p&gt; &lt;p&gt;How are other people exposing multiple MCP servers as a &lt;em&gt;singular&lt;/em&gt; Docker service, as part of their Docker Compose stack?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/trevorstr"&gt; /u/trevorstr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx0b5w/manage_multiple_mcp_servers_for_ollama_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx0b5w/manage_multiple_mcp_servers_for_ollama_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx0b5w/manage_multiple_mcp_servers_for_ollama_openwebui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T07:04:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwk84b</id>
    <title>Why do base models give gibberish and need further 'fine tuning'</title>
    <updated>2025-07-10T18:27:26+00:00</updated>
    <author>
      <name>/u/QFGTrialByFire</name>
      <uri>https://old.reddit.com/user/QFGTrialByFire</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to understand why does something like say llama 3.1 8b need further instruction by something like alpaca? If you just load the base model and ask something of it it just responds with gibberish. If you train it with say even just 1000 samples of alpaca data it starts responding coherently. But why does that happen when the original is already trained on next token generation? The q/a instruction training is also next token generation why does a little nudge in the weights from alpaca or other small data sets suddenly get it to respond with coherent responses. When I've looked around in sites etc it just says the further instruction gets the model to align to respond but doesn't say why. How come a few samples (say just 1000 alpaca samples) of 'fine tuning' next token generation suddenly go from gibberish to coherent responses when that is also just doing next token generation as well. I get its training directed towards producing responses to questions so it would shift the weights towards that but the original next token training would have had similar q/a data sets in it already so why doesn't it already do it?&lt;/p&gt; &lt;p&gt;Just for context i'm using &lt;a href="https://huggingface.co/meta-llama/Llama-3.1-8B"&gt;https://huggingface.co/meta-llama/Llama-3.1-8B&lt;/a&gt; with lora to train on the alpaca data.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QFGTrialByFire"&gt; /u/QFGTrialByFire &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwk84b/why_do_base_models_give_gibberish_and_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T18:27:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx2hn2</id>
    <title>Uncensored LLM ranking for roleplay?</title>
    <updated>2025-07-11T09:31:05+00:00</updated>
    <author>
      <name>/u/mikemend</name>
      <uri>https://old.reddit.com/user/mikemend</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every day, a bunch of models appear, making it difficult to choose which ones to use for uncensored role-playing. Previously, the Ayumi LLM Role Play &amp;amp; ERP Ranking data was somewhat of a guide, but now I can't find a list that is even close to being up to date. It's difficult to choose from among the many models with fantasy names.&lt;/p&gt; &lt;p&gt;Is there a list that might help with which models are better for role-playing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mikemend"&gt; /u/mikemend &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2hn2/uncensored_llm_ranking_for_roleplay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2hn2/uncensored_llm_ranking_for_roleplay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2hn2/uncensored_llm_ranking_for_roleplay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T09:31:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx2j1l</id>
    <title>I built a tool to run Humanity's Last Exam on your favorite local models!</title>
    <updated>2025-07-11T09:33:40+00:00</updated>
    <author>
      <name>/u/mags0ft</name>
      <uri>https://old.reddit.com/user/mags0ft</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/"&gt; &lt;img alt="I built a tool to run Humanity's Last Exam on your favorite local models!" src="https://external-preview.redd.it/o-H6rWtrxMkGu6ppy9Z76PzirPIQLGU4dUkkkXovQww.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81757699f7df25dfc08d7353f703eb730b727ec6" title="I built a tool to run Humanity's Last Exam on your favorite local models!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/h7ayiozbt7cf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f25b54d35a6eaf43451f5d78d2046edf55d974c4"&gt;https://preview.redd.it/h7ayiozbt7cf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f25b54d35a6eaf43451f5d78d2046edf55d974c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey there,&lt;/p&gt; &lt;p&gt;in the last few weeks, I've spent a lot of time learning about Local LLMs but always noticed one glaringly obvious, missing thing: &lt;em&gt;good&lt;/em&gt; tools to run LLM benchmarks on (in terms of output quality, not talking about speed here!) in order to decide which LLM is best suited for a given task.&lt;/p&gt; &lt;p&gt;Thus, I've built a small Python tool to run the new and often-discussed HLE benchmark on local Ollama models. Grok 4 just passed the 40% milestone, but how far can our local models go?&lt;/p&gt; &lt;p&gt;My tool supports:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;both &lt;strong&gt;vision-based&lt;/strong&gt; and &lt;strong&gt;text-only&lt;/strong&gt; prompting&lt;/li&gt; &lt;li&gt;&lt;strong&gt;automatic judging&lt;/strong&gt; by a third-party model not involved in answering the exam questions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;question randomization&lt;/strong&gt; and only testing for a small subset of HLE&lt;/li&gt; &lt;li&gt;export of the results to &lt;strong&gt;machine-readable JSON&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;running &lt;strong&gt;several evaluations&lt;/strong&gt; for different models all in one go&lt;/li&gt; &lt;li&gt;support for &lt;strong&gt;external Ollama instances&lt;/strong&gt; with Bearer Authentication&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The entire source code is on GitHub! &lt;a href="https://github.com/mags0ft/hle-eval-ollama"&gt;https://github.com/mags0ft/hle-eval-ollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;To anyone new to HLE (Humanity's Last Exam)&lt;/strong&gt;, let me give you a quick rundown: The benchmark has been made by the Center for AI Safety and is known to be one of the hardest currently available. Many people believe that once models reach close to 100%, we're only a few steps away from AGI (sounds more like buzz than actual facts to me, but whatever...)&lt;/p&gt; &lt;p&gt;My project extends the usability of HLE to local Ollama models. It also improves code quality over the provided benchmarking scripts by the HLE authors (because those only provided support for OpenAI API endpoints) due to more documentation and extended formatting efforts.&lt;/p&gt; &lt;p&gt;I'd love to get some feedback, so don't hesitate to comment! Have fun trying it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mags0ft"&gt; /u/mags0ft &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2j1l/i_built_a_tool_to_run_humanitys_last_exam_on_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T09:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwwh8s</id>
    <title>Tired of writing /no_think every time you prompt?</title>
    <updated>2025-07-11T03:22:47+00:00</updated>
    <author>
      <name>/u/Iq1pl</name>
      <uri>https://old.reddit.com/user/Iq1pl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just add &lt;code&gt;/no_think&lt;/code&gt; in the system prompt and the model will mostly stop reasoning &lt;/p&gt; &lt;p&gt;You can also add your own conditions like &lt;code&gt;when i write /nt it means /no_think&lt;/code&gt; or &lt;code&gt;always /no_think except if i write /think&lt;/code&gt; if the model is smart enough it will mostly follow your orders&lt;/p&gt; &lt;p&gt;Tested on qwen3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iq1pl"&gt; /u/Iq1pl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwwh8s/tired_of_writing_no_think_every_time_you_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwwh8s/tired_of_writing_no_think_every_time_you_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwwh8s/tired_of_writing_no_think_every_time_you_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T03:22:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx2dw4</id>
    <title>Is a heavily quantised Q235b any better than Q32b?</title>
    <updated>2025-07-11T09:24:06+00:00</updated>
    <author>
      <name>/u/Secure_Reflection409</name>
      <uri>https://old.reddit.com/user/Secure_Reflection409</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've come to the conclusion that Qwen's 235b at Q2K~, perhaps unsurprisingly, is not better than Qwen3 32b Q4KL but I still wonder about the Q3? Gemma2 27b Q3KS used to be awesome, for example. Perhaps Qwen's 235b at Q3 will be amazing? Amazing enough to warrant 10 t/s?&lt;/p&gt; &lt;p&gt;I'm in the process of getting a mish mash of RAM I have in the cupboard together to go from 96GB to 128GB which should allow me to test Q3... if it'll POST.&lt;/p&gt; &lt;p&gt;Is anyone already running the Q3? Is it better for code / design work than the current 32b GOAT?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Reflection409"&gt; /u/Secure_Reflection409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx2dw4/is_a_heavily_quantised_q235b_any_better_than_q32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T09:24:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwgy9m</id>
    <title>RekaAI/reka-flash-3.1 Â· Hugging Face</title>
    <updated>2025-07-10T16:20:22+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwgy9m/rekaairekaflash31_hugging_face/"&gt; &lt;img alt="RekaAI/reka-flash-3.1 Â· Hugging Face" src="https://external-preview.redd.it/zbNhNQUmPXM9SLVErydaa9wkoEOK9vHi2m-oz-KSF4o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c35dd881d1547a11e35792b26300d6dd95afdbaa" title="RekaAI/reka-flash-3.1 Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/RekaAI/reka-flash-3.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwgy9m/rekaairekaflash31_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwgy9m/rekaairekaflash31_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T16:20:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwif50</id>
    <title>Using Siri to talk to a local LLM</title>
    <updated>2025-07-10T17:17:57+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwif50/using_siri_to_talk_to_a_local_llm/"&gt; &lt;img alt="Using Siri to talk to a local LLM" src="https://external-preview.redd.it/aGlwODdkbm95MmNmMRvvqErtxjzejWwi2v2r9K5PMHU_4HV4j8Gxryp_Peji.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb0506fda08c175bd9066c66126214cc5eb38cde" title="Using Siri to talk to a local LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently added Shortcuts support to my iOS app &lt;a href="https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692"&gt;Locally AI&lt;/a&gt; and worked to integrate it with Siri.&lt;/p&gt; &lt;p&gt;It's using Apple MLX to run the models.&lt;/p&gt; &lt;p&gt;Here's a demo of me asking Qwen 3 a question via Siri (sorry for my accent). It will call the app shortcut, get the answer and forward it to the Siri interface. It works with the Siri interface but also with AirPods or HomePod where Siri reads it. &lt;/p&gt; &lt;p&gt;Everything running on-device.&lt;/p&gt; &lt;p&gt;Did my best to have a seamless integration. It doesnâ€™t require any setup other than downloading a model first.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wjksocsoy2cf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwif50/using_siri_to_talk_to_a_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwif50/using_siri_to_talk_to_a_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T17:17:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwmpqf</id>
    <title>New Devstral 2707 with mistral.rs - MCP client, automatic tool calling!</title>
    <updated>2025-07-10T20:05:57+00:00</updated>
    <author>
      <name>/u/EricBuehler</name>
      <uri>https://old.reddit.com/user/EricBuehler</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="http://Mistral.rs"&gt;Mistral.rs&lt;/a&gt; has support for Mistral AI's newest model (no affiliation)!&lt;/p&gt; &lt;p&gt;Grab optimized UQFF files here: &lt;a href="https://huggingface.co/EricB/Devstral-Small-2507-UQFF"&gt;https://huggingface.co/EricB/Devstral-Small-2507-UQFF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More information: &lt;a href="https://github.com/EricLBuehler/mistral.rs"&gt;https://github.com/EricLBuehler/mistral.rs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In my testing, this model is really great at tool calling, and works very well with some of our newest features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Agentic/automatic tool calling&lt;/strong&gt;: you can specify custom tool callbacks in Python or Rust and dedicate the entire toolcalling workflow to mistral.rs!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI web search support&lt;/strong&gt;: &lt;a href="http://mistral.rs"&gt;mistral.rs&lt;/a&gt; allows models to have access to automatic web search, 100% compatible with the OpenAI API.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MCP client&lt;/strong&gt;: there is a builtin MCP client! Just like ChatGPT or Claude, all you need to do is specify the MCP server and it just works!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These features make &lt;a href="http://mistral.rs"&gt;mistral.rs&lt;/a&gt; a really powerful tool for leveraging the strong capabilities of Devstral!&lt;/p&gt; &lt;p&gt;What do you think? Excited to see what you build with this ðŸš€!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EricBuehler"&gt; /u/EricBuehler &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwmpqf/new_devstral_2707_with_mistralrs_mcp_client/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwmpqf/new_devstral_2707_with_mistralrs_mcp_client/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwmpqf/new_devstral_2707_with_mistralrs_mcp_client/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T20:05:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwnj5x</id>
    <title>Performance benchmarks on DeepSeek V3-0324/R1-0528/TNG-R1T2-Chimera on consumer CPU (7800X3D, 192GB RAM at 6000Mhz) and 208GB VRAM (5090x2/4090x2/3090x2/A6000) on ikllamacpp! From 3bpw (Q2_K_XL) to 4.2 bpw (IQ4_XS)</title>
    <updated>2025-07-10T20:37:31+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/"&gt; &lt;img alt="Performance benchmarks on DeepSeek V3-0324/R1-0528/TNG-R1T2-Chimera on consumer CPU (7800X3D, 192GB RAM at 6000Mhz) and 208GB VRAM (5090x2/4090x2/3090x2/A6000) on ikllamacpp! From 3bpw (Q2_K_XL) to 4.2 bpw (IQ4_XS)" src="https://b.thumbs.redditmedia.com/2-o5OWNt03DgmcUIElRxpmapK3b8mnf9fOYvDpwJaPg.jpg" title="Performance benchmarks on DeepSeek V3-0324/R1-0528/TNG-R1T2-Chimera on consumer CPU (7800X3D, 192GB RAM at 6000Mhz) and 208GB VRAM (5090x2/4090x2/3090x2/A6000) on ikllamacpp! From 3bpw (Q2_K_XL) to 4.2 bpw (IQ4_XS)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there guys, hope you're having a good day!&lt;/p&gt; &lt;p&gt;After latest improvements on ik llamacpp, &lt;a href="https://github.com/ikawrakow/ik_llama.cpp/commits/main/"&gt;https://github.com/ikawrakow/ik_llama.cpp/commits/main/&lt;/a&gt;, I have found that DeepSeek MoE models runs noticeably faster than llamacpp, at the point that I get about half PP t/s and 0.85-0.9X TG t/s vs ikllamacpp. This is the case only for MoE models I'm testing.&lt;/p&gt; &lt;p&gt;My setup is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen 7 7800X3D&lt;/li&gt; &lt;li&gt;192GB RAM, DDR5 6000Mhz, max bandwidth at about 60-62 GB/s&lt;/li&gt; &lt;li&gt;3 1600W PSUs (Corsair 1600i)&lt;/li&gt; &lt;li&gt;AM5 MSI Carbon X670E&lt;/li&gt; &lt;li&gt;5090/5090 at PCIe X8/X8 5.0&lt;/li&gt; &lt;li&gt;4090/4090 at PCIe X4/X4 4.0&lt;/li&gt; &lt;li&gt;3090/3090 at PCIe X4/X4 4.0&lt;/li&gt; &lt;li&gt;A6000 at PCIe X4 4.0.&lt;/li&gt; &lt;li&gt;Fedora Linux 41 (instead of 42 just because I'm lazy doing some roundabouts to compile with GCC15, waiting until NVIDIA adds support to it)&lt;/li&gt; &lt;li&gt;SATA and USB-&amp;gt;M2 Storage&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The benchmarks are based on mostly, R1-0528, BUT it has the same size and it's quants on V3-0324 and TNG-R1T2-Chimera.&lt;/p&gt; &lt;p&gt;I have tested the next models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth"&gt;unsloth&lt;/a&gt; DeepSeek Q2_K_XL: &lt;ul&gt; &lt;li&gt;llm_load_print_meta: model size = 233.852 GiB (2.994 BPW)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth"&gt;unsloth&lt;/a&gt; DeepSeek IQ3_XXS: &lt;ul&gt; &lt;li&gt;llm_load_print_meta: model size = 254.168 GiB (3.254 BPW)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth"&gt;unsloth&lt;/a&gt; DeepSeek Q3_K_XL: &lt;ul&gt; &lt;li&gt;llm_load_print_meta: model size = 275.576 GiB (3.528 BPW)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ubergarm"&gt;ubergarm&lt;/a&gt; DeepSeek IQ3_KS: &lt;ul&gt; &lt;li&gt;llm_load_print_meta: model size = 281.463 GiB (3.598 BPW)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth"&gt;unsloth&lt;/a&gt; DeepSeek IQ4_XS: &lt;ul&gt; &lt;li&gt;llm_load_print_meta: model size = 333.130 GiB (4.264 BPW)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each model may have been tested on different formats. Q2_K_XL and IQ3_XXS has less info, but the rest have a lot more. So here we go!&lt;/p&gt; &lt;h1&gt;unsloth DeepSeek Q2_K_XL&lt;/h1&gt; &lt;p&gt;Running the model with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server -m '/models_llm/DeepSeek-R1-0528-UD-Q2_K_XL-merged.gguf' \ -c 32768 --no-mmap -ngl 999 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6|7).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(8|9|10|11).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(12|13|14|15).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(16|17|18|19|20).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(21|22|23|24).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(25|26|27|28).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.(29|30|31|32|33|34|35|36|37|38).ffn.=CUDA6&amp;quot; \ -ot exps=CPU \ -fa -mg 0 -ub 5120 -b 5120 -mla 3 -amb 256 -fmoe &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I get:&lt;/p&gt; &lt;p&gt;main: n_kv_max = 32768, n_batch = 5120, n_ubatch = 5120, flash_attn = 1, n_gpu_layers = 999, n_threads = 8, n_threads_batch = 8&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| PP | TG | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | |-------|--------|--------|----------|----------|----------|----------| | 5120 | 1280 | 0 | 12.481 | 410.21 | 104.088 | 12.30 | | 5120 | 1280 | 5120 | 14.630 | 349.98 | 109.724 | 11.67 | | 5120 | 1280 | 10240 | 17.167 | 298.25 | 112.938 | 11.33 | | 5120 | 1280 | 15360 | 20.008 | 255.90 | 119.037 | 10.75 | | 5120 | 1280 | 20480 | 22.444 | 228.12 | 122.706 | 10.43 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r9tt4pktt3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e55870952c3069d16bb1a1b211b83b870e87da7"&gt;Perf comparison (ignore 4096 as I forgor to save the perf)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Q2_K_XL performs really good for a system like this! And it's performance as LLM is really good as well. I still prefer this above any other local model, for example, even if it's at 3bpw.&lt;/p&gt; &lt;h1&gt;unsloth DeepSeek IQ3_XXS&lt;/h1&gt; &lt;p&gt;Running the model with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server -m '/models_llm/DeepSeek-R1-0528-UD-IQ3_XXS-merged.gguf' \ -c 32768 --no-mmap -ngl 999 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(7|8|9|10).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(11|12|13|14).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(15|16|17|18|19).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(20|21|22|23).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(24|25|26|27).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.(28|29|30|31|32|33|34|35).ffn.=CUDA6&amp;quot; \ -ot exps=CPU \ -fa -mg 0 -ub 4096 -b 4096 -mla 3 -amb 256 -fmoe &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I get&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Small test for this one! | PP | TG | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | |-------|--------|--------|----------|----------|----------|----------| | 4096 | 1024 | 0 | 10.671 | 383.83 | 117.496 | 8.72 | | 4096 | 1024 | 4096 | 11.322 | 361.77 | 120.192 | 8.52 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dtrfsnabu3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34fa15b35573c0d7ce936d9da953d4d483320902"&gt;https://preview.redd.it/dtrfsnabu3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34fa15b35573c0d7ce936d9da953d4d483320902&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sorry on this one to have few data! IQ3_XXS quality is really good for it's size.&lt;/p&gt; &lt;h1&gt;unsloth DeepSeek Q3_K_XL&lt;/h1&gt; &lt;p&gt;Now we enter a bigger territory. Note that you will notice Q3_K_XL being faster than IQ3_XXS, despite being bigger.&lt;/p&gt; &lt;p&gt;Running the faster PP one with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server -m '/DeepSeek-R1-0528-UD-Q3_K_XL-merged.gguf' \ -c 32768 --no-mmap -ngl 999 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6|7).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(8|9|10|11).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(12|13|14|15).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(16|17|18|19|20).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(21|22|23).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(24|25|26).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.(27|28|29|30|31|32|33|34).ffn.=CUDA6&amp;quot; \ -ot exps=CPU \ -fa -mg 0 -ub 2560 -b 2560 -mla 1 -fmoe -amb 256 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Results look like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| PP | TG | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | |-------|--------|--------|----------|----------|----------|----------| | 2560 | 640 | 0 | 9.781 | 261.72 | 65.367 | 9.79 | | 2560 | 640 | 2560 | 10.048 | 254.78 | 65.824 | 9.72 | | 2560 | 640 | 5120 | 10.625 | 240.93 | 66.134 | 9.68 | | 2560 | 640 | 7680 | 11.167 | 229.24 | 67.225 | 9.52 | | 2560 | 640 | 10240 | 12.268 | 208.68 | 67.475 | 9.49 | | 2560 | 640 | 12800 | 13.433 | 190.58 | 68.743 | 9.31 | | 2560 | 640 | 15360 | 14.564 | 175.78 | 69.585 | 9.20 | | 2560 | 640 | 17920 | 15.734 | 162.70 | 70.589 | 9.07 | | 2560 | 640 | 20480 | 16.889 | 151.58 | 72.524 | 8.82 | | 2560 | 640 | 23040 | 18.100 | 141.43 | 74.534 | 8.59 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With more layers on GPU, but smaller batch size, I get&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| PP | TG | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | |-------|--------|--------|----------|----------|----------|----------| | 2048 | 512 | 0 | 9.017 | 227.12 | 50.612 | 10.12 | | 2048 | 512 | 2048 | 9.113 | 224.73 | 51.027 | 10.03 | | 2048 | 512 | 4096 | 9.436 | 217.05 | 51.864 | 9.87 | | 2048 | 512 | 6144 | 9.680 | 211.56 | 52.818 | 9.69 | | 2048 | 512 | 8192 | 9.984 | 205.12 | 53.354 | 9.60 | | 2048 | 512 | 10240 | 10.349 | 197.90 | 53.896 | 9.50 | | 2048 | 512 | 12288 | 10.936 | 187.27 | 54.600 | 9.38 | | 2048 | 512 | 14336 | 11.688 | 175.22 | 55.150 | 9.28 | | 2048 | 512 | 16384 | 12.419 | 164.91 | 55.852 | 9.17 | | 2048 | 512 | 18432 | 13.113 | 156.18 | 56.436 | 9.07 | | 2048 | 512 | 20480 | 13.871 | 147.65 | 56.823 | 9.01 | | 2048 | 512 | 22528 | 14.594 | 140.33 | 57.590 | 8.89 | | 2048 | 512 | 24576 | 15.335 | 133.55 | 58.278 | 8.79 | | 2048 | 512 | 26624 | 16.073 | 127.42 | 58.723 | 8.72 | | 2048 | 512 | 28672 | 16.794 | 121.95 | 59.553 | 8.60 | | 2048 | 512 | 30720 | 17.522 | 116.88 | 59.921 | 8.54 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And with less GPU layers on GPU, but higher batch size, I get&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| PP | TG | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | |-------|--------|--------|----------|----------|----------|----------| | 4096 | 1024 | 0 | 12.005 | 341.19 | 111.632 | 9.17 | | 4096 | 1024 | 4096 | 12.515 | 327.28 | 138.930 | 7.37 | | 4096 | 1024 | 8192 | 13.389 | 305.91 | 118.220 | 8.66 | | 4096 | 1024 | 12288 | 15.018 | 272.74 | 119.289 | 8.58 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So then, performance for different batch sizes and layers, looks like this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l9jswixxu3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fed2cf27373207f9bb3e0e702f391214334adcd1"&gt;Higher ub/b is because I ended the test earlier!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So you can choose between having more TG t/s with having possibly smaller batch sizes (so then slower PP), or try to max PP by offloading more layers to the CPU.&lt;/p&gt; &lt;h1&gt;ubergarm DeepSeek IQ3_KS (&lt;a href="https://huggingface.co/ubergarm/DeepSeek-TNG-R1T2-Chimera-GGUF"&gt;TNG-R1T2-Chimera&lt;/a&gt;)&lt;/h1&gt; &lt;p&gt;This one is really good! And it has some more optimizations that may apply more on iklcpp.&lt;/p&gt; &lt;p&gt;Running this one with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server -m '/GGUFs/DeepSeek-TNG-R1T2-Chimera-IQ3_KS-merged.gguf' \ -c 32768 --no-mmap -ngl 999 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(7|8|9).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(10|11|12).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(13|14|15|16).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(17|18|19).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(20|21|22).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.(23|24|25|26|27|28|29|30).ffn.=CUDA6&amp;quot; \ -ot exps=CPU \ -fa -mg 0 -ub 6144 -b 6144 -mla 3 -fmoe -amb 256 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I get&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| PP | TG | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | |-------|--------|--------|----------|----------|----------|----------| | 6144 | 1536 | 0 | 15.406 | 398.81 | 174.929 | 8.78 | | 6144 | 1536 | 6144 | 18.289 | 335.94 | 180.393 | 8.51 | | 6144 | 1536 | 12288 | 22.229 | 276.39 | 186.113 | 8.25 | | 6144 | 1536 | 18432 | 24.533 | 250.44 | 191.037 | 8.04 | | 6144 | 1536 | 24576 | 28.122 | 218.48 | 196.268 | 7.83 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Or 8192 batch size/ubatch size, I get&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| PP | TG | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | |-------|--------|--------|----------|----------|----------|----------| | 8192 | 2048 | 0 | 20.147 | 406.61 | 232.476 | 8.81 | | 8192 | 2048 | 8192 | 26.009 | 314.97 | 242.648 | 8.44 | | 8192 | 2048 | 16384 | 32.628 | 251.07 | 253.309 | 8.09 | | 8192 | 2048 | 24576 | 39.010 | 210.00 | 264.415 | 7.75 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So the graph looks like this&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rj0kip6gw3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac996b223c86d5d30668be4995436a4aa1bc8dbf"&gt;https://preview.redd.it/rj0kip6gw3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac996b223c86d5d30668be4995436a4aa1bc8dbf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Again, this model is really good, and really fast! Totally recommended.&lt;/p&gt; &lt;h1&gt;unsloth DeepSeek IQ4_XS&lt;/h1&gt; &lt;p&gt;At this point is where I have to do compromises to run it on my PC, by either having less PP, less TG or use more RAM at the absolute limit.&lt;/p&gt; &lt;p&gt;Running this model with the best balance with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-sweep-bench -m '/models_llm/DeepSeek-R1-0528-IQ4_XS-merged.gguf' -c 32768 --no-mmap -ngl 999 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(7|8|9).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(10|11|12).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(13|14|15|16).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(17|18|19).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(20|21|22).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.(23|24|25|26|27|28|29).ffn.=CUDA6&amp;quot; \ -ot &amp;quot;blk.30.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.30.ffn_gate_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.30.ffn_down_exps.weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.30.ffn_up_exps.weight=CUDA4&amp;quot; \ -ot &amp;quot;blk.31.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA5&amp;quot; \ -ot &amp;quot;blk.31.ffn_gate_exps.weight=CUDA5&amp;quot; \ -ot &amp;quot;blk.31.ffn_down_exps.weight=CUDA0&amp;quot; \ -ot &amp;quot;blk.31.ffn_up_exps.weight=CUDA3&amp;quot; \ -ot &amp;quot;blk.32.ffn_gate_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.32.ffn_down_exps.weight=CUDA2&amp;quot; \ -ot exps=CPU \ -fa -mg 0 -ub 1024 -mla 1 -amb 256 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Using 161GB of RAM and the GPUs totally maxed, I get&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| PP | TG | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | |-------|--------|--------|----------|----------|----------|----------| | 1024 | 256 | 0 | 9.336 | 109.69 | 31.102 | 8.23 | | 1024 | 256 | 1024 | 9.345 | 109.57 | 31.224 | 8.20 | | 1024 | 256 | 2048 | 9.392 | 109.03 | 31.193 | 8.21 | | 1024 | 256 | 3072 | 9.452 | 108.34 | 31.472 | 8.13 | | 1024 | 256 | 4096 | 9.540 | 107.34 | 31.623 | 8.10 | | 1024 | 256 | 5120 | 9.750 | 105.03 | 32.674 | 7.83 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Running a variant with less layers on GPU, but more on CPU, using 177GB RAM and higher ubatch size, at 1792:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| PP | TG | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | |-------|--------|--------|----------|----------|----------|----------| | 1792 | 448 | 0 | 10.701 | 167.46 | 56.284 | 7.96 | | 1792 | 448 | 1792 | 10.729 | 167.02 | 56.638 | 7.91 | | 1792 | 448 | 3584 | 10.947 | 163.71 | 57.194 | 7.83 | | 1792 | 448 | 5376 | 11.099 | 161.46 | 58.003 | 7.72 | | 1792 | 448 | 7168 | 11.267 | 159.06 | 58.127 | 7.71 | | 1792 | 448 | 8960 | 11.450 | 156.51 | 58.697 | 7.63 | | 1792 | 448 | 10752 | 11.627 | 154.12 | 59.421 | 7.54 | | 1792 | 448 | 12544 | 11.809 | 151.75 | 59.686 | 7.51 | | 1792 | 448 | 14336 | 12.007 | 149.24 | 60.075 | 7.46 | | 1792 | 448 | 16128 | 12.251 | 146.27 | 60.624 | 7.39 | | 1792 | 448 | 17920 | 12.639 | 141.79 | 60.977 | 7.35 | | 1792 | 448 | 19712 | 13.113 | 136.66 | 61.481 | 7.29 | | 1792 | 448 | 21504 | 13.639 | 131.39 | 62.117 | 7.21 | | 1792 | 448 | 23296 | 14.184 | 126.34 | 62.393 | 7.18 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And there is a less efficient result with ub 1536, but this will be shown on the graph, which looks like this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r8xka0tcx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d374193172785b18b03858204abb37a0ac8b9fa"&gt;https://preview.redd.it/r8xka0tcx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d374193172785b18b03858204abb37a0ac8b9fa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As you can see, the most conservative one with RAM has really slow PP, but a bit faster TG. While with less layers on GPU and more RAM usage, since we left some layers, we can increase PP and increment is noticeable.&lt;/p&gt; &lt;h1&gt;Final comparison&lt;/h1&gt; &lt;p&gt;An image comparing 1 of each in one image, looks like this&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/owlrn4cqx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f05a460e56395c8890bc2dc18d6e78aa15cf91a6"&gt;https://preview.redd.it/owlrn4cqx3cf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f05a460e56395c8890bc2dc18d6e78aa15cf91a6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I don't have PPL values in hand sadly, besides the PPL on TNG-R1T2-Chimera that ubergarm did, in where DeepSeek R1 0528 is just 3% better than this quant at 3.8bpw (&lt;code&gt;3.2119 +/- 0.01697&lt;/code&gt; vs 3.3167 +/- 0.01789), but take in mind that original TNG-R1T2-Chimera is already, at Q8, a bit worse on PPL vs R1 0528, &lt;strong&gt;so these quants are quite good quality.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For the models on the post and based for max batch size (less layers on GPU, so more RAM usage because offloading more to CPU), or based on max TG speed (more layers on GPU, less on RAM):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;90-95GB RAM on Q2_K_XL, rest on VRAM.&lt;/li&gt; &lt;li&gt;100-110GB RAM on IQ3_XXS, rest on VRAM.&lt;/li&gt; &lt;li&gt;115-140GB RAM on Q3_K_XL, rest on VRAM.&lt;/li&gt; &lt;li&gt;115-135GB RAM on IQ3_KS, rest on VRAM.&lt;/li&gt; &lt;li&gt;161-177GB RAM on IQ4_XS, rest on VRAM.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Someone may be wondering that with these values, it is still not total 400GB (192GB RAM + 208GB VRAM), and it's because I have not contemplated the compute buffer sizes, which can range between 512MB up to 5GB per GPU.&lt;/p&gt; &lt;p&gt;For DeepSeek models with MLA, in general it is 1GB per 8K ctx at fp16. So 1GB per 16K with q8_0 ctx (I didn't use it here, but it lets me use 64K at q8 with the same config as 32K at f16).&lt;/p&gt; &lt;p&gt;Hope this post can help someone interested in these results, any question is welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwnj5x/performance_benchmarks_on_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T20:37:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwgohu</id>
    <title>I'm curating a list of every OCR out there and running tests on their features. Contribution welcome!</title>
    <updated>2025-07-10T16:09:38+00:00</updated>
    <author>
      <name>/u/Ok_Help9178</name>
      <uri>https://old.reddit.com/user/Ok_Help9178</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwgohu/im_curating_a_list_of_every_ocr_out_there_and/"&gt; &lt;img alt="I'm curating a list of every OCR out there and running tests on their features. Contribution welcome!" src="https://external-preview.redd.it/esm18p-jYs33hdE6QXSUjic3dHA7d2Ru25KXKPwNU0k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=845ce9e8cd2f0eec39ba036028db301cc1226bab" title="I'm curating a list of every OCR out there and running tests on their features. Contribution welcome!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I'm compiling a list of document parsers available on the market and testing their feature coverage. &lt;/p&gt; &lt;p&gt;So far, I've tested 14 OCRs/parsers for tables, equations, handwriting, two-column layouts, and multiple-column layouts. You can view the outputs from each parser in the `results` folder. The ones I've tested are mostly open source or with generous free quota.&lt;/p&gt; &lt;p&gt;ðŸš© Coming soon: benchmarks for each OCR - score from 0 (doesn't work) to 5 (perfect)&lt;/p&gt; &lt;p&gt;Feedback &amp;amp; contribution are welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Help9178"&gt; /u/Ok_Help9178 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/GiftMungmeeprued/document-parsers-list"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwgohu/im_curating_a_list_of_every_ocr_out_there_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwgohu/im_curating_a_list_of_every_ocr_out_there_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T16:09:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwn3ut</id>
    <title>Grok 4 on Fiction.liveBench Long Context Comprehension</title>
    <updated>2025-07-10T20:21:08+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwn3ut/grok_4_on_fictionlivebench_long_context/"&gt; &lt;img alt="Grok 4 on Fiction.liveBench Long Context Comprehension" src="https://preview.redd.it/rzwo8emcv3cf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87eea0d411941e119cea7f8020ad7157923da79a" title="Grok 4 on Fiction.liveBench Long Context Comprehension" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rzwo8emcv3cf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwn3ut/grok_4_on_fictionlivebench_long_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwn3ut/grok_4_on_fictionlivebench_long_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T20:21:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwkrg4</id>
    <title>Reka Flash 3.1 benchmarks show strong progress in LLM quantisation</title>
    <updated>2025-07-10T18:48:27+00:00</updated>
    <author>
      <name>/u/benja0x40</name>
      <uri>https://old.reddit.com/user/benja0x40</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwkrg4/reka_flash_31_benchmarks_show_strong_progress_in/"&gt; &lt;img alt="Reka Flash 3.1 benchmarks show strong progress in LLM quantisation" src="https://external-preview.redd.it/AmnK5PwhGeRRRtQ5S0hpzGcRHIn74hIOsBvGyS0ABGA.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1933f58a3a9b626d8fe0fc66ca8cbd276198d9d8" title="Reka Flash 3.1 benchmarks show strong progress in LLM quantisation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, Reka just open-sourced a new quantisation method which looks promising for local inference and VRAM-limited setups.&lt;/p&gt; &lt;p&gt;According to their benchmarks, the new method significantly outperforms llama.cpp's standard Q3_K_S, narrowing the performance gap with Q4_K_M or higher quants. This could be great news for the local inference community.&lt;/p&gt; &lt;p&gt;What are your thoughts on this new method?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blog Post: &lt;a href="https://reka.ai/news/reka-quantization-technology"&gt;Reka Quantization Technology&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Source Code: &lt;a href="https://github.com/reka-ai/rekaquant"&gt;GitHub&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Quantised Model: &lt;a href="https://huggingface.co/RekaAI/reka-flash-3.1-rekaquant-q3_k_s"&gt;reka-flash-3.1-rekaquant-q3_k_s&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/benja0x40"&gt; /u/benja0x40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwkrg4/reka_flash_31_benchmarks_show_strong_progress_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwkrg4/reka_flash_31_benchmarks_show_strong_progress_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwkrg4/reka_flash_31_benchmarks_show_strong_progress_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T18:48:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1lx10ja</id>
    <title>With a 1M context Gemini, does it still make sense to do embedding or use RAG for long texts?</title>
    <updated>2025-07-11T07:51:04+00:00</updated>
    <author>
      <name>/u/GyozaHoop</name>
      <uri>https://old.reddit.com/user/GyozaHoop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m trying to build an AI application that transcribes long audio recordings (around hundreds of thousands of tokens) and allows interaction with an LLM. However, every answer I get from searches and inquiries tells me that I need to chunk and vectorize the long text.&lt;/p&gt; &lt;p&gt;But with LLMs like Gemini that support 1M-token context, isnâ€™t building a RAG system somewhat extra?&lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GyozaHoop"&gt; /u/GyozaHoop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lx10ja/with_a_1m_context_gemini_does_it_still_make_sense/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T07:51:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwxr2l</id>
    <title>What other models would you like to see on Design Arena?</title>
    <updated>2025-07-11T04:30:26+00:00</updated>
    <author>
      <name>/u/adviceguru25</name>
      <uri>https://old.reddit.com/user/adviceguru25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwxr2l/what_other_models_would_you_like_to_see_on_design/"&gt; &lt;img alt="What other models would you like to see on Design Arena?" src="https://b.thumbs.redditmedia.com/vyaiGpslyuDvhVMrcw8-eJ5unh7Eixe4i35Y204tnqU.jpg" title="What other models would you like to see on Design Arena?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just hit &lt;a href="https://www.designarena.ai/"&gt;15K users&lt;/a&gt;! For context of course, see &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/"&gt;this post&lt;/a&gt;. Since then, we have added Grok 4, several Devstral Small, Devstral Medium, Gemini 2.5 Flash, and Qwen-235B-A22B. &lt;/p&gt; &lt;p&gt;We now thankfully have more access to various kind of models (particularly OS and open weight) thanks to &lt;a href="https://app.fireworks.ai/account/home"&gt;Fireworks AI&lt;/a&gt; and we'll be periodically adding more models throughout the weekend. &lt;/p&gt; &lt;p&gt;Which models would you like to see added to the leaderboard? We're looking to add as many as possible. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adviceguru25"&gt; /u/adviceguru25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lwxr2l"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwxr2l/what_other_models_would_you_like_to_see_on_design/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwxr2l/what_other_models_would_you_like_to_see_on_design/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T04:30:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwe5y8</id>
    <title>mistralai/Devstral-Small-2507</title>
    <updated>2025-07-10T14:29:19+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwe5y8/mistralaidevstralsmall2507/"&gt; &lt;img alt="mistralai/Devstral-Small-2507" src="https://external-preview.redd.it/2w0SYAlXrI0T76c0g4EW9E9VAtmz5Fj81y8zFN0Exrg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=299e4f7a5df68d789749c7d30f346b534a08b8ba" title="mistralai/Devstral-Small-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Devstral-Small-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwe5y8/mistralaidevstralsmall2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwe5y8/mistralaidevstralsmall2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T14:29:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwl9ai</id>
    <title>The New Nvidia Model is Really Chatty</title>
    <updated>2025-07-10T19:07:49+00:00</updated>
    <author>
      <name>/u/SpyderJack</name>
      <uri>https://old.reddit.com/user/SpyderJack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwl9ai/the_new_nvidia_model_is_really_chatty/"&gt; &lt;img alt="The New Nvidia Model is Really Chatty" src="https://external-preview.redd.it/Z3dvOGNyZDZpM2NmMeEzo3-lIfyzuWEbQM-S8hxJnpNgq3nRHs7JWUUcsQJx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64406c81316cc6f179ed0040ddcdc5047147395f" title="The New Nvidia Model is Really Chatty" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpyderJack"&gt; /u/SpyderJack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8bnc2od6i3cf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwl9ai/the_new_nvidia_model_is_really_chatty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwl9ai/the_new_nvidia_model_is_really_chatty/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-10T19:07:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwztnp</id>
    <title>Granite-speech-3.3-8b</title>
    <updated>2025-07-11T06:33:44+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwztnp/granitespeech338b/"&gt; &lt;img alt="Granite-speech-3.3-8b" src="https://external-preview.redd.it/qCjJtYOA1xCC4NeAQLlvmQH4l0rYxhSxDnkaBD28QmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a3cfa1633e9a330cab59c33f8413530288842b0" title="Granite-speech-3.3-8b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Granite-speech-3.3-8b is a compact and efficient speech-language model, specifically designed for automatic speech recognition (ASR) and automatic speech translation (AST). Granite-speech-3.3-8b uses a two-pass design, unlike integrated models that combine speech and language into a single pass. Initial calls to granite-speech-3.3-8b will transcribe audio files into text. To process the transcribed text using the underlying Granite language model, users must make a second call as each step must be explicitly initiated.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-speech-3.3-8b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwztnp/granitespeech338b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwztnp/granitespeech338b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T06:33:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwx50s</id>
    <title>2-bit Quant: CCQ, Convolutional Code for Extreme Low-bit Quantization in LLMs</title>
    <updated>2025-07-11T03:57:38+00:00</updated>
    <author>
      <name>/u/ortegaalfredo</name>
      <uri>https://old.reddit.com/user/ortegaalfredo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Creators of Earnie just published a new quantization algorithm that compress Ernie-300B to 85GB and Deepseek-V3 to 184 GB, with minimal (&amp;lt;2%) performance degradation in benchmarks. Paper here: &lt;a href="https://arxiv.org/pdf/2507.07145"&gt;https://arxiv.org/pdf/2507.07145&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ortegaalfredo"&gt; /u/ortegaalfredo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwx50s/2bit_quant_ccq_convolutional_code_for_extreme/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T03:57:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwsrx7</id>
    <title>Support for the upcoming IBM Granite 4.0 has been merged into llama.cpp</title>
    <updated>2025-07-11T00:20:50+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/"&gt; &lt;img alt="Support for the upcoming IBM Granite 4.0 has been merged into llama.cpp" src="https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be85c7d84d9bd88e720aef5b6d229ca49e85ef9a" title="Support for the upcoming IBM Granite 4.0 has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Whereas prior generations of Granite LLMs utilized a conventional transformer architecture, all models in the Granite 4.0 family utilize a new &lt;strong&gt;hybrid Mamba-2/Transformer architecture,&lt;/strong&gt; marrying the speed and efficiency of Mamba with the precision of transformer-based self-attention. &lt;/p&gt; &lt;p&gt;Granite 4.0 Tiny-Preview, specifically, is a &lt;strong&gt;fine-grained hybrid&lt;/strong&gt; &lt;a href="https://www.ibm.com/think/topics/mixture-of-experts"&gt;&lt;strong&gt;mixture of experts (MoE)&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;model,&lt;/strong&gt; with 7B total parameters and only 1B active parameters at inference time.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-4.0-tiny-preview"&gt;https://huggingface.co/ibm-granite/granite-4.0-tiny-preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ibm-granite/granite-4.0-tiny-base-preview"&gt;https://huggingface.co/ibm-granite/granite-4.0-tiny-base-preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ibm-ai-platform/Bamba-9B-v1"&gt;https://huggingface.co/ibm-ai-platform/Bamba-9B-v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/13550"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T00:20:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lwta86</id>
    <title>AMD's Pull Request for llama.cpp: Enhancing GPU Support</title>
    <updated>2025-07-11T00:45:46+00:00</updated>
    <author>
      <name>/u/Rrraptr</name>
      <uri>https://old.reddit.com/user/Rrraptr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, good news for AMD GPU users! It seems AMD is getting serious about boosting support for their graphics cards in llama.cpp&lt;/p&gt; &lt;p&gt;Word is, someone from AMD dropped a pull request to tweak the code, aimed at adapting the project for use with AMD graphics cards.&lt;br /&gt; Discussions with the project leaders are planned in the near future to explore opportunities for further enhancements.&lt;br /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/pull/14624"&gt;https://github.com/ggml-org/llama.cpp/pull/14624&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rrraptr"&gt; /u/Rrraptr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-11T00:45:46+00:00</published>
  </entry>
</feed>
