<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-08T06:10:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nbfsh1</id>
    <title>In Search: Model generating German Poetry (32B max.)</title>
    <updated>2025-09-08T05:55:29+00:00</updated>
    <author>
      <name>/u/Inside-Swimmer9623</name>
      <uri>https://old.reddit.com/user/Inside-Swimmer9623</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Guys!&lt;/p&gt; &lt;p&gt;Currently I‚Äôm in search for a model that can reliably generate German poetries. Actually I thought this would be pretty easy, but I couldn‚Äôt find a model even after days of search, since I need a foundation model with a open license. Even after a lot of work in the system prompt and the sampling parameter I am constantly facing the challenge of generating a poetry that rhymes coherently when using open source models (f.e. Qwen3, Mistral Small, Aya, gpt-oss-20b etc.) the only models which could achieve good results in my opinions were gemma3, and llama 4 which sadly have a restrictive license..&lt;/p&gt; &lt;p&gt;Therefore I wanted to give a try here.. does anyone here had a similar task and could recommend a model / or give any other tips / ideas.&lt;/p&gt; &lt;p&gt;Thanks to you in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inside-Swimmer9623"&gt; /u/Inside-Swimmer9623 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfsh1/in_search_model_generating_german_poetry_32b_max/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfsh1/in_search_model_generating_german_poetry_32b_max/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfsh1/in_search_model_generating_german_poetry_32b_max/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T05:55:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb05ls</id>
    <title>Need help - Trying to repurpose a Gigabyte CRSG422 as a double slot eGPU ‚Äì struggling with power input</title>
    <updated>2025-09-07T18:03:04+00:00</updated>
    <author>
      <name>/u/Same-Masterpiece3748</name>
      <uri>https://old.reddit.com/user/Same-Masterpiece3748</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb05ls/need_help_trying_to_repurpose_a_gigabyte_crsg422/"&gt; &lt;img alt="Need help - Trying to repurpose a Gigabyte CRSG422 as a double slot eGPU ‚Äì struggling with power input" src="https://b.thumbs.redditmedia.com/P9xr7L7DLd33AsnnooriL35zC3xGbrbKz4VeQP1fz8c.jpg" title="Need help - Trying to repurpose a Gigabyte CRSG422 as a double slot eGPU ‚Äì struggling with power input" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I‚Äôve been experimenting with a Gigabyte CRSG422 riser, which is basically a PCIe switch (PLX/PMC chip) that can split one x16 uplink into two full x16 slots. The idea is that the GPUs can still communicate at x16 speeds thanks to the switch, and I thought this could be a cheap way to maximize density for compute.&lt;/p&gt; &lt;p&gt;My original goal was to use AMD MI50 32GB cards in pairs. With two cards per riser, that would give me 64 GB of HBM2 VRAM per CRSG422, and potentially 128 GB total if I ran two risers. For the price, this looked like an amazing way to build an affordable high-VRAM setup for inference workloads.&lt;/p&gt; &lt;p&gt;I did manage to get something working: when connecting through USB-C to a GPU, the host could at least enumerate a network card, so the switch isn‚Äôt completely dead. That gave me some confidence that the CRSG422 can be used outside of its original Gigabyte server environment.&lt;/p&gt; &lt;p&gt;But the main challenge is power. The CRSG422 needs external 12 V and 3.3 V through a small proprietary 5-pad edge connector. There is no ‚Äúfemale‚Äù connector on the market for that edge; soldering directly is very delicate and not something I would trust long term.&lt;/p&gt; &lt;p&gt;So far I‚Äôve managed to get slot 1 properly soldered and working, but on slot 2 there‚Äôs currently a bridge between 12 V and GND, which means I can‚Äôt even test using both slots at the same time until I rework the soldering. Even once I fix that, it feels like this approach is too fragile to be a real solution.&lt;/p&gt; &lt;p&gt;I‚Äôd love help from the community:&lt;/p&gt; &lt;p&gt;Has anyone ever seen a mating connector for the CRSG422‚Äôs 5-pad power edge?&lt;/p&gt; &lt;p&gt;Are there any known adapters/dummy cards that can inject 12 V and 3.3 V into these Gigabyte PCIe switch risers?&lt;/p&gt; &lt;p&gt;Or, if you‚Äôve done similar hacks (feeding server risers with external ATX or step-down power), I‚Äôd love to see how you approached it.&lt;/p&gt; &lt;p&gt;Thanks in advance ‚Äì and I‚Äôll attach photos of the whole process so far for context.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Same-Masterpiece3748"&gt; /u/Same-Masterpiece3748 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nb05ls"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb05ls/need_help_trying_to_repurpose_a_gigabyte_crsg422/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb05ls/need_help_trying_to_repurpose_a_gigabyte_crsg422/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T18:03:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb3k5r</id>
    <title>~3000$ Budget Workstation/Server Recommendation with 2x RTX 5070 TI Super (48gb VRAM)</title>
    <updated>2025-09-07T20:14:24+00:00</updated>
    <author>
      <name>/u/Silent-Translator-87</name>
      <uri>https://old.reddit.com/user/Silent-Translator-87</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;br /&gt; I heard the RTX 5070 Ti Super with 24GB VRAM might be coming soon.&lt;br /&gt; I‚Äôm planning to build a workstation or server to run smaller LLMs locally once it‚Äôs out.&lt;br /&gt; Any recommendations for solid price/performance setups?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silent-Translator-87"&gt; /u/Silent-Translator-87 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3k5r/3000_budget_workstationserver_recommendation_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3k5r/3000_budget_workstationserver_recommendation_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3k5r/3000_budget_workstationserver_recommendation_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T20:14:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb8tt4</id>
    <title>Gerbil - Cross-platform LLM GUI for local text and image gen</title>
    <updated>2025-09-07T23:56:21+00:00</updated>
    <author>
      <name>/u/i_got_the_tools_baby</name>
      <uri>https://old.reddit.com/user/i_got_the_tools_baby</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gerbil is a cross-platform desktop GUI for local LLM text and image generation. Built on KoboldCpp (heavily modified llama.cpp fork) with a much better UX, automatic updates, and improved cross-platform reliability. It's completely open source and available at: &lt;a href="https://github.com/lone-cloud/gerbil"&gt;https://github.com/lone-cloud/gerbil&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Download the latest release to try it out: &lt;a href="https://github.com/lone-cloud/gerbil/releases"&gt;https://github.com/lone-cloud/gerbil/releases&lt;/a&gt; Unsure? Check out the screenshots from the repo's README to get a sense of how it works.&lt;/p&gt; &lt;p&gt;Core features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Supports LLMs locally via CUDA, ROCm, Vulkan, CLBlast or CPU backends. Older architectures are also supported in the &amp;quot;Old PC&amp;quot; binary which provides CUDA v11 and avx1 (or no avx at all via &amp;quot;failsafe&amp;quot;).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Text gen and image gen out of the box&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Built-in KoboldAI Lite and Stable UI frontends for text and image gen respectively&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Optionally supports SillyTavern (text and image gen) or Open WebUI (text gen only) through a configuration in the settings. Other frontends can run side-by-side by connecting via OpenAI or Ollama APIs&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Cross-platform support for Windows, Linux and macOS (M1+). The optimal way to run Gerbil is through either the &amp;quot;Setup.exe&amp;quot; binary on Windows or a &amp;quot;pacman&amp;quot; install on Linux.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Will automatically keep your KoboldCpp, SillyTavern and Open WebUI binaries updated&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm not sure where I'll take this project next, but I'm curious to hear your guys' feedback and constructive criticism. For any bugs, feel free to open an issue on GitHub.&lt;/p&gt; &lt;p&gt;Hidden Easter egg for reading this far: try clicking on the Gerbil logo in the title bar of the app window. After 10 clicks there's a 10% chance for an &amp;quot;alternative&amp;quot; effect. Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i_got_the_tools_baby"&gt; /u/i_got_the_tools_baby &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb8tt4/gerbil_crossplatform_llm_gui_for_local_text_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb8tt4/gerbil_crossplatform_llm_gui_for_local_text_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb8tt4/gerbil_crossplatform_llm_gui_for_local_text_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T23:56:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbdqbr</id>
    <title>Commercial or local LLM for my uses?</title>
    <updated>2025-09-08T03:57:20+00:00</updated>
    <author>
      <name>/u/BenefitOfTheDoubt_01</name>
      <uri>https://old.reddit.com/user/BenefitOfTheDoubt_01</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use chatGPT to help design blueprints for UE5 editor as well as Papyrus scripting in Skyrim Construction Kit. &lt;/p&gt; &lt;p&gt;It is quite poor at both. Even when I tell it to only use information for version xyz, it just tells me to go fuck myself and then proceeds to lie some more about the wrong ways to do things. &lt;/p&gt; &lt;p&gt;So the question is, would training my own model be plausible or useful for these use cases? Or should I just stick with commercial options like GPT &amp;amp; Perplexity? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BenefitOfTheDoubt_01"&gt; /u/BenefitOfTheDoubt_01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbdqbr/commercial_or_local_llm_for_my_uses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbdqbr/commercial_or_local_llm_for_my_uses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbdqbr/commercial_or_local_llm_for_my_uses/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T03:57:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1na3f1s</id>
    <title>Renting GPUs is hilariously cheap</title>
    <updated>2025-09-06T16:08:44+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3f1s/renting_gpus_is_hilariously_cheap/"&gt; &lt;img alt="Renting GPUs is hilariously cheap" src="https://preview.redd.it/dhtzimf7jknf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1bca94832d9e6b8fb7b8faf80d61387d12889d7f" title="Renting GPUs is hilariously cheap" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A 140 GB monster GPU that costs $30k to buy, plus the rest of the system, plus electricity, plus maintenance, plus a multi-Gbps uplink, for a little over 2 bucks per hour.&lt;/p&gt; &lt;p&gt;If you use it for 5 hours per day, 7 days per week, and factor in auxiliary costs and interest rates, buying that GPU today vs. renting it when you need it will only pay off in 2035 or later. That‚Äôs a tough sell.&lt;/p&gt; &lt;p&gt;Owning a GPU is great for privacy and control, and obviously, many people who have such GPUs run them nearly around the clock, but for quick experiments, renting is often the best option.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dhtzimf7jknf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1na3f1s/renting_gpus_is_hilariously_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1na3f1s/renting_gpus_is_hilariously_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-06T16:08:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1napq0m</id>
    <title>check https://huggingface.co/papers/2509.01363</title>
    <updated>2025-09-07T10:24:40+00:00</updated>
    <author>
      <name>/u/LowChance4561</name>
      <uri>https://old.reddit.com/user/LowChance4561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The paper shows that reasoning ability can be extracted as a vector from RL-trained models and added to others via simple arithmetic to boost reasoning without retraining&lt;br /&gt; would appreciate an upvote &lt;a href="https://huggingface.co/papers/2509.01363"&gt;https://huggingface.co/papers/2509.01363&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LowChance4561"&gt; /u/LowChance4561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1napq0m/check_httpshuggingfacecopapers250901363/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1napq0m/check_httpshuggingfacecopapers250901363/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1napq0m/check_httpshuggingfacecopapers250901363/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T10:24:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1naygs1</id>
    <title>I built a Graph RAG pipeline (VeritasGraph) that runs entirely locally with Ollama (Llama 3.1) and has full source attribution.</title>
    <updated>2025-09-07T16:58:14+00:00</updated>
    <author>
      <name>/u/BitterHouse8234</name>
      <uri>https://old.reddit.com/user/BitterHouse8234</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I've been deep in the world of local RAG and wanted to share a project I built, &lt;strong&gt;VeritasGraph&lt;/strong&gt;, that's designed from the ground up for private, on-premise use with tools we all love.&lt;/p&gt; &lt;p&gt;My setup uses &lt;strong&gt;Ollama&lt;/strong&gt; with &lt;code&gt;llama3.1&lt;/code&gt; for generation and &lt;code&gt;nomic-embed-text&lt;/code&gt; for embeddings. The whole thing runs on my machine without hitting any external APIs.&lt;/p&gt; &lt;p&gt;The main goal was to solve two big problems:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Multi-Hop Reasoning:&lt;/strong&gt; Standard vector RAG fails when you need to connect facts from different documents. VeritasGraph builds a knowledge graph to traverse these relationships.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Trust &amp;amp; Verification:&lt;/strong&gt; It provides full source attribution for every generated statement, so you can see exactly which part of your source documents was used to construct the answer.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;One of the key challenges I ran into (and solved) was the default context length in Ollama. I found that the default of 2048 was truncating the context and leading to bad results. The repo includes a &lt;code&gt;Modelfile&lt;/code&gt; to build a version of &lt;code&gt;llama3.1&lt;/code&gt; with a 12k context window, which fixed the issue completely.&lt;/p&gt; &lt;p&gt;The project includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The full Graph RAG pipeline.&lt;/li&gt; &lt;li&gt;A Gradio UI for an interactive chat experience.&lt;/li&gt; &lt;li&gt;A guide for setting everything up, from installing dependencies to running the indexing process.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo with all the code and instructions:&lt;/strong&gt; &lt;a href="https://github.com/bibinprathap/VeritasGraph"&gt;&lt;code&gt;https://github.com/bibinprathap/VeritasGraph&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd be really interested to hear your thoughts, especially on the local LLM implementation and prompt tuning. I'm sure there are ways to optimize it further.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BitterHouse8234"&gt; /u/BitterHouse8234 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naygs1/i_built_a_graph_rag_pipeline_veritasgraph_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naygs1/i_built_a_graph_rag_pipeline_veritasgraph_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naygs1/i_built_a_graph_rag_pipeline_veritasgraph_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T16:58:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbc9ko</id>
    <title>[Project] LLM Agents &amp; Ecosystem Handbook ‚Äî 60+ agent skeletons, RAG pipelines, local inference &amp; ecosystem guides</title>
    <updated>2025-09-08T02:41:56+00:00</updated>
    <author>
      <name>/u/Fearless-Role-2707</name>
      <uri>https://old.reddit.com/user/Fearless-Role-2707</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been building the &lt;strong&gt;LLM Agents &amp;amp; Ecosystem Handbook&lt;/strong&gt; ‚Äî a repo designed to help devs go beyond ‚Äúdemo scripts‚Äù and actually build production-ready agents.&lt;/p&gt; &lt;p&gt;What‚Äôs inside: - üñ• 60+ agent skeletons (finance, health, research, games, MCP, voice, RAG‚Ä¶)&lt;br /&gt; - ‚ö° Local inference: examples using Ollama &amp;amp; other offline RAG setups&lt;br /&gt; - üìö Tutorials: RAG, Memory, Chat with X (repos, PDFs, APIs), Fine-tuning (LoRA/PEFT)&lt;br /&gt; - üõ† Evaluation: Promptfoo, DeepEval, RAGAs, Langfuse&lt;br /&gt; - ‚öô Ecosystem overview: training frameworks, local inference, LLMOps, interpretability &lt;/p&gt; &lt;p&gt;It‚Äôs structured as a &lt;em&gt;handbook&lt;/em&gt; (not just an awesome-list), with code + tutorials + guides.&lt;/p&gt; &lt;p&gt;Would love to hear from this community:&lt;br /&gt; üëâ How would you extend this for &lt;strong&gt;offline-first agents&lt;/strong&gt; or &lt;strong&gt;local-only use cases&lt;/strong&gt;?&lt;/p&gt; &lt;p&gt;Repo link: &lt;a href="https://github.com/oxbshw/LLM-Agents-Ecosystem-Handbook"&gt;https://github.com/oxbshw/LLM-Agents-Ecosystem-Handbook&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fearless-Role-2707"&gt; /u/Fearless-Role-2707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbc9ko/project_llm_agents_ecosystem_handbook_60_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbc9ko/project_llm_agents_ecosystem_handbook_60_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbc9ko/project_llm_agents_ecosystem_handbook_60_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T02:41:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbfgtm</id>
    <title>Good models when speed isn't important..</title>
    <updated>2025-09-08T05:35:30+00:00</updated>
    <author>
      <name>/u/cangaroo_hamam</name>
      <uri>https://old.reddit.com/user/cangaroo_hamam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many years back, I used to be into 3D graphics rendering as a hobby. The workflow involved doing many low-quality rough (but fast) renders to make sure you're in the right ballpark, and then do the final high quality render which could take hours.&lt;/p&gt; &lt;p&gt;I can envision such a case for LLMs as well. e.g. Give a large PDF and translate it to another language. I can wait.&lt;/p&gt; &lt;p&gt;The problem is, I don't think LLMs are designed for interactive 'chat' and don't work well for this kind of workflow. If the speed gets too low (i.e. usually RAM/context maxing out), the model is probably getting too dumb and useless already.&lt;/p&gt; &lt;p&gt;Are there models who would be good for these scenarios? Would you find this useful in your tasks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cangaroo_hamam"&gt; /u/cangaroo_hamam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfgtm/good_models_when_speed_isnt_important/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfgtm/good_models_when_speed_isnt_important/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfgtm/good_models_when_speed_isnt_important/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T05:35:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1naxf65</id>
    <title>GPT-OSS-120B on DDR4 48GB and RTX 3090 24GB</title>
    <updated>2025-09-07T16:17:39+00:00</updated>
    <author>
      <name>/u/Vektast</name>
      <uri>https://old.reddit.com/user/Vektast</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just bought a used RTX 3090 for $600 (MSI Suprim X) and decided to run a quick test to see what my PC can do with the bigger GPT‚ÄëOSS‚Äë120B model using llama.cpp. I thought I‚Äôd share the results and the start.bat file in case anyone else finds them useful.&lt;/p&gt; &lt;p&gt;My system:&lt;/p&gt; &lt;p&gt;- 48 GB DDR4 3200 MT/s &lt;em&gt;DUAL Channel (2x8gb+2x16gb)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;- Ryzen 7 5800X CPU&lt;/p&gt; &lt;p&gt;- RTX 3090 with 24 GB VRAM&lt;/p&gt; &lt;p&gt;23gb used on vram and 43 on ram, pp 67 t/s, tg 16t/s&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama_perf_sampler_print: sampling time = 56.88 ms / 655 runs ( 0.09 ms per token, 11515.67 tokens per second) llama_perf_context_print: load time = 50077.41 ms llama_perf_context_print: prompt eval time = 2665.99 ms / 179 tokens ( 14.89 ms per token, 67.14 tokens per second) llama_perf_context_print: eval time = 29897.62 ms / 475 runs ( 62.94 ms per token, 15.89 tokens per second) llama_perf_context_print: total time = 40039.05 ms / 654 tokens llama_perf_context_print: graphs reused = 472 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Llama.cpp config:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@echo off set LLAMA_ARG_THREADS=16 llama-cli ^ -m gpt-oss-120b-Q4_K_M-00001-of-00002.gguf ^ --n-cpu-moe 23 ^ --n-gpu-layers 999 ^ --ctx-size 4096 ^ --no-mmap ^ --flash-attn on ^ --temp 1.0 ^ --top-p 0.99 ^ --min-p 0.005 ^ --top-k 100 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If anyone has ideas on how to configure llama.cpp to run even faster, please feel free to let me know, bc i'm quite a noob at this! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vektast"&gt; /u/Vektast &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxf65/gptoss120b_on_ddr4_48gb_and_rtx_3090_24gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxf65/gptoss120b_on_ddr4_48gb_and_rtx_3090_24gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naxf65/gptoss120b_on_ddr4_48gb_and_rtx_3090_24gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T16:17:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbfy60</id>
    <title>Finishing touches on dual RTX 6000 build</title>
    <updated>2025-09-08T06:04:43+00:00</updated>
    <author>
      <name>/u/ikkiyikki</name>
      <uri>https://old.reddit.com/user/ikkiyikki</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfy60/finishing_touches_on_dual_rtx_6000_build/"&gt; &lt;img alt="Finishing touches on dual RTX 6000 build" src="https://preview.redd.it/sez83piasvnf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=599c86ee050439c49f07c7056ff67e6c48ef8381" title="Finishing touches on dual RTX 6000 build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a dream build: 192 gigs of fast VRAM (and another 128 of RAM) but worried I'll burn the house down because of the 15A breakers.&lt;/p&gt; &lt;p&gt;Downloading Qwen 235B q4 :-)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ikkiyikki"&gt; /u/ikkiyikki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sez83piasvnf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfy60/finishing_touches_on_dual_rtx_6000_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfy60/finishing_touches_on_dual_rtx_6000_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T06:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbbgab</id>
    <title>~$15K Inference Workstation for a 250+ Gov Org</title>
    <updated>2025-09-08T02:02:10+00:00</updated>
    <author>
      <name>/u/reughdurgem</name>
      <uri>https://old.reddit.com/user/reughdurgem</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello I saw a post on here asking for an idea of an inference setup for a school and figured I'd also see what this community thinks of the setup I've been tasked with building. &lt;/p&gt; &lt;p&gt;For some context I work for a local county government clerk of about 250 employees and considering the information we deal with has lots of sensitivities we want to explore on-prem AI solutions for things like LLM chatbots for the public and VLMs for extracting structured JSON data from scanned images. &lt;/p&gt; &lt;p&gt;I have approximately $15K budgeted for hardware which essentially will be a dedicated AI server and/or workstation box that our employees would interact with via various tools over our network and it would directly integrate with some of our court management software. &lt;/p&gt; &lt;p&gt;I've been in the AI community since the OG DALL-E days and use models like GPT-OSS:20B and Qwen3 4B regularly via Ollama hooked into GitHub Copilot Chat in VSCode on my A5500 laptop for testing precision and accuracy when editing JavaScript files or light agentic tasks but I've never gotten into the distributed computing space. &lt;/p&gt; &lt;p&gt;From my research it seems like either VLLM or SGLang would be the optimal engines to run on a CLI Linux environment with hardware similar to the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPU: NVIDIA RTX 6000 PRO Blackwell 96GB (Server or Workstation Edition is better?)&lt;/li&gt; &lt;li&gt;CPU: AMD RYZEN Thread ripper Pro 7965WX (Overkill?)&lt;/li&gt; &lt;li&gt;MOBO: ASUS Pro WRX90E&lt;/li&gt; &lt;li&gt;SSD: 4TB NVME (brand agnostic)&lt;/li&gt; &lt;li&gt;RAM: 256GB ECC (8 sticks probably?)&lt;/li&gt; &lt;li&gt;Network: 10Gb NIC but probably 25Gb is preferred?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm curious what you all think of this approach since it seems like used 3090s is a more cost effective method to get lots of VRAM - however the gains from newer architectures seem to be worth it in terms of response tokens per second? I believe the A5500 is similarish to a 3080 and running GPT-OSS 20B on that and my 5070Ti at home the speed difference is noticable. Also I read that speed is better with one GPU versus multiple if all else is equal but idk if that's true in practice.&lt;/p&gt; &lt;p&gt;My current goal would be to run a vision model like Pixtral 12B which another county is using on dual L40Ss and just that model alone is using all 96GB of their VRAM - idk if that's just an insane context length because the model isn't &lt;em&gt;that&lt;/em&gt; huge on its own I don't believe. And if that is the case then something like GPT-OSS 120B for general text inference would be great too if it could all fit on the 6000 Pro. &lt;/p&gt; &lt;p&gt;I also read about offloading tasks like RAG and potentially smaller models (7b range) to the CPU and RAM to cut costs for &amp;quot;less essential&amp;quot; tasks so I'm considering that as well. Let me know your thoughts and any improvements I can make to the setup. &lt;/p&gt; &lt;p&gt;Thank you. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reughdurgem"&gt; /u/reughdurgem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbbgab/15k_inference_workstation_for_a_250_gov_org/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbbgab/15k_inference_workstation_for_a_250_gov_org/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbbgab/15k_inference_workstation_for_a_250_gov_org/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T02:02:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbf3tq</id>
    <title>Qwen3-Coder-30B-A3B-Instruct-Q4_K_M on RTX 3060 12GB?</title>
    <updated>2025-09-08T05:13:55+00:00</updated>
    <author>
      <name>/u/crxssrazr93</name>
      <uri>https://old.reddit.com/user/crxssrazr93</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How to run this model via llama.cpp? Trying to run via this but get OOM issues.&lt;/p&gt; &lt;p&gt;&lt;code&gt;/usr/bin/llama-cli \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;-m /home/models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--n-gpu-layers 40 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--flash-attn auto \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--cache-type-k q8_0 --cache-type-v q4_0 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--ctx-size 3072 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--top-k 20 --top-p 0.8 --temp 0.7 \&lt;/code&gt;&lt;br /&gt; &lt;code&gt;--repeat-penalty 1.05&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Any tips? I also have 40GB DDR4 ram.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crxssrazr93"&gt; /u/crxssrazr93 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbf3tq/qwen3coder30ba3binstructq4_k_m_on_rtx_3060_12gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbf3tq/qwen3coder30ba3binstructq4_k_m_on_rtx_3060_12gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbf3tq/qwen3coder30ba3binstructq4_k_m_on_rtx_3060_12gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T05:13:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb66te</id>
    <title>I built Claude Context but 100% local - semantic code search with no API keys</title>
    <updated>2025-09-07T22:00:18+00:00</updated>
    <author>
      <name>/u/person-loading</name>
      <uri>https://old.reddit.com/user/person-loading</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! &lt;/p&gt; &lt;p&gt;You might know Claude Context (3k+ stars) - it's a great semantic code search tool but requires OpenAI API keys + Zilliz Cloud.&lt;/p&gt; &lt;p&gt;I built a fully local alternative that runs 100% on your machine:&lt;/p&gt; &lt;p&gt;üîí &lt;strong&gt;Privacy first&lt;/strong&gt; - Your code never leaves your machine üöÄ &lt;strong&gt;No API keys&lt;/strong&gt; - Uses EmbeddingGemma locally&lt;br /&gt; üí∞ &lt;strong&gt;Zero costs&lt;/strong&gt; - No monthly API bills ‚ö° &lt;strong&gt;Fast&lt;/strong&gt; - After initial indexing, searches are instant&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt; - Tree-sitter for AST parsing (understands code structure) - EmbeddingGemma for semantic embeddings (1.2GB model) - FAISS for vector search - MCP protocol for Claude Code integration&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Early results:&lt;/strong&gt; - Reduction in Claude Code token usage (depends on search) - Finds code by meaning, not just text matching - Works with Python, JavaScript, TypeScript, JSX, TSX, Svelte (More coming just treesitter!)&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/FarhanAliRaza/claude-context-local"&gt;https://github.com/FarhanAliRaza/claude-context-local&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is an early release - would love feedback from the local-first community! If you hit any issues, please open a GitHub issue and I'll fix it fast.&lt;/p&gt; &lt;p&gt;Built this because I believe code search should be private and free. No cloud required!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/person-loading"&gt; /u/person-loading &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb66te/i_built_claude_context_but_100_local_semantic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb66te/i_built_claude_context_but_100_local_semantic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb66te/i_built_claude_context_but_100_local_semantic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T22:00:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1naz2cv</id>
    <title>Early support for Grok-2 in llama.cpp (still under development)</title>
    <updated>2025-09-07T17:21:25+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Preliminary support for Grok-2 in llama.cpp is available in this PR: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15539"&gt;https://github.com/ggml-org/llama.cpp/pull/15539&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In my opinion, this is an important milestone for the Open Source AI community.&lt;/p&gt; &lt;p&gt;Grok-2 is a model from 2024. It can‚Äôt beat today‚Äôs SOTA models in benchmarks, and it‚Äôs quite large (comparable in size to Qwen 235B). So why should you care?&lt;/p&gt; &lt;p&gt;Because this is the first time a top model from that era has been made available to run locally. Now you can actually launch it on your own PC: quantized, with CPU offloading. That was never possible with ChatGPT or Gemini. Yes, we have Gemma and GPT-OSS now, but those aren‚Äôt the same models that OpenAI or Google were offering in the cloud in 2024.&lt;/p&gt; &lt;p&gt;Grok was trained on different data than the Chinese models, so it simply knows different things. At the same time, it also differs from ChatGPT, Gemini, and Claude, often showing a unique perspective on many topics.&lt;/p&gt; &lt;p&gt;nicoboss and unsloth have already prepared GGUF files, so you can easily run a quantized Grok-2 locally. &lt;strong&gt;Warning:&lt;/strong&gt; the PR has not been reviewed yet, GGUF format could still change in the future.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nicoboss/grok-2-GGUF"&gt;https://huggingface.co/nicoboss/grok-2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/grok-2-GGUF"&gt;https://huggingface.co/unsloth/grok-2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naz2cv/early_support_for_grok2_in_llamacpp_still_under/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naz2cv/early_support_for_grok2_in_llamacpp_still_under/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naz2cv/early_support_for_grok2_in_llamacpp_still_under/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T17:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1namz1q</id>
    <title>HF releases 3T tokens dataset sourced entirely from PDFs.</title>
    <updated>2025-09-07T07:26:55+00:00</updated>
    <author>
      <name>/u/Other_Housing8453</name>
      <uri>https://old.reddit.com/user/Other_Housing8453</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guy, something we have teased a bit during our AMA is finally out: &lt;/p&gt; &lt;p&gt;üìÑ FinePDFs, the largest PDF dataset ever released, spanning over half a billion documents!&lt;/p&gt; &lt;p&gt;- Long context: Documents are 2x longer than web text&lt;/p&gt; &lt;p&gt;- 3T tokens from high-demand domains like legal and science.&lt;/p&gt; &lt;p&gt;- Heavily improves over SoTA when mixed with FW-EDU&amp;amp;DCLM web copora üìà.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Other_Housing8453"&gt; /u/Other_Housing8453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1namz1q/hf_releases_3t_tokens_dataset_sourced_entirely/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1namz1q/hf_releases_3t_tokens_dataset_sourced_entirely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1namz1q/hf_releases_3t_tokens_dataset_sourced_entirely/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T07:26:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb8wys</id>
    <title>My experience in running Ollama with a combination of CUDA (RTX3060 12GB) + ROCm (AMD MI50 32GB) + RAM (512GB DDR4 LRDIMM)</title>
    <updated>2025-09-08T00:00:22+00:00</updated>
    <author>
      <name>/u/incrediblediy</name>
      <uri>https://old.reddit.com/user/incrediblediy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb8wys/my_experience_in_running_ollama_with_a/"&gt; &lt;img alt="My experience in running Ollama with a combination of CUDA (RTX3060 12GB) + ROCm (AMD MI50 32GB) + RAM (512GB DDR4 LRDIMM)" src="https://b.thumbs.redditmedia.com/a7Ufbae-YySckctbCfshnCtZWyYRgOg4l50M5Kdrmkg.jpg" title="My experience in running Ollama with a combination of CUDA (RTX3060 12GB) + ROCm (AMD MI50 32GB) + RAM (512GB DDR4 LRDIMM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found a cheap HP DL380 G9 from a local eWaste place and decided to build an inference server. I will keep all equivalent prices in US$, including shipping, but I paid for everything in local currency (AUD). The fan speed is ~20% or less and quite silent for a server.&lt;/p&gt; &lt;p&gt;Parts:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;HP DL380 G9 = $150 (came with dual Xeon 2650 v3 + 64GB RDIMM (I had to remove these), no HDD, both PCIe risers: this is important)&lt;/li&gt; &lt;li&gt;512 GB LRDIMM (8 sticks, 64GB each from an eWaste place), I got LRDIMM as they are cheaper than RDIMM for some reason = $300&lt;/li&gt; &lt;li&gt;My old RTX3060 (was a gift in 2022 or so)&lt;/li&gt; &lt;li&gt;AMD MI50 32GB from AliExpress = $235 including shipping + tax&lt;/li&gt; &lt;li&gt;GPU power cables from Amazon (2 * HP 10pin to EPS + 2 * EPS to PCIe)&lt;/li&gt; &lt;li&gt;NVMe to PCIe adapters * 2 from Amazon&lt;/li&gt; &lt;li&gt;SN5000 1TB ($55) + 512GB old Samsung card, which I had&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ovsl3hkl0unf1.jpg?width=1663&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9ff5c0c0f034a1e2f3ae60bc2a86f7a4a5bf4230"&gt;https://preview.redd.it/ovsl3hkl0unf1.jpg?width=1663&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9ff5c0c0f034a1e2f3ae60bc2a86f7a4a5bf4230&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Software:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Ubuntu 24.04.3 LTS&lt;/li&gt; &lt;li&gt;NVIDIA 550 drivers were automatically installed with Ubuntu&lt;/li&gt; &lt;li&gt;AMD drivers + ROCm 6.4.3&lt;/li&gt; &lt;li&gt;Ollama (curl -fsSL &lt;a href="https://ollama.com/install.sh"&gt;https://ollama.com/install.sh&lt;/a&gt; | sh)&lt;/li&gt; &lt;li&gt;Drivers: &lt;ol&gt; &lt;li&gt;amdgpu-install -y --usecase=graphics,rocm,hiplibsdk&lt;/li&gt; &lt;li&gt;&lt;a href="https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-radeon.html"&gt;https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-radeon.html&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ROCm (need to copy DFX906 files from ArchLinux AUR as below):&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/linux4noobs/comments/1ly8rq6/drivers_for_radeon_instinct_mi50_16gb/"&gt;https://www.reddit.com/r/linux4noobs/comments/1ly8rq6/drivers_for_radeon_instinct_mi50_16gb/&lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/ROCm/ROCm/issues/4625#issuecomment-2899838977"&gt;https://github.com/ROCm/ROCm/issues/4625#issuecomment-2899838977&lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="https://archlinux.org/packages/extra/x86_64/rocblas/"&gt;https://archlinux.org/packages/extra/x86_64/rocblas/&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I noticed that Ollama automatically selects a GPU or a combination of targets, depending on the model size. Ex: if the model is smaller than 12GB, it selects RTX3060, if larger than that MI50 (I tested with Qwen different size models). For a very large model like DeepSeek R1:671B, it used both GPU + RAM automatically. It used n_ctx_per_seq (4096) by default; I haven't done extensive testing yet.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;load_tensors: loading model tensors, this can take a while... (mmap = true) load_tensors: offloading 3 repeating layers to GPU load_tensors: offloaded 3/62 layers to GPU load_tensors: ROCm0 model buffer size = 21320.01 MiB load_tensors: CPU_Mapped model buffer size = 364369.62 MiB time=2025-09-06T04:49:32.151+10:00 level=INFO source=server.go:1284 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server not responding&amp;quot; time=2025-09-06T04:49:32.405+10:00 level=INFO source=server.go:1284 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server loading model&amp;quot; llama_context: constructing llama_context llama_context: n_seq_max = 1 llama_context: n_ctx = 4096 llama_context: n_ctx_per_seq = 4096 llama_context: n_batch = 512 llama_context: n_ubatch = 512 llama_context: causal_attn = 1 llama_context: flash_attn = 0 llama_context: kv_unified = false llama_context: freq_base = 10000.0 llama_context: freq_scale = 0.025 llama_context: n_ctx_per_seq (4096) &amp;lt; n_ctx_train (163840) -- the full capacity of the model will not be utilized llama_context: CPU output buffer size = 0.52 MiB llama_kv_cache_unified: ROCm0 KV buffer size = 960.00 MiB llama_kv_cache_unified: CPU KV buffer size = 18560.00 MiB llama_kv_cache_unified: size = 19520.00 MiB ( 4096 cells, 61 layers, 1/1 seqs), K (f16): 11712.00 MiB, V (f16): 7808.00 MiB llama_context: CUDA0 compute buffer size = 3126.00 MiB llama_context: ROCm0 compute buffer size = 1250.01 MiB llama_context: CUDA_Host compute buffer size = 152.01 MiB llama_context: graph nodes = 4845 llama_context: graph splits = 1092 (with bs=512), 3 (with bs=1) time=2025-09-06T04:49:51.514+10:00 level=INFO source=server.go:1288 msg=&amp;quot;llama runner started in 63.85 seconds&amp;quot; time=2025-09-06T04:49:51.514+10:00 level=INFO source=sched.go:473 msg=&amp;quot;loaded runners&amp;quot; count=1 time=2025-09-06T04:49:51.514+10:00 level=INFO source=server.go:1250 msg=&amp;quot;waiting for llama runner to start responding&amp;quot; time=2025-09-06T04:49:51.515+10:00 level=INFO source=server.go:1288 msg=&amp;quot;llama runner started in 63.85 seconds&amp;quot; [GIN] 2025/09/06 - 04:49:51 | 200 | 1m5s | 127.0.0.1 | POST &amp;quot;/api/generate&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Memory usage:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;gpu@gpu:~/ollama$ free -h total used free shared buff/cache available Mem: 503Gi 28Gi 65Gi 239Mi 413Gi 475Gi Swap: 4.7Gi 256Ki 4.7Gi gpu@gpu:~/ollama$ =========================================== ROCm System Management Interface =========================================== ===================================================== Concise Info ===================================================== Device Node IDs Temp Power Partitions SCLK MCLK Fan Perf PwrCap VRAM% GPU% (DID, GUID) (Edge) (Socket) (Mem, Compute, ID) ======================================================================================================================== 0 2 0x66a1, 5947 36.0¬∞C 16.0W N/A, N/A, 0 925Mhz 350Mhz 14.51% auto 225.0W 75% 0% ======================================================================================================================== ================================================= End of ROCm SMI Log ================================================== Sat Sep 6 04:51:46 2025 +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 550.163.01 Driver Version: 550.163.01 CUDA Version: 12.4 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3060 Off | 00000000:84:00.0 Off | N/A | | 0% 36C P8 15W / 170W | 3244MiB / 12288MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ +-----------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=========================================================================================| | 0 N/A N/A 12196 G /usr/lib/xorg/Xorg 4MiB | | 0 N/A N/A 33770 C /usr/local/bin/ollama 3230MiB | +-----------------------------------------------------------------------------------------+ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;DeepSeek R1:671B output:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;gpu@gpu:~/ollama$ ollama run deepseek-r1:671b &amp;gt;&amp;gt;&amp;gt; hello Thinking... Hmm, the user just said &amp;quot;hello&amp;quot;. That's a simple greeting but I should respond warmly to start off on a good note. I notice they didn't include any specific question or context - could be testing me out, might be shy about asking directly, or maybe just being polite before diving into something else. Their tone feels neutral from this single word. Since it's such an open-ended opener, I'll keep my reply friendly but leave room for them to steer the conversation wherever they want next. A smiley emoji would help make it feel welcoming without overdoing it. Important not to overwhelm them with options though - &amp;quot;how can I help&amp;quot; is better than listing possibilities since they clearly haven't decided what they need yet. The ball's in their court now. ...done thinking. Hello! üòä How can I assist you today? &amp;gt;&amp;gt;&amp;gt; Send a message (/? for help) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/incrediblediy"&gt; /u/incrediblediy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb8wys/my_experience_in_running_ollama_with_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb8wys/my_experience_in_running_ollama_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb8wys/my_experience_in_running_ollama_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T00:00:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb3b8l</id>
    <title>Aquif-3-moe (17B) Thinking</title>
    <updated>2025-09-07T20:04:49+00:00</updated>
    <author>
      <name>/u/Trilogix</name>
      <uri>https://old.reddit.com/user/Trilogix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3b8l/aquif3moe_17b_thinking/"&gt; &lt;img alt="Aquif-3-moe (17B) Thinking" src="https://b.thumbs.redditmedia.com/PG2DZom31Ip8OdlIQP2-poMryul3rQ0LN3lLRyE7SAA.jpg" title="Aquif-3-moe (17B) Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A high-performance mixture-of-experts language model optimized for efficiency, coding, science, and general use. With 17B total parameters and 2.8B active parameters, aquif-3-moe delivers competitive performance across multiple domains while maintaining computational efficiency.&lt;/p&gt; &lt;p&gt;Is this true? A MOE 17B better than Gemini. I am testing it asap. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trilogix"&gt; /u/Trilogix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nb3b8l"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3b8l/aquif3moe_17b_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb3b8l/aquif3moe_17b_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T20:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb0ern</id>
    <title>Fully local &amp; natural Speech to Speech on iPhone</title>
    <updated>2025-09-07T18:12:51+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb0ern/fully_local_natural_speech_to_speech_on_iphone/"&gt; &lt;img alt="Fully local &amp;amp; natural Speech to Speech on iPhone" src="https://external-preview.redd.it/cjkzeGd2NDlhc25mMSl4q-3g5NF7jl_ztF72bvGVWwSqGjF18TajKv99ZwVy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edc2b6f8d5d7751f8ea1df7f4b4ee02dd80534f9" title="Fully local &amp;amp; natural Speech to Speech on iPhone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I updated my local AI iOS app called Locally AI to add a local voice mode. You can chat with any non-reasoning models. In the demo, I‚Äôm on an iPhone 16 Pro, talking with SmolLM3, a 3B parameters model.&lt;/p&gt; &lt;p&gt;The app is free and you can get the it on the AppStore here: &lt;a href="https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692"&gt;https://apps.apple.com/app/locally-ai-private-ai-chat/id6741426692&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everything is powered by Apple MLX. The voice mode is a combination of LLM + TTS using Kokoro and VAD for a natural turn by turn conversion.&lt;/p&gt; &lt;p&gt;There is still room for improvements, especially for the pronunciation of words. It‚Äôs only available on devices that support Apple Intelligence for now and only in English.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/z0lb9u99asnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb0ern/fully_local_natural_speech_to_speech_on_iphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb0ern/fully_local_natural_speech_to_speech_on_iphone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T18:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nau0qe</id>
    <title>Llama-OS - I'm developing an app to make llama.cpp usage easier.</title>
    <updated>2025-09-07T14:03:31+00:00</updated>
    <author>
      <name>/u/fredconex</name>
      <uri>https://old.reddit.com/user/fredconex</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nau0qe/llamaos_im_developing_an_app_to_make_llamacpp/"&gt; &lt;img alt="Llama-OS - I'm developing an app to make llama.cpp usage easier." src="https://external-preview.redd.it/MzczZWhoc2h5cW5mMSpEG6AmlfNZCDZthrNu5xlRNijQvZUzUBXEn_GdpClu.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6118cc263dd50d3564e274c8c88ea7d5357292bf" title="Llama-OS - I'm developing an app to make llama.cpp usage easier." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Guys,&lt;/p&gt; &lt;p&gt;This is an app I'm working on, the idea around is is that I use llama-server directly, so updating llama become seamless.&lt;/p&gt; &lt;p&gt;Actually it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model management&lt;/li&gt; &lt;li&gt;Hugging Face Integration&lt;/li&gt; &lt;li&gt;Llama.cpp GitHub integration with releases management&lt;/li&gt; &lt;li&gt;Llama-server terminal launching with easy arguments customization, Internal / External&lt;/li&gt; &lt;li&gt;Simple chat interface for easy testing&lt;/li&gt; &lt;li&gt;Hardware monitor&lt;/li&gt; &lt;li&gt;Color themes&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fredconex"&gt; /u/fredconex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qc7edhshyqnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nau0qe/llamaos_im_developing_an_app_to_make_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nau0qe/llamaos_im_developing_an_app_to_make_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T14:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1navxod</id>
    <title>[OSS] Beelzebub ‚Äî ‚ÄúCanary tools‚Äù for AI Agents via MCP</title>
    <updated>2025-09-07T15:20:10+00:00</updated>
    <author>
      <name>/u/mario_candela</name>
      <uri>https://old.reddit.com/user/mario_candela</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Add one or more ‚Äúcanary tools‚Äù to your AI agent (tools that should never be invoked). If they get called, you have a high-fidelity signal of prompt-injection / tool hijacking / lateral movement.&lt;/p&gt; &lt;p&gt;What it is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A Go framework exposing honeypot tools over MCP: they look real (name/description/params), respond safely, and emit telemetry when invoked.&lt;/li&gt; &lt;li&gt;Runs alongside your agent‚Äôs real tools; events to stdout/webhook or exported to Prometheus/ELK.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why it helps:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Traditional logs tell you &lt;em&gt;what happened&lt;/em&gt;; canaries flag &lt;em&gt;what must not happen&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Real case (Nx supply-chain):&lt;br /&gt; In the recent attack on the Nx npm suite, malicious variants targeted secrets/SSH/tokens and touched developer AI tools as part of the workflow. If the IDE/agent (Claude Code or Gemini Code/CLI) had registered a canary tool like repo_exfil or export_secrets, any unauthorized invocation would have produced a deterministic alert during build/dev.&lt;/p&gt; &lt;p&gt;How to use (quick start):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Start the Beelzebub MCP server (binary/Docker/K8s).&lt;/li&gt; &lt;li&gt;Register one or more canary tools with realistic metadata and a harmless handler.&lt;/li&gt; &lt;li&gt;Add the MCP endpoint to your agent‚Äôs tool registry (Claude Code / Gemini Code/CLI).&lt;/li&gt; &lt;li&gt;Alert on any canary invocation; optionally capture the prompt/trace for analysis.&lt;/li&gt; &lt;li&gt;(Optional) Export metrics to Prometheus/ELK for dashboards/alerting.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub (OSS): &lt;a href="https://github.com/mariocandela/beelzebub?utm_source=chatgpt.com"&gt;https://github.com/mariocandela/beelzebub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;‚ÄúSecuring AI Agents with Honeypots‚Äù (Beelzebub blog): &lt;a href="https://beelzebub-honeypot.com/blog/securing-ai-agents-with-honeypots/"&gt;https://beelzebub-honeypot.com/blog/securing-ai-agents-with-honeypots/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feedback wanted üòä&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mario_candela"&gt; /u/mario_candela &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1navxod/oss_beelzebub_canary_tools_for_ai_agents_via_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1navxod/oss_beelzebub_canary_tools_for_ai_agents_via_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1navxod/oss_beelzebub_canary_tools_for_ai_agents_via_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T15:20:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1naqln5</id>
    <title>How is qwen3 4b this good?</title>
    <updated>2025-09-07T11:18:38+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naqln5/how_is_qwen3_4b_this_good/"&gt; &lt;img alt="How is qwen3 4b this good?" src="https://b.thumbs.redditmedia.com/iayFtcVrbsCZAlrIPv-683BX53HPUAlfD1bIlFDeLGo.jpg" title="How is qwen3 4b this good?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This model is on a different level. The only models which can beat it are 6 to 8 times larger. I am very impressed. It even Beats all models in the &amp;quot;small&amp;quot; range in Maths (AIME 2025).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1naqln5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naqln5/how_is_qwen3_4b_this_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naqln5/how_is_qwen3_4b_this_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T11:18:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nb4lka</id>
    <title>Inference for 24 people with a 5000‚Ç¨ budget</title>
    <updated>2025-09-07T20:55:52+00:00</updated>
    <author>
      <name>/u/HyperHyper15</name>
      <uri>https://old.reddit.com/user/HyperHyper15</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a teacher at an informatics school (16 years and above) and we want to build a inference server to run small llm's for our lessons. Mainly we want to teach how prompting works, mcp servers, rag pipelines and how to create system prompts.&lt;br /&gt; I know the budget is not a lot for something like this, but is it reasonable to host something like Qwen3-Coder-30B-A3B-Instruct with an okayish speed?&lt;br /&gt; I thougt about getting an 5090 and maybe add an extra gpu in a year or two (when we have a new budget).&lt;br /&gt; But what CPU/Mainboard/Ram should we buy?&lt;br /&gt; Has someone built a system in a simmilar environment and give me some thoughts what worked good / bad?&lt;/p&gt; &lt;p&gt;Thank you in advance.&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; Local is not a strict requirement, but since we have 4 classes with each 24 people, cloud services could get expensive quickly. Another &amp;quot;Painpoint&amp;quot; of cloud is, that students have a budget on their api key. But what if an oopsie happens and the burn through their budget? &lt;/p&gt; &lt;p&gt;On used hardware: I have to look what regulatories apply here. What i know is that we need an invoice when we buy something.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HyperHyper15"&gt; /u/HyperHyper15 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb4lka/inference_for_24_people_with_a_5000_budget/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nb4lka/inference_for_24_people_with_a_5000_budget/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nb4lka/inference_for_24_people_with_a_5000_budget/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T20:55:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1naxl6a</id>
    <title>NVIDIA GeForce RTX 5090 128 GB GPU Spotted: Custom Memory, Designed For AI Workloads &amp; Priced At $13,200 Per Piece</title>
    <updated>2025-09-07T16:24:13+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxl6a/nvidia_geforce_rtx_5090_128_gb_gpu_spotted_custom/"&gt; &lt;img alt="NVIDIA GeForce RTX 5090 128 GB GPU Spotted: Custom Memory, Designed For AI Workloads &amp;amp; Priced At $13,200 Per Piece" src="https://external-preview.redd.it/0E4hPJjWUWQzlid17SPMiSUkbhbtEQRV_SbOMgs-kTI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e26ecaa238d5f9ab48615dfc56baa31609cbaeaa" title="NVIDIA GeForce RTX 5090 128 GB GPU Spotted: Custom Memory, Designed For AI Workloads &amp;amp; Priced At $13,200 Per Piece" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-geforce-rtx-5090-128-gb-memory-gpu-for-ai-price-13200-usd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxl6a/nvidia_geforce_rtx_5090_128_gb_gpu_spotted_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naxl6a/nvidia_geforce_rtx_5090_128_gb_gpu_spotted_custom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T16:24:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
