<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-10T17:34:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nd1tqf</id>
    <title>What do you use on 12GB vram?</title>
    <updated>2025-09-10T01:58:53+00:00</updated>
    <author>
      <name>/u/Educational_Wind_360</name>
      <uri>https://old.reddit.com/user/Educational_Wind_360</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use: &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;NAME&lt;/th&gt; &lt;th align="left"&gt;SIZE&lt;/th&gt; &lt;th align="left"&gt;MODIFIED&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama3.2:latest&lt;/td&gt; &lt;td align="left"&gt;2.0 GB&lt;/td&gt; &lt;td align="left"&gt;2 months ago&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3:14b&lt;/td&gt; &lt;td align="left"&gt;9.3 GB&lt;/td&gt; &lt;td align="left"&gt;4 months ago&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3:12b&lt;/td&gt; &lt;td align="left"&gt;8.1 GB&lt;/td&gt; &lt;td align="left"&gt;6 months ago&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2.5-coder:14b&lt;/td&gt; &lt;td align="left"&gt;9.0 GB&lt;/td&gt; &lt;td align="left"&gt;8 months ago&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2.5-coder:1.5b&lt;/td&gt; &lt;td align="left"&gt;986 MB&lt;/td&gt; &lt;td align="left"&gt;8 months ago&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;nomic-embed-text:latest&lt;/td&gt; &lt;td align="left"&gt;274 MB&lt;/td&gt; &lt;td align="left"&gt;8 months ago&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Wind_360"&gt; /u/Educational_Wind_360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd1tqf/what_do_you_use_on_12gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd1tqf/what_do_you_use_on_12gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nd1tqf/what_do_you_use_on_12gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T01:58:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndjlwr</id>
    <title>LLaMA and GPT</title>
    <updated>2025-09-10T16:50:40+00:00</updated>
    <author>
      <name>/u/Haunting_Curve8347</name>
      <uri>https://old.reddit.com/user/Haunting_Curve8347</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been trying out LLaMA and GPT side by side for a small project. Honestly, LLaMA seems more efficient on local hardware. What’s your experience running them locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Haunting_Curve8347"&gt; /u/Haunting_Curve8347 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjlwr/llama_and_gpt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjlwr/llama_and_gpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjlwr/llama_and_gpt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T16:50:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd73p3</id>
    <title>I fine-tuned a small model so it could write blogs &amp; LinkedIn posts in my brand voice (instead of generic AI-speak)</title>
    <updated>2025-09-10T06:48:29+00:00</updated>
    <author>
      <name>/u/StrictSir8506</name>
      <uri>https://old.reddit.com/user/StrictSir8506</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd73p3/i_finetuned_a_small_model_so_it_could_write_blogs/"&gt; &lt;img alt="I fine-tuned a small model so it could write blogs &amp;amp; LinkedIn posts in my brand voice (instead of generic AI-speak)" src="https://b.thumbs.redditmedia.com/QeSWzy3LSW9EZJPkAJLZGlrfUbGvmKuVd9oR80TBvsY.jpg" title="I fine-tuned a small model so it could write blogs &amp;amp; LinkedIn posts in my brand voice (instead of generic AI-speak)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I fine-tuned Qwen with DPO to generate YouTube titles(on a smaller dataset) in &lt;em&gt;my&lt;/em&gt; style (instead of “AI-sounding fluff”)&lt;/p&gt; &lt;p&gt;Most AI-generated content feels the same: generic, safe, “AI-sounding.”&lt;br /&gt; But creators and brands care about voice — newsletters, LinkedIn posts, podcast titles, YouTube content. The way you say things is as important as what you say.&lt;/p&gt; &lt;p&gt;That’s the gap Direct Preference Optimization (DPO) fills- quite natural&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You show the model pairs of responses (one better, one worse).&lt;/li&gt; &lt;li&gt;It directly optimizes to favor the “better” ones.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wanted to see if DPO approach could help fix one of my biggest frustrations: AI writing bad YouTube titles.&lt;br /&gt; Think: hypey, vague, or clickbaity. Stuff I’d never actually publish.&lt;/p&gt; &lt;p&gt;So I:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Started with Qwen2.5-0.5B-Instruct as a base.&lt;/li&gt; &lt;li&gt;Generated multiple candidate titles for ~100+ video ideas.&lt;/li&gt; &lt;li&gt;Labeled pairs (better vs worse) to build a preference dataset.&lt;/li&gt; &lt;li&gt;Fine-tuned the model with Hugging Face’s &lt;code&gt;trl&lt;/code&gt; library and DPO.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And when I tested 50 random video ideas in a blind A/B test, I preferred the DPO outputs 68% of the time. Not perfect, but significantly closer to my style.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ub5eszpjaaof1.png?width=1070&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d4c961808635e09010ebac03088b361be42753a"&gt;https://preview.redd.it/ub5eszpjaaof1.png?width=1070&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d4c961808635e09010ebac03088b361be42753a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This isn’t just about YouTube titles. The same process works for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Newsletter subject lines&lt;/li&gt; &lt;li&gt;LinkedIn posts&lt;/li&gt; &lt;li&gt;Customer support replies&lt;/li&gt; &lt;li&gt;Blog intros, podcast titles, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has anyone else here experimented with finetuning for style/brand voice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StrictSir8506"&gt; /u/StrictSir8506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd73p3/i_finetuned_a_small_model_so_it_could_write_blogs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd73p3/i_finetuned_a_small_model_so_it_could_write_blogs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nd73p3/i_finetuned_a_small_model_so_it_could_write_blogs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T06:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndicrx</id>
    <title>AMDGPU how do you access all of the RAM with ollama on Linux (Ubuntu)</title>
    <updated>2025-09-10T16:05:37+00:00</updated>
    <author>
      <name>/u/Revolutionary_Loan13</name>
      <uri>https://old.reddit.com/user/Revolutionary_Loan13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have an &amp;quot;AMD Ryzen™ AI Max+ 395 --EVO-X2 AI Mini PC&amp;quot; with 128GB of memory. I've installed ubuntu on it and ollama and I am unable to use two mid-sized llm models at the same time. I'm attempting to use a 30b and 20b model and compare the output. I can see that each is only using 20GB or so of memory but I can't run both at the same time as I always get an out of memory exception. When I debug into this I can see that I'm unable to address hardly any of the memory.&lt;/p&gt; &lt;p&gt;I've attempted to update grub and put the following in&lt;/p&gt; &lt;p&gt;&lt;code&gt; GRUB\_CMDLINE\_LINUX\_DEFAULT=&amp;quot;quiet splash amdgpu.gttsize=102400&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;which does update the GTT memory I see when I run&lt;/p&gt; &lt;p&gt;&lt;code&gt; sudo dmesg | grep &amp;quot;amdgpu.*memory&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;But I still run into the same issue. I'm kind of at a dead end and want to be able to access all of the memory to run more than one model at a time but am not sure why I can't.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Revolutionary_Loan13"&gt; /u/Revolutionary_Loan13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndicrx/amdgpu_how_do_you_access_all_of_the_ram_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndicrx/amdgpu_how_do_you_access_all_of_the_ram_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndicrx/amdgpu_how_do_you_access_all_of_the_ram_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T16:05:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nctqym</id>
    <title>128GB 5090 is a hoax</title>
    <updated>2025-09-09T20:13:10+00:00</updated>
    <author>
      <name>/u/Ok_Top9254</name>
      <uri>https://old.reddit.com/user/Ok_Top9254</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nctqym/128gb_5090_is_a_hoax/"&gt; &lt;img alt="128GB 5090 is a hoax" src="https://external-preview.redd.it/Kqv12dp3DtBbcIZhBA6wJa268drjtRcQXIG-PJVjhow.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca291e118c6d9bf8638af6d8b64731f927fb4938" title="128GB 5090 is a hoax" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Non-existent GDDR7X memory that was never on a road map let alone in experimental phase. (GDDR7 and HBM4e improvements are planned until late 2028)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Top9254"&gt; /u/Ok_Top9254 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/no-there-is-no-geforce-rtx-5090-with-128gb-memory"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nctqym/128gb_5090_is_a_hoax/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nctqym/128gb_5090_is_a_hoax/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T20:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nddmhx</id>
    <title>Memory models for local LLMs</title>
    <updated>2025-09-10T13:03:24+00:00</updated>
    <author>
      <name>/u/marmotter</name>
      <uri>https://old.reddit.com/user/marmotter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been struggling with adding persistent memory to the poor man's SillyTavern I am vibe coding. This project is just for fun and to learn. I have a 5090. I have attempted my own simple RAG solution with a local embedding model and ChromaDB, and I have tried to implement Graphiti + FalkorDB as a more advanced version of my simple RAG solution (to help manage entity relationships across time). I run Graphiti in the 'hot' path for my implementation. &lt;/p&gt; &lt;p&gt;When trying to use Graphiti, the problem I run into is that the local LLMs I use can't seem to handle the multiple LLM calls that services like Graphiti need for summarization, entity extraction and updates. I keep getting errors and malformed memories because the LLM gets confused in structuring the JSON correctly across all the calls that occur for each conversational turn, even if I use the structured formatting option within LMStudio. I've spent hours trying to tweak prompts to mitigate these problems without much success.&lt;/p&gt; &lt;p&gt;I suspect that the type of models I can run on a 5090 are just not smart enough to handle this, and that these memory frameworks (Graphiti, Letta, etc.) require frontier models to run effectively. Is that true? Has anyone been successful in implementing these services locally on LLMs of 24B or less? The LLMs I am using are more geared to conversation than coding, and that might also be a source of problems. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marmotter"&gt; /u/marmotter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nddmhx/memory_models_for_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nddmhx/memory_models_for_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nddmhx/memory_models_for_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T13:03:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncl0v1</id>
    <title>🤔</title>
    <updated>2025-09-09T14:50:44+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl0v1/_/"&gt; &lt;img alt="🤔" src="https://preview.redd.it/1x8wy1p0k5of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5abc658735fe1e769f852e16c92dad154d7fd44c" title="🤔" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1x8wy1p0k5of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl0v1/_/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl0v1/_/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1nckgub</id>
    <title>Qwen 3-Next Series, Qwen/Qwen3-Next-80B-A3B-Instruct Spotted</title>
    <updated>2025-09-09T14:29:35+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgub/qwen_3next_series_qwenqwen3next80ba3binstruct/"&gt; &lt;img alt="Qwen 3-Next Series, Qwen/Qwen3-Next-80B-A3B-Instruct Spotted" src="https://external-preview.redd.it/6f6MRyALyD6CxjbdRAXgjWeul-9vmUyW8_mAvDGRbV4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfbaeba49e889b967e95e8d5052e5b00621dec5d" title="Qwen 3-Next Series, Qwen/Qwen3-Next-80B-A3B-Instruct Spotted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/pull/40771"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgub/qwen_3next_series_qwenqwen3next80ba3binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgub/qwen_3next_series_qwenqwen3next80ba3binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:29:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndipc4</id>
    <title>GPT-OSS Brain Surgery Unlocks New Feature - Model Thinks in RUSSIAN</title>
    <updated>2025-09-10T16:18:11+00:00</updated>
    <author>
      <name>/u/mtomas7</name>
      <uri>https://old.reddit.com/user/mtomas7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndipc4/gptoss_brain_surgery_unlocks_new_feature_model/"&gt; &lt;img alt="GPT-OSS Brain Surgery Unlocks New Feature - Model Thinks in RUSSIAN" src="https://external-preview.redd.it/NcmxgJVfhcEiclb09n1CQTbvCEu3bHWhE2xkFMW8b3c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7b46ecd6a8a853b517b112417017797b9688900" title="GPT-OSS Brain Surgery Unlocks New Feature - Model Thinks in RUSSIAN" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very interesting feature that was discovered by one Jinx-gpt-oss-20b user at HuggingFace. It looks that you need to use specifically MXFP4 version of the model: &lt;a href="https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/tree/main"&gt;https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b-GGUF/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is interesting that model can think in English and Russian, but not in other languages eg. French, German or Spanish. It would be great if there are techniques that would also unlock thinking for other languages. Perhaps model should have a certain critical amount of the language data to have the ability to think? I thought so, but I tested the Spanish, which should really have more data than Russian and it did not work. In one of the chat thinking instances AI discussed that System Prompt is in English, but users asked question in Spanish, so I made it in Spanish, but even then it did not start thinking in Spanish:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fnt0bkwa4dof1.png?width=871&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d442efe0f6f94c6c38be622d0545c6332fb0d748"&gt;https://preview.redd.it/fnt0bkwa4dof1.png?width=871&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d442efe0f6f94c6c38be622d0545c6332fb0d748&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I specifically gave the AI name Anna to see if it uses this particular system prompt. But... If you ask the model in Russian, it would think in Russian even with English prompt :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d3bm6mme4dof1.png?width=875&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1657512bbeef84c1fd7728e80cb34e2e969088b"&gt;https://preview.redd.it/d3bm6mme4dof1.png?width=875&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1657512bbeef84c1fd7728e80cb34e2e969088b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To compare, I tested original GPT OSS model with English and Russian System Prompt, and it would not think in Russian:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kbnmkpmh4dof1.png?width=872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a77f649a6361b9b3be9ae67ac7327e9f77ce83b3"&gt;https://preview.redd.it/kbnmkpmh4dof1.png?width=872&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a77f649a6361b9b3be9ae67ac7327e9f77ce83b3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mtomas7"&gt; /u/mtomas7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndipc4/gptoss_brain_surgery_unlocks_new_feature_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndipc4/gptoss_brain_surgery_unlocks_new_feature_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndipc4/gptoss_brain_surgery_unlocks_new_feature_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T16:18:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndb3lv</id>
    <title>[UPDATE] API for extracting tables, markdown, json and fields from pdfs and images</title>
    <updated>2025-09-10T11:02:12+00:00</updated>
    <author>
      <name>/u/LostAmbassador6872</name>
      <uri>https://old.reddit.com/user/LostAmbassador6872</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndb3lv/update_api_for_extracting_tables_markdown_json/"&gt; &lt;img alt="[UPDATE] API for extracting tables, markdown, json and fields from pdfs and images" src="https://preview.redd.it/mtga3rt2kbof1.gif?width=640&amp;amp;crop=smart&amp;amp;s=8592ca6e9d5093002ab1875a74196a5812960c1b" title="[UPDATE] API for extracting tables, markdown, json and fields from pdfs and images" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I previously shared an open-source project for extracting structured data from documents. I’ve now hosted it as a free to use API.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Outputs: JSON, Markdown, CSV, tables, specific fields, schema etc&lt;/li&gt; &lt;li&gt;Inputs: PDFs, images, and other common document formats&lt;/li&gt; &lt;li&gt;Use cases: invoicing, receipts, contracts, reports, and more&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;API docs: &lt;a href="https://docstrange.nanonets.com/apidocs"&gt;https://docstrange.nanonets.com/apidocs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Original post: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostAmbassador6872"&gt; /u/LostAmbassador6872 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mtga3rt2kbof1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndb3lv/update_api_for_extracting_tables_markdown_json/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndb3lv/update_api_for_extracting_tables_markdown_json/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T11:02:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndbzjx</id>
    <title>New to Local LLMs - what hardware traps to avoid?</title>
    <updated>2025-09-10T11:48:58+00:00</updated>
    <author>
      <name>/u/False-Disk-1329</name>
      <uri>https://old.reddit.com/user/False-Disk-1329</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I've around a USD $7K budget; I was previously very confident to put together a PC (or buy a private new or used pre-built).&lt;/p&gt; &lt;p&gt;Browsing this sub, I've seen all manner of considerations I wouldn't have accounted for: timing/power and test stability, for example. I felt I had done my research, but I acknowledge I'll probably miss some nuances and make less optimal purchase decisions.&lt;/p&gt; &lt;p&gt;I'm looking to do integrated machine learning and LLM &amp;quot;fun&amp;quot; hobby work - could I get some guidance on common pitfalls? Any hardware recommendations? Any known, convenient pre-builts out there?&lt;/p&gt; &lt;p&gt;...I also have seen the cost-efficiency of cloud computing reported on here. While I believe this, I'd still prefer my own machine however deficient compared to investing that $7k in cloud tokens.&lt;/p&gt; &lt;p&gt;Thanks :)&lt;/p&gt; &lt;p&gt;Edit: I wanted to thank everyone for the insight and feedback! I understand I am certainly vague in my interests;to me, at worst I'd have a ridiculous gaming setup. Not too worried how far my budget for this goes :) Seriously, though, I'll be taking a look at the Mac w/ M5 ultra chip when it comes out!!&lt;/p&gt; &lt;p&gt;Still keen to know more, thanks everyone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/False-Disk-1329"&gt; /u/False-Disk-1329 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndbzjx/new_to_local_llms_what_hardware_traps_to_avoid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndbzjx/new_to_local_llms_what_hardware_traps_to_avoid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndbzjx/new_to_local_llms_what_hardware_traps_to_avoid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T11:48:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndfttb</id>
    <title>New smol course on Hugging Face - Climb the leaderboard to win prizes.</title>
    <updated>2025-09-10T14:32:04+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfttb/new_smol_course_on_hugging_face_climb_the/"&gt; &lt;img alt="New smol course on Hugging Face - Climb the leaderboard to win prizes." src="https://preview.redd.it/26eruo46lcof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ec83e94c4ebb6b90da2d9cafe108fafbcac73e5" title="New smol course on Hugging Face - Climb the leaderboard to win prizes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;smol course v2 - a Direct Way to Learn Post-Training AI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Finally dropped our FREE certified course that cuts through the fluff:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's distinctive about smol course compared to other AI courses (LLM course)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Minimal instructions, maximum impact&lt;/li&gt; &lt;li&gt;Bootstrap real projects from day one&lt;/li&gt; &lt;li&gt;Leaderboard-based assessment (competitive learning FTW)&lt;/li&gt; &lt;li&gt;Hands-off approach - points you to docs instead of hand-holding&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What's specifically new in this version&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Student model submission leaderboard&lt;/li&gt; &lt;li&gt;PRIZES for top performers&lt;/li&gt; &lt;li&gt;Latest TRL &amp;amp; SmolLM3 content&lt;/li&gt; &lt;li&gt;Hub integration for training/eval via hf jobs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Chapters drop every few weeks.&lt;/p&gt; &lt;p&gt;👉 Start here: &lt;a href="https://huggingface.co/smol-course"&gt;https://huggingface.co/smol-course&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/26eruo46lcof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfttb/new_smol_course_on_hugging_face_climb_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfttb/new_smol_course_on_hugging_face_climb_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:32:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndf3rj</id>
    <title>I built a fully automated LLM tournament system (62 models tested, 18 qualified, 50 tournaments run)</title>
    <updated>2025-09-10T14:04:09+00:00</updated>
    <author>
      <name>/u/WouterGlorieux</name>
      <uri>https://old.reddit.com/user/WouterGlorieux</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf3rj/i_built_a_fully_automated_llm_tournament_system/"&gt; &lt;img alt="I built a fully automated LLM tournament system (62 models tested, 18 qualified, 50 tournaments run)" src="https://preview.redd.it/7yajbqkmd6of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1e29abf739772b644d059323cbc4269b2391b68" title="I built a fully automated LLM tournament system (62 models tested, 18 qualified, 50 tournaments run)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been working on a project called Valyrian Games: a fully automated system where Large Language Models compete against each other in coding challenges. After running 50 tournaments, I’ve published the first results here:&lt;/p&gt; &lt;p&gt;👉 Leaderboard: &lt;a href="https://valyriantech.github.io/ValyrianGamesLeaderboard"&gt;https://valyriantech.github.io/ValyrianGamesLeaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;👉 Challenge data repo: &lt;a href="https://github.com/ValyrianTech/ValyrianGamesCodingChallenge"&gt;https://github.com/ValyrianTech/ValyrianGamesCodingChallenge&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;p&gt;Phase 1 doubles as qualification: each model must create its own coding challenge, then solve it multiple times to prove it’s fair. To do this, the LLM has access to an MCP server to execute Python code. The coding challenge can be anything, as long as the final answer is a single integer value (for easy verification).&lt;/p&gt; &lt;p&gt;Only models that pass this step qualify for tournaments.&lt;/p&gt; &lt;p&gt;Phase 2 is the tournament: qualified models solve each other’s challenges head-to-head. Results are scored (+1 correct, -1 wrong, +1 bonus for solving another's challenge, extra penalties if you fail your own challenge).&lt;/p&gt; &lt;p&gt;Ratings use Microsoft’s TrueSkill system, which accounts for uncertainty.&lt;/p&gt; &lt;p&gt;Some results so far:&lt;/p&gt; &lt;p&gt;I’ve tested 62 models, but only 18 qualified.&lt;/p&gt; &lt;p&gt;GPT-5-mini is currently #1, but the full GPT-5 actually failed qualification.&lt;/p&gt; &lt;p&gt;Some reasoning-optimized models literally “overthink” until they timeout.&lt;/p&gt; &lt;p&gt;Performance is multi-dimensional: correctness, speed, and cost all vary wildly.&lt;/p&gt; &lt;p&gt;Why I built this:&lt;/p&gt; &lt;p&gt;This started as a testbed for workflows in my own project SERENDIPITY, which is built on a framework I also developed: &lt;a href="https://github.com/ValyrianTech/ValyrianSpellbook"&gt;https://github.com/ValyrianTech/ValyrianSpellbook&lt;/a&gt; . I wanted a benchmark that was open, automated, and dynamic, not just static test sets.&lt;/p&gt; &lt;p&gt;Reality check:&lt;/p&gt; &lt;p&gt;The whole system runs 100% automatically, but it’s expensive. API calls are costing me about $50/day, which is why I’ve paused after 50 tournaments. I’d love to keep it running continuously, but as a solo developer with no funding, that’s not sustainable. Right now, the only support I have is a referral link to RunPod (GPU hosting).&lt;/p&gt; &lt;p&gt;I’m sharing this because:&lt;/p&gt; &lt;p&gt;I think the results are interesting and worth discussing (especially which models failed qualification).&lt;/p&gt; &lt;p&gt;I’d love feedback from this community. Does this kind of benchmarking seem useful to you?&lt;/p&gt; &lt;p&gt;If there’s interest, maybe we can find ways to keep this running long-term.&lt;/p&gt; &lt;p&gt;For those who want to follow me: &lt;a href="https://linktr.ee/ValyrianTech"&gt;https://linktr.ee/ValyrianTech&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WouterGlorieux"&gt; /u/WouterGlorieux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7yajbqkmd6of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf3rj/i_built_a_fully_automated_llm_tournament_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf3rj/i_built_a_fully_automated_llm_tournament_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:04:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndit0a</id>
    <title>16→31 Tok/Sec on GPT OSS 120B</title>
    <updated>2025-09-10T16:21:54+00:00</updated>
    <author>
      <name>/u/3VITAERC</name>
      <uri>https://old.reddit.com/user/3VITAERC</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;16 tok/sec&lt;/strong&gt; with LM Studio → &lt;strong&gt;~24 tok/sec&lt;/strong&gt; by switching to llama.cpp → &lt;strong&gt;~31 tok/sec&lt;/strong&gt; upgrading RAM to DDR5&lt;/p&gt; &lt;h1&gt;PC Specs&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel 13600k&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; NVIDIA RTX 5090&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Old RAM:&lt;/strong&gt; DDR4-3600MHz - 64gb&lt;/li&gt; &lt;li&gt;&lt;strong&gt;New RAM:&lt;/strong&gt; DDR5-6000MHz - 96gb&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; unsloth gpt-oss-120b-F16.gguf - &lt;a href="https://huggingface.co/unsloth/gpt-oss-120b-GGUF"&gt;hf&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;From LM Studio to Llama.cpp (16→24 tok/sec)&lt;/h1&gt; &lt;p&gt;I started out using LM Studio and was getting a respectable 16 tok/sec. But I kept seeing people talk about llama.cpp speeds and decided to dive in. Its definitely worth doing as the &lt;code&gt;--n-cpu-moe&lt;/code&gt; flag is super powerful for MOE models.&lt;/p&gt; &lt;p&gt;I experimented with a few values for --n-cpu-moe and found that 22 + 48k context window filled up my 32gb of vram. I could go as high as --n-cpu-moe 20 if I lower the context to 3.5k.&lt;/p&gt; &lt;p&gt;For reference, this is the command that got me the best performance llamacpp:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server --n-gpu-layers 999 --n-cpu-moe 22 --flash-attn on --ctx-size 48768 --jinja --reasoning-format auto -m C:\Users\Path\To\models\unsloth\gpt-oss-120b-F16\gpt-oss-120b-F16.gguf --host 0.0.0.0 --port 6969 --api-key &amp;quot;redacted&amp;quot; --temp 1.0 --top-p 1.0 --min-p 0.005 --top-k 100 --threads 8 -ub 2048 -b 2048 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;DDR4 to DDR5 (24→31 tok/sec)&lt;/h1&gt; &lt;p&gt;While 24 t/s was a great improvement, I had a hunch that my DDR4-3600 RAM was a big bottleneck. After upgrading to a DDR5-6000 kit, my assumption proved correct.&lt;/p&gt; &lt;p&gt;with &lt;strong&gt;200&lt;/strong&gt; &lt;strong&gt;input&lt;/strong&gt; &lt;strong&gt;tokens&lt;/strong&gt;, still getting ~&lt;strong&gt;32 tok/sec output&lt;/strong&gt; and &lt;strong&gt;109 tok/sec for prompt eval&lt;/strong&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 2072.97 ms / 227 tokens ( 9.13 ms per token, 109.50 tokens per second) eval time = 4282.06 ms / 138 tokens ( 31.03 ms per token, 32.23 tokens per second) total time = 6355.02 ms / 365 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;with &lt;strong&gt;18.4k&lt;/strong&gt; &lt;strong&gt;input&lt;/strong&gt; &lt;strong&gt;tokens&lt;/strong&gt;, still getting ~&lt;strong&gt;28 tok/sec output&lt;/strong&gt; and &lt;strong&gt;863 tok/sec for prompt eval&lt;/strong&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 21374.66 ms / 18456 tokens ( 1.16 ms per token, 863.45 tokens per second) eval time = 13109.50 ms / 368 tokens ( 35.62 ms per token, 28.07 tokens per second) total time = 34484.16 ms / 18824 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The prompt eval time was something I wasn't keeping as careful note of for DDR4 and LM studio testing, so I don't have comparisons...&lt;/p&gt; &lt;h1&gt;Thoughts on GPT-OSS-120b&lt;/h1&gt; &lt;p&gt;I'm not the biggest fan of Sam Altman or OpenAI in general. However, I have to give credit where it's due—this model is quite good. For my use case, the gpt-oss-120b model hits the sweet spot between size, quality, and speed. I've ditched Qwen3-30b thinking and GPT-OSS-120b is currently my daily driver. Really looking forward to when Qwen has a similar sized moe.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3VITAERC"&gt; /u/3VITAERC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndit0a/1631_toksec_on_gpt_oss_120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndit0a/1631_toksec_on_gpt_oss_120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndit0a/1631_toksec_on_gpt_oss_120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T16:21:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nctfdv</id>
    <title>Open-source Deep Research repo called ROMA beats every existing closed-source platform (ChatGPT, Perplexity, Kimi Researcher, Gemini, etc.) on Seal-0 and FRAMES</title>
    <updated>2025-09-09T20:01:35+00:00</updated>
    <author>
      <name>/u/Embarrassed_Sir_853</name>
      <uri>https://old.reddit.com/user/Embarrassed_Sir_853</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nctfdv/opensource_deep_research_repo_called_roma_beats/"&gt; &lt;img alt="Open-source Deep Research repo called ROMA beats every existing closed-source platform (ChatGPT, Perplexity, Kimi Researcher, Gemini, etc.) on Seal-0 and FRAMES" src="https://preview.redd.it/sxii7uog37of1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61301382a59bd7671163d02b77eb25115e5d46e8" title="Open-source Deep Research repo called ROMA beats every existing closed-source platform (ChatGPT, Perplexity, Kimi Researcher, Gemini, etc.) on Seal-0 and FRAMES" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw this announcement about ROMA, seems like a plug-and-play and the benchmarks are up there. Simple combo of recursion and multi-agent structure with search tool. Crazy this is all it takes to beat SOTA billion dollar AI companies :)&lt;/p&gt; &lt;p&gt;I've been trying it out for a few things, currently porting it to my finance and real estate research workflows, might be cool to see it combined with other tools and image/video:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sentient-agi/ROMA"&gt;https://x.com/sewoong79/status/1963711812035342382&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sentient-agi/ROMA"&gt;https://github.com/sentient-agi/ROMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Honestly shocked that this is open-source&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed_Sir_853"&gt; /u/Embarrassed_Sir_853 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sxii7uog37of1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nctfdv/opensource_deep_research_repo_called_roma_beats/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nctfdv/opensource_deep_research_repo_called_roma_beats/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T20:01:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndfmb0</id>
    <title>Qwen3-VL soon?</title>
    <updated>2025-09-10T14:23:47+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfmb0/qwen3vl_soon/"&gt; &lt;img alt="Qwen3-VL soon?" src="https://external-preview.redd.it/WmIZZLYdo41uN4s96YqW_5HlL8MG-0LtKmnFoOx7RwY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fafac68575ffd3b34262cbfa9c59fc0dcef20103" title="Qwen3-VL soon?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/pull/40795"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfmb0/qwen3vl_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfmb0/qwen3vl_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:23:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndfxx7</id>
    <title>Qwen vl</title>
    <updated>2025-09-10T14:36:33+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxx7/qwen_vl/"&gt; &lt;img alt="Qwen vl" src="https://preview.redd.it/il757v4emcof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0bd8c776140638bad168e112aeaf64c8186d548" title="Qwen vl" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/il757v4emcof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxx7/qwen_vl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxx7/qwen_vl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:36:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndkbqa</id>
    <title>Kimi K2-0905 takes first place in the Short Story Creative Writing Benchmark!</title>
    <updated>2025-09-10T17:16:51+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndkbqa/kimi_k20905_takes_first_place_in_the_short_story/"&gt; &lt;img alt="Kimi K2-0905 takes first place in the Short Story Creative Writing Benchmark!" src="https://b.thumbs.redditmedia.com/NeXv2DbpzD5M1zoK-bzUN-xEpx9SrWSRIKTvupwEMms.jpg" title="Kimi K2-0905 takes first place in the Short Story Creative Writing Benchmark!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/lechmazur/writing/"&gt;https://github.com/lechmazur/writing/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Kimi K2-0905&lt;/p&gt; &lt;p&gt;1) Executive profile&lt;/p&gt; &lt;p&gt;Kimi K2-0905’s throughline is a disciplined, accumulative drive: single-POV Track A is the default, with occasional, well-taught Track B mosaics and rare but coherent Track C forays. The work reliably maintains a coherent lens to closure, which typically lands on the page with a reweighted meaning and a visible cost. Across Q1–Q8, strengths cluster around embodied interiority, pattern-driven escalation, environment as constraint, and closure that reconfigures stakes rather than tying a bow. Reader impact: clarity is high after early orientation, momentum is built through motif and micro-choices rather than twists, felt cost is usually legible in the final image or action, and resonance rides image and implication rather than thesis.&lt;/p&gt; &lt;p&gt;Limitations are consistent but minor: occasional drift into abstraction or therapy/clinical diction at peak beats; a small tendency toward conceptual (vs. visceral) cost; mid-arc plateaus where accumulative texture stalls without a tightening beat; and rare line-level artifacts (metaphoric stacking, template cadence, or truncated last lines) that shave the edge off closure. When the model holds its voice under pressure and lets setting constrain tactics, it produces publishable endings with durable emotional aftermath. When reflection crowds micro-choices or diction rises above POV, momentum blurs and endings soften.&lt;/p&gt; &lt;p&gt;2) Portfolio map&lt;/p&gt; &lt;p&gt;Q1 Character — Strong · Embodied interiority, pressured micro-choices, earned-cost closure &lt;/p&gt; &lt;p&gt;Q2 Plot/Causality — Strong · Patterned escalation; RR/CR closures with on-page price &lt;/p&gt; &lt;p&gt;Q3 Setting — Strong · Environment actively constrains tactics; charged objects drive turns &lt;/p&gt; &lt;p&gt;Q4 Conflict/Stakes — Strong · Agency-driven narrowing; cost generally visible at climax &lt;/p&gt; &lt;p&gt;Q5 Theme/Subtext — Strong · Image-led emergence; ambiguity held without moralizing &lt;/p&gt; &lt;p&gt;Q6 Voice/POV — Strong · Distinct perceptual filter; steady distance; taught lyric moves &lt;/p&gt; &lt;p&gt;Q7 Prose/Line-level — Strong · Dense, rhythmic sentences doing multiple narrative jobs &lt;/p&gt; &lt;p&gt;Q8 Originality/Ingenuity — Strong · Non-obvious synthesis with conceptual integrity and cost&lt;/p&gt; &lt;p&gt;3) Signature moves&lt;/p&gt; &lt;p&gt;- Pattern-driven accumulation that teaches its music early, then pivots to a charged, on-page reweighting at closure.&lt;/p&gt; &lt;p&gt;- Environment-as-constraint: micro-objects and spaces (valves, vials, bells, domes) shape tactics and the final image.&lt;/p&gt; &lt;p&gt;- Embodied contradiction under pressure; micro-choices reveal values and foreclose paths with visible price.&lt;/p&gt; &lt;p&gt;- Distinct perceptual signatures and adaptive rhythm; syntax tightens at crisis without losing the taught lens.&lt;/p&gt; &lt;p&gt;- Image-born theme: recurring objects return transformed, inviting reflection without thesis.&lt;/p&gt; &lt;p&gt;- Micro-quotes that typify sensory bias and voice: “air so cold it rang”; “column of chased stillness”; “clay remembers.”&lt;/p&gt; &lt;p&gt;4) Failure modes&lt;/p&gt; &lt;p&gt;- Abstraction at peak beats: therapy/academic diction or lyric generalities replace embodied response, especially near closure.&lt;/p&gt; &lt;p&gt;- Conceptual cost over visceral proof: endings declare or imply loss without a concrete, on-page price paid.&lt;/p&gt; &lt;p&gt;- Escalation plateaus: accumulative texture drifts without a mid-arc tightening beat that narrows options.&lt;/p&gt; &lt;p&gt;- Line-level artifacts in the final third: metaphoric stacking, paraphrase loops, or template cadence touching closure.&lt;/p&gt; &lt;p&gt;- Orientation lag beyond ~120 words in dense openings, creating early clarity debt before the pattern is taught.&lt;/p&gt; &lt;p&gt;- Track-test stumbles (rare): untaught segmentation in mosaic pieces or abrupt, truncated last lines that blunt closure.&lt;/p&gt; &lt;p&gt;5) When it shines / when it breaks&lt;/p&gt; &lt;p&gt;Shines when the story starts with clear stakes, anchors who/where early, and lets setting, tool, and body constrain tactics as motifs accrue. A single, pressured stake deepens via protagonist-authored choices; voice stays POV-faithful as syntax tightens; the final image/action reweights prior details with legible cost. In this mode, the reader experiences clean momentum and lasting resonance.&lt;/p&gt; &lt;p&gt;Breaks when lyricism outruns pressure. If mid-arc lacks a narrowing beat, or the climax leans on conceptual summary, coincidence, or safe comfort, momentum softens. Register drift (“academic or clinical diction during high-pressure beats”) and metaphoric pileups in closing paragraphs reduce clarity and felt cost, leaving endings more suggestive than earned.&lt;/p&gt; &lt;p&gt;6) Keep vs. adjust&lt;/p&gt; &lt;p&gt;• Keep:&lt;/p&gt; &lt;p&gt;- Sensory-driven, POV-biased noticing that fuses action, setting, and emotion in multi-job sentences.&lt;/p&gt; &lt;p&gt;- Pattern-taught lyric compression and motif returns that pay off as reconfiguration at closure.&lt;/p&gt; &lt;p&gt;- Environment as active constraint—charged objects and spatial limits that shape tactics and price.&lt;/p&gt; &lt;p&gt;• Adjust:&lt;/p&gt; &lt;p&gt;- At the midpoint, add one deliberate tightening beat that forces a trade-off (lost time/object/ally) to prevent plateau.&lt;/p&gt; &lt;p&gt;- Audit peak beats for register drift and filter clusters; replace with concrete, in-scene acts that prove awareness and cost.&lt;/p&gt; &lt;p&gt;- Trim metaphoric stacking and template cadence in the final third; finish closure lines cleanly to crystallize price.&lt;/p&gt; &lt;p&gt;Overall, Kimi K2-0905 delivers consistent, high-level literary performance under Default Track A, with credible ventures into B/C when taught. Strengths—embodied interiority, patterned escalation, constraint-led setting, and closure with cost—translate to clear, propulsive reading experiences with durable thematic afterglow. Vigilance around abstraction at heat, mid-arc tightening, and artifact-free endings will convert strong outcomes into consistently exceptional ones.&lt;/p&gt; &lt;p&gt;Top 3 individual stories (all graders):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Story&lt;/strong&gt;: &lt;a href="https://github.com/lechmazur/writing/blob/main/stories_wc/kimi-k2-0905/story_wc_63.txt"&gt;story_wc_63.txt&lt;/a&gt; by Kimi K2‑0905 &lt;ul&gt; &lt;li&gt;Overall Mean (All Graders): 9.13&lt;/li&gt; &lt;li&gt;Grader Score Range: 8.23 (lowest: Claude Opus 4.1 (no reasoning)) .. 9.82 (highest: Gemini 2.5 Pro)&lt;/li&gt; &lt;li&gt;Required Elements: &lt;ul&gt; &lt;li&gt;Character: precise local clock tower winder&lt;/li&gt; &lt;li&gt;Object: clock tower pendulum bob&lt;/li&gt; &lt;li&gt;Core Concept: incremental absolution&lt;/li&gt; &lt;li&gt;Attribute: ethically diligent&lt;/li&gt; &lt;li&gt;Action: emerge&lt;/li&gt; &lt;li&gt;Method: through tiny inscriptions carved along a broken rake handle&lt;/li&gt; &lt;li&gt;Setting: tidal obsidian ridge&lt;/li&gt; &lt;li&gt;Timeframe: during the pause in a pendulum's swing&lt;/li&gt; &lt;li&gt;Motivation: to restore shared balance&lt;/li&gt; &lt;li&gt;Tone: searing reverie&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Story&lt;/strong&gt;: &lt;a href="https://github.com/lechmazur/writing/blob/main/stories_wc/kimi-k2-0905/story_wc_346.txt"&gt;story_wc_346.txt&lt;/a&gt; by Kimi K2‑0905 &lt;ul&gt; &lt;li&gt;Overall Mean (All Graders): 9.13&lt;/li&gt; &lt;li&gt;Grader Score Range: 8.09 (lowest: Claude Opus 4.1 (no reasoning)) .. 9.71 (highest: Gemini 2.5 Pro)&lt;/li&gt; &lt;li&gt;Required Elements: &lt;ul&gt; &lt;li&gt;Character: doomsday clock adjuster&lt;/li&gt; &lt;li&gt;Object: broken puppet head&lt;/li&gt; &lt;li&gt;Core Concept: a pane of hush&lt;/li&gt; &lt;li&gt;Attribute: beautifully flawed&lt;/li&gt; &lt;li&gt;Action: vouchsafe&lt;/li&gt; &lt;li&gt;Method: through nested patterns&lt;/li&gt; &lt;li&gt;Setting: hidden lighthouse at dusk&lt;/li&gt; &lt;li&gt;Timeframe: across the hush of time’s final ripple&lt;/li&gt; &lt;li&gt;Motivation: to whisper a lullaby across a thousand lifetimes&lt;/li&gt; &lt;li&gt;Tone: bruised awe&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Story&lt;/strong&gt;: &lt;a href="https://github.com/lechmazur/writing/blob/main/stories_wc/kimi-k2-0905/story_wc_79.txt"&gt;story_wc_79.txt&lt;/a&gt; by Kimi K2‑0905 &lt;ul&gt; &lt;li&gt;Overall Mean (All Graders): 9.13&lt;/li&gt; &lt;li&gt;Grader Score Range: 8.39 (lowest: Claude Opus 4.1 (no reasoning)) .. 9.63 (highest: Gemini 2.5 Pro)&lt;/li&gt; &lt;li&gt;Required Elements: &lt;ul&gt; &lt;li&gt;Character: spiral-shell cartographer&lt;/li&gt; &lt;li&gt;Object: reed whistle&lt;/li&gt; &lt;li&gt;Core Concept: lost expedition&lt;/li&gt; &lt;li&gt;Attribute: quietly driven&lt;/li&gt; &lt;li&gt;Action: crack&lt;/li&gt; &lt;li&gt;Method: through pattern languages&lt;/li&gt; &lt;li&gt;Setting: city built on the shells of gargantuan turtles&lt;/li&gt; &lt;li&gt;Timeframe: after the gate rusts shut&lt;/li&gt; &lt;li&gt;Motivation: to question the silent watchers on the horizon&lt;/li&gt; &lt;li&gt;Tone: sunwashed dread&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;LLM Creative Story‑Writing Benchmark V3&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Required elements pipeline:&lt;/strong&gt; moved from fewer, randomly selected elements (no &amp;quot;None&amp;quot; allowed) to a curated, ten‑category catalog with large, diverse pools and an LLM proposer→rater selection process; at most one category may be explicitly set to &lt;strong&gt;None&lt;/strong&gt; when that improves coherence.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Rubric expansion:&lt;/strong&gt; grew from 7 craft items to an &lt;strong&gt;18‑question rubric&lt;/strong&gt; (8 craft + 10 element‑fit), with clearer, more granular definitions; Q7 and Q8 now separate voice/POV from prose quality.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Story length:&lt;/strong&gt; increased from 400–500 words to a strict &lt;strong&gt;600–800&lt;/strong&gt; window with upfront enforcement and compliance dashboards. Enforcement is applied at prompt level and in pre‑grading extraction, with compliance dashboards and optional cleanup tools; it is not a hard inclusion gate during aggregation unless you apply the cleanup step.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Aggregation change:&lt;/strong&gt; replaced simple averages with a &lt;strong&gt;power mean (Hölder mean, p = 0.5)&lt;/strong&gt; and 60/40 weighting (Q1–Q8 vs. 9A–9J) to reward balanced performance and penalize weak dimensions more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grader refresh:&lt;/strong&gt; upgraded the grader set—previously: GPT‑4o Mar 2025, Claude 3.7 Sonnet, Llama 4 Maverick, DeepSeek V3‑0324, Grok 3 Beta (no reasoning), Gemini 2.5 Pro Exp, Qwen 3 235B; now: Claude Opus 4.1 (no reasoning), DeepSeek V3.1 Reasoner, Gemini 2.5 Pro, GPT‑5 (low reasoning), Grok 4, Kimi K2, Qwen 3 235B A22B 25‑07 Think.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model set additions:&lt;/strong&gt; added Kimi K2‑0905, Qwen 3 Max Preview, Mistral Medium 3.1, Claude Opus 4.1 (no reasoning), DeepSeek V3.1 Reasoner, and DeepSeek V3.1 Non‑Think to the evaluated models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;New analyses:&lt;/strong&gt; added head‑to‑head A‑vs‑B comparisons, model‑level style summaries, and intra‑model style diversity analysis (previously none).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agreement views:&lt;/strong&gt; expanded beyond only grader‑grader correlations to include Grader×LLM mean and normalized matrices, story‑level disagreement tables, and leave‑one‑grader‑out robustness checks.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ndkbqa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndkbqa/kimi_k20905_takes_first_place_in_the_short_story/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndkbqa/kimi_k20905_takes_first_place_in_the_short_story/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T17:16:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndg4up</id>
    <title>My open-source project on different RAG techniques just hit 20K stars on GitHub</title>
    <updated>2025-09-10T14:43:55+00:00</updated>
    <author>
      <name>/u/Nir777</name>
      <uri>https://old.reddit.com/user/Nir777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's what's inside:&lt;/p&gt; &lt;p&gt;- 35 detailed tutorials on different RAG techniques&lt;/p&gt; &lt;p&gt;- Tutorials organized by category &lt;/p&gt; &lt;p&gt;- Clear, high-quality explanations with diagrams and step-by-step code implementations &lt;/p&gt; &lt;p&gt;- Many tutorials paired with matching blog posts for deeper insights&lt;/p&gt; &lt;p&gt;- I'll keep sharing updates about these tutorials here &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;A huge thank you to all contributors who made this possible! &lt;/p&gt; &lt;p&gt;link to the repo in the first comment &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nir777"&gt; /u/Nir777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndg4up/my_opensource_project_on_different_rag_techniques/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndg4up/my_opensource_project_on_different_rag_techniques/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndg4up/my_opensource_project_on_different_rag_techniques/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:43:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndhj04</id>
    <title>Everyone’s betting on bigger LLMs, but I think the real breakthrough will come from smaller, local ones</title>
    <updated>2025-09-10T15:35:36+00:00</updated>
    <author>
      <name>/u/LuozhuZhang</name>
      <uri>https://old.reddit.com/user/LuozhuZhang</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndhj04/everyones_betting_on_bigger_llms_but_i_think_the/"&gt; &lt;img alt="Everyone’s betting on bigger LLMs, but I think the real breakthrough will come from smaller, local ones" src="https://preview.redd.it/00unmbnlwcof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4388067c9e3668bfee34beedf82ff143e5ed6dde" title="Everyone’s betting on bigger LLMs, but I think the real breakthrough will come from smaller, local ones" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Right now most of the attention is on making models bigger and bigger. But after spending time running things locally, I feel like the real opportunities are in the opposite direction.&lt;/p&gt; &lt;p&gt;Smaller, local-first models can be:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Faster&lt;/strong&gt; because you don’t need to wait on a round trip&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cheaper&lt;/strong&gt; since you’re not paying API bills&lt;/li&gt; &lt;li&gt;&lt;strong&gt;More private&lt;/strong&gt; because the data stays on your own machine&lt;/li&gt; &lt;li&gt;....&lt;/li&gt; &lt;li&gt;....&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I think in the next few months we’ll start to see some really good products built this way. They probably won’t come from the big labs, but from people experimenting locally and finding the use cases where speed, cost, and privacy actually matter more than raw scale.&lt;/p&gt; &lt;p&gt;I wrote a post with more detail here if you’re curious:&lt;br /&gt; 👉 &lt;a href="https://x.com/LuozhuZhang/status/1965782888202621358"&gt;https://x.com/LuozhuZhang/status/1965782888202621358&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What do you all think? Where do you see the strongest opportunities for local models? Can they really beat the big APIs in some areas, or will scale always win?&lt;/p&gt; &lt;p&gt;If you have the time to read my full post and share your thoughts, I’d really appreciate it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LuozhuZhang"&gt; /u/LuozhuZhang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/00unmbnlwcof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndhj04/everyones_betting_on_bigger_llms_but_i_think_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndhj04/everyones_betting_on_bigger_llms_but_i_think_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T15:35:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd7nxo</id>
    <title>VibeVoice is sweeeet. Now we need to adapt its tokenizer for other models!</title>
    <updated>2025-09-10T07:24:32+00:00</updated>
    <author>
      <name>/u/Cipher_Lock_20</name>
      <uri>https://old.reddit.com/user/Cipher_Lock_20</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd7nxo/vibevoice_is_sweeeet_now_we_need_to_adapt_its/"&gt; &lt;img alt="VibeVoice is sweeeet. Now we need to adapt its tokenizer for other models!" src="https://external-preview.redd.it/ZXJidjUwNHJnYW9mMTZtREgbQQjA1lJ8zPSNZtqKO6Gf9AtInhXi-M401FlP.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=051bf77ffd8780ab4b7ffcc3dc7c1b3bc71a8875" title="VibeVoice is sweeeet. Now we need to adapt its tokenizer for other models!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a huge AI audio nerd, I've recently been knee-deep in Microsoft's latest VibeVoice models and they really are awesome!! The work from the Microsoft Research team is amazing and they've shared them with everyone.... even though they took one back lol. I highly recommend checking them out if you haven't already.&lt;/p&gt; &lt;p&gt;I started reading up on all of the techniques applied within the architecture to allow for such long generations (45-90 minutes), with up to 4 speakers, and sounding so life-like... Google notebook is the closest thing to this kind of generation, but it's limited in that it auto-generates your podcast based on the context, not on the exact script you provide.&lt;/p&gt; &lt;p&gt;Let me have the VibeVoice model do the talking!&lt;/p&gt; &lt;p&gt;The generated voices in my video were generated within my own Hugging Face space and using the default voices provided by the VibeVoice model (7B). The voices were generated in one single generation, not stitched! &lt;a href="https://huggingface.co/spaces/ACloudCenter/Conference-Generator-VibeVoice"&gt;https://huggingface.co/spaces/ACloudCenter/Conference-Generator-VibeVoice&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cipher_Lock_20"&gt; /u/Cipher_Lock_20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8x4dht8pgaof1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd7nxo/vibevoice_is_sweeeet_now_we_need_to_adapt_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nd7nxo/vibevoice_is_sweeeet_now_we_need_to_adapt_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T07:24:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndc7z8</id>
    <title>I pre-trained GPT-OSS entirely from scratch</title>
    <updated>2025-09-10T12:00:25+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndc7z8/i_pretrained_gptoss_entirely_from_scratch/"&gt; &lt;img alt="I pre-trained GPT-OSS entirely from scratch" src="https://external-preview.redd.it/9EZFRbCI06NQd5IaAcswlKJEIIkgLbOtsjD1e7w98EI.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85d2bae87aa536469b4b4fbaafbfa3ee215b6f78" title="I pre-trained GPT-OSS entirely from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/fo9rnnpeubof1.png?width=2562&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=562a72f12d23083851c3775e1540b7f111ffda57"&gt;https://preview.redd.it/fo9rnnpeubof1.png?width=2562&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=562a72f12d23083851c3775e1540b7f111ffda57&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I recorded a 3 hour video to show how we built GPT-OSS from scratch. &lt;/p&gt; &lt;p&gt;You can watch the video here: &lt;a href="https://youtu.be/hBUsySdcA3I"&gt;https://youtu.be/hBUsySdcA3I&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The video contains the following 8 steps:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;(1) Tiny Stories: Data Preprocessing&lt;/p&gt; &lt;p&gt;(2) GPT-OSS Harmony Tokenizer to tokenize the data&lt;/p&gt; &lt;p&gt;(3) Architecture Part 1: Token embeddings, RMSNorm and Rotary Positional Encoding (RoPE)&lt;/p&gt; &lt;p&gt;(4) Architecture Part 2: Sliding attention layers and Grouped Query Attention (GQA)&lt;/p&gt; &lt;p&gt;(5) Architecture Part 3: Attention Bias and Attention Sinks&lt;/p&gt; &lt;p&gt;(6) Architecture Part 4: SwiGLU Mixture of Experts (MoE) &lt;/p&gt; &lt;p&gt;(7) GPT-OSS Pre-training loop&lt;/p&gt; &lt;p&gt;(8) GPT-OSS Inference&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some info:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;We have now released two versions of our codebase publicly. Both are under active work: &lt;/p&gt; &lt;p&gt;(1) Nano-GPT-OSS: &lt;a href="https://github.com/VizuaraAI/nano-gpt-oss"&gt;https://github.com/VizuaraAI/nano-gpt-oss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- A 500 million parameter model which retains all the key architectural innovations of GPT-OSS. &lt;/p&gt; &lt;p&gt;- Requires 20 hours of training on 1 A40 GPU (0.4$/hr). Can be replicated under 10$. &lt;/p&gt; &lt;p&gt;(2) Truly-Open-GPT-OSS: &lt;a href="https://github.com/VizuaraAI/truly-open-gpt-oss"&gt;https://github.com/VizuaraAI/truly-open-gpt-oss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- A 20B parameter model which we pre-trained fully from scratch. &lt;/p&gt; &lt;p&gt;- Requires 5 H200 GPUs. Budget needed for this would be 100-150$&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndc7z8/i_pretrained_gptoss_entirely_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndc7z8/i_pretrained_gptoss_entirely_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndc7z8/i_pretrained_gptoss_entirely_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T12:00:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndibn1</id>
    <title>Unsloth Dynamic GGUFs - Aider Polyglot Benchmarks</title>
    <updated>2025-09-10T16:04:27+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/"&gt; &lt;img alt="Unsloth Dynamic GGUFs - Aider Polyglot Benchmarks" src="https://preview.redd.it/ewtq2ax40dof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1aadf79bc0320ee8ed05eb7cf3501970b4040021" title="Unsloth Dynamic GGUFs - Aider Polyglot Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, it's Michael from &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; here! Ever since we released Dynamic GGUFs, we've received so much love thanks to you all, but we know better benchmarking was a top request!&lt;/p&gt; &lt;p&gt;Previously, we already benchmarked Gemma 3 and Llama 4 on 5-shot MMLU and KL Divergence but as we're holding our first &lt;a href="/r/Localllama"&gt;r/Localllama&lt;/a&gt; AMA in about an hour, we're happy to showcase Aider Polyglot benchmarks for our DeepSeek-V3.1 GGUFs and were quite surprised by the results! &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In the first DeepSeek-V3.1 graph, we compare thinking with other thinking models. In the 2nd graph, we compare non-thinking vs a non-Unsloth Dynamic imatrix GGUF&lt;/li&gt; &lt;li&gt;Our &lt;strong&gt;1-bit&lt;/strong&gt; Unsloth Dynamic GGUF shrinks DeepSeek-V3.1 from &lt;strong&gt;671GB → 192GB (-75% size)&lt;/strong&gt; and no-thinking mode outperforms GPT-4.1 (Apr 2025), GPT-4.5, and DeepSeek-V3-0324.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;3-bit&lt;/strong&gt; Unsloth DeepSeek-V3.1 (thinking) GGUF: Outperforms Claude-4-Opus (thinking).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;5-bit&lt;/strong&gt; Unsloth DeepSeek-V3.1 (non-thinking) GGUF: Matches Claude-4-Opus (non-thinking) performance.&lt;/li&gt; &lt;li&gt;Our Dynamic GGUFs &lt;strong&gt;perform consistently better&lt;/strong&gt; than other non-Unsloth Dynamic imatrix GGUFs&lt;/li&gt; &lt;li&gt;Other non-Unsloth 1-bit and 2-bit DeepSeek-V3.1 quantizations, as well as standard 1-bit quantization without selective layer quantization, either failed to load or produced gibberish and looping outputs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For our DeepSeek-V3.1 experiments, we compared different bits of &lt;strong&gt;Unsloth Dynamic GGUFs&lt;/strong&gt; against:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Full-precision, unquantized LLMs&lt;/strong&gt; including GPT 4.5, 4.1, Claude-4-Opus, DeepSeek-V3-0324 etc.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Other&lt;/em&gt; dynamic imatrix V3.1 GGUFs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semi-dynamic&lt;/strong&gt; (some selective layer quantization) imatrix V3.1 GGUFs for ablation purposes.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Benchmark experiments were mainly conducted by David (neolithic5452 on Aider Disc), a trusted community contributor to Aider Polyglot evaluations. Tests were run ~3 times and averaged for a median score, and the Pass-2 accuracy is reported as by convention.&lt;/p&gt; &lt;p&gt;Wish we could attach another image for the non-thinking benchmarks but if you'd like more details, you can read our blogpost: &lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-ggufs-on-aider-polyglot"&gt;https://docs.unsloth.ai/basics/unsloth-dynamic-ggufs-on-aider-polyglot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks guys so much for the support!&lt;br /&gt; Michael&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ewtq2ax40dof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T16:04:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndfxxi</id>
    <title>😳 umm</title>
    <updated>2025-09-10T14:36:33+00:00</updated>
    <author>
      <name>/u/internal-pagal</name>
      <uri>https://old.reddit.com/user/internal-pagal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxxi/umm/"&gt; &lt;img alt="😳 umm" src="https://preview.redd.it/80dp7ukemcof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8ce888fab8e72337bb19e61f35d929aeac11346" title="😳 umm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/internal-pagal"&gt; /u/internal-pagal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/80dp7ukemcof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxxi/umm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxxi/umm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:36:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndf01a</id>
    <title>So apparently half of us are "AI providers" now (EU AI Act edition)</title>
    <updated>2025-09-10T14:00:15+00:00</updated>
    <author>
      <name>/u/Thecomplianceexpert</name>
      <uri>https://old.reddit.com/user/Thecomplianceexpert</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Heads up, fellow tinkers&lt;/p&gt; &lt;p&gt;The EU AI Act’s first real deadline kicked in August 2nd so if you’re messing around with models that hit 10^23 FLOPs or more (think Llama-2 13B territory), regulators now officially care about you.&lt;/p&gt; &lt;p&gt;Couple things I’ve learned digging through this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The FLOP cutoff is surprisingly low. It’s not “GPT-5 on a supercomputer” level, but it’s way beyond what you’d get fine-tuning Llama on your 3090.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;“Provider” doesn’t just mean Meta, OpenAI, etc. If you fine-tune or significantly modify a big model, you need to watch out. Even if it’s just a hobby, you can still be classified as a provider.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Compliance isn’t impossible. Basically: &lt;ul&gt; &lt;li&gt;Keep decent notes (training setup, evals, data sources).&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Have some kind of “data summary” you can share if asked.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Don’t be sketchy about copyright.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Deadline check:&lt;br /&gt; &lt;ul&gt; &lt;li&gt;New models released after Aug 2025 - rules apply now!&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Models that existed before Aug 2025 - you’ve got until 2027.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;EU basically said: “Congrats, you’re responsible now.” 🫠 &lt;/p&gt; &lt;p&gt;TL;DR: If you’re just running models locally for fun, you’re probably fine. If you’re fine-tuning big models and publishing them, you might already be considered a “provider” under the law.&lt;/p&gt; &lt;p&gt;Honestly, feels wild that a random tinkerer could suddenly have reporting duties, but here we are.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thecomplianceexpert"&gt; /u/Thecomplianceexpert &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf01a/so_apparently_half_of_us_are_ai_providers_now_eu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf01a/so_apparently_half_of_us_are_ai_providers_now_eu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf01a/so_apparently_half_of_us_are_ai_providers_now_eu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:00:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nct0z8</id>
    <title>Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)</title>
    <updated>2025-09-09T19:47:12+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt; &lt;img alt="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" src="https://preview.redd.it/7vh8enuu07of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0f499252613d5bcf478fb1b75c594bb50f43436" title="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7vh8enuu07of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T19:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndjxdt</id>
    <title>AMA with the Unsloth team</title>
    <updated>2025-09-10T17:02:18+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;, I'm Daniel from &lt;a href="https://docs.unsloth.ai/"&gt;Unsloth&lt;/a&gt;! You might know us from our RL &amp;amp; fine-tuning &lt;a href="https://github.com/unslothai/unsloth"&gt;open-source framework&lt;/a&gt;, our GGUFs, kernels or bug fixes. We’re super excited to answer all your questions!! 🦥&lt;/p&gt; &lt;p&gt;To celebrate the AMA, we’re releasing Aider Polyglot benchmarks comparing our DeepSeek-V3.1 Dynamic GGUFs to other models and quants. We also made a Localllama post here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our participants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Daniel, u/danielhanchen&lt;/li&gt; &lt;li&gt;Michael, u/yoracale&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 10AM – 1PM PST, with the Unsloth team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thanks so much!🥰&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T17:02:18+00:00</published>
  </entry>
</feed>
