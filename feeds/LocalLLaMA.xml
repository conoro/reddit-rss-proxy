<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-08T18:57:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q7h3pi</id>
    <title>Storytelling Model</title>
    <updated>2026-01-08T16:57:21+00:00</updated>
    <author>
      <name>/u/slrg1968</name>
      <uri>https://old.reddit.com/user/slrg1968</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HI folks, I am looking for a model that will write fiction for me -- ideally, I'll give it the ideas and a rough outline, and it will be able to develop it into a full blown short story type item. it would be nice if it also comes in an abliterated version&lt;/p&gt; &lt;p&gt;do you have any recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slrg1968"&gt; /u/slrg1968 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7h3pi/storytelling_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7h3pi/storytelling_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7h3pi/storytelling_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T16:57:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7bh5h</id>
    <title>I built instant persistent memory for local LLMs (binary KV cache save/restore, sub-second restore, 67% VRAM savings)</title>
    <updated>2026-01-08T13:14:36+00:00</updated>
    <author>
      <name>/u/thejosephBlanco</name>
      <uri>https://old.reddit.com/user/thejosephBlanco</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7bh5h/i_built_instant_persistent_memory_for_local_llms/"&gt; &lt;img alt="I built instant persistent memory for local LLMs (binary KV cache save/restore, sub-second restore, 67% VRAM savings)" src="https://b.thumbs.redditmedia.com/4Cdt4Nm2DZvxewkouNaozbOYuUnmcBEsfWWhcMGfgEU.jpg" title="I built instant persistent memory for local LLMs (binary KV cache save/restore, sub-second restore, 67% VRAM savings)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm not a professional developer, I used AI and a lot of free time over 18 months specifically building this. I am Technical Support professional with zero programming experience, learned C++, CUDA, Qt6, and llama.cpp integration entirely through AI-assisted learning and trial-and-error.&lt;/p&gt; &lt;p&gt;This project is part of VyreVault Studios, my personal development ecosystem focused on local-first, ownership-based software. The &amp;quot;Dreams for the Dreamless&amp;quot; philosophy: democratizing access to creative technology by building tools that run on your hardware, with your data, under your control. Everything I build is under one service, to the user, to help with creativity. Not to do it for you, to have a sounding board to brainstorm your ideas. I spend a lot of my time actually arguing the stories i write with the LLM because it suggests the weirdest off the wall shit.&lt;/p&gt; &lt;p&gt;Every tool I used, every method i tried either forgets everything (Ollama, LM Studio, Chatgpt, Claude, Grok, Gemini. Yes I've tried everything) or takes 30+ seconds to replay your conversation history token-by-token.&lt;/p&gt; &lt;p&gt;So I built binary KV cache persistence with instant restore. And yes, I am writing this post and Yes I rewrote it hundreds of times. I had to learn about what all this stuff was and I still have no clue, but I think I built something interesting, so here it goes:&lt;/p&gt; &lt;p&gt;What It Does:&lt;/p&gt; &lt;p&gt;Saves the model's actual memory state (KV cache) to disk after each response&lt;/p&gt; &lt;p&gt;Restores it instantly on app restart (sub-second for hundreds of tokens)&lt;/p&gt; &lt;p&gt;Model remembers the conversation perfectly - no replay, no summarization&lt;/p&gt; &lt;p&gt;Background async save (no UI freeze)&lt;/p&gt; &lt;p&gt;Q8_0 quantized KV cache (67% VRAM reduction vs FP16)&lt;/p&gt; &lt;p&gt;The Results:&lt;/p&gt; &lt;p&gt;Tested with Mistral 7B on dual NVIDIA GPUs (RTX 5070 Ti + RTX 3080):&lt;/p&gt; &lt;p&gt;[PHASE 1] Seeding: &amp;quot;The secret code is BLUE-OMEGA-99&amp;quot;&lt;/p&gt; &lt;p&gt;Saved binary state: 11.3 MB (160 tokens)&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;[PHASE 2] Simulated restart&lt;/p&gt; &lt;p&gt;Loaded binary state: 11.3 MB&lt;/p&gt; &lt;p&gt;Restore time: &amp;lt;1 second&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;[PHASE 3] Testing recall&lt;/p&gt; &lt;p&gt;Question: &amp;quot;What is the secret code?&amp;quot;&lt;/p&gt; &lt;p&gt;Response: &amp;quot;The secret code is BLUE-OMEGA-99&amp;quot;&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;SUCCESS: Binary Persistence Verified&lt;/p&gt; &lt;p&gt;How It Works: (For me anyways)&lt;/p&gt; &lt;p&gt;Uses llama.cpp's llama_state_get_data and llama_state_set_data APIs to serialize the entire KV cache to disk. On restart, it loads the binary state directly back into GPU memory and synchronizes the sequence positions.&lt;/p&gt; &lt;p&gt;Key implementation details:&lt;/p&gt; &lt;p&gt;Async save thread (no UI blocking)&lt;/p&gt; &lt;p&gt;Q8_0 quantization for KV cache (saves VRAM) But have the Option for Q4_0, depending on size and ersonal preference.&lt;/p&gt; &lt;p&gt;Proper n_past synchronization to prevent &amp;quot;inconsistent sequence positions&amp;quot; crashes&lt;/p&gt; &lt;p&gt;Session management with isolated KV caches per conversation&lt;/p&gt; &lt;p&gt;You can now:&lt;/p&gt; &lt;p&gt;Work on multi-day projects (novels, code refactoring, research) with full persistent memory&lt;/p&gt; &lt;p&gt;Close the app anytime without losing context&lt;/p&gt; &lt;p&gt;Resume instantly the next day&lt;/p&gt; &lt;p&gt;No waiting for 30-second token replay&lt;/p&gt; &lt;p&gt;Context loads faster than ChatGPT's API responds (Although, Guilty I still use Chatgpt for when i get stuck)&lt;/p&gt; &lt;p&gt;Stack:&lt;/p&gt; &lt;p&gt;C++17 + Qt6 (native desktop UI)&lt;/p&gt; &lt;p&gt;llama.cpp (inference engine)&lt;/p&gt; &lt;p&gt;CUDA 12.6 (dual-GPU support)&lt;/p&gt; &lt;p&gt;Automated verification tests&lt;/p&gt; &lt;p&gt;Currently working prototype on Windows + NVIDIA. Tested with Mistral 7B and Qwen 30B models. File sizes scale with context (roughly 70KB per 1K tokens for 7B models with Q8_0 KV cache).&lt;/p&gt; &lt;p&gt;Plan for continued build:&lt;/p&gt; &lt;p&gt;Add manual save/load controls in UI&lt;/p&gt; &lt;p&gt;Multi-model testing (larger models, different architectures)&lt;/p&gt; &lt;p&gt;Optimize file format (compression, delta encoding)&lt;/p&gt; &lt;p&gt;Cross-platform support (Linux, Mac)&lt;/p&gt; &lt;p&gt;This is not a soapbox moment, or BS, I built this for one reason, I write stories, and I cannot stand when degradation sets in and i have to recap everything start a new chat and explain the details all over again.. The memory is real, verified, and instant.&lt;/p&gt; &lt;p&gt;Any questions I can answer, as this is my first time posting my actual progress in any forum or my actual build for anyone other than myself. I am not self promoting anything, im working out the UI kinks for the app right now, and plan on uploading it to GITHUB whe i get the best MVP versin i can that can be used by people if they are interested.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9gkkr2cdl4cg1.png?width=1402&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4db58a5b3323546c282ebf7569e98f7702faa0eb"&gt;https://preview.redd.it/9gkkr2cdl4cg1.png?width=1402&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4db58a5b3323546c282ebf7569e98f7702faa0eb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Early test to see how system responded&lt;/p&gt; &lt;p&gt;Edit: Clarifying what makes this different - this is heterogeneous dual-GPU inference (different NVIDIA cards, RTX 5070 Ti + 3080) in a single process, with KV cache split across GPUs and binary persistence. Not just &amp;quot;calling the llama.cpp API&amp;quot; - it's the multi-GPU architecture + single-process + persistence combination that I couldn‚Äôt find for my personal needs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thejosephBlanco"&gt; /u/thejosephBlanco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7bh5h/i_built_instant_persistent_memory_for_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7bh5h/i_built_instant_persistent_memory_for_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7bh5h/i_built_instant_persistent_memory_for_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T13:14:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6c9wc</id>
    <title>DeepSeek-R1‚Äôs paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.</title>
    <updated>2026-01-07T10:49:12+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/"&gt; &lt;img alt="DeepSeek-R1‚Äôs paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail." src="https://b.thumbs.redditmedia.com/DUX7Mp3MJPzZfCIE1yl1lv9VDENeyLGC6ZkgyRLizOw.jpg" title="DeepSeek-R1‚Äôs paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;arXiv:2501.12948 [cs.CL]: &lt;a href="https://arxiv.org/abs/2501.12948"&gt;https://arxiv.org/abs/2501.12948&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q6c9wc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T10:49:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7bei7</id>
    <title>LLM-Shield: Privacy proxy - masks PII or routes to local LLM</title>
    <updated>2026-01-08T13:11:21+00:00</updated>
    <author>
      <name>/u/sgasser88</name>
      <uri>https://old.reddit.com/user/sgasser88</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7bei7/llmshield_privacy_proxy_masks_pii_or_routes_to/"&gt; &lt;img alt="LLM-Shield: Privacy proxy - masks PII or routes to local LLM" src="https://preview.redd.it/n48hxqs274cg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60fe1151f79f62321c6fe1f6d5c625cd5f2c768f" title="LLM-Shield: Privacy proxy - masks PII or routes to local LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using cloud LLMs but worried about sending client data? Built a proxy for that.&lt;/p&gt; &lt;p&gt;OpenAI-compatible proxy with two privacy modes:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mask Mode&lt;/strong&gt; (no GPU needed):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;You send: &amp;quot;Email john@acme.com about meeting with Sarah Miller&amp;quot; OpenAI receives: &amp;quot;Email &amp;lt;EMAIL_1&amp;gt; about meeting with &amp;lt;PERSON_1&amp;gt;&amp;quot; You get back: Original names restored in response &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Route Mode&lt;/strong&gt; (for local LLM setups):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;Help with this code review&amp;quot; ‚Üí OpenAI &amp;quot;Email john@acme.com about...&amp;quot; ‚Üí Ollama (PII stays local) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Detects names, emails, phones, credit cards, IBANs, IPs, and locations across 24 languages with automatic language detection. Uses Microsoft Presidio under the hood.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/sgasser/llm-shield cd llm-shield &amp;amp;&amp;amp; cp config.example.yaml config.yaml docker compose up -d &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Point your app to &lt;code&gt;http://localhost:3000/openai/v1&lt;/code&gt; and you're set. Works with anything that uses the OpenAI API ‚Äî Open WebUI, Cursor, your own scripts. Dashboard included for monitoring.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/sgasser/llm-shield"&gt;https://github.com/sgasser/llm-shield&lt;/a&gt; ‚Äî just open-sourced&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Next up:&lt;/strong&gt; Chrome extension for ChatGPT.com and PDF/attachment masking.&lt;/p&gt; &lt;p&gt;Would love feedback on detection accuracy and what entity types would be useful for your setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sgasser88"&gt; /u/sgasser88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n48hxqs274cg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7bei7/llmshield_privacy_proxy_masks_pii_or_routes_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7bei7/llmshield_privacy_proxy_masks_pii_or_routes_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T13:11:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6v7v5</id>
    <title>What hardware would it take to get Claude Code-level performance?</title>
    <updated>2026-01-07T23:22:28+00:00</updated>
    <author>
      <name>/u/cashmillionair</name>
      <uri>https://old.reddit.com/user/cashmillionair</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my previous company I had a Claude license and my work was basically interacting with Claude Code all day long. The code base was rather complex and I was automating testing and ‚ÄúDevOps‚Äù stuff for an embedded device development so Claude Code saved me tons of time (it was much faster to ask and tune that to do it all by myself).&lt;/p&gt; &lt;p&gt;Im currently unemployed but got a freelancing gig and the company doesn‚Äôt provide access to commercial AI tools for contractors like me, but once again the work is rather demanding and I don‚Äôt think I‚Äôll meet the deadlines without AI help (it‚Äôs a fairly old code base using mostly Java in a concurrent and distributed fashion), and of course due to compliance I can‚Äôt just use a license I paid for by myself.&lt;/p&gt; &lt;p&gt;So, in new to all this. To be honest I have very little hardware, as I would always prioritize power efficiency since I never really needed to do anything hardware intensive before (I don‚Äôt have a gaming PC or anything like that). I have an old HP Z2 G4 Tower I use as virtualization server and was thinking of getting a 3060 12GB for ~300 USD (locally). Will I be able to run anything decent with that? Anything that would truly help me?&lt;/p&gt; &lt;p&gt;I see everyone recommends a 3090 but I‚Äôd need a whole new PSU and build an entire computer around that. So that‚Äôd be roughly 2K USD (is it worth it? I don‚Äôt know, maybe?)&lt;/p&gt; &lt;p&gt;What hardware is requires to run anything remotely close to Claude Code? Something like 6x3090s (144GB VRAM)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cashmillionair"&gt; /u/cashmillionair &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6v7v5/what_hardware_would_it_take_to_get_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6v7v5/what_hardware_would_it_take_to_get_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6v7v5/what_hardware_would_it_take_to_get_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T23:22:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7kbbz</id>
    <title>Kimi K2 Thinking, Q2, 3 nodes Strix Halo, llama.cpp. Has anyone tried a multiple-node setup using vLLM yet? And how it compares to Llama.cpp. Thank you.</title>
    <updated>2026-01-08T18:50:56+00:00</updated>
    <author>
      <name>/u/el3mancee</name>
      <uri>https://old.reddit.com/user/el3mancee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7kbbz/kimi_k2_thinking_q2_3_nodes_strix_halo_llamacpp/"&gt; &lt;img alt="Kimi K2 Thinking, Q2, 3 nodes Strix Halo, llama.cpp. Has anyone tried a multiple-node setup using vLLM yet? And how it compares to Llama.cpp. Thank you." src="https://preview.redd.it/792wso0696cg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d942cd6a66303123de4eba5fb1fd9f269d83aaa" title="Kimi K2 Thinking, Q2, 3 nodes Strix Halo, llama.cpp. Has anyone tried a multiple-node setup using vLLM yet? And how it compares to Llama.cpp. Thank you." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Managed to run Kimi K2 Thinking, q2 on a 3-node Strix Halo setup. Got around 9t/s. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/el3mancee"&gt; /u/el3mancee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/792wso0696cg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7kbbz/kimi_k2_thinking_q2_3_nodes_strix_halo_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7kbbz/kimi_k2_thinking_q2_3_nodes_strix_halo_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T18:50:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1q77cnk</id>
    <title>RAG Paper 26.1.7</title>
    <updated>2026-01-08T09:24:29+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.03981v1"&gt;RADAR: Retrieval-Augmented Detector with Adversarial Refinement for Robust Fake News Detection&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.03979v1"&gt;SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.03948v1"&gt;Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.03908v1"&gt;Decide Then Retrieve: A Training-Free Framework with Uncertainty-Guided Triggering and Dual-Path Retrieval&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.03903v1"&gt;Unleashing the Potential of Neighbors: Diffusion-based Latent Neighbor Generation for Session-based Recommendation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.03792v1"&gt;VietMed-MCQ: A Consistency-Filtered Data Synthesis Framework for Vietnamese Traditional Medicine Evaluation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.03748v1"&gt;Bridging OLAP and RAG: A Multidimensional Approach to the Design of Corpus Partitioning&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.03746v1"&gt;Whose Facts Win? LLM Source Preferences under Knowledge Conflicts&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/"&gt;&lt;strong&gt;RagView.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/&lt;/strong&gt; &lt;a href="https://github.com/RagView/RagView"&gt;&lt;strong&gt;github/RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q77cnk/rag_paper_2617/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q77cnk/rag_paper_2617/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q77cnk/rag_paper_2617/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T09:24:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7fejp</id>
    <title>Are MiniMax M2.1 quants usable for coding?</title>
    <updated>2026-01-08T15:54:30+00:00</updated>
    <author>
      <name>/u/val_in_tech</name>
      <uri>https://old.reddit.com/user/val_in_tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please share your real life experience. Especially interesting to hear from someone who had a chance to compare higher quants with lower ones.&lt;/p&gt; &lt;p&gt;Also, speaking of the model itself - do you feel it's worth the buzz around it?&lt;/p&gt; &lt;p&gt;Use case - coding via opencode or claude proxy.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/val_in_tech"&gt; /u/val_in_tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7fejp/are_minimax_m21_quants_usable_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7fejp/are_minimax_m21_quants_usable_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7fejp/are_minimax_m21_quants_usable_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T15:54:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7b3wy</id>
    <title>A 2.5M 10MB TinyStories model trained using GRU and attention (vs.TinyStories-1M)</title>
    <updated>2026-01-08T12:57:45+00:00</updated>
    <author>
      <name>/u/ValuableLucky8566</name>
      <uri>https://old.reddit.com/user/ValuableLucky8566</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using a 20MB TinyStories datasheet, this TinyStories model 5x smaller than TinyStories-1M.&lt;/p&gt; &lt;p&gt;Since this was trained on google colab free(nvidia t4), the loss is only converged to ~0.75.&lt;/p&gt; &lt;p&gt;The architecture used was a hybrid of GRU, particularly GRUcell with a single attention layer.&lt;/p&gt; &lt;p&gt;In a single, large GRUcell layer, I used a residual memory logic which writes decoded data into the drive, and feeds it to the input as for the hidden state.&lt;/p&gt; &lt;p&gt;The model creates a proposed memory:&lt;/p&gt; &lt;p&gt;M~t=tanh‚Å°(Wcht+bc)&lt;/p&gt; &lt;p&gt;Finally, the old memory is mixed with the new one:&lt;/p&gt; &lt;p&gt;Mt=(1‚àípt)‚äôMt‚àí1+pt‚äôM~t&lt;/p&gt; &lt;p&gt;This allows for the architecture to train a model so small (0.36M), that can memorize words and output meaningful words at a train loss of 2.2.&lt;/p&gt; &lt;p&gt;Finally, I added a self-attention layer that allows the model to see context of the previous words to a limited extent. This allows the model to remember what it said 5-10 words ago and prevent complete drifting, which was a limitation of standard GRU.&lt;/p&gt; &lt;p&gt;This brings the attention cost to O(T¬≥), but the model still remains faster than TinyStories-1M (50MB) for short bursts below ~300 tokens when self attention becomes an overhead.&lt;/p&gt; &lt;p&gt;Example output(TinyStories-1M): &lt;/p&gt; &lt;p&gt;Prompt: once upon a time&lt;/p&gt; &lt;p&gt;Output: once upon a time there was an old girl named Lily. She loved to play with her toys and pretend she was his friend. One day, they went outside to see the park. The sun looked blue and it had big eyes.&lt;/p&gt; &lt;p&gt;Lily's mom said &amp;quot;Let me help you get my doll!&amp;quot; They started to walk around the park and had lots of fun together. When it was dark and it seemed like something strange happened, a little bunny came up over to them.&lt;/p&gt; &lt;p&gt;&amp;quot;Hello there,&amp;quot; said Mom. &amp;quot;It's so pretty.&amp;quot;&lt;/p&gt; &lt;p&gt;The bunny smiled at Lily and said, &amp;quot;Yes! I'm glad we can find some more toys later.&amp;quot; Then he opened his backpack and found that she was able to provide for him in a special place where he could have all the toys ready!&lt;/p&gt; &lt;p&gt;(165 words, 67.97 it/s) (could be roughly around 200 chars/sec for BPE tokeniser)&lt;/p&gt; &lt;p&gt;tinystoriesgru:&lt;/p&gt; &lt;p&gt;Prompt: once upon a time&lt;br /&gt; Output: once upon a time to hear the wolf with a smile on his face. She was so happy that the monster was so cold.&lt;/p&gt; &lt;p&gt;But then, the piece of colorful circle came in. She wanted to see what was inside, but she thought it would be fun. She started to cry and started to cry. She quickly ran and ran until she found the crayon and started to cry.&lt;/p&gt; &lt;p&gt;The cat saw the pretty flower and started to shake and showed them the magazine. She thought it would be fun to cut the leaves. She was so happy with her new ball. She wanted to take h&lt;/p&gt; &lt;p&gt;(500 tokens, 112.02 it/s)&lt;/p&gt; &lt;p&gt;At lower characters, the GRU scales to be much faster while the transformer remains consistent with 67-68it/s, for more/less words.&lt;/p&gt; &lt;p&gt;The pure transformer continues to have better context overall.&lt;/p&gt; &lt;p&gt;I've included the &lt;a href="http://train.py"&gt;train.py&lt;/a&gt; here (if anyone can train it further):&lt;br /&gt; &lt;a href="https://github.com/kavyamali/tinystoriesgru"&gt;https://github.com/kavyamali/tinystoriesgru&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thank you for reading.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ValuableLucky8566"&gt; /u/ValuableLucky8566 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7b3wy/a_25m_10mb_tinystories_model_trained_using_gru/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7b3wy/a_25m_10mb_tinystories_model_trained_using_gru/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7b3wy/a_25m_10mb_tinystories_model_trained_using_gru/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T12:57:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7iaul</id>
    <title>M2 Ultra 128gb ram 2TB ssd or PC with two rtx3090 64gb ram 2TB ssd?</title>
    <updated>2026-01-08T17:40:14+00:00</updated>
    <author>
      <name>/u/royal_robert</name>
      <uri>https://old.reddit.com/user/royal_robert</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Should I buy a M2 Ultra 128gb ram 2TB ssd or PC with two rtx3090 64gb ram 2TB ssd?&lt;/p&gt; &lt;p&gt;$2500-3000 for the used Mac Studio&lt;/p&gt; &lt;p&gt;To build the PC with the two rtx3090 would be around the same price.&lt;/p&gt; &lt;p&gt;Would like to work with 70b models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/royal_robert"&gt; /u/royal_robert &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7iaul/m2_ultra_128gb_ram_2tb_ssd_or_pc_with_two_rtx3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7iaul/m2_ultra_128gb_ram_2tb_ssd_or_pc_with_two_rtx3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7iaul/m2_ultra_128gb_ram_2tb_ssd_or_pc_with_two_rtx3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T17:40:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7h6hz</id>
    <title>I fine-tuned a 7B model for reasoning on free Colab with GRPO + TRL</title>
    <updated>2026-01-08T17:00:07+00:00</updated>
    <author>
      <name>/u/External-Rub5414</name>
      <uri>https://old.reddit.com/user/External-Rub5414</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just created a &lt;strong&gt;Colab notebook&lt;/strong&gt; that lets you &lt;strong&gt;add reasoning to 7B+ models&lt;/strong&gt; on free Colab(T4 GPU)!&lt;/p&gt; &lt;p&gt;Thanks to &lt;strong&gt;TRL's full set of memory optimizations&lt;/strong&gt;, this setup reduces memory usage by &lt;strong&gt;~7√ó&lt;/strong&gt; compared to naive FP16, making it possible to fine-tune large models in a free Colab session.&lt;/p&gt; &lt;p&gt;Notebook:&lt;br /&gt; üëâ &lt;a href="https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/grpo_trl_lora_qlora.ipynb"&gt;GRPO + TRL Colab notebook&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Check out other notebooks I worked on:&lt;br /&gt; üëâ &lt;a href="https://github.com/huggingface/trl/tree/main/examples/notebooks"&gt;TRL examples&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy hacking! üòÑ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External-Rub5414"&gt; /u/External-Rub5414 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7h6hz/i_finetuned_a_7b_model_for_reasoning_on_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7h6hz/i_finetuned_a_7b_model_for_reasoning_on_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7h6hz/i_finetuned_a_7b_model_for_reasoning_on_free/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T17:00:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7ivay</id>
    <title>Im planning on buying a single gpu to run llms and contemplating between two gpus</title>
    <updated>2026-01-08T18:00:15+00:00</updated>
    <author>
      <name>/u/WhiteSupremacistMonk</name>
      <uri>https://old.reddit.com/user/WhiteSupremacistMonk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im planning on either getting an nvidia v100 (32gb version) or a mi50 (32gb version) &lt;/p&gt; &lt;p&gt;is the extra money to get the v100 worth it ?, or should i just buy the cheaper mi50 for around 200-300 usd &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WhiteSupremacistMonk"&gt; /u/WhiteSupremacistMonk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7ivay/im_planning_on_buying_a_single_gpu_to_run_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7ivay/im_planning_on_buying_a_single_gpu_to_run_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7ivay/im_planning_on_buying_a_single_gpu_to_run_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T18:00:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6sp4b</id>
    <title>Sopro: A 169M parameter real-time TTS model with zero-shot voice cloning</title>
    <updated>2026-01-07T21:46:19+00:00</updated>
    <author>
      <name>/u/SammyDaBeast</name>
      <uri>https://old.reddit.com/user/SammyDaBeast</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a fun side project, I trained a small text-to-speech model that I call Sopro. Some features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;169M parameters&lt;/li&gt; &lt;li&gt;Streaming support&lt;/li&gt; &lt;li&gt;Zero-shot voice cloning&lt;/li&gt; &lt;li&gt;0.25 RTF on CPU, meaning it generates 30 seconds of audio in 7.5 seconds&lt;/li&gt; &lt;li&gt;Requires 3-12 seconds of reference audio for voice cloning&lt;/li&gt; &lt;li&gt;Apache 2.0 license&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Yes, I know, another English-only TTS model. This is mainly due to data availability and a limited compute budget. The model was trained on a single L40S GPU.&lt;/p&gt; &lt;p&gt;It‚Äôs not SOTA in most cases, can be a bit unstable, and sometimes fails to capture voice likeness. Nonetheless, I hope you like it!&lt;/p&gt; &lt;p&gt;GitHub repo: &lt;a href="https://github.com/samuel-vitorino/sopro"&gt;https://github.com/samuel-vitorino/sopro&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SammyDaBeast"&gt; /u/SammyDaBeast &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6sp4b/sopro_a_169m_parameter_realtime_tts_model_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6sp4b/sopro_a_169m_parameter_realtime_tts_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6sp4b/sopro_a_169m_parameter_realtime_tts_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T21:46:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q79n6x</id>
    <title>I was trying out an activation-steering method for Qwen3-Next, but I accidentally corrupted the model weights. Somehow, the model still had enough ‚Äúconscience‚Äù to realize something was wrong and freak out.</title>
    <updated>2026-01-08T11:42:44+00:00</updated>
    <author>
      <name>/u/ikergarcia1996</name>
      <uri>https://old.reddit.com/user/ikergarcia1996</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q79n6x/i_was_trying_out_an_activationsteering_method_for/"&gt; &lt;img alt="I was trying out an activation-steering method for Qwen3-Next, but I accidentally corrupted the model weights. Somehow, the model still had enough ‚Äúconscience‚Äù to realize something was wrong and freak out." src="https://b.thumbs.redditmedia.com/Pou7U9wzorYlQFrD5KCYo6fxr8k5_S3OwV7_B5qbW8s.jpg" title="I was trying out an activation-steering method for Qwen3-Next, but I accidentally corrupted the model weights. Somehow, the model still had enough ‚Äúconscience‚Äù to realize something was wrong and freak out." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I now feel bad seeing the model realize it was losing its mind and struggling with it, it feels like I was torturing it :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ikergarcia1996"&gt; /u/ikergarcia1996 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q79n6x"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q79n6x/i_was_trying_out_an_activationsteering_method_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q79n6x/i_was_trying_out_an_activationsteering_method_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T11:42:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6n5vl</id>
    <title>16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)</title>
    <updated>2026-01-07T18:22:05+00:00</updated>
    <author>
      <name>/u/ai-infos</name>
      <uri>https://old.reddit.com/user/ai-infos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/"&gt; &lt;img alt="16x AMD MI50 32GB at 10 t/s (tg) &amp;amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)" src="https://preview.redd.it/lor8ccu2xybg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=857ab421f987d81024114a5c2bc2cf35859061b4" title="16x AMD MI50 32GB at 10 t/s (tg) &amp;amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek 3.2 AWQ 4bit @ 10 tok/s (output) // 2000 tok/s (input of 23k tok)&lt;/p&gt; &lt;p&gt;on vllm-gfx906-deepseek with 69000 context length&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Power draw&lt;/strong&gt;: 550W (idle) / 2400W (peak inference)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: run Deepseek V3.2 AWQ 4-bit on most cost effective hardware like 16*MI50 at decent speed (token generation &amp;amp; prompt processing)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Coming next&lt;/strong&gt;: open source a future test setup of 32 AMD MI50 32GB for Kimi K2 Thinking&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Credits&lt;/strong&gt;: BIG thanks to the Global Open source Community!&lt;/p&gt; &lt;p&gt;All setup details here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32"&gt;https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Feel free to ask any questions and/or share any comments.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;ps: it might be a good alternative to CPU hardwares as RAM price increases and the prompt processing speed will be much better with 16 TB/s bandwidth + tensor parallelism! &lt;/p&gt; &lt;p&gt;ps2: i'm just a random guy with average software dev background using LLMs to make it run. Goal is to be ready for LOCAL AGI without spending +300k$... &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai-infos"&gt; /u/ai-infos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lor8ccu2xybg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T18:22:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7hywi</id>
    <title>How do you manage quality when AI agents write code faster than humans can review it?</title>
    <updated>2026-01-08T17:28:30+00:00</updated>
    <author>
      <name>/u/lostsoul8282</name>
      <uri>https://old.reddit.com/user/lostsoul8282</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are shifting to an agentic workflow. My thesis is &amp;quot;Code at Inference Speed.&amp;quot; My CTO's counter-argument is that &lt;strong&gt;reviewing code is harder than writing it&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;His concern is simple: If AI increases code volume by 10x, human review becomes a fatal bottleneck. He predicts technical debt will explode because humans can‚Äôt mentally verify that much logic that quickly.&lt;/p&gt; &lt;p&gt;How do handle this? I know one option is to slow down releases but is there any other approaches people are taking.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lostsoul8282"&gt; /u/lostsoul8282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7hywi/how_do_you_manage_quality_when_ai_agents_write/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7hywi/how_do_you_manage_quality_when_ai_agents_write/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7hywi/how_do_you_manage_quality_when_ai_agents_write/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T17:28:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7hikw</id>
    <title>Qwen3-4B-Instruct-2507 multilingual FT with upscaled Polish language</title>
    <updated>2026-01-08T17:12:08+00:00</updated>
    <author>
      <name>/u/Significant_Focus134</name>
      <uri>https://old.reddit.com/user/Significant_Focus134</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;Just wanted to share a preview of my latest finetuned model based on Qwen3-4B-Instruct-2507.&lt;/p&gt; &lt;p&gt;Languages ratio:&lt;/p&gt; &lt;p&gt;Polish - high&lt;br /&gt; English - medium&lt;br /&gt; Chinese - medium&lt;br /&gt; Czech - medium/low&lt;br /&gt; Ukrainian - medium/low&lt;br /&gt; Russian - medium/low&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/piotr-ai/polanka_4b_v0.3_preview_260108_qwen3_gguf"&gt;https://huggingface.co/piotr-ai/polanka_4b_v0.3_preview_260108_qwen3_gguf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant_Focus134"&gt; /u/Significant_Focus134 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7hikw/qwen34binstruct2507_multilingual_ft_with_upscaled/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7hikw/qwen34binstruct2507_multilingual_ft_with_upscaled/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7hikw/qwen34binstruct2507_multilingual_ft_with_upscaled/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T17:12:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7jd1a</id>
    <title>LFM2.5 1.2B Instruct is amazing</title>
    <updated>2026-01-08T18:17:04+00:00</updated>
    <author>
      <name>/u/Paramecium_caudatum_</name>
      <uri>https://old.reddit.com/user/Paramecium_caudatum_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This model punches way above its weight. It outperforms every other model I've tried in this size range and runs smoothly on basically any hardware. If you haven't tried it yet, you definitely should.&lt;/p&gt; &lt;p&gt;Important note:&lt;br /&gt; &amp;quot;&amp;quot;&amp;quot;&lt;br /&gt; We recommend using it for agentic tasks, data extraction, and RAG. It is not recommended for knowledge-intensive tasks and programming.&lt;/p&gt; &lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct"&gt;https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Paramecium_caudatum_"&gt; /u/Paramecium_caudatum_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7jd1a/lfm25_12b_instruct_is_amazing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7jd1a/lfm25_12b_instruct_is_amazing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7jd1a/lfm25_12b_instruct_is_amazing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T18:17:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7c0pd</id>
    <title>AI21 releases Jamba2 3B and Jamba2 Mini, built for grounding and instruction following</title>
    <updated>2026-01-08T13:38:34+00:00</updated>
    <author>
      <name>/u/zennaxxarion</name>
      <uri>https://old.reddit.com/user/zennaxxarion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Disclaimer: I work for AI21, creator of the Jamba model family.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;We‚Äôre excited to announce the public release of Jamba2 3B and Jamba2 Mini.&lt;/p&gt; &lt;p&gt;The Jamba2 family aims to give enterprises cost-effective models that will integrate well into production agent stacks.&lt;/p&gt; &lt;p&gt;These models are designed for reliable instruction following and grounded outputs, working well over long documents and avoiding drifting once context becomes large.&lt;/p&gt; &lt;p&gt;They perform best for precise question answering over internal policies, technical manuals and knowledge bases, without the overhead of thinking tokens which can become costly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key performance data&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Jamba2 3B and Jamba2 Mini outperform peers due to their hybrid SSM-Transformer architecture and KV cache innovations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Outpaces Ministral3 14B and Qwen3 30B A3B across FACTS, IFBench and IFEval. &lt;/li&gt; &lt;li&gt;Beats Ministral3 3B and Qwen3 4B on IFEval and IFBench, tying with Qwen3 4B as category leader on FACTS.&lt;/li&gt; &lt;li&gt;At context lengths of 100K, Jamba2 Mini delivers 2.7X greater throughput than Ministral3 14B and 1.4X greater throughout than Qwen3 30B A3B.&lt;/li&gt; &lt;li&gt;At context lengths of 100K, Jamba2 3B delivers 1.7X greater throughout than Ministral3 3B and 2.7X greater throughput than Qwen 3 14B.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It‚Äôs available today in AI21‚Äôs SaaS and from Hugging Face.&lt;/p&gt; &lt;p&gt;Happy to answer questions or dig into benchmarks if people want more detail.&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="http://www.ai21.com/blog/introducing-jamba2"&gt;http://www.ai21.com/blog/introducing-jamba2&lt;/a&gt;&lt;br /&gt; Hugging Face: &lt;a href="https://huggingface.co/collections/ai21labs/jamba2"&gt;https://huggingface.co/collections/ai21labs/jamba2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zennaxxarion"&gt; /u/zennaxxarion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7c0pd/ai21_releases_jamba2_3b_and_jamba2_mini_built_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7c0pd/ai21_releases_jamba2_3b_and_jamba2_mini_built_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7c0pd/ai21_releases_jamba2_3b_and_jamba2_mini_built_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T13:38:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1q71sbe</id>
    <title>Dialogue Tree Search - MCTS-style tree search to find optimal dialogue paths (so you don't have to trial-and-error it yourself)</title>
    <updated>2026-01-08T04:08:39+00:00</updated>
    <author>
      <name>/u/ManavTheWorld</name>
      <uri>https://old.reddit.com/user/ManavTheWorld</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/"&gt; &lt;img alt="Dialogue Tree Search - MCTS-style tree search to find optimal dialogue paths (so you don't have to trial-and-error it yourself)" src="https://b.thumbs.redditmedia.com/6NWh2JOC5gMK_IIgH5CXHBl--P730mcKTIYRJh91W0w.jpg" title="Dialogue Tree Search - MCTS-style tree search to find optimal dialogue paths (so you don't have to trial-and-error it yourself)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! I'm sharing an updated version of my MCTS-for-conversations project. Instead of generating single responses, it explores entire conversation trees to find dialogue strategies and prunes bad paths. I built it to help get better research directions for projects, but it can be used for anything&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/shr3e0liv1cg1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eec800c6dcd9f1a4fd033d003fe80e102cba8079"&gt;https://preview.redd.it/shr3e0liv1cg1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eec800c6dcd9f1a4fd033d003fe80e102cba8079&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/MVPandey/DTS"&gt;https://github.com/MVPandey/DTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Motivation: I like MCTS :3 and I originally wanted to make this a dataset-creation agent, but this is what it evolved into on its own. Basically:DTS runs parallel beam search over conversation branches. You give it a goal and opening message, and it:&lt;/p&gt; &lt;p&gt;(Note: this isnt mcts. It's parallel beam search. UCB1 is too wild with llms for me)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Generates N diverse strategies&lt;/li&gt; &lt;li&gt;Forks each into user intent variants - skeptical, cooperative, confused, resistant (if enabled, or defaults to engaged + probing)&lt;/li&gt; &lt;li&gt;Rolls out full multi-turn conversations down each branch&lt;/li&gt; &lt;li&gt;Has 3 independent LLM judges score each trajectory, takes the median&lt;/li&gt; &lt;li&gt;Prunes branches below threshold, backpropagates scores&lt;/li&gt; &lt;li&gt;Repeats for however many rounds you configure&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zkii0idvv1cg1.png?width=762&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=905f9787a8b7c7bfafcc599e95a3b73005c331b4"&gt;https://preview.redd.it/zkii0idvv1cg1.png?width=762&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=905f9787a8b7c7bfafcc599e95a3b73005c331b4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Three judges with median voting helps a lot with the LLM-as-judge variance problem from CAE. Still not grounded in anything real, but outlier scores get filtered. Research context helps but the scroing is still stochastic. I tried a rubric based approach but it was trash.&lt;/p&gt; &lt;p&gt;Main additions over CAE:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;user intent forking (strategies get stress-tested against different personas)&lt;/li&gt; &lt;li&gt;deep research integration via GPT-Researcher for domain context&lt;/li&gt; &lt;li&gt;proper visualization with conversation playback&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Only supports openai compatible endpoints atm - works with whatever models you have access to there. It's token-hungry though, a full run can hit 300+ LLM calls depending on config. If running locally, disable parallel calls&lt;/p&gt; &lt;p&gt;It's open source (Apache 2.0) and I'm happy to take contributions if anyone wants to help out. Just a project.&lt;/p&gt; &lt;p&gt;--&lt;/p&gt; &lt;p&gt;BTW: Backend was done mostly by me as the planner/sys designer, etc + Claude Code for implementation/refactoring. Frontend was purely vibe coded. Sorry if the code is trash.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ManavTheWorld"&gt; /u/ManavTheWorld &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T04:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7dlkn</id>
    <title>Qwen3-VL-Reranker - a Qwen Collection</title>
    <updated>2026-01-08T14:45:00+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7dlkn/qwen3vlreranker_a_qwen_collection/"&gt; &lt;img alt="Qwen3-VL-Reranker - a Qwen Collection" src="https://external-preview.redd.it/p_EUBuVnZfgcYfu2zAo996Hix2TFsBWGTVl7mQyY9Tk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b4e2c1310fa6d24d7fb43df7672f7329a04cfbc" title="Qwen3-VL-Reranker - a Qwen Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl-reranker"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7dlkn/qwen3vlreranker_a_qwen_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7dlkn/qwen3vlreranker_a_qwen_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T14:45:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1q77rxh</id>
    <title>Z-image base model is being prepared for release</title>
    <updated>2026-01-08T09:51:33+00:00</updated>
    <author>
      <name>/u/Ravencloud007</name>
      <uri>https://old.reddit.com/user/Ravencloud007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q77rxh/zimage_base_model_is_being_prepared_for_release/"&gt; &lt;img alt="Z-image base model is being prepared for release" src="https://preview.redd.it/038zb25ok3cg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2cbdfd4fbd53811cd0fc218bed6e466b49ff678" title="Z-image base model is being prepared for release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher&amp;amp;since=2025-12-31&amp;amp;until=2026-01-08"&gt;https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher&amp;amp;since=2025-12-31&amp;amp;until=2026-01-08&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ravencloud007"&gt; /u/Ravencloud007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/038zb25ok3cg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q77rxh/zimage_base_model_is_being_prepared_for_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q77rxh/zimage_base_model_is_being_prepared_for_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T09:51:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7a62a</id>
    <title>AI21 Labs releases Jamba2</title>
    <updated>2026-01-08T12:10:15+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/"&gt; &lt;img alt="AI21 Labs releases Jamba2" src="https://b.thumbs.redditmedia.com/Il111fZ012O0JrQgLsZklbi8sbSvI68SHycPLPcigNc.jpg" title="AI21 Labs releases Jamba2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zmo6dijns4cg1.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ba9fd085bb5b3fb720adf85cf28c3a8b63ba44cb"&gt;https://preview.redd.it/zmo6dijns4cg1.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ba9fd085bb5b3fb720adf85cf28c3a8b63ba44cb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;52B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-Mini"&gt;https://huggingface.co/ai21labs/AI21-Jamba2-Mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Jamba2 Mini is an open source small language model built for enterprise reliability. With 12B active parameters (52B total), it delivers precise question answering without the computational overhead of reasoning models. The model's SSM-Transformer architecture provides a memory-efficient solution for production agent stacks where consistent, grounded outputs are critical.&lt;/p&gt; &lt;p&gt;Released under Apache 2.0 License with a 256K context window, Jamba2 Mini is designed for enterprise workflows that demand accuracy and steerability. For more details, read the &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-Mini/blob/main/ai21.com/blog/introducing-jamba2"&gt;full release blog post&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Key Advantages&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Superior reliability-to-throughput ratio:&lt;/strong&gt; Maintains high performance at 100K+ token contexts&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Category-leading benchmarks:&lt;/strong&gt; Excels on IFBench, IFEval, Collie, and FACTS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Statistically significant quality wins:&lt;/strong&gt; Outperforms comparable models on real-world enterprise tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;256K context window:&lt;/strong&gt; Processes technical manuals, research papers, and knowledge bases&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apache 2.0 License:&lt;/strong&gt; Fully open source for commercial use&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Production-optimized:&lt;/strong&gt; Lean memory footprint for scalable deployments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cqwicpwts4cg1.png?width=2400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=593fed6a7d2094908b6f1878ea12a8e4f5e67e6d"&gt;https://preview.redd.it/cqwicpwts4cg1.png?width=2400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=593fed6a7d2094908b6f1878ea12a8e4f5e67e6d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;3B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-3B"&gt;https://huggingface.co/ai21labs/AI21-Jamba2-3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Jamba2 3B is an ultra-compact open source model designed to bring enterprise-grade reliability to on-device deployments. At just 3B parameters, it runs efficiently on consumer devices‚ÄîiPhones, Androids, Macs, and PCs‚Äîwhile maintaining the grounding and instruction-following capabilities required for production use.&lt;/p&gt; &lt;p&gt;Released under Apache 2.0 License with a 256K context window, Jamba2 3B enables developers to build reliable AI applications for edge environments. For more details, read the &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-3B/blob/main/ai21.com/blog/introducing-jamba2"&gt;full release blog post&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Key Advantages&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;On-device deployment:&lt;/strong&gt; Runs efficiently on iPhones, Androids, Macs, and PCs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ultra-compact footprint:&lt;/strong&gt; 3B parameters enabling edge deployments with minimal resources&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmark leadership:&lt;/strong&gt; Excels on IFBench, IFEval, Collie, and FACTS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;256K context window:&lt;/strong&gt; Processes long documents and knowledge bases&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apache 2.0 License:&lt;/strong&gt; Fully open source for commercial use&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SSM-Transformer architecture:&lt;/strong&gt; Memory-efficient design for resource-constrained environments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;it works in llama.cpp, tested on my Windows desktop:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ijzgde7bg5cg1.png?width=3802&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=983bc8e27ec59065d4b548e78eb4f50405507c71"&gt;https://preview.redd.it/ijzgde7bg5cg1.png?width=3802&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=983bc8e27ec59065d4b548e78eb4f50405507c71&lt;/a&gt;&lt;/p&gt; &lt;p&gt;fixed blog post &lt;a href="https://www.ai21.com/blog/introducing-jamba2/"&gt;https://www.ai21.com/blog/introducing-jamba2/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUFs are in progress &lt;a href="https://huggingface.co/mradermacher/model_requests/discussions/1683"&gt;https://huggingface.co/mradermacher/model_requests/discussions/1683&lt;/a&gt;&lt;/p&gt; &lt;p&gt;previous generation of Jamba models&lt;/p&gt; &lt;p&gt;399B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7"&gt;https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;52B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7"&gt;https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;3B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B"&gt;https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T12:10:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7d8bj</id>
    <title>Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt</title>
    <updated>2026-01-08T14:29:47+00:00</updated>
    <author>
      <name>/u/Prior-Arm-6705</name>
      <uri>https://old.reddit.com/user/Prior-Arm-6705</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/"&gt; &lt;img alt="Jensen Huang saying &amp;quot;AI&amp;quot; 121 times during the NVIDIA CES keynote - cut with one prompt" src="https://external-preview.redd.it/M2cyNzBqaHB4NGNnMeuNas4_kS8fQc08s_eqp1ss4JB4szq45v23OyPEbFog.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a660094eff764f4c2c968c5de87ba1bafcb35b9" title="Jensen Huang saying &amp;quot;AI&amp;quot; 121 times during the NVIDIA CES keynote - cut with one prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone had to count it. Turns out Jensen said &amp;quot;AI&amp;quot; exactly 121 times in the CES 2025 keynote.&lt;/p&gt; &lt;p&gt;I used &lt;a href="https://github.com/OpenAgentPlatform/Dive"&gt;https://github.com/OpenAgentPlatform/Dive&lt;/a&gt; (open-source MCP client) + two MCPs I made:&lt;/p&gt; &lt;p&gt;- &lt;a href="https://github.com/kevinwatt/yt-dlp-mcp"&gt;https://github.com/kevinwatt/yt-dlp-mcp&lt;/a&gt; - YouTube download&lt;br /&gt; - &lt;a href="https://github.com/kevinwatt/ffmpeg-mcp-lite"&gt;https://github.com/kevinwatt/ffmpeg-mcp-lite&lt;/a&gt; - video editing&lt;/p&gt; &lt;p&gt;&lt;strong&gt;One prompt:&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Task: Create a compilation video of every exact moment Jensen Huang says &amp;quot;AI&amp;quot;.&lt;br /&gt; Video source: &lt;a href="https://www.youtube.com/watch?v=0NBILspM4c4"&gt;https://www.youtube.com/watch?v=0NBILspM4c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Instructions:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Download video in 720p + subtitles in JSON3 format (word-level timestamps)&lt;/p&gt; &lt;p&gt;Parse JSON3 to find every &amp;quot;AI&amp;quot; instance with precise start/end times&lt;/p&gt; &lt;p&gt;Use ffmpeg to cut clips (~50-100ms padding for natural sound)&lt;/p&gt; &lt;p&gt;Concatenate all clips chronologically&lt;/p&gt; &lt;p&gt;Output: Jensen_CES_AI.mp4&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Dive chained the two MCPs together - download ‚Üí parse timestamps ‚Üí cut 121 clips ‚Üí merge. All local, no cloud.&lt;/p&gt; &lt;p&gt;If you want to see how it runs: &lt;a href="https://www.youtube.com/watch?v=u_7OtyYAX74"&gt;https://www.youtube.com/watch?v=u_7OtyYAX74&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The result is... hypnotic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prior-Arm-6705"&gt; /u/Prior-Arm-6705 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hein55gpx4cg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T14:29:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
