<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-13T21:46:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r41s17</id>
    <title>FIX: The step-3.5-flash GGUF has botched embedded jinja</title>
    <updated>2026-02-13T21:30:41+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r41s17/fix_the_step35flash_gguf_has_botched_embedded/"&gt; &lt;img alt="FIX: The step-3.5-flash GGUF has botched embedded jinja" src="https://preview.redd.it/v9crofskxbjg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73a2d6a0e7750a9a5e8c05a0fe01a5f4ced107cf" title="FIX: The step-3.5-flash GGUF has botched embedded jinja" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For anybody who tries some of the GGUF of step-3.5-flash, the embedded jinja template in many of them (all I tried) is missing the &amp;lt;think&amp;gt; flag for some reason or other. The official &lt;a href="https://huggingface.co/stepfun-ai/Step-3.5-Flash/tree/main"&gt;Step-3.5-Flash&lt;/a&gt; / chat_template.jinja has the &amp;lt;think&amp;gt; but the embedded GGUFs (likely grabbed before they added it) do not. (That is true with the official GGUF from stepfun as well - missing the &amp;lt;think&amp;gt; tag in the embedded jinja).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v9crofskxbjg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r41s17/fix_the_step35flash_gguf_has_botched_embedded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r41s17/fix_the_step35flash_gguf_has_botched_embedded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T21:30:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3l572</id>
    <title>MiniMax onX: Weights dropping REALLY, REALLY, SOON</title>
    <updated>2026-02-13T09:51:33+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3l572/minimax_onx_weights_dropping_really_really_soon/"&gt; &lt;img alt="MiniMax onX: Weights dropping REALLY, REALLY, SOON" src="https://preview.redd.it/jrgpe9krh8jg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=30a1ae2be695a2a4f2dee2ca962e2fa76614dcc1" title="MiniMax onX: Weights dropping REALLY, REALLY, SOON" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jrgpe9krh8jg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3l572/minimax_onx_weights_dropping_really_really_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3l572/minimax_onx_weights_dropping_really_really_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T09:51:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3yn5r</id>
    <title>MiniMax 2.5 locally on my setup</title>
    <updated>2026-02-13T19:29:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yn5r/minimax_25_locally_on_my_setup/"&gt; &lt;img alt="MiniMax 2.5 locally on my setup" src="https://preview.redd.it/23280myscbjg1.png?width=140&amp;amp;height=111&amp;amp;auto=webp&amp;amp;s=a54ed47e68245438a955f0e68be7760279ba461b" title="MiniMax 2.5 locally on my setup" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GGUF created by &lt;a href="/u/Remarkable_Jicama775"&gt;u/Remarkable_Jicama775&lt;/a&gt;, available from here: &lt;a href="https://huggingface.co/ox-ox/MiniMax-M2.5-GGUF"&gt;https://huggingface.co/ox-ox/MiniMax-M2.5-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;20 t/s on Q3&lt;/p&gt; &lt;p&gt;setup: 3x3090 + X399 Taichi + DDR4 (2667 MT/s) + Threadripper 1920X&lt;/p&gt; &lt;p&gt;command:&lt;/p&gt; &lt;p&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2 llama-server --jinja --host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--parallel 1 --flash-attn on -m /mnt/models2/MiniMax/minimax-m2.5-Q3_K_L.gguf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;EDIT: Enabling an additional 3060 increases the speed to over 22 tok/s, simply because the bottleneck is my slow RAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r3yn5r"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yn5r/minimax_25_locally_on_my_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yn5r/minimax_25_locally_on_my_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T19:29:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3ob0r</id>
    <title>GLM 5 has a regression in international language writing according to NCBench</title>
    <updated>2026-02-13T12:49:51+00:00</updated>
    <author>
      <name>/u/jugalator</name>
      <uri>https://old.reddit.com/user/jugalator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This surprised me but also confirmed my poor first impression of it, since I happened to use it for text generation in a less common language and it performed very poorly, barely even like the aging Gemini 2.5 Flash and more like a good 70B Llama 3.x model.&lt;/p&gt; &lt;p&gt;At NCBench - Language Writing, it trails GLM 4.5-4.7 by quite a distance when tested for European languages and Hindi. GLM 4.5 is the clear, superior release in this regard according to NCBench.&lt;/p&gt; &lt;p&gt;Interestingly, Language &lt;em&gt;Comprehension&lt;/em&gt; didn't seem to regress much at all!&lt;/p&gt; &lt;p&gt;GLM 5 may be great and all, but just a heads up if you use it for this particular scenario since I think it's been flying below the radar.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jugalator"&gt; /u/jugalator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.nc-bench.com/tests/language-writing"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ob0r/glm_5_has_a_regression_in_international_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ob0r/glm_5_has_a_regression_in_international_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T12:49:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3r3bt</id>
    <title>Make a SVG of a Pelican riding a bicycle - Small MoE edition.</title>
    <updated>2026-02-13T14:48:21+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3r3bt/make_a_svg_of_a_pelican_riding_a_bicycle_small/"&gt; &lt;img alt="Make a SVG of a Pelican riding a bicycle - Small MoE edition." src="https://preview.redd.it/hrah141zm9jg1.png?width=140&amp;amp;height=65&amp;amp;auto=webp&amp;amp;s=1be312b3eb2454d314cc7c1a60371b3afb3f0f8d" title="Make a SVG of a Pelican riding a bicycle - Small MoE edition." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r3r3bt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3r3bt/make_a_svg_of_a_pelican_riding_a_bicycle_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3r3bt/make_a_svg_of_a_pelican_riding_a_bicycle_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T14:48:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3q0qb</id>
    <title>llama.cpp llama-server running SSM models VRAM fix merged</title>
    <updated>2026-02-13T14:04:54+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;During my time fixing the Kimi Linear server bug reported by &lt;a href="/u/Lord_Pazzu"&gt;u/Lord_Pazzu&lt;/a&gt;, I discovered that running llama-server running SSM hybrid models in general uses KV cache that is multiple of the number of parallel threads (--parallel), so for example, if you run Nemotron 3 Nano at 1M context and --parallel 8, then it would use 48GB VRAM KV cache instead of 6GB even though each server instance can only serve 128K context. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/19552"&gt;https://github.com/ggml-org/llama.cpp/issues/19552&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With this fix, you will only use 6GB just like the transformer models. That means with 48GB VRAM to spare, you can now serve 8 users simultaneously with 1M context each.&lt;/p&gt; &lt;p&gt;Merged PR:&lt;br /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/pull/19559"&gt;https://github.com/ggml-org/llama.cpp/pull/19559&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This applies to all SSM hybrid models like Qwen3Next, Kimi Linear, Nemotron 3 Nano, etc.&lt;/p&gt; &lt;p&gt;So if u r a llama-server user with these new models, then it will be a great news to you. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3q0qb/llamacpp_llamaserver_running_ssm_models_vram_fix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3q0qb/llamacpp_llamaserver_running_ssm_models_vram_fix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3q0qb/llamacpp_llamaserver_running_ssm_models_vram_fix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T14:04:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3toe5</id>
    <title>MiniMax 2.5 full precision FP8 running LOCALLY on vLLM x 8x Pro 6000</title>
    <updated>2026-02-13T16:25:08+00:00</updated>
    <author>
      <name>/u/cyysky</name>
      <uri>https://old.reddit.com/user/cyysky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe5/minimax_25_full_precision_fp8_running_locally_on/"&gt; &lt;img alt="MiniMax 2.5 full precision FP8 running LOCALLY on vLLM x 8x Pro 6000" src="https://preview.redd.it/o66j8wb57ajg1.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=470df23fa8a32ba9ccb885ddda7d09d8607191ca" title="MiniMax 2.5 full precision FP8 running LOCALLY on vLLM x 8x Pro 6000" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MiniMax 2.5 full precision FP8 running LOCALLY on vLLM x 8x Pro 6000 &lt;/p&gt; &lt;p&gt;Hosting it is easier then I thought, it just reuse the same script for M2.1.&lt;br /&gt; Time to do the vibe coding test! &lt;/p&gt; &lt;p&gt;Generation: 70 tokens-per-sec and 122 tokens-per-sec for two conneciton&lt;br /&gt; Peak Memory: 728GB&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o66j8wb57ajg1.png?width=1916&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddf90e73b3792510afd31f58604a8ccd0ab18246"&gt;https://preview.redd.it/o66j8wb57ajg1.png?width=1916&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddf90e73b3792510afd31f58604a8ccd0ab18246&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/99vp2ub57ajg1.png?width=845&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=40fe8e0b643735c6fc10b5d6e47bb5fa279b45f2"&gt;https://preview.redd.it/99vp2ub57ajg1.png?width=845&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=40fe8e0b643735c6fc10b5d6e47bb5fa279b45f2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cyysky"&gt; /u/cyysky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe5/minimax_25_full_precision_fp8_running_locally_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe5/minimax_25_full_precision_fp8_running_locally_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe5/minimax_25_full_precision_fp8_running_locally_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:25:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3qwyi</id>
    <title>oMLX - open-source MLX inference server with paged SSD caching for Apple Silicon</title>
    <updated>2026-02-13T14:41:23+00:00</updated>
    <author>
      <name>/u/cryingneko</name>
      <uri>https://old.reddit.com/user/cryingneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3qwyi/omlx_opensource_mlx_inference_server_with_paged/"&gt; &lt;img alt="oMLX - open-source MLX inference server with paged SSD caching for Apple Silicon" src="https://preview.redd.it/gy8epdcex9jg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63f504c27de7ef48c0affbd687b587bbf921db42" title="oMLX - open-source MLX inference server with paged SSD caching for Apple Silicon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I know things are buzzing with the MiniMax and GLM releases right now, so I'm not sure if today is the best day to post this - but I wanted to share something I've been working on and I'm genuinely proud of.&lt;/p&gt; &lt;p&gt;Whether you love or hate Ollama, we all know what it is. Setting aside the technical debates, I think Ollama absolutely nailed the concept of making LLMs accessible to everyday users. But it always bugged me that there wasn't a really easy-to-use, open-source app built on MLX. So I built one.&lt;/p&gt; &lt;h2&gt;What is oMLX?&lt;/h2&gt; &lt;p&gt;An LLM inference server for Apple Silicon with a native macOS menubar app. Download the DMG, drag to Applications, done. No terminal, no config files to start.&lt;/p&gt; &lt;h2&gt;Why I built this&lt;/h2&gt; &lt;p&gt;I don't need VLM or TTS/STT right now. What I needed was:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A server I can easily spin up on my Mac&lt;/li&gt; &lt;li&gt;An LLM backend for my Obsidian Copilot&lt;/li&gt; &lt;li&gt;Embedding + Reranking for my notes - all in one app, not three separate tools&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And then there's the coding agent use case. I personally wanted to use local LLMs with Claude Code. But here's the thing we all know - prefix caching in existing apps is... rough. Coding agents send requests where the prefix keeps shifting, invalidating the cache. A few turns later, the agent circles back to a previous prefix, and now your Mac has to re-prefill that entire context from scratch. Painfully slow.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;oMLX solves this with paged SSD caching.&lt;/strong&gt; Every KV cache block gets persisted to SSD. When a previous prefix comes back, it's restored instantly instead of being recomputed. This is a game-changer for long coding sessions.&lt;/p&gt; &lt;p&gt;OK, enough rambling. Here's what's under the hood:&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Features&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt; - Continuous batching via mlx-lm - handles multiple concurrent requests - Multi-model serving - load LLM + Embedding + Reranker simultaneously, with LRU eviction - Reasoning model support - automatic &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; tag handling (DeepSeek, MiniMax, etc.) - Harmony protocol - native support for gpt-oss models&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Caching&lt;/strong&gt; - Paged KV Cache - block-based with prefix sharing and copy-on-write (vLLM&amp;amp;vllm-mlx project inspired) - SSD tiered caching - automatic GPU-to-SSD offloading for virtually unlimited context caching - Hybrid cache - mixed KVCache + RotatingKVCache for complex architectures (Gemma3, etc.) - Persistent cache - KV cache blocks survive server restarts&lt;/p&gt; &lt;p&gt;&lt;strong&gt;API&lt;/strong&gt; - OpenAI compatible - &lt;code&gt;/v1/chat/completions&lt;/code&gt;, &lt;code&gt;/v1/completions&lt;/code&gt;, &lt;code&gt;/v1/models&lt;/code&gt;, &lt;code&gt;/v1/embeddings&lt;/code&gt; - Anthropic compatible - &lt;code&gt;/v1/messages&lt;/code&gt; - Tool calling - JSON, Qwen, Gemma, MiniMax, GLM formats + MCP - Structured output - JSON mode and JSON Schema&lt;/p&gt; &lt;p&gt;&lt;strong&gt;macOS App&lt;/strong&gt; - Native menubar app (PyObjC, not Electron) - Admin dashboard with built-in chat and real-time monitoring - HuggingFace model downloader built into the dashboard - Signed &amp;amp; notarized DMG&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt; Apple Silicon (M1+), macOS 14.0+&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/jundot/omlx"&gt;github.com/jundot/omlx&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Download:&lt;/strong&gt; &lt;a href="https://github.com/jundot/omlx/releases"&gt;github.com/jundot/omlx/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm genuinely happy with what I've built. If you have similar needs, I hope oMLX makes your workflow better too. It's 100% open source - if my hobby project can help someone out there, that's even better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cryingneko"&gt; /u/cryingneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gy8epdcex9jg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3qwyi/omlx_opensource_mlx_inference_server_with_paged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3qwyi/omlx_opensource_mlx_inference_server_with_paged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T14:41:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3ntgi</id>
    <title>Deepseek announced they are testing a new model.</title>
    <updated>2026-02-13T12:25:39+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ntgi/deepseek_announced_they_are_testing_a_new_model/"&gt; &lt;img alt="Deepseek announced they are testing a new model." src="https://preview.redd.it/y5kcxf8699jg1.jpg?width=140&amp;amp;height=58&amp;amp;auto=webp&amp;amp;s=7a1927f261bcd4f8544cc048c08f30ee495de504" title="Deepseek announced they are testing a new model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/y5kcxf8699jg1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1a7fad6dc2630447bffd04ec671cfb62edc9187f"&gt;https://preview.redd.it/y5kcxf8699jg1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1a7fad6dc2630447bffd04ec671cfb62edc9187f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From Chinese group&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rutggcjgf9jg1.png?width=342&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e393275ade2c1baff47b0855457c817e87bca3e3"&gt;https://preview.redd.it/rutggcjgf9jg1.png?width=342&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e393275ade2c1baff47b0855457c817e87bca3e3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;the benchmark model name is fake (placeholder) . This is just for distinguishing&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The benchmark is test reading skills&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Testing completed, OpenAI MRCR 8-pin&lt;/p&gt; &lt;p&gt;-------------------------------------------------------------&lt;/p&gt; &lt;p&gt;Index | Target | Tokens | Prefix | Score | Result&lt;/p&gt; &lt;p&gt;-------------------------------------------------------------&lt;/p&gt; &lt;p&gt;78 | 128000 | 130660 | BfohKPVF96 | 0.9821 | üÜó 0.98&lt;/p&gt; &lt;p&gt;71 | 128000 | 132425 | NPnuBb2ccE | 0.0575 | üÜó 0.06&lt;/p&gt; &lt;p&gt;32 | 128000 | 132828 | dlUj2XS3iz | 1.0000 | ‚úÖ Pass&lt;/p&gt; &lt;p&gt;45 | 128000 | 135258 | VAUPEFeyUy | 1.0000 | ‚úÖ Pass&lt;/p&gt; &lt;p&gt;56 | 128000 | 136965 | kZWrPWyo2z | 0.0276 | üÜó 0.03&lt;/p&gt; &lt;p&gt;7 | 128000 | 136974 | kej4Qdr9Mf | 0.0101 | üÜó 0.01&lt;/p&gt; &lt;p&gt;57 | 128000 | 137211 | HdvXqxVvwQ | 0.0420 | üÜó 0.04&lt;/p&gt; &lt;p&gt;87 | 128000 | 138158 | 4KJuJvpDKt | 0.1123 | üÜó 0.11&lt;/p&gt; &lt;p&gt;64 | 128000 | 138512 | piNIebm2Zr | 0.0560 | üÜó 0.06&lt;/p&gt; &lt;p&gt;88 | 128000 | 138628 | 9W0rMIR3gM | 0.0963 | üÜó 0.10&lt;/p&gt; &lt;p&gt;69 | 256000 | 255410 | BdPq3nqqWy | 0.0307 | üÜó 0.03&lt;/p&gt; &lt;p&gt;40 | 256000 | 255073 | mlzCS98ySY | 0.0221 | üÜó 0.02&lt;/p&gt; &lt;p&gt;58 | 256000 | 254750 | 7ABmnzg5oI | 0.9830 | üÜó 0.98&lt;/p&gt; &lt;p&gt;61 | 256000 | 254317 | gkaLloQvjH | 0.1098 | üÜó 0.11&lt;/p&gt; &lt;p&gt;97 | 256000 | 253819 | 9RNBXn2Gh5 | 1.0000 | ‚úÖ Pass&lt;/p&gt; &lt;p&gt;51 | 256000 | 251993 | E4c3w7oF2w | 0.0703 | üÜó 0.07&lt;/p&gt; &lt;p&gt;23 | 256000 | 251766 | SNtG1BhaDM | 0.9952 | üÜó 1.00&lt;/p&gt; &lt;p&gt;280 | 256000 | 261742 | RsuyJ8tkrC | 0.0681 | üÜó 0.07&lt;/p&gt; &lt;p&gt;278 | 256000 | 263214 | D7Ndj9vdKm | 0.1613 | üÜó 0.16&lt;/p&gt; &lt;p&gt;224 | 256000 | 265550 | 1YZYhQtMCW | 0.1545 | üÜó 0.15&lt;/p&gt; &lt;p&gt;-------------------------------------------------------------&lt;/p&gt; &lt;p&gt;üèÜ Total Marks: 0.3489&lt;/p&gt; &lt;p&gt;üìè Length statistics:&lt;/p&gt; &lt;p&gt;- 128000 Tokens: 0.3384 (n=10)&lt;/p&gt; &lt;p&gt;- 256000 Tokens: 0.3595 (n=10)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ntgi/deepseek_announced_they_are_testing_a_new_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ntgi/deepseek_announced_they_are_testing_a_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ntgi/deepseek_announced_they_are_testing_a_new_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T12:25:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3uj0h</id>
    <title>MiniMax-M2.5 (230B MoE) GGUF is here - First impressions on M3 Max 128GB</title>
    <updated>2026-02-13T16:56:58+00:00</updated>
    <author>
      <name>/u/Remarkable_Jicama775</name>
      <uri>https://old.reddit.com/user/Remarkable_Jicama775</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üî• UPDATE 2: Strict Perplexity Benchmark &amp;amp; Trade-off Analysis&lt;/p&gt; &lt;p&gt;Thanks to &lt;a href="/u/ubergarm"&gt;u/ubergarm&lt;/a&gt; and the community for pointing out the context discrepancy in my initial PPL run (I used -c 4096, which inflated the score).&lt;/p&gt; &lt;p&gt;I just re-ran the benchmark on the M3 Max using standard comparison parameters (-c 512, -b 2048, --seed 1337) to get an apples-to-apples comparison with SOTA custom mixes (like IQ4_XS).&lt;/p&gt; &lt;p&gt;The Real Numbers:&lt;/p&gt; &lt;p&gt;My Q3_K_L (Standard): 8.7948 PPL (+/- 0.07)&lt;/p&gt; &lt;p&gt;Custom IQ4_XS Mix (ubergarm): ~8.57 PPL&lt;/p&gt; &lt;p&gt;The Verdict / Why use this Q3_K_L? While the custom mix wins on pure reasoning density (~0.22 PPL delta), this Q3_K_L remains the &amp;quot;bandwidth king&amp;quot; for Mac users.&lt;/p&gt; &lt;p&gt;RAM Headroom: It fits comfortably in 128GB with room for context (unlike Q4 which hits swap).&lt;/p&gt; &lt;p&gt;Speed: Because the attn.* tensors are smaller (Q3 vs Q8 in custom mixes), we are seeing 28.7 t/s generation speed due to lower memory bandwidth pressure.&lt;/p&gt; &lt;p&gt;TL;DR: Use this Q3_K_L if you are strictly limited to 128GB RAM and prioritize speed/compatibility. Use an IQ4_XS mix if you have 192GB+ or prioritize absolute maximum reasoning over speed. &lt;strong&gt;Update: Q3_K_L is officially LIVE on Hugging Face! Link. Tested and verified at 28.7 t/s on M3 Max. Enjoy the native RAM performance!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;üî¨ &lt;strong&gt;Perplexity Validation (WikiText-2)&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Final PPL: 8.2213 +/- 0.09&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Context: 4096 / 32 chunks&lt;/p&gt; &lt;p&gt;Outcome: The Q3_K_L quantization maintains high logical coherence while boosting speed to 28.7 t/s. Minimal degradation for a ~20GB size reduction vs Q4. Just ran PPL on my Q3_K_L (110.22 GiB). Got a Final PPL of 8.2213 (+/- 0.09) on WikiText-2. It seems that going the FP8 -&amp;gt; F16 Master -&amp;gt; Q3_K_L route really paid off compared to standard quants. It beats the IQ4_XS efficiency curve while fitting perfectly in 128GB RAM at 28.7 t/s&lt;/p&gt; &lt;p&gt;The new MiniMax-M2.5 is a beast, but running a 230B MoE locally isn't easy. I‚Äôve finished the quantization process using llama.cpp (b8022) and optimized it specifically for high-RAM Apple Silicon.&lt;/p&gt; &lt;p&gt;üöÄ The &amp;quot;Sweet Spot&amp;quot; for 128GB RAM: Q3_K_L After initial testing with Q4_K_M (132GB), it was clear that hitting the swap was killing performance. I went back to the F16 Master (457GB) to cook a high-quality Q3_K_L (~110GB).&lt;/p&gt; &lt;p&gt;Benchmarks (M3 Max 128GB):&lt;/p&gt; &lt;p&gt;Prompt Processing: &lt;strong&gt;99.2 t/s&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Generation: &lt;strong&gt;28.7 t/s üöÄ&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;RAM Behavior: 100% native RAM usage. Zero swap lag.&lt;/p&gt; &lt;p&gt;üõ† Technical Details To ensure maximum reasoning fidelity, I avoided direct FP8-to-Quant conversion. The workflow was: Original FP8 -&amp;gt; F16 GGUF Master -&amp;gt; K-Quants (Q4_K_M &amp;amp; Q3_K_L).&lt;/p&gt; &lt;p&gt;Architecture: 230B Mixture of Experts (MiniMax-M2).&lt;/p&gt; &lt;p&gt;Logic: The Jinja chat template is working perfectly; &amp;lt;think&amp;gt; tags are isolated as intended.&lt;/p&gt; &lt;p&gt;Context: Native 196k support.&lt;/p&gt; &lt;p&gt;üì• Links &amp;amp; Resources GGUF Repo: &lt;a href="https://huggingface.co/ox-ox/MiniMax-M2.5-GGUF"&gt;https://huggingface.co/ox-ox/MiniMax-M2.5-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Usage: ./llama-cli -m minimax-m2.5-Q3_K_L.gguf -n -1 \ -c 262000 \ -ngl 99 -fa on -ctk q4_0 -ctv q4_0 -b 2048 -ub 1024 --port 8080 --jinja --verbose -sm none --draft 16 -ncmoe 0 --cache-reuse 1024 --draft-p-min 0.5&lt;/p&gt; &lt;p&gt;For those with 64GB or 96GB setups, let me know if there's interest in IQ2_XXS or IQ3_XS versions. I'm happy to cook more if the demand is there!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable_Jicama775"&gt; /u/Remarkable_Jicama775 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3uj0h/minimaxm25_230b_moe_gguf_is_here_first/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3uj0h/minimaxm25_230b_moe_gguf_is_here_first/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3uj0h/minimaxm25_230b_moe_gguf_is_here_first/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:56:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3mnm3</id>
    <title>ByteDance Releases Protenix-v1</title>
    <updated>2026-02-13T11:22:41+00:00</updated>
    <author>
      <name>/u/techlatest_net</name>
      <uri>https://old.reddit.com/user/techlatest_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;ByteDance Releases Protenix-v1: A New Open-Source Model Achieving AF3-Level Performance in Biomolecular Structure Prediction&lt;/h1&gt; &lt;p&gt;Link: &lt;a href="https://github.com/bytedance/Protenix"&gt;https://github.com/bytedance/Protenix&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techlatest_net"&gt; /u/techlatest_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3mnm3/bytedance_releases_protenixv1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3mnm3/bytedance_releases_protenixv1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3mnm3/bytedance_releases_protenixv1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T11:22:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3toe1</id>
    <title>Minimax-M2.5 at same level of GLM-4.7 and DeepSeek-3.2</title>
    <updated>2026-02-13T16:25:08+00:00</updated>
    <author>
      <name>/u/Rascazzione</name>
      <uri>https://old.reddit.com/user/Rascazzione</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe1/minimaxm25_at_same_level_of_glm47_and_deepseek32/"&gt; &lt;img alt="Minimax-M2.5 at same level of GLM-4.7 and DeepSeek-3.2" src="https://preview.redd.it/ps0fnwi7fajg1.png?width=140&amp;amp;height=38&amp;amp;auto=webp&amp;amp;s=e14c464f298d56218853942eb3cb6fdf095ae035" title="Minimax-M2.5 at same level of GLM-4.7 and DeepSeek-3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ps0fnwi7fajg1.png?width=1462&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1209b5ed071f67d465b5ab243fcbc309a676c17"&gt;Coding Index 13/02/2026 Artificial Analisys&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fepkt4hffajg1.png?width=1468&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c457992a63fd80a590b2c3296b1ce95843c7f8f8"&gt;General Index Intelligence 13/02/2026 Artificial Analisys&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Seems Minimax-M2.5 is on par with GLM-4.7 and DeepSeek-3.2, let's see if the Agent capabilities makes differences. &lt;/p&gt; &lt;p&gt;Stats from &lt;a href="https://artificialanalysis.ai/"&gt;https://artificialanalysis.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rascazzione"&gt; /u/Rascazzione &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe1/minimaxm25_at_same_level_of_glm47_and_deepseek32/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe1/minimaxm25_at_same_level_of_glm47_and_deepseek32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe1/minimaxm25_at_same_level_of_glm47_and_deepseek32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:25:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r40o83</id>
    <title>ubergarm/MiniMax-2.5-GGUF</title>
    <updated>2026-02-13T20:47:04+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r40o83/ubergarmminimax25gguf/"&gt; &lt;img alt="ubergarm/MiniMax-2.5-GGUF" src="https://preview.redd.it/e7zeec20qbjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dec023595454edde747bd1bebdaab70e22a17fe5" title="ubergarm/MiniMax-2.5-GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just cooked and benchmarked (perplexity) of some MiniMax-M2.5 GGUF quants over at: &lt;a href="https://huggingface.co/ubergarm/MiniMax-M2.5-GGUF"&gt;https://huggingface.co/ubergarm/MiniMax-M2.5-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The IQ4_XS works on mainline llama.cpp, LMStudio, Kobold CPP etc. The other quants require ik_llama.cpp (which supports all of the quant types of mainline as well).&lt;/p&gt; &lt;p&gt;Gonna get some llama-sweep-bench tests for PP/TG drop off across context depth next. The smol-IQ3_KS was working in my `opencode` local testing and seems promising but probably a bit too large for enough context on 96GB VRAM hence the smaller IQ2_KS is also available at a cost to quality.&lt;/p&gt; &lt;p&gt;Fun stuff!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e7zeec20qbjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r40o83/ubergarmminimax25gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r40o83/ubergarmminimax25gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T20:47:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3hlfq</id>
    <title>UG student launches Dhi-5B (Trained from Scratch)</title>
    <updated>2026-02-13T06:13:29+00:00</updated>
    <author>
      <name>/u/gradNorm</name>
      <uri>https://old.reddit.com/user/gradNorm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3hlfq/ug_student_launches_dhi5b_trained_from_scratch/"&gt; &lt;img alt="UG student launches Dhi-5B (Trained from Scratch)" src="https://preview.redd.it/5tsgquvue7jg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fac59fbf4b00df28aabae2f993f4d65bb88169c" title="UG student launches Dhi-5B (Trained from Scratch)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hii everyone,&lt;/p&gt; &lt;p&gt;I present Dhi-5B: A 5 billion parameter Multimodal Language Model trained compute optimally with just ‚Çπ1.1 lakh ($1200).&lt;/p&gt; &lt;p&gt;I incorporate the latest architecture design and training methodologies in this. And I also use a custom built codebase for training these models.&lt;/p&gt; &lt;p&gt;I train the Dhi-5B in 5 stages:-&lt;/p&gt; &lt;p&gt;üìö Pre-Training: The most compute heavy phase, where the core is built. (Gives the Base varient.)&lt;/p&gt; &lt;p&gt;üìú Context-Length-Extension: The model learns to handle 16k context from the 4k learned during PT.&lt;/p&gt; &lt;p&gt;üìñ Mid-Training: Annealing on very high quality datasets.&lt;/p&gt; &lt;p&gt;üí¨ Supervised-Fine-Tuning: Model learns to handle conversations. (Gives the Instruct model.)&lt;/p&gt; &lt;p&gt;üëÄ Vision-Extension: The model learns to see. (Results in The Dhi-5B.)&lt;/p&gt; &lt;p&gt;I'll be dropping it in 3 phases:-&lt;/p&gt; &lt;p&gt;i. Dhi-5B-Base (available now)&lt;/p&gt; &lt;p&gt;ii. Dhi-5B-Instruct (coming soon)&lt;/p&gt; &lt;p&gt;iii. The Dhi-5B (coming soon)&lt;/p&gt; &lt;p&gt;Some details about the Dhi-5B-Base model:-&lt;/p&gt; &lt;p&gt;The base varient is of 4 billion parameters. It is trained on 40 billion natural language tokens mostly in english from FineWeb-Edu dataset.&lt;/p&gt; &lt;p&gt;I use the new Muon optimizer for optimising the Matrix Layers, and rest are optimized by AdamW.&lt;/p&gt; &lt;p&gt;The model has 32 layers, with 3072 width, SwiGLU MLPs, the full MHA attention with FlashAttention-3, 4096 context length, 64k vocab and 2 million batch size during training.&lt;/p&gt; &lt;p&gt;Attached are some evaluations of the base model, the compared models are about 10x more expensive than ours.&lt;/p&gt; &lt;p&gt;Thank you, everyone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gradNorm"&gt; /u/gradNorm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5tsgquvue7jg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3hlfq/ug_student_launches_dhi5b_trained_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3hlfq/ug_student_launches_dhi5b_trained_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T06:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3kzce</id>
    <title>MiniMax-M2.5 Checkpoints on huggingface will be in 8 hours</title>
    <updated>2026-02-13T09:41:01+00:00</updated>
    <author>
      <name>/u/Own_Forever_5997</name>
      <uri>https://old.reddit.com/user/Own_Forever_5997</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzce/minimaxm25_checkpoints_on_huggingface_will_be_in/"&gt; &lt;img alt="MiniMax-M2.5 Checkpoints on huggingface will be in 8 hours" src="https://preview.redd.it/p94fz9gsf8jg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=920f76b1a80dd8b1b58e34745f143966274a40a4" title="MiniMax-M2.5 Checkpoints on huggingface will be in 8 hours" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own_Forever_5997"&gt; /u/Own_Forever_5997 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p94fz9gsf8jg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzce/minimaxm25_checkpoints_on_huggingface_will_be_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzce/minimaxm25_checkpoints_on_huggingface_will_be_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T09:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3yahe</id>
    <title>LLaDA2.1 (100B/16B) released ‚Äî now with token editing for massive speed gains</title>
    <updated>2026-02-13T19:16:39+00:00</updated>
    <author>
      <name>/u/FeelingWatercress871</name>
      <uri>https://old.reddit.com/user/FeelingWatercress871</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLaDA2.1 builds on LLaDA2.0 by introducing Token-to-Token (T2T) editing alongside the standard Mask-to-Token decoding. Instead of locking in tokens once generated, the model can now retroactively correct errors during inference ‚Äî enabling much more aggressive parallel drafting.&lt;/p&gt; &lt;p&gt;Two decoding modes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;S Mode (Speedy): Aggressively low masking threshold + T2T correction. On coding tasks, LLaDA2.1-flash (100B) hits 892 TPS on HumanEval+, 801 TPS on BigCodeBench, 663 TPS on LiveCodeBench.&lt;/li&gt; &lt;li&gt;Q Mode (Quality): Conservative thresholds for best benchmark scores ‚Äî surpasses LLaDA2.0 on both Mini and Flash.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;First large-scale RL framework for diffusion LLMs (EBPO), improving reasoning and instruction following&lt;/li&gt; &lt;li&gt;Multi-Block Editing (MBE): revisit and revise previously generated blocks, consistent gains on reasoning/coding at modest speed cost&lt;/li&gt; &lt;li&gt;LLaDA2.1-mini (16B) peaks at ~1587 TPS on HumanEval+&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/collections/inclusionAI/llada21"&gt;https://huggingface.co/collections/inclusionAI/llada21&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/inclusionAI/LLaDA2.X"&gt;https://github.com/inclusionAI/LLaDA2.X&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tech Report: &lt;a href="https://huggingface.co/papers/2602.08676"&gt;https://huggingface.co/papers/2602.08676&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeelingWatercress871"&gt; /u/FeelingWatercress871 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yahe/llada21_100b16b_released_now_with_token_editing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yahe/llada21_100b16b_released_now_with_token_editing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yahe/llada21_100b16b_released_now_with_token_editing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T19:16:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3o6je</id>
    <title>New DeepSeek update: "DeepSeek Web / APP is currently testing a new long-context model architecture, supporting a 1M context window."</title>
    <updated>2026-02-13T12:43:50+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3o6je/new_deepseek_update_deepseek_web_app_is_currently/"&gt; &lt;img alt="New DeepSeek update: &amp;quot;DeepSeek Web / APP is currently testing a new long-context model architecture, supporting a 1M context window.&amp;quot;" src="https://preview.redd.it/dg94ujw1c9jg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55de58cf8a3e4a397d81184a2473b94f7a31aa33" title="New DeepSeek update: &amp;quot;DeepSeek Web / APP is currently testing a new long-context model architecture, supporting a 1M context window.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From AiBattle on ùïè: &lt;a href="https://x.com/AiBattle_/status/2022280288643039235"&gt;https://x.com/AiBattle_/status/2022280288643039235&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dg94ujw1c9jg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3o6je/new_deepseek_update_deepseek_web_app_is_currently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3o6je/new_deepseek_update_deepseek_web_app_is_currently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T12:43:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3yuyd</id>
    <title>has it begun?</title>
    <updated>2026-02-13T19:38:01+00:00</updated>
    <author>
      <name>/u/Acceptable_Home_</name>
      <uri>https://old.reddit.com/user/Acceptable_Home_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yuyd/has_it_begun/"&gt; &lt;img alt="has it begun?" src="https://preview.redd.it/ei9lt0u4ebjg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=36034757efbb832ba75f43ed04c4dc8c7bb34675" title="has it begun?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.bloomberg.com/news/articles/2026-02-13/us-to-put-alibaba-on-list-for-aiding-china-s-military-reuters"&gt;https://www.bloomberg.com/news/articles/2026-02-13/us-to-put-alibaba-on-list-for-aiding-china-s-military-reuters&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They were about to present the name of alibaba and Baidu as a potential threat or issue for helping chinese military in the Pentagon, but ultimately took their names off the list&lt;/p&gt; &lt;p&gt;Would love to hear what y'all think about this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable_Home_"&gt; /u/Acceptable_Home_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ei9lt0u4ebjg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yuyd/has_it_begun/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yuyd/has_it_begun/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T19:38:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3uixu</id>
    <title>GPT-OSS (20B) running 100% locally in your browser on WebGPU</title>
    <updated>2026-02-13T16:56:53+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3uixu/gptoss_20b_running_100_locally_in_your_browser_on/"&gt; &lt;img alt="GPT-OSS (20B) running 100% locally in your browser on WebGPU" src="https://external-preview.redd.it/azltbmk2OWprYWpnMcJUN0NJi-FsRvjcOQ-2jdC_J8rSz1PUOqY6x-ztdpX7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c42411a8a77fe60dfe81ecb5c06b854e8c0ac88" title="GPT-OSS (20B) running 100% locally in your browser on WebGPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, I released a demo showcasing GPT-OSS (20B) running 100% locally in-browser on WebGPU, powered by Transformers.js v4 (preview) and ONNX Runtime Web. Hope you like it! &lt;/p&gt; &lt;p&gt;Links:&lt;br /&gt; - Demo (+ source code): &lt;a href="https://huggingface.co/spaces/webml-community/GPT-OSS-WebGPU"&gt;https://huggingface.co/spaces/webml-community/GPT-OSS-WebGPU&lt;/a&gt;&lt;br /&gt; - Optimized ONNX model: &lt;a href="https://huggingface.co/onnx-community/gpt-oss-20b-ONNX"&gt;https://huggingface.co/onnx-community/gpt-oss-20b-ONNX&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ioqb4q8jkajg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3uixu/gptoss_20b_running_100_locally_in_your_browser_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3uixu/gptoss_20b_running_100_locally_in_your_browser_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3zuuf</id>
    <title>GPT-OSS 120b Uncensored Aggressive Release (MXFP4 GGUF)</title>
    <updated>2026-02-13T20:15:33+00:00</updated>
    <author>
      <name>/u/hauhau901</name>
      <uri>https://old.reddit.com/user/hauhau901</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, made an uncensored version of GPT-OSS 120B.&lt;/p&gt; &lt;p&gt;Quick specs: 117B total params, ~5.1B active (MoE with 128 experts, top-4 routing), 128K context. MXFP4 is the model's native precision - this isn't a quantization, it's how it was trained. No overall quality loss, though you can see CoT behave differently at times.&lt;/p&gt; &lt;p&gt;This is the aggressive variant - &lt;strong&gt;observed 0 refusals to any query during testing.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Completely uncensored while keeping full model capabilities intact.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://huggingface.co/HauhauCS/GPTOSS-120B-Uncensored-HauhauCS-Aggressive"&gt;https://huggingface.co/HauhauCS/GPTOSS-120B-Uncensored-HauhauCS-Aggressive&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sampling settings:&lt;/p&gt; &lt;p&gt;- --temp 1.0 --top-k 40&lt;/p&gt; &lt;p&gt;- Disable everything else (top_p, min_p, repeat penalty, etc.) - some clients turn&lt;/p&gt; &lt;p&gt;these on by default&lt;/p&gt; &lt;p&gt;- llama.cpp users: --jinja is required for the Harmony response format or the model won't work right&lt;/p&gt; &lt;p&gt;- Example: llama-server -m model.gguf --jinja -fa -b 2048 -ub 2048&lt;/p&gt; &lt;p&gt;Single 61GB file. Fits on one H100. For lower VRAM, use --n-cpu-moe N in llama.cpp to offload MoE layers to CPU.&lt;/p&gt; &lt;p&gt;Works with llama.cpp, LM Studio, Ollama, etc.&lt;/p&gt; &lt;p&gt;If you want smaller models, I also have GPT-OSS 20B, GLM 4.7 Flash and Qwen3 8b VL uncensored:&lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/HauhauCS/models/"&gt;https://huggingface.co/HauhauCS/models/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As with all my releases, the goal is effectively lossless uncensoring - no dataset changes and no capability loss.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hauhau901"&gt; /u/hauhau901 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3zuuf/gptoss_120b_uncensored_aggressive_release_mxfp4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3zuuf/gptoss_120b_uncensored_aggressive_release_mxfp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3zuuf/gptoss_120b_uncensored_aggressive_release_mxfp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T20:15:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3t8ro</id>
    <title>Nvidia‚Äôs new technique cuts LLM reasoning costs by 8x without losing accuracy</title>
    <updated>2026-02-13T16:09:31+00:00</updated>
    <author>
      <name>/u/Mission-Street4214</name>
      <uri>https://old.reddit.com/user/Mission-Street4214</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nvidia developed a new technique called Dynamic Memory Sparsification (DMS) that vastly improves how LLMs manage their KV cache during inference. It accomplishes this by retrofitting existing models so that the attention layers output a &lt;strong&gt;learned keep or evict&lt;/strong&gt; signal for each token in the KV cache. &lt;/p&gt; &lt;p&gt;In addition, they've added a &amp;quot;delayed eviction&amp;quot; that marks a token as low-importance, but doesn't delete it immediately. Instead, it remains accessible for a short time and allows the model to extract any useful information into newer tokens before it's discarded.&lt;/p&gt; &lt;p&gt;These advancements reduce KV memory usage by up to &lt;strong&gt;8x&lt;/strong&gt;, allowing the model to think longer, run faster and handle more concurrent requests.&lt;/p&gt; &lt;p&gt;Definitely recommend reading the full article. Looking forward to seeing this on self hosted hardware.&lt;/p&gt; &lt;p&gt;&lt;a href="https://venturebeat.com/orchestration/nvidias-new-technique-cuts-llm-reasoning-costs-by-8x-without-losing-accuracy"&gt;VentureBeat Article&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mission-Street4214"&gt; /u/Mission-Street4214 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t8ro/nvidias_new_technique_cuts_llm_reasoning_costs_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t8ro/nvidias_new_technique_cuts_llm_reasoning_costs_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t8ro/nvidias_new_technique_cuts_llm_reasoning_costs_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:09:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3weq3</id>
    <title>SWE-rebench Jan 2026: GLM-5, MiniMax M2.5, Qwen3-Coder-Next, Opus 4.6, Codex Performance</title>
    <updated>2026-02-13T18:06:40+00:00</updated>
    <author>
      <name>/u/CuriousPlatypus1881</name>
      <uri>https://old.reddit.com/user/CuriousPlatypus1881</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I‚Äôm Anton from Nebius.&lt;/p&gt; &lt;p&gt;We‚Äôve updated the &lt;strong&gt;SWE-rebench leaderboard&lt;/strong&gt; with our &lt;strong&gt;January runs&lt;/strong&gt; on &lt;strong&gt;48 fresh GitHub PR tasks&lt;/strong&gt; (PRs created in the previous month only). The setup is standard SWE-bench: models read real PR issues, edit code, run tests, and must make the full suite pass.&lt;/p&gt; &lt;p&gt;Key observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Claude Code (Opus 4.6)&lt;/strong&gt; leads this snapshot at &lt;strong&gt;52.9% resolved rate&lt;/strong&gt; and also achieves the highest &lt;strong&gt;pass@5 (70.8%)&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Claude Opus 4.6&lt;/strong&gt; and &lt;strong&gt;gpt-5.2-xhigh&lt;/strong&gt; follow very closely (51.7%), making the top tier extremely tight.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;gpt-5.2-medium (51.0%)&lt;/strong&gt; performs surprisingly close to the frontier configuration.&lt;/li&gt; &lt;li&gt;Among open models, &lt;strong&gt;Kimi K2 Thinking (43.8%)&lt;/strong&gt;, &lt;strong&gt;GLM-5 (42.1%)&lt;/strong&gt;, and &lt;strong&gt;Qwen3-Coder-Next (40.0%)&lt;/strong&gt; lead the pack.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MiniMax M2.5 (39.6%)&lt;/strong&gt; continues to show strong performance while remaining one of the cheapest options.&lt;/li&gt; &lt;li&gt;Clear gap between Kimi variants: &lt;strong&gt;K2 Thinking (43.8%)&lt;/strong&gt; vs &lt;strong&gt;K2.5 (37.9%)&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Newer smaller/flash variants (e.g., GLM-4.7 Flash, gpt-5-mini-medium) trade performance for efficiency, landing in the 25‚Äì31% range.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to your thoughts and feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CuriousPlatypus1881"&gt; /u/CuriousPlatypus1881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/?insight=jan_2026"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3weq3/swerebench_jan_2026_glm5_minimax_m25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3weq3/swerebench_jan_2026_glm5_minimax_m25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T18:06:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3pxy7</id>
    <title>MiniMaxAI/MiniMax-M2.5 ¬∑ Hugging Face</title>
    <updated>2026-02-13T14:01:52+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/"&gt; &lt;img alt="MiniMaxAI/MiniMax-M2.5 ¬∑ Hugging Face" src="https://external-preview.redd.it/U31ABjps-QUJnQnBzpD57R8aGaCakUKOyzXjWrvNYSg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de0bab4be78008336f973196f0ed98e2bbe49764" title="MiniMaxAI/MiniMax-M2.5 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can monitor quants begin to appear with this search: &lt;a href="https://huggingface.co/models?sort=modified&amp;amp;search=minimax+m2.5"&gt;https://huggingface.co/models?sort=modified&amp;amp;search=minimax+m2.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T14:01:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3t775</id>
    <title>AMA with MiniMax ‚Äî Ask Us Anything!</title>
    <updated>2026-02-13T16:07:54+00:00</updated>
    <author>
      <name>/u/HardToVary</name>
      <uri>https://old.reddit.com/user/HardToVary</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt; &lt;img alt="AMA with MiniMax ‚Äî Ask Us Anything!" src="https://preview.redd.it/5z2li1ntcajg1.jpg?width=140&amp;amp;height=59&amp;amp;auto=webp&amp;amp;s=ce3340cd37b7e9408878509be00dd7b871efebde" title="AMA with MiniMax ‚Äî Ask Us Anything!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;We're &lt;strong&gt;MiniMax&lt;/strong&gt;, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;.5&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining the channel today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; ‚Äî Founder of MiniMax&lt;/li&gt; &lt;li&gt;&lt;a href="/u/Wise_Evidence9973"&gt;u/Wise_Evidence9973&lt;/a&gt; ‚Äî Head of LLM Research&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ryan85127704"&gt;u/ryan85127704&lt;/a&gt; ‚Äî Head of Engineering&lt;/li&gt; &lt;li&gt;&lt;a href="/u/HardToVary"&gt;u/HardToVary&lt;/a&gt; ‚Äî LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c"&gt;https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S. We'll continue monitoring and responding to questions for 48 hours after the end of the AMA.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HardToVary"&gt; /u/HardToVary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:07:54+00:00</published>
  </entry>
</feed>
