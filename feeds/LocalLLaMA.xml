<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-08T21:38:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q7nifj</id>
    <title>Nothing crashed. Puppeteer MCP still broke my agent.</title>
    <updated>2026-01-08T20:47:15+00:00</updated>
    <author>
      <name>/u/hack_the_developer</name>
      <uri>https://old.reddit.com/user/hack_the_developer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7nifj/nothing_crashed_puppeteer_mcp_still_broke_my_agent/"&gt; &lt;img alt="Nothing crashed. Puppeteer MCP still broke my agent." src="https://external-preview.redd.it/N2UydWhqZGV0NmNnMbZSoI9x_23KPtN2XOalGkhNuH3zGXLop-0uhZ_7QxRE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c52e78d4d08f88ef1bae23eca635ce2d3a7dc51" title="Nothing crashed. Puppeteer MCP still broke my agent." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was testing the Puppeteer MCP server, and everything looked fine at first. It connected, tools showed up, and no errors. But once the agent started running, things slowly went sideways. Clicks &amp;quot;worked&amp;quot;, but nothing downstream knew what changed, so steps kept getting retried.&lt;/p&gt; &lt;p&gt;At first, it felt like LLM weirdness, so I looked at the MCP server itself. Turns out most Puppeteer tools don‚Äôt declare what they return, and some rely on poorly described parameters or implicit context. Nothing breaks, but agents quietly get confused.&lt;/p&gt; &lt;p&gt;I recorded a short video showing the analysis and why runtime testing misses this. I used a tool I‚Äôm building called Syrin to run the check.&lt;/p&gt; &lt;p&gt;Curious how others validate MCP servers before runtime.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hack_the_developer"&gt; /u/hack_the_developer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/q19le8det6cg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7nifj/nothing_crashed_puppeteer_mcp_still_broke_my_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7nifj/nothing_crashed_puppeteer_mcp_still_broke_my_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T20:47:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7nutq</id>
    <title>Creative Writing - anything under 150GB equal or close to Sonnet 3.7?</title>
    <updated>2026-01-08T21:00:21+00:00</updated>
    <author>
      <name>/u/elsung</name>
      <uri>https://old.reddit.com/user/elsung</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i know this question has been asked a few times before but its been some time since i've seen discussion around this and there a quite a few new kids (models) on the block. Wondering if anyone has had good experiences on models that are equal to or better than Sonnet 3.7 in terms of creative writing like fiction stories, poems, or lyrics?&lt;/p&gt; &lt;p&gt;It feels especially pertinent since anthropic deprecated the model and i much prefer its writing style and adherance over even all of the new claude variants. Im finding my current alternative is to leverage openrounter to get to sonnet 3.7 but im not sure how long that would last. would love to find a suitable alternative locally if possible.&lt;/p&gt; &lt;p&gt;im am running on a mac studio M2 Ultra with 192GB so i can run decently sized models, but not quite to the point of being able to run something like Deepseek 3.1 or KimiK2. For context i've tried a couple options but heres what ive found so far:&lt;/p&gt; &lt;p&gt;- GLM 4.7 - very very good , closest to sonnet in terms of creative writing style, prompt adherence and consistency. Almost as good conceptually but falls short of the writing style that sonnet 3.7 seems to have where theres like a writers polish, and GLM still feels quite raw.&lt;br /&gt; - GLM 4.5 air / Intellect 3 - very close to GLM 4.7. not as strong conceptually but usable, still makes sense for the most part and is somewhat consistent. voicing is a bit flat comparatively&lt;br /&gt; - Kimi K2 thinking &amp;amp; Nothinking - suprisingly very dumb when it comes to creative for me. ideas were sorta non-sensical and also cliche / cheesy. did not follow instructions all that well to get the mood/feel that i wanted in terms of writing style.&lt;br /&gt; - Minimax 2.1 - similar to kimi k2. felt overly forcibly structured and generic and sorta dumb/falt.&lt;br /&gt; - Gemma 3 - decent, but writing feels amateurish compared to GLM 4.5. writing style also feels much to be desired. maybe there are good fine tunes to try for this?&lt;/p&gt; &lt;p&gt;would love to hear what people have experienced / recommendations to try! (so like models, techniquess, prompts, inference settings etc~)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elsung"&gt; /u/elsung &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7nutq/creative_writing_anything_under_150gb_equal_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7nutq/creative_writing_anything_under_150gb_equal_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7nutq/creative_writing_anything_under_150gb_equal_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T21:00:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7lls5</id>
    <title>Help Needed - Need to setup local AI server, with local file access.</title>
    <updated>2026-01-08T19:37:11+00:00</updated>
    <author>
      <name>/u/Puzzleheaded_Cake183</name>
      <uri>https://old.reddit.com/user/Puzzleheaded_Cake183</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;After many days of research, i have come to the conclusion that i need someone smarter than me to help me out in my project.&lt;/p&gt; &lt;p&gt;Available hardware:&lt;/p&gt; &lt;p&gt;- Lenovo SR655 server with AMD Epyc 7313 16c/32t cpu (willing to upgrade to 7703 64c/128t)&lt;/p&gt; &lt;p&gt;- 64gb Ram ddr4 3200mhz ecc 2rx4 (2x32gb sticks. sadly i dont have more sticks, although the epyc has 8 memory channels so i am sacrificing bandwidth).&lt;/p&gt; &lt;p&gt;- 120TB zfs with parity + mirror on rust hdd (dedicated server with truenas, 64gb ddr4, and 2288g xeon cpu.) over 10gb fiber.&lt;/p&gt; &lt;p&gt;- 4tb in raid 0 nvme drives (2x2tb nvme pcie 4x4)&lt;/p&gt; &lt;p&gt;- Running Proxmox VE 9.xx.&lt;/p&gt; &lt;p&gt;- EFI q35 virtual machine with 60gb ram passed to it, and all cpu cores (set as host for best performance and all features). Running Ubuntu server 24.04. Latest docker setup. &lt;/p&gt; &lt;p&gt;- The ubuntu vm has access to storage over smb share (hosted in a different machine over 10gb fiber). 2tb given as local hdd to the ubuntu (nvme storage) for models.&lt;/p&gt; &lt;p&gt;- I am willing to purchase a GPU for the server. It can handle up to 3 GPUs. I dont have much budget for this so i was looking at RTX 2000E Ada, or v100? I would need some help with this as well. Given that the server requires server size GPUs and i can not just buy off the shelf 3060s or such. I would need help figuring out what GPUs are best for this application.&lt;/p&gt; &lt;p&gt;- My old workstation with the following specs&lt;/p&gt; &lt;p&gt;- Gigabyte Aurus master z790, 13900k cpu, 32gb ddr5 (dont remember the speed), 2 x 2tb nvme 4x4 in raid 0, nvidia rtx4090. Cpu has been delided and its watercooled with liquid metal. so is the gpu. custom loop with 2 360mm radiators in the loop. 10gb net.&lt;/p&gt; &lt;p&gt;- i am willing to use my old workstation as needed to make this project work.&lt;/p&gt; &lt;p&gt;- My very old workstation&lt;/p&gt; &lt;p&gt;- this is a am5 system with 5900x cpu, 3090rtx, 32gb ddr4 at 3200. single 1tb nvme 3x4. cpu and gpu both water cooled with custom loops.&lt;/p&gt; &lt;p&gt;- i am willing to use this as needed as well. its collecting dust anyway.&lt;/p&gt; &lt;p&gt;Goal:&lt;/p&gt; &lt;p&gt;I need to be able to provide the following services to one of the vms im running. Nextcloud AIO.&lt;/p&gt; &lt;p&gt;- Whisper for voice to text services. &lt;/p&gt; &lt;p&gt;- tts for text to sound services.&lt;/p&gt; &lt;p&gt;- local ai with access to SMB share files with context etc etc. (this is the only thing im really lost at)&lt;/p&gt; &lt;p&gt;- Some way to get the OpenAI API (that nextcloud uses) to be able to call some instance of ConfyUI Warkflow for image generation. I guess that would be called a api gateway. &lt;/p&gt; &lt;p&gt;- Setting up agents for specific tasks. I am lost on this one as well.&lt;/p&gt; &lt;p&gt;- Local AI running backend for the AI chat on Nextcloud. This i have figured out with LocalAI hosting the models i like and i am able to use the built in OpenAI API in nextcloud to connect to LocalAI as the service provider. Perhaps there is a better way?&lt;/p&gt; &lt;p&gt;If you can help, or have done a similar setup prior and have some pointers, Please Please Please DM me. I dont want to fill up the entire post random info and bother people. I would like to directly communicate so i can gain some knowledge and perhaps get this done.&lt;/p&gt; &lt;p&gt;I would like to thank all of you in advance. Thank you all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Puzzleheaded_Cake183"&gt; /u/Puzzleheaded_Cake183 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7lls5/help_needed_need_to_setup_local_ai_server_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7lls5/help_needed_need_to_setup_local_ai_server_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7lls5/help_needed_need_to_setup_local_ai_server_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T19:37:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7kbbz</id>
    <title>Kimi K2 Thinking, Q2, 3 nodes Strix Halo, llama.cpp. Has anyone tried a multiple-node setup using vLLM yet? And how it compares to Llama.cpp. Thank you.</title>
    <updated>2026-01-08T18:50:56+00:00</updated>
    <author>
      <name>/u/el3mancee</name>
      <uri>https://old.reddit.com/user/el3mancee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7kbbz/kimi_k2_thinking_q2_3_nodes_strix_halo_llamacpp/"&gt; &lt;img alt="Kimi K2 Thinking, Q2, 3 nodes Strix Halo, llama.cpp. Has anyone tried a multiple-node setup using vLLM yet? And how it compares to Llama.cpp. Thank you." src="https://preview.redd.it/792wso0696cg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d942cd6a66303123de4eba5fb1fd9f269d83aaa" title="Kimi K2 Thinking, Q2, 3 nodes Strix Halo, llama.cpp. Has anyone tried a multiple-node setup using vLLM yet? And how it compares to Llama.cpp. Thank you." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Managed to run Kimi K2 Thinking, q2 on a 3-node Strix Halo setup. Got around 9t/s. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/el3mancee"&gt; /u/el3mancee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/792wso0696cg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7kbbz/kimi_k2_thinking_q2_3_nodes_strix_halo_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7kbbz/kimi_k2_thinking_q2_3_nodes_strix_halo_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T18:50:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6sp4b</id>
    <title>Sopro: A 169M parameter real-time TTS model with zero-shot voice cloning</title>
    <updated>2026-01-07T21:46:19+00:00</updated>
    <author>
      <name>/u/SammyDaBeast</name>
      <uri>https://old.reddit.com/user/SammyDaBeast</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a fun side project, I trained a small text-to-speech model that I call Sopro. Some features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;169M parameters&lt;/li&gt; &lt;li&gt;Streaming support&lt;/li&gt; &lt;li&gt;Zero-shot voice cloning&lt;/li&gt; &lt;li&gt;0.25 RTF on CPU, meaning it generates 30 seconds of audio in 7.5 seconds&lt;/li&gt; &lt;li&gt;Requires 3-12 seconds of reference audio for voice cloning&lt;/li&gt; &lt;li&gt;Apache 2.0 license&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Yes, I know, another English-only TTS model. This is mainly due to data availability and a limited compute budget. The model was trained on a single L40S GPU.&lt;/p&gt; &lt;p&gt;It‚Äôs not SOTA in most cases, can be a bit unstable, and sometimes fails to capture voice likeness. Nonetheless, I hope you like it!&lt;/p&gt; &lt;p&gt;GitHub repo: &lt;a href="https://github.com/samuel-vitorino/sopro"&gt;https://github.com/samuel-vitorino/sopro&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SammyDaBeast"&gt; /u/SammyDaBeast &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6sp4b/sopro_a_169m_parameter_realtime_tts_model_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6sp4b/sopro_a_169m_parameter_realtime_tts_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6sp4b/sopro_a_169m_parameter_realtime_tts_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T21:46:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7h6hz</id>
    <title>I fine-tuned a 7B model for reasoning on free Colab with GRPO + TRL</title>
    <updated>2026-01-08T17:00:07+00:00</updated>
    <author>
      <name>/u/External-Rub5414</name>
      <uri>https://old.reddit.com/user/External-Rub5414</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just created a &lt;strong&gt;Colab notebook&lt;/strong&gt; that lets you &lt;strong&gt;add reasoning to 7B+ models&lt;/strong&gt; on free Colab(T4 GPU)!&lt;/p&gt; &lt;p&gt;Thanks to &lt;strong&gt;TRL's full set of memory optimizations&lt;/strong&gt;, this setup reduces memory usage by &lt;strong&gt;~7√ó&lt;/strong&gt; compared to naive FP16, making it possible to fine-tune large models in a free Colab session.&lt;/p&gt; &lt;p&gt;Notebook:&lt;br /&gt; üëâ &lt;a href="https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/grpo_trl_lora_qlora.ipynb"&gt;GRPO + TRL Colab notebook&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Check out other notebooks I worked on:&lt;br /&gt; üëâ &lt;a href="https://github.com/huggingface/trl/tree/main/examples/notebooks"&gt;TRL examples&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy hacking! üòÑ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External-Rub5414"&gt; /u/External-Rub5414 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7h6hz/i_finetuned_a_7b_model_for_reasoning_on_free/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7h6hz/i_finetuned_a_7b_model_for_reasoning_on_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7h6hz/i_finetuned_a_7b_model_for_reasoning_on_free/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T17:00:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7nhp0</id>
    <title>Using Llama-3.1-8B‚Äôs perplexity scores to predict suicide risk (preprint + code)</title>
    <updated>2026-01-08T20:46:30+00:00</updated>
    <author>
      <name>/u/AI_Psych_Research</name>
      <uri>https://old.reddit.com/user/AI_Psych_Research</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just uploaded a preprint where we used local Llama 3.1 to detect suicide risk 18 months in advance. We needed access to raw token probabilities to measure perplexity (the model's &amp;quot;surprise&amp;quot;), so open weights were mandatory.&lt;/p&gt; &lt;p&gt;The pipeline was pretty simple. We got recordings of people talking about their expected future self, used Claude Sonnet to generate two &amp;quot;future narratives&amp;quot; for each person (one where they have a crisis, one where they don't). Then we fed those into Llama-3.1-8B to score which narrative was more linguistically plausible based on the patient's interview transcript.&lt;/p&gt; &lt;p&gt;The results were that if the suicidal narrative was more probable (lower perplexity), that person was significantly more likely to report suicidal ideation 18 months later. It actually caught 75% of the high-risk people that standard suicide medical questionnaires missed.&lt;/p&gt; &lt;p&gt;Paper and Code: &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fosf.io%2Fpreprints%2Fpsyarxiv%2Ffhzum_v1"&gt;https://osf.io/preprints/psyarxiv/fhzum_v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm planning on exploring other models (larger, newer, thinking models, etc). I'm not a comp sci person, so I am sure the code and LLM tech can be improved. If anyone looks this over and has ideas on how to optimize the pipeline or which open models might be better at &amp;quot;reasoning&amp;quot; about psychological states, I would love to hear them.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; We used Llama-3.1-8B to measure the &amp;quot;perplexity&amp;quot; of future narratives. It successfully predicted suicidal ideation 18 months out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AI_Psych_Research"&gt; /u/AI_Psych_Research &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7nhp0/using_llama318bs_perplexity_scores_to_predict/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7nhp0/using_llama318bs_perplexity_scores_to_predict/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7nhp0/using_llama318bs_perplexity_scores_to_predict/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T20:46:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7ivay</id>
    <title>Im planning on buying a single gpu to run llms and contemplating between two gpus</title>
    <updated>2026-01-08T18:00:15+00:00</updated>
    <author>
      <name>/u/WhiteSupremacistMonk</name>
      <uri>https://old.reddit.com/user/WhiteSupremacistMonk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;im planning on either getting an nvidia v100 (32gb version) or a mi50 (32gb version) &lt;/p&gt; &lt;p&gt;is the extra money to get the v100 worth it ?, or should i just buy the cheaper mi50 for around 200-300 usd &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WhiteSupremacistMonk"&gt; /u/WhiteSupremacistMonk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7ivay/im_planning_on_buying_a_single_gpu_to_run_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7ivay/im_planning_on_buying_a_single_gpu_to_run_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7ivay/im_planning_on_buying_a_single_gpu_to_run_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T18:00:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6n5vl</id>
    <title>16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)</title>
    <updated>2026-01-07T18:22:05+00:00</updated>
    <author>
      <name>/u/ai-infos</name>
      <uri>https://old.reddit.com/user/ai-infos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/"&gt; &lt;img alt="16x AMD MI50 32GB at 10 t/s (tg) &amp;amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)" src="https://preview.redd.it/lor8ccu2xybg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=857ab421f987d81024114a5c2bc2cf35859061b4" title="16x AMD MI50 32GB at 10 t/s (tg) &amp;amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek 3.2 AWQ 4bit @ 10 tok/s (output) // 2000 tok/s (input of 23k tok)&lt;/p&gt; &lt;p&gt;on vllm-gfx906-deepseek with 69000 context length&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Power draw&lt;/strong&gt;: 550W (idle) / 2400W (peak inference)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: run Deepseek V3.2 AWQ 4-bit on most cost effective hardware like 16*MI50 at decent speed (token generation &amp;amp; prompt processing)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Coming next&lt;/strong&gt;: open source a future test setup of 32 AMD MI50 32GB for Kimi K2 Thinking&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Credits&lt;/strong&gt;: BIG thanks to the Global Open source Community!&lt;/p&gt; &lt;p&gt;All setup details here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32"&gt;https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Feel free to ask any questions and/or share any comments.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;ps: it might be a good alternative to CPU hardwares as RAM price increases and the prompt processing speed will be much better with 16 TB/s bandwidth + tensor parallelism! &lt;/p&gt; &lt;p&gt;ps2: i'm just a random guy with average software dev background using LLMs to make it run. Goal is to be ready for LOCAL AGI without spending +300k$... &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai-infos"&gt; /u/ai-infos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lor8ccu2xybg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T18:22:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1q79n6x</id>
    <title>I was trying out an activation-steering method for Qwen3-Next, but I accidentally corrupted the model weights. Somehow, the model still had enough ‚Äúconscience‚Äù to realize something was wrong and freak out.</title>
    <updated>2026-01-08T11:42:44+00:00</updated>
    <author>
      <name>/u/ikergarcia1996</name>
      <uri>https://old.reddit.com/user/ikergarcia1996</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q79n6x/i_was_trying_out_an_activationsteering_method_for/"&gt; &lt;img alt="I was trying out an activation-steering method for Qwen3-Next, but I accidentally corrupted the model weights. Somehow, the model still had enough ‚Äúconscience‚Äù to realize something was wrong and freak out." src="https://b.thumbs.redditmedia.com/Pou7U9wzorYlQFrD5KCYo6fxr8k5_S3OwV7_B5qbW8s.jpg" title="I was trying out an activation-steering method for Qwen3-Next, but I accidentally corrupted the model weights. Somehow, the model still had enough ‚Äúconscience‚Äù to realize something was wrong and freak out." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I now feel bad seeing the model realize it was losing its mind and struggling with it, it feels like I was torturing it :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ikergarcia1996"&gt; /u/ikergarcia1996 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q79n6x"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q79n6x/i_was_trying_out_an_activationsteering_method_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q79n6x/i_was_trying_out_an_activationsteering_method_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T11:42:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7fejp</id>
    <title>Are MiniMax M2.1 quants usable for coding?</title>
    <updated>2026-01-08T15:54:30+00:00</updated>
    <author>
      <name>/u/val_in_tech</name>
      <uri>https://old.reddit.com/user/val_in_tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please share your real life experience. Especially interesting to hear from someone who had a chance to compare higher quants with lower ones.&lt;/p&gt; &lt;p&gt;Also, speaking of the model itself - do you feel it's worth the buzz around it?&lt;/p&gt; &lt;p&gt;Use case - coding via opencode or claude proxy.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/val_in_tech"&gt; /u/val_in_tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7fejp/are_minimax_m21_quants_usable_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7fejp/are_minimax_m21_quants_usable_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7fejp/are_minimax_m21_quants_usable_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T15:54:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7m2eh</id>
    <title>Built a blind benchmark for coding models - which local models should I add?</title>
    <updated>2026-01-08T19:54:08+00:00</updated>
    <author>
      <name>/u/Equivalent-Yak2407</name>
      <uri>https://old.reddit.com/user/Equivalent-Yak2407</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7m2eh/built_a_blind_benchmark_for_coding_models_which/"&gt; &lt;img alt="Built a blind benchmark for coding models - which local models should I add?" src="https://preview.redd.it/6ocf1gbxj6cg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4dc23f03c864b314bc32210b4178f4a8e53727f4" title="Built a blind benchmark for coding models - which local models should I add?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;3 AI judges score each output blind. Early results from 10 coding tasks - Deepseek V3.2 at #9. GLM 4.7 at #6, beating Claude Opus 4.5.&lt;/p&gt; &lt;p&gt;Some open-source models are free to evaluate. Which local models should I evaluate and add to the leaderboard?&lt;/p&gt; &lt;p&gt;&lt;a href="http://codelens.ai/leaderboard"&gt;codelens.ai/leaderboard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Equivalent-Yak2407"&gt; /u/Equivalent-Yak2407 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6ocf1gbxj6cg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7m2eh/built_a_blind_benchmark_for_coding_models_which/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7m2eh/built_a_blind_benchmark_for_coding_models_which/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T19:54:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7hywi</id>
    <title>How do you manage quality when AI agents write code faster than humans can review it?</title>
    <updated>2026-01-08T17:28:30+00:00</updated>
    <author>
      <name>/u/lostsoul8282</name>
      <uri>https://old.reddit.com/user/lostsoul8282</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are shifting to an agentic workflow. My thesis is &amp;quot;Code at Inference Speed.&amp;quot; My CTO's counter-argument is that &lt;strong&gt;reviewing code is harder than writing it&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;His concern is simple: If AI increases code volume by 10x, human review becomes a fatal bottleneck. He predicts technical debt will explode because humans can‚Äôt mentally verify that much logic that quickly.&lt;/p&gt; &lt;p&gt;How do handle this? I know one option is to slow down releases but is there any other approaches people are taking.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lostsoul8282"&gt; /u/lostsoul8282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7hywi/how_do_you_manage_quality_when_ai_agents_write/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7hywi/how_do_you_manage_quality_when_ai_agents_write/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7hywi/how_do_you_manage_quality_when_ai_agents_write/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T17:28:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7nqxl</id>
    <title>llama.cpp has Out-of-bounds Write in llama-server</title>
    <updated>2026-01-08T20:56:15+00:00</updated>
    <author>
      <name>/u/radarsat1</name>
      <uri>https://old.reddit.com/user/radarsat1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Maybe good to know for some of you that might be running llama.cpp on a regular basis.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;llama.cpp is an inference of several LLM models in C/C++. In commits 55d4206c8 and prior, the n_discard parameter is parsed directly from JSON input in the llama.cpp server's completion endpoints without validation to ensure it's non-negative. When a negative value is supplied and the context fills up, llama_memory_seq_rm/add receives a reversed range and negative offset, causing out-of-bounds memory writes in the token evaluation loop. This deterministic memory corruption can crash the process or enable remote code execution (RCE). There is no fix at the time of publication.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Also reported &lt;a href="https://security-tracker.debian.org/tracker/CVE-2026-21869"&gt;for Debian&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/radarsat1"&gt; /u/radarsat1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cve.org/CVERecord?id=CVE-2026-21869"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7nqxl/llamacpp_has_outofbounds_write_in_llamaserver/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7nqxl/llamacpp_has_outofbounds_write_in_llamaserver/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T20:56:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7k754</id>
    <title>toy model</title>
    <updated>2026-01-08T18:46:43+00:00</updated>
    <author>
      <name>/u/Eduard_T</name>
      <uri>https://old.reddit.com/user/Eduard_T</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If anyone is interested in creating, training, and chatting with a toy model, I‚Äôve created &lt;a href="https://github.com/EduardTalianu/toygpt"&gt;https://github.com/EduardTalianu/toygpt&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;a model script to create a model&lt;/li&gt; &lt;li&gt;a training script to train it on a&lt;code&gt;.txt&lt;/code&gt; file&lt;/li&gt; &lt;li&gt;a chat script to interact with the trained model&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It‚Äôs a PyTorch research implementation of a Manifold-Constrained Hyper-Connection Transformer (mHC), combining Mixture-of-Experts efficiency, Sinkhorn-based routing, and architectural stability enhancements.&lt;/p&gt; &lt;p&gt;Slower per step than a vanilla Transformer ‚Äî but &lt;em&gt;much&lt;/em&gt; more sample-efficient. At &amp;lt;1 epoch it already learns grammar, structure, and style instead of collapsing into mush.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eduard_T"&gt; /u/Eduard_T &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7k754/toy_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7k754/toy_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7k754/toy_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T18:46:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7hikw</id>
    <title>Qwen3-4B-Instruct-2507 multilingual FT with upscaled Polish language</title>
    <updated>2026-01-08T17:12:08+00:00</updated>
    <author>
      <name>/u/Significant_Focus134</name>
      <uri>https://old.reddit.com/user/Significant_Focus134</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, &lt;/p&gt; &lt;p&gt;Just wanted to share a preview of my latest finetuned model based on Qwen3-4B-Instruct-2507.&lt;/p&gt; &lt;p&gt;Languages ratio:&lt;/p&gt; &lt;p&gt;Polish - high&lt;br /&gt; English - medium&lt;br /&gt; Chinese - medium&lt;br /&gt; Czech - medium/low&lt;br /&gt; Ukrainian - medium/low&lt;br /&gt; Russian - medium/low&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/piotr-ai/polanka_4b_v0.3_preview_260108_qwen3_gguf"&gt;https://huggingface.co/piotr-ai/polanka_4b_v0.3_preview_260108_qwen3_gguf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant_Focus134"&gt; /u/Significant_Focus134 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7hikw/qwen34binstruct2507_multilingual_ft_with_upscaled/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7hikw/qwen34binstruct2507_multilingual_ft_with_upscaled/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7hikw/qwen34binstruct2507_multilingual_ft_with_upscaled/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T17:12:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7c0pd</id>
    <title>AI21 releases Jamba2 3B and Jamba2 Mini, built for grounding and instruction following</title>
    <updated>2026-01-08T13:38:34+00:00</updated>
    <author>
      <name>/u/zennaxxarion</name>
      <uri>https://old.reddit.com/user/zennaxxarion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Disclaimer: I work for AI21, creator of the Jamba model family.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;We‚Äôre excited to announce the public release of Jamba2 3B and Jamba2 Mini.&lt;/p&gt; &lt;p&gt;The Jamba2 family aims to give enterprises cost-effective models that will integrate well into production agent stacks.&lt;/p&gt; &lt;p&gt;These models are designed for reliable instruction following and grounded outputs, working well over long documents and avoiding drifting once context becomes large.&lt;/p&gt; &lt;p&gt;They perform best for precise question answering over internal policies, technical manuals and knowledge bases, without the overhead of thinking tokens which can become costly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key performance data&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Jamba2 3B and Jamba2 Mini outperform peers due to their hybrid SSM-Transformer architecture and KV cache innovations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Outpaces Ministral3 14B and Qwen3 30B A3B across FACTS, IFBench and IFEval. &lt;/li&gt; &lt;li&gt;Beats Ministral3 3B and Qwen3 4B on IFEval and IFBench, tying with Qwen3 4B as category leader on FACTS.&lt;/li&gt; &lt;li&gt;At context lengths of 100K, Jamba2 Mini delivers 2.7X greater throughput than Ministral3 14B and 1.4X greater throughout than Qwen3 30B A3B.&lt;/li&gt; &lt;li&gt;At context lengths of 100K, Jamba2 3B delivers 1.7X greater throughout than Ministral3 3B and 2.7X greater throughput than Qwen 3 14B.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It‚Äôs available today in AI21‚Äôs SaaS and from Hugging Face.&lt;/p&gt; &lt;p&gt;Happy to answer questions or dig into benchmarks if people want more detail.&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="http://www.ai21.com/blog/introducing-jamba2"&gt;http://www.ai21.com/blog/introducing-jamba2&lt;/a&gt;&lt;br /&gt; Hugging Face: &lt;a href="https://huggingface.co/collections/ai21labs/jamba2"&gt;https://huggingface.co/collections/ai21labs/jamba2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zennaxxarion"&gt; /u/zennaxxarion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7c0pd/ai21_releases_jamba2_3b_and_jamba2_mini_built_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7c0pd/ai21_releases_jamba2_3b_and_jamba2_mini_built_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7c0pd/ai21_releases_jamba2_3b_and_jamba2_mini_built_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T13:38:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1q71sbe</id>
    <title>Dialogue Tree Search - MCTS-style tree search to find optimal dialogue paths (so you don't have to trial-and-error it yourself)</title>
    <updated>2026-01-08T04:08:39+00:00</updated>
    <author>
      <name>/u/ManavTheWorld</name>
      <uri>https://old.reddit.com/user/ManavTheWorld</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/"&gt; &lt;img alt="Dialogue Tree Search - MCTS-style tree search to find optimal dialogue paths (so you don't have to trial-and-error it yourself)" src="https://b.thumbs.redditmedia.com/6NWh2JOC5gMK_IIgH5CXHBl--P730mcKTIYRJh91W0w.jpg" title="Dialogue Tree Search - MCTS-style tree search to find optimal dialogue paths (so you don't have to trial-and-error it yourself)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! I'm sharing an updated version of my MCTS-for-conversations project. Instead of generating single responses, it explores entire conversation trees to find dialogue strategies and prunes bad paths. I built it to help get better research directions for projects, but it can be used for anything&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/shr3e0liv1cg1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eec800c6dcd9f1a4fd033d003fe80e102cba8079"&gt;https://preview.redd.it/shr3e0liv1cg1.png?width=2560&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eec800c6dcd9f1a4fd033d003fe80e102cba8079&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/MVPandey/DTS"&gt;https://github.com/MVPandey/DTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Motivation: I like MCTS :3 and I originally wanted to make this a dataset-creation agent, but this is what it evolved into on its own. Basically:DTS runs parallel beam search over conversation branches. You give it a goal and opening message, and it:&lt;/p&gt; &lt;p&gt;(Note: this isnt mcts. It's parallel beam search. UCB1 is too wild with llms for me)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Generates N diverse strategies&lt;/li&gt; &lt;li&gt;Forks each into user intent variants - skeptical, cooperative, confused, resistant (if enabled, or defaults to engaged + probing)&lt;/li&gt; &lt;li&gt;Rolls out full multi-turn conversations down each branch&lt;/li&gt; &lt;li&gt;Has 3 independent LLM judges score each trajectory, takes the median&lt;/li&gt; &lt;li&gt;Prunes branches below threshold, backpropagates scores&lt;/li&gt; &lt;li&gt;Repeats for however many rounds you configure&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zkii0idvv1cg1.png?width=762&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=905f9787a8b7c7bfafcc599e95a3b73005c331b4"&gt;https://preview.redd.it/zkii0idvv1cg1.png?width=762&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=905f9787a8b7c7bfafcc599e95a3b73005c331b4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Three judges with median voting helps a lot with the LLM-as-judge variance problem from CAE. Still not grounded in anything real, but outlier scores get filtered. Research context helps but the scroing is still stochastic. I tried a rubric based approach but it was trash.&lt;/p&gt; &lt;p&gt;Main additions over CAE:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;user intent forking (strategies get stress-tested against different personas)&lt;/li&gt; &lt;li&gt;deep research integration via GPT-Researcher for domain context&lt;/li&gt; &lt;li&gt;proper visualization with conversation playback&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Only supports openai compatible endpoints atm - works with whatever models you have access to there. It's token-hungry though, a full run can hit 300+ LLM calls depending on config. If running locally, disable parallel calls&lt;/p&gt; &lt;p&gt;It's open source (Apache 2.0) and I'm happy to take contributions if anyone wants to help out. Just a project.&lt;/p&gt; &lt;p&gt;--&lt;/p&gt; &lt;p&gt;BTW: Backend was done mostly by me as the planner/sys designer, etc + Claude Code for implementation/refactoring. Frontend was purely vibe coded. Sorry if the code is trash.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ManavTheWorld"&gt; /u/ManavTheWorld &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T04:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1q77rxh</id>
    <title>Z-image base model is being prepared for release</title>
    <updated>2026-01-08T09:51:33+00:00</updated>
    <author>
      <name>/u/Ravencloud007</name>
      <uri>https://old.reddit.com/user/Ravencloud007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q77rxh/zimage_base_model_is_being_prepared_for_release/"&gt; &lt;img alt="Z-image base model is being prepared for release" src="https://preview.redd.it/038zb25ok3cg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2cbdfd4fbd53811cd0fc218bed6e466b49ff678" title="Z-image base model is being prepared for release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher&amp;amp;since=2025-12-31&amp;amp;until=2026-01-08"&gt;https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher&amp;amp;since=2025-12-31&amp;amp;until=2026-01-08&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ravencloud007"&gt; /u/Ravencloud007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/038zb25ok3cg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q77rxh/zimage_base_model_is_being_prepared_for_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q77rxh/zimage_base_model_is_being_prepared_for_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T09:51:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7a62a</id>
    <title>AI21 Labs releases Jamba2</title>
    <updated>2026-01-08T12:10:15+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/"&gt; &lt;img alt="AI21 Labs releases Jamba2" src="https://b.thumbs.redditmedia.com/Il111fZ012O0JrQgLsZklbi8sbSvI68SHycPLPcigNc.jpg" title="AI21 Labs releases Jamba2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zmo6dijns4cg1.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ba9fd085bb5b3fb720adf85cf28c3a8b63ba44cb"&gt;https://preview.redd.it/zmo6dijns4cg1.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ba9fd085bb5b3fb720adf85cf28c3a8b63ba44cb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;52B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-Mini"&gt;https://huggingface.co/ai21labs/AI21-Jamba2-Mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Jamba2 Mini is an open source small language model built for enterprise reliability. With 12B active parameters (52B total), it delivers precise question answering without the computational overhead of reasoning models. The model's SSM-Transformer architecture provides a memory-efficient solution for production agent stacks where consistent, grounded outputs are critical.&lt;/p&gt; &lt;p&gt;Released under Apache 2.0 License with a 256K context window, Jamba2 Mini is designed for enterprise workflows that demand accuracy and steerability. For more details, read the &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-Mini/blob/main/ai21.com/blog/introducing-jamba2"&gt;full release blog post&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Key Advantages&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Superior reliability-to-throughput ratio:&lt;/strong&gt; Maintains high performance at 100K+ token contexts&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Category-leading benchmarks:&lt;/strong&gt; Excels on IFBench, IFEval, Collie, and FACTS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Statistically significant quality wins:&lt;/strong&gt; Outperforms comparable models on real-world enterprise tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;256K context window:&lt;/strong&gt; Processes technical manuals, research papers, and knowledge bases&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apache 2.0 License:&lt;/strong&gt; Fully open source for commercial use&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Production-optimized:&lt;/strong&gt; Lean memory footprint for scalable deployments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cqwicpwts4cg1.png?width=2400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=593fed6a7d2094908b6f1878ea12a8e4f5e67e6d"&gt;https://preview.redd.it/cqwicpwts4cg1.png?width=2400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=593fed6a7d2094908b6f1878ea12a8e4f5e67e6d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;3B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-3B"&gt;https://huggingface.co/ai21labs/AI21-Jamba2-3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Jamba2 3B is an ultra-compact open source model designed to bring enterprise-grade reliability to on-device deployments. At just 3B parameters, it runs efficiently on consumer devices‚ÄîiPhones, Androids, Macs, and PCs‚Äîwhile maintaining the grounding and instruction-following capabilities required for production use.&lt;/p&gt; &lt;p&gt;Released under Apache 2.0 License with a 256K context window, Jamba2 3B enables developers to build reliable AI applications for edge environments. For more details, read the &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-3B/blob/main/ai21.com/blog/introducing-jamba2"&gt;full release blog post&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Key Advantages&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;On-device deployment:&lt;/strong&gt; Runs efficiently on iPhones, Androids, Macs, and PCs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ultra-compact footprint:&lt;/strong&gt; 3B parameters enabling edge deployments with minimal resources&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmark leadership:&lt;/strong&gt; Excels on IFBench, IFEval, Collie, and FACTS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;256K context window:&lt;/strong&gt; Processes long documents and knowledge bases&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apache 2.0 License:&lt;/strong&gt; Fully open source for commercial use&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SSM-Transformer architecture:&lt;/strong&gt; Memory-efficient design for resource-constrained environments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;it works in llama.cpp, tested on my Windows desktop:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ijzgde7bg5cg1.png?width=3802&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=983bc8e27ec59065d4b548e78eb4f50405507c71"&gt;https://preview.redd.it/ijzgde7bg5cg1.png?width=3802&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=983bc8e27ec59065d4b548e78eb4f50405507c71&lt;/a&gt;&lt;/p&gt; &lt;p&gt;fixed blog post &lt;a href="https://www.ai21.com/blog/introducing-jamba2/"&gt;https://www.ai21.com/blog/introducing-jamba2/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUFs are in progress &lt;a href="https://huggingface.co/mradermacher/model_requests/discussions/1683"&gt;https://huggingface.co/mradermacher/model_requests/discussions/1683&lt;/a&gt;&lt;/p&gt; &lt;p&gt;previous generation of Jamba models&lt;/p&gt; &lt;p&gt;399B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7"&gt;https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;52B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7"&gt;https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;3B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B"&gt;https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T12:10:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7dlkn</id>
    <title>Qwen3-VL-Reranker - a Qwen Collection</title>
    <updated>2026-01-08T14:45:00+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7dlkn/qwen3vlreranker_a_qwen_collection/"&gt; &lt;img alt="Qwen3-VL-Reranker - a Qwen Collection" src="https://external-preview.redd.it/p_EUBuVnZfgcYfu2zAo996Hix2TFsBWGTVl7mQyY9Tk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b4e2c1310fa6d24d7fb43df7672f7329a04cfbc" title="Qwen3-VL-Reranker - a Qwen Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl-reranker"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7dlkn/qwen3vlreranker_a_qwen_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7dlkn/qwen3vlreranker_a_qwen_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T14:45:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7jd1a</id>
    <title>LFM2.5 1.2B Instruct is amazing</title>
    <updated>2026-01-08T18:17:04+00:00</updated>
    <author>
      <name>/u/Paramecium_caudatum_</name>
      <uri>https://old.reddit.com/user/Paramecium_caudatum_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This model punches way above its weight. It outperforms every other model I've tried in this size range and runs smoothly on basically any hardware. If you haven't tried it yet, you definitely should.&lt;/p&gt; &lt;p&gt;Important note:&lt;br /&gt; &amp;quot;&amp;quot;&amp;quot;&lt;br /&gt; We recommend using it for agentic tasks, data extraction, and RAG. It is not recommended for knowledge-intensive tasks and programming.&lt;/p&gt; &lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct"&gt;https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Paramecium_caudatum_"&gt; /u/Paramecium_caudatum_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7jd1a/lfm25_12b_instruct_is_amazing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7jd1a/lfm25_12b_instruct_is_amazing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7jd1a/lfm25_12b_instruct_is_amazing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T18:17:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7mvuf</id>
    <title>Z.ai (the AI lab behind GLM) has officially IPO'd on the Hong Kong Stock Exchange</title>
    <updated>2026-01-08T20:23:59+00:00</updated>
    <author>
      <name>/u/Old-School8916</name>
      <uri>https://old.reddit.com/user/Old-School8916</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-School8916"&gt; /u/Old-School8916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Zai_org/status/2009290783678239032"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7mvuf/zai_the_ai_lab_behind_glm_has_officially_ipod_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7mvuf/zai_the_ai_lab_behind_glm_has_officially_ipod_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T20:23:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7d8bj</id>
    <title>Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt</title>
    <updated>2026-01-08T14:29:47+00:00</updated>
    <author>
      <name>/u/Prior-Arm-6705</name>
      <uri>https://old.reddit.com/user/Prior-Arm-6705</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/"&gt; &lt;img alt="Jensen Huang saying &amp;quot;AI&amp;quot; 121 times during the NVIDIA CES keynote - cut with one prompt" src="https://external-preview.redd.it/M2cyNzBqaHB4NGNnMeuNas4_kS8fQc08s_eqp1ss4JB4szq45v23OyPEbFog.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a660094eff764f4c2c968c5de87ba1bafcb35b9" title="Jensen Huang saying &amp;quot;AI&amp;quot; 121 times during the NVIDIA CES keynote - cut with one prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone had to count it. Turns out Jensen said &amp;quot;AI&amp;quot; exactly 121 times in the CES 2025 keynote.&lt;/p&gt; &lt;p&gt;I used &lt;a href="https://github.com/OpenAgentPlatform/Dive"&gt;https://github.com/OpenAgentPlatform/Dive&lt;/a&gt; (open-source MCP client) + two MCPs I made:&lt;/p&gt; &lt;p&gt;- &lt;a href="https://github.com/kevinwatt/yt-dlp-mcp"&gt;https://github.com/kevinwatt/yt-dlp-mcp&lt;/a&gt; - YouTube download&lt;br /&gt; - &lt;a href="https://github.com/kevinwatt/ffmpeg-mcp-lite"&gt;https://github.com/kevinwatt/ffmpeg-mcp-lite&lt;/a&gt; - video editing&lt;/p&gt; &lt;p&gt;&lt;strong&gt;One prompt:&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Task: Create a compilation video of every exact moment Jensen Huang says &amp;quot;AI&amp;quot;.&lt;br /&gt; Video source: &lt;a href="https://www.youtube.com/watch?v=0NBILspM4c4"&gt;https://www.youtube.com/watch?v=0NBILspM4c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Instructions:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Download video in 720p + subtitles in JSON3 format (word-level timestamps)&lt;/p&gt; &lt;p&gt;Parse JSON3 to find every &amp;quot;AI&amp;quot; instance with precise start/end times&lt;/p&gt; &lt;p&gt;Use ffmpeg to cut clips (~50-100ms padding for natural sound)&lt;/p&gt; &lt;p&gt;Concatenate all clips chronologically&lt;/p&gt; &lt;p&gt;Output: Jensen_CES_AI.mp4&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Dive chained the two MCPs together - download ‚Üí parse timestamps ‚Üí cut 121 clips ‚Üí merge. All local, no cloud.&lt;/p&gt; &lt;p&gt;If you want to see how it runs: &lt;a href="https://www.youtube.com/watch?v=u_7OtyYAX74"&gt;https://www.youtube.com/watch?v=u_7OtyYAX74&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The result is... hypnotic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prior-Arm-6705"&gt; /u/Prior-Arm-6705 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hein55gpx4cg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T14:29:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
