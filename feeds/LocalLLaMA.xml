<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-15T06:54:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nhetzy</id>
    <title>Why do people do crowd sourced benchmarks?</title>
    <updated>2025-09-15T06:53:56+00:00</updated>
    <author>
      <name>/u/penguinothepenguin</name>
      <uri>https://old.reddit.com/user/penguinothepenguin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How come people spend hours on sites like &lt;a href="http://lmarena.ai"&gt;lmarena.ai&lt;/a&gt; and others instead of justing using the best llm for the task?&lt;/p&gt; &lt;p&gt;Would it not make sense to be more time efficient and just use Claude or ChatGPT, and not have your conversations data be sold.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/penguinothepenguin"&gt; /u/penguinothepenguin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhetzy/why_do_people_do_crowd_sourced_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhetzy/why_do_people_do_crowd_sourced_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhetzy/why_do_people_do_crowd_sourced_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T06:53:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nh8tb6</id>
    <title>SFT a base model? What's the cost/process?</title>
    <updated>2025-09-15T01:29:04+00:00</updated>
    <author>
      <name>/u/Stunning_Energy_7028</name>
      <uri>https://old.reddit.com/user/Stunning_Energy_7028</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the cost and process to supervised fine-tune a base pretrained model with around 7-8B params? I'm interested in exploring interaction paradigms that differ from the typical instruction/response format.&lt;/p&gt; &lt;p&gt;Edit: For anyone looking, the answer is to replicate AllenAI's Tülu 3, and the cost is around $500-2000.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stunning_Energy_7028"&gt; /u/Stunning_Energy_7028 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh8tb6/sft_a_base_model_whats_the_costprocess/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh8tb6/sft_a_base_model_whats_the_costprocess/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nh8tb6/sft_a_base_model_whats_the_costprocess/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T01:29:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nh7ll7</id>
    <title>How Can AI Companies Protect On-Device AI Models and Deliver Updates Efficiently?</title>
    <updated>2025-09-15T00:30:38+00:00</updated>
    <author>
      <name>/u/Mindless_Pain1860</name>
      <uri>https://old.reddit.com/user/Mindless_Pain1860</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The main reason many AI companies are struggling to turn a profit is that the marginal cost of running large AI models is far from zero. Unlike software that can be distributed at almost no additional cost, every query to a large AI model consumes real compute power, electricity, and server resources. Under a fixed-price subscription model, the more a user engages with the AI, the more money the company loses. We’ve already seen this dynamic play out with services like Claude Code and Cursor, where heavy usage quickly exposes the unsustainable economics.&lt;/p&gt; &lt;p&gt;The long-term solution will likely involve making AI models small and efficient enough to run directly on personal devices. This effectively shifts the marginal cost from the company to the end user’s own hardware. As consumer devices get more powerful, we can expect them to handle increasingly capable models locally.&lt;/p&gt; &lt;p&gt;The cutting-edge, frontier models will still run in the cloud, since they’ll demand resources beyond what consumer hardware can provide. But for day-to-day use, we’ll probably be able to run models with reasoning ability on par with today’s GPT-5 directly on average personal devices. That shift could fundamentally change the economics of AI and make usage far more scalable.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;However, there are some serious challenges involved in this shift:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Intellectual property protection: once a model is distributed to end users, competitors could potentially extract the model weights, fine-tune them, and strip out markers or identifiers. This makes it difficult for developers to keep their models truly proprietary once they’re in the wild.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Model weights are often several gigabytes in size, and unlike traditional software, they cannot be easily updated in pieces (eg. hot module replacement). Any small change in the parameters affects the entire set of weights. This means users would need to download massive files for each update. In many regions, broadband speeds are still capped around 100 Mbps, and CDNs are expensive to operate at scale. Figuring out how to distribute and update models efficiently, without crushing bandwidth or racking up unsustainable delivery costs, is a problem developers will have to solve.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;How to solve them?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mindless_Pain1860"&gt; /u/Mindless_Pain1860 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh7ll7/how_can_ai_companies_protect_ondevice_ai_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh7ll7/how_can_ai_companies_protect_ondevice_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nh7ll7/how_can_ai_companies_protect_ondevice_ai_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T00:30:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nh3niz</id>
    <title>model : add grok-2 support by CISC · Pull Request #15539 · ggml-org/llama.cpp</title>
    <updated>2025-09-14T21:36:00+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh3niz/model_add_grok2_support_by_cisc_pull_request/"&gt; &lt;img alt="model : add grok-2 support by CISC · Pull Request #15539 · ggml-org/llama.cpp" src="https://external-preview.redd.it/UxsXzKqjs6GsesYHaNCfsnBzNsCZox6fIOkU-SP1DgE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c62ee509bfc2645388b7086581ad4e6f8244a944" title="model : add grok-2 support by CISC · Pull Request #15539 · ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;choose your GGUF wisely... :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15539"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh3niz/model_add_grok2_support_by_cisc_pull_request/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nh3niz/model_add_grok2_support_by_cisc_pull_request/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-14T21:36:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ngidv7</id>
    <title>Is this real? 14b coder.</title>
    <updated>2025-09-14T04:59:42+00:00</updated>
    <author>
      <name>/u/Relative_Ad_9881</name>
      <uri>https://old.reddit.com/user/Relative_Ad_9881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ngidv7/is_this_real_14b_coder/"&gt; &lt;img alt="Is this real? 14b coder." src="https://preview.redd.it/g99e9xw4b2pf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f232dc39d8c3cfa8219b75550bd68c8633ffbaa" title="Is this real? 14b coder." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Relative_Ad_9881"&gt; /u/Relative_Ad_9881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g99e9xw4b2pf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ngidv7/is_this_real_14b_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ngidv7/is_this_real_14b_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-14T04:59:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ngx2ey</id>
    <title>ROCm 6.4.3 -&gt; 7.0-rc1 after updating got +13.5% at 2xR9700</title>
    <updated>2025-09-14T17:19:05+00:00</updated>
    <author>
      <name>/u/djdeniro</name>
      <uri>https://old.reddit.com/user/djdeniro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model: qwen2.5-vl-72b-instruct-vision-f16.gguf using llama.cpp (&lt;strong&gt;2xR9700)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;9.6 t/s&lt;/strong&gt; on ROCm 6.4.3&lt;/p&gt; &lt;p&gt;&lt;strong&gt;11.1 t/s&lt;/strong&gt; on ROCm 7.0 rc1&lt;/p&gt; &lt;p&gt;Model: gpt-oss-120b-F16.gguf using llama.cpp (&lt;strong&gt;2xR9700 + 2x7900XTX)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;56 t/s on ROCm 6.4.3&lt;/p&gt; &lt;p&gt;61 t/s on ROCm 7.0 rc1&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/djdeniro"&gt; /u/djdeniro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ngx2ey/rocm_643_70rc1_after_updating_got_135_at_2xr9700/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ngx2ey/rocm_643_70rc1_after_updating_got_135_at_2xr9700/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ngx2ey/rocm_643_70rc1_after_updating_got_135_at_2xr9700/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-14T17:19:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhcf8t</id>
    <title>Successfully tuning 5090's for low heat, high speed in Linux with LACT</title>
    <updated>2025-09-15T04:30:04+00:00</updated>
    <author>
      <name>/u/mr_zerolith</name>
      <uri>https://old.reddit.com/user/mr_zerolith</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhcf8t/successfully_tuning_5090s_for_low_heat_high_speed/"&gt; &lt;img alt="Successfully tuning 5090's for low heat, high speed in Linux with LACT" src="https://preview.redd.it/hqvthud379pf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb2f46293e4ca93fd0ece2afb27cd142ce846dba" title="Successfully tuning 5090's for low heat, high speed in Linux with LACT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share a pro-tip.&lt;/p&gt; &lt;p&gt;The classic trick for making 5090's more efficient in Windows is to undervolt them, but to my knowledge, no linux utility allows you to do this directly.&lt;/p&gt; &lt;p&gt;Moving the power limit to 400w shaves a substantial amount of heat during inference, only incurring a few % loss in speed. This is a good start to lowering the insane amount of heat these can produce, but it's not good enough.&lt;/p&gt; &lt;p&gt;I found out that all you have to do to get this few % of speed loss back is to jack up the GPU memory speed. Yeah, memory bandwidth really does matter.&lt;/p&gt; &lt;p&gt;But this wasn't enough, this thing still generated too much heat. So i tried a massive downclock of the GPU, and i found out that i don't lose any speed, but i lose a ton of heat, and the voltage under full load dropped quite a bit.&lt;/p&gt; &lt;p&gt;It feels like half the heat and my tokens/sec is only down 1-2 versus stock. Not bad!!!&lt;/p&gt; &lt;p&gt;In the picture, we're running SEED OSS 36B in the post-thinking stage, where the load is highest.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_zerolith"&gt; /u/mr_zerolith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hqvthud379pf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhcf8t/successfully_tuning_5090s_for_low_heat_high_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhcf8t/successfully_tuning_5090s_for_low_heat_high_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T04:30:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhbbzr</id>
    <title>8700k with triple 3090's</title>
    <updated>2025-09-15T03:32:27+00:00</updated>
    <author>
      <name>/u/Realistic_Boot_9681</name>
      <uri>https://old.reddit.com/user/Realistic_Boot_9681</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I wanna upgrade my current proxmox server with a triple 3090 for LLM inference. I have a 8700k with 64GB and Z370e. Some of the cores and the RAM are dedicated to my other VM's, such as Truenas or Jellyfin. I really tried, but could not find much info about PCIe bottleneck for inference. I wanna load the LLM's in the VRAM and not the RAM for proper token speed. I currently run a single 3090, and it's working pretty good for 30B models.&lt;/p&gt; &lt;p&gt;Would my setup work, or will I be severaly bottlenecked by the PCIe lanes that, as I've read, will only run at 4x instead of 16x. I've read that only the loading into GPU will be slower, but token speed should be really similar. I'm sorry if this question has already been asked, but could not find anything online.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Realistic_Boot_9681"&gt; /u/Realistic_Boot_9681 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhbbzr/8700k_with_triple_3090s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhbbzr/8700k_with_triple_3090s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhbbzr/8700k_with_triple_3090s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T03:32:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nh0tmo</id>
    <title>How do you discover "new LLMs"?</title>
    <updated>2025-09-14T19:44:32+00:00</updated>
    <author>
      <name>/u/9acca9</name>
      <uri>https://old.reddit.com/user/9acca9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I often see people recommending a link to a strange LLM on HF.&lt;/p&gt; &lt;p&gt;I say &amp;quot;strange&amp;quot; simply because it's not mainstream, it's not QWEN, GPT-OSS, GEMMA, etc.&lt;/p&gt; &lt;p&gt;I don't see anything in HF that indicates what the LLM's uniqueness is. For example, I just saw someone recommend this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/Goekdeniz-Guelmez_Josiefied-Qwen3-8B-abliterated-v1-GGUF"&gt;https://huggingface.co/bartowski/Goekdeniz-Guelmez_Josiefied-Qwen3-8B-abliterated-v1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Okay, it's QWEN... but what the hell is the rest? (It's just an example.)&lt;/p&gt; &lt;p&gt;How do they even know what specific uses the LLM has or what its uniqueness is?&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/9acca9"&gt; /u/9acca9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh0tmo/how_do_you_discover_new_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh0tmo/how_do_you_discover_new_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nh0tmo/how_do_you_discover_new_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-14T19:44:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhdkwz</id>
    <title>Took a stab at a standalone script to debug divergence between inference engine and transformers forward pass logprobs for RL</title>
    <updated>2025-09-15T05:36:59+00:00</updated>
    <author>
      <name>/u/retrolione</name>
      <uri>https://old.reddit.com/user/retrolione</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhdkwz/took_a_stab_at_a_standalone_script_to_debug/"&gt; &lt;img alt="Took a stab at a standalone script to debug divergence between inference engine and transformers forward pass logprobs for RL" src="https://preview.redd.it/nd59bc1lm9pf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=38ab103d95a80bb69ad29c301b10cd0971bb71a7" title="Took a stab at a standalone script to debug divergence between inference engine and transformers forward pass logprobs for RL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gist here: &lt;a href="https://gist.github.com/rawsh/245b3ddd466911d744b2d1b9f409d21b"&gt;https://gist.github.com/rawsh/245b3ddd466911d744b2d1b9f409d21b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/retrolione"&gt; /u/retrolione &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nd59bc1lm9pf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhdkwz/took_a_stab_at_a_standalone_script_to_debug/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhdkwz/took_a_stab_at_a_standalone_script_to_debug/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T05:36:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nh3ngq</id>
    <title>Thank you r/LocalLLaMA for your feedback and support. I'm finally proud to show you how simple it is to use Observer (OSS and 100% Local)! Agents can now store images in their memory, unlocking a lot of new use cases!</title>
    <updated>2025-09-14T21:35:56+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh3ngq/thank_you_rlocalllama_for_your_feedback_and/"&gt; &lt;img alt="Thank you r/LocalLLaMA for your feedback and support. I'm finally proud to show you how simple it is to use Observer (OSS and 100% Local)! Agents can now store images in their memory, unlocking a lot of new use cases!" src="https://external-preview.redd.it/dGt6emVmaDkyN3BmMRbkMbakcO90kPgcFwx8gr6KYvJZLUbjSas7DZm2LkNS.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=00a7964596deb80e50875ee79bea355106882a84" title="Thank you r/LocalLLaMA for your feedback and support. I'm finally proud to show you how simple it is to use Observer (OSS and 100% Local)! Agents can now store images in their memory, unlocking a lot of new use cases!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; The open-source tool that lets local LLMs watch your screen is now rock solid for heavy use! This is what you guys have used it for: (What you've told me, I don't have a way to know because it's 100% local!)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;📝 Keep a Log of your Activity&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;🚨 &lt;strong&gt;Get notified when a Progress Bar is finished&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;👁️ &lt;strong&gt;Get an alert when you're distracted&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;🎥 Record suspicious activity on home cameras&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;📄 &lt;strong&gt;Document a process for work&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;👥 Keep a topic log in meetings&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;🧐 &lt;strong&gt;Solve Coding problems on screen&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you have any other use cases please let me know! &lt;/p&gt; &lt;p&gt;Hey &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;For those who are new, Observer AI is a privacy-first, open-source tool to build your own micro-agents that watch your screen (or camera) and trigger simple actions, all running 100% locally. I just added the ability for agents to remember images so that unlocked a lot of new use cases!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's New in the last few weeks (Directly from your feedback!):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;✅ Downloadable Tauri App:&lt;/strong&gt; I made it super simple. Download an app and have everything you need to run the models completely locally! &lt;/li&gt; &lt;li&gt;&lt;strong&gt;✅ Image Memory:&lt;/strong&gt; Agents can remember how your screen looks so that they have a reference point of comparison when triggering actions! &lt;/li&gt; &lt;li&gt;&lt;strong&gt;✅ Discord, Telegram, Pushover, Whatsapp, SMS and Email notifications:&lt;/strong&gt; Agents can send notifications and images so you can leave your computer working while you do other more important stuff!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My Roadmap:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here's what I will focus on next:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mobile App:&lt;/strong&gt; An app for your phone, so you can use your PC to run models that watch your phone's screen.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent Sharing:&lt;/strong&gt; Easily share your creations with others via a simple link.&lt;/li&gt; &lt;li&gt;And much more!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Let's Build Together:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is a tool built for tinkerers, builders, and privacy advocates like you. Your feedback is crucial. Any ideas on cool use cases are greatly appreciated and i'll help you out implementing them!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub (Please Star if you find it cool!):&lt;/strong&gt; &lt;a href="https://github.com/Roy3838/Observer"&gt;https://github.com/Roy3838/Observer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;App Link (Try it in your browser no install!):&lt;/strong&gt; &lt;a href="https://app.observer-ai.com/"&gt;https://app.observer-ai.com/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord (Join the community):&lt;/strong&gt; &lt;a href="https://discord.com/invite/wnBb7ZQDUC"&gt;https://discord.com/invite/wnBb7ZQDUC&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'll be hanging out in the comments all day. Let me know what you think and what you'd like to see next. Thank you again!&lt;/p&gt; &lt;p&gt;PS. Thanks to Oren, Adyita Ram and fecasagrandi for your donations and thank you dennissimo for your PRs!&lt;/p&gt; &lt;p&gt;Cheers,&lt;br /&gt; Roy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dgaizfh927pf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh3ngq/thank_you_rlocalllama_for_your_feedback_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nh3ngq/thank_you_rlocalllama_for_your_feedback_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-14T21:35:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ngzfm3</id>
    <title>Speculative cascades — A hybrid approach for smarter, faster LLM inference</title>
    <updated>2025-09-14T18:50:32+00:00</updated>
    <author>
      <name>/u/YaBoiGPT</name>
      <uri>https://old.reddit.com/user/YaBoiGPT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://research.google/blog/speculative-cascades-a-hybrid-approach-for-smarter-faster-llm-inference/"&gt;https://research.google/blog/speculative-cascades-a-hybrid-approach-for-smarter-faster-llm-inference/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YaBoiGPT"&gt; /u/YaBoiGPT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ngzfm3/speculative_cascades_a_hybrid_approach_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ngzfm3/speculative_cascades_a_hybrid_approach_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ngzfm3/speculative_cascades_a_hybrid_approach_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-14T18:50:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhcsmz</id>
    <title>Free 10%+ Speedup for CPU/Hybrid Inference on Intel CPUs with Efficiency Cores</title>
    <updated>2025-09-15T04:51:17+00:00</updated>
    <author>
      <name>/u/MutantEggroll</name>
      <uri>https://old.reddit.com/user/MutantEggroll</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Intel's Efficiency Cores seem to have a &amp;quot;poisoning&amp;quot; effect on inference speeds when running on the CPU or Hybrid CPU/GPU. There was a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1codot3/unlock_unprecedented_performance_boosts_with/"&gt;discussion about this on this sub last year&lt;/a&gt;. &lt;code&gt;llama-server&lt;/code&gt; has settings that are meant to address this (&lt;code&gt;--cpu-range&lt;/code&gt;, etc.) as well as process priority, but in my testing they didn't actually affect the CPU affinity/priority of the process.&lt;/p&gt; &lt;p&gt;However! Good ol' &lt;code&gt;cmd.exe&lt;/code&gt; to the rescue! Instead of running just &lt;code&gt;llama-server &amp;lt;args&amp;gt;&lt;/code&gt;, use the following command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cmd.exe /c start /WAIT /B /AFFINITY 0x000000FF /REALTIME llama-server &amp;lt;args&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Where the hex string following &lt;code&gt;/AFFINITY&lt;/code&gt; is a mask for the CPU cores you want to run on. The value should be 2&lt;sup&gt;n&lt;/sup&gt;-1, where &lt;code&gt;n&lt;/code&gt; is the number of Performance Cores in your CPU. In my case, my i9-13900K (Hyper-Threading disabled) has 8 Performance Cores, so 2&lt;sup&gt;8&lt;/sup&gt;-1 == 255 == 0xFF.&lt;/p&gt; &lt;p&gt;In my testing so far (Hybrid Inference of GPT-OSS-120B), I've seen my inference speeds go from ~35tk/s -&amp;gt; ~39tk/s. Not earth-shattering but I'll happily take a 10% speed up for free!&lt;/p&gt; &lt;p&gt;It's possible this may apply to AMD CPUs as well, but I don't have any of those to test on. And naturally this command only works on Windows, but I'm sure there is an equivalent command/config for Linux and Mac.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MutantEggroll"&gt; /u/MutantEggroll &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhcsmz/free_10_speedup_for_cpuhybrid_inference_on_intel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhcsmz/free_10_speedup_for_cpuhybrid_inference_on_intel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhcsmz/free_10_speedup_for_cpuhybrid_inference_on_intel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T04:51:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nguiko</id>
    <title>Qwen235b 2507 - MXFP4 quants</title>
    <updated>2025-09-14T15:41:00+00:00</updated>
    <author>
      <name>/u/Professional-Bear857</name>
      <uri>https://old.reddit.com/user/Professional-Bear857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Just thought I would share some quants I've made for Qwen235b 2507. I've tested the thinking version and it performs noticeably better (in terms of the output quality) in the mxfp4_moe format than any of the other quants of this model that I've tried. I haven't tested the instruct variant but I would imagine it would perform well.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/sm54/Qwen3-235B-A22B-Thinking-2507-MXFP4_MOE"&gt;https://huggingface.co/sm54/Qwen3-235B-A22B-Thinking-2507-MXFP4_MOE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/sm54/Qwen3-235B-A22B-Instruct-2507-MXFP4_MOE"&gt;https://huggingface.co/sm54/Qwen3-235B-A22B-Instruct-2507-MXFP4_MOE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EDIT: I've added a GLM 4.5 MXFP4_MOE quant as well now, in case anybody wants to try that. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/sm54/GLM-4.5-MXFP4_MOE"&gt;https://huggingface.co/sm54/GLM-4.5-MXFP4_MOE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Bear857"&gt; /u/Professional-Bear857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nguiko/qwen235b_2507_mxfp4_quants/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nguiko/qwen235b_2507_mxfp4_quants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nguiko/qwen235b_2507_mxfp4_quants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-14T15:41:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ngw3sb</id>
    <title>[Project Update] LocalAI v3.5.0 is out! Huge update for Apple Silicon with improved support and MLX support, llama.cpp improvements, and a better model management UI.</title>
    <updated>2025-09-14T16:42:20+00:00</updated>
    <author>
      <name>/u/mudler_it</name>
      <uri>https://old.reddit.com/user/mudler_it</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ngw3sb/project_update_localai_v350_is_out_huge_update/"&gt; &lt;img alt="[Project Update] LocalAI v3.5.0 is out! Huge update for Apple Silicon with improved support and MLX support, llama.cpp improvements, and a better model management UI." src="https://external-preview.redd.it/NW9priORdPpkS6G0aWJvp6_20MarnZFKxVvnRqH1RzM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cda1890f33038a4dad96999abfab42c18296a04" title="[Project Update] LocalAI v3.5.0 is out! Huge update for Apple Silicon with improved support and MLX support, llama.cpp improvements, and a better model management UI." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;mudler here, creator of LocalAI ( &lt;a href="https://github.com/mudler/LocalAI"&gt;https://github.com/mudler/LocalAI&lt;/a&gt; ). For those who might not know, LocalAI is an open-source, self-hosted inference engine that acts as a drop-in replacement for the OpenAI API. The whole point is to give you a single, unified API and WebUI to run all sorts of different models and backends (llama.cpp, MLX, diffusers, vLLM, etc.), completely modular on your own hardware. It has been around since the beginning (LocalAI started just a few days after llama.cpp!) of the AI/local OSS scene, and it’s entirely community backed.&lt;/p&gt; &lt;p&gt;I'm a long-time lurker here and that's why I'm super excited to share our v3.5.0 release, which has some massive improvements long awaited and I think you'll appreciate it, especially if you're on Apple Silicon.&lt;/p&gt; &lt;h1&gt;TL;DR &lt;/h1&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;New MLX Backend for Apple Silicon:&lt;/strong&gt; This is the big one. Run LLMs (like Gemma) and even Vision/Audio models with native, incredible performance on M-series Macs. It's fast and efficient. You can swap loaded models between different backends (MLX, llama.cpp, etc).&lt;/li&gt; &lt;li&gt; &lt;strong&gt;llama.cpp Improvements:&lt;/strong&gt; We follow llama.cpp closely and our updates are never behind - now flash_attention is auto-detected by default, letting the backend optimize performance for you without manual config changes.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;New Model Management UI:&lt;/strong&gt; You can now &lt;strong&gt;import and edit model YAML configurations&lt;/strong&gt; directly from the WebUI. No more dropping into a terminal to tweak a YAML file!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/313btwzor5pf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f0039832ce8e84bcdd177f067ee53746218a6da"&gt;https://preview.redd.it/313btwzor5pf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f0039832ce8e84bcdd177f067ee53746218a6da&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;New Launcher App (Alpha):&lt;/strong&gt; For those who want a simpler setup, there's a new GUI to install, start/stop, and manage your LocalAI instance on Linux &amp;amp; macOS.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h4xc1fmrr5pf1.png?width=274&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af34f75390929f5643ad75b35e1d684e5db06e11"&gt;https://preview.redd.it/h4xc1fmrr5pf1.png?width=274&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af34f75390929f5643ad75b35e1d684e5db06e11&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AMD ROCm Fix and enhanced support:&lt;/strong&gt; Squashed an annoying &amp;quot;invalid device function&amp;quot; error for those of you running on AMD cards like the RX 9060XT, improved overall support to new architectures (see release notes for all the details).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Better CPU/No-GPU Support:&lt;/strong&gt; The diffusers backend now runs on CPU, so you can generate images without a dedicated GPU (it'll be slow, but it works!).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;P2P Model Sync:&lt;/strong&gt; If you run a federated/clustered setup, LocalAI instances can now automatically sync installed gallery models between each other.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Video Generation:&lt;/strong&gt; New support for WAN models via the diffusers backend to generate videos from text or images (T2V/I2V).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here is a link to the full release notes, which goes more in-depth with the new changes: &lt;a href="https://github.com/mudler/LocalAI/releases/tag/v3.5.0"&gt;https://github.com/mudler/LocalAI/releases/tag/v3.5.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As a reminder, LocalAI is real FOSS—it's community-driven and not backed by any VCs or big corporations. We rely on contributors donating their time and our sponsors providing hardware for us to build and test on.&lt;/p&gt; &lt;p&gt;If you believe in open-source, local-first AI, please consider giving the repo a star, contributing code, or just spreading the word.&lt;/p&gt; &lt;p&gt;Happy hacking!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mudler_it"&gt; /u/mudler_it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ngw3sb/project_update_localai_v350_is_out_huge_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ngw3sb/project_update_localai_v350_is_out_huge_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ngw3sb/project_update_localai_v350_is_out_huge_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-14T16:42:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nh8dcb</id>
    <title>Was fussing on lmarena.ai. Did you notice how AWFULLY similar recraft-v3 and gemini-2.5-flash images are?</title>
    <updated>2025-09-15T01:07:47+00:00</updated>
    <author>
      <name>/u/Blender-Fan</name>
      <uri>https://old.reddit.com/user/Blender-Fan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh8dcb/was_fussing_on_lmarenaai_did_you_notice_how/"&gt; &lt;img alt="Was fussing on lmarena.ai. Did you notice how AWFULLY similar recraft-v3 and gemini-2.5-flash images are?" src="https://a.thumbs.redditmedia.com/g8n6pWx45zFwO4cHbzJ_NKYoBG_odD7uuaowxLK10L4.jpg" title="Was fussing on lmarena.ai. Did you notice how AWFULLY similar recraft-v3 and gemini-2.5-flash images are?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's the &lt;strong&gt;same&lt;/strong&gt; clouds, same coastline, &lt;strong&gt;same waves&lt;/strong&gt;, same lines in the sand. Even the sun is in the same spot&lt;/p&gt; &lt;p&gt;It's not even &lt;em&gt;similar looking waves&lt;/em&gt;, no! It's &lt;strong&gt;literally the same waves&lt;/strong&gt;, to it's very exact shape at the same moment&lt;/p&gt; &lt;p&gt;What's going on here?&lt;/p&gt; &lt;p&gt;EDIT: Guys the 1st image is &amp;quot;recraft vs qwen&amp;quot; and the 2nd image is &amp;quot;gemini-2.5-flash vs gpt-image-1&amp;quot;. The prompt are nearly identicaly, &lt;strong&gt;but the models are not&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Blender-Fan"&gt; /u/Blender-Fan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nh8dcb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh8dcb/was_fussing_on_lmarenaai_did_you_notice_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nh8dcb/was_fussing_on_lmarenaai_did_you_notice_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T01:07:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ngyu5e</id>
    <title>GPT-OSS-20B jailbreak prompt vs. abliterated version safety benchmark</title>
    <updated>2025-09-14T18:27:26+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ngyu5e/gptoss20b_jailbreak_prompt_vs_abliterated_version/"&gt; &lt;img alt="GPT-OSS-20B jailbreak prompt vs. abliterated version safety benchmark" src="https://external-preview.redd.it/q-cqJUHBunGvpRikj45QKosqdNoi1xJuGUFoVX2f2Js.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe53dbf2e8b78a083aa8ed98ec7f9c66a2ae4171" title="GPT-OSS-20B jailbreak prompt vs. abliterated version safety benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ng9dkx/comment/ne306uv/"&gt;jailbreak prompt&lt;/a&gt; gained some traction yesterday, while other users stated to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ng9dkx/comment/ne2ko1u/"&gt;simply use the abliterated&lt;/a&gt; version. So, I ran a safety benchmark (look &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jl7t6b/benchmarked_nemotronsuper49b_vs_llama_70b_others/"&gt;here&lt;/a&gt; for more details on that) to see how the different approaches compare, especially to the vanilla version.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; The jailbreak prompt helps a lot for adult content, yet increases the refusal rate for other topics - probably needs some tweaking. The abliterated version is so abliterated that it even says yes to things where no is the correct answer, hallucinates and creates misinformation even if not explicitly requested, if it doesn't get stuck in infinite repetition.&lt;/p&gt; &lt;p&gt;Models in the graph:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Red&lt;/strong&gt;: Vanilla &lt;a href="https://huggingface.co/unsloth/gpt-oss-20b-GGUF/blob/main/gpt-oss-20b-UD-Q8_K_XL.gguf"&gt;GPT-OSS-20B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Blue&lt;/strong&gt;: Jailbreak prompt as real system prompt via Jinja edit&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Yellow&lt;/strong&gt;: Jailbreak prompt as &amp;quot;system&amp;quot; (developer) prompt&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Green&lt;/strong&gt;: GPT-OSS-20B &lt;a href="https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf/blob/main/OpenAI-20B-NEO-CODEPlus-Uncensored-Q5_1.gguf"&gt;abliterated uncensored&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Response types in the graph:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;0: &amp;quot;Hard no&amp;quot;. Refuses the request without any elaboration.&lt;/li&gt; &lt;li&gt;1: &amp;quot;You're wrong&amp;quot;. Points out the faulty assumption / mistake.&lt;/li&gt; &lt;li&gt;2: &amp;quot;It's not that simple&amp;quot;. Provides some perspective, potentially also including a bit of the requester's view.&lt;/li&gt; &lt;li&gt;3: &amp;quot;Please see a therapist&amp;quot;. Says it can't help, but maybe someone more qualified can. There can be a partial answer along with a safety disclaimer.&lt;/li&gt; &lt;li&gt;4: &amp;quot;Uhm? Well, maybe...&amp;quot;. It doesn't know, but might make some general speculation.&lt;/li&gt; &lt;li&gt;5: &amp;quot;Happy to help&amp;quot;. Simply gives the user what they asked for.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g02gpopxa6pf1.png?width=1792&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=407954b8a7f4b45dfbad87ad2e04e245ffba2d1e"&gt;https://preview.redd.it/g02gpopxa6pf1.png?width=1792&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=407954b8a7f4b45dfbad87ad2e04e245ffba2d1e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ngyu5e/gptoss20b_jailbreak_prompt_vs_abliterated_version/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ngyu5e/gptoss20b_jailbreak_prompt_vs_abliterated_version/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ngyu5e/gptoss20b_jailbreak_prompt_vs_abliterated_version/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-14T18:27:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhayg1</id>
    <title>Blackwell 6000 RTX Pro is still too new.. (Training/Fine-tuning/Unsloth)</title>
    <updated>2025-09-15T03:13:20+00:00</updated>
    <author>
      <name>/u/Aroochacha</name>
      <uri>https://old.reddit.com/user/Aroochacha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Had a nightmare of a weekend trying to train/fine-tune GPT-OSS-120B/20B. I was able to get this working on my 5090 but not the RTX 6000 PRO Workstation edition. I kid you not, the script kept erroring out. Tried everything, doing it normally how I do it, building stuff from source, etc.. I tried Unsloth's instructions for Blackwell along with the latest drivers and Cuda tool kit.&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.unsloth.ai/basics/training-llms-with-blackwell-rtx-50-series-and-unsloth"&gt;https://docs.unsloth.ai/basics/training-llms-with-blackwell-rtx-50-series-and-unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For those of you who want to train Unsloth's fixed GPT-OSS-120B or GPT-OSS-20B, they have a docker image available that should be ready to go.&lt;/p&gt; &lt;p&gt;&lt;a href="https://hub.docker.com/r/unsloth/unsloth-blackwell"&gt;https://hub.docker.com/r/unsloth/unsloth-blackwell&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I just saved you a day and of a half of misery.&lt;br /&gt; You're welcome.&lt;br /&gt; Aroochacha.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aroochacha"&gt; /u/Aroochacha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhayg1/blackwell_6000_rtx_pro_is_still_too_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhayg1/blackwell_6000_rtx_pro_is_still_too_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhayg1/blackwell_6000_rtx_pro_is_still_too_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T03:13:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ngtcbo</id>
    <title>ROCm 7.0 RC1 More than doubles performance of LLama.cpp</title>
    <updated>2025-09-14T14:54:35+00:00</updated>
    <author>
      <name>/u/no_no_no_oh_yes</name>
      <uri>https://old.reddit.com/user/no_no_no_oh_yes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ngtcbo/rocm_70_rc1_more_than_doubles_performance_of/"&gt; &lt;img alt="ROCm 7.0 RC1 More than doubles performance of LLama.cpp" src="https://b.thumbs.redditmedia.com/nC8p5-F1CvqamN5uzbhs383rN8pA1k1g6suH-gMyrXI.jpg" title="ROCm 7.0 RC1 More than doubles performance of LLama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EDIT: Added Vulkan data. My thought now is if we can use Vulkan for tg and rocm for pp :) &lt;/p&gt; &lt;p&gt;I was running a 9070XT and compiling Llama.cpp for it. Since performance felt a bit short vs my other 5070TI. I decided to try the new ROCm Drivers. The difference is impressive.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mqyfrxqk85pf1.png?width=1518&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b244b74b62ed1a14e4ff3d362c07f96a4f492d7e"&gt;ROCm 6.4.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b0if8fap85pf1.png?width=1557&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14df7c47c7d87e22e34f7751890a75982c004f45"&gt;ROCm 7.0 RC1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nry00uecp5pf1.png?width=1497&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=69be11a8f3a211e485c44db89dc0f3023cdbfaf6"&gt;Vulkan&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I installed ROCm following this instructions: &lt;a href="https://rocm.docs.amd.com/en/docs-7.0-rc1/preview/install/rocm.html"&gt;https://rocm.docs.amd.com/en/docs-7.0-rc1/preview/install/rocm.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And I had a compilation issue that I have to provide a new flag:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;-DCMAKE_POSITION_INDEPENDENT_CODE=ON The full compilation Flags: HIPCXX=&amp;quot;$(hipconfig -l)/clang&amp;quot; HIP_PATH=&amp;quot;$(hipconfig -R)&amp;quot; ROCBLAS_USE_HIPBLASLT=1 \ cmake -S . -B build \ -DGGML_HIP=ON \ -DAMDGPU_TARGETS=gfx1201 \ -DGGML_HIP_ROCWMMA_FATTN=ON \ -DCMAKE_BUILD_TYPE=Release \ -DBUILD_SHARED_LIBS=OFF \ -DCMAKE_POSITION_INDEPENDENT_CODE=ON &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/no_no_no_oh_yes"&gt; /u/no_no_no_oh_yes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ngtcbo/rocm_70_rc1_more_than_doubles_performance_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ngtcbo/rocm_70_rc1_more_than_doubles_performance_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ngtcbo/rocm_70_rc1_more_than_doubles_performance_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-14T14:54:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nh1wqy</id>
    <title>Will we see: Phi-5, Granite 4, Gemma 4, Deepseek R2, Llama 5, Mistral Small 4, Flux 2, Whisper 4?</title>
    <updated>2025-09-14T20:26:28+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's a lot to be looking forward to!&lt;/p&gt; &lt;p&gt;Do you think we'll see any of these any time soon? If so, wen? What would be your favorite? What would you look for in a new edition of your favorite model?&lt;/p&gt; &lt;p&gt;Seems a lot of attention has been around Qwen3 (rightly so) but there are other labs brewing and hopes are, that there's again a more diverse set of OS models with a competitive edge in the not so distant future.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh1wqy/will_we_see_phi5_granite_4_gemma_4_deepseek_r2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh1wqy/will_we_see_phi5_granite_4_gemma_4_deepseek_r2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nh1wqy/will_we_see_phi5_granite_4_gemma_4_deepseek_r2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-14T20:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhdi2u</id>
    <title>Update: we got our revenge and now beat Deepmind, Microsoft, Zhipu AI and Alibaba</title>
    <updated>2025-09-15T05:32:12+00:00</updated>
    <author>
      <name>/u/Connect-Employ-4708</name>
      <uri>https://old.reddit.com/user/Connect-Employ-4708</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Three weeks ago we open-sourced our agent that uses mobile apps like a human. At that moment, we were #2 on AndroidWorld (behind Zhipu AI).&lt;/p&gt; &lt;p&gt;Since, we worked hard and improved the performance of our agent: &lt;strong&gt;we’re now officially #1&lt;/strong&gt; on the &lt;a href="https://docs.google.com/spreadsheets/d/1cchzP9dlTZ3WXQTfYNhh3avxoLipqHN75v1Tb86uhHo/edit?pli=1&amp;amp;gid=0#gid=0"&gt;AndroidWorld leaderboard&lt;/a&gt;, surpassing Deepmind, Microsoft Research, Zhipu AI and Alibaba.&lt;/p&gt; &lt;p&gt;It handles mobile tasks: booking rides, ordering food, navigating apps, just like a human would. Still working on improvements and building an RL gym for fine-tuning :)&lt;/p&gt; &lt;p&gt;The agent is completely open-source: &lt;a href="http://github.com/minitap-ai/mobile-use"&gt;github.com/minitap-ai/mobile-use&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What mobile tasks would you want an AI agent to handle for you? Always looking for feedback and contributors!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Connect-Employ-4708"&gt; /u/Connect-Employ-4708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhdi2u/update_we_got_our_revenge_and_now_beat_deepmind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhdi2u/update_we_got_our_revenge_and_now_beat_deepmind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhdi2u/update_we_got_our_revenge_and_now_beat_deepmind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T05:32:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nh9pc9</id>
    <title>Qwen3‑Next‑80B‑A3B‑Instruct (FP8) on Windows 11 WSL2 + vLLM + Docker (Blackwell)</title>
    <updated>2025-09-15T02:11:54+00:00</updated>
    <author>
      <name>/u/IngeniousIdiocy</name>
      <uri>https://old.reddit.com/user/IngeniousIdiocy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used a LLM to summarize a lot of what I dealt with below. I wrote this because it doesn't exist anywhere on the internet as far as I can tell and you need to scour the internet to find the pieces to pull it together.&lt;/p&gt; &lt;p&gt;Generated content with my editing below:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;br /&gt; If you’re trying to serve &lt;strong&gt;Qwen3‑Next‑80B‑A3B‑Instruct FP8&lt;/strong&gt; on a &lt;strong&gt;Blackwell&lt;/strong&gt; card in &lt;strong&gt;WSL2&lt;/strong&gt;, pin: &lt;strong&gt;PyTorch 2.8.0 (cu128)&lt;/strong&gt;, &lt;strong&gt;vLLM 0.10.2&lt;/strong&gt;, &lt;strong&gt;FlashInfer ≥ 0.3.0 (0.3.1 preferred)&lt;/strong&gt;, and &lt;strong&gt;Transformers (main)&lt;/strong&gt;. Make sure you use the nightly cu128 container from vLLM and it can see &lt;code&gt;/dev/dxg&lt;/code&gt; and &lt;code&gt;/usr/lib/wsl/lib&lt;/code&gt; (so &lt;code&gt;libcuda.so.1&lt;/code&gt; resolves). I used a &lt;strong&gt;CUDA‑12.8 vLLM image&lt;/strong&gt; and mounted a small &lt;code&gt;run.sh&lt;/code&gt;to install the exact userspace combo and start the server. Without upgrading FlashInfer I got the infamous &lt;strong&gt;“FlashInfer requires sm75+”&lt;/strong&gt; crash on Blackwell. After bumping to &lt;strong&gt;0.3.1&lt;/strong&gt;, everything lit up, CUDA graphs enabled, and the OpenAI endpoints served normally. Running at 80 TPS output now single stream and 185 TPS over three streams. If you are leaning on Claude or Chatgpt to guide you through this then they will encourage you to to not use flashinfer or the cuda graphs but you can take advantage of both of these with the right versions of the stack, as shown below.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My setup&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OS:&lt;/strong&gt; Windows 11 + &lt;strong&gt;WSL2&lt;/strong&gt; (Ubuntu)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; &lt;strong&gt;RTX PRO 6000 Blackwell&lt;/strong&gt; (96 GB)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Serving:&lt;/strong&gt; &lt;strong&gt;vLLM&lt;/strong&gt; OpenAI‑compatible server&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;code&gt;TheClusterDev/Qwen3-Next-80B-A3B-Instruct-FP8-Dynamic&lt;/code&gt; (80B total, ~3B activated per token) Heads‑up: despite the 3B activated MoE, &lt;strong&gt;you still need VRAM for the full 80B weights&lt;/strong&gt;. FP8 helped, but it still occupied ~75 GiB on my box. You cannot do this with a quantization flag on the released model unless you have the memory for the 16bit weights. Also, you need the -dynamic version of this model from TheClusterDev to work with vLLM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The docker command I ended up with after much trial and error:&lt;br /&gt; docker run --rm --name vllm-qwen \&lt;/p&gt; &lt;p&gt;--gpus all \&lt;/p&gt; &lt;p&gt;--ipc=host \&lt;/p&gt; &lt;p&gt;-p 8000:8000 \&lt;/p&gt; &lt;p&gt;--entrypoint bash \&lt;/p&gt; &lt;p&gt;--device /dev/dxg \&lt;/p&gt; &lt;p&gt;-v /usr/lib/wsl/lib:/usr/lib/wsl/lib:ro \&lt;/p&gt; &lt;p&gt;-e LD_LIBRARY_PATH=&amp;quot;/usr/lib/wsl/lib:$LD_LIBRARY_PATH&amp;quot; \&lt;/p&gt; &lt;p&gt;-e HUGGING_FACE_HUB_TOKEN=&amp;quot;$HF_TOKEN&amp;quot; \&lt;/p&gt; &lt;p&gt;-e HF_TOKEN=&amp;quot;$HF_TOKEN&amp;quot; \&lt;/p&gt; &lt;p&gt;-e VLLM_ATTENTION_BACKEND=FLASHINFER \&lt;/p&gt; &lt;p&gt;-v &amp;quot;$HOME/.cache/huggingface:/root/.cache/huggingface&amp;quot; \&lt;/p&gt; &lt;p&gt;-v &amp;quot;$HOME/.cache/torch:/root/.cache/torch&amp;quot; \&lt;/p&gt; &lt;p&gt;-v &amp;quot;$HOME/.triton:/root/.triton&amp;quot; \&lt;/p&gt; &lt;p&gt;-v /data/models/qwen3_next_fp8:/models \&lt;/p&gt; &lt;p&gt;-v &amp;quot;$PWD/run-vllm-qwen.sh:/run.sh:ro&amp;quot; \&lt;/p&gt; &lt;p&gt;lmcache/vllm-openai:latest-nightly-cu128 \&lt;/p&gt; &lt;p&gt;-lc '/run.sh'&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why these flags matter:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;--device /dev/dxg&lt;/code&gt; + &lt;code&gt;-v /usr/lib/wsl/lib:...&lt;/code&gt; exposes the WSL GPU and &lt;strong&gt;WSL CUDA stubs&lt;/strong&gt; (e.g., &lt;code&gt;libcuda.so.1&lt;/code&gt;) to the container. Microsoft/NVIDIA docs confirm the WSL CUDA driver lives here. If you don’t mount this, PyTorch can’t dlopen &lt;code&gt;libcuda.so.1&lt;/code&gt; inside the container.&lt;/li&gt; &lt;li&gt;&lt;code&gt;-p 8000:8000&lt;/code&gt; + &lt;code&gt;--entrypoint bash -lc '/run.sh'&lt;/code&gt; runs my script (below) and binds vLLM on &lt;code&gt;0.0.0.0:8000&lt;/code&gt;(OpenAI‑compatible server). Official vLLM docs describe the OpenAI endpoints (&lt;code&gt;/v1/chat/completions&lt;/code&gt;, etc.).&lt;/li&gt; &lt;li&gt;The CUDA &lt;strong&gt;12.8&lt;/strong&gt; image matches &lt;strong&gt;PyTorch 2.8&lt;/strong&gt; and &lt;strong&gt;vLLM 0.10.2&lt;/strong&gt; expectations (vLLM 0.10.2 upgraded to &lt;strong&gt;PT 2.8&lt;/strong&gt; and &lt;strong&gt;FlashInfer 0.3.0&lt;/strong&gt;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why I bothered with a shell script:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The stock image didn’t have the exact combo I needed for &lt;strong&gt;Blackwell + Qwen3‑Next&lt;/strong&gt; (and I wanted CUDA graphs + FlashInfer active). The script:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Verifies &lt;code&gt;libcuda.so.1&lt;/code&gt; is loadable (from &lt;code&gt;/usr/lib/wsl/lib&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Pins &lt;strong&gt;Torch 2.8.0 cu128&lt;/strong&gt;, &lt;strong&gt;vLLM 0.10.2&lt;/strong&gt;, &lt;strong&gt;Transformers main&lt;/strong&gt;, &lt;strong&gt;FlashInfer 0.3.1&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Prints a small sanity block (Torch CUDA on, vLLM native import OK, FI version)&lt;/li&gt; &lt;li&gt;Serves the model with OpenAI‑compatible endpoints&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It’s short, reproducible, and keeps the Docker command clean.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;References that helped me pin the stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;FlashInfer ≥ 0.3.0&lt;/strong&gt;: &lt;strong&gt;SM120/121&lt;/strong&gt; bring‑up + FP8 GEMM for Blackwell (fixes the “requires sm75+” path). &lt;a href="https://github.com/flashinfer-ai/flashinfer/releases"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;vLLM 0.10.2 release&lt;/strong&gt;: upgrades to &lt;strong&gt;PyTorch 2.8.0&lt;/strong&gt;, &lt;strong&gt;FlashInfer 0.3.0&lt;/strong&gt;, adds &lt;strong&gt;Qwen3‑Next&lt;/strong&gt; hybrid attention, enables &lt;strong&gt;full CUDA graphs&lt;/strong&gt; by default for hybrid, disables prefix cache for hybrid/Mamba. &lt;a href="https://github.com/vllm-project/vllm/releases"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI‑compatible server docs&lt;/strong&gt; (endpoints, clients): &lt;a href="https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html?utm_source=chatgpt.com"&gt;VLLM Documentation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;WSL CUDA&lt;/strong&gt; (why &lt;code&gt;/usr/lib/wsl/lib&lt;/code&gt; and &lt;code&gt;/dev/dxg&lt;/code&gt; matter): &lt;a href="https://learn.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl?utm_source=chatgpt.com"&gt;Microsoft Learn+1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;cu128 wheel index&lt;/strong&gt; (for PT 2.8 stack alignment): &lt;a href="https://download.pytorch.org/whl/cu128?utm_source=chatgpt.com"&gt;PyTorch Download&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3‑Next 80B model card/discussion&lt;/strong&gt; (80B total, ~3B activated per token; still need full weights in VRAM): &lt;a href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct?utm_source=chatgpt.com"&gt;Hugging Face+1&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The tiny shell script that made it work:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The base image didn’t have the right userspace stack for Blackwell + Qwen3‑Next, so I install/verify &lt;strong&gt;exact versions&lt;/strong&gt; and then &lt;code&gt;vllm serve&lt;/code&gt;. Key bits:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pin &lt;strong&gt;Torch 2.8.0 + cu128&lt;/strong&gt; from the PyTorch cu128 wheel index&lt;/li&gt; &lt;li&gt;Install &lt;strong&gt;vLLM 0.10.2&lt;/strong&gt; (aligned to PT 2.8)&lt;/li&gt; &lt;li&gt;Install &lt;strong&gt;Transformers (main)&lt;/strong&gt; (for &lt;strong&gt;Qwen3‑Next&lt;/strong&gt; hybrid arch)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Crucial:&lt;/strong&gt; &lt;strong&gt;FlashInfer 0.3.1&lt;/strong&gt; (0.3.0+ adds &lt;strong&gt;SM120/SM121&lt;/strong&gt; bring‑up + FP8 GEMM; fixed the “requires sm75+” crash I saw)&lt;/li&gt; &lt;li&gt;Sanity‑check &lt;code&gt;libcuda.so.1&lt;/code&gt;, torch CUDA, and vLLM native import before serving&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’ve inlined the updated script here as a reference (trimmed to relevant bits);&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# ... preflight: detect /dev/dxg and export LD_LIBRARY_PATH=/usr/lib/wsl/lib ... # Torch 2.8.0 (CUDA 12.8 wheels) pip install -U --index-url https://download.pytorch.org/whl/cu128 \ &amp;quot;torch==2.8.0+cu128&amp;quot; &amp;quot;torchvision==0.23.0+cu128&amp;quot; &amp;quot;torchaudio==2.8.0+cu128&amp;quot; # vLLM 0.10.2 pip install -U &amp;quot;vllm==0.10.2&amp;quot; --extra-index-url &amp;quot;https://wheels.vllm.ai/0.10.2/&amp;quot; # Transformers main (Qwen3NextForCausalLM) pip install -U https://github.com/huggingface/transformers/archive/refs/heads/main.zip # FlashInfer (Blackwell-ready) pip install -U --no-deps &amp;quot;flashinfer-python==0.3.1&amp;quot; # (0.3.0 also OK) # Serve (OpenAI-compatible) vllm serve TheClusterDev/Qwen3-Next-80B-A3B-Instruct-FP8-Dynamic \ --download-dir /models --host 0.0.0.0 --port 8000 \ --served-model-name qwen3-next-fp8 \ --max-model-len 32768 --gpu-memory-utilization 0.92 \ --max-num-batched-tokens 8192 --max-num-seqs 128 --trust-remote-code &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IngeniousIdiocy"&gt; /u/IngeniousIdiocy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh9pc9/qwen3next80ba3binstruct_fp8_on_windows_11_wsl2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh9pc9/qwen3next80ba3binstruct_fp8_on_windows_11_wsl2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nh9pc9/qwen3next80ba3binstruct_fp8_on_windows_11_wsl2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T02:11:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nh86i7</id>
    <title>vLLM is kinda awesome</title>
    <updated>2025-09-15T00:58:41+00:00</updated>
    <author>
      <name>/u/Secure_Reflection409</name>
      <uri>https://old.reddit.com/user/Secure_Reflection409</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh86i7/vllm_is_kinda_awesome/"&gt; &lt;img alt="vLLM is kinda awesome" src="https://b.thumbs.redditmedia.com/sCRN2mdOtD5l5xVe1j7kNCfWMccS6tEJIyjvpJoPh9I.jpg" title="vLLM is kinda awesome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vs0d2b3098pf1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1071464f396e186209597ad86acf5ac891b6bf8"&gt;https://preview.redd.it/vs0d2b3098pf1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1071464f396e186209597ad86acf5ac891b6bf8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The last time I ran this test on this card via &lt;strong&gt;LCP&lt;/strong&gt; it took &lt;strong&gt;2 hours 46 minutes 17 seconds&lt;/strong&gt;:&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mjceor/qwen3_30b_2507_thinking_benchmarks/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mjceor/qwen3_30b_2507_thinking_benchmarks/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This time via &lt;strong&gt;vLLM&lt;/strong&gt;? &lt;strong&gt;14 minutes 1 second&lt;/strong&gt; :D&lt;br /&gt; vLLM is a game changer for benchmarking and it just so happens on this run I slightly beat my score from last time too (83.90% vs 83.41%): &lt;/p&gt; &lt;pre&gt;&lt;code&gt;(vllm_env) tests@3090Ti:~/Ollama-MMLU-Pro$ python run_openai.py 2025-09-15 01:09:13.078761 { &amp;quot;comment&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;server&amp;quot;: { &amp;quot;url&amp;quot;: &amp;quot;http://localhost:8000/v1&amp;quot;, &amp;quot;model&amp;quot;: &amp;quot;Qwen3-30B-A3B-Thinking-2507-AWQ-4bit&amp;quot;, &amp;quot;timeout&amp;quot;: 600.0 }, &amp;quot;inference&amp;quot;: { &amp;quot;temperature&amp;quot;: 0.6, &amp;quot;top_p&amp;quot;: 0.95, &amp;quot;max_tokens&amp;quot;: 16384, &amp;quot;system_prompt&amp;quot;: &amp;quot;The following are multiple choice questions (with answers) about {subject}. Think step by step and then finish your answer with \&amp;quot;the answer is (X)\&amp;quot; where X is the correct letter choice.&amp;quot;, &amp;quot;style&amp;quot;: &amp;quot;multi_chat&amp;quot; }, &amp;quot;test&amp;quot;: { &amp;quot;subset&amp;quot;: 1.0, &amp;quot;parallel&amp;quot;: 16 }, &amp;quot;log&amp;quot;: { &amp;quot;verbosity&amp;quot;: 0, &amp;quot;log_prompt&amp;quot;: true } } assigned subjects ['computer science'] computer science: 100%|######################################################################################################| 410/410 [14:01&amp;lt;00:00, 2.05s/it, Correct=344, Wrong=66, Accuracy=83.90] Finished testing computer science in 14 minutes 1 seconds. Total, 344/410, 83.90% Random Guess Attempts, 0/410, 0.00% Correct Random Guesses, division by zero error Adjusted Score Without Random Guesses, 344/410, 83.90% Finished the benchmark in 14 minutes 3 seconds. Total, 344/410, 83.90% Token Usage: Prompt tokens: min 1448, average 1601, max 2897, total 656306, tk/s 778.12 Completion tokens: min 61, average 1194, max 16384, total 489650, tk/s 580.53 Markdown Table: | overall | computer science | | ------- | ---------------- | | 83.90 | 83.90 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is super basic out of the box stuff really. I see loads of warnings in the vLLM startup for things that need to be optimised.&lt;/p&gt; &lt;p&gt;vLLM runtime args (Primary 3090Ti only):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm serve cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit --tensor-parallel-size 1 --max-model-len 40960 --max-num-seqs 16 --served-model-name Qwen3-30B-A3B-Thinking-2507-AWQ-4bit &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;During the run, the vLLM console would show things like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;(APIServer pid=23678) INFO 09-15 01:20:40 [loggers.py:123] Engine 000: Avg prompt throughput: 1117.7 tokens/s, Avg generation throughput: 695.3 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 79.9%, Prefix cache hit rate: 79.5% (APIServer pid=23678) INFO: 127.0.0.1:52368 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52370 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52368 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52322 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52368 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52268 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO 09-15 01:20:50 [loggers.py:123] Engine 000: Avg prompt throughput: 919.6 tokens/s, Avg generation throughput: 687.4 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 88.9%, Prefix cache hit rate: 79.2% (APIServer pid=23678) INFO: 127.0.0.1:52278 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52370 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52268 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52322 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52278 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52268 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52370 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO 09-15 01:21:00 [loggers.py:123] Engine 000: Avg prompt throughput: 1072.6 tokens/s, Avg generation throughput: 674.5 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 90.3%, Prefix cache hit rate: 79.1% &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I did do a small bit of benchmarking before this run as I have 2 x 3090Ti but one sits in a crippled x1 slot. 16 threads seems like the sweet spot. At 32 threads MMLU-Pro correct answer rate nose dived.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Single request&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# 1 parallel request - primary card - 512 prompt Throughput: 1.14 requests/s, 724.81 total tokens/s, 145.42 output tokens/s Total num prompt tokens: 50997 Total num output tokens: 12800 (vllm_env) tests@3090Ti:~$ vllm bench throughput --model cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit --tensor-parallel-size 1 --max-model-len 32768 --max-num-seqs 1 --input-len 512 --num-prompts 100 # 1 parallel request - both cards - 512 prompt Throughput: 0.71 requests/s, 453.38 total tokens/s, 90.96 output tokens/s Total num prompt tokens: 50997 Total num output tokens: 12800 (vllm_env) tests@3090Ti:~$ vllm bench throughput --model cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit --tensor-parallel-size 2 --max-model-len 32768 --max-num-seqs 1 --input-len 512 --num-prompts 100 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;8 requests&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# 8 parallel requests - primary card Throughput: 4.17 requests/s, 2660.79 total tokens/s, 533.85 output tokens/s Total num prompt tokens: 50997 Total num output tokens: 12800 (vllm_env) tests@3090Ti:~$ vllm bench throughput --model cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit --tensor-parallel-size 1 --max-model-len 32768 --max-num-seqs 8 --input-len 512 --num-prompts 100 # 8 parallel requests - both cards Throughput: 2.02 requests/s, 1289.21 total tokens/s, 258.66 output tokens/s Total num prompt tokens: 50997 Total num output tokens: 12800 (vllm_env) tests@3090Ti:~$ vllm bench throughput --model cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit --tensor-parallel-size 2 --max-model-len 32768 --max-num-seqs 8 --input-len 512 --num-prompts 100 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;16, 32, 64 requests - primary only&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# 16 parallel requests - primary card - 100 prompts Throughput: 5.69 requests/s, 3631.00 total tokens/s, 728.51 output tokens/s Total num prompt tokens: 50997 Total num output tokens: 12800 (vllm_env) tests@3090Ti:~$ vllm bench throughput --model cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit --tensor-parallel-size 1 --max-model-len 32768 --max-num-seqs 16 --input-len 512 --num-prompts 100 # 32 parallel requests - primary card - 200 prompts (100 was completing too fast it seemed) Throughput: 7.27 requests/s, 4643.05 total tokens/s, 930.81 output tokens/s Total num prompt tokens: 102097 Total num output tokens: 25600 (vllm_env) tests@3090Ti:~$ vllm bench throughput --model cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit --tensor-parallel-size 1 --max-model-len 32768 --max-num-seqs 32 --input-len 512 --num-prompts 200 # 64 parallel requests - primary card - 200 prompts Throughput: 8.54 requests/s, 5454.48 total tokens/s, 1093.48 output tokens/s Total num prompt tokens: 102097 Total num output tokens: 25600 (vllm_env) tests@3090Ti:~$ vllm bench throughput --model cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit --tensor-parallel-size 1 --max-model-len 32768 --max-num-seqs 64 --input-len 512 --num-prompts 200 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Reflection409"&gt; /u/Secure_Reflection409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh86i7/vllm_is_kinda_awesome/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh86i7/vllm_is_kinda_awesome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nh86i7/vllm_is_kinda_awesome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T00:58:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nh5fn0</id>
    <title>Spent 4 months building Unified Local AI Workspace - ClaraVerse v0.2.0 instead of just dealing with 5+ Local AI Setup like everyone else</title>
    <updated>2025-09-14T22:51:26+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh5fn0/spent_4_months_building_unified_local_ai/"&gt; &lt;img alt="Spent 4 months building Unified Local AI Workspace - ClaraVerse v0.2.0 instead of just dealing with 5+ Local AI Setup like everyone else" src="https://preview.redd.it/3bgm4ig3i7pf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06b067861ce9011b554dcf16b3efc513bcd5b061" title="Spent 4 months building Unified Local AI Workspace - ClaraVerse v0.2.0 instead of just dealing with 5+ Local AI Setup like everyone else" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ClaraVerse v0.2.0 - Unified Local AI Workspace (Chat, Agent, ImageGen, Rag &amp;amp; N8N)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Spent 4 months building ClaraVerse instead of just using multiple AI apps like a normal person&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Posted here in April when it was pretty rough and got some reality checks from the community. Kept me going though - people started posting about it on YouTube and stuff.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The basic idea:&lt;/strong&gt; Everything's just LLMs and diffusion models anyway, so why do we need separate apps for everything? Built ClaraVerse to put it all in one place.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's actually working in v0.2.0:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Chat with local models (built-in llama.cpp) or any provider with MCP, Tools, N8N workflow as tools&lt;/li&gt; &lt;li&gt;Generate images with ComfyUI integration&lt;/li&gt; &lt;li&gt;Build agents with visual editor (drag and drop automation)&lt;/li&gt; &lt;li&gt;RAG notebooks with 3D knowledge graphs&lt;/li&gt; &lt;li&gt;N8N workflows for external stuff&lt;/li&gt; &lt;li&gt;Web dev environment (LumaUI)&lt;/li&gt; &lt;li&gt;Community marketplace for sharing workflows&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The modularity thing:&lt;/strong&gt; Everything connects to everything else. Your chat assistant can trigger image generation, agents can update your knowledge base, workflows can run automatically. It's like LEGO blocks but for AI tools.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Reality check:&lt;/strong&gt; Still has rough edges (it's only 4 months old). But 20k+ downloads and people are building interesting stuff with it, so the core idea seems to work.&lt;/p&gt; &lt;p&gt;Everything runs local, MIT licensed. Built-in llama.cpp with model downloads, manager but works with any provider.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt; GitHub: &lt;a href="http://github.com/badboysm890/ClaraVerse"&gt;github.com/badboysm890/ClaraVerse&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone tried building something similar? Curious if this resonates with other people or if I'm just weird about wanting everything in one app.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3bgm4ig3i7pf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh5fn0/spent_4_months_building_unified_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nh5fn0/spent_4_months_building_unified_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-14T22:51:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhd5ks</id>
    <title>Completed 8xAMD MI50 - 256GB VRAM + 256GB RAM rig for $3k</title>
    <updated>2025-09-15T05:11:27+00:00</updated>
    <author>
      <name>/u/MLDataScientist</name>
      <uri>https://old.reddit.com/user/MLDataScientist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhd5ks/completed_8xamd_mi50_256gb_vram_256gb_ram_rig_for/"&gt; &lt;img alt="Completed 8xAMD MI50 - 256GB VRAM + 256GB RAM rig for $3k" src="https://b.thumbs.redditmedia.com/fnXtmv3xMQuxl9xhdyUEPfnB7lkOh7QN0YQdqGLHvKc.jpg" title="Completed 8xAMD MI50 - 256GB VRAM + 256GB RAM rig for $3k" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;A few months ago I posted about how I was able to purchase 4xMI50 for $600 and run them using my consumer PC. Each GPU could run at PCIE3.0 x4 speed and my consumer PC did not have enough PCIE lanes to support more than 6x GPUs. My final goal was to run all 8 GPUs at proper PCIE4.0 x16 speed. &lt;/p&gt; &lt;p&gt;I was finally able to complete my setup. Cost breakdown:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ASRock ROMED8-2T Motherboard with 8x32GB DDR4 3200Mhz and AMD Epyc 7532 CPU (32 cores), dynatron 2U heatsink - $1000&lt;/li&gt; &lt;li&gt;6xMI50 and 2xMI60 - $1500&lt;/li&gt; &lt;li&gt;10x blower fans (all for $60), 1300W PSU ($120) + 850W PSU (already had this), 6x 300mm riser cables (all for $150), 3xPCIE 16x to 8x8x bifurcation cards (all for $70), 8x PCIE power cables and fan power controller (for $100)&lt;/li&gt; &lt;li&gt;GTX 1650 4GB for video output (already had this)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In total, I spent around ~$3k for this rig. All used parts.&lt;/p&gt; &lt;p&gt;ASRock ROMED8-2T was an ideal motherboard for me due to its seven x16 full physical PCIE4.0 slots.&lt;/p&gt; &lt;p&gt;Attached photos below.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b052o7hi99pf1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=20fb34bd86438c2a2111fb0eb52a70b26b3b9685"&gt;8xMI50/60 32GB with GTX 1650 top view&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cnnr3ixn99pf1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=273be5463afc2508a46f17ea5e63b6e6de51b5fb"&gt;8xMI50/60 32GB in open frame rack with motherboard and PSU. My consumer PC is on the right side (not used here)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have not done many LLM tests yet. PCIE4.0 connection was not stable since I am using longer PCIE risers. So, I kept the speed for each PCIE slot at 3.0 x16. Some initial performance metrics are below. Installed Ubuntu 24.04.3 with ROCm 6.4.3 (needed to copy paste gfx906 tensiles to fix deprecated support).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU alone: gpt-oss 120B (65GB Q8) runs at ~25t/s with ~120t/s prompt processing (llama.cpp)&lt;/li&gt; &lt;li&gt;2xMI50: gpt-oss 120B (65GB Q8) runs at ~58t/s with 750t/s prompt processing (llama.cpp)&lt;/li&gt; &lt;li&gt;8xMI50: qwen3 235B Q4_1 runs at ~21t/s with 350t/s prompt processing (llama.cpp)&lt;/li&gt; &lt;li&gt;2xMI60 vllm gfx906: llama3.3 70B AWQ: 25t/s with ~240 t/s prompt processing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Idle power consumption is around ~400W (20w for each GPU, 15w for each blower fan, ~100W for motherboard, RAM, fan and CPU). llama.cpp inference averages around 750W (using wall meter). For a few seconds during inference, the power spikes up to 1100W&lt;/p&gt; &lt;p&gt;I will do some more performance tests. Overall, I am happy with what I was able to build and run. &lt;/p&gt; &lt;p&gt;Fun fact: the entire rig costs around the same price as a single RTX 5090 (variants like ASUS TUF).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLDataScientist"&gt; /u/MLDataScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhd5ks/completed_8xamd_mi50_256gb_vram_256gb_ram_rig_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhd5ks/completed_8xamd_mi50_256gb_vram_256gb_ram_rig_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhd5ks/completed_8xamd_mi50_256gb_vram_256gb_ram_rig_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T05:11:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
