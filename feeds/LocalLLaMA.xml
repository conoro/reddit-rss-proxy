<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-21T13:09:05+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nm4b0q</id>
    <title>Efficient 4B parameter gpt OSS distillation without the over-censorship</title>
    <updated>2025-09-20T17:32:58+00:00</updated>
    <author>
      <name>/u/ApprehensiveTart3158</name>
      <uri>https://old.reddit.com/user/ApprehensiveTart3158</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've personally loved using gpt oss, but it wasn't very fast locally and was totally over censored. &lt;/p&gt; &lt;p&gt;So I've thought about it and made a fine tune of qwen3 4B thinking on GPT OSS outputs, with MOST of the &amp;quot;I can't comply with that&amp;quot; removed from the fine tuning dataset. &lt;/p&gt; &lt;p&gt;You can find it here: &lt;a href="https://huggingface.co/Pinkstack/DistilGPT-OSS-qwen3-4B"&gt;https://huggingface.co/Pinkstack/DistilGPT-OSS-qwen3-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yes, it is small and no it cannot be properly used for speculative decoding but it is pretty cool to play around with and it is very fast. &lt;/p&gt; &lt;p&gt;From my personal testing (note, not benchmarked yet as that does take quite a bit of compute that I don't have right now): Reasoning efforts (low, high, medium) all works as intended and absolutely do change how long the model thinks which is huge. It thinks almost exactly like gpt oss and yes it does think about &amp;quot;policies&amp;quot; but from what I've seen with high reasoning it may start thinking about rejecting then convince itself to answer.. Lol(for example if you ask it to let's say swear at you, it would most of the time comply), unless what you asked is really unsafe it would probably comply, and it feels exactly like gpt oss, same style of code, almost identical output styles just not as much general knowledge as it is just 4b parameters!!&lt;/p&gt; &lt;p&gt;If you have questions or want to share something please comment and let me know, would live to hear what you think! :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveTart3158"&gt; /u/ApprehensiveTart3158 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm4b0q/efficient_4b_parameter_gpt_oss_distillation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm4b0q/efficient_4b_parameter_gpt_oss_distillation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm4b0q/efficient_4b_parameter_gpt_oss_distillation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T17:32:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmm9hy</id>
    <title>Best way to benchmark offline LLMs?</title>
    <updated>2025-09-21T08:08:56+00:00</updated>
    <author>
      <name>/u/YT_Brian</name>
      <uri>https://old.reddit.com/user/YT_Brian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wondering if anyone had a favorite way to test your PC for benchmarking, specific LLM you use just for that or prompt, that type of thing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YT_Brian"&gt; /u/YT_Brian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmm9hy/best_way_to_benchmark_offline_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmm9hy/best_way_to_benchmark_offline_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmm9hy/best_way_to_benchmark_offline_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T08:08:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmf0hw</id>
    <title>New E-commerce encoders in town: RexBERT</title>
    <updated>2025-09-21T01:16:54+00:00</updated>
    <author>
      <name>/u/Minute_Smile5698</name>
      <uri>https://old.reddit.com/user/Minute_Smile5698</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HF blog published: &lt;a href="https://huggingface.co/blog/thebajajra/rexbert-encoders"&gt;https://huggingface.co/blog/thebajajra/rexbert-encoders&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Outperforms ModernBERT&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Minute_Smile5698"&gt; /u/Minute_Smile5698 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmf0hw/new_ecommerce_encoders_in_town_rexbert/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmf0hw/new_ecommerce_encoders_in_town_rexbert/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmf0hw/new_ecommerce_encoders_in_town_rexbert/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T01:16:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmp3m1</id>
    <title>Life Coach / Diary - Best Model? (for “average PC”)</title>
    <updated>2025-09-21T11:04:18+00:00</updated>
    <author>
      <name>/u/Plastic-Educator-129</name>
      <uri>https://old.reddit.com/user/Plastic-Educator-129</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to build a simple local app that I can talk with, have my chats documented, and then receive advice… Essentially a life coach and diary.&lt;/p&gt; &lt;p&gt;Is there a model I should use from Ollama or should I use a free API such as the Google Gemini one? &lt;/p&gt; &lt;p&gt;I have a tower PC that has around 32 GB of RAM, an AMD RX 7800 GPU and AMD Ryzen CPU. And then another older tower PC with a RX480 which is much slower. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plastic-Educator-129"&gt; /u/Plastic-Educator-129 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmp3m1/life_coach_diary_best_model_for_average_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmp3m1/life_coach_diary_best_model_for_average_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmp3m1/life_coach_diary_best_model_for_average_pc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T11:04:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmq1m2</id>
    <title>Are LLMs good at modifying Large SQLs correctly?</title>
    <updated>2025-09-21T11:57:50+00:00</updated>
    <author>
      <name>/u/help_all</name>
      <uri>https://old.reddit.com/user/help_all</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;My problem : Run KPIs using LLM.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;the tool must take SQL of the KPI, modify it using the user question and generate right SQL which will be executed to get data.&lt;/p&gt; &lt;p&gt;The problem is the KPIs have large and complex SQLs involving multiple joins, group by etc. I am not able to get LLM giving me right SQL.&lt;/p&gt; &lt;p&gt;E.g. The user may ask question - &amp;quot;Break down last week's stock-on-hands by division numbers&amp;quot;. The SQL for KPI is quite large and complex (close to 90 lines). In the context of the given question, it should just give me final results grouped by Division number.&lt;/p&gt; &lt;p&gt;What is the best way to get the final SQL generate correctly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/help_all"&gt; /u/help_all &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmq1m2/are_llms_good_at_modifying_large_sqls_correctly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmq1m2/are_llms_good_at_modifying_large_sqls_correctly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmq1m2/are_llms_good_at_modifying_large_sqls_correctly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T11:57:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmq7pw</id>
    <title>Best model for humour?</title>
    <updated>2025-09-21T12:06:12+00:00</updated>
    <author>
      <name>/u/ANONYMOUS_GAMER_07</name>
      <uri>https://old.reddit.com/user/ANONYMOUS_GAMER_07</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made this post over an year ago... but I couldn't find any model that could actually make someone laugh or atleast smirk. I tried jailbreak system prompts, custom rp comedy conversations, tried local models finetuned for roleplay... but I am yet to see any such model.&lt;br /&gt; Maybe GPT-4o got close to that for many people, which we learnt after the 4o removal and reinstation debacle... but still I wouldn't really call it &amp;quot;humour&amp;quot;&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1f4yuh1/best_model_for_humour/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1f4yuh1/best_model_for_humour/&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Most of the LLMs I've used have very boring, synthetic, sounding Humour... and they don't generate anything new or original or creative. So, are there any models which can write jokes which don't sound like toddler-humour?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Do we have anything now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ANONYMOUS_GAMER_07"&gt; /u/ANONYMOUS_GAMER_07 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmq7pw/best_model_for_humour/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmq7pw/best_model_for_humour/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmq7pw/best_model_for_humour/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T12:06:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm9uye</id>
    <title>My first local run using Magistral 1.2 - 4 bit and I'm thrilled to bits (no pun intended)</title>
    <updated>2025-09-20T21:17:42+00:00</updated>
    <author>
      <name>/u/picturpoet</name>
      <uri>https://old.reddit.com/user/picturpoet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm9uye/my_first_local_run_using_magistral_12_4_bit_and/"&gt; &lt;img alt="My first local run using Magistral 1.2 - 4 bit and I'm thrilled to bits (no pun intended)" src="https://preview.redd.it/0dhjzmgzydqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c79b9409fbebd0a546d3ef854d3b29ce2460c94e" title="My first local run using Magistral 1.2 - 4 bit and I'm thrilled to bits (no pun intended)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My Mac Studio M4 Max base model just came through and I was so excited to run something locally having always depended on cloud based models.&lt;/p&gt; &lt;p&gt;I don't know what use cases I will build yet but just so exciting that there's a new fun model available to try the moment I began.&lt;/p&gt; &lt;p&gt;Any ideas of what I should do next on my Local Llama roadmap and how I can get to being an intermediate localllm user from my current noob status is fully appreciated. 😄&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/picturpoet"&gt; /u/picturpoet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0dhjzmgzydqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm9uye/my_first_local_run_using_magistral_12_4_bit_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm9uye/my_first_local_run_using_magistral_12_4_bit_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T21:17:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nly3w1</id>
    <title>Qwen 3 VL next week</title>
    <updated>2025-09-20T13:24:32+00:00</updated>
    <author>
      <name>/u/Long_Bluejay_5368</name>
      <uri>https://old.reddit.com/user/Long_Bluejay_5368</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nly3w1/qwen_3_vl_next_week/"&gt; &lt;img alt="Qwen 3 VL next week" src="https://a.thumbs.redditmedia.com/fLB-QxQX_aAn0F5HXNaiy2dlb5JbWlBjS-VuT3q3TC0.jpg" title="Qwen 3 VL next week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/19dxif2kmbqf1.png?width=788&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56a8c11d753f68cacd685640484117a43de99ce3"&gt;https://preview.redd.it/19dxif2kmbqf1.png?width=788&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56a8c11d753f68cacd685640484117a43de99ce3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;what do you think about it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Long_Bluejay_5368"&gt; /u/Long_Bluejay_5368 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nly3w1/qwen_3_vl_next_week/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nly3w1/qwen_3_vl_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nly3w1/qwen_3_vl_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T13:24:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmr43i</id>
    <title>Is Qwen3 4B enough?</title>
    <updated>2025-09-21T12:50:03+00:00</updated>
    <author>
      <name>/u/Dreamingmathscience</name>
      <uri>https://old.reddit.com/user/Dreamingmathscience</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to run my coding agent locally so I am looking for a appropriate model. &lt;/p&gt; &lt;p&gt;I don't really need tool calling abilities. Instead I want better quality of the generated code. &lt;/p&gt; &lt;p&gt;I am finding 4B to 10B models and if they don't have dramatic code quality diff I prefer the small one. &lt;/p&gt; &lt;p&gt;Is Qwen3 enough for me? Is there any alternative? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dreamingmathscience"&gt; /u/Dreamingmathscience &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmr43i/is_qwen3_4b_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmr43i/is_qwen3_4b_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmr43i/is_qwen3_4b_enough/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T12:50:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm0mzw</id>
    <title>Whisper Large v3 running in real-time on a M2 Macbook Pro</title>
    <updated>2025-09-20T15:08:21+00:00</updated>
    <author>
      <name>/u/rruk01</name>
      <uri>https://old.reddit.com/user/rruk01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm0mzw/whisper_large_v3_running_in_realtime_on_a_m2/"&gt; &lt;img alt="Whisper Large v3 running in real-time on a M2 Macbook Pro" src="https://external-preview.redd.it/NnkxeHk1bTIxY3FmMbdFe5hFZkGFnrWFqBq5GQzhAAe-tezJH5BHnp8SS6Dh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4c3798d766be6b03e6447b1663fb9590cdfcffe" title="Whisper Large v3 running in real-time on a M2 Macbook Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on using the Whisper models on device for 2-3 years now and wanted to share my progress. &lt;/p&gt; &lt;p&gt;I've figured out several optimisations which combined together means I can run the Whisper Large v3 (not turbo) model on a macbook with about 350-600ms latency for live (hypothesis/cyan) requests and 900-1200ms for completed (white) requests. It can also run on an iPhone 14 Pro with about 650-850ms latency for live requests and 1900ms for completed requests. The optimisations work for all the Whisper models and would probably work for the NVIDIA Parakeet / Canary models too. &lt;/p&gt; &lt;p&gt;The optimisations include speeding up the encoder on Apple Neural Engine so it runs at &lt;strong&gt;150ms&lt;/strong&gt; per run, this is compared to a naive 'ANE-optimised' encoder which runs at about &lt;strong&gt;500ms&lt;/strong&gt;. This does not require significant quantisation. The model running in the demo is quantised at Q8, but mainly so it takes up less hard-disk space, FP16 runs at similar speed. I've also optimised hypothesis requests so the output is much more stable. &lt;/p&gt; &lt;p&gt;If there's interest I'd be happy to write up a blog post on these optimisations, I'm also considering making an open source SDK so people can run this themselves, again if there's interest. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rruk01"&gt; /u/rruk01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2ibrz4m21cqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm0mzw/whisper_large_v3_running_in_realtime_on_a_m2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm0mzw/whisper_large_v3_running_in_realtime_on_a_m2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T15:08:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmn66e</id>
    <title>Running LLMs locally with iGPU or CPU not dGPU (keep off plz lol)? Post t/s</title>
    <updated>2025-09-21T09:05:37+00:00</updated>
    <author>
      <name>/u/General-Cookie6794</name>
      <uri>https://old.reddit.com/user/General-Cookie6794</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This thread may help a middle to low rage laptop buyer make a decision. Any hardware is welcomed weather new or old, snapdragon elite, Intel, AMD. Not for Dedicated GPU users. &lt;/p&gt; &lt;p&gt;Post your hardware(laptop type ram size and speed if possible, CPU type), AI model and if using lmstudio or ollama we want to see token generation in t/s. Prefil tokens is optional. Some clips maybe useful.&lt;/p&gt; &lt;p&gt;Let's go &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/General-Cookie6794"&gt; /u/General-Cookie6794 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmn66e/running_llms_locally_with_igpu_or_cpu_not_dgpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmn66e/running_llms_locally_with_igpu_or_cpu_not_dgpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmn66e/running_llms_locally_with_igpu_or_cpu_not_dgpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T09:05:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlu3cd</id>
    <title>The iPhone 17 Pro can run LLMs fast!</title>
    <updated>2025-09-20T09:53:52+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlu3cd/the_iphone_17_pro_can_run_llms_fast/"&gt; &lt;img alt="The iPhone 17 Pro can run LLMs fast!" src="https://a.thumbs.redditmedia.com/lazvh4ZugenSKXRU1IYFLEO6hichFPkV7Tw3LqJ6h_8.jpg" title="The iPhone 17 Pro can run LLMs fast!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The new A19 Pro finally integrates neural accelerators into the GPU cores themselves, essentially Apple’s version of Nvidia’s Tensor cores which are used for accelerating matrix multiplication that is prevalent in the transformers models we love so much. So I thought it would be interesting to test out running our smallest finetuned models on it!&lt;/p&gt; &lt;p&gt;Boy does the GPU fly compared to running the model only on CPU. The token generation is only about double but the prompt processing is over 10x faster! It’s so much faster that it’s actually usable even on longer context as the prompt processing doesn’t quickly become too long and the token generation speed is still high.&lt;/p&gt; &lt;p&gt;I tested using the Pocket Pal app on IOS which runs regular llamacpp with MLX Metal optimizations as far as I know. Shown are the comparison of the model running on GPU fully offloaded with Metal API and flash attention enabled vs running on CPU only. &lt;/p&gt; &lt;p&gt;Judging by the token generation speed, the A19 Pro must have about 70-80GB/s of memory bandwidth to the GPU and the CPU can access only about half of that bandwidth. &lt;/p&gt; &lt;p&gt;Anyhow the new GPU with the integrated tensor cores now look very interesting for running LLMs. Perhaps when new Mac Studios with updated M chips comes out with a big version of this new GPU architecture, I might even be able to use them to serve models for our low cost API. 🤔&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nlu3cd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlu3cd/the_iphone_17_pro_can_run_llms_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlu3cd/the_iphone_17_pro_can_run_llms_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T09:53:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmeq9w</id>
    <title>Llama.cpp support for Ling Mini 2.0 is probably coming next week</title>
    <updated>2025-09-21T01:02:01+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeq9w/llamacpp_support_for_ling_mini_20_is_probably/"&gt; &lt;img alt="Llama.cpp support for Ling Mini 2.0 is probably coming next week" src="https://external-preview.redd.it/2pTMNMbI2akSWow2DVQcK_a-oWX8FigInIZ74WH_NyQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d2f02e74afa0abefa42b4330fc05577e733ff328" title="Llama.cpp support for Ling Mini 2.0 is probably coming next week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Llama.cpp support for Ling Mini 2.0 is coming in the following days, it seems there’s already a PR waiting to be merged and some GGUFs already out.&lt;/p&gt; &lt;p&gt;An interesting thing about this model is that it has 16B total parameters, but only 1.4B are activated per input token, and it outperforms Ernie 4.5 21B A3B, which is a tad bigger and uses more active parameters. Quite a nice addition for the GPU-poor folks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16036"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeq9w/llamacpp_support_for_ling_mini_20_is_probably/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeq9w/llamacpp_support_for_ling_mini_20_is_probably/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T01:02:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmlluu</id>
    <title>Mini-PC Dilemma: 96GB vs 128GB. How Much RAM is it worth buying?</title>
    <updated>2025-09-21T07:27:46+00:00</updated>
    <author>
      <name>/u/Dull-Breadfruit-3241</name>
      <uri>https://old.reddit.com/user/Dull-Breadfruit-3241</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I'm planning to pick up one of the new mini-PCs powered by the AMD Ryzen AI Max+ 395 CPU,specifically the Bosgame M5. The 96GB RAM model looks more cost-effective, but I'm weighing whether it's worth spending ~15% more for the 128GB version.&lt;/p&gt; &lt;p&gt;From what I understand, the 96GB config allows up to 64GB to be allocated to the integrated GPU, while the 128GB model can push that up to 96GB. That extra memory could make a difference on whether be able to run larger LLMs.&lt;/p&gt; &lt;p&gt;So here’s my question: will larger models that fit thanks to the extra memory actually run at decent speeds? Will I miss out on larger better models that would still run at decent speed on this machine by choosing the model that can allocate only 64GB of RAM to the GPU?&lt;/p&gt; &lt;p&gt;My goal is to experiment with LLMs and other AI projects locally, and I’d love to hear from anyone who’s tested similar setups or has insight into how well these systems scale with RAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dull-Breadfruit-3241"&gt; /u/Dull-Breadfruit-3241 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmlluu/minipc_dilemma_96gb_vs_128gb_how_much_ram_is_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmlluu/minipc_dilemma_96gb_vs_128gb_how_much_ram_is_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmlluu/minipc_dilemma_96gb_vs_128gb_how_much_ram_is_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T07:27:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm6v83</id>
    <title>Qwen Next 80b q4 vs q8 vs GPT 120b vs Qwen Coder 30b</title>
    <updated>2025-09-20T19:14:37+00:00</updated>
    <author>
      <name>/u/Baldur-Norddahl</name>
      <uri>https://old.reddit.com/user/Baldur-Norddahl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm6v83/qwen_next_80b_q4_vs_q8_vs_gpt_120b_vs_qwen_coder/"&gt; &lt;img alt="Qwen Next 80b q4 vs q8 vs GPT 120b vs Qwen Coder 30b" src="https://b.thumbs.redditmedia.com/Ol0Pxbro6vazPUaSzrEtZA_JvTzjW_-by2F4t8qWnuU.jpg" title="Qwen Next 80b q4 vs q8 vs GPT 120b vs Qwen Coder 30b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran this test on my M4 Max MacBook Pro 128 GB laptop. The interesting find is how prompt processing speed stays relatively flat as context grows. This is completely different behavior from Qwen3 Coder.&lt;/p&gt; &lt;p&gt;GPT 120b starts out faster but then becomes slower as context fills. However only the 4 bit quant of Qwen Next manages to overtake it when looking at total elapsed time. And that first happens at 80k context length. For most cases the GPT model stays the fastest then.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Baldur-Norddahl"&gt; /u/Baldur-Norddahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nm6v83"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm6v83/qwen_next_80b_q4_vs_q8_vs_gpt_120b_vs_qwen_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm6v83/qwen_next_80b_q4_vs_q8_vs_gpt_120b_vs_qwen_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T19:14:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlyy6n</id>
    <title>Intel Arc Pro B60 24GB professional GPU listed at $599, in stock and shipping</title>
    <updated>2025-09-20T14:00:49+00:00</updated>
    <author>
      <name>/u/PhantomWolf83</name>
      <uri>https://old.reddit.com/user/PhantomWolf83</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlyy6n/intel_arc_pro_b60_24gb_professional_gpu_listed_at/"&gt; &lt;img alt="Intel Arc Pro B60 24GB professional GPU listed at $599, in stock and shipping" src="https://external-preview.redd.it/942g63AteF3sF5KI6YzwLlHNUjooze5_uZcUA7PiVqQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=259bd6663f4a689dc50651317dca845a29e37f3f" title="Intel Arc Pro B60 24GB professional GPU listed at $599, in stock and shipping" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhantomWolf83"&gt; /u/PhantomWolf83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/intel-arc-pro-b60-24gb-professional-gpu-listed-at-599-in-stock-and-shipping"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlyy6n/intel_arc_pro_b60_24gb_professional_gpu_listed_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlyy6n/intel_arc_pro_b60_24gb_professional_gpu_listed_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T14:00:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmnrgw</id>
    <title>Raylight tensor split distributed GPU now can do LoRa for Wan, Flux and Qwen. Why by 5090 when you can buy 2x5060Tis</title>
    <updated>2025-09-21T09:42:48+00:00</updated>
    <author>
      <name>/u/Altruistic_Heat_9531</name>
      <uri>https://old.reddit.com/user/Altruistic_Heat_9531</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmnrgw/raylight_tensor_split_distributed_gpu_now_can_do/"&gt; &lt;img alt="Raylight tensor split distributed GPU now can do LoRa for Wan, Flux and Qwen. Why by 5090 when you can buy 2x5060Tis" src="https://b.thumbs.redditmedia.com/MFuCkMnKE4sjWkjqo3iTJG6fWRBCd4JYLtj-VDPofaM.jpg" title="Raylight tensor split distributed GPU now can do LoRa for Wan, Flux and Qwen. Why by 5090 when you can buy 2x5060Tis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/komikndr/raylight"&gt;https://github.com/komikndr/raylight&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just update for Raylight, some model still a bit unstable so you need to restart the ComfyUI&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can now install it &lt;strong&gt;without&lt;/strong&gt; FlashAttention, so yey to Pascal(but i am not testing it yet).&lt;/li&gt; &lt;li&gt;Supported Attention : &lt;strong&gt;Sage, Flash, Torch&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Full &lt;strong&gt;LoRA&lt;/strong&gt; support&lt;/li&gt; &lt;li&gt;&lt;strong&gt;FSDP CPU offload,&lt;/strong&gt; analogous to block swap.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AMD&lt;/strong&gt; User confirmed working on 8xMI300X using ROCm compiled PyTorch and Flash Attention&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Realtime Qwen on 2x RTX Ada 2000 , forgot to mute audio&lt;/p&gt; &lt;p&gt;&lt;a href="https://files.catbox.moe/a5rgon.mp4"&gt;https://files.catbox.moe/a5rgon.mp4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altruistic_Heat_9531"&gt; /u/Altruistic_Heat_9531 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nmnrgw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmnrgw/raylight_tensor_split_distributed_gpu_now_can_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmnrgw/raylight_tensor_split_distributed_gpu_now_can_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T09:42:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmiqjh</id>
    <title>OPEN WEIGHTS: Isaac 0.1. Perceptive-language model. 2B params. Matches or beats models significantly larger on core perception as claimed by Perceptron AI. Links to download in bodytext.</title>
    <updated>2025-09-21T04:34:19+00:00</updated>
    <author>
      <name>/u/AdFluffy920</name>
      <uri>https://old.reddit.com/user/AdFluffy920</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmiqjh/open_weights_isaac_01_perceptivelanguage_model_2b/"&gt; &lt;img alt="OPEN WEIGHTS: Isaac 0.1. Perceptive-language model. 2B params. Matches or beats models significantly larger on core perception as claimed by Perceptron AI. Links to download in bodytext." src="https://b.thumbs.redditmedia.com/d6NOhInKANeH8JCDvtmFBeHE3iU2bdeRVBiujDCsTrI.jpg" title="OPEN WEIGHTS: Isaac 0.1. Perceptive-language model. 2B params. Matches or beats models significantly larger on core perception as claimed by Perceptron AI. Links to download in bodytext." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog: &lt;a href="https://www.perceptron.inc/blog/introducing-isaac-0-1"&gt;https://www.perceptron.inc/blog/introducing-isaac-0-1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://www.perceptron.inc/demo"&gt;https://www.perceptron.inc/demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Download weights: &lt;a href="https://huggingface.co/PerceptronAI/Isaac-0.1"&gt;https://huggingface.co/PerceptronAI/Isaac-0.1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdFluffy920"&gt; /u/AdFluffy920 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nmiqjh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmiqjh/open_weights_isaac_01_perceptivelanguage_model_2b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmiqjh/open_weights_isaac_01_perceptivelanguage_model_2b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T04:34:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nme5xy</id>
    <title>4x MI50 32GB reach 22 t/s with Qwen3 235B-A22B and 36 t/s with Qwen2.5 72B in vllm</title>
    <updated>2025-09-21T00:33:31+00:00</updated>
    <author>
      <name>/u/MLDataScientist</name>
      <uri>https://old.reddit.com/user/MLDataScientist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;It is exciting to see AMD is finally fixing their software stack. I recently updated my MI50 GPU drivers and ROCm stack to 6.4.3. AMD officially deprecated support for MI50 (gfx906). But ROCm 6.4.3 works with one simple fix. You need to copy tensile library of MI50 from a package and paste it in rocm folder (details: &lt;a href="https://github.com/ROCm/ROCm/issues/4625#issuecomment-2899838977"&gt;https://github.com/ROCm/ROCm/issues/4625#issuecomment-2899838977&lt;/a&gt; ).&lt;/p&gt; &lt;p&gt;For performance tests, I used vllm backend - &lt;a href="https://github.com/nlzy/vllm-gfx906"&gt;https://github.com/nlzy/vllm-gfx906&lt;/a&gt; . Thank you &lt;a href="/u/NaLanZeYu"&gt;u/NaLanZeYu&lt;/a&gt; for supporting gfx906 in a separate vllm fork!&lt;/p&gt; &lt;p&gt;In my venv, I installed pytorch 2.8. I kept the original triton 3.3 but I earlier checked and triton 3.5 was also working with MI50. For single GPU, there were no package issues. For multi-GPU, there was an issue - rccl was compiled without gfx906 support. What I did was I compiled rccl with gfx906 support.&lt;/p&gt; &lt;p&gt;Downloaded rccl 2.22.3 (for ROCm 6.4.3) from &lt;a href="https://github.com/ROCm/rccl/releases/tag/rocm-6.4.3"&gt;https://github.com/ROCm/rccl/releases/tag/rocm-6.4.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;extracted the zip file.&lt;/p&gt; &lt;p&gt;installed in ubuntu terminal:&lt;/p&gt; &lt;p&gt;```sudo ./install.sh --amdgpu_targets gfx906 -i -j 32 -p -r```&lt;/p&gt; &lt;p&gt;in vllmenv installation folder find &lt;a href="http://lbrccl.so"&gt;lbrccl.so&lt;/a&gt; and rename or delete it so that pytorch cannot use it. e.g. _librccl.so&lt;/p&gt; &lt;p&gt;in vllmenv, import the new rccl library location:&lt;/p&gt; &lt;p&gt;VLLM_NCCL_SO_PATH=/opt/rocm/lib&lt;/p&gt; &lt;p&gt;(or LD_LIBRARY_PATH=/opt/rocm/lib:$LD_LIBRARY_PATH)&lt;/p&gt; &lt;p&gt;now, vllm supports multi-GPU properly for MI50 with ROCm 6.4.3.&lt;/p&gt; &lt;p&gt;Some metrics:&lt;/p&gt; &lt;p&gt;single MI50 - single requests in vllm bench serve:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama-3.1-8B-AWQ-4bit - TG 93t/s; PP 945t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;four MI50 - single requests in vllm bench serve:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5 72B gptq int4 (TP 4) - TG 36/s; PP 500t/s&lt;/li&gt; &lt;li&gt;Qwen3-235B-A22B-AWQ (TP 4) - TG 22t/s; PP 290t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All of them are connected to my MB with PCIE4.0 16x speed. CPU: AMD EPYC 7532 with 8x32GB DDR4 3200Mhz ECC RAM.&lt;/p&gt; &lt;p&gt;Overall, there is a great performance uplift (up to 25%) when we use ROCm 6.4.3 with gfx906.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLDataScientist"&gt; /u/MLDataScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nme5xy/4x_mi50_32gb_reach_22_ts_with_qwen3_235ba22b_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nme5xy/4x_mi50_32gb_reach_22_ts_with_qwen3_235ba22b_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nme5xy/4x_mi50_32gb_reach_22_ts_with_qwen3_235ba22b_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T00:33:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmnmqh</id>
    <title>Wan 2.2 Animate : Open-Sourced model for character replacement and animation in videos</title>
    <updated>2025-09-21T09:34:38+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wan 2.2 Animate 14B is released which can animate static pictures using reference videos with movement and expression replication Hugging Face : &lt;a href="https://huggingface.co/Wan-AI/Wan2.2-Animate-14B"&gt;https://huggingface.co/Wan-AI/Wan2.2-Animate-14B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmnmqh/wan_22_animate_opensourced_model_for_character/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmnmqh/wan_22_animate_opensourced_model_for_character/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmnmqh/wan_22_animate_opensourced_model_for_character/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T09:34:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmj3cr</id>
    <title>Lucy-Edit : 1st Open-sourced model for Video editing</title>
    <updated>2025-09-21T04:54:54+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lucy-Edit-Dev, based on Wan2.2 5B is the first open-sourced AI model with video editing capabilities, calling itself the nano banana for video editing. It can change clothes, characters, backgrounds, object, etc.&lt;/p&gt; &lt;p&gt;Model weights : &lt;a href="https://huggingface.co/decart-ai/Lucy-Edit-Dev"&gt;https://huggingface.co/decart-ai/Lucy-Edit-Dev&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmj3cr/lucyedit_1st_opensourced_model_for_video_editing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmj3cr/lucyedit_1st_opensourced_model_for_video_editing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmj3cr/lucyedit_1st_opensourced_model_for_video_editing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T04:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmeu5s</id>
    <title>Qwen3-Omni, Qwen/Qwen3-Omni-7B spotted</title>
    <updated>2025-09-21T01:07:48+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeu5s/qwen3omni_qwenqwen3omni7b_spotted/"&gt; &lt;img alt="Qwen3-Omni, Qwen/Qwen3-Omni-7B spotted" src="https://external-preview.redd.it/VoGpbOIxrqAHEzxUbIOFVzMNSL9glnfyk27odhpB_Jk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d87cfdb2cbad767672c45769d597618162abb80" title="Qwen3-Omni, Qwen/Qwen3-Omni-7B spotted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/pull/41025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeu5s/qwen3omni_qwenqwen3omni7b_spotted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeu5s/qwen3omni_qwenqwen3omni7b_spotted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T01:07:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmg185</id>
    <title>Qwen3Omni</title>
    <updated>2025-09-21T02:08:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmg185/qwen3omni/"&gt; &lt;img alt="Qwen3Omni" src="https://preview.redd.it/wcxu5ypyefqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b0e169e57d635253c780f31d6542861df594c98" title="Qwen3Omni" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wcxu5ypyefqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmg185/qwen3omni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmg185/qwen3omni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T02:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmkswn</id>
    <title>Just dropped: Qwen3-4B Function calling on just 6GB VRAM</title>
    <updated>2025-09-21T06:37:33+00:00</updated>
    <author>
      <name>/u/Honest-Debate-6863</name>
      <uri>https://old.reddit.com/user/Honest-Debate-6863</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to bring this to you if you are looking for a superior model for toolcalling to use with ollama for local Codex style personal coding assistant on terminal:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Manojb/Qwen3-4B-toolcalling-gguf-codex"&gt;https://huggingface.co/Manojb/Qwen3-4B-toolcalling-gguf-codex&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;✅ &lt;strong&gt;Fine-tuned on 60K function calling examples&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;✅ &lt;strong&gt;4B parameters&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;✅ &lt;strong&gt;GGUF format&lt;/strong&gt; (optimized for CPU/GPU inference)&lt;/li&gt; &lt;li&gt;✅ &lt;strong&gt;3.99GB download&lt;/strong&gt; (fits on any modern system)&lt;/li&gt; &lt;li&gt;✅ &lt;strong&gt;Production-ready&lt;/strong&gt; with 0.518 training loss&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;this works with&lt;br /&gt; &lt;a href="https://github.com/ymichael/open-codex/"&gt;https://github.com/ymichael/open-codex/&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/8ankur8/anything-codex"&gt;https://github.com/8ankur8/anything-codex&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/dnakov/anon-codex"&gt;https://github.com/dnakov/anon-codex&lt;/a&gt;&lt;br /&gt; preferable: &lt;a href="https://github.com/search?q=repo%3Adnakov%2Fanon-codex%20ollama&amp;amp;type=code"&gt;https://github.com/search?q=repo%3Adnakov%2Fanon-codex%20ollama&amp;amp;type=code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Honest-Debate-6863"&gt; /u/Honest-Debate-6863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmkswn/just_dropped_qwen34b_function_calling_on_just_6gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmkswn/just_dropped_qwen34b_function_calling_on_just_6gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmkswn/just_dropped_qwen34b_function_calling_on_just_6gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T06:37:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmii5y</id>
    <title>Magistral 1.2 is incredible. Wife prefers it over Gemini 2.5 Pro.</title>
    <updated>2025-09-21T04:21:00+00:00</updated>
    <author>
      <name>/u/My_Unbiased_Opinion</name>
      <uri>https://old.reddit.com/user/My_Unbiased_Opinion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL:DR - AMAZING general use model. Y'all gotta try it. &lt;/p&gt; &lt;p&gt;Just wanna let y'all know that Magistral is worth trying. Currently running the UD Q3KXL quant from Unsloth on Ollama with Openwebui. &lt;/p&gt; &lt;p&gt;The model is incredible. It doesn't overthink and waste tokens unnecessarily in the reasoning chain. &lt;/p&gt; &lt;p&gt;The responses are focused, concise and to the point. No fluff, just tells you what you need to know. &lt;/p&gt; &lt;p&gt;The censorship is VERY minimal. My wife has been asking it medical-adjacent questions and it always gives you a solid answer. I am an ICU nurse by trade and am studying for advanced practice and can vouch for the advice magistral is giving is legit. &lt;/p&gt; &lt;p&gt;Before this, wife has been using Gemini 2.5 pro and hates the censorship and the way it talks to you like a child (let's break this down, etc). &lt;/p&gt; &lt;p&gt;The general knowledge in Magistral is already really good. Seems to know obscure stuff quite well. &lt;/p&gt; &lt;p&gt;Now, once you hook it up to a web search tool call is where this model I feel like can hit as hard as proprietary LLMs. The model really does wake up even more when hooked up to the web. &lt;/p&gt; &lt;p&gt;Model even supports image input. I have not tried that specifically but I loved image processing from Mistral 3.2 2506 so I expect no issues there.&lt;/p&gt; &lt;p&gt;Currently using with Openwebui with the recommended parameters. If you do use it with OWUI, be sure to set up the reasoning tokens in the model settings so thinking is kept separate from the model response. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/My_Unbiased_Opinion"&gt; /u/My_Unbiased_Opinion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmii5y/magistral_12_is_incredible_wife_prefers_it_over/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmii5y/magistral_12_is_incredible_wife_prefers_it_over/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmii5y/magistral_12_is_incredible_wife_prefers_it_over/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T04:21:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1njjz7j</id>
    <title>Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)</title>
    <updated>2025-09-17T17:44:17+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt; &lt;img alt="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" src="https://preview.redd.it/4xt9enbairpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cc1ac16a9a2b96134934fcb2f81a9f3d4916b31" title="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4xt9enbairpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T17:44:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkft9l</id>
    <title>AMA with the LM Studio team</title>
    <updated>2025-09-18T18:12:24+00:00</updated>
    <author>
      <name>/u/yags-lms</name>
      <uri>https://old.reddit.com/user/yags-lms</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We're excited for this AMA. Thank you for having us here today. We got a full house from the LM Studio team: &lt;/p&gt; &lt;p&gt;- Yags &lt;a href="https://t.co/ERfA4NrR96"&gt;https://reddit.com/user/yags-lms/&lt;/a&gt; (founder)&lt;br /&gt; - Neil &lt;a href="https://t.co/KyiHVfv0QG"&gt;https://reddit.com/user/neilmehta24/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Will &lt;a href="https://t.co/IjAZJL2JMK"&gt;https://reddit.com/user/will-lms/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Matt &lt;a href="https://t.co/6MNkItPYnI"&gt;https://reddit.com/user/matt-lms/&lt;/a&gt; (LLM engines, runtime, and APIs)&lt;br /&gt; - Ryan &lt;a href="https://t.co/0snuNUPizo"&gt;https://reddit.com/user/ryan-lms/&lt;/a&gt; (Core system and APIs)&lt;br /&gt; - Rugved &lt;a href="https://t.co/xGtYHsJZI3"&gt;https://reddit.com/user/rugved_lms/&lt;/a&gt; (CLI and SDKs)&lt;br /&gt; - Alex &lt;a href="https://t.co/wtT2IFf0z6"&gt;https://reddit.com/user/alex-lms/&lt;/a&gt; (App)&lt;br /&gt; - Julian &lt;a href="https://www.reddit.com/user/julian-lms/"&gt;https://www.reddit.com/user/julian-lms/&lt;/a&gt; (Ops) &lt;/p&gt; &lt;p&gt;Excited to chat about: the latest local models, UX for local models, steering local models effectively, LM Studio SDK and APIs, how we support multiple LLM engines (llama.cpp, MLX, and more), privacy philosophy, why local AI matters, our open source projects (mlx-engine, lms, lmstudio-js, lmstudio-python, venvstacks), why ggerganov and Awni are the GOATs, where is TheBloke, and more. &lt;/p&gt; &lt;p&gt;Would love to hear about people's setup, which models you use, use cases that really work, how you got into local AI, what needs to improve in LM Studio and the ecosystem as a whole, how you use LM Studio, and anything in between!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Everyone: it was awesome to see your questions here today and share replies! Thanks a lot for the welcoming AMA. We will continue to monitor this post for more questions over the next couple of days, but for now we're signing off to continue building 🔨&lt;/p&gt; &lt;p&gt;We have several marquee features we've been working on for a loong time coming out later this month that we hope you'll love and find lots of value in. And don't worry, UI for n cpu moe is on the way too :)&lt;/p&gt; &lt;p&gt;Special shoutout and thanks to ggerganov, Awni Hannun, TheBloke, Hugging Face, and all the rest of the open source AI community!&lt;/p&gt; &lt;p&gt;Thank you and see you around! - Team LM Studio 👾&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yags-lms"&gt; /u/yags-lms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T18:12:24+00:00</published>
  </entry>
</feed>
