<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-09T10:25:19+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q82ulm</id>
    <title>Designing an on-prem AI + vision + automation stack, looking for architecture advice...</title>
    <updated>2026-01-09T08:25:07+00:00</updated>
    <author>
      <name>/u/Jefftoro</name>
      <uri>https://old.reddit.com/user/Jefftoro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’m in the process of designing a &lt;strong&gt;self-hosted, on-prem infrastructure&lt;/strong&gt; for a company and I want to inquire about the architecture before locking anything in.&lt;/p&gt; &lt;p&gt;Keep in mind while reading this I'm a 19 year old in school for business. I taught myself everything about this so i apologize if I say anything incorrrect or that doesnt make sense. And yes gpt helped me write this obviously, this is alot of writing...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I’m trying to run (all self-hosted, mostly open source):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Frigate&lt;/strong&gt; for IP cameras + computer vision (event detection, progress tracking, safety, etc.)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;n8n&lt;/strong&gt; for automation / workflows&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Twenty CRM&lt;/strong&gt; as our core CRM (This needs to be built heavily to do what we need it to)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local LLM inference&lt;/strong&gt; (internal assistants, summaries, event tracking, PMing)(We can spend some bank here, I want a decent system that I know can handle some serious stuff. Lets say 10k max but if you think a cheaper or more expensive option would work for me let me hear it!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MCP servers&lt;/strong&gt; to expose internal info and tools to LLMs&lt;/li&gt; &lt;li&gt;Some &lt;strong&gt;light LLM / vision training for the frigate system&lt;/strong&gt; (this is the tricky part and i still haven't looked into it but im planning on training a model to analyze progress of the factory and report back to a tracking system, also point out inefficiencies, errors and workplace hazards)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Current system:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ISP: &lt;strong&gt;100 Mbps up / 100 Mbps down&lt;/strong&gt; unfortunately :( | im looking on getting direct fibre but its not available right now, maybe in the future&lt;/li&gt; &lt;li&gt;Network: &lt;strong&gt;UniFi UDM Pro + UniFi 500W 48-port PoE switch&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Cameras will be PoE IP cameras, currently have hikvision cameras but also willing to spend money on camera that work better with the ai model training, all will be hard wired, cat5e, but if cat6 is needed let me know (I doubt it)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I’m unsure about / want feedback on:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Best overall &lt;strong&gt;hardware strategy&lt;/strong&gt; (single or multiple systems? Which parts? Mac or Nvidia for Ai? the Gmtec or the Spark???? This stuff is really driving me nuts as new stuff keeps coming out and i cant get clear answers anywhere)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Docker vs Proxmox vs&lt;/strong&gt; what ever else??? ( Whats the best option, i was certain on docker but then chatgpt told me proxmox and something about Kubernetes so now im lost)&lt;/li&gt; &lt;li&gt;How to best separate: &lt;ul&gt; &lt;li&gt;Core business services (CRM, n8n, DBs)&lt;/li&gt; &lt;li&gt;AI/LLM workloads&lt;/li&gt; &lt;li&gt;Frigate/video workloads&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Storage layout for: &lt;ul&gt; &lt;li&gt;Databases ( maybe a Ugreen nas or something better?) &lt;/li&gt; &lt;li&gt;Video recordings ( Lets say 2 weeks of recording across 25 cameras? Im thinking 8-16TB?)&lt;/li&gt; &lt;li&gt;AI datasets ( Still unsure which models will be run.)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;High-level goal:&lt;/strong&gt;&lt;br /&gt; I want this to function like an internal “company operating system”:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Reliable day-to-day helpers (CRM, automations, MPC servers and etc)&lt;/li&gt; &lt;li&gt;Ai models that can be trained to learn how the factory and office is supposed to work and improve everything. &lt;/li&gt; &lt;li&gt;No dependency on other companies paid softwares that leave no room for customizability or development&lt;/li&gt; &lt;li&gt;If you were designing this today, &lt;strong&gt;what would you do differently or watch out for?&lt;/strong&gt; Happy to provide more details if needed.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks in advance, this has been really stressing me out. I've taken on too many tasks and now getting them all launched is killing me. &lt;/p&gt; &lt;p&gt;Please feel free to write as much as you can because i need to learn!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jefftoro"&gt; /u/Jefftoro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q82ulm/designing_an_onprem_ai_vision_automation_stack/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q82ulm/designing_an_onprem_ai_vision_automation_stack/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q82ulm/designing_an_onprem_ai_vision_automation_stack/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T08:25:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1q77rxh</id>
    <title>Z-image base model is being prepared for release</title>
    <updated>2026-01-08T09:51:33+00:00</updated>
    <author>
      <name>/u/Ravencloud007</name>
      <uri>https://old.reddit.com/user/Ravencloud007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q77rxh/zimage_base_model_is_being_prepared_for_release/"&gt; &lt;img alt="Z-image base model is being prepared for release" src="https://preview.redd.it/038zb25ok3cg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2cbdfd4fbd53811cd0fc218bed6e466b49ff678" title="Z-image base model is being prepared for release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher&amp;amp;since=2025-12-31&amp;amp;until=2026-01-08"&gt;https://github.com/modelscope/DiffSynth-Studio/commits?author=Artiprocher&amp;amp;since=2025-12-31&amp;amp;until=2026-01-08&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ravencloud007"&gt; /u/Ravencloud007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/038zb25ok3cg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q77rxh/zimage_base_model_is_being_prepared_for_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q77rxh/zimage_base_model_is_being_prepared_for_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T09:51:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1q80fcn</id>
    <title>what communities can i join for real time chat about models, model performance, etc.</title>
    <updated>2026-01-09T06:01:55+00:00</updated>
    <author>
      <name>/u/throwawaycanc3r</name>
      <uri>https://old.reddit.com/user/throwawaycanc3r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;looking for like a highly active discord version of this sub.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/throwawaycanc3r"&gt; /u/throwawaycanc3r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q80fcn/what_communities_can_i_join_for_real_time_chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q80fcn/what_communities_can_i_join_for_real_time_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q80fcn/what_communities_can_i_join_for_real_time_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T06:01:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7a62a</id>
    <title>AI21 Labs releases Jamba2</title>
    <updated>2026-01-08T12:10:15+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/"&gt; &lt;img alt="AI21 Labs releases Jamba2" src="https://b.thumbs.redditmedia.com/Il111fZ012O0JrQgLsZklbi8sbSvI68SHycPLPcigNc.jpg" title="AI21 Labs releases Jamba2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zmo6dijns4cg1.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ba9fd085bb5b3fb720adf85cf28c3a8b63ba44cb"&gt;https://preview.redd.it/zmo6dijns4cg1.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ba9fd085bb5b3fb720adf85cf28c3a8b63ba44cb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;52B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-Mini"&gt;https://huggingface.co/ai21labs/AI21-Jamba2-Mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Jamba2 Mini is an open source small language model built for enterprise reliability. With 12B active parameters (52B total), it delivers precise question answering without the computational overhead of reasoning models. The model's SSM-Transformer architecture provides a memory-efficient solution for production agent stacks where consistent, grounded outputs are critical.&lt;/p&gt; &lt;p&gt;Released under Apache 2.0 License with a 256K context window, Jamba2 Mini is designed for enterprise workflows that demand accuracy and steerability. For more details, read the &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-Mini/blob/main/ai21.com/blog/introducing-jamba2"&gt;full release blog post&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Key Advantages&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Superior reliability-to-throughput ratio:&lt;/strong&gt; Maintains high performance at 100K+ token contexts&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Category-leading benchmarks:&lt;/strong&gt; Excels on IFBench, IFEval, Collie, and FACTS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Statistically significant quality wins:&lt;/strong&gt; Outperforms comparable models on real-world enterprise tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;256K context window:&lt;/strong&gt; Processes technical manuals, research papers, and knowledge bases&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apache 2.0 License:&lt;/strong&gt; Fully open source for commercial use&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Production-optimized:&lt;/strong&gt; Lean memory footprint for scalable deployments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cqwicpwts4cg1.png?width=2400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=593fed6a7d2094908b6f1878ea12a8e4f5e67e6d"&gt;https://preview.redd.it/cqwicpwts4cg1.png?width=2400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=593fed6a7d2094908b6f1878ea12a8e4f5e67e6d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;3B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-3B"&gt;https://huggingface.co/ai21labs/AI21-Jamba2-3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Jamba2 3B is an ultra-compact open source model designed to bring enterprise-grade reliability to on-device deployments. At just 3B parameters, it runs efficiently on consumer devices—iPhones, Androids, Macs, and PCs—while maintaining the grounding and instruction-following capabilities required for production use.&lt;/p&gt; &lt;p&gt;Released under Apache 2.0 License with a 256K context window, Jamba2 3B enables developers to build reliable AI applications for edge environments. For more details, read the &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba2-3B/blob/main/ai21.com/blog/introducing-jamba2"&gt;full release blog post&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Key Advantages&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;On-device deployment:&lt;/strong&gt; Runs efficiently on iPhones, Androids, Macs, and PCs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ultra-compact footprint:&lt;/strong&gt; 3B parameters enabling edge deployments with minimal resources&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmark leadership:&lt;/strong&gt; Excels on IFBench, IFEval, Collie, and FACTS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;256K context window:&lt;/strong&gt; Processes long documents and knowledge bases&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apache 2.0 License:&lt;/strong&gt; Fully open source for commercial use&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SSM-Transformer architecture:&lt;/strong&gt; Memory-efficient design for resource-constrained environments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;it works in llama.cpp, tested on my Windows desktop:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ijzgde7bg5cg1.png?width=3802&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=983bc8e27ec59065d4b548e78eb4f50405507c71"&gt;https://preview.redd.it/ijzgde7bg5cg1.png?width=3802&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=983bc8e27ec59065d4b548e78eb4f50405507c71&lt;/a&gt;&lt;/p&gt; &lt;p&gt;fixed blog post &lt;a href="https://www.ai21.com/blog/introducing-jamba2/"&gt;https://www.ai21.com/blog/introducing-jamba2/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUFs are in progress &lt;a href="https://huggingface.co/mradermacher/model_requests/discussions/1683"&gt;https://huggingface.co/mradermacher/model_requests/discussions/1683&lt;/a&gt;&lt;/p&gt; &lt;p&gt;previous generation of Jamba models&lt;/p&gt; &lt;p&gt;399B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7"&gt;https://huggingface.co/ai21labs/AI21-Jamba-Large-1.7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;52B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7"&gt;https://huggingface.co/ai21labs/AI21-Jamba-Mini-1.7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;3B &lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B"&gt;https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7a62a/ai21_labs_releases_jamba2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T12:10:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7o8kl</id>
    <title>GLM-4.7 on 4x RTX 3090 with ik_llama.cpp</title>
    <updated>2026-01-08T21:14:19+00:00</updated>
    <author>
      <name>/u/iamn0</name>
      <uri>https://old.reddit.com/user/iamn0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the help of Opus 4.5 I got unsloth/GLM-4.7-GGUF (Q4_K_M) running on my 4x RTX 3090 setup using ik_llama.cpp in Docker. I wanted to share my benchmark results and configuration, and ask if these numbers are what I should expect - or if there's room for improvement.&lt;/p&gt; &lt;h1&gt;My Setup&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Component&lt;/th&gt; &lt;th align="left"&gt;Specs&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Motherboard&lt;/td&gt; &lt;td align="left"&gt;Supermicro H12SSL-i&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU&lt;/td&gt; &lt;td align="left"&gt;AMD EPYC 7282&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPUs&lt;/td&gt; &lt;td align="left"&gt;4x NVIDIA RTX 3090 (96GB VRAM total, all at PCIe x16)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM&lt;/td&gt; &lt;td align="left"&gt;256GB DDR4-2133&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Storage&lt;/td&gt; &lt;td align="left"&gt;2 TB NVMe SSD&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Benchmark Results&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Config&lt;/th&gt; &lt;th align="left"&gt;Context&lt;/th&gt; &lt;th align="left"&gt;n-cpu-moe&lt;/th&gt; &lt;th align="left"&gt;Batch&lt;/th&gt; &lt;th align="left"&gt;VRAM/GPU&lt;/th&gt; &lt;th align="left"&gt;Prompt&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Generation&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Initial (mmap)&lt;/td&gt; &lt;td align="left"&gt;16K&lt;/td&gt; &lt;td align="left"&gt;all&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;~5 GB&lt;/td&gt; &lt;td align="left"&gt;2.8 t/s&lt;/td&gt; &lt;td align="left"&gt;3.1 t/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;split-mode layer&lt;/td&gt; &lt;td align="left"&gt;16K&lt;/td&gt; &lt;td align="left"&gt;partial&lt;/td&gt; &lt;td align="left"&gt;4096&lt;/td&gt; &lt;td align="left"&gt;~17 GB&lt;/td&gt; &lt;td align="left"&gt;2.8 t/s&lt;/td&gt; &lt;td align="left"&gt;⚠️ 0.29 t/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;+ no-mmap&lt;/td&gt; &lt;td align="left"&gt;16K&lt;/td&gt; &lt;td align="left"&gt;all&lt;/td&gt; &lt;td align="left"&gt;4096&lt;/td&gt; &lt;td align="left"&gt;~10 GB&lt;/td&gt; &lt;td align="left"&gt;8.5 t/s&lt;/td&gt; &lt;td align="left"&gt;3.45 t/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;+ n-cpu-moe 72&lt;/td&gt; &lt;td align="left"&gt;16K&lt;/td&gt; &lt;td align="left"&gt;72&lt;/td&gt; &lt;td align="left"&gt;4096&lt;/td&gt; &lt;td align="left"&gt;~17 GB&lt;/td&gt; &lt;td align="left"&gt;9.9 t/s&lt;/td&gt; &lt;td align="left"&gt;4.12 t/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Best 8K&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;8K&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;65&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;4096&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;~21 GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;12.0 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;4.48 t/s&lt;/strong&gt; ⭐&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Best 16K&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;16K&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;68&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;2048&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;~19 GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;10.5 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;4.28 t/s&lt;/strong&gt; ⭐&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Benchmark Methodology&lt;/h1&gt; &lt;p&gt;All tests were performed using the same simple request via curl:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl http://localhost:8080/v1/chat/completions \ -H &amp;quot;Content-Type: application/json&amp;quot; \ -d '{ &amp;quot;model&amp;quot;: &amp;quot;GLM-4.7-GUFF&amp;quot;, &amp;quot;messages&amp;quot;: [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Write a short Haiku.&amp;quot;}], &amp;quot;temperature&amp;quot;: 0.7, &amp;quot;max_tokens&amp;quot;: 100 }' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The response includes timing information:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;timings&amp;quot;: { &amp;quot;prompt_n&amp;quot;: 17, &amp;quot;prompt_ms&amp;quot;: 1419.902, &amp;quot;prompt_per_second&amp;quot;: 11.97, &amp;quot;predicted_n&amp;quot;: 100, &amp;quot;predicted_ms&amp;quot;: 22301.81, &amp;quot;predicted_per_second&amp;quot;: 4.48 } } &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;prompt_per_second&lt;/strong&gt;: How fast the input tokens are processed&lt;/li&gt; &lt;li&gt;&lt;strong&gt;predicted_per_second&lt;/strong&gt;: How fast new tokens are generated (this is what matters most for chat)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each configuration was tested with a fresh server start (cold start) and the first request after warmup. Note that GLM-4.7 has a &amp;quot;thinking/reasoning&amp;quot; mode enabled by default, so the 100 generated tokens include internal reasoning tokens.&lt;/p&gt; &lt;h1&gt;My Current Configuration&lt;/h1&gt; &lt;h1&gt;Best for 8K Context (fastest):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;llama-server \ --model &amp;quot;/models/GLM-4-Q4_K_M-00001-of-00005.gguf&amp;quot; \ --host 0.0.0.0 --port 8080 \ --ctx-size 8192 \ --n-gpu-layers 999 \ --split-mode graph \ --flash-attn on \ --no-mmap \ -b 4096 -ub 4096 \ --cache-type-k q4_0 --cache-type-v q4_0 \ --k-cache-hadamard \ --jinja \ --n-cpu-moe 65 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Best for 16K Context:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;llama-server \ --model &amp;quot;/models/GLM-4-Q4_K_M-00001-of-00005.gguf&amp;quot; \ --host 0.0.0.0 --port 8080 \ --ctx-size 16384 \ --n-gpu-layers 999 \ --split-mode graph \ --flash-attn on \ --no-mmap \ -b 2048 -ub 2048 \ --cache-type-k q4_0 --cache-type-v q4_0 \ --k-cache-hadamard \ --jinja \ --n-cpu-moe 68 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Key Findings:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;--no-mmap&lt;/code&gt; &lt;strong&gt;is crucial&lt;/strong&gt; - Loading the model into RAM instead of memory-mapping from SSD &lt;strong&gt;tripled&lt;/strong&gt; my prompt processing speed (2.8 → 12 t/s)&lt;/li&gt; &lt;li&gt;&lt;code&gt;--split-mode graph&lt;/code&gt; &lt;strong&gt;not&lt;/strong&gt; &lt;code&gt;layer&lt;/code&gt; - Layer mode gave me only 0.29 t/s because GPUs process sequentially. Graph mode enables true tensor parallelism.&lt;/li&gt; &lt;li&gt;&lt;code&gt;--n-cpu-moe X&lt;/code&gt; - This flag controls how many MoE layers stay on CPU.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Batch size matters&lt;/strong&gt; - Smaller batches (2048) allowed more MoE layers on GPU for 16K context.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Docker Setup&lt;/h1&gt; &lt;p&gt;I'm running this in Docker. Here's my &lt;code&gt;docker-compose.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;services: glm-4: build: context: . dockerfile: Dockerfile container_name: glm-4-server deploy: resources: reservations: devices: - driver: nvidia count: all capabilities: [gpu] volumes: - /path/to/models:/models:ro ports: - &amp;quot;8080:8080&amp;quot; environment: - CTX_MODE=${CTX_MODE:-8k} # Switch between 8k/16k - NO_MMAP=true - KV_CACHE_K=q4_0 - KV_CACHE_V=q4_0 - K_CACHE_HADAMARD=true shm_size: '32gb' ipc: host restart: unless-stopped &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And my &lt;code&gt;Dockerfile&lt;/code&gt; builds ik_llama.cpp with CUDA support:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;FROM nvidia/cuda:12.4.0-devel-ubuntu22.04 # Install dependencies RUN apt-get update &amp;amp;&amp;amp; apt-get install -y \ git cmake build-essential curl \ &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/* # Clone and build ik_llama.cpp WORKDIR /opt RUN git clone https://github.com/ikawrakow/ik_llama.cpp.git WORKDIR /opt/ik_llama.cpp RUN cmake -B build \ -DGGML_CUDA=ON \ -DGGML_CUDA_FA_ALL_QUANTS=ON \ -DCMAKE_CUDA_ARCHITECTURES=&amp;quot;86&amp;quot; \ -DCMAKE_BUILD_TYPE=Release \ &amp;amp;&amp;amp; cmake --build build --config Release -j$(nproc) \ &amp;amp;&amp;amp; cmake --install build EXPOSE 8080 COPY entrypoint.sh /entrypoint.sh RUN chmod +x /entrypoint.sh ENTRYPOINT [&amp;quot;/entrypoint.sh&amp;quot;] &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Questions&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Are these speeds (4.48 t/s generation) normal for this setup?&lt;/strong&gt; I've seen some posts mentioning 5-6 t/s with 2x RTX 5090, but they had 64GB VRAM total vs my 96GB.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Any other flags I should try?&lt;/strong&gt; I tested &lt;code&gt;--run-time-repack&lt;/code&gt; but it didn't help much.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Is there a better MoE offloading strategy?&lt;/strong&gt; I'm using &lt;code&gt;--n-cpu-moe&lt;/code&gt; but I know there's also the &lt;code&gt;-ot&lt;/code&gt; regex approach.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Would a different quantization help?&lt;/strong&gt; Currently using Q4_K_M. Would IQ4_XS or Q5_K_M be faster/better?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Low GPU power usage during inference?&lt;/strong&gt; My cards are power-limited to 275W each, but during inference they only draw ~100-120W. Could this be a bottleneck limiting my token/s?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I would love to hear your thoughts and any optimization tips.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamn0"&gt; /u/iamn0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7o8kl/glm47_on_4x_rtx_3090_with_ik_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7o8kl/glm47_on_4x_rtx_3090_with_ik_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7o8kl/glm47_on_4x_rtx_3090_with_ik_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T21:14:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7xd96</id>
    <title>Gemma-3-4b (null-space) abliteration &amp; RP fine-tune</title>
    <updated>2026-01-09T03:30:31+00:00</updated>
    <author>
      <name>/u/JEs4</name>
      <uri>https://old.reddit.com/user/JEs4</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7xd96/gemma34b_nullspace_abliteration_rp_finetune/"&gt; &lt;img alt="Gemma-3-4b (null-space) abliteration &amp;amp; RP fine-tune" src="https://external-preview.redd.it/8MsDm6oseUFMBQKroxuYj3kQ8ddgGPXg7n46GwYAb90.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a93da97e2ba4d4ae0a33f7ba1f40c4fc8cc75c24" title="Gemma-3-4b (null-space) abliteration &amp;amp; RP fine-tune" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been branching out from research to actually building models recently, and this is my first attempt at applying a lora adapter on top of my abliterations.&lt;/p&gt; &lt;p&gt;I used my null-space abliteration &lt;a href="https://huggingface.co/jwest33/gemma-3-4b-it-null-space-abliterated"&gt;Gemma-3-4B-IT&lt;/a&gt; model with an adapter trained from a subset of the &lt;a href="https://huggingface.co/datasets/lemonilia/LimaRP"&gt;lemonilia/LimaRP&lt;/a&gt; roleplaying dataset. I plan on removing the step limit and reducing the learning rate but wanted to start here.&lt;/p&gt; &lt;p&gt;The model card should have all the information needed to know how I trained it but I'm happy to share anything else if I missed anything. Looking for any feedback before I start on larger models. Thanks! &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/jwest33/gemma-3-4b-null-space-abliterated-RP-writer"&gt;https://huggingface.co/jwest33/gemma-3-4b-null-space-abliterated-RP-writer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/jwest33/gemma-3-4b-null-space-abliterated-RP-writer-GGUF"&gt;https://huggingface.co/jwest33/gemma-3-4b-null-space-abliterated-RP-writer-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JEs4"&gt; /u/JEs4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/jwest33/gemma-3-4b-null-space-abliterated-RP-writer-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7xd96/gemma34b_nullspace_abliteration_rp_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7xd96/gemma34b_nullspace_abliteration_rp_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T03:30:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7uo7u</id>
    <title>SimpleLLM — a minimal (~950 LOC) LLM inference engine built from scratch</title>
    <updated>2026-01-09T01:30:58+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7uo7u/simplellm_a_minimal_950_loc_llm_inference_engine/"&gt; &lt;img alt="SimpleLLM — a minimal (~950 LOC) LLM inference engine built from scratch" src="https://external-preview.redd.it/eW56MWo5OGo3OGNnMQt6mXHkLBiOyVm9E_-7IBj4RKtoglrz47V6J4dn3Gg-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac2113c19b230d70c00cd56b79a99cc4aa1f0903" title="SimpleLLM — a minimal (~950 LOC) LLM inference engine built from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SimpleLLM's engine is async by default. Every request goes through a background inference loop that continuously batches work to keep the GPU saturated &amp;amp; prioritizing throughput.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Benchmark&lt;/th&gt; &lt;th align="left"&gt;SimpleLLM&lt;/th&gt; &lt;th align="left"&gt;vLLM&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;batch_size = 1&lt;/td&gt; &lt;td align="left"&gt;135 tok/s&lt;/td&gt; &lt;td align="left"&gt;138 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;batch_size = 64&lt;/td&gt; &lt;td align="left"&gt;4,041 tok/s&lt;/td&gt; &lt;td align="left"&gt;3,846 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Note: Currently, this repository ONLY supports OpenAI/gpt-oss-120b on a single NVIDIA H100.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Usage&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;from llm import LLM&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;engine = LLM(&amp;quot;./gpt-oss-120b&amp;quot;)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;outputs = engine.generate([&amp;quot;What is the meaning of life?&amp;quot;], max_tokens=100).result()&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;print(outputs[0].text)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Github Repo - &lt;a href="https://github.com/naklecha/simple-llm"&gt;https://github.com/naklecha/simple-llm&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/twqirt3j78cg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7uo7u/simplellm_a_minimal_950_loc_llm_inference_engine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7uo7u/simplellm_a_minimal_950_loc_llm_inference_engine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T01:30:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q83ls8</id>
    <title>Quick questions for M3 Ultra mac studio holders with 256-612GB RAM</title>
    <updated>2026-01-09T09:12:13+00:00</updated>
    <author>
      <name>/u/djdeniro</name>
      <uri>https://old.reddit.com/user/djdeniro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I'm thinking of buying a used or refurbished M3 Ultra (with 192GB unified memory) to run GLM 4.7 Q4. I need to handle about 1-2 concurrent requests.&lt;/p&gt; &lt;p&gt;Can anyone share their experience with this setup? What kind of output speed (tokens/s) should I expect?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/djdeniro"&gt; /u/djdeniro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q83ls8/quick_questions_for_m3_ultra_mac_studio_holders/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q83ls8/quick_questions_for_m3_ultra_mac_studio_holders/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q83ls8/quick_questions_for_m3_ultra_mac_studio_holders/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T09:12:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q82l7m</id>
    <title>Show us your llama.cpp command line arguments</title>
    <updated>2026-01-09T08:09:15+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And mention your hardware.&lt;/p&gt; &lt;p&gt;Recently I switched to llama.cpp and I have to say the hardest part was to optimise the arguments. Please share yours and if you are running it within a service or just a script, share it as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q82l7m/show_us_your_llamacpp_command_line_arguments/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q82l7m/show_us_your_llamacpp_command_line_arguments/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q82l7m/show_us_your_llamacpp_command_line_arguments/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T08:09:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7dlkn</id>
    <title>Qwen3-VL-Reranker - a Qwen Collection</title>
    <updated>2026-01-08T14:45:00+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7dlkn/qwen3vlreranker_a_qwen_collection/"&gt; &lt;img alt="Qwen3-VL-Reranker - a Qwen Collection" src="https://external-preview.redd.it/p_EUBuVnZfgcYfu2zAo996Hix2TFsBWGTVl7mQyY9Tk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b4e2c1310fa6d24d7fb43df7672f7329a04cfbc" title="Qwen3-VL-Reranker - a Qwen Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl-reranker"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7dlkn/qwen3vlreranker_a_qwen_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7dlkn/qwen3vlreranker_a_qwen_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T14:45:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1q82wak</id>
    <title>Completely stumped with strange issue with my dual RTX 6000 Pro LLM server</title>
    <updated>2026-01-09T08:28:02+00:00</updated>
    <author>
      <name>/u/itsjustmarky</name>
      <uri>https://old.reddit.com/user/itsjustmarky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is really out there, and I've tried a lot and have yet to find a solution.&lt;/p&gt; &lt;p&gt;First off, my system.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Ryzen 5950X 32G DDR4 Asus Dark Hero RTX 6000 Pro Workstation 600W RTX 6000 pro Workstation 600W Arch Linux &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here's where things gets weird, I've been running this system with zero problems for months. I usually run GLM Air or MiniMax M2 on it 24/7. I use sglang, and it just works. Never a hiccup.&lt;/p&gt; &lt;p&gt;I started to test some other models, which I started to use vLLM for. After 30 minutes to a couple hours, I lose connection to it on the lan. The gpus go blank and I can't see the error or anything through my IP KVM.&lt;/p&gt; &lt;p&gt;This happens any model I load with vLLM. I later figured out, it happens even if I just start the server and I don't load anything at all.&lt;/p&gt; &lt;p&gt;My first feeling was a power issue, I do power limit the gpus to 300W and it idles at around 124W. I have a 1200W PSU and the system never breaks 825W, but it always is happening when it is idle. I even removed the power limit to see if it was a power limit issue. I've used nvidia persistent mode to keep it out of p8 state to see if it was just getting too low clock and locking the gpu.&lt;/p&gt; &lt;p&gt;Things I tried:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;* Removing 300W power limit * Nvidia persistent mode * Disabling pcie_aspm * Setting processor max cstate to 1 and enabling idle=nomwait * iommu=pt * disabled sleep * disabled virtualization * nvidia locked clocks -lgc 300,1800 * latest nvidia drivers * older nvidia drivers &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I've tried everything I can think of, it's absolutely bizarre sglang will run for months with no issues, yet anything else just dies in a couple of hours.&lt;/p&gt; &lt;p&gt;I've left watch nvidia-smi running and when the system gets disconnected, I have confirmed it is in p5 state, so it have managed to keep it out of lower power states to eliminate any weird locking that might happen if the gpus power down.&lt;/p&gt; &lt;p&gt;When it happens, all my SSH sessions just show a disconnection. I can't ping the server, I can't see any output on the display port, and the system looks like it is running and takes normal power ~124w as if it is running but not actively doing anything.&lt;/p&gt; &lt;p&gt;I know it isn't ram, even when it is in full tilt, the ram usage is tiny as I only use gpu.&lt;/p&gt; &lt;p&gt;I never go over 824W, so the psu is never stressed.&lt;/p&gt; &lt;p&gt;It is stable as a rock and very fast (~630 tokens/sec with Mini Max M2.1 when using parallel tasks).&lt;/p&gt; &lt;p&gt;I haven't found anything useful in the logs, as it just stops cold turkey and I have no errors to work off.&lt;br /&gt; It isn't heat as the temps are extremely low and it's always when it is idle.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/itsjustmarky"&gt; /u/itsjustmarky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q82wak/completely_stumped_with_strange_issue_with_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q82wak/completely_stumped_with_strange_issue_with_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q82wak/completely_stumped_with_strange_issue_with_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T08:28:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7nqxl</id>
    <title>llama.cpp has Out-of-bounds Write in llama-server</title>
    <updated>2026-01-08T20:56:15+00:00</updated>
    <author>
      <name>/u/radarsat1</name>
      <uri>https://old.reddit.com/user/radarsat1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Maybe good to know for some of you that might be running llama.cpp on a regular basis.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;llama.cpp is an inference of several LLM models in C/C++. In commits 55d4206c8 and prior, the n_discard parameter is parsed directly from JSON input in the llama.cpp server's completion endpoints without validation to ensure it's non-negative. When a negative value is supplied and the context fills up, llama_memory_seq_rm/add receives a reversed range and negative offset, causing out-of-bounds memory writes in the token evaluation loop. This deterministic memory corruption can crash the process or enable remote code execution (RCE). There is no fix at the time of publication.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Also reported &lt;a href="https://security-tracker.debian.org/tracker/CVE-2026-21869"&gt;for Debian&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/radarsat1"&gt; /u/radarsat1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cve.org/CVERecord?id=CVE-2026-21869"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7nqxl/llamacpp_has_outofbounds_write_in_llamaserver/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7nqxl/llamacpp_has_outofbounds_write_in_llamaserver/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T20:56:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q84j0r</id>
    <title>Is it just me or has CES really not delivered anything exciting for local LLM setups?</title>
    <updated>2026-01-09T10:10:02+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;CES this year has been strangely quiet imho. There's no big banger announcement. There's Phison with their AiDaptiv+ solution that supposedly extends VRAM to some SSD setup, but that's been talked about at Computex already and if I'm not mistaken a year ago, but nothing about availability. What do you think is the reason for this being so quiet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q84j0r/is_it_just_me_or_has_ces_really_not_delivered/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q84j0r/is_it_just_me_or_has_ces_really_not_delivered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q84j0r/is_it_just_me_or_has_ces_really_not_delivered/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T10:10:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1q82ae8</id>
    <title>Start of 2026 what’s the best open coding model?</title>
    <updated>2026-01-09T07:50:51+00:00</updated>
    <author>
      <name>/u/alexp702</name>
      <uri>https://old.reddit.com/user/alexp702</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using Qwen Coder 480b at 4 bit, and it’s ok for a first draft, but once it’s wrong it fills my code base with junk very quickly. I am mainly Typescript, but other languages interesting - PHP, C#, Python Java.&lt;/p&gt; &lt;p&gt;I have no time for 30b models, they are brain dead compared to the bigger ones. I hear good things about Kimi K2, GLM 4.7 etc but working with a model takes time and lots of junk code.&lt;/p&gt; &lt;p&gt;Are any noticeably better than Qwen 480b? I have a 512Gb Mac Studio, so something that fits on that. Speed unimportant - I can always do something else.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alexp702"&gt; /u/alexp702 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q82ae8/start_of_2026_whats_the_best_open_coding_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q82ae8/start_of_2026_whats_the_best_open_coding_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q82ae8/start_of_2026_whats_the_best_open_coding_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T07:50:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1q842tz</id>
    <title>kimi k3 model coming with 500m funding. anyone tested k2 thinking mode for coding?</title>
    <updated>2026-01-09T09:42:03+00:00</updated>
    <author>
      <name>/u/Jealous-Leek-5428</name>
      <uri>https://old.reddit.com/user/Jealous-Leek-5428</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;moonshot (kimi) just closed 500m series c. idg led, alibaba and tencent followed. funding going to k3 model development and compute expansion.&lt;/p&gt; &lt;p&gt;k2 thinking mode already out. scored decent on benchmarks but curious about real world performance for coding tasks.&lt;/p&gt; &lt;p&gt;been testing k2 through verdent for a few weeks. the thinking mode is interesting , takes longer but sometimes catches edge cases better. had it trace through a race condition in async code that other models missed. not sure if thats consistent or just got lucky.&lt;/p&gt; &lt;p&gt;the approach feels similar to deepseek r1 reasoning but less verbose. doesnt show full chain of thought, just gives you the result after &amp;quot;thinking&amp;quot;.&lt;/p&gt; &lt;p&gt;api access has been inconsistent tho. sometimes fast responses, sometimes timeouts. not sure if thats capacity issues or just growing pains. verdent lets me switch between models easily so when kimi times out i just fall back to claude, but would prefer more stability.&lt;/p&gt; &lt;p&gt;compared to other chinese models (deepseek, glm, minimax), kimi seems more focused on reasoning over raw speed. wondering if k3 will push that further or try to balance both.&lt;/p&gt; &lt;p&gt;the 500m raise is interesting timing. glm just dropped GLM4.7, minimax has m2.1 out. feels like chinese ai companies are in a different funding cycle than western ones , massive war chests, less pressure to monetize immediately.&lt;/p&gt; &lt;p&gt;also curious if anyone knows technical details about k3. havent seen much beyond &amp;quot;better reasoning&amp;quot; in the announcements.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jealous-Leek-5428"&gt; /u/Jealous-Leek-5428 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q842tz/kimi_k3_model_coming_with_500m_funding_anyone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q842tz/kimi_k3_model_coming_with_500m_funding_anyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q842tz/kimi_k3_model_coming_with_500m_funding_anyone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T09:42:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7jd1a</id>
    <title>LFM2.5 1.2B Instruct is amazing</title>
    <updated>2026-01-08T18:17:04+00:00</updated>
    <author>
      <name>/u/Paramecium_caudatum_</name>
      <uri>https://old.reddit.com/user/Paramecium_caudatum_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This model punches way above its weight. It outperforms every other model I've tried in this size range and runs smoothly on basically any hardware. If you haven't tried it yet, you definitely should.&lt;/p&gt; &lt;p&gt;Important note:&lt;br /&gt; &amp;quot;&amp;quot;&amp;quot;&lt;br /&gt; We recommend using it for agentic tasks, data extraction, and RAG. It is not recommended for knowledge-intensive tasks and programming.&lt;/p&gt; &lt;p&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct"&gt;https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Paramecium_caudatum_"&gt; /u/Paramecium_caudatum_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7jd1a/lfm25_12b_instruct_is_amazing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7jd1a/lfm25_12b_instruct_is_amazing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7jd1a/lfm25_12b_instruct_is_amazing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T18:17:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1q80265</id>
    <title>Introducing nanoRLHF project!</title>
    <updated>2026-01-09T05:42:29+00:00</updated>
    <author>
      <name>/u/hyunwoongko</name>
      <uri>https://old.reddit.com/user/hyunwoongko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to introduce nanoRLHF, a project I have been actively developing over the past three months.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/hyunwoongko/nanoRLHF"&gt;https://github.com/hyunwoongko/nanoRLHF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;nanoRLHF is a project that implements almost all core components of RLHF from scratch using only PyTorch and Triton. Each module is an educational reimplementation of large scale systems, prioritizing clarity and core ideas over efficiency. The project includes minimal Python implementations inspired by Apache Arrow, Ray, Megatron-LM, vLLM, and verl. It also contains several custom Triton kernels that I implemented directly, including Flash Attention.&lt;/p&gt; &lt;p&gt;In addition, it provides SFT and RL training pipelines that leverage open source math datasets to train a small Qwen3 model. By training a Qwen3 base model, I was able to achieve Math-500 performance comparable to the official Qwen3 Instruct model. I believe this can be excellent learning material for anyone who wants to understand how RL training frameworks like verl work internally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hyunwoongko"&gt; /u/hyunwoongko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q80265/introducing_nanorlhf_project/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q80265/introducing_nanorlhf_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q80265/introducing_nanorlhf_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T05:42:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7zywf</id>
    <title>Devstral Small 2 (Q4_K_M) on 5060 Ti 16GB and Zed Agent is amazing!</title>
    <updated>2026-01-09T05:37:33+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: Here's my setup&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PC: RTX 5060 Ti 16GB, 32GB DDR5-6000 (just flexing, no RAM offloading needed here)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lmstudio-community/Devstral-Small-2-24B-Instruct-2512-GGUF"&gt;Devstral-Small-2-24B-Instruct-2512-GGUF&lt;/a&gt;, Q4_K_M, 24k context length (the lmstudio-community version was slightly faster than the one from mistral)&lt;/li&gt; &lt;li&gt;Zed editor (with Zed Agent)&lt;/li&gt; &lt;li&gt;Performance: tg 9-11 tok/s, pp ~648tok/s&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;After many failed attempts (Qwen3 Coder 30B A3B was too big for a meaningful tg speed on my card, anything smaller than 14B was trash,...) I almost gave up on the dream of having a local AI coding setup.&lt;/p&gt; &lt;p&gt;Tonight, while scrolling through &lt;a href="https://swe-rebench.com/"&gt;swe-rebench&lt;/a&gt;, I noticed that Devstral Small 2 was actually ranked above Minimax M2, and just below Kimi K2 and Minimax M2.1, I decided to give it a try.&lt;/p&gt; &lt;p&gt;I was skeptical about a dense 24B model at first, but turned out, the key is to fit everything in the GPU's 16GB VRAM, so it won't offload anything to the RAM, maintaining a good tg speed. For my case, with a 24k context, that's about 15.2GB on the card.&lt;/p&gt; &lt;p&gt;The model works great in both Claude Code and Zed Editor, by great I mean the ability to produce a thinking, then chain of tool calls to explore the codebase, read multiple files, making edits, run commands to build/test.&lt;/p&gt; &lt;p&gt;I find that using Zed Agent was slightly faster than Claude Code because the system prompt was much shorter, so I still have plently of context window for the actual project's code.&lt;/p&gt; &lt;p&gt;For the code quality, it's a mix, I let it work on a few examples using my custom Rust framework. &lt;/p&gt; &lt;p&gt;For the first attempt, I tried with a very short instruction (just like what I usually do with... Opus 4.5), something like &amp;quot;build a multi agent example using this framework&amp;quot;. Devstral generated the code but ran into some cloning issues, then it went on to modify the framework to make the code work (a classical LLM's hack).&lt;/p&gt; &lt;p&gt;When I retried with a more detailed instruction, including a clear plan and some reference code, the model was able to generate the code, run build commands to test, takes a few rounds and a few rewrites but in the end, it completed the task without me having to intervene or clarify anything else.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/9wMI57W.png"&gt;screenshot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The performance was great too, prompt processing was around ~600-650 tok/s, token gen was around 9-11 tok/s, the GPU never ran above 45C, the fans weren't too loud. And I haven't run into looping issue like other posts in this sub mentioned.&lt;/p&gt; &lt;p&gt;So I guess I can postpone the plan to sell my kidney for a 2nd GPU or a Claude Max plan now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7zywf/devstral_small_2_q4_k_m_on_5060_ti_16gb_and_zed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7zywf/devstral_small_2_q4_k_m_on_5060_ti_16gb_and_zed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7zywf/devstral_small_2_q4_k_m_on_5060_ti_16gb_and_zed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T05:37:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7mvuf</id>
    <title>Z.ai (the AI lab behind GLM) has officially IPO'd on the Hong Kong Stock Exchange</title>
    <updated>2026-01-08T20:23:59+00:00</updated>
    <author>
      <name>/u/Old-School8916</name>
      <uri>https://old.reddit.com/user/Old-School8916</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-School8916"&gt; /u/Old-School8916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Zai_org/status/2009290783678239032"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7mvuf/zai_the_ai_lab_behind_glm_has_officially_ipod_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7mvuf/zai_the_ai_lab_behind_glm_has_officially_ipod_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T20:23:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7ysj2</id>
    <title>We benchmarked every 4-bit quantization method in vLLM 👀</title>
    <updated>2026-01-09T04:38:29+00:00</updated>
    <author>
      <name>/u/LayerHot</name>
      <uri>https://old.reddit.com/user/LayerHot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7ysj2/we_benchmarked_every_4bit_quantization_method_in/"&gt; &lt;img alt="We benchmarked every 4-bit quantization method in vLLM 👀" src="https://b.thumbs.redditmedia.com/m8O7xkrgA45EVdPp2UmUufoulHAEZRrosao1Uv_SFws.jpg" title="We benchmarked every 4-bit quantization method in vLLM 👀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just published a deep dive on vLLM quantization. Tested AWQ, GPTQ, Marlin, GGUF, and BitsandBytes on Qwen2.5-32B using an H200.&lt;/p&gt; &lt;p&gt;Stuff we found:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Marlin hits 712 tok/s, baseline FP16 does 461. Quantized and faster.&lt;/li&gt; &lt;li&gt;GPTQ without Marlin kernel is actually slower than FP16 (276 tok/s)&lt;/li&gt; &lt;li&gt;BitsandBytes had the smallest quality drop and doesn't need pre-quantized weights&lt;/li&gt; &lt;li&gt;GGUF had the worst perplexity but best HumanEval score among quantized methods&lt;/li&gt; &lt;li&gt;AWQ was weirdly slow in vLLM (67 tok/s)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Blog covers how each technique actually works under the hood if you want the details.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/t4212ygj59cg1.png?width=3169&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=97eff0fcb212924355a7feb7262b25895de5603a"&gt;https://preview.redd.it/t4212ygj59cg1.png?width=3169&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=97eff0fcb212924355a7feb7262b25895de5603a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://docs.jarvislabs.ai/blog/vllm-quantization-complete-guide-benchmarks"&gt;https://docs.jarvislabs.ai/blog/vllm-quantization-complete-guide-benchmarks&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LayerHot"&gt; /u/LayerHot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7ysj2/we_benchmarked_every_4bit_quantization_method_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7ysj2/we_benchmarked_every_4bit_quantization_method_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7ysj2/we_benchmarked_every_4bit_quantization_method_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T04:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7d8bj</id>
    <title>Jensen Huang saying "AI" 121 times during the NVIDIA CES keynote - cut with one prompt</title>
    <updated>2026-01-08T14:29:47+00:00</updated>
    <author>
      <name>/u/Prior-Arm-6705</name>
      <uri>https://old.reddit.com/user/Prior-Arm-6705</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/"&gt; &lt;img alt="Jensen Huang saying &amp;quot;AI&amp;quot; 121 times during the NVIDIA CES keynote - cut with one prompt" src="https://external-preview.redd.it/M2cyNzBqaHB4NGNnMeuNas4_kS8fQc08s_eqp1ss4JB4szq45v23OyPEbFog.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a660094eff764f4c2c968c5de87ba1bafcb35b9" title="Jensen Huang saying &amp;quot;AI&amp;quot; 121 times during the NVIDIA CES keynote - cut with one prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone had to count it. Turns out Jensen said &amp;quot;AI&amp;quot; exactly 121 times in the CES 2025 keynote.&lt;/p&gt; &lt;p&gt;I used &lt;a href="https://github.com/OpenAgentPlatform/Dive"&gt;https://github.com/OpenAgentPlatform/Dive&lt;/a&gt; (open-source MCP client) + two MCPs I made:&lt;/p&gt; &lt;p&gt;- &lt;a href="https://github.com/kevinwatt/yt-dlp-mcp"&gt;https://github.com/kevinwatt/yt-dlp-mcp&lt;/a&gt; - YouTube download&lt;br /&gt; - &lt;a href="https://github.com/kevinwatt/ffmpeg-mcp-lite"&gt;https://github.com/kevinwatt/ffmpeg-mcp-lite&lt;/a&gt; - video editing&lt;/p&gt; &lt;p&gt;&lt;strong&gt;One prompt:&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Task: Create a compilation video of every exact moment Jensen Huang says &amp;quot;AI&amp;quot;.&lt;br /&gt; Video source: &lt;a href="https://www.youtube.com/watch?v=0NBILspM4c4"&gt;https://www.youtube.com/watch?v=0NBILspM4c4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Instructions:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Download video in 720p + subtitles in JSON3 format (word-level timestamps)&lt;/p&gt; &lt;p&gt;Parse JSON3 to find every &amp;quot;AI&amp;quot; instance with precise start/end times&lt;/p&gt; &lt;p&gt;Use ffmpeg to cut clips (~50-100ms padding for natural sound)&lt;/p&gt; &lt;p&gt;Concatenate all clips chronologically&lt;/p&gt; &lt;p&gt;Output: Jensen_CES_AI.mp4&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Dive chained the two MCPs together - download → parse timestamps → cut 121 clips → merge. All local, no cloud.&lt;/p&gt; &lt;p&gt;If you want to see how it runs: &lt;a href="https://www.youtube.com/watch?v=u_7OtyYAX74"&gt;https://www.youtube.com/watch?v=u_7OtyYAX74&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The result is... hypnotic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prior-Arm-6705"&gt; /u/Prior-Arm-6705 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hein55gpx4cg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T14:29:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1q82zdm</id>
    <title>Minimax also live on Hong Kong Stock Exchange</title>
    <updated>2026-01-09T08:33:27+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q82zdm/minimax_also_live_on_hong_kong_stock_exchange/"&gt; &lt;img alt="Minimax also live on Hong Kong Stock Exchange" src="https://preview.redd.it/999goi9xbacg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5d20235c10219672401efcf3df3bcdf3da53b9a5" title="Minimax also live on Hong Kong Stock Exchange" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/999goi9xbacg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q82zdm/minimax_also_live_on_hong_kong_stock_exchange/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q82zdm/minimax_also_live_on_hong_kong_stock_exchange/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T08:33:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7uuxo</id>
    <title>OK I get it, now I love llama.cpp</title>
    <updated>2026-01-09T01:39:13+00:00</updated>
    <author>
      <name>/u/vulcan4d</name>
      <uri>https://old.reddit.com/user/vulcan4d</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just made the switch from Ollama to llama.cpp. Ollama is fantastic for the beginner because it lets you super easily run LLMs and switch between them all. Once you realize what you truly want to run, llama.cpp is really the way to go.&lt;/p&gt; &lt;p&gt;My hardware ain't great, I have a single 3060 12GB GPU and three P102-100 GPUs for a total of 42GB. My system ram is 96GB along with an Intel i7-9800x. It blows my mind that with some tuning what difference it can make. You really need to understand each of the commands for llama.cpp to get the most out of it especially with uneven vram like mine. I used Chatgpt, Perplexity and suprisingly only Google AI studio could optimize my settings while teaching me along the way.&lt;/p&gt; &lt;p&gt;Crazy how these two commands both fill up the ram but one is twice as fast as the other. Chatgpt helped me with the first one, Google AI with the other ;). Now I'm happy running local lol.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;11t/s:&lt;/strong&gt;&lt;br /&gt; sudo pkill -f llama-server; sudo nvidia-smi --gpu-reset -i 0,1,2,3 || true; sleep 5; sudo CUDA_VISIBLE_DEVICES=0,1,2,3 ./llama-server --model /home/llm/llama.cpp/models/gpt-oss-120b/Q4_K_M/gpt-oss-120b-Q4_K_M-00001-of-00002.gguf --n-gpu-layers 21 --main-gpu 0 --flash-attn off --cache-type-k q8_0 --cache-type-v f16 --ctx-size 30000 --port 8080 --host &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; --mmap --numa distribute --batch-size 384 --ubatch-size 256 --jinja --threads $(nproc) --parallel 2 --tensor-split 12,10,10,10 --mlock&lt;/p&gt; &lt;p&gt;&lt;strong&gt;21t/s&lt;/strong&gt;&lt;br /&gt; sudo pkill -f llama-server; sudo nvidia-smi --gpu-reset -i 0,1,2,3 || true; sleep 5; sudo GGML_CUDA_ENABLE_UNIFIED_MEMORY=0 CUDA_VISIBLE_DEVICES=0,1,2,3 ./llama-server --model /home/llm/llama.cpp/models/gpt-oss-120b/Q4_K_M/gpt-oss-120b-Q4_K_M-00001-of-00002.gguf --n-gpu-layers 99 --main-gpu 0 --split-mode layer --tensor-split 5,5,6,20 -ot &amp;quot;blk\.(2[1-9]|[3-9][0-9])\.ffn_.*_exps\.weight=CPU&amp;quot; --ctx-size 30000 --port 8080 --host &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; --batch-size 512 --ubatch-size 256 --threads 8 --parallel 1 --mlock&lt;/p&gt; &lt;p&gt;Nothing here is worth copying and pasting as it is unique to my config but the moral of the story is, if you tune llama.cpp this thing will FLY!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vulcan4d"&gt; /u/vulcan4d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7uuxo/ok_i_get_it_now_i_love_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7uuxo/ok_i_get_it_now_i_love_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7uuxo/ok_i_get_it_now_i_love_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-09T01:39:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q7qcux</id>
    <title>The NO FAKES Act has a "Fingerprinting" Trap that kills Open Source. We need to lobby for a Safe Harbor.</title>
    <updated>2026-01-08T22:33:33+00:00</updated>
    <author>
      <name>/u/PostEasy7183</name>
      <uri>https://old.reddit.com/user/PostEasy7183</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, ​I’ve been reading the text of the &amp;quot;NO FAKES Act&amp;quot; currently in Congress, and it’s worse than I thought. ​The Tldr: It creates a &amp;quot;digital replica right&amp;quot; for voices/likenesses. That sounds fine for stopping deepfake porn, but the liability language is a trap. It targets anyone who &amp;quot;makes available&amp;quot; a tool that is primarily used for replicas.&lt;br /&gt; ​The Problem: If you release a TTS model or a voice-conversion RVC model on HuggingFace, and someone else uses it to fake a celebrity, you (the dev) can be liable for statutory damages ($5k-$25k per violation). ​There is no Section 230 protection here. This effectively makes hosting open weights for audio models a legal s*icide mission unless you are OpenAI or Google.&lt;/p&gt; &lt;p&gt;What I did: I contacted my reps email to flag this as an &amp;quot;innovation killer.&amp;quot; If you run a repo or care about open weights, you might want to do the same. We need them to add a &amp;quot;Safe Harbor&amp;quot; for tool devs.&lt;/p&gt; &lt;p&gt;S.1367 - 119th Congress (2025-2026): NO FAKES Act of 2025 | Congress.gov | Library of Congress &lt;a href="https://share.google/u6dpy7ZQDvZWUrlfc"&gt;https://share.google/u6dpy7ZQDvZWUrlfc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;UPDATE: ACTION ITEMS (How to actually stop this) ​If you don't want to go to jail for hosting a repo, you need to make noise now. ​1. The &amp;quot;Lazy&amp;quot; Email (Takes 30 seconds): Go to Democracy.io or your Senator’s contact page. ​Subject: Opposition to NO FAKES Act (H.R. 2794 / S. 1367) - Open Source Liability ​Message: &amp;quot;I am a constituent and software engineer. I oppose the NO FAKES Act unless it includes a specific Safe Harbor for Open Source Code Repositories. The current 'Digital Fingerprinting' requirement (Section 3) is technically impossible for raw model weights to comply with. This bill effectively bans open-source AI hosting in the US and hands a monopoly to Big Tech. Please amend it to protect tool developers.&amp;quot; ​2. The &amp;quot;Nuclear&amp;quot; Option (Call them): ​Call the Capitol Switchboard: (202) 224-3121 ​Ask for Senators Wyden (D) or Massie (R) if you want to thank them for being tech-literate, or call your own Senator to complain. ​Script: &amp;quot;The NO FAKES Act kills open-source innovation. We need a Safe Harbor for developers who write code, separate from the bad actors who use it.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PostEasy7183"&gt; /u/PostEasy7183 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-08T22:33:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
