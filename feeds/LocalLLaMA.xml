<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-10T11:34:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o2y6xi</id>
    <title>Help! Is this good enough for daily AI coding</title>
    <updated>2025-10-10T11:21:56+00:00</updated>
    <author>
      <name>/u/IntroductionSouth513</name>
      <uri>https://old.reddit.com/user/IntroductionSouth513</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys just checking if anyone has any advice if the below specs are good enough for daily AI assisted coding pls. not looking for those highly specialized AI servers or machines as I'm using it for personal gaming too. I got the below advice from chatgpt. thanks so much &lt;/p&gt; &lt;hr /&gt; &lt;p&gt;for daily coding: Qwen2.5-Coder-14B (speed) and Qwen2.5-Coder-32B (quality).&lt;/p&gt; &lt;p&gt;your box can also run 70B+ via offload, but it‚Äôs not as smooth for iterative dev.&lt;/p&gt; &lt;p&gt;pair with Ollama + Aider (CLI) or VS Code + Continue (GUI) and you‚Äôre golden.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;CPU: AMD Ryzen 7 7800X3D | 5 GHz | 8 cores 16 threads Motherboard: ASRock Phantom Gaming X870 Riptide WiFi GPU: Inno3D NVIDIA GeForce RTX 5090 | 32 GB VRAM RAM: 48 GB DDR5 6000 MHz Storage: 2 TB Gen 4 NVMe SSD CPU Cooler: Armaggeddon Deepfreeze 360 AIO Liquid Cooler Chassis: Armaggeddon Aquaron X-Curve Giga 10 Chassis Fans: Armaggeddon 12 cm x 7 PSU: Armaggeddon Voltron 80+ Gold 1200W Wi-Fi + Bluetooth: Included OS: Windows 11 Home 64-bit (Unactivated) Service: 3-Year In-House PC Cleaning Warranty: 5-Year Limited Warranty (1st year onsite pickup &amp;amp; return) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IntroductionSouth513"&gt; /u/IntroductionSouth513 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2y6xi/help_is_this_good_enough_for_daily_ai_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2y6xi/help_is_this_good_enough_for_daily_ai_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2y6xi/help_is_this_good_enough_for_daily_ai_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T11:21:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2y87n</id>
    <title>Why there's still no local models that can output PDF/DOCX files</title>
    <updated>2025-10-10T11:23:49+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can't seem to find any model that can output files suck as PDF or Docx like chatGPT, locally or via API, Any reason why? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2y87n/why_theres_still_no_local_models_that_can_output/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2y87n/why_theres_still_no_local_models_that_can_output/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2y87n/why_theres_still_no_local_models_that_can_output/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T11:23:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2ydqm</id>
    <title>What they didn't teach in software startup school: China</title>
    <updated>2025-10-10T11:32:07+00:00</updated>
    <author>
      <name>/u/kaggleqrdl</name>
      <uri>https://old.reddit.com/user/kaggleqrdl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the software startup school, china has mostly just been a source of talent. Maybe as a competitor, but largely only in China. &lt;/p&gt; &lt;p&gt;When it came to software tech startups in the US, they really only had to worry about other startups - usually in the bay area. And the worry was limited as they all had the same financial constraints and similar need to eventually get ROI.&lt;/p&gt; &lt;p&gt;But China changes the rules of the game, and in ways I'm not sure investors quite appreciate - mostly because it's never been like this before in the software industry.&lt;/p&gt; &lt;p&gt;OpenAI, Anthropic and their &amp;quot;Get Big Fast&amp;quot; plan made sense because that's how it has always worked. The first one to get big fast was able to get network effects, brand goodwill, and economy of scale and suck up all the investment and attention. Other startups vying for the same space would just wither and die as all the oxygen was consumed.&lt;/p&gt; &lt;p&gt;China, however, is a new twist in how &amp;quot;Get Big Fast&amp;quot; is going to play out. Not only do they play by different economic rules, they also have different pools of capital not readily accessible to US players. Government will happily invest and clear the way. &lt;/p&gt; &lt;p&gt;And, ofc, it's not just China. Any country can enter this game, all they really need is capital. The moat is surprisingly thin and shallow. &lt;/p&gt; &lt;p&gt;Oh, and btw, it looks like every other country *wants* to enter this very important game.&lt;/p&gt; &lt;p&gt;So now OpenAI and Anthropic find themselves on a never ending training treadmill and they might just run out of oxygen as it speeds up faster than they can go. &lt;strong&gt;If they stop training the next latest and greatest, Chinese (and others) will most certainly catch up.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Inevitably, there are three potential outcomes to this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Regulatory capture and government intervention to keep out the chinese / open / other models, allowing OpenAI/Anthropic to squeeze profit out of their work by not having to train as much. We see a lot of signs of this revving up already, and I think is the most likely outcome under the guise of 'safety' and 'security'. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Pop Goes the Bubble - things start going asymptotic or even way worse - Chinese / other models innovate faster than the proprietary ones. Even if those other models go prop and not open, AI will become pretty commodified (unless the other models step-change innovate!). Either way, OpenAI and Anthropic lose their ability to command the attention of the industry and all that money they spent on 'Get Big Fast' isn't going to help them much.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;OpenAI / Anthropic are able to keep upping their game until AGI+ / ASI occurs and then all the rules change completely. Nobody can predict past the singularity, except that probably it's a good idea to be the first who made it happen. Maybe! &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Some weighted blend of them all is likely, ofc, though my money is mostly on #1. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaggleqrdl"&gt; /u/kaggleqrdl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2ydqm/what_they_didnt_teach_in_software_startup_school/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2ydqm/what_they_didnt_teach_in_software_startup_school/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2ydqm/what_they_didnt_teach_in_software_startup_school/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T11:32:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2uulc</id>
    <title>What's your experience with quantizing MoE with tiny experts?</title>
    <updated>2025-10-10T07:57:49+00:00</updated>
    <author>
      <name>/u/arimoto02</name>
      <uri>https://old.reddit.com/user/arimoto02</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As i've read, quantizing a small model of size less than 8B can seriously degrade their performance. But since MoE model (qwen30b with 3b experts, gpt-oss with 5b experts,...) are just a combination of small size experts, how is this affecting them? Can i quantize them to Q4, or should i only run them at Q8 and only quantize dense models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arimoto02"&gt; /u/arimoto02 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2uulc/whats_your_experience_with_quantizing_moe_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2uulc/whats_your_experience_with_quantizing_moe_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2uulc/whats_your_experience_with_quantizing_moe_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T07:57:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o25uy1</id>
    <title>OpenAI forum post: ‚ÄúTop 30 customers who‚Äôve used 1T+ tokens‚Äù (unconfirmed)</title>
    <updated>2025-10-09T13:24:58+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A list circulating via the OpenAI community forum claims 30 orgs (e.g., Duolingo, Shopify, Notion, Salesforce, T-Mobile) each crossed &lt;strong&gt;1T+ tokens&lt;/strong&gt; on OpenAI models. Interesting signal of who‚Äôs scaling‚Äî&lt;strong&gt;treat as unverified&lt;/strong&gt;. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Why it matters: points to heavy production use across edtech, SaaS, dev tools, and telecom. &lt;/li&gt; &lt;li&gt;Caveat: not officially confirmed; appears sourced from event chatter/screens. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Link to thread:&lt;br /&gt; &lt;a href="https://community.openai.com/t/openai-just-shared-the-top30-customers-whove-used-1t-tokens/1361452"&gt;&lt;code&gt;https://community.openai.com/t/openai-just-shared-the-top30-customers-whove-used-1t-tokens/1361452&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Company&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Industry / Product / Service&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Sector&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Type&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;Duolingo&lt;/td&gt; &lt;td align="left"&gt;Language learning platform&lt;/td&gt; &lt;td align="left"&gt;Education / EdTech&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;OpenRouter&lt;/td&gt; &lt;td align="left"&gt;AI model routing &amp;amp; API platform&lt;/td&gt; &lt;td align="left"&gt;AI Infrastructure&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;Indeed&lt;/td&gt; &lt;td align="left"&gt;Job search &amp;amp; recruitment platform&lt;/td&gt; &lt;td align="left"&gt;Employment / HR Tech&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;Salesforce&lt;/td&gt; &lt;td align="left"&gt;CRM &amp;amp; business cloud software&lt;/td&gt; &lt;td align="left"&gt;Enterprise SaaS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;CodeRabbit&lt;/td&gt; &lt;td align="left"&gt;AI code review assistant&lt;/td&gt; &lt;td align="left"&gt;Developer Tools&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;iSolutionsAI&lt;/td&gt; &lt;td align="left"&gt;AI automation &amp;amp; consulting&lt;/td&gt; &lt;td align="left"&gt;AI / Consulting&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;Outtake&lt;/td&gt; &lt;td align="left"&gt;AI for video and creative content&lt;/td&gt; &lt;td align="left"&gt;Media / Creative AI&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;Tiger Analytics&lt;/td&gt; &lt;td align="left"&gt;Data analytics &amp;amp; AI solutions&lt;/td&gt; &lt;td align="left"&gt;Data / Analytics&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9&lt;/td&gt; &lt;td align="left"&gt;Ramp&lt;/td&gt; &lt;td align="left"&gt;Finance automation &amp;amp; expense management&lt;/td&gt; &lt;td align="left"&gt;Fintech&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;Abridge&lt;/td&gt; &lt;td align="left"&gt;AI medical transcription &amp;amp; clinical documentation&lt;/td&gt; &lt;td align="left"&gt;Healthcare / MedTech&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;11&lt;/td&gt; &lt;td align="left"&gt;Sider AI&lt;/td&gt; &lt;td align="left"&gt;AI coding assistant&lt;/td&gt; &lt;td align="left"&gt;Developer Tools&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;12&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="http://Warp.dev"&gt;Warp.dev&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;AI-powered terminal&lt;/td&gt; &lt;td align="left"&gt;Developer Tools&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;13&lt;/td&gt; &lt;td align="left"&gt;Shopify&lt;/td&gt; &lt;td align="left"&gt;E-commerce platform&lt;/td&gt; &lt;td align="left"&gt;E-commerce / Retail Tech&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;14&lt;/td&gt; &lt;td align="left"&gt;Notion&lt;/td&gt; &lt;td align="left"&gt;Productivity &amp;amp; collaboration tool&lt;/td&gt; &lt;td align="left"&gt;Productivity / SaaS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;15&lt;/td&gt; &lt;td align="left"&gt;WHOOP&lt;/td&gt; &lt;td align="left"&gt;Fitness wearable &amp;amp; health tracking&lt;/td&gt; &lt;td align="left"&gt;Health / Wearables&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;HubSpot&lt;/td&gt; &lt;td align="left"&gt;CRM &amp;amp; marketing automation&lt;/td&gt; &lt;td align="left"&gt;Marketing / SaaS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;17&lt;/td&gt; &lt;td align="left"&gt;JetBrains&lt;/td&gt; &lt;td align="left"&gt;Developer IDE &amp;amp; tools&lt;/td&gt; &lt;td align="left"&gt;Developer Tools&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;18&lt;/td&gt; &lt;td align="left"&gt;Delphi&lt;/td&gt; &lt;td align="left"&gt;AI data analysis &amp;amp; decision support&lt;/td&gt; &lt;td align="left"&gt;Data / AI&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;19&lt;/td&gt; &lt;td align="left"&gt;Decagon&lt;/td&gt; &lt;td align="left"&gt;AI communication for healthcare&lt;/td&gt; &lt;td align="left"&gt;Healthcare / MedTech&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;20&lt;/td&gt; &lt;td align="left"&gt;Rox&lt;/td&gt; &lt;td align="left"&gt;AI automation &amp;amp; workflow tools&lt;/td&gt; &lt;td align="left"&gt;AI / Productivity&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;21&lt;/td&gt; &lt;td align="left"&gt;T-Mobile&lt;/td&gt; &lt;td align="left"&gt;Telecommunications provider&lt;/td&gt; &lt;td align="left"&gt;Telecom&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;22&lt;/td&gt; &lt;td align="left"&gt;Zendesk&lt;/td&gt; &lt;td align="left"&gt;Customer support software&lt;/td&gt; &lt;td align="left"&gt;Customer Service / SaaS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;23&lt;/td&gt; &lt;td align="left"&gt;Harvey&lt;/td&gt; &lt;td align="left"&gt;AI assistant for legal professionals&lt;/td&gt; &lt;td align="left"&gt;Legal Tech&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;24&lt;/td&gt; &lt;td align="left"&gt;Read AI&lt;/td&gt; &lt;td align="left"&gt;AI meeting summary &amp;amp; productivity tools&lt;/td&gt; &lt;td align="left"&gt;Productivity / AI&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;25&lt;/td&gt; &lt;td align="left"&gt;Canva&lt;/td&gt; &lt;td align="left"&gt;Graphic design &amp;amp; creative tools&lt;/td&gt; &lt;td align="left"&gt;Design / SaaS&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;26&lt;/td&gt; &lt;td align="left"&gt;Cognition&lt;/td&gt; &lt;td align="left"&gt;AI coding agent (Devin)&lt;/td&gt; &lt;td align="left"&gt;Developer Tools&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;27&lt;/td&gt; &lt;td align="left"&gt;Datadog&lt;/td&gt; &lt;td align="left"&gt;Cloud monitoring &amp;amp; observability&lt;/td&gt; &lt;td align="left"&gt;Cloud / DevOps&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;28&lt;/td&gt; &lt;td align="left"&gt;Perplexity&lt;/td&gt; &lt;td align="left"&gt;AI search engine&lt;/td&gt; &lt;td align="left"&gt;AI Search / Information&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;29&lt;/td&gt; &lt;td align="left"&gt;Mercado Libre&lt;/td&gt; &lt;td align="left"&gt;E-commerce &amp;amp; fintech (LatAm)&lt;/td&gt; &lt;td align="left"&gt;E-commerce / Fintech&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Scaled&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;Genspark AI&lt;/td&gt; &lt;td align="left"&gt;AI education &amp;amp; training platform&lt;/td&gt; &lt;td align="left"&gt;Education / AI&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Startup&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o25uy1/openai_forum_post_top_30_customers_whove_used_1t/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o25uy1/openai_forum_post_top_30_customers_whove_used_1t/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o25uy1/openai_forum_post_top_30_customers_whove_used_1t/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T13:24:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2wjyg</id>
    <title>finetuning Medium or Small language model for factual and memorizing data.</title>
    <updated>2025-10-10T09:48:57+00:00</updated>
    <author>
      <name>/u/InteractionLevel6625</name>
      <uri>https://old.reddit.com/user/InteractionLevel6625</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a builder projects data in a csv. The issues with RAG is that it is fetching non similar data and it is fetching lot of unwanted data. Also there is a limitation of context length. &lt;/p&gt; &lt;p&gt;So I'm planning to fine tune llama 3.1 on my data. And if i ask any question related to that data it should give me the answer like if i say i want to buy a flat in marathalli then it should give me the project names.&lt;/p&gt; &lt;p&gt;I have two options to fine tune. one is supervised FT where i give question answer pairs and other unsupervised FT which is a next token prediction or CLM.&lt;/p&gt; &lt;p&gt;This is how my data look like &lt;/p&gt; &lt;p&gt;Project_ID,Project_Name,Project_Developer_Name,Project_Area,Project_Total_Units,Project_Description,Project_Advantage,Project_Specification,Project_Address,Project_Latitude,Project_Longitude,Project_Auto_Description,Project_Possession_Date,Project_Launch_Date,country,state,city,project_status,Locality,Total_Towers,Minimum_Tower_Floors,Maximum_Tower_Floors,Total_Unique_Configuration_Units_Count,Property_Type,Unique_BHK_Type_Count,Available_BHK_Types,Amenity_Types_And_Amenities,Landmark_Between_3Km_to_5Km,Landmark_Within_3Km,Phase_possession,rag_docs.....these are COlumn names.&lt;/p&gt; &lt;p&gt;5000001,BSR Paradise,Winning Edge Group,Data Unavailable,100.0,&amp;quot;BSR Paradise is located in the suburb of Bangalore city,‚Äô Marathahalli‚Äô. In this era, where work has become quite hectic, if you get a chance to live in amidst of nature than that‚Äôs not the bad deal, isn‚Äôt it. Healthy living begins with a healthy, natural lifestyleThe township is located in Panathur locality hardly 1 km away from Marathahalli Bridge. It is a multi-storeyed building having 2 blocks and 6 floors. The township offers you 2BHK flats (1100-1900 sq. ft) and 3BHK flats (1300-1400 sq. ft). BSR Paradise makes it possible to live a life which is healthy and in the lap of nature along with landscaped gardens and different kinds of trees around you. The project provides all the residence for sale.Some of the other amenities that are made available to the residents are sufficient covered parking, garden, gym area, rain water harvesting, community hall, club house and much more. Railway station, metro, ATM and hospitals are within 3 km of this project. The project will allow the residents to live a lavish life. &amp;quot;,Data Unavailable,Data Unavailable,Data Unavailable,12.93162,77.697706,&amp;quot;BSR Paradise StatusReady To MoveBSR Paradise Launch Date30 October 2011BSR Paradise Possession Date01 August 2013Towers in BSR Paradise1Situated at a prime location of Marathahalli, BSR Paradise is a meticulously designed project of Bangalore. The property comprises of 100 units which are enclosed within a peaceful environment. The commencement certificate of the impressive BSR Paradise project has not been grantedIn addition to this, the occupancy certificate not granted. BSR Paradise project is an offering from the well-established developer Winning Edge Group. The project's pin code is 560037. BSR Paradise lets you enjoy a convenient lifestyle with all contemporary conveniences at your disposal. Top Amenities in BSR ParadiseLiftMaintenance StaffWaste DisposalInternet/Wi-Fi ConnectivityDTH Television FacilityRO Water SystemConference Room&amp;quot;,2013-08-01,2011-10-30,India,Karnataka,Bangalore,Ready To Move,Marathahalli,5.0,20.0,21.0,35.0,&amp;quot;Residential Plot,Multistorey Apartment&amp;quot;,3.0,&amp;quot;1BHK,2BHK,3BHK&amp;quot;,&amp;quot;Exteriror Amenities: Lift,Rain Water Harvesting,Club House,Swimming Pool,Gymnasium,Park,Reserved Parking,Security,Water Storage,Visitor Parking,Maintenance Staff,Waste Disposal,DTH Television Facility,Conference Room&lt;/p&gt; &lt;p&gt;Interiror Amenities: Vaastu Compliant,Air Conditioned,Intercom Facility,Internet/Wi-Fi Connectivity,RO Water System,Piped Gas&lt;/p&gt; &lt;p&gt;Project Amenities: Coffee Lounge &amp;amp; Restaurants,Flower Gardens,Kids Play Area,Fire Fighting Equipment&amp;quot;,Data Unavailable,Data Unavailable,Data Unavailable,&amp;quot;BSR Paradise, developed by Winning Edge Group, is located in Marathahalli, Bangalore, at coordinates 12.93162 latitude and 77.697706 longitude. This residential project features 100 units across 5 towers, each with 20 to 21 floors. The available configurations include 2BHK flats ranging from 1100 to 1900 sq. ft and 3BHK flats from 1300 to 1400 sq. ft. The project is ready to move in, having launched on October 30, 2011, with possession starting from August 1, 2013.&lt;/p&gt; &lt;p&gt;BSR Paradise offers a blend of nature and modern living with landscaped gardens and ample amenities, including a gym, clubhouse, swimming pool, and community hall. Additional features include covered parking, rainwater harvesting, and security services. The project is conveniently located within 3 km of essential services like railway stations, metro stations, ATMs, and hospitals, enhancing connectivity and lifestyle. Interior amenities include air conditioning, intercom facilities, and Wi-Fi connectivity, ensuring a comfortable living experience.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InteractionLevel6625"&gt; /u/InteractionLevel6625 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2wjyg/finetuning_medium_or_small_language_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2wjyg/finetuning_medium_or_small_language_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2wjyg/finetuning_medium_or_small_language_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T09:48:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2prgn</id>
    <title>Funny/Humor LLMs</title>
    <updated>2025-10-10T03:03:18+00:00</updated>
    <author>
      <name>/u/ionlycreate42</name>
      <uri>https://old.reddit.com/user/ionlycreate42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How do LLMs handle humor? From what I understand, they basically learn by guessing what word comes next based on tons of text they‚Äôve seen. Over time, they get better at it by adjusting their internal weights.&lt;/p&gt; &lt;p&gt;So when you ask them to tell a joke, they can do it because they‚Äôve come across lots of jokes during training. They recognize the usual setups and punchlines. They can even explain why something might be funny, but it feels like they‚Äôre mostly repeating patterns instead of actually ‚Äúgetting‚Äù the joke. I know this is obvious but that leads me to the actual humor part. &lt;/p&gt; &lt;p&gt;I tried an experiment to test that. I gave the model a few jokes that I personally find funny, they weren‚Äôt the usual dad jokes or puns, and asked it to explain them. It didn‚Äôt really seem to understand why they were funny, so I added my own explanation and then asked it to make new jokes in the same style. What it came up with kind of looked like my sense of humor, but it still felt off. Like it was following the rules but didn‚Äôt have any real spark behind it.&lt;/p&gt; &lt;p&gt;My guess is that it‚Äôs copying the structure of the humor but not the feeling. That makes sense, since it doesn‚Äôt really ‚Äúunderstand‚Äù things like people do. It just works off patterns it‚Äôs learned from text.&lt;/p&gt; &lt;p&gt;I guess what I‚Äôm trying to figure out is how I should think about this. Am I understanding it right, or am I missing something important about how these models handle humor?&lt;/p&gt; &lt;p&gt;In short, my point is that it‚Äôs obvious that LLMs aren‚Äôt understanding like humans are, everyone on this sub knows that it‚Äôs just semantic understanding through multidimensional space. So while it can mimic jokes it‚Äôs seen or produce common answers to jokes it‚Äôs seen, (from my limited tests), it cannot produce jokes that make me laugh if we give it examples of what I find funny, it mostly takes the examples and produces the underlying structure of the text but the actual essence of what makes it funny disappears. This only happens when I explicitly have it look at the examples I like, and have it create novel humor and my expectation was some form of understanding of why I think it was funny, but it failed. Im not referring to when I make a joke and say it‚Äôs funny and then I tell it to disregard the structure and naturally generate humor without pattern, pseudoscience but that seems to work a bit better &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ionlycreate42"&gt; /u/ionlycreate42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2prgn/funnyhumor_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2prgn/funnyhumor_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2prgn/funnyhumor_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T03:03:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2e76z</id>
    <title>Deepmind notebook on how to finetune Gemma 3 270m</title>
    <updated>2025-10-09T18:43:51+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepmind just dropped a handy little colab on fine-tuning gemma3-270m for emoji generation. It's nothing SOTA, but it's a great notebook for learning TRL and fine-tuning.&lt;/p&gt; &lt;p&gt;This is a super lower resource task with 270m parameter model, qlora, short sequences. so it's a great one to try out locally or on colab. It's also a nice one to deploy in a js app with transformers.js.&lt;/p&gt; &lt;p&gt;fine tuning colab: &lt;a href="https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Fine_tune_Gemma_3_270M_for_emoji_generation.ipynb"&gt;https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Fine_tune_Gemma_3_270M_for_emoji_generation.ipynb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2e76z/deepmind_notebook_on_how_to_finetune_gemma_3_270m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2e76z/deepmind_notebook_on_how_to_finetune_gemma_3_270m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2e76z/deepmind_notebook_on_how_to_finetune_gemma_3_270m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T18:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2b1yo</id>
    <title>ReasonScape Evaluation: AI21 Jamba Reasoning vs Qwen3 4B vs Qwen3 4B 2507</title>
    <updated>2025-10-09T16:45:05+00:00</updated>
    <author>
      <name>/u/kryptkpr</name>
      <uri>https://old.reddit.com/user/kryptkpr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2b1yo/reasonscape_evaluation_ai21_jamba_reasoning_vs/"&gt; &lt;img alt="ReasonScape Evaluation: AI21 Jamba Reasoning vs Qwen3 4B vs Qwen3 4B 2507" src="https://b.thumbs.redditmedia.com/mXfHDrZxVVROKy_p9678yNQ7EdgzvWk9JT57FO-XqAw.jpg" title="ReasonScape Evaluation: AI21 Jamba Reasoning vs Qwen3 4B vs Qwen3 4B 2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's an open secret that LLM benchmarks are bullshit. I built &lt;a href="https://reasonscape.com/"&gt;ReasonScape&lt;/a&gt; to be different, lets see what it tells us about how AI21's latest drop compared to the high quality 4B we know and love.&lt;/p&gt; &lt;p&gt;My usual disclaimer is that these are all &lt;a href="https://github.com/the-crypt-keeper/reasonscape/blob/develop/docs/tasks.md"&gt;information processing tasks&lt;/a&gt; so I make no claims of performance on summarization, creative writing or similar tasks. This evaluation is a counting letters, tracking objects, doing math, following instructions kinda thing.&lt;/p&gt; &lt;p&gt;The second disclaimer is that I am sharing data from my &lt;a href="https://github.com/the-crypt-keeper/reasonscape/tree/develop"&gt;development branch&lt;/a&gt; that's not yet been published to the leaderboard or explorer apps - working on it, aiming for this weekend.&lt;/p&gt; &lt;p&gt;Caveats aside lets start with high-level views:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7rrhce1au3uf1.png?width=1349&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f4abfa1cbcca3c2e5b4931e8c8492be6bc3d10fe"&gt;Overview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In terms of average tokens, this model sits somewhere between the OG and 2507-Thinking. Performance was incredibly weak outside of 2 domains: Cars (&lt;a href="https://github.com/the-crypt-keeper/reasonscape/blob/develop/docs/tasks/cars.md"&gt;Spatial state tracking&lt;/a&gt;) and Dates (&lt;a href="https://github.com/the-crypt-keeper/reasonscape/blob/develop/docs/tasks/dates.md"&gt;Time operations&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;The reasonscape &lt;a href="https://github.com/the-crypt-keeper/reasonscape/blob/develop/docs/methodology.md"&gt;methodology &lt;/a&gt;requires me to run &lt;strong&gt;*a lot\&lt;/strong&gt;* of tests, but also gives us a way to look deeper inside the performance of each task:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z50u525o34uf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af5e03a87914f0904ae7d82d2edd2f1cbcb86080"&gt;Task Deep Dive 1: Arithmetic, Boolean, Brackets, Cars, Shuffle, Objects&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8c3i9xcq34uf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f78ed06f64910d1dec0c09ac7284a2cd0e85aeb"&gt;Task Deep Dive 2: Dates, Letters, Movie, Sequence, Shapes, Sort&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The original Qwen3-4B was a really strong model, the 2507 release that split it into two halves was a mixed bag - the resulting Thinking model is quite good but it does not universally outperform the OG, &lt;a href="https://github.com/the-crypt-keeper/reasonscape/blob/develop/docs/tasks/sequence.md"&gt;Sequence &lt;/a&gt;is an example of a task the 2507 regressed on.&lt;/p&gt; &lt;p&gt;Before final thoughts, lets directly compare Jamba to the OG Qwen3-4B across the gamut of tasks:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lpkrxumi44uf1.png?width=857&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=37102fa70a4780f987d27ec56a0eefbae349562c"&gt;Bar Plot: Jamba Reasoning 3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v2n3y3zn44uf1.png?width=854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c2bf95b75862dde463664a92ad223e961d9891b"&gt;Bar Plot: Qwen3-4B OG&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I think it's fair to say that task-performance of Jamba Reasoning 3B leaves much to be desired. &lt;a href="https://github.com/the-crypt-keeper/reasonscape/blob/develop/docs/tasks/letters.md"&gt;Letters &lt;/a&gt;is a parametric version the 'count rs in strawberry' tests and for a native-thinking model to fail it this hard is pretty embarassing imo.&lt;/p&gt; &lt;p&gt;The glaring problem with this model is &lt;strong&gt;truncation&lt;/strong&gt;. All these evaluations were run at &lt;strong&gt;8K context&lt;/strong&gt;, so naively you'd think truncation means the model is just running out of tokens but this is absolutely not the case - if you look at Dates for example all successfull responces are ~2K but truncation rate is still a crazy ~10% the just model loses its mind:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;We need to parse the problem: &amp;quot;Given a small set of sentences about a particular date, answer the provided question. Respond only with the final date in MM/DD/DD/YYYY[*six thousand more Y*] We need to find today's date given that Xavier moved to the city on 04/11/2023, and 299 days have passed since then. So we need to add 299 days to 04/11/2023.\n\nWe need to compute the date.\n\nLet's do step by step.\n\nStart date: April 11, 2023.\n\nAdd 299 days.\n\nWe can break into years, months, days.\n\nFirst, add full years. Since 299 days is less than a year, we can approximate.\n\nBut we need to be precise.\n\nWe can use a date calculator approach.\n\nLet's compute:\n\nApril 11, 2023 + 299 days.\n\nWe can add month by month, but need to account for days.\n\nLet's do it systematically.\n\nWe can use a date addition: 299 days from April 11, 2023.\n\nWe can break into quarters maybe.\n\nBut maybe easier: compute month by month.\n\nWe can start from April 11, 2023.\n\nAdd 30 days to get May 11, 2023.\n\nAdd another 30 days: June 11, 2023.\n\nAdd another 30 days: July 11, 2023.\n\nAdd another 30 days: August 11, 2023.\n\nAdd another 30 days: September 11, 2023.\n\nAdd another 30 days: October 11, 2023.\n\nAdd another 30 days: November 11, 2023.\n\nWait, but we only have 299 days. Let's see.\n\nWe can count the days month by month.\n\nFrom April 11 to next April 11 is 365 days? But we only add 299 days.\n\nLet's do a more precise method.\n\nWe can use the fact that 299 days from April 11, 2023.\n\nWe can compute by adding days month by month.\n\nApril 11, 2023 + 299 days.\n\nWe can break into months:\n\n- April: 30 days (from April 11 to April 30). That's 19 days? Wait, from April 11 to May 11 is 30 days? Let's do step by step.\n\n\n\n\nAlternatively, we can use a date calculator logic.\n\nLet's use a simpler approach: add 299 days to April 11, 2023.\n\nWe can use the fact that 2023 is not a leap year (2023 is not divisible by 4). So February has 28 days.\n\nWe can use a date addition.\n\nBut maybe we can use a known method: add 299 days to April 11.\n\nWe can add 365 days to get next year, but we only need 299.\n\nLet's do step by step:\n\nApril 11, 2023.\n\nAdd 1 day: April 12, 2023.\n\nAdd 298 days: we need to add 298 days.\n\nWe can break 298 days into years, months, days.\n\nBut maybe we can use a simpler approach: find the date by adding 299 days to April 11, 2023.\n\nWe can use a date calculator mental.\n\nLet's add 299 days to April 11, 2023.\n\nApril 11 + 299 days.\n\nWe can compute:\n\n- April has 30 days, so adding 30 days would get to May 0? Wait.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is too messy.\n\nLet'solve the date calculation.\nWe need to compute the current date:\nThe task: Given a small set of sentences about a particular date.\n\nThe task:\nPlease write a function that is to\n&amp;lt;think&amp;gt;\nWe need to write a function that returns the\n\nWe need to\n&amp;lt;/think&amp;gt;\n\nWe need to\n\n\n\n&amp;lt;think&amp;gt;\n&amp;lt;/think&amp;gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&amp;lt;/think&amp;gt;\nWe need to write a small set of the problem.\n\n\n&amp;lt;think&amp;gt;\nWe need to write a function.\n\nWe need to think\n\n&amp;lt;think&amp;gt;\n\n\n\n\n\nWe need to think\n\n\n&amp;lt;/think&amp;gt;\n&amp;lt;/think&amp;gt;\n\n\n&amp;lt;think&amp;gt;\nWe need to start a new task:\n\n\nWe need to start a new task.\n\n\n&amp;lt;think&amp;gt;\n\nWe need to\n\nWe need to start a new start\n\n\n\n\n&amp;lt;/think&amp;gt;\n\nWe need to start new start\n\n\n\n\n\n\n&amp;lt;think&amp;gt;\n\nWe need to start a new start\n\n\n\n\n\n\n&amp;lt;/think&amp;gt;\n\nWe need to start start to start start start [start repeats 4000 times] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I ran all models with &lt;strong&gt;{&amp;quot;temperature&amp;quot;: 0.6, &amp;quot;top_p&amp;quot;: 0.95, &amp;quot;top_k&amp;quot;: 20, &amp;quot;min_p&amp;quot;: 0 }&lt;/strong&gt; which is my standard sampler for reasoning models, perhaps there is a different configuration that works better for Jamba reasoning specifically.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/itf6y0k674uf1.png?width=1450&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=972e3d53f7eaa361101ab32a00c11bb257fedd62"&gt;https://preview.redd.it/itf6y0k674uf1.png?width=1450&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=972e3d53f7eaa361101ab32a00c11bb257fedd62&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In closing, &lt;strong&gt;I don't believe this model is comparable to Qwen3-4B on practical tasks&lt;/strong&gt;. It's far worse at basically all tasks, and has a universal truncation problem.&lt;/p&gt; &lt;p&gt;Thanks for reading and keep it local! &amp;lt;3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kryptkpr"&gt; /u/kryptkpr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2b1yo/reasonscape_evaluation_ai21_jamba_reasoning_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2b1yo/reasonscape_evaluation_ai21_jamba_reasoning_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2b1yo/reasonscape_evaluation_ai21_jamba_reasoning_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T16:45:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2vnf2</id>
    <title>Temperatures for MI50 during inference? Anyone with experience re-pasting processor?</title>
    <updated>2025-10-10T08:51:07+00:00</updated>
    <author>
      <name>/u/EdenistTech</name>
      <uri>https://old.reddit.com/user/EdenistTech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As many others in here, I am experimenting with the MI50 at the moment due to the fantastic value-for-money relationship of this card (at least w.r.t. $ / GB VRAM). I am getting 80c-85c degrees on the edge sensor running full tilt with a &amp;quot;custom cooling solution&amp;quot;. The junction sensor shows &amp;gt;100c (which is high but acceptable, I am told). Decreasing the power limit with rocm-smi does not seem to affect temps much. Idle temps are 30c-40c. What is your experience with temperatures? Have any of you successfully re-pasted the processor?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EdenistTech"&gt; /u/EdenistTech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2vnf2/temperatures_for_mi50_during_inference_anyone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2vnf2/temperatures_for_mi50_during_inference_anyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2vnf2/temperatures_for_mi50_during_inference_anyone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T08:51:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2t72w</id>
    <title>Olla v0.0.19 is out with SGLang &amp; lemonade support</title>
    <updated>2025-10-10T06:11:39+00:00</updated>
    <author>
      <name>/u/2shanigans</name>
      <uri>https://old.reddit.com/user/2shanigans</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2t72w/olla_v0019_is_out_with_sglang_lemonade_support/"&gt; &lt;img alt="Olla v0.0.19 is out with SGLang &amp;amp; lemonade support" src="https://external-preview.redd.it/EmvlZmNJDsCM03xUIgjfe04GISheKc3GwRTX95FXeeM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cd56b94ddf46de09ffeea2a2f9edca7346d5f857" title="Olla v0.0.19 is out with SGLang &amp;amp; lemonade support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've added native &lt;a href="https://github.com/sgl-project/sglang"&gt;sglang&lt;/a&gt; and &lt;a href="https://lemonade-server.ai/"&gt;lemonade&lt;/a&gt; support and released v0.0.19 of Olla, the fast unifying LLM Proxy - which already supports Ollama, LM Studio, LiteLLM natively (see &lt;a href="https://thushan.github.io/olla/integrations/overview/"&gt;the list&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;We‚Äôve been using Olla extensively with OpenWebUI and the OpenAI-compatible endpoint for vLLM and SGLang experimentation on Blackwell GPUs running under Proxmox, and there‚Äôs &lt;a href="https://thushan.github.io/olla/integrations/frontend/openwebui-openai/"&gt;now an example available for that setup&lt;/a&gt; too.&lt;/p&gt; &lt;p&gt;With &lt;a href="https://github.com/thushan/olla"&gt;Olla&lt;/a&gt;, you can expose a unified OpenAI-compatible API to OpenWebUI (or LibreChat, etc.), while your models run on separate backends like vLLM and SGLang. From OpenWebUI‚Äôs perspective, it‚Äôs just one API to read them all.&lt;/p&gt; &lt;p&gt;Best part is that we can swap models around (or tear down vllm, start a new node etc) and they just come and go (in the UI) without restarting (as long as we put them all in Olla's config).&lt;/p&gt; &lt;p&gt;Let us know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/2shanigans"&gt; /u/2shanigans &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/thushan/olla"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2t72w/olla_v0019_is_out_with_sglang_lemonade_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2t72w/olla_v0019_is_out_with_sglang_lemonade_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T06:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2ug6s</id>
    <title>We just launched Observability for LLMs that works without code changes and redeployment of apps</title>
    <updated>2025-10-10T07:30:27+00:00</updated>
    <author>
      <name>/u/patcher99</name>
      <uri>https://old.reddit.com/user/patcher99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You know that moment when your AI app is live and suddenly slows down or costs more than expected? You check the logs and still have no clue what happened. &lt;/p&gt; &lt;p&gt;That is exactly why we built OpenLIT Operator. It gives you observability for LLMs and AI agents without touching your code, rebuilding containers, or redeploying. &lt;/p&gt; &lt;p&gt;‚úÖ Traces every LLM, agent, and tool call automatically&lt;br /&gt; ‚úÖ Shows latency, cost, token usage, and errors&lt;br /&gt; ‚úÖ Works with OpenAI, Anthropic, AgentCore, Ollama, and others&lt;br /&gt; ‚úÖ Connects with OpenTelemetry, Grafana, Jaeger, and Prometheus&lt;br /&gt; ‚úÖ Runs anywhere like Docker, Helm, or Kubernetes &lt;/p&gt; &lt;p&gt;You can set it up once and start seeing everything in a few minutes. It also works with any OpenTelemetry instrumentations like Openinference or anything custom you have. &lt;/p&gt; &lt;p&gt;We just launched it on Product Hunt today üéâ&lt;br /&gt; üëâ &lt;a href="https://www.producthunt.com/products/openlit?launch=openlit-s-zero-code-llm-observability"&gt;https://www.producthunt.com/products/openlit?launch=openlit-s-zero-code-llm-observability&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Open source repo here:&lt;br /&gt; üß† &lt;a href="https://github.com/openlit/openlit"&gt;https://github.com/openlit/openlit&lt;/a&gt; &lt;/p&gt; &lt;p&gt;If you have ever said &amp;quot;I'll add observability later,&amp;quot; this might be the easiest way to start.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/patcher99"&gt; /u/patcher99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2ug6s/we_just_launched_observability_for_llms_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2ug6s/we_just_launched_observability_for_llms_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2ug6s/we_just_launched_observability_for_llms_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T07:30:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2y43t</id>
    <title>LLaMA that plays chess</title>
    <updated>2025-10-10T11:17:54+00:00</updated>
    <author>
      <name>/u/neurocod</name>
      <uri>https://old.reddit.com/user/neurocod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2y43t/llama_that_plays_chess/"&gt; &lt;img alt="LLaMA that plays chess" src="https://external-preview.redd.it/geYQQAxqjX90o0mQq_3STTNJ2tR_-toI4zN0X160D4k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4a0e7749765f565b43f6f79308b17aa4c64b216" title="LLaMA that plays chess" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a &lt;a href="https://github.com/neurocod/llm-chess-hybrid"&gt;hybrid&lt;/a&gt; of LLaMA and several other neural networks that can play chess quite well. It‚Äôs part of my ongoing series of &lt;a href="https://www.linkedin.com/pulse/hybrid-neural-networks-inside-llms-konstantine-kozachuk-xquze/"&gt;articles&lt;/a&gt; about hybrid neural networks. The hippocampus model is still missing and outsourced to traditional C++ code.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z916pjlqp9uf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d470e2d693d723ffd1d3f1493df16faf36be7a2"&gt;https://preview.redd.it/z916pjlqp9uf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d470e2d693d723ffd1d3f1493df16faf36be7a2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neurocod"&gt; /u/neurocod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2y43t/llama_that_plays_chess/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2y43t/llama_that_plays_chess/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2y43t/llama_that_plays_chess/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T11:17:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2vwh1</id>
    <title>Whats the best local model i can run with 16 GB VRAM and 96 GB RAM</title>
    <updated>2025-10-10T09:07:19+00:00</updated>
    <author>
      <name>/u/Tricky_Reflection_75</name>
      <uri>https://old.reddit.com/user/Tricky_Reflection_75</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;1 general model that has some intelligence with really good tool calling capabilties / (Would be good if it was uncensored to some capacity too, not for any specific purpose but just generally don't want it to turn down stuff cause of &amp;quot;Safety&amp;quot; or something.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tricky_Reflection_75"&gt; /u/Tricky_Reflection_75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2vwh1/whats_the_best_local_model_i_can_run_with_16_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2vwh1/whats_the_best_local_model_i_can_run_with_16_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2vwh1/whats_the_best_local_model_i_can_run_with_16_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T09:07:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1o27xsj</id>
    <title>Introducing Playable1-GGUF, by far the world's best open-source 7B model for vibe coding retro arcade games!</title>
    <updated>2025-10-09T14:48:44+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o27xsj/introducing_playable1gguf_by_far_the_worlds_best/"&gt; &lt;img alt="Introducing Playable1-GGUF, by far the world's best open-source 7B model for vibe coding retro arcade games!" src="https://external-preview.redd.it/amdjandqbjRtM3VmMYQO32V0xtyx5agXCcjKDs7UavRodqRFn_xjnlOv-a1T.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7afce15d0578922ac9c1cd32c16d01c0eb31c425" title="Introducing Playable1-GGUF, by far the world's best open-source 7B model for vibe coding retro arcade games!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've taken this idea too far, clearly, but the results are fun! Playable1-GGUF is a q4_k_m Qwen2.5-Coder-7B-Instruct fine-tuned on 52,809 lines of Python pygame scripts.&lt;/p&gt; &lt;p&gt;Over the past week I've dialed in the LORA parameters, added games, ironed the bugs out of the dataset, and open-sourced everything.&lt;/p&gt; &lt;p&gt;No q4 model, 8B or smaller, comes anywhere close to this level of performance. Most struggle to make a few basic games and can't do many creative twists on them.&lt;/p&gt; &lt;p&gt;Playable1-GGUF features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Oneshot code Galaga, Space Invaders, Breakout, Flappy Bird, Snake, and Pong.&lt;/li&gt; &lt;li&gt;Modify existing games, like &amp;quot;give the invaders rainbow colors&amp;quot;, &amp;quot;make the bullets explode&amp;quot;, etc.&lt;/li&gt; &lt;li&gt;Oneshot code games with a twist, like &amp;quot;pong but the paddles can move in 2d.&amp;quot;&lt;/li&gt; &lt;li&gt;Debug a variety of simple Python errors to fix broken games.&lt;/li&gt; &lt;li&gt;No RAG or templates needed in the prompts!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also built an app, Infinity Arcade, that provides the right prompts and a nice UI for demonstrating the features of the model.&lt;/p&gt; &lt;p&gt;Assets (all MIT license):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Quantized GGUF: &lt;a href="https://huggingface.co/playable/Playable1-GGUF"&gt;https://huggingface.co/playable/Playable1-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Full-precision SafeTensors: &lt;a href="https://huggingface.co/playable/Playable1"&gt;playable/Playable1 ¬∑ Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Dataset: &lt;a href="https://github.com/lemonade-sdk/playable-data/tree/main"&gt;https://github.com/lemonade-sdk/playable-data/tree/main&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Infinity Arcade app: &lt;a href="https://github.com/lemonade-sdk/infinity-arcade"&gt;https://github.com/lemonade-sdk/infinity-arcade&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Next steps (if there's interest):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full SFT on MI 300X GPUs (instead of LORA)&lt;/li&gt; &lt;li&gt;Prompting guide for the model&lt;/li&gt; &lt;li&gt;e2e tutorial on how to make this kind of thing&lt;/li&gt; &lt;li&gt;More games (a DDR-style rhythm game is probably next)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Posting here to get people's feedback. Take it for a spin and let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hscmojn4m3uf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o27xsj/introducing_playable1gguf_by_far_the_worlds_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o27xsj/introducing_playable1gguf_by_far_the_worlds_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T14:48:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1o27ex3</id>
    <title>Will open-source (or more accurately open-weight) models always lag behind closed-source models?</title>
    <updated>2025-10-09T14:28:23+00:00</updated>
    <author>
      <name>/u/Striking_Wedding_461</name>
      <uri>https://old.reddit.com/user/Striking_Wedding_461</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o27ex3/will_opensource_or_more_accurately_openweight/"&gt; &lt;img alt="Will open-source (or more accurately open-weight) models always lag behind closed-source models?" src="https://preview.redd.it/a1fnssvaj3uf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da172e76a7d257ced3c5cb728441b52635db6b3f" title="Will open-source (or more accurately open-weight) models always lag behind closed-source models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like open source LLM's are always one step behind closed-source companies. The question here is, is there a possibility for open-weight LLM's to overtake these companies?&lt;/p&gt; &lt;p&gt;Claude, Grok, ChatGPT and other's have billions of dollars in investments yet we saw the leaps DeepSeek was capable of. &lt;/p&gt; &lt;p&gt;Shaking Silicon Valley a bit to the point where banning it was debated. So I see no reason why they can't be eventually overtaken?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking_Wedding_461"&gt; /u/Striking_Wedding_461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a1fnssvaj3uf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o27ex3/will_opensource_or_more_accurately_openweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o27ex3/will_opensource_or_more_accurately_openweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T14:28:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2wvvy</id>
    <title>China blacklists major chip research firm TechInsights following report on Huawei</title>
    <updated>2025-10-10T10:08:58+00:00</updated>
    <author>
      <name>/u/vancity-boi-in-tdot</name>
      <uri>https://old.reddit.com/user/vancity-boi-in-tdot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2wvvy/china_blacklists_major_chip_research_firm/"&gt; &lt;img alt="China blacklists major chip research firm TechInsights following report on Huawei" src="https://external-preview.redd.it/cXviE8IZIrPbvKrVn_b-IwEX0IhaM8bGQVFM7IF9Z1o.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ba6db197a71a303a6de965206d6fa42d8ec4ee3" title="China blacklists major chip research firm TechInsights following report on Huawei" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vancity-boi-in-tdot"&gt; /u/vancity-boi-in-tdot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/10/10/china-blacklists-major-chip-ai-research-firm-techinsights-analyzed-report-huawei.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2wvvy/china_blacklists_major_chip_research_firm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2wvvy/china_blacklists_major_chip_research_firm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T10:08:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2x2xs</id>
    <title>Qwen team auto-closed all issues on Qwen2-VL repository</title>
    <updated>2025-10-10T10:20:21+00:00</updated>
    <author>
      <name>/u/CasualCapybara</name>
      <uri>https://old.reddit.com/user/CasualCapybara</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just noticed that the Qwen2-VL repository has been renamed to Qwen3-VL and that all issues on GitHub are being closed. It sits currently at 475 open issues/859 closed issues, and changing quickly: &lt;a href="https://github.com/QwenLM/Qwen3-VL/issues"&gt;https://github.com/QwenLM/Qwen3-VL/issues&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I think this is somewhat rude, because it ignores the effort of all the people that took time out of their day to report issues. They could just as easily have created a new repository.&lt;/p&gt; &lt;p&gt;Of course I hugely appreciate all the open models that the Qwen team gave us, but I still think that this could have been handled in a better way.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CasualCapybara"&gt; /u/CasualCapybara &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2x2xs/qwen_team_autoclosed_all_issues_on_qwen2vl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2x2xs/qwen_team_autoclosed_all_issues_on_qwen2vl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2x2xs/qwen_team_autoclosed_all_issues_on_qwen2vl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T10:20:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2k0cw</id>
    <title>Is there any way I can finetune the GrayWolf models faster? It currently takes 10,000 years to create a LoRA on my current GPU rig and I want to speed up the process.</title>
    <updated>2025-10-09T22:31:49+00:00</updated>
    <author>
      <name>/u/Arkhos-Winter</name>
      <uri>https://old.reddit.com/user/Arkhos-Winter</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2k0cw/is_there_any_way_i_can_finetune_the_graywolf/"&gt; &lt;img alt="Is there any way I can finetune the GrayWolf models faster? It currently takes 10,000 years to create a LoRA on my current GPU rig and I want to speed up the process." src="https://preview.redd.it/9aievo3cx5uf1.gif?width=320&amp;amp;crop=smart&amp;amp;s=0776eb0fcf74d851620f3d811dfc1d496ccc9441" title="Is there any way I can finetune the GrayWolf models faster? It currently takes 10,000 years to create a LoRA on my current GPU rig and I want to speed up the process." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arkhos-Winter"&gt; /u/Arkhos-Winter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9aievo3cx5uf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2k0cw/is_there_any_way_i_can_finetune_the_graywolf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2k0cw/is_there_any_way_i_can_finetune_the_graywolf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T22:31:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o23vqf</id>
    <title>microsoft/UserLM-8b - ‚ÄúUnlike typical LLMs that are trained to play the role of the 'assistant' in conversation, we trained UserLM-8b to simulate the 'user' role‚Äù</title>
    <updated>2025-10-09T11:54:17+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o23vqf/microsoftuserlm8b_unlike_typical_llms_that_are/"&gt; &lt;img alt="microsoft/UserLM-8b - ‚ÄúUnlike typical LLMs that are trained to play the role of the 'assistant' in conversation, we trained UserLM-8b to simulate the 'user' role‚Äù" src="https://external-preview.redd.it/AHBKf2lyQkjC30uEVnsWIAOzEiogI0jy6tjwogMGZ5A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e0a4856ffac6d098a7303d1eafccab9d20ceaa1" title="microsoft/UserLM-8b - ‚ÄúUnlike typical LLMs that are trained to play the role of the 'assistant' in conversation, we trained UserLM-8b to simulate the 'user' role‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/UserLM-8b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o23vqf/microsoftuserlm8b_unlike_typical_llms_that_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o23vqf/microsoftuserlm8b_unlike_typical_llms_that_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T11:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2lq5n</id>
    <title>Is there anything faster or smaller with equal quality to Qwen 30B A3B?</title>
    <updated>2025-10-09T23:49:17+00:00</updated>
    <author>
      <name>/u/WEREWOLF_BX13</name>
      <uri>https://old.reddit.com/user/WEREWOLF_BX13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Specs: RTX 3060 12GB - 4+8+16GB RAM - R5 4600G&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I've tried mistral small, instruct and nemo in 7b, 14b and 24b sizes but unfortunately 7b just can't handle much nothing except for those 200 tokens &lt;a href="http://c.ai"&gt;c.ai&lt;/a&gt; chatbots and they're thrice slower than Qwen.&lt;/p&gt; &lt;p&gt;Do you know anything smaller than &lt;strong&gt;Qwen A3B 30B&lt;/strong&gt; with at least same quality as the &lt;strong&gt;Q3_K_M quant (14,3GB)&lt;/strong&gt; and 28k context window? Not using for programming, but more complex reasoning tasks and super long story-writing/advanced character creation with amateur psychology knowledge. I saw that this model has different processing methods, that's why its faster.&lt;/p&gt; &lt;p&gt;&lt;em&gt;I'm planning on getting a 24GB VRAM gpu like RTX 3090, but it will be absolute pointless if there isn't anything noticeably better than Qwen or Video Generation models keep getting worse in optimization considering how slow it is even for the 4090.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WEREWOLF_BX13"&gt; /u/WEREWOLF_BX13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2lq5n/is_there_anything_faster_or_smaller_with_equal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2lq5n/is_there_anything_faster_or_smaller_with_equal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2lq5n/is_there_anything_faster_or_smaller_with_equal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T23:49:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2xzep</id>
    <title>üö® Local AI is the only sane path if you care about privacy</title>
    <updated>2025-10-10T11:11:05+00:00</updated>
    <author>
      <name>/u/Code-Forge-Temple</name>
      <uri>https://old.reddit.com/user/Code-Forge-Temple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta recently announced that AI chat interactions on Facebook and Instagram will be used for ad targeting.&lt;br /&gt; Everything you type can shape how you are profiled... a stark reminder that cloud AI often means zero privacy.&lt;/p&gt; &lt;p&gt;Local-first AI puts you in control. Models run entirely on your own device, keeping your data private and giving you full ownership over results.&lt;/p&gt; &lt;p&gt;Here are some of my projects exploring this approach:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://agentic-signal.com"&gt;&lt;strong&gt;Agentic Signal&lt;/strong&gt;&lt;/a&gt;: privacy-first workflows and browser AI agent framework. Open-source for personal use; commercial licensing available for businesses and SaaS.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/code-forge-temple/scribe-pal"&gt;ScribePal&lt;/a&gt;: local browser AI assistant that summarizes and interacts with content without sending data to external servers.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/code-forge-temple/local-llm-npc"&gt;Local LLM NPC&lt;/a&gt;: educational Godot game powered by Gemma 3n via Ollama, with offline-first NPCs teaching sustainable farming and botany.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Local AI isn‚Äôt just a technical preference - it‚Äôs essential for privacy, autonomy, and transparency in AI.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://www.cnbc.com/2025/10/01/meta-facebook-instagram-ads-ai-chat.html"&gt;https://www.cnbc.com/2025/10/01/meta-facebook-instagram-ads-ai-chat.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Code-Forge-Temple"&gt; /u/Code-Forge-Temple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2xzep/local_ai_is_the_only_sane_path_if_you_care_about/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2xzep/local_ai_is_the_only_sane_path_if_you_care_about/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2xzep/local_ai_is_the_only_sane_path_if_you_care_about/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T11:11:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2wkyw</id>
    <title>We can now run wan or any heavy models even on a 6GB NVIDIA laptop GPU | Thanks to upcoming GDS integration in comfy</title>
    <updated>2025-10-10T09:50:47+00:00</updated>
    <author>
      <name>/u/maifee</name>
      <uri>https://old.reddit.com/user/maifee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2wkyw/we_can_now_run_wan_or_any_heavy_models_even_on_a/"&gt; &lt;img alt="We can now run wan or any heavy models even on a 6GB NVIDIA laptop GPU | Thanks to upcoming GDS integration in comfy" src="https://b.thumbs.redditmedia.com/p7rn0Ix6tXlo4gn9p81TJIfGWsn4o8QzjNEOzhKU77w.jpg" title="We can now run wan or any heavy models even on a 6GB NVIDIA laptop GPU | Thanks to upcoming GDS integration in comfy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello&lt;/p&gt; &lt;p&gt;I am Maifee. I am integrating GDS (GPU Direct Storage) in ComfyUI. And it's working, if you want to test, just do the following:&lt;/p&gt; &lt;p&gt;&lt;code&gt; git clone https://github.com/maifeeulasad/ComfyUI.git cd ComfyUI git checkout offloader-maifee python3 main.py --enable-gds --gds-stats # gds enabled run &lt;/code&gt;&lt;/p&gt; &lt;p&gt;And you no longer need custome offloader, or just be happy with quantized version. Or you don't even have to wait. Just run with GDS enabled flag and we are good to go. Everything will be handled for you. I have already created issue and raised MR, review is going on, hope this gets merged real quick.&lt;/p&gt; &lt;p&gt;If you have some suggestions or feedback, please let me know.&lt;/p&gt; &lt;p&gt;And thanks to these helpful sub reddits, where I got so many advices, and trust me it was always more than enough.&lt;/p&gt; &lt;p&gt;Enjoy your weekend!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maifee"&gt; /u/maifee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o2wkyw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2wkyw/we_can_now_run_wan_or_any_heavy_models_even_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2wkyw/we_can_now_run_wan_or_any_heavy_models_even_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T09:50:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2q5n6</id>
    <title>I made a multimodal local RAG system with LM Studio</title>
    <updated>2025-10-10T03:22:45+00:00</updated>
    <author>
      <name>/u/donotfire</name>
      <uri>https://old.reddit.com/user/donotfire</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2q5n6/i_made_a_multimodal_local_rag_system_with_lm/"&gt; &lt;img alt="I made a multimodal local RAG system with LM Studio" src="https://external-preview.redd.it/cnBqa3NkeWhkN3VmMQb9PvMVdq9S5Iwim0AbH7Nkf50d3AGCnXrbNmpKxt0b.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f129228772f39b10328979337e92d94e8f2fd7ab" title="I made a multimodal local RAG system with LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I couldn‚Äôt find a RAG system that worked with Google Docs and could have more than 10,000 synced files, so I made one myself. This thing is a beast, it works with Gemma 3 4B decently well but I think the results would be way better with a larger model and a larger dataset. I‚Äôll share the full code later on but I‚Äôm tired rn&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/donotfire"&gt; /u/donotfire &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/09paso3id7uf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2q5n6/i_made_a_multimodal_local_rag_system_with_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2q5n6/i_made_a_multimodal_local_rag_system_with_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T03:22:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2rppj</id>
    <title>Qwen3 VL 4B to be released?</title>
    <updated>2025-10-10T04:44:56+00:00</updated>
    <author>
      <name>/u/Signal-Run7450</name>
      <uri>https://old.reddit.com/user/Signal-Run7450</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2rppj/qwen3_vl_4b_to_be_released/"&gt; &lt;img alt="Qwen3 VL 4B to be released?" src="https://preview.redd.it/68mmis87s7uf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ab80165de7b5dc3e27595de513c638e5086c61d" title="Qwen3 VL 4B to be released?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen released cookbooks and in one of them this model Qwen3 VL 4B is present but I can't find it anywhere on huggingface. Link of the cookbook- &lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/long_document_understanding.ipynb"&gt;https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/long_document_understanding.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This would be quite amazing for OCR use cases. Qwen2.5/2 VL 3b/7b was foundation for many good OCR models&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Signal-Run7450"&gt; /u/Signal-Run7450 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/68mmis87s7uf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2rppj/qwen3_vl_4b_to_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2rppj/qwen3_vl_4b_to_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T04:44:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect ‚Äî Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect ‚Äî Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM ‚Äì 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
