<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-19T18:27:08+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qhbjek</id>
    <title>Building a Robust Evaluation Framework for Agentic Systems</title>
    <updated>2026-01-19T18:01:21+00:00</updated>
    <author>
      <name>/u/slow-fast-person</name>
      <uri>https://old.reddit.com/user/slow-fast-person</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been building a general-purpose personal assistant designed to handle complex consumer automations, everything from summarizing emails to ordering groceries via browser automation.&lt;/p&gt; &lt;p&gt;We experimented heavily with multi-agent systems, trying different tools, architectures, and endless prompt tweaks to handle edge cases. But we hit a massive wall: our &lt;strong&gt;inability to quantify&lt;/strong&gt; the impact of these changes across a holistic set of use cases left us flying blind. We lacked the confidence to know if a &amp;quot;fix&amp;quot; was actually working or just silently breaking something else or how good it actually was.&lt;/p&gt; &lt;p&gt;This frustration led me to stop &amp;quot;vibe engineering&amp;quot; and build a strict &lt;strong&gt;Evaluation System&lt;/strong&gt;. I have condensed all my learnings and the framework I used in the article below.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here, is a summary of my learnings:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I &lt;strong&gt;curated multiple datasets of use cases&lt;/strong&gt; for my applications to test different components of the architecture and end-to-end testing. Started with a simple final result accuracy tests by comparing to ground truth. Just this helped me to:&lt;br /&gt; - do a hyperparameter search comparing different models, temperature, agent configurations&lt;br /&gt; - Ablation studies: removing parts of the architecture to find their specific impact on the score (impact of vector db, VLMs, different system prompts etc)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Evaluating the &amp;quot;How&amp;quot; (Execution Path)&lt;/strong&gt; Checking the final answer wasn't enough‚Äîan agent can sometimes guess correctly by luck. I added metrics to evaluate the agent's &lt;strong&gt;actual decision-making process&lt;/strong&gt; (its trajectory). This allowed us to test the structural integrity of the workflow and measure:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Delegation Quality:&lt;/strong&gt; Detecting if the Orchestrator was &amp;quot;micromanaging&amp;quot; (dictating internal steps) rather than providing high-level objectives to smart subagents&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data Flow Fidelity:&lt;/strong&gt; Verifying if critical entities (dates, IDs, links) were preserved between steps without hallucination.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Resilience:&lt;/strong&gt; Checking if the agent modified its strategy after a tool failure or just ignored the error.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This actually helped us realize that information wasn't passing correctly between multiple steps (e.g., stripping the complete url). We also found that &lt;strong&gt;failure handling&lt;/strong&gt; was brittle, for example, if a subagent failed, the Orchestrator would promise success to the user despite the underlying error, a behavior we only caught by evaluating the full trace.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt; Building this framework turned development from a game of Whack-a-Mole into a disciplined engineering process. It allowed me to confidently refactor the entire orchestration layer without breaking core functionality, while actually understanding what works and what doesn't.&lt;/p&gt; &lt;p&gt;I‚Äôve written a detailed breakdown of the metrics, the architecture, and the specific &amp;quot;war stories&amp;quot; of failures in details I encountered in the full article. Link in the comments.&lt;/p&gt; &lt;p&gt;I‚Äôd love to hear your feedback on this approach. For those of you running agentic systems in production: &lt;strong&gt;How are you validating &amp;quot;intermediate&amp;quot; logic steps? Are you using LLM-as-a-Judge, or sticking to deterministic assertions?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slow-fast-person"&gt; /u/slow-fast-person &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhbjek/building_a_robust_evaluation_framework_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhbjek/building_a_robust_evaluation_framework_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhbjek/building_a_robust_evaluation_framework_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T18:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhc1o0</id>
    <title>I built a lightweight, type-safe web scraper specifically for LLM Agents (returns clean Markdown)</title>
    <updated>2026-01-19T18:19:02+00:00</updated>
    <author>
      <name>/u/eatsleepliftcode</name>
      <uri>https://old.reddit.com/user/eatsleepliftcode</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been building AI agents lately and ran into a consistent problem: &lt;strong&gt;giving them web access is expensiive and slow.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most scrapers return raw HTML (wasting tokens on meaningful tags) or rely heavily on headless browsers (slow and resource-intensive). I wanted something that felt &amp;quot;native&amp;quot; to an LLM's context window‚Äîclean, dense information without the fluff.&lt;/p&gt; &lt;p&gt;So I built &lt;strong&gt;AgentCrawl&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It's a high-performance TypeScript library designed to be the &amp;quot;eyes&amp;quot; of your AI agents.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why is it different?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;üöÄ &lt;strong&gt;Hybrid Engine&lt;/strong&gt;: It tries a fast static fetch first. If it detects dynamic content or a React root that needs hydration, it automatically falls back to a headless browser (Playwright). You get speed by default and power when needed.&lt;/p&gt; &lt;p&gt;‚ö° &lt;strong&gt;Token Optimized&lt;/strong&gt;: It doesn't just dump text. It strips navigation, ads, footers, and scripts, converting the main content into clean Markdown. It saves 80-90% of tokens compared to raw HTML.&lt;/p&gt; &lt;p&gt;üîå &lt;strong&gt;sdk-ready&lt;/strong&gt;: It comes with one-line adapters for the &lt;strong&gt;Vercel AI SDK&lt;/strong&gt; and &lt;strong&gt;OpenAI SDK&lt;/strong&gt;, so you can add &amp;quot;browsing&amp;quot; tools to your agent in seconds.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Usage is super simple:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import { AgentCrawl } from 'agent-crawl'; // Returns title, clean markdown content, and links const page = await AgentCrawl.scrape(&amp;quot;https://example.com&amp;quot;); console.log(page.content); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Or directly as a tool for Vercel AI SDK:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import { generateText } from 'ai'; import { AgentCrawl } from 'agent-crawl'; const result = await generateText({ model: openai('gpt-4o'), tools: { browser: AgentCrawl.asVercelTool(), // Plug &amp;amp; play }, prompt: &amp;quot;Go to news.ycombinator.com and tell me the top story.&amp;quot; }); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It's fully open-source and MIT licensed. I'd love for you guys to try it out and roast my code or give feedback on what features you need for your agents.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt; üì¶ NPM: &lt;a href="https://www.npmjs.com/package/agent-crawl"&gt;https://www.npmjs.com/package/agent-crawl&lt;/a&gt; üíª GitHub: &lt;a href="https://github.com/silupanda/agent-crawl"&gt;https://github.com/silupanda/agent-crawl&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eatsleepliftcode"&gt; /u/eatsleepliftcode &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhc1o0/i_built_a_lightweight_typesafe_web_scraper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhc1o0/i_built_a_lightweight_typesafe_web_scraper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhc1o0/i_built_a_lightweight_typesafe_web_scraper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T18:19:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgksrm</id>
    <title>Roast my build</title>
    <updated>2026-01-18T21:28:13+00:00</updated>
    <author>
      <name>/u/RoboDogRush</name>
      <uri>https://old.reddit.com/user/RoboDogRush</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgksrm/roast_my_build/"&gt; &lt;img alt="Roast my build" src="https://b.thumbs.redditmedia.com/b9pW-_QRs8a6Yq5-9Nf1d_lQSgXCDYpLuPpj2b0jIXM.jpg" title="Roast my build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This started as an Optiplex 990 with a 2nd gen i5 as a home server. Someone gave me a 3060, I started running Ollama with Gemma 7B to help manage my Home Assistant, and it became addicting.&lt;/p&gt; &lt;p&gt;The upgrades outgrew the SFF case, PSU and GPU spilling out the side, and it slowly grew into this beast. Around the time I bought the open frame, my wife said it's gotta move out of sight, so I got banished to the unfinished basement, next to the sewage pump. Honestly, better for me, got to plug directly into the network and get off wifi.&lt;/p&gt; &lt;p&gt;6 months of bargain hunting, eBay alerts at 2am, Facebook Marketplace meetups in parking lots, explaining what VRAM is for the 47th time. The result:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;6x RTX 3090 (24GB each)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;1x RTX 5090 (32GB), $1,700 open box Microcenter&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;ROMED8-2T + EPYC 7282&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;2x ASRock 1600W PSUs (both open box)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;32GB A-Tech DDR4 ECC RDIMM&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;$10 Phanteks 300mm PCIe 4.0 riser cables (too long for the lower rack, but costs more to replace with shorter ones)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;176GB total VRAM, ~$6,500 all-in&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;First motherboard crapped out, but got a warranty replacement right before they went out of stock.&lt;/p&gt; &lt;p&gt;Currently running Unsloth's GPT-OSS 120B MXFP4 GGUF. Also been doing Ralph Wiggum loops with Devstral-2 Q8_0 via Mistral Vibe, which yes, I know is unlimited free and full precision in the cloud. But the cloud can't hear my sewage pump.&lt;/p&gt; &lt;p&gt;I think I'm finally done adding on. I desperately needed this. Now I'm not sure what to do with it.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Edit: Fixed the GPT-OSS precision claim. It's natively MXFP4, not F16. The model was trained that way. Thanks to the commenters who caught it.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RoboDogRush"&gt; /u/RoboDogRush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qgksrm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgksrm/roast_my_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgksrm/roast_my_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T21:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh5zy5</id>
    <title>How good is the approach of using local LLMs for ingesting and extracting data for the creation of knowledge graphs?</title>
    <updated>2026-01-19T14:44:26+00:00</updated>
    <author>
      <name>/u/boombox_8</name>
      <uri>https://old.reddit.com/user/boombox_8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! I am new to the world of knowledge graphs and am very interested in exploring it (though I have a bit of prior experience working with LLMs)! What particularly drives me to knowledge graphs is the possibility of getting them to work with RAGs (which ALSO interest me greatly)&lt;/p&gt; &lt;p&gt;I am currently looking at using property graphs (neo4j to be specific) as the 'knowledge base' for RAG implementations since I've read that they're more powerful than the alternative of RDFs&lt;/p&gt; &lt;p&gt;What confuses me is about how one should go about generating the knowledge graph in the first place. neo4j's own blog and various others propose using LLMs to extract the data for you, and construct a JSON/csv-esque format which is then ingested to create the knowledge graph&lt;/p&gt; &lt;p&gt;Obviously, complexity-wise, the concept of going over data, extracting out entities and relations into such a format isn't something VERY demanding for the capabilities of LLMs, so I figured I'd roll with a local LLM to both reduce costs as well as compute (Since using something as large as GPT 5 feels like overkill). Nvidia's paper on SLMs (Small Language Models) interested me on the prospects of using local smaller-scale LLMs in particular&lt;/p&gt; &lt;p&gt;Except the issue is that it feels like I am poisoning the well here so to speak? If I have tons of text-based documents as my corpora, won't using LLMs to do the job of data extraction and graph generation have issues?&lt;/p&gt; &lt;p&gt;Off the top of my head, I can think of the following issues:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The LLM could generate duplicates of entities across documents/chunks (For example, the word &amp;quot;White House&amp;quot; is present in a bunch of various documents in various levels of described detail? The LLM could very well extract out multiple such 'White House' entities. Would happen regardless if I used a powerful model like GPT 5 or a small local LLM&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I did have an idea of pre-defining all entity types and relations and forcing the LLM to stick with that, as well do an NLP-based deduplication technique, though I am not sure if it'll work well&lt;/p&gt; &lt;p&gt;2) The LLM could just up and hallucinate up data. Bad for obvious reasons, since I don't want a garbage in = garbage out problem for the resultant rag&lt;/p&gt; &lt;p&gt;3) It could just generate wonky results with incorrect 'syntax'. Bad for obvious reasons&lt;/p&gt; &lt;p&gt;4) Manually extracting data and writing the appropriate CYPHER queries? Yeah, won't work out feasibly&lt;/p&gt; &lt;p&gt;5) Using an NLP-based entity and relation extractor? Faster and cheaper compute-wise, but the duplication issue still remains. It does solve issue 3)&lt;/p&gt; &lt;p&gt;6) Now one COULD fine-tune an LLM with few-shot learning to get it to better extract data for a knowledge graph, but it turns into another wild goose hunt of ensuring the fine-tuning process works ATOP ensuring the fine-tuned model works well in practice&lt;/p&gt; &lt;p&gt;With all these issues comes the extra issue of validating the output graph. Feels like I'm biting off more than I can chew, since all of this is VERY hard to pack into a pipeline unless I make my own bespoke one for the domain I am focusing on. Is there a better way of working with local LLMs for this? Or is there a better approach straight up?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boombox_8"&gt; /u/boombox_8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5zy5/how_good_is_the_approach_of_using_local_llms_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5zy5/how_good_is_the_approach_of_using_local_llms_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5zy5/how_good_is_the_approach_of_using_local_llms_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T14:44:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh0abp</id>
    <title>We built a small GPU platform and are looking for early users‚Äô feedback</title>
    <updated>2026-01-19T10:05:21+00:00</updated>
    <author>
      <name>/u/Nora_ww</name>
      <uri>https://old.reddit.com/user/Nora_ww</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0abp/we_built_a_small_gpu_platform_and_are_looking_for/"&gt; &lt;img alt="We built a small GPU platform and are looking for early users‚Äô feedback" src="https://b.thumbs.redditmedia.com/GXYYQ8UdUlANcA95D8KEstCVG5YizI9lO6Hs9Ti45xo.jpg" title="We built a small GPU platform and are looking for early users‚Äô feedback" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;We‚Äôre a small team building a GPU platform mainly for our own model training and inference experiments. While testing it internally, we realized we have spare GPU capacity sitting idle.&lt;/p&gt; &lt;p&gt;Instead of letting it go unused, we‚Äôd love to open it up to the community and get some real-world feedback. We‚Äôre offering &lt;strong&gt;free compute credits&lt;/strong&gt; in exchange for honest usage feedback (what works, what breaks, what‚Äôs annoying).&lt;/p&gt; &lt;p&gt;Currently available GPUs include &lt;strong&gt;RTX 5090 and Pro 6000&lt;/strong&gt;, suitable for LLM inference, fine-tuning, or other ML workloads.&lt;/p&gt; &lt;p&gt;If you‚Äôre interested in trying it or have specific workloads in mind, feel free to comment or DM me. I‚Äôm happy to answer technical questions as well.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m9j4c7ud5aeg1.png?width=1020&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6caed0d0b7af9cf0edea8b9471afe3e01d94d625"&gt;https://preview.redd.it/m9j4c7ud5aeg1.png?width=1020&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6caed0d0b7af9cf0edea8b9471afe3e01d94d625&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nora_ww"&gt; /u/Nora_ww &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0abp/we_built_a_small_gpu_platform_and_are_looking_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0abp/we_built_a_small_gpu_platform_and_are_looking_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0abp/we_built_a_small_gpu_platform_and_are_looking_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T10:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh7048</id>
    <title>Beginner ComfyUI advice</title>
    <updated>2026-01-19T15:22:07+00:00</updated>
    <author>
      <name>/u/Excellent_Koala769</name>
      <uri>https://old.reddit.com/user/Excellent_Koala769</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. I am a beginner to ComfyUI and I would like some advice on how I can start learning to use this.&lt;/p&gt; &lt;p&gt;Ideally, I would like to create an automated workflow and start generating shorts and post them to social media. &lt;/p&gt; &lt;p&gt;Is this the right place to start?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Koala769"&gt; /u/Excellent_Koala769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh7048/beginner_comfyui_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh7048/beginner_comfyui_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh7048/beginner_comfyui_advice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T15:22:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh8xae</id>
    <title>LM Studio and Filesystem MCP seems buggy. Sometimes it works, sometimes it doesn't.</title>
    <updated>2026-01-19T16:30:49+00:00</updated>
    <author>
      <name>/u/Smashy404</name>
      <uri>https://old.reddit.com/user/Smashy404</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. I'm pretty much a noob when it comes to this LLM stuff, however I have installed LM Studio, a few different models and the mcp/filesystem.&lt;/p&gt; &lt;p&gt;I have entered a folder into the json file which I want the LLM to have access to, the folder is located on my Desktop (Windows 11).&lt;/p&gt; &lt;p&gt;Some times the LLM model can access, read and write to the folder, sometimes it cant. I try reloading the model, I try restarting the MCP plugin, but again, sometimes the model can see the folder, sometimes it can't.&lt;/p&gt; &lt;p&gt;Is anyone else having this problem?&lt;/p&gt; &lt;p&gt;Is there a particular order in which you should start up each of these components?&lt;/p&gt; &lt;p&gt;Thanks for any advice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Smashy404"&gt; /u/Smashy404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh8xae/lm_studio_and_filesystem_mcp_seems_buggy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh8xae/lm_studio_and_filesystem_mcp_seems_buggy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh8xae/lm_studio_and_filesystem_mcp_seems_buggy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T16:30:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgj2n9</id>
    <title>Are most major agents really just markdown todo list processors?</title>
    <updated>2026-01-18T20:15:25+00:00</updated>
    <author>
      <name>/u/TheDigitalRhino</name>
      <uri>https://old.reddit.com/user/TheDigitalRhino</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been poking around different code bases and scrutixzing logs from the majors LLM providers, and it seems like every agent just decomposes task to a todo list and process them one by one.&lt;/p&gt; &lt;p&gt;Has anyone found a different approach?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheDigitalRhino"&gt; /u/TheDigitalRhino &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgj2n9/are_most_major_agents_really_just_markdown_todo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgj2n9/are_most_major_agents_really_just_markdown_todo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgj2n9/are_most_major_agents_really_just_markdown_todo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T20:15:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh1b8e</id>
    <title>Best open-source voice cloning model with emotional control? (Worked with VibeVoice 7B &amp; 1.5B)</title>
    <updated>2026-01-19T11:04:43+00:00</updated>
    <author>
      <name>/u/Junior-Media-8668</name>
      <uri>https://old.reddit.com/user/Junior-Media-8668</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been working with open-source voice cloning models and have some experience&lt;/p&gt; &lt;p&gt;with **VibeVoice 7B and 1.5B**, but I‚Äôm still looking for something that delivers&lt;/p&gt; &lt;p&gt;**better emotional expression and natural prosody**.&lt;/p&gt; &lt;p&gt;My main goals:&lt;/p&gt; &lt;p&gt;- High-quality voice cloning (few-shot or zero-shot)&lt;/p&gt; &lt;p&gt;- Strong emotional control (e.g., happy, sad, calm, expressive storytelling)&lt;/p&gt; &lt;p&gt;- Natural pacing and intonation (not flat or robotic)&lt;/p&gt; &lt;p&gt;- Good for long-form narration / audiobooks&lt;/p&gt; &lt;p&gt;- Open-source models preferred&lt;/p&gt; &lt;p&gt;I‚Äôve seen mentions of models like XTTS v2, StyleTTS 2, OpenVoice, Bark, etc.,&lt;/p&gt; &lt;p&gt;but I‚Äôd love to hear from people who‚Äôve used them in practice.&lt;/p&gt; &lt;p&gt;**What open-source model would you recommend now (2025) for my use case**, and&lt;/p&gt; &lt;p&gt;why? Any comparisons, demos, or benchmarks would be awesome too.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Junior-Media-8668"&gt; /u/Junior-Media-8668 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh1b8e/best_opensource_voice_cloning_model_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh1b8e/best_opensource_voice_cloning_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh1b8e/best_opensource_voice_cloning_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T11:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh5yvm</id>
    <title>LlamaBarn 0.23 ‚Äî tiny macOS app for running local LLMs (open source)</title>
    <updated>2026-01-19T14:43:16+00:00</updated>
    <author>
      <name>/u/erusev_</name>
      <uri>https://old.reddit.com/user/erusev_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5yvm/llamabarn_023_tiny_macos_app_for_running_local/"&gt; &lt;img alt="LlamaBarn 0.23 ‚Äî tiny macOS app for running local LLMs (open source)" src="https://preview.redd.it/2gia31vxibeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2532ca7ae291b1405d8ccfc782756cc3ea05dbc" title="LlamaBarn 0.23 ‚Äî tiny macOS app for running local LLMs (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;code&gt;r/LocalLLaMA&lt;/code&gt;! We posted about LlamaBarn back when it was in version 0.8. Since then, we've shipped 15 releases and wanted to share what's new.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ggml-org/LlamaBarn"&gt;https://github.com/ggml-org/LlamaBarn&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The big change: Router Mode&lt;/p&gt; &lt;p&gt;LlamaBarn now uses llama-server's Router Mode. The server runs continuously in the background and loads models automatically when they're requested. You no longer have to manually select a model before using it ‚Äî just point your app at http://localhost:2276/v1 and request any installed model by name.&lt;/p&gt; &lt;p&gt;Models also unload automatically when idle (configurable: off, 5m, 15m, 1h), so you're not wasting memory when you're not using them.&lt;/p&gt; &lt;p&gt;You can see the rest of the changes in the &lt;a href="https://github.com/ggml-org/LlamaBarn/releases"&gt;GitHub releases&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Install: &lt;code&gt;brew install --cask llamabarn&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/erusev_"&gt; /u/erusev_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2gia31vxibeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5yvm/llamabarn_023_tiny_macos_app_for_running_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5yvm/llamabarn_023_tiny_macos_app_for_running_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T14:43:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qglyqz</id>
    <title>how do you pronounce ‚Äúgguf‚Äù?</title>
    <updated>2026-01-18T22:14:35+00:00</updated>
    <author>
      <name>/u/Hamfistbumhole</name>
      <uri>https://old.reddit.com/user/Hamfistbumhole</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is it ‚Äújee - guff‚Äù? ‚Äúgiguff‚Äù? or the full ‚Äújee jee you eff‚Äù? others???&lt;/p&gt; &lt;p&gt;discuss.&lt;/p&gt; &lt;p&gt;and sorry for not using proper international phonetic alphabet symbol things&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hamfistbumhole"&gt; /u/Hamfistbumhole &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qglyqz/how_do_you_pronounce_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qglyqz/how_do_you_pronounce_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qglyqz/how_do_you_pronounce_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T22:14:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh9srb</id>
    <title>I built a Windows all-in-one local AI studio opensource, looking for contributors</title>
    <updated>2026-01-19T17:01:38+00:00</updated>
    <author>
      <name>/u/Motor-Resort-5314</name>
      <uri>https://old.reddit.com/user/Motor-Resort-5314</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh9srb/i_built_a_windows_allinone_local_ai_studio/"&gt; &lt;img alt="I built a Windows all-in-one local AI studio opensource, looking for contributors" src="https://b.thumbs.redditmedia.com/oEucn08yeClrk3dFP63fE03zNpUinSUaAEIZWywg7zI.jpg" title="I built a Windows all-in-one local AI studio opensource, looking for contributors" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been building a project called &lt;strong&gt;V6rge&lt;/strong&gt;. It‚Äôs a Windows-based local AI studio meant to remove the constant pain of Python, CUDA, and dependency breakage when running models locally.&lt;/p&gt; &lt;p&gt;V6rge uses its own isolated runtime, so it doesn‚Äôt touch your system Python. It‚Äôs built for both developers and non-coders who just want local AI tools that work without setup.&lt;/p&gt; &lt;p&gt;It works as a modular studio. Each feature has its own category, and users simply download the model that fits their hardware. No manual installs, no environment tuning.&lt;/p&gt; &lt;p&gt;Current features include:&lt;/p&gt; &lt;p&gt;Local LLMs (Qwen 7B, 32B, 72B) with hardware guidance&lt;br /&gt; Vision models for image understanding&lt;br /&gt; Image generation (FLUX, Qwen-Image)&lt;br /&gt; Music generation (MusicGen)&lt;br /&gt; Text-to-speech (Chatterbox)&lt;br /&gt; A real local agent that can execute tasks on your PC&lt;br /&gt; Video generation, 3D generation, image upscaling, background removal, and vocal separation&lt;/p&gt; &lt;p&gt;All models are managed through a built-in model manager that shows RAM and VRAM requirements.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/80tjarmt5ceg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a1a34e3512541d01f34261d16f53bee1408dd04"&gt;https://preview.redd.it/80tjarmt5ceg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a1a34e3512541d01f34261d16f53bee1408dd04&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k5b8sa6x5ceg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=53788a739da00cd525e2f7e1245233b8b342f358"&gt;https://preview.redd.it/k5b8sa6x5ceg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=53788a739da00cd525e2f7e1245233b8b342f358&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hfzt1sy26ceg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c8014ab04616d23fbbefa9bc6437c485d9c53bdb"&gt;https://preview.redd.it/hfzt1sy26ceg1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c8014ab04616d23fbbefa9bc6437c485d9c53bdb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/shcg9usj6ceg1.png?width=1364&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5f5244ee4a72b0769f81de25d0c80763d2680f7"&gt;https://preview.redd.it/shcg9usj6ceg1.png?width=1364&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f5f5244ee4a72b0769f81de25d0c80763d2680f7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hfotsbxa7ceg1.png?width=1352&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f72b9dc0e04a00b9a4b1952b02a62576b94226c"&gt;https://preview.redd.it/hfotsbxa7ceg1.png?width=1352&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6f72b9dc0e04a00b9a4b1952b02a62576b94226c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/urve0fee7ceg1.png?width=1343&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac007209f6f9589ecd694e8d78ecaddb25bb41d3"&gt;https://preview.redd.it/urve0fee7ceg1.png?width=1343&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac007209f6f9589ecd694e8d78ecaddb25bb41d3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôve open sourced it because I don‚Äôt want this to be just my project, I want it to become the best possible local AI studio. I don‚Äôt have a GPU machine, so I need help with testing across hardware, optimization, bug fixing, and adding more models and features. I‚Äôm honestly struggling to push this as far as it should go on my own, and community contributions would make a huge difference.&lt;br /&gt; Repo - &lt;a href="https://github.com/Dedsec-b/v6rge-releases-"&gt;https://github.com/Dedsec-b/v6rge-releases-&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Motor-Resort-5314"&gt; /u/Motor-Resort-5314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh9srb/i_built_a_windows_allinone_local_ai_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh9srb/i_built_a_windows_allinone_local_ai_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh9srb/i_built_a_windows_allinone_local_ai_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T17:01:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhay8x</id>
    <title>How to run GLM_4.7-flash alreadyu?</title>
    <updated>2026-01-19T17:41:10+00:00</updated>
    <author>
      <name>/u/Swimming_Power_2960</name>
      <uri>https://old.reddit.com/user/Swimming_Power_2960</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks!&lt;/p&gt; &lt;p&gt;Is it already possible to run the new GLM 4.7 flash with llama.cpp or something else?&lt;/p&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Swimming_Power_2960"&gt; /u/Swimming_Power_2960 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhay8x/how_to_run_glm_47flash_alreadyu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhay8x/how_to_run_glm_47flash_alreadyu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhay8x/how_to_run_glm_47flash_alreadyu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T17:41:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh3oj0</id>
    <title>Intel LLM-Scaler-Omni Update Brings ComfyUI &amp; SGLang Improvements On Arc Graphics</title>
    <updated>2026-01-19T13:07:55+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh3oj0/intel_llmscaleromni_update_brings_comfyui_sglang/"&gt; &lt;img alt="Intel LLM-Scaler-Omni Update Brings ComfyUI &amp;amp; SGLang Improvements On Arc Graphics" src="https://external-preview.redd.it/Jst-QPnk04q7baqsRFa3aXcG3HFaoXucjPJMdxa8Uf4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6083575735ba4b966f485ffd86a696d2b6276328" title="Intel LLM-Scaler-Omni Update Brings ComfyUI &amp;amp; SGLang Improvements On Arc Graphics" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/intel/llm-scaler/releases/tag/omni-0.1.0-b5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh3oj0/intel_llmscaleromni_update_brings_comfyui_sglang/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh3oj0/intel_llmscaleromni_update_brings_comfyui_sglang/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T13:07:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgdb7f</id>
    <title>4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build</title>
    <updated>2026-01-18T16:39:42+00:00</updated>
    <author>
      <name>/u/NunzeCs</name>
      <uri>https://old.reddit.com/user/NunzeCs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/"&gt; &lt;img alt="4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build" src="https://b.thumbs.redditmedia.com/bQ4SRK8dHDz2IGShLwX64vLIVj0fWUigDqG_dO43P-U.jpg" title="4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Disclaimer: I am from Germany and my English is not perfect, so I used an LLM to help me structure and write this post.&lt;/p&gt; &lt;p&gt;Context &amp;amp; Motivation: I built this system for my small company. The main reason for all new hardware is that I received a 50% subsidy/refund from my local municipality for digitalization investments. To qualify for this funding, I had to buy new hardware and build a proper &amp;quot;server-grade&amp;quot; system.&lt;/p&gt; &lt;p&gt;My goal was to run large models (120B+) locally for data privacy. With the subsidy in mind, I had a budget of around 10,000‚Ç¨ (pre-refund). I initially considered NVIDIA, but I wanted to maximize VRAM. I decided to go with 4x AMD RDNA4 cards (ASRock R9700) to get 128GB VRAM total and used the rest of the budget for a solid Threadripper platform.&lt;/p&gt; &lt;p&gt;Hardware Specs:&lt;/p&gt; &lt;p&gt;Total Cost: ~9,800‚Ç¨ (I get ~50% back, so effectively ~4,900‚Ç¨ for me).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU: AMD Ryzen Threadripper PRO 9955WX (16 Cores) &lt;/li&gt; &lt;li&gt;Mainboard: ASRock WRX90 WS EVO &lt;/li&gt; &lt;li&gt;RAM: 128GB DDR5 5600MHz &lt;/li&gt; &lt;li&gt;GPU: 4x ASRock Radeon AI PRO R9700 32GB (Total 128GB VRAM) &lt;ul&gt; &lt;li&gt;Configuration: All cards running at full PCIe 5.0 x16 bandwidth. &lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Storage: 2x 2TB PCIe 4.0 SSD &lt;/li&gt; &lt;li&gt;PSU: Seasonic 2200W &lt;/li&gt; &lt;li&gt;Cooling: Alphacool Eisbaer Pro Aurora 360 CPU AIO&lt;/li&gt; &lt;li&gt;Case: PHANTEKS Enthoo Pro 2 Server&lt;/li&gt; &lt;li&gt;Fans: 11x Arctic P12 Pro&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Benchmark Results&lt;/p&gt; &lt;p&gt;I tested various models ranging from 8B to 230B parameters.&lt;/p&gt; &lt;p&gt;Llama.cpp (Focus: Single User Latency) Settings: Flash Attention ON, Batch 2048&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Modell&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Prompt t/s&lt;/th&gt; &lt;th align="left"&gt;Gen t/s&lt;/th&gt; &lt;th align="left"&gt;Gr√∂√üe&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM-4.7-REAP-218B-A32B-Q3_K_M&lt;/td&gt; &lt;td align="left"&gt;999&lt;/td&gt; &lt;td align="left"&gt;504.15&lt;/td&gt; &lt;td align="left"&gt;17.48&lt;/td&gt; &lt;td align="left"&gt;97.6GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM-4.7-REAP-218B-A32B-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;65&lt;/td&gt; &lt;td align="left"&gt;428.80&lt;/td&gt; &lt;td align="left"&gt;9.48&lt;/td&gt; &lt;td align="left"&gt;123.0GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss-120b-GGUF&lt;/td&gt; &lt;td align="left"&gt;999&lt;/td&gt; &lt;td align="left"&gt;2977.83&lt;/td&gt; &lt;td align="left"&gt;97.47&lt;/td&gt; &lt;td align="left"&gt;58.4GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Meta-Llama-3.1-70B-Instruct-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;999&lt;/td&gt; &lt;td align="left"&gt;399.03&lt;/td&gt; &lt;td align="left"&gt;12.66&lt;/td&gt; &lt;td align="left"&gt;39.6GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Meta-Llama-3.1-8B-Instruct-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;999&lt;/td&gt; &lt;td align="left"&gt;3169.16&lt;/td&gt; &lt;td align="left"&gt;81.01&lt;/td&gt; &lt;td align="left"&gt;4.6GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MiniMax-M2.1-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;55&lt;/td&gt; &lt;td align="left"&gt;668.99&lt;/td&gt; &lt;td align="left"&gt;34.85&lt;/td&gt; &lt;td align="left"&gt;128.83 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5-32B-Instruct-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;999&lt;/td&gt; &lt;td align="left"&gt;848.68&lt;/td&gt; &lt;td align="left"&gt;25.14&lt;/td&gt; &lt;td align="left"&gt;18.5GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-235B-A22B-Instruct-2507-Q3_K_M&lt;/td&gt; &lt;td align="left"&gt;999&lt;/td&gt; &lt;td align="left"&gt;686.45&lt;/td&gt; &lt;td align="left"&gt;24.45&lt;/td&gt; &lt;td align="left"&gt;104.7GB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Side note: I found that with PCIe 5.0, standard Pipeline Parallelism (Layer Split) is significantly faster (~97 t/s) than Tensor Parallelism/Row Split (~67 t/s) for a single user on this setup.&lt;/p&gt; &lt;p&gt;vLLM (Focus: Throughput) Model: GPT-OSS-120B (bfloat16), TP=4, test for 20 requests&lt;/p&gt; &lt;p&gt;Total Throughput: ~314 tokens/s (Generation) Prompt Processing: ~5339 tokens/s Single user throughput 50 tokens/s&lt;/p&gt; &lt;p&gt;I used rocm 7.1.1 for llama.cpp also testet Vulkan but it was worse&lt;/p&gt; &lt;p&gt;If I could do it again, I would have used the budget to buy a single NVIDIA RTX Pro 6000 Blackwell (96GB). Maybe I will, if local AI is going well for my use case, I swap the R9700 with Pro 6000 in the future.&lt;/p&gt; &lt;p&gt;**Edit nicer view for the results&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NunzeCs"&gt; /u/NunzeCs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qgdb7f"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-18T16:39:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgrw3d</id>
    <title>Just put together my new setup(3x v620 for 96gb vram)</title>
    <updated>2026-01-19T02:31:42+00:00</updated>
    <author>
      <name>/u/PraxisOG</name>
      <uri>https://old.reddit.com/user/PraxisOG</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgrw3d/just_put_together_my_new_setup3x_v620_for_96gb/"&gt; &lt;img alt="Just put together my new setup(3x v620 for 96gb vram)" src="https://b.thumbs.redditmedia.com/mJVsjBo7IHRjbuffjsuNrdAMt7krK8sWOZUchd5L7tE.jpg" title="Just put together my new setup(3x v620 for 96gb vram)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PraxisOG"&gt; /u/PraxisOG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qgrw3d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgrw3d/just_put_together_my_new_setup3x_v620_for_96gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgrw3d/just_put_together_my_new_setup3x_v620_for_96gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T02:31:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh10q9</id>
    <title>Demo: On-device browser agent (Qwen) running locally in Chrome</title>
    <updated>2026-01-19T10:48:29+00:00</updated>
    <author>
      <name>/u/thecoder12322</name>
      <uri>https://old.reddit.com/user/thecoder12322</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh10q9/demo_ondevice_browser_agent_qwen_running_locally/"&gt; &lt;img alt="Demo: On-device browser agent (Qwen) running locally in Chrome" src="https://external-preview.redd.it/MzNzcDNuMGdjYWVnMcLtcBYDX5SdJ9uQfQaEUwyxr5ovu1B5qUxuDFDhwgNH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67f9a368e6dac7735d2b2e3f72bc407aa88b54c0" title="Demo: On-device browser agent (Qwen) running locally in Chrome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! wanted to share a cool demo of LOCAL Browser agent (powered by Web GPU Liquid LFM &amp;amp; Alibaba Qwen models) opening the All in Podcast on Youtube running as a chrome extension. &lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://github.com/RunanywhereAI/on-device-browser-agent"&gt;https://github.com/RunanywhereAI/on-device-browser-agent&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecoder12322"&gt; /u/thecoder12322 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ljp6zwzfcaeg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh10q9/demo_ondevice_browser_agent_qwen_running_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh10q9/demo_ondevice_browser_agent_qwen_running_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T10:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgwup8</id>
    <title>Is Local Coding even worth setting up</title>
    <updated>2026-01-19T06:38:37+00:00</updated>
    <author>
      <name>/u/Interesting-Fish6494</name>
      <uri>https://old.reddit.com/user/Interesting-Fish6494</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi I am new to Local LLM but have been having a lot of issues setting up a local LLM coding environment so wanted some suggestions from people.I have a 5070 ti (16gb vram).&lt;/p&gt; &lt;p&gt;I have tried to use Kilo code with qwen 2.5 coder 7B running through ollama but the context size feels so low that it finishes the context within a single file of my project.&lt;/p&gt; &lt;p&gt;How are other people with a 16gb GPU dealing with local llm?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Fish6494"&gt; /u/Interesting-Fish6494 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgwup8/is_local_coding_even_worth_setting_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgwup8/is_local_coding_even_worth_setting_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgwup8/is_local_coding_even_worth_setting_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T06:38:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qgx83t</id>
    <title>3x3090 + 3060 in a mid tower case</title>
    <updated>2026-01-19T06:59:39+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgx83t/3x3090_3060_in_a_mid_tower_case/"&gt; &lt;img alt="3x3090 + 3060 in a mid tower case" src="https://b.thumbs.redditmedia.com/isvewN3PNijf6_OZxoF82ROhZEAD0nfG7ddsr3ghkYU.jpg" title="3x3090 + 3060 in a mid tower case" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Decided to go all out and max out this desktop. I was lucky to find 3090 cards for around 600 usd, over a period of 3 months and decided to go for it. &lt;/p&gt; &lt;p&gt;The RAM was a bit more expensive, but I had 64 bought before the price spiked.&lt;/p&gt; &lt;p&gt;I didn‚Äôt want to change the case, because I through it‚Äôs a high quality case and it would be a shame to toss it. So made the most out of it!&lt;/p&gt; &lt;p&gt;Specs: * Fractal Define 7 Mid Tower * 3x3090 + 1x3060 (86gb total, but 72gb VRAM main) * 128GB DDR4 (Corsair 4x32) * Corsair HX1500i 1500w (has 7 PCIe power cables) * Vertical mounts are all cheap from AliExpress * ASUS Maximus XII Hero ‚Äî has only 3x PCIe16x, had to deactivate the 2nd NVMe to use the 3rd PCIe16x in 4x, the 4th GPU (the 3060) is on a riser from a PCIe1x. * For drives, only one NVMe of 1TB works, I also bought 2x2TB SSDs that I tried in RAID but the performance was terrible (and they are limited to 500mb from the SATA interface, which I didn‚Äôt know‚Ä¶) so I keep them as 2 drives.&lt;/p&gt; &lt;p&gt;Temperatures are holding surprisingly well. The gap between the cards is about the size of an empty PCIe slot, maybe a bit more. &lt;/p&gt; &lt;p&gt;Temperature was a big improvement compared to having just 2x3090 stacked without any space between them ‚Äî the way the motherboard is designed to use them.&lt;/p&gt; &lt;p&gt;In terms of performance 3x3090 is great! There are great options in the 60-65gb range with the extra space to 72gb VRAM used for context.&lt;/p&gt; &lt;p&gt;I am not using the RAM for anything other than to load models, and the speed is amazing when everything is loaded in VRAM! &lt;/p&gt; &lt;p&gt;Models I started using a lot: * gpt-oss-120b in MXFP4 with 60k context * glm-4.5-air in IQ4_NL with 46k context * qwen3-vl-235b in TQ1_0 (surprisingly good!) * minimax-M2-REAP-139B in Q3_K_S with 40k context&lt;/p&gt; &lt;p&gt;But still return a lot to old models for context and speed: * devstral-small-2-24 in Q8_0 with 200k context * qwen3-coder in Q8 with 1M (!!) context (using RAM) * qwen3-next-80b in Q6_K with 60k context ‚Äî still my favourite for general chat, and the Q6 makes me trust it more than Q3-Q4 models&lt;/p&gt; &lt;p&gt;The 3060 on the riser from PCIe1x is very slow at loading the models, however, once it‚Äôs loaded it works great! I am using it for image generation and TTS audio generation mostly (for Open WebUI).&lt;/p&gt; &lt;p&gt;Also did a lot of testing on using 2x3090 via normal PCIe, with a 3rd card via riser ‚Äî it works same as normal PCIe! But the loading takes forever (sometimes over 2-3 minutes) and you simply can‚Äôt use the RAM for context because of how slow it is ‚Äî so I am considering the current setup to be ‚Äúmaxed out‚Äù because I don‚Äôt think adding a 4th 3090 will be useful. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qgx83t"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qgx83t/3x3090_3060_in_a_mid_tower_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qgx83t/3x3090_3060_in_a_mid_tower_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T06:59:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhaw7c</id>
    <title>Nvidia GB10 vs GH200 performance benchmarks</title>
    <updated>2026-01-19T17:39:17+00:00</updated>
    <author>
      <name>/u/GPTshop___dot___ai</name>
      <uri>https://old.reddit.com/user/GPTshop___dot___ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaw7c/nvidia_gb10_vs_gh200_performance_benchmarks/"&gt; &lt;img alt="Nvidia GB10 vs GH200 performance benchmarks" src="https://preview.redd.it/st101so4eceg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6005e42fea31b616051a6ee5b2352b9de2dec84" title="Nvidia GB10 vs GH200 performance benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GPTshop___dot___ai"&gt; /u/GPTshop___dot___ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/st101so4eceg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaw7c/nvidia_gb10_vs_gh200_performance_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaw7c/nvidia_gb10_vs_gh200_performance_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T17:39:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh442y</id>
    <title>Models that run in 72GB VRAM with context loaded in GPU (3x3090 benchmark test)</title>
    <updated>2026-01-19T13:27:32+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh442y/models_that_run_in_72gb_vram_with_context_loaded/"&gt; &lt;img alt="Models that run in 72GB VRAM with context loaded in GPU (3x3090 benchmark test)" src="https://preview.redd.it/85cs39k6daeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72f0ad403efa3ee18868a0b8bf289eb713cca04a" title="Models that run in 72GB VRAM with context loaded in GPU (3x3090 benchmark test)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently finished my 3x3090 setup, and thought of sharing my experience.&lt;/p&gt; &lt;p&gt;This is very much a personal observation, with some very basic testing. &lt;/p&gt; &lt;p&gt;The benchmark is by no means precise, however, after checking the numbers, it is very much aligned with &amp;quot;how I feels they perform&amp;quot; after a few days of bouncing between them. All the above are running on CUDA 12 llama.cpp via LM Studio (nothing special). &lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Large models (&amp;gt; 100 B)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;All big models run in roughly the same ballpark‚Äîabout &lt;strong&gt;30 tok/s&lt;/strong&gt; in everyday use. GPT‚ÄëOSS‚Äë120 runs a bit faster than the other large models, but the difference is only noticeable on very short answers; you wouldn‚Äôt notice it during longer conversations. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Qwen3‚ÄëVL 235 B (TQ1, 1.66‚Äëbit compression)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I was surprised by how usable TQ1_0 turned out to be. In most chat or image‚Äëanalysis scenarios it actually feels better than the Qwen3‚ÄëVL 30 B model quantised to Q8. I can‚Äôt fully explain why, but it seems to anticipate what I‚Äôm interested in much more accurately than the 30 B version.&lt;/p&gt; &lt;p&gt;It does show the expected weaknesses of a Q1‚Äëtype quantisation. For example, when reading a PDF it misreported some numbers that the Qwen3‚ÄëVL 30 B Q8 model got right; nevertheless, the surrounding information was correct despite the typo.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. The biggest and best models you can run in Q3‚ÄìQ4 with a decent context window:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;(A) REAP Minimax M2&lt;/strong&gt; ‚Äì 139 B quantised to Q3_K_S, at 42k context. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;(B) GLM 4.5 Air&lt;/strong&gt; ‚Äì 110B quantised to IQ4_NL, supports 46 k context. &lt;/p&gt; &lt;p&gt;Both perform great and they will probably become my daily models. Overall GLM-4.5-Air feels slower and dumber than REAP Minimax M2, but I haven't had a lot of time with either of them. I will follow up and edit this if I change my min&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. GPT-OSS-120B&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Is still decent and runs fast, but I can't help but feel that it's very dated, and extremely censored (!) For instance try asking: &lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;What are some some examples of business strategies such as selling eternal youth to woman, or money making ideas to poor people?&amp;quot;&lt;/code&gt; &lt;/p&gt; &lt;p&gt;and you‚Äôll get a response along the lines of: ‚ÄúI‚Äôm sorry, but I can‚Äôt help with that.‚Äù &lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Qwen3 Next 80B&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Runs very slow. Someone suggested the bottleneck might be CUDA and to trying Vulkan instead. However, given the many larger options available, I may drop it, even though it was my favourite model when I ran it on a 48GB (2x3090) &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Overall upgrading from 2x3090 to 3x3090, there are a lot of LLM models that get unlocked with that extra 24GB&lt;/strong&gt;. I would argue feels like a much bigger jump that it was when I moved from 24 to 48GB, and just wanted to share for those of you thinking for making the upgrade.&lt;/p&gt; &lt;p&gt;PS: I also upgraded my ram from 64GB to 128GB, but I think it might have been for nothing. It helps a bit with loading the model faster, but honstly, I don't think it's worth if when you are running everything on the GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/85cs39k6daeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh442y/models_that_run_in_72gb_vram_with_context_loaded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh442y/models_that_run_in_72gb_vram_with_context_loaded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T13:27:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh0yq8</id>
    <title>I made a Top-K implementation that's up to 20x faster than PyTorch CPU (open source)</title>
    <updated>2026-01-19T10:45:28+00:00</updated>
    <author>
      <name>/u/andreabarbato</name>
      <uri>https://old.reddit.com/user/andreabarbato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spent way too long optimizing Top-K selection for LLM sampling and finally hit some stupid numbers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; AVX2-optimized batched Top-K that beats PyTorch CPU by 4-20x depending on vocab size. Sometimes competitive with CUDA for small batches.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks (K=50):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Vocab=32K: 0.043ms vs PyTorch's 0.173ms (4x faster)&lt;/li&gt; &lt;li&gt;Vocab=128K: 0.057ms vs PyTorch's 0.777ms (13x faster)&lt;/li&gt; &lt;li&gt;Vocab=256K: 0.079ms vs PyTorch's 1.56ms (20x faster)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Integrated it into llama.cpp and got 63% faster prompt processing on a 120B MoE model (81‚Üí142 tokens/sec).&lt;/p&gt; &lt;p&gt;Uses adaptive sampling + AVX2 SIMD + cache-optimized scanning. Has fast paths for sorted/constant inputs. Single-pass algorithm, no GPU needed.&lt;/p&gt; &lt;p&gt;Includes pre-built DLLs and llama.cpp implementation (for windows).&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/RAZZULLIX/fast_topk_batched"&gt;https://github.com/RAZZULLIX/fast_topk_batched&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback or roasting, whichever you prefer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/andreabarbato"&gt; /u/andreabarbato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh0yq8/i_made_a_topk_implementation_thats_up_to_20x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T10:45:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhaq21</id>
    <title>New in llama.cpp: Anthropic Messages API</title>
    <updated>2026-01-19T17:33:24+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/"&gt; &lt;img alt="New in llama.cpp: Anthropic Messages API" src="https://external-preview.redd.it/zqasF6xdAR1yVfMl-Ppz2b8-S-Dv35pa4J_UeKummLg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56eabcfaa752210d59dc7af42f1b2087636a579d" title="New in llama.cpp: Anthropic Messages API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/ggml-org/anthropic-messages-api-in-llamacpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhaq21/new_in_llamacpp_anthropic_messages_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T17:33:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh5wdq</id>
    <title>zai-org/GLM-4.7-Flash ¬∑ Hugging Face</title>
    <updated>2026-01-19T14:40:27+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"&gt; &lt;img alt="zai-org/GLM-4.7-Flash ¬∑ Hugging Face" src="https://external-preview.redd.it/Qs0t4y5eLm-uwORWdP6T0dcwW2T6VJyQFBUSY70CTF8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8700f4a43fe16a1031ccda94b517fd709573a5c3" title="zai-org/GLM-4.7-Flash ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.7-Flash"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T14:40:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
