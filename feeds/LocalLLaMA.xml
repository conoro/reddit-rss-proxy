<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-15T23:54:57+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r5tlin</id>
    <title>Micro-LLM training on "orthogonal" corpora</title>
    <updated>2026-02-15T23:31:50+00:00</updated>
    <author>
      <name>/u/Dumbest-Questions</name>
      <uri>https://old.reddit.com/user/Dumbest-Questions</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Had to spend a day traveling so I wrote a basic LLM from scratch. Single-layer, decoder-only transformer that uses (BPE) for its vocabulary (you'll see later why that matters), with causal masked self-attention for context, and layer normalization for stability. It was trained via stochastic gradient descent. Took me about five hours to write and probably about 20 minutes to train. &lt;/p&gt; &lt;p&gt;Now for the fun part. I've trained it on a concatenation of the Bible (ASV) and preliminary draft of C++ programming language specification (early draft of C++26). I am trying to decide if I want to call it &amp;quot;The Sacred Standard&amp;quot; or &amp;quot;B++&amp;quot; :) &lt;/p&gt; &lt;p&gt;On a more scientific note, I was interested on how linguistic idiosyncrasies in the two corpora would influence the results. As you can imagine, the resulting model is very dumb but the hallucinations are kinda great. So I created a bunch of adversarial(ish) prompts and the results did not disappoint:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The &amp;quot;Shall&amp;quot; Convergence. The word &amp;quot;shall&amp;quot; is the primary connector, since The Bible uses it for commandments while C++ uses it for requirements. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Best in class: &amp;quot;The implementation shall not commit adultery&amp;quot; and &amp;quot;Thou shalt be of type int&amp;quot;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The &amp;quot;Undefined Behavior&amp;quot; Apocalypse. In a way, both texts deal with the consequences of breaking the law. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Best in class: &amp;quot;And if any man shall take away from the words of this book, it results in undefined behavior.&amp;quot;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Symbolic Soups. Since I am using BPE, the model learned that std:: is a high-probability prefix. It ended up applying them to Biblical characters a few times.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Best in class: &amp;quot;The son of std::david was &amp;quot;&lt;/p&gt; &lt;p&gt;Just thought it was fun to share this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dumbest-Questions"&gt; /u/Dumbest-Questions &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5tlin/microllm_training_on_orthogonal_corpora/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5tlin/microllm_training_on_orthogonal_corpora/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5tlin/microllm_training_on_orthogonal_corpora/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T23:31:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5o34g</id>
    <title>QED-Nano: Teaching a Tiny Model to Prove Hard Theorems</title>
    <updated>2026-02-15T19:48:51+00:00</updated>
    <author>
      <name>/u/ThePrimeClock</name>
      <uri>https://old.reddit.com/user/ThePrimeClock</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New Maths model by Hugging face.&lt;/p&gt; &lt;p&gt;Similar line of thought to VibeThinker 1.5B, Hugging Face have released a new model that has been RL trained on solving maths problems. They had an innovative approach that broke down large problems into smaller parts.&lt;/p&gt; &lt;p&gt;Writeup here: &lt;a href="https://huggingface.co/spaces/lm-provers/qed-nano-blogpost#introducing-qed-nano-a-4b-model-for-olympiad-level-proofs"&gt;https://huggingface.co/spaces/lm-provers/qed-nano-blogpost#introducing-qed-nano-a-4b-model-for-olympiad-level-proofs&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;a href="https://huggingface.co/lm-provers/QED-Nano"&gt;QED-Nano&lt;/a&gt; and &lt;a href="https://huggingface.co/lm-provers/QED-Nano-SFT"&gt;QED-Nano-SFT&lt;/a&gt; models.&lt;/li&gt; &lt;li&gt;The &lt;a href="https://huggingface.co/datasets/lm-provers/FineProofs-SFT"&gt;FineProofs-SFT&lt;/a&gt; and &lt;a href="https://huggingface.co/datasets/lm-provers/FineProofs-RL"&gt;FineProofs-RL&lt;/a&gt;datasets for post-training our models.&lt;/li&gt; &lt;li&gt;The &lt;a href="https://github.com/CMU-AIRe/QED-Nano"&gt;training and evaluation code&lt;/a&gt;, including the agent scaffolds.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To quote an author over on Linkedin:&lt;br /&gt; &lt;em&gt;Very excited to share QED-Nano: the smallest theorem proving model to date&lt;/em&gt; &lt;/p&gt; &lt;p&gt;&lt;em&gt;At just 4B parameters, it matches the performance of much larger models on the challenging IMO-ProofBench benchmark and operates entirely in natural language, with no reliance on Lean or external tools.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;With an agent scaffold that scales test-time compute to over 1M tokens per proof, QED-Nano approaches the performance of Gemini 3 Pro, while being ~4X cheaper. Frontier math on your laptop!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;We post-trained QED-Nano using RL with rubrics as rewards, along with a neat trick to enable efficient use of test-time compute. Today, we open source the model and will share the full training recipe and data very soon :)&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThePrimeClock"&gt; /u/ThePrimeClock &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5o34g/qednano_teaching_a_tiny_model_to_prove_hard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5o34g/qednano_teaching_a_tiny_model_to_prove_hard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5o34g/qednano_teaching_a_tiny_model_to_prove_hard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T19:48:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5lk68</id>
    <title>RobinLLM - Free LLM Router (OpenRouter)</title>
    <updated>2026-02-15T18:11:30+00:00</updated>
    <author>
      <name>/u/akumaburn</name>
      <uri>https://old.reddit.com/user/akumaburn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing &lt;strong&gt;RobinLLM&lt;/strong&gt; — a quick passion project born from a burst of inspiration. It queries OpenRouter for available free LLMs and intelligently routes requests to the fastest-responding model. Under the hood, it leverages concurrency so that a single misbehaving model doesn't bottleneck your experience — if one provider stalls, traffic seamlessly shifts to the next best option.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/akumaburn/RobinLLM"&gt;https://github.com/akumaburn/RobinLLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fair warning: this has been tested, but not extensively — your mileage may vary.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/akumaburn"&gt; /u/akumaburn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5lk68/robinllm_free_llm_router_openrouter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5lk68/robinllm_free_llm_router_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5lk68/robinllm_free_llm_router_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T18:11:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5ng7l</id>
    <title>Nvfp4 now working on mlx using lm studio</title>
    <updated>2026-02-15T19:24:13+00:00</updated>
    <author>
      <name>/u/Professional-Bear857</name>
      <uri>https://old.reddit.com/user/Professional-Bear857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I just thought I would make a thread as I've just found after downloading some mlx nvfp4 quants that they now load and run in lm studio. I did try this last month but they didn't work then, I suppose mlx has been updated now in lm studio and so it works. I'm not sure how good the quality is vs other quants in my limited use so far. Hopefully we will see more quants in future that use this format, the speed seems reasonably good compared to standard mlx quants.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Bear857"&gt; /u/Professional-Bear857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5ng7l/nvfp4_now_working_on_mlx_using_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5ng7l/nvfp4_now_working_on_mlx_using_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5ng7l/nvfp4_now_working_on_mlx_using_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T19:24:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r58ca8</id>
    <title>jdopensource/JoyAI-LLM-Flash • HuggingFace</title>
    <updated>2026-02-15T07:18:52+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r58ca8/jdopensourcejoyaillmflash_huggingface/"&gt; &lt;img alt="jdopensource/JoyAI-LLM-Flash • HuggingFace" src="https://preview.redd.it/vkpqjjqj4mjg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=ec8a0dbc966144fb6a5b31a4dec1781fb812a67e" title="jdopensource/JoyAI-LLM-Flash • HuggingFace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vkpqjjqj4mjg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=37e9ae1daf8fb794ef27f75590b6ad7557e0e326"&gt;https://preview.redd.it/vkpqjjqj4mjg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=37e9ae1daf8fb794ef27f75590b6ad7557e0e326&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/jdopensource/JoyAI-LLM-Flash"&gt;https://huggingface.co/jdopensource/JoyAI-LLM-Flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kl2loe9c0mjg1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1b1437da4ce6468f7f9b580b3a7f88bb359f23e9"&gt;https://preview.redd.it/kl2loe9c0mjg1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1b1437da4ce6468f7f9b580b3a7f88bb359f23e9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r58ca8/jdopensourcejoyaillmflash_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r58ca8/jdopensourcejoyaillmflash_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r58ca8/jdopensourcejoyaillmflash_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T07:18:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r50ohq</id>
    <title>Qwen3 Coder Next Speedup with Latest Llama.cpp</title>
    <updated>2026-02-15T00:34:02+00:00</updated>
    <author>
      <name>/u/StardockEngineer</name>
      <uri>https://old.reddit.com/user/StardockEngineer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like it released just a few hours ago. Previously, I was getting 80ish tokens, max, on either of my GPUS in any combination.&lt;/p&gt; &lt;p&gt;Now I'm over 110+ in dual and 130+ on my RTX Pro&lt;/p&gt; &lt;p&gt;PR: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/19375"&gt;https://github.com/ggml-org/llama.cpp/pull/19375&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Update your llama.cpp.&lt;/p&gt; &lt;p&gt;Edit: This is for CUDA devices.&lt;/p&gt; &lt;p&gt;Previous: ``` ❯ llama-bench -m ~/.cache/llama.cpp/Qwen_Qwen3-Coder-Next-GGUF_Qwen3-Coder-Next-Q8_0_Qwen3-Coder-Next-Q8_0-00001-of-00004.gguf -fa 1 -d 0,500,1000 -p 500 -n 32 -ub 2048 -mmp 0&lt;/p&gt; &lt;p&gt;ggml_cuda_init: found 2 CUDA devices: Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9, VMM: yes Device 1: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes | model | size | params | backend | ngl | n_ubatch | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | --------------: | -------------------: | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | pp500 | 2470.78 ± 3.84 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | tg32 | 87.35 ± 0.48 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | pp500 @ d500 | 2468.72 ± 23.27 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | tg32 @ d500 | 85.99 ± 0.53 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | pp500 @ d1000 | 2451.68 ± 19.96 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | tg32 @ d1000 | 87.15 ± 0.57 |&lt;/p&gt; &lt;p&gt;build: e06088da0 (7972) ```&lt;/p&gt; &lt;p&gt;New ``` ❯ llama-bench -m ~/.cache/llama.cpp/Qwen_Qwen3-Coder-Next-GGUF_Qwen3-Coder-Next-Q8_0_Qwen3-Coder-Next-Q8_0-00001-of-00004.gguf -fa 1 -d 0,500,1000 -p 500 -n 32 -ub 2048 -mmp 0 &lt;/p&gt; &lt;p&gt;ggml_cuda_init: found 2 CUDA devices: Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9, VMM: yes Device 1: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes | model | size | params | backend | ngl | n_ubatch | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | --------------: | -------------------: | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | pp500 | 2770.34 ± 3.40 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | tg32 | 118.63 ± 1.14 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | pp500 @ d500 | 2769.27 ± 23.92 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | tg32 @ d500 | 119.69 ± 1.65 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | pp500 @ d1000 | 2753.07 ± 21.85 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | tg32 @ d1000 | 112.34 ± 0.74 |&lt;/p&gt; &lt;p&gt;build: 079feab9e (8055) ```&lt;/p&gt; &lt;p&gt;RTX by itself on new build ``` ❯ llama-bench -m ~/.cache/llama.cpp/Qwen_Qwen3-Coder-Next-GGUF_Qwen3-Coder-Next-Q8_0_Qwen3-Coder-Next-Q8_0-00001-of-00004.gguf -fa 1 -d 0,500,1000 -p 500 -n 32 -ub 2048 -mmp 0 -dev CUDA1 ggml_cuda_init: found 2 CUDA devices: Device 0: NVIDIA RTX 6000 Ada Generation, compute capability 8.9, VMM: yes Device 1: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, compute capability 12.0, VMM: yes | model | size | params | backend | ngl | n_ubatch | fa | dev | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | ------------ | --------------: | -------------------: | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | CUDA1 | pp500 | 3563.60 ± 4.35 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | CUDA1 | tg32 | 132.09 ± 1.07 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | CUDA1 | pp500 @ d500 | 3481.63 ± 33.66 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | CUDA1 | tg32 @ d500 | 119.57 ± 1.43 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | CUDA1 | pp500 @ d1000 | 3534.69 ± 30.89 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA | 99 | 2048 | 1 | CUDA1 | tg32 @ d1000 | 131.07 ± 7.27 |&lt;/p&gt; &lt;p&gt;build: 079feab9e (8055) ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StardockEngineer"&gt; /u/StardockEngineer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r50ohq/qwen3_coder_next_speedup_with_latest_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r50ohq/qwen3_coder_next_speedup_with_latest_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r50ohq/qwen3_coder_next_speedup_with_latest_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T00:34:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4sivv</id>
    <title>KaniTTS2 — open-source 400M TTS model with voice cloning, runs in 3GB VRAM. Pretrain code included.</title>
    <updated>2026-02-14T18:48:10+00:00</updated>
    <author>
      <name>/u/ylankgz</name>
      <uri>https://old.reddit.com/user/ylankgz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4sivv/kanitts2_opensource_400m_tts_model_with_voice/"&gt; &lt;img alt="KaniTTS2 — open-source 400M TTS model with voice cloning, runs in 3GB VRAM. Pretrain code included." src="https://external-preview.redd.it/ZDNyNTR0bGRhaWpnMXVXG3fU6KXmgjVT8aA3qyPzkraI-m8wnbRR2eD2luPt.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=89937a54be408692a953dcd50857e8ea58cf59a4" title="KaniTTS2 — open-source 400M TTS model with voice cloning, runs in 3GB VRAM. Pretrain code included." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, we just open-sourced KaniTTS2 - a text-to-speech model designed for real-time conversational use cases.&lt;/p&gt; &lt;p&gt;## Models:&lt;/p&gt; &lt;p&gt;Multilingual (English, Spanish), and English-specific with local accents. Language support is actively expanding - more languages coming in future updates&lt;/p&gt; &lt;p&gt;## Specs&lt;/p&gt; &lt;p&gt;* 400M parameters (BF16)&lt;/p&gt; &lt;p&gt;* 22kHz sample rate&lt;/p&gt; &lt;p&gt;* Voice Cloning&lt;/p&gt; &lt;p&gt;* ~0.2 RTF on RTX 5090&lt;/p&gt; &lt;p&gt;* 3GB GPU VRAM&lt;/p&gt; &lt;p&gt;* Pretrained on ~10k hours of speech&lt;/p&gt; &lt;p&gt;* Training took 6 hours on 8x H100s&lt;/p&gt; &lt;p&gt;## Full pretrain code - train your own TTS from scratch&lt;/p&gt; &lt;p&gt;This is the part we’re most excited to share. We’re releasing the complete pretraining framework so anyone can train a TTS model for their own language, accent, or domain.&lt;/p&gt; &lt;p&gt;## Links&lt;/p&gt; &lt;p&gt;* Pretrained model: &lt;a href="https://huggingface.co/nineninesix/kani-tts-2-pt"&gt;https://huggingface.co/nineninesix/kani-tts-2-pt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* English model: &lt;a href="https://huggingface.co/nineninesix/kani-tts-2-en"&gt;https://huggingface.co/nineninesix/kani-tts-2-en&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* Pretrain code: &lt;a href="https://github.com/nineninesix-ai/kani-tts-2-pretrain"&gt;https://github.com/nineninesix-ai/kani-tts-2-pretrain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* HF Spaces: &lt;a href="https://huggingface.co/spaces/nineninesix/kani-tts-2-pt"&gt;https://huggingface.co/spaces/nineninesix/kani-tts-2-pt&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/nineninesix/kanitts-2-en"&gt;https://huggingface.co/spaces/nineninesix/kanitts-2-en&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* License: Apache 2.0&lt;/p&gt; &lt;p&gt;Happy to answer any questions. Would love to see what people build with this, especially for underrepresented languages.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ylankgz"&gt; /u/ylankgz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/swybh9pdaijg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4sivv/kanitts2_opensource_400m_tts_model_with_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4sivv/kanitts2_opensource_400m_tts_model_with_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T18:48:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5d9ax</id>
    <title>Step 3.5 and Minimax m. 2.5 on a local hardware - some tests (ik_llama)</title>
    <updated>2026-02-15T12:18:26+00:00</updated>
    <author>
      <name>/u/ZealousidealBunch220</name>
      <uri>https://old.reddit.com/user/ZealousidealBunch220</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5d9ax/step_35_and_minimax_m_25_on_a_local_hardware_some/"&gt; &lt;img alt="Step 3.5 and Minimax m. 2.5 on a local hardware - some tests (ik_llama)" src="https://preview.redd.it/c9gndrc3cnjg1.png?width=140&amp;amp;height=94&amp;amp;auto=webp&amp;amp;s=2ec4cba976ac1c2e02467f93831a94951a7c5b45" title="Step 3.5 and Minimax m. 2.5 on a local hardware - some tests (ik_llama)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I did some llama-bench tests (on ik_llama.cpp fork - it has sota quants (iq4_kss and others, and is faster on prompt processing on both CPU only and CUDA + CPU option)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c9gndrc3cnjg1.png?width=720&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d5b1bfd500f3eff470e671bcaf991ffbd5e4a793"&gt;on my machine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r2kfu09fcnjg1.png?width=2688&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5c3ad692f1fae786fa6baffeecb1682cc493410a"&gt;./ik_llama.cpp/build/bin/llama-bench -m /home/serv/.cache/huggingface/hub/models--ubergarm--Step-3.5-Flash-GGUF/snapshots/c1aefbd3ed11507a02ba452e8e6af10ba36352e8/smol-IQ4_KSS/Step-3.5-Flash-smol-IQ4_KSS-00001-of-00004.gguf --n-cpu-moe 43 -ngl 99 -t 64 -ctk q8_0 -ctv q8_0 -fa 1 -b 4096 -ub 4096 -r 5 -p 16000 -n 4000&lt;/a&gt;&lt;/p&gt; &lt;p&gt;step 3.5 - 529 on prompt (16k), 30 on text gen (4k)&lt;/p&gt; &lt;p&gt;(batch size 2048 instead of 4096 gives 300 tk/s on prompt)&lt;/p&gt; &lt;p&gt;step 3.5 is a GREAT model, it is very nuanced , but the thinking time and token consumption is crippling (up to 10k-20k tokens on thinking with all the details).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zpan44hvenjg1.png?width=2596&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa3443f57c5fd18f7cabe57cfa3fee0a17e713a6"&gt;./ik_llama.cpp/build/bin/llama-bench -m /media/serv/E/MiniMax-M2.5-smol-IQ4_KSS-00001-of-00004.gguf --n-cpu-moe 54 -ngl 99 -t 64 -ctk q8_0 -ctv q8_0 -fa 1 -b 4096 -ub 4096 -r 2 -p 16000 -n 4000&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I didn’t want to wait as long as the five repeats used with step 3.5, so I ran only two repeats minimax m.2.5 - 470 on prompt (16), 26,5 on text gen (4k) &lt;/p&gt; &lt;p&gt;With the new models that are able to perform at the level of the top paid models I'm starting to have a feeling of freedom&lt;/p&gt; &lt;p&gt;I invite everyone to discuss the new models and the methods and optimizations for running them locally!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZealousidealBunch220"&gt; /u/ZealousidealBunch220 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5d9ax/step_35_and_minimax_m_25_on_a_local_hardware_some/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5d9ax/step_35_and_minimax_m_25_on_a_local_hardware_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5d9ax/step_35_and_minimax_m_25_on_a_local_hardware_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T12:18:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5q6ib</id>
    <title>Prometheus metrics for NVIDIA DGX Spark clusters</title>
    <updated>2026-02-15T21:11:19+00:00</updated>
    <author>
      <name>/u/Icy_Programmer7186</name>
      <uri>https://old.reddit.com/user/Icy_Programmer7186</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5q6ib/prometheus_metrics_for_nvidia_dgx_spark_clusters/"&gt; &lt;img alt="Prometheus metrics for NVIDIA DGX Spark clusters" src="https://preview.redd.it/0zab5ars4qjg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a1bad8aae55728c775afaf40fa6b42267647763" title="Prometheus metrics for NVIDIA DGX Spark clusters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I’m sharing &lt;strong&gt;dgx-spark-prometheus&lt;/strong&gt; — a small repo to help you get &lt;strong&gt;Prometheus monitoring/metrics for NVIDIA DGX Spark&lt;/strong&gt; clusters.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ateska/dgx-spark-prometheus"&gt;https://github.com/ateska/dgx-spark-prometheus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it’s for&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Making DGX Spark cluster easier to observe with &lt;strong&gt;Prometheus &amp;amp; Grafana&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Providing a practical, repo-based setup you can adapt to your own DGX Spark cluster&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Feedback wanted&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Does this match how you monitor your Spark cluster?&lt;/li&gt; &lt;li&gt;Any improvements you’d like (dashboards, alerts, example scrape configs, Helm/K8s flavor, Grafana panels, etc.)?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you try it, I’d appreciate notes/PRs/issues.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Programmer7186"&gt; /u/Icy_Programmer7186 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0zab5ars4qjg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5q6ib/prometheus_metrics_for_nvidia_dgx_spark_clusters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5q6ib/prometheus_metrics_for_nvidia_dgx_spark_clusters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T21:11:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5o3y2</id>
    <title>cant tell if this is true or not</title>
    <updated>2026-02-15T19:49:45+00:00</updated>
    <author>
      <name>/u/panic_in_the_cosmos</name>
      <uri>https://old.reddit.com/user/panic_in_the_cosmos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5o3y2/cant_tell_if_this_is_true_or_not/"&gt; &lt;img alt="cant tell if this is true or not" src="https://preview.redd.it/wfiz477bqpjg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f034a759832af400ab9446fb3fd23d2155b63908" title="cant tell if this is true or not" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panic_in_the_cosmos"&gt; /u/panic_in_the_cosmos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wfiz477bqpjg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5o3y2/cant_tell_if_this_is_true_or_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5o3y2/cant_tell_if_this_is_true_or_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T19:49:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1r59th1</id>
    <title>Kreuzberg v4.3.0 and benchmarks</title>
    <updated>2026-02-15T08:49:49+00:00</updated>
    <author>
      <name>/u/Eastern-Surround7763</name>
      <uri>https://old.reddit.com/user/Eastern-Surround7763</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks,&lt;/p&gt; &lt;p&gt;we have two announcements to share about &lt;a href="https://github.com/kreuzberg-dev/kreuzberg"&gt;Kreuzberg&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;First, we’ve published a new set of comparative benchmarks with an interactive UI and fully reproducible results. We’ve been working on these for quite some time, and the goal is to help developers understand how Kreuzberg behaves in real production scenarios and to make performance claims transparent and verifiable.&lt;/p&gt; &lt;p&gt;Second, we released Kreuzberg v4.3.0, which brings several improvements and adds PaddleOCR as an optional backend through a native Rust integration. This release is particularly important for teams working with Chinese and other East Asian languages, where Paddle models perform very well.&lt;/p&gt; &lt;p&gt;What is Kreuzberg?&lt;/p&gt; &lt;p&gt;Kreuzberg is an open-source (MIT-licensed) polyglot document intelligence framework written in Rust, with bindings for Python, TypeScript/JavaScript (Node, Bun, and WASM), Ruby, Java, Go, PHP, Elixir, and C#. It’s also available as a CLI tool, Docker image, REST API server, and MCP server.&lt;/p&gt; &lt;p&gt;In practical terms, Kreuzberg helps you extract text, metadata, tables, and structured information from 75+ document and image formats, perform OCR, and prepare data for search, embeddings, or LLM pipelines. This kind of preprocessing step is necessary in many AI applications, document workflows, and data pipelines, where the quality of ingestion directly affects downstream results.&lt;/p&gt; &lt;p&gt;Comparative benchmarks: &lt;a href="https://kreuzberg.dev/benchmarks"&gt;https://kreuzberg.dev/benchmarks&lt;/a&gt; &lt;/p&gt; &lt;p&gt;The new benchmarks compare Kreuzberg with several widely used document extraction tools, including Apache Tika, Docling, Unstructured, PDFPlumber, PyMuPDF4LLM, MarkItDown, and Mineru.&lt;/p&gt; &lt;p&gt;All benchmarks are executed automatically in GitHub Actions using a standardized Linux environment and a shared harness, so each framework is tested under the same conditions. We measure throughput, extraction duration, memory consumption, CPU usage, tail latencies, success rates, and extraction quality, both in single-file scenarios (latency and cold start) and batch processing scenarios (parallelism and throughput).&lt;/p&gt; &lt;p&gt;At a high level, the results show significantly higher throughput across common document types such as PDFs, DOCX, PPTX, and HTML. Processing times are often measured in milliseconds rather than seconds, cold start times are lower than most alternatives, and the installation footprint is smaller.&lt;/p&gt; &lt;p&gt;You can explore the benchmarks and download the raw results from the project pages if you want to take a deeper look.&lt;/p&gt; &lt;p&gt;What’s new in v4.3.0&lt;/p&gt; &lt;p&gt;Alongside the benchmarks, we’ve continued shipping improvements and fixes.&lt;/p&gt; &lt;p&gt;One of the biggest additions in this release is PaddleOCR support through a native Rust integration, with automatic model downloading and caching. This currently supports six languages: English, Chinese, Japanese, Korean, German, and French, and makes it easier to build pipelines that require high-quality OCR for Asian languages without leaving the Rust ecosystem.&lt;/p&gt; &lt;p&gt;We also added structured document data extraction, expanded format support, and removed LibreOffice as a dependency by introducing native extraction for legacy formats such as .doc and .ppt. Reducing external dependencies has been an ongoing focus for us because it simplifies deployment and reduces installation size, especially in containerized environments.&lt;/p&gt; &lt;p&gt;The full changelog is available here:&lt;br /&gt; &lt;a href="https://github.com/kreuzberg-dev/kreuzberg/blob/main/CHANGELOG.md"&gt;https://github.com/kreuzberg-dev/kreuzberg/blob/main/CHANGELOG.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Getting involved&lt;/p&gt; &lt;p&gt;Kreuzberg is an open-source project and contributions are always welcome!Thanks for reading, and we’d love to hear what you think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eastern-Surround7763"&gt; /u/Eastern-Surround7763 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r59th1/kreuzberg_v430_and_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r59th1/kreuzberg_v430_and_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r59th1/kreuzberg_v430_and_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T08:49:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5fxyd</id>
    <title>Qwen3-Code-Next ggufs: Any difference between Q4KXL and MXPF4?</title>
    <updated>2026-02-15T14:27:39+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The later is a few GBs smaller, but are there any meaningful differences performance wise?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5fxyd/qwen3codenext_ggufs_any_difference_between_q4kxl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5fxyd/qwen3codenext_ggufs_any_difference_between_q4kxl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5fxyd/qwen3codenext_ggufs_any_difference_between_q4kxl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T14:27:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5kgn0</id>
    <title>Does anyone know how Nanbeige4.1-3B can be so impressive compared with other models of similar size?</title>
    <updated>2026-02-15T17:29:10+00:00</updated>
    <author>
      <name>/u/cloudxaas</name>
      <uri>https://old.reddit.com/user/cloudxaas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seemed extremely consistent, cohesive, no repetition so far I've tested, and it works very well on small vram size.&lt;/p&gt; &lt;p&gt;How is this possible?&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; &lt;a href="https://huggingface.co/Nanbeige/Nanbeige4.1-3B"&gt;https://huggingface.co/Nanbeige/Nanbeige4.1-3B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cloudxaas"&gt; /u/cloudxaas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5kgn0/does_anyone_know_how_nanbeige413b_can_be_so/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5kgn0/does_anyone_know_how_nanbeige413b_can_be_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5kgn0/does_anyone_know_how_nanbeige413b_can_be_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T17:29:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5pdnn</id>
    <title>rednote-hilab/dots.ocr-1.5</title>
    <updated>2026-02-15T20:39:43+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5pdnn/rednotehilabdotsocr15/"&gt; &lt;img alt="rednote-hilab/dots.ocr-1.5" src="https://external-preview.redd.it/XrUFWJLhWfPf18dDw3gY1NttJAJXB7-AhzrL3jFOW4M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b563f4bfc26c48348de6f8733f089e70363d7903" title="rednote-hilab/dots.ocr-1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/rednote-hilab/dots.ocr-1.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5pdnn/rednotehilabdotsocr15/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5pdnn/rednotehilabdotsocr15/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T20:39:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5beqe</id>
    <title>The current top 4 models on openrouter are all open-weight</title>
    <updated>2026-02-15T10:29:20+00:00</updated>
    <author>
      <name>/u/svantana</name>
      <uri>https://old.reddit.com/user/svantana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5beqe/the_current_top_4_models_on_openrouter_are_all/"&gt; &lt;img alt="The current top 4 models on openrouter are all open-weight" src="https://preview.redd.it/jjpkakoaxmjg1.png?width=140&amp;amp;height=132&amp;amp;auto=webp&amp;amp;s=0fe256dcd41b962138f1a252df9f49245bdb579d" title="The current top 4 models on openrouter are all open-weight" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I could be wrong but I think this is the first time this has happened. Is this a pivotal moment or just a temporary fluke?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jjpkakoaxmjg1.png?width=1738&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5072055e50df1701fe5ab51ce67e1b7476f8c62d"&gt;https://preview.redd.it/jjpkakoaxmjg1.png?width=1738&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5072055e50df1701fe5ab51ce67e1b7476f8c62d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/svantana"&gt; /u/svantana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5beqe/the_current_top_4_models_on_openrouter_are_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5beqe/the_current_top_4_models_on_openrouter_are_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5beqe/the_current_top_4_models_on_openrouter_are_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T10:29:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1r569eb</id>
    <title>PSA: NVIDIA DGX Spark has terrible CUDA &amp; software compatibility; and seems like a handheld gaming chip.</title>
    <updated>2026-02-15T05:17:53+00:00</updated>
    <author>
      <name>/u/goldcakes</name>
      <uri>https://old.reddit.com/user/goldcakes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've spent the past week experimenting with the DGX Spark and I am about to return it. While I had understood the memory bandwidth and performance limitations, I like the CUDA ecosystem and was willing to pay the premium. Unfortunately, my experiences have been quite poor, and I suspect this is actually handheld gaming scraps that NVIDIA rushed to turn into a product to compete with Apple and Strix Halo.&lt;/p&gt; &lt;p&gt;The biggest issue: DGX Spark is not datacentre Blackwell, it's not even gaming Blackwell, it has its own special snowflake sm121 architecture. A lot of software do not work with it, or &lt;a href="https://github.com/triton-lang/triton/issues/8335#issuecomment-3417643519"&gt;have been patched to run sm80&lt;/a&gt; (Ampere, 6 years old!) codepaths which means it doesn't take advantage of blackwell optimisations.&lt;/p&gt; &lt;p&gt;When questioned about this on NVIDIA support forum, &lt;a href="https://forums.developer.nvidia.com/t/dgx-spark-sm121-software-support-is-severely-lacking-official-roadmap-needed/357663/9#p-1745639-h-1-when-will-sm121-receive-native-support-instead-of-sm80-fallbacks-10"&gt;an official NVIDIA representative said&lt;/a&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;sm80-class kernels can execute on DGX Spark because Tensor Core behavior is very similar, particularly for GEMM/MMAs (closer to the GeForce Ampere-style MMA model). &lt;strong&gt;DGX Spark not has tcgen05 like jetson Thor or GB200, due die space with RT Cores and DLSS algorithm&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Excuse me?? The reason we're getting cut-down tensor cores (not real blackwell) is because of RT Cores and &amp;quot;DLSS algorithm&amp;quot;? This is an AI dev kit; why would I need RT Cores, and additionally how does DLSS come into play? This makes me think they tried to turn a gaming handheld GPU (which needs/supports unified memory) into a poor competitor for a market they weren't prepared for.&lt;/p&gt; &lt;p&gt;In addition, in the same post the rep posted what appears to be LLM hallucinations, mentioning issues have been fixed in version numbers and releases for software libraries that &lt;em&gt;do not exist&lt;/em&gt;. &lt;/p&gt; &lt;p&gt;Just be careful when buying a DGX Spark. You are not really getting a modern CUDA experience. Yes, everything works fine if you pretend you only have an Ampere, but attempting to use any Blackwell features is an exercise in futility.&lt;/p&gt; &lt;p&gt;Additionally, for something that is supposed to be ready 'out of the box', many people (including myself and servethehome) reports basic issues like &lt;strong&gt;HDMI display output&lt;/strong&gt;. I originally thought my Spark was DOA; nope; it just refuses to work with my 1080p144 viewsonic (which works with all other GPUs; including my NVIDIA ones); and had to switch to my 4K60 monitor. Dear NVIDIA, you should not have basic display output issues...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goldcakes"&gt; /u/goldcakes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r569eb/psa_nvidia_dgx_spark_has_terrible_cuda_software/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r569eb/psa_nvidia_dgx_spark_has_terrible_cuda_software/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r569eb/psa_nvidia_dgx_spark_has_terrible_cuda_software/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T05:17:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5k46x</id>
    <title>If you were starting with local LLMs today, what would you do differently</title>
    <updated>2026-02-15T17:15:44+00:00</updated>
    <author>
      <name>/u/Bubbly_Run_2349</name>
      <uri>https://old.reddit.com/user/Bubbly_Run_2349</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I am seriously considering investing a significant portion of my signing bonus into a local LLM setup as a hobby and learning project once I start my job in August.&lt;/p&gt; &lt;p&gt;I am currently in university. I have studied a lot of theory, but I feel I am missing practical, hands-on experience.&lt;/p&gt; &lt;p&gt;If you were starting from scratch today, knowing what you know now, what would you do differently?&lt;/p&gt; &lt;p&gt;Specifically:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What hardware would you prioritize&lt;/li&gt; &lt;li&gt;What inference stack would you start with&lt;/li&gt; &lt;li&gt;What beginner mistakes should be avoided&lt;/li&gt; &lt;li&gt;What models are actually practical on consumer GPUs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I know much of this information already exists, but it is often fragmented across many threads, benchmark posts, and user experiences.&lt;/p&gt; &lt;p&gt;I would really appreciate any lessons learned from people who have been running local setups for a while.&lt;/p&gt; &lt;p&gt;Thank you :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bubbly_Run_2349"&gt; /u/Bubbly_Run_2349 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5k46x/if_you_were_starting_with_local_llms_today_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5k46x/if_you_were_starting_with_local_llms_today_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5k46x/if_you_were_starting_with_local_llms_today_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T17:15:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5dyna</id>
    <title>how to train a tiny model (4B) to prove hard theorems</title>
    <updated>2026-02-15T12:55:39+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5dyna/how_to_train_a_tiny_model_4b_to_prove_hard/"&gt; &lt;img alt="how to train a tiny model (4B) to prove hard theorems" src="https://preview.redd.it/pqtgdyl5onjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6b29a2f167dc4c50fd079038a0edf17bc75ba3f" title="how to train a tiny model (4B) to prove hard theorems" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pqtgdyl5onjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5dyna/how_to_train_a_tiny_model_4b_to_prove_hard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5dyna/how_to_train_a_tiny_model_4b_to_prove_hard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T12:55:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5lra1</id>
    <title>Bad Apple but it's GPT-2 XL Attention Maps</title>
    <updated>2026-02-15T18:19:02+00:00</updated>
    <author>
      <name>/u/TheLatentExplorer</name>
      <uri>https://old.reddit.com/user/TheLatentExplorer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5lra1/bad_apple_but_its_gpt2_xl_attention_maps/"&gt; &lt;img alt="Bad Apple but it's GPT-2 XL Attention Maps" src="https://external-preview.redd.it/bQ8_O8mHCtpCo5Q-asAduYJCGmACnuapiWfZUdt-AYQ.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b50e9ecd47f69e52dab8879c365cdc6251af431c" title="Bad Apple but it's GPT-2 XL Attention Maps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I optimized learnable input embeddings for a frozen GPT-2 XL model so that its attention maps display the frames of the Bad Apple music video. The model never saw an image in its life, The optimizer just found the right inputs. &lt;/p&gt; &lt;p&gt;This is a silly little project but I found it interesting, here are some details about how I made that work:&lt;br /&gt; - freeze the entire model, only optimize a raw 256x1600 embedding tensor per frame&lt;br /&gt; - target a single attention head (head 0, layer 0), only compute Q and K projections&lt;br /&gt; - use MSE loss in logit space (pre-softmax) instead of on the attention weights, gives ~250x stronger gradients&lt;br /&gt; - multi-start optimization: 3 random seeds, keep the best, refine&lt;br /&gt; - post-processing: per-row z-score normalization + gaussian blur + magma colormap &lt;/p&gt; &lt;p&gt;3286 frames, ~12 minutes on an RTX 5070 Ti, 4.5 GB VRAM. &lt;/p&gt; &lt;p&gt;Blog post (full writeup with math): &lt;a href="https://brayevalerien.com/blog/bad-apple-but-its-gpt2/"&gt;https://brayevalerien.com/blog/bad-apple-but-its-gpt2/&lt;/a&gt;&lt;br /&gt; Code: &lt;a href="https://github.com/brayevalerien/bad-apple-but-its-gpt2"&gt;https://github.com/brayevalerien/bad-apple-but-its-gpt2&lt;/a&gt;&lt;br /&gt; YouTube: &lt;a href="https://www.youtube.com/watch?v=UU14rQO6VzU"&gt;https://www.youtube.com/watch?v=UU14rQO6VzU&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLatentExplorer"&gt; /u/TheLatentExplorer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=UU14rQO6VzU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5lra1/bad_apple_but_its_gpt2_xl_attention_maps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5lra1/bad_apple_but_its_gpt2_xl_attention_maps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T18:19:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5m4vl</id>
    <title>How to run Qwen3-Coder-Next 80b parameters model on 8Gb VRAM</title>
    <updated>2026-02-15T18:33:14+00:00</updated>
    <author>
      <name>/u/AccomplishedLeg527</name>
      <uri>https://old.reddit.com/user/AccomplishedLeg527</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am running large llms on my &lt;strong&gt;8Gb&lt;/strong&gt; &lt;strong&gt;laptop 3070ti&lt;/strong&gt;. I have optimized: &lt;a href="https://github.com/nalexand/LTX-2-OPTIMIZED"&gt;&lt;strong&gt;LTX-2&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="https://github.com/nalexand/Wan2.2"&gt;&lt;strong&gt;Wan2.2&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="https://github.com/nalexand/HeartMula-OPTIMIZED-8GB"&gt;&lt;strong&gt;HeartMula&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="https://github.com/nalexand/ACE-Step-1.5-OPTIMIZED"&gt;&lt;strong&gt;ACE-STEP 1.5&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;And now i abble to run 80b parameters model &lt;strong&gt;Qwen3-Coder-Next !!!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Instruction here:&lt;/strong&gt; &lt;a href="https://github.com/nalexand/Qwen3-Coder-OPTIMIZED"&gt;https://github.com/nalexand/Qwen3-Coder-OPTIMIZED&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is FP8 quant 80Gb in size, it is impossible to fit it on 8Gb VRAM + 32Gb RAM.&lt;/p&gt; &lt;p&gt;So first i tried offloading to disk with device=&amp;quot;auto&amp;quot; using accelerate and i got &lt;strong&gt;1 token per 255 second&lt;/strong&gt; :(.&lt;/p&gt; &lt;p&gt;Than i found that most of large tensors is mlp experts and all other fit in 4.6Gb VRAM so i build custom lazy loading for experts with 2 layers caching VRAM + pinned RAM and got up to 85% cache hit rate and speed up to 1.2t/s it`s &lt;strong&gt;300x speedup.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I wonder what speed will be on 4090 or 5090 desktop..&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;self.max_gpu_cache = 18 # TODO: calculate based on free ram and context window size self.max_ram_cache = 100 # TODO: calculate based on available pinable memory or use unpinned (slow) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Tune this two parameters for your RAM/VRAM (each 18 it is about 3GB). For 5090 max_gpu_cache = 120 and it is &amp;gt;85% cache hit rate. Who can check speed?&lt;/p&gt; &lt;p&gt;Best for loading speed: PCE 5.0 Raid 0 up to 30Gb/s NVME SSD.&lt;/p&gt; &lt;p&gt;Available pinable ram (usualy 1/2 RAM) with DMA - much faster than RAM.&lt;/p&gt; &lt;p&gt;Hope 5090 will give &amp;gt; 20 t/s..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AccomplishedLeg527"&gt; /u/AccomplishedLeg527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5m4vl/how_to_run_qwen3codernext_80b_parameters_model_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5m4vl/how_to_run_qwen3codernext_80b_parameters_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5m4vl/how_to_run_qwen3codernext_80b_parameters_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T18:33:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5qfb8</id>
    <title>inclusionAI/Ling-2.5-1T · Hugging Face</title>
    <updated>2026-02-15T21:20:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5qfb8/inclusionailing251t_hugging_face/"&gt; &lt;img alt="inclusionAI/Ling-2.5-1T · Hugging Face" src="https://external-preview.redd.it/nCxW8JHyfmzzv3lMTtcAqL8Ez3yOAkDeuLrrPCFMKz4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c049d1c20ccf2dbac44fc910e04d8dc862b0d7b1" title="inclusionAI/Ling-2.5-1T · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;another 1T model :)&lt;/p&gt; &lt;p&gt;from &lt;strong&gt;inclusionAI&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;Ling-2.5-1T, Inclusive Intelligence, Instant Impact.&lt;/p&gt; &lt;p&gt;Today, we launch Ling-2.5-1T and make it open source.&lt;/p&gt; &lt;p&gt;Thinking models raise the ceiling of intelligence, while instant models expand its reach by balancing efficiency and performance—making AGI not only more powerful, but also more accessible. As the latest flagship instant model in the Ling family, Ling-2.5-1T delivers comprehensive upgrades across model architecture, token efficiency, and preference alignment, designed to bring universally accessible AI to a new level of quality.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ling-2.5-1T features 1T total parameters (with 63B active parameters). Its pre-training corpus has expanded from 20T to 29T tokens compared to the previous generation. Leveraging an efficient hybrid linear attention architecture and refined data strategy, the model delivers exceptionally high throughput while processing context lengths of up to 1M tokens.&lt;/li&gt; &lt;li&gt;By introducing a composite reward mechanism combining &amp;quot;Correctness&amp;quot; and &amp;quot;Process Redundancy&amp;quot;, Ling-2.5-1T further pushes the frontier of efficiency-performance balance in instant models. At comparable token efficiency levels, Ling-2.5-1T’s reasoning capabilities significantly outperform its predecessor, approaching the level of frontier &amp;quot;thinking models&amp;quot; that typically consume ~4x the output tokens.&lt;/li&gt; &lt;li&gt;Through refined alignment strategies—such as bidirectional RL feedback and Agent-based instruction constraint verification—Ling-2.5-1T achieves substantial improvements over the previous generation in preference alignment tasks, including creative writing and instruction following.&lt;/li&gt; &lt;li&gt;Trained with Agentic RL in large-scale high-fidelity interactive environments, Ling-2.5-1T is compatible with mainstream agent platforms such as Claude Code, OpenCode, and OpenClaw. It achieves leading open-source performance on the general tool-calling benchmark, BFCL-V4.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-2.5-1T"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5qfb8/inclusionailing251t_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5qfb8/inclusionailing251t_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T21:20:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5nnhz</id>
    <title>GLM-5 is officially on NVIDIA NIM, and you can now use it to power Claude Code for FREE 🚀</title>
    <updated>2026-02-15T19:31:53+00:00</updated>
    <author>
      <name>/u/PreparationAny8816</name>
      <uri>https://old.reddit.com/user/PreparationAny8816</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5nnhz/glm5_is_officially_on_nvidia_nim_and_you_can_now/"&gt; &lt;img alt="GLM-5 is officially on NVIDIA NIM, and you can now use it to power Claude Code for FREE 🚀" src="https://external-preview.redd.it/jeBOY9b76BQPslI7xSt75z5frSsGlOBOeFPAwBsENE8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f0a43cd3f6b53ec76e275ecbc699697f2afc0fc3" title="GLM-5 is officially on NVIDIA NIM, and you can now use it to power Claude Code for FREE 🚀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA just added &lt;strong&gt;z-ai/glm5&lt;/strong&gt; to their NIM inventory, and I’ve just updated &lt;strong&gt;free-claude-code&lt;/strong&gt; to support it fully. This means you can now run Anthropic’s powerful &lt;strong&gt;Claude Code CLI&lt;/strong&gt; using GLM-5 as the backend engine completely free.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is this?&lt;/strong&gt; &lt;code&gt;free-claude-code&lt;/code&gt; is a lightweight proxy that converts Claude Code’s Anthropic API requests into NVIDIA NIM format. Since NVIDIA offers a free tier with a generous &lt;strong&gt;40 requests/min&lt;/strong&gt; limit, you can basically use Claude Code autonomously without a paid Anthropic subscription.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why GLM-5 in with this harness is a game changer:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Zero Cost:&lt;/strong&gt; Leverage NVIDIA NIM’s free API credits to explore codebases.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interleaved Thinking:&lt;/strong&gt; Native interleaved thinking tokens are preserved across turns allowing GLM-5 to full advantage of thinking from previous turn, this is not supported in OpenCode.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Remote Control:&lt;/strong&gt; I’ve integrated a &lt;strong&gt;Telegram bot&lt;/strong&gt; so you can send coding tasks to GLM-5 from your phone while you're away from your desk.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optimizations:&lt;/strong&gt; Currently there are 5 optimizations to reduce calls to the LLMs which are not present in OpenCode.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;More features:&lt;/strong&gt; Built-in configurable sliding window rate limiter for concurrent sessions, telegram session forking and persistence and more.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Popular Models Supported:&lt;/strong&gt; Beyond &lt;code&gt;z-ai/glm5&lt;/code&gt;, the proxy supports other heavy hitters like &lt;code&gt;kimi-k2.5&lt;/code&gt; and &lt;code&gt;minimax-m2.1&lt;/code&gt;. You can find the full list in the &lt;code&gt;nvidia_nim_models.json&lt;/code&gt; file in the repo.&lt;/p&gt; &lt;p&gt;Check it out on GitHub and let me know what you think! Leave a star if you like it. I built it as a side project to have some fun.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit 1:&lt;/strong&gt; Added instructions for free usage with Claude Code VSCode extension.&lt;br /&gt; &lt;strong&gt;Edit 2:&lt;/strong&gt; Added OpenRouter as a provider.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PreparationAny8816"&gt; /u/PreparationAny8816 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Alishahryar1/free-claude-code"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5nnhz/glm5_is_officially_on_nvidia_nim_and_you_can_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5nnhz/glm5_is_officially_on_nvidia_nim_and_you_can_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T19:31:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r5h1gj</id>
    <title>You can run MiniMax-2.5 locally</title>
    <updated>2026-02-15T15:14:51+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5h1gj/you_can_run_minimax25_locally/"&gt; &lt;img alt="You can run MiniMax-2.5 locally" src="https://preview.redd.it/hd369oaucojg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=baf9267391b3836cb000418670d350915c3a8405" title="You can run MiniMax-2.5 locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MiniMax-2.5 is a new open LLM achieving SOTA in coding, agentic tool use and search and office work.&lt;/p&gt; &lt;p&gt;The 230B parameters (10B active) model has a &lt;strong&gt;200K context&lt;/strong&gt; window and unquantized bf16 requires &lt;strong&gt;457GB&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;Unsloth Dynamic &lt;strong&gt;3-bit&lt;/strong&gt; GGUF reduces size to &lt;strong&gt;101GB&lt;/strong&gt; &lt;strong&gt;(-62%).&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official Guide -&lt;/strong&gt; &lt;a href="https://unsloth.ai/docs/models/minimax-2.5"&gt;&lt;strong&gt;https://unsloth.ai/docs/models/minimax-2.5&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GGUF Models -&lt;/strong&gt; &lt;a href="https://huggingface.co/unsloth/MiniMax-M2.5-GGUF"&gt;&lt;strong&gt;https://huggingface.co/unsloth/MiniMax-M2.5-GGUF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hd369oaucojg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r5h1gj/you_can_run_minimax25_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r5h1gj/you_can_run_minimax25_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-15T15:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3t775</id>
    <title>AMA with MiniMax — Ask Us Anything!</title>
    <updated>2026-02-13T16:07:54+00:00</updated>
    <author>
      <name>/u/HardToVary</name>
      <uri>https://old.reddit.com/user/HardToVary</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt; &lt;img alt="AMA with MiniMax — Ask Us Anything!" src="https://preview.redd.it/5z2li1ntcajg1.jpg?width=140&amp;amp;height=59&amp;amp;auto=webp&amp;amp;s=ce3340cd37b7e9408878509be00dd7b871efebde" title="AMA with MiniMax — Ask Us Anything!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;We're &lt;strong&gt;MiniMax&lt;/strong&gt;, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;.5&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining the channel today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; — Founder of MiniMax&lt;/li&gt; &lt;li&gt;&lt;a href="/u/Wise_Evidence9973"&gt;u/Wise_Evidence9973&lt;/a&gt; — Head of LLM Research&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ryan85127704"&gt;u/ryan85127704&lt;/a&gt; — Head of Engineering&lt;/li&gt; &lt;li&gt;&lt;a href="/u/HardToVary"&gt;u/HardToVary&lt;/a&gt; — LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c"&gt;https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S. We'll continue monitoring and responding to questions for 48 hours after the end of the AMA.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HardToVary"&gt; /u/HardToVary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:07:54+00:00</published>
  </entry>
</feed>
