<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-11T03:02:14+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1otwcg0</id>
    <title>Hello I’m planning to open-source my Sesame alternative. It’s kinda rough, but not too bad!</title>
    <updated>2025-11-11T01:26:50+00:00</updated>
    <author>
      <name>/u/Danny-1257</name>
      <uri>https://old.reddit.com/user/Danny-1257</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otwcg0/hello_im_planning_to_opensource_my_sesame/"&gt; &lt;img alt="Hello I’m planning to open-source my Sesame alternative. It’s kinda rough, but not too bad!" src="https://external-preview.redd.it/dNMvETR3TAgky9xFPLP6B7g5tW9HVTS23HKHMQaioOs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=17e8eafbceac1243a6cb9d4832551a0bb15664c7" title="Hello I’m planning to open-source my Sesame alternative. It’s kinda rough, but not too bad!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1otwcg0/video/bzrf0ety5j0g1/player"&gt;https://reddit.com/link/1otwcg0/video/bzrf0ety5j0g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;I wanted to share a project I’ve been working on. I’m a founder currently building a new product, but until last month I was making a conversational AI. After pivoting, I thought I should share my codes.&lt;/p&gt; &lt;p&gt;The project is a voice AI that can have real-time conversations. The client side runs on the web, and the backend runs models in the cloud with gpu.&lt;/p&gt; &lt;p&gt;In detail : for STT, I used whisper-large-v3-turbo, and for TTS, I modified chatterbox for real-time streaming. LLM is gpt api or gpt-oss-20b by ollama.&lt;/p&gt; &lt;p&gt;One advantage of local llm is that all data can remain local on your machine. In terms of speed and performance, I also recommend using the api. and the pricing is not expensive anymore. (costs $0.1 for 30 minutes? I guess)&lt;/p&gt; &lt;p&gt;In numbers: TTFT is around 1000 ms, and even with the llm api cost included, it’s roughly $0.50 per hour on a runpod A40 instance.&lt;/p&gt; &lt;p&gt;There are a few small details I built to make conversations feel more natural (though they might not be obvious in the demo video):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;When the user is silent, it occasionally generates small self-talk.&lt;/li&gt; &lt;li&gt;The llm is always prompted to start with a pre-set “first word,” and that word’s audio is pre-generated to reduce TTFT.&lt;/li&gt; &lt;li&gt;It can insert short silences mid sentence for more natural pacing.&lt;/li&gt; &lt;li&gt;You can interrupt mid-speech, and only what’s spoken before interruption gets logged in the conversation history.&lt;/li&gt; &lt;li&gt;Thanks to multilingual Chatterbox, it can talk in any language and voice (English works best so far).&lt;/li&gt; &lt;li&gt;Audio is encoded and decoded with Opus.&lt;/li&gt; &lt;li&gt;Smart turn detection.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This is the repo! It includes both client and server codes. &lt;a href="https://github.com/thxxx/harper"&gt;https://github.com/thxxx/harper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’d love to hear what the community thinks. what do you think matters most for truly natural voice conversations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Danny-1257"&gt; /u/Danny-1257 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otwcg0/hello_im_planning_to_opensource_my_sesame/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otwcg0/hello_im_planning_to_opensource_my_sesame/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otwcg0/hello_im_planning_to_opensource_my_sesame/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T01:26:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1otislj</id>
    <title>After a year building an open-source AI framework, I’m starting to wonder what actually gets attention</title>
    <updated>2025-11-10T16:44:16+00:00</updated>
    <author>
      <name>/u/DocteurW</name>
      <uri>https://old.reddit.com/user/DocteurW</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;It took me over a year to finally write this.&lt;br /&gt; Even now, I’m not sure it's worth it.&lt;br /&gt; But whatever, yolo.&lt;/p&gt; &lt;p&gt;I’m the creator of Yacana, a free and open source multi-agent framework.&lt;br /&gt; I’ve spent more than a year working late nights on it, &lt;strong&gt;thinking that if the software was good, people would naturally show up.&lt;/strong&gt;&lt;br /&gt; Turns out… not really.&lt;/p&gt; &lt;h1&gt;How it started&lt;/h1&gt; &lt;p&gt;Back when local LLMs first became usable, there was no proper tool calling.&lt;br /&gt; That made it nearly impossible to build anything useful on top of them.&lt;/p&gt; &lt;p&gt;So I started writing a framework to fix that. That’s how Yacana began. Its main goal was to let LLMs call tools automatically.&lt;br /&gt; Around the same time, LangChain released a buggy &amp;quot;function calling&amp;quot; thing for Ollama, but it still wasn’t real tool calling. You had to handle everything manually.&lt;/p&gt; &lt;p&gt;That’s why I can confidently say Yacana was the first official framework to actually make it work.&lt;/p&gt; &lt;p&gt;I dare to say &amp;quot;official&amp;quot; because roughly at the same time it got added to the Ollama Github's main page which I thought would be enough to attract some users.&lt;/p&gt; &lt;p&gt;Spoiler: it wasn’t.&lt;/p&gt; &lt;h1&gt;How it went&lt;/h1&gt; &lt;p&gt;As time passed, tool calling became standard across the board.&lt;br /&gt; Everyone started using the OpenAI-style syntax.&lt;br /&gt; Yacana followed that path too but also kept its original tool calling mechanism.&lt;/p&gt; &lt;p&gt;I added a ton of stuff since then: checkpoints, history management, state saving, VLLM support, thinking model support, streaming, structured outputs, and so on.&lt;br /&gt; And still… almost no feedback.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The GitHub stars and PyPI downloads? Let’s just say they’re modest.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Then came MCP, which looked like the next big standard.&lt;br /&gt; I added support for MCP tools, staying true to Yacana’s simple OOP API (unlike LangChain’s tangle of abstractions).&lt;br /&gt; Still no big change.&lt;/p&gt; &lt;h1&gt;Self-reflection time&lt;/h1&gt; &lt;p&gt;At one point, I thought maybe I just needed to advertized some more. &lt;/p&gt; &lt;p&gt;But I hesitated.&lt;br /&gt; There were already so many &amp;quot;agentic&amp;quot; frameworks popping up...&lt;br /&gt; I started wondering if I was just fooling myself.&lt;br /&gt; Was Yacana really good enough to deserve a small spotlight?&lt;br /&gt; Was I just promoting something that wasn’t as advanced as the competition?&lt;/p&gt; &lt;p&gt;Maybe.&lt;/p&gt; &lt;p&gt;And yet, I kept thinking that it deserved a bit more.&lt;br /&gt; There aren’t that many frameworks out there that are both independent (not backed by a company ~Strands~) and actually documented (sorry, LangChain).&lt;/p&gt; &lt;h1&gt;Meanwhile, in AI-land...&lt;/h1&gt; &lt;p&gt;Fast forward to today. It’s been 1 year and ~4 months.&lt;br /&gt; Yacana sits at around 60+ GitHub stars.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Meanwhile, random fake AI projects get thousands of stars.&lt;/strong&gt;&lt;br /&gt; Some of them aren’t even real, just flashy demos or vaporware.&lt;br /&gt; Sometimes I genuinely wonder if there are bots starring repos to make them look more popular.&lt;br /&gt; Like some invisible puppeteer trying to shape developers attention.&lt;/p&gt; &lt;h1&gt;A little sting&lt;/h1&gt; &lt;p&gt;Recently I was reading through LangChain’s docs and saw they had a &amp;quot;checkpoints&amp;quot; feature.&lt;br /&gt; Not gonna lie, that one stung a bit.&lt;br /&gt; It wasn’t the first time I stumbled upon a Yacana feature that had been implemented elsewhere.&lt;br /&gt; What hurts is that Yacana’s features weren’t copied from other frameworks, they were &lt;strong&gt;invented&lt;/strong&gt;.&lt;br /&gt; And seeing them appear somewhere else kind of proves that I might actually be good at what I do. But the fact that so few people seem to care about my work just reinforces the feeling that maybe I’m doing all of this for nothing.&lt;/p&gt; &lt;h1&gt;My honest take&lt;/h1&gt; &lt;p&gt;I don’t think agentic frameworks are a revolution.&lt;br /&gt; The real revolution is the LLMs themselves.&lt;br /&gt; Frameworks like Yacana (or LangChain, CrewAI, etc.) are mostly structured wrappers around POST requests to an inference server.&lt;/p&gt; &lt;p&gt;Still, Yacana has a purpose.&lt;br /&gt; It’s simple, lightweight, easy to learn, and can work with models that aren’t fine-tuned for function calling.&lt;br /&gt; It’s great for people who don't want to invest 100+ hours in Langchain. Not saying that Langchain isn't worth it, but it's not always needed depending on the problem to solve.&lt;/p&gt; &lt;h1&gt;Where things stand&lt;/h1&gt; &lt;p&gt;So why isn’t it catching on?&lt;br /&gt; I am still unsure.&lt;/p&gt; &lt;p&gt;I’ve written detailed docs, made examples, and even started recording video tutorials.&lt;br /&gt; The problem doesn’t seem to be the learning curve.&lt;br /&gt; Maybe it still lacks something, like native RAG support. But after having followed the hype curve for more than a year, I’ve realized there’s probably more to it than just features.&lt;/p&gt; &lt;p&gt;I’ll keep updating Yacana regardless.&lt;br /&gt; I just think it deserves a (tiny) bit more visibility.&lt;br /&gt; Not because it’s revolutionary, but because it’s real.&lt;/p&gt; &lt;p&gt;And maybe that should count for something.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Github:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/rememberSoftwares/yacana"&gt;https://github.com/rememberSoftwares/yacana&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Documentation:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://remembersoftwares.github.io/yacana"&gt;https://remembersoftwares.github.io/yacana&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DocteurW"&gt; /u/DocteurW &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otislj/after_a_year_building_an_opensource_ai_framework/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otislj/after_a_year_building_an_opensource_ai_framework/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otislj/after_a_year_building_an_opensource_ai_framework/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T16:44:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot80p0</id>
    <title>Is it too early for local LLMs?</title>
    <updated>2025-11-10T07:58:46+00:00</updated>
    <author>
      <name>/u/Substantial_Mode_167</name>
      <uri>https://old.reddit.com/user/Substantial_Mode_167</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been thinking for a while about setting up a local environment for running an LLM. Since I was already planning to build a gaming PC, I saw it as a good opportunity to tweak the setup so I could also use AI tools locally, I use them quite a lot.&lt;/p&gt; &lt;p&gt;But after looking into the market, it really feels like it’s still too early. Everything is overpriced, full of compromises, or the few uncompromising options cost an absurd amount. It just doesn’t seem worth it yet. I feel like we’ll need to wait another couple of years before running an LLM locally becomes truly viable for most people.&lt;/p&gt; &lt;p&gt;Of course, it depends on your use case and budget, but I think only a few can realistically justify or get a real return on such an investment right now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Mode_167"&gt; /u/Substantial_Mode_167 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot80p0/is_it_too_early_for_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot80p0/is_it_too_early_for_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot80p0/is_it_too_early_for_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T07:58:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ottfbi</id>
    <title>What I learned from stress testing LLM on NPU vs CPU on a phone</title>
    <updated>2025-11-10T23:19:45+00:00</updated>
    <author>
      <name>/u/Material_Shopping496</name>
      <uri>https://old.reddit.com/user/Material_Shopping496</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We ran a 10-minute LLM stress test on Samsung S25 Ultra CPU vs Qualcomm Hexagon NPU to see how the same model (LFM2-1.2B, 4 Bit quantization) performed. And I wanted to share some test results here for anyone interested in real on-device performance data. &lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ottfbi/video/00ha3zfcgi0g1/player"&gt;https://reddit.com/link/1ottfbi/video/00ha3zfcgi0g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In 3 minutes, the CPU hit 42 °C and throttled: throughput fell from ~37 t/s → ~19 t/s.&lt;/p&gt; &lt;p&gt;The NPU stayed cooler (36–38 °C) and held a steady ~90 t/s—2–4× faster than CPU under load.&lt;/p&gt; &lt;p&gt;Same 10-min, both used 6% battery, but productivity wasn’t equal:&lt;/p&gt; &lt;p&gt;NPU: ~54k tokens → ~9,000 tokens per 1% battery&lt;/p&gt; &lt;p&gt;CPU: ~14.7k tokens → ~2,443 tokens per 1% battery&lt;/p&gt; &lt;p&gt;That’s ~3.7× more work per battery on the NPU—without throttling.&lt;/p&gt; &lt;p&gt;(Setup: S25 Ultra, LFM2-1.2B, Inference using Nexa Android SDK) &lt;/p&gt; &lt;p&gt;To recreate the test, I used Nexa Android SDK to run the latest models on NPU and CPU：&lt;a href="https://github.com/NexaAI/nexa-sdk/tree/main/bindings/android"&gt;https://github.com/NexaAI/nexa-sdk/tree/main/bindings/android&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What other NPU vs CPU benchmarks are you interested in? Would love to hear your thoughts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Material_Shopping496"&gt; /u/Material_Shopping496 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ottfbi/what_i_learned_from_stress_testing_llm_on_npu_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ottfbi/what_i_learned_from_stress_testing_llm_on_npu_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ottfbi/what_i_learned_from_stress_testing_llm_on_npu_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T23:19:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1otreir</id>
    <title>Imagine you’re stuck with one local model forever: GPT-OSS 120B or GLM 4.5 Air. Which one are you picking and why?</title>
    <updated>2025-11-10T22:00:11+00:00</updated>
    <author>
      <name>/u/Adventurous-Gold6413</name>
      <uri>https://old.reddit.com/user/Adventurous-Gold6413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Gold6413"&gt; /u/Adventurous-Gold6413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otreir/imagine_youre_stuck_with_one_local_model_forever/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otreir/imagine_youre_stuck_with_one_local_model_forever/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otreir/imagine_youre_stuck_with_one_local_model_forever/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T22:00:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1oty5a9</id>
    <title>Realtime video analysis with Moondream</title>
    <updated>2025-11-11T02:50:19+00:00</updated>
    <author>
      <name>/u/radiiquark</name>
      <uri>https://old.reddit.com/user/radiiquark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oty5a9/realtime_video_analysis_with_moondream/"&gt; &lt;img alt="Realtime video analysis with Moondream" src="https://external-preview.redd.it/ZGZqb20yZHBrajBnMRQH5Ip-LXWUY-NVe752F9-1VFqZvu8plOUVm68qhzC0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=17f2b9159f6d23bd4df70a756251c6b0a9849903" title="Realtime video analysis with Moondream" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Live demo (no login required): &lt;a href="https://moondream.ai/solutions/analyze-live-video"&gt;https://moondream.ai/solutions/analyze-live-video&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/m87-labs/Analyze-Live-Video-Solution"&gt;https://github.com/m87-labs/Analyze-Live-Video-Solution&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/radiiquark"&gt; /u/radiiquark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/norsa3dpkj0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oty5a9/realtime_video_analysis_with_moondream/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oty5a9/realtime_video_analysis_with_moondream/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T02:50:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1otwdq0</id>
    <title>[Update] mlx-knife 2.0 stable — MLX model manager for Apple Silicon</title>
    <updated>2025-11-11T01:28:32+00:00</updated>
    <author>
      <name>/u/broke_team</name>
      <uri>https://old.reddit.com/user/broke_team</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Posted here in August, now hitting 2.0 stable.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt; CLI for managing HuggingFace MLX models on Mac. Like ollama but for MLX.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's new in 2.0:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;JSON API for automation (--json on all commands)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Runtime compatibility checks (catches broken models upfront)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Proper exit codes for scripting&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Fixed stop token handling (no more visible &amp;amp;lt;|end|&amp;amp;gt; tokens)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Structured logging&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Install:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt; pip install mlx-knife &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Basic usage:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;```&lt;br /&gt; mlxk list # Show cached models&lt;br /&gt; mlxk pull mlx-community/Llama-3.3-70B-Instruct-4bit # Download&lt;br /&gt; mlxk run Llama-3.3-70B # Interactive chat&lt;br /&gt; mlxk server # OpenAI-compatible API server&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Experimental:&lt;/strong&gt; Testing mlxk clone (APFS CoW) and mlxk push (HF uploads). Feedback welcome.&lt;/p&gt; &lt;p&gt;Python 3.9-3.13, M1/M2/M3/M4.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mzau/mlx-knife"&gt;https://github.com/mzau/mlx-knife&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/broke_team"&gt; /u/broke_team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otwdq0/update_mlxknife_20_stable_mlx_model_manager_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otwdq0/update_mlxknife_20_stable_mlx_model_manager_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otwdq0/update_mlxknife_20_stable_mlx_model_manager_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T01:28:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot6k56</id>
    <title>Kimi infra team: Quantization is not a compromise, it's the next paradigm</title>
    <updated>2025-11-10T06:25:55+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After K2-Thinking's release, many developers have been curious about its native INT4 quantization format.&lt;/p&gt; &lt;p&gt;Shaowei Liu, &lt;strong&gt;infra engineer&lt;/strong&gt; at &lt;a href="/u/Kimi-Moonshot"&gt;u/Kimi-Moonshot&lt;/a&gt; shares an insider's view on why this choice matters, and why quantization today isn't just about sacrificing precision for speed.&lt;/p&gt; &lt;h1&gt;Key idea&lt;/h1&gt; &lt;p&gt;In the context of LLMs, &lt;strong&gt;quantization is no longer a trade-off&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;With the evolution of param-scaling and test-time-scaling, native low-bit quantization will become a standard paradigm for large model training.&lt;/p&gt; &lt;h1&gt;Why Low-bit Quantization Matters&lt;/h1&gt; &lt;p&gt;In modern LLM inference, there are two distinct optimization goals:&lt;/p&gt; &lt;p&gt;• &lt;strong&gt;High throughput&lt;/strong&gt; (cost-oriented): maximize GPU utilization via large batch sizes.&lt;/p&gt; &lt;p&gt;• &lt;strong&gt;Low latency&lt;/strong&gt; (user-oriented): minimize per-query response time.&lt;/p&gt; &lt;p&gt;For Kimi-K2's MoE structure (with &lt;strong&gt;1/48 sparsity&lt;/strong&gt;), &lt;strong&gt;decoding is memory-bound&lt;/strong&gt; — the smaller the model weights, the faster the compute.&lt;/p&gt; &lt;p&gt;FP8 weights (≈1 TB) already hit the limit of what a single high-speed interconnect GPU node can handle.&lt;/p&gt; &lt;p&gt;By switching to W4A16, latency drops sharply while maintaining quality — a perfect fit for low-latency inference.&lt;/p&gt; &lt;h1&gt;Why QAT over PTQ&lt;/h1&gt; &lt;p&gt;Post-training quantization (PTQ) worked well for shorter generations, but &lt;strong&gt;failed in longer reasoning chains&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;• Error accumulation during long decoding degraded precision.&lt;/p&gt; &lt;p&gt;• Dependence on calibration data caused &amp;quot;expert distortion&amp;quot; in sparse MoE layers.&lt;/p&gt; &lt;p&gt;Thus, K2-Thinking adopted QAT for &lt;strong&gt;minimal loss&lt;/strong&gt; and &lt;strong&gt;more stable long-context reasoning&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;How it works&lt;/h1&gt; &lt;p&gt;K2-Thinking uses a &lt;strong&gt;weight-only QAT&lt;/strong&gt; with &lt;strong&gt;fake quantization + STE (straight-through estimator)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The pipeline was fully integrated in just days — from QAT training → INT4 inference → RL rollout — enabling near lossless results without extra tokens or retraining.&lt;/p&gt; &lt;h1&gt;INT4's hidden advantage in RL&lt;/h1&gt; &lt;p&gt;Few people mention this: &lt;strong&gt;native INT4&lt;/strong&gt; doesn't just speed up inference — it &lt;strong&gt;accelerates RL training&lt;/strong&gt; itself.&lt;/p&gt; &lt;p&gt;Because RL rollouts often suffer from &amp;quot;long-tail&amp;quot; inefficiency, INT4's low-latency profile makes those stages much faster.&lt;/p&gt; &lt;p&gt;In practice, each RL iteration runs &lt;strong&gt;10-20% faster end-to-end.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Moreover, quantized RL brings stability: smaller representational space reduces accumulation error, improving learning robustness.&lt;/p&gt; &lt;h1&gt;Why INT4, not MXFP4&lt;/h1&gt; &lt;p&gt;Kimi chose INT4 over &amp;quot;fancier&amp;quot; MXFP4/NVFP4 to better support &lt;strong&gt;non-Blackwell GPUs&lt;/strong&gt;, with strong existing kernel support (e.g., Marlin).&lt;/p&gt; &lt;p&gt;At a quant scale of 1×32, INT4 matches FP4 formats in expressiveness while being more hardware-adaptable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot6k56/kimi_infra_team_quantization_is_not_a_compromise/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot6k56/kimi_infra_team_quantization_is_not_a_compromise/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot6k56/kimi_infra_team_quantization_is_not_a_compromise/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T06:25:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1otxs37</id>
    <title>Our sub got a shout-out from the Corridor Crew</title>
    <updated>2025-11-11T02:33:11+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otxs37/our_sub_got_a_shoutout_from_the_corridor_crew/"&gt; &lt;img alt="Our sub got a shout-out from the Corridor Crew" src="https://external-preview.redd.it/MG5qbWE3OXZoajBnMfJFc8SM8imSZJpbD6BkmsMZ2u1jbLaP-XMJEPc_yiXX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=269b48a90a4a2de8bb79dd262a86c54e29a97ed9" title="Our sub got a shout-out from the Corridor Crew" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From their recent video &lt;a href="https://youtu.be/6hI9T4jnrSI?si=h7An0736C93hs7YO"&gt;AI Experts Debunk The Latest SLOP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/10yfbe8vhj0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otxs37/our_sub_got_a_shoutout_from_the_corridor_crew/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otxs37/our_sub_got_a_shoutout_from_the_corridor_crew/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T02:33:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1otdr19</id>
    <title>What is the best hardware under 10k to run local big models with over 200b parameters?</title>
    <updated>2025-11-10T13:28:39+00:00</updated>
    <author>
      <name>/u/nadiemeparaestavez</name>
      <uri>https://old.reddit.com/user/nadiemeparaestavez</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I'm looking to build an AI rig that can run these big models for coding purposes, but also as a hobby.&lt;/p&gt; &lt;p&gt;I have been playing around with a 3090 I had for gaming, but I'm interested in running bigger models. So far my options seem:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Upgrade motherboard/psu/case and get another 3090/4090, total 42gb vram, 128gb ram, and a server-cpu to support more channels.&lt;/li&gt; &lt;li&gt;Buy a mac studio with m3 ultra.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My questions are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Would a mixed ram/vram setup like 1 be slower than the m3 when running 230b models? What about models like minimax m2 which use MoE? Would those run much faster on the gpu+ram approach?&lt;/li&gt; &lt;li&gt;Is there any other sensible option to get huge amounts of ram/vram and enough performance for inference on 1 user without going over 10k?&lt;/li&gt; &lt;li&gt;Would it be worth it to go for a mix of 1 3090 and 1 5090? Or would the 5090 just be bottle necked waiting for the 3090?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I'm in no rush, I'm starting to save up to buy something in a few months, but I want to understand what direction should I go for. If something like option 1 was the best idea I might upgrade little by little from my current setup.&lt;/p&gt; &lt;p&gt;Short term I will use this to refactor codebases, coding features, etc. I don't mind if it runs slow, but I need to be able to run thinking/high quality models that can follow long processes (like splitting big tasks into smaller ones, and following procedures). But long term I just want to learn and experiment, so anything that can actually run big models would be good enough, even if slow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nadiemeparaestavez"&gt; /u/nadiemeparaestavez &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otdr19/what_is_the_best_hardware_under_10k_to_run_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otdr19/what_is_the_best_hardware_under_10k_to_run_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otdr19/what_is_the_best_hardware_under_10k_to_run_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T13:28:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1otwk39</id>
    <title>AI Black&amp;Blonde for a 230% boost on inference speed</title>
    <updated>2025-11-11T01:36:39+00:00</updated>
    <author>
      <name>/u/OldEffective9726</name>
      <uri>https://old.reddit.com/user/OldEffective9726</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otwk39/ai_blackblonde_for_a_230_boost_on_inference_speed/"&gt; &lt;img alt="AI Black&amp;amp;Blonde for a 230% boost on inference speed" src="https://b.thumbs.redditmedia.com/O0FYna7i2U2Capgt_qA5NPup2uouLdNiqcu-nLZn8uY.jpg" title="AI Black&amp;amp;Blonde for a 230% boost on inference speed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;R9700 AI Pro had only 32 GB Vram ddr6 that limits its ability to run locally LLM at Q8 precision due to large overall model size.&lt;/p&gt; &lt;p&gt;Paired it with an RTX 5060 8GB vram ddr7 from my girlfriend's gaming PC and got a 230% boost. 4k context window partial offloading: the inference speed was 6.39 tps with AMD only vs. 14.81 tps with AMD&amp;amp;nvidia 100% GPU offloading for a 15k context window. Vulkan engine for both cards use command (below) so the 5060 is compute-only and the monitor is connected to R9700. Qwen 3 32B Q8 precision. 100% GPU offloading and 15k context window when using the Black&amp;amp;Blonde.&lt;/p&gt; &lt;p&gt;Just plugged and played - no special setup but you will need to install both AMD and nvidia-580-open drivers. AMD is the display driver.&lt;/p&gt; &lt;p&gt;# Set NVIDIA GPU to compute-exclusive mode (no display)&lt;/p&gt; &lt;p&gt;sudo nvidia-smi -c EXCLUSIVE_PROCESS&lt;/p&gt; &lt;p&gt;# Or set to compute mode (allows display but prioritizes compute)&lt;/p&gt; &lt;p&gt;sudo nvidia-smi -c DEFAULT&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OldEffective9726"&gt; /u/OldEffective9726 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1otwk39"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otwk39/ai_blackblonde_for_a_230_boost_on_inference_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otwk39/ai_blackblonde_for_a_230_boost_on_inference_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T01:36:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1otx50l</id>
    <title>Is open-webui vibe coded? Why else is the documentation littered with emoji?</title>
    <updated>2025-11-11T02:03:13+00:00</updated>
    <author>
      <name>/u/ksoops</name>
      <uri>https://old.reddit.com/user/ksoops</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's like every other 5 words: an emoji.&lt;/p&gt; &lt;p&gt;God damn, the future is bleak&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ksoops"&gt; /u/ksoops &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otx50l/is_openwebui_vibe_coded_why_else_is_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otx50l/is_openwebui_vibe_coded_why_else_is_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otx50l/is_openwebui_vibe_coded_why_else_is_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T02:03:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1otu6ez</id>
    <title>Hi reddit, I rebuilt Karpathy's Nanochat in pure Rust [nanochat-rs]</title>
    <updated>2025-11-10T23:51:18+00:00</updated>
    <author>
      <name>/u/Exciting-Camera3226</name>
      <uri>https://old.reddit.com/user/Exciting-Camera3226</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The repo is at: &lt;a href="https://github.com/AntigmaLabs/nanochat-rs"&gt;https://github.com/AntigmaLabs/nanochat-rs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The goal to provide the community with a reference implementation in a different language and possibly a clean nice little hackable cognitive core that is easier to understand and deploy(without the python weak types and heavy pytorch dependencies) &lt;/p&gt; &lt;p&gt;Main features &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Native rust&lt;/li&gt; &lt;li&gt;Integration with HuggingFace&lt;/li&gt; &lt;li&gt;Centralized model loader resilient to tensor name changes&lt;/li&gt; &lt;li&gt;Minimal surface area to keep cognitive load low (not product-grade)&lt;/li&gt; &lt;li&gt;Compatible with tiktoken &lt;code&gt;.pkl&lt;/code&gt; tokenizer configs&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Exciting-Camera3226"&gt; /u/Exciting-Camera3226 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otu6ez/hi_reddit_i_rebuilt_karpathys_nanochat_in_pure/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otu6ez/hi_reddit_i_rebuilt_karpathys_nanochat_in_pure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otu6ez/hi_reddit_i_rebuilt_karpathys_nanochat_in_pure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T23:51:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot3lxv</id>
    <title>I tested Strix Halo clustering w/ ~50Gig IB to see if networking is really the bottleneck</title>
    <updated>2025-11-10T03:42:05+00:00</updated>
    <author>
      <name>/u/Hungry_Elk_3276</name>
      <uri>https://old.reddit.com/user/Hungry_Elk_3276</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot3lxv/i_tested_strix_halo_clustering_w_50gig_ib_to_see/"&gt; &lt;img alt="I tested Strix Halo clustering w/ ~50Gig IB to see if networking is really the bottleneck" src="https://preview.redd.it/ezjtolwnoc0g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d9f058ee27bdab9923ee3d40ab306fea5558c71" title="I tested Strix Halo clustering w/ ~50Gig IB to see if networking is really the bottleneck" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; While InfiniBand is cool, 10 Gbps Thunderbolt is sufficient for llama.cpp.&lt;/p&gt; &lt;p&gt;Recently I got really fascinated by clustering with Strix Halo to get a potential 200 GB of VRAM without significant costs. I'm currently using a 4x4090 solution for research, but it's very loud and power-hungry (plus it doesn't make much sense for normal 1-2 user inference—this machine is primarily used for batch generation for research purposes). I wanted to look for a low-power but efficient way to inference ~230B models at Q4. And here we go.&lt;/p&gt; &lt;p&gt;I always had this question of how exactly networking would affect the performance. So I got two modded Mellanox ConnectX-5 Ex 100 Gig NICs which I had some experience with on NCCL. These cards are very cool with reasonable prices and are quite capable. However, due to the Strix Halo platform limitation, I only got a PCIe 4.0 x4 link. But I was still able to get around 6700 MB/s or roughly 55 Gbps networking between the nodes, which is far better than using IP over Thunderbolt (10 Gbps).&lt;/p&gt; &lt;p&gt;I tried using vLLM first and quickly found out that RCCL is not supported on Strix Halo. :( Then I tried using llama.cpp RPC mode with the &lt;code&gt;-c&lt;/code&gt; flag to enable caching, and here are the results I got:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test Type&lt;/th&gt; &lt;th align="left"&gt;Single Machine w/o rpc&lt;/th&gt; &lt;th align="left"&gt;2.5 Gbps&lt;/th&gt; &lt;th align="left"&gt;10 Gbps (TB)&lt;/th&gt; &lt;th align="left"&gt;50 Gbps&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;pp512&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;653.74&lt;/td&gt; &lt;td align="left"&gt;603.00&lt;/td&gt; &lt;td align="left"&gt;654.03&lt;/td&gt; &lt;td align="left"&gt;663.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;tg128&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;49.73&lt;/td&gt; &lt;td align="left"&gt;30.98&lt;/td&gt; &lt;td align="left"&gt;36.44&lt;/td&gt; &lt;td align="left"&gt;35.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;tg512&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;47.54&lt;/td&gt; &lt;td align="left"&gt;29.13&lt;/td&gt; &lt;td align="left"&gt;35.07&lt;/td&gt; &lt;td align="left"&gt;34.30&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;pp512 @ d512&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;601.75&lt;/td&gt; &lt;td align="left"&gt;554.17&lt;/td&gt; &lt;td align="left"&gt;599.76&lt;/td&gt; &lt;td align="left"&gt;611.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;tg128 @ d512&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;45.81&lt;/td&gt; &lt;td align="left"&gt;27.78&lt;/td&gt; &lt;td align="left"&gt;33.88&lt;/td&gt; &lt;td align="left"&gt;32.67&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;tg512 @ d512&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;44.90&lt;/td&gt; &lt;td align="left"&gt;27.14&lt;/td&gt; &lt;td align="left"&gt;31.33&lt;/td&gt; &lt;td align="left"&gt;32.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;pp512 @ d2048&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;519.40&lt;/td&gt; &lt;td align="left"&gt;485.93&lt;/td&gt; &lt;td align="left"&gt;528.52&lt;/td&gt; &lt;td align="left"&gt;537.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;tg128 @ d2048&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;41.84&lt;/td&gt; &lt;td align="left"&gt;25.34&lt;/td&gt; &lt;td align="left"&gt;31.22&lt;/td&gt; &lt;td align="left"&gt;30.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;tg512 @ d2048&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;41.33&lt;/td&gt; &lt;td align="left"&gt;25.01&lt;/td&gt; &lt;td align="left"&gt;30.66&lt;/td&gt; &lt;td align="left"&gt;30.11&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As you can see, the Thunderbolt connection almost matches the 50 Gbps MLX5 on token generation. Compared to the non-RPC single node inference, the performance difference is still quite substantial—with about a 15 token/s difference—but as the context lengthens, the text generation difference somehow gets smaller and smaller. Another strange thing is that somehow the prompt processing is better on RPC over 50 Gbps, even better than the single machine. That's very interesting to see.&lt;/p&gt; &lt;p&gt;During inference, I observed that the network was never used at more than maybe ~100 Mbps or 10 MB/s most of the time, suggesting the gain might not come from bandwidth—maybe latency? But I don't have a way to prove what exactly is affecting the performance gain from 2.5 Gbps to 10 Gbps IP over Thunderbolt.&lt;/p&gt; &lt;p&gt;Here is the llama-bench command I'm using:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-bench -m ./gpt-oss-120b-mxfp4-00001-of-00003.gguf -d 0,512,2048 -n 128,512 -o md --rpc &amp;lt;IP:PORT&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So the result is pretty clear: you don't need a fancy IB card to gain usable results on llama.cpp with Strix Halo. At least until RCCL supports Strix Halo, I think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hungry_Elk_3276"&gt; /u/Hungry_Elk_3276 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ezjtolwnoc0g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot3lxv/i_tested_strix_halo_clustering_w_50gig_ib_to_see/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot3lxv/i_tested_strix_halo_clustering_w_50gig_ib_to_see/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T03:42:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1otpql6</id>
    <title>LLM-driven puzzle sandbox: anything you try becomes an action (Cosmic Egg)</title>
    <updated>2025-11-10T20:56:23+00:00</updated>
    <author>
      <name>/u/VirtualJamesHarrison</name>
      <uri>https://old.reddit.com/user/VirtualJamesHarrison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otpql6/llmdriven_puzzle_sandbox_anything_you_try_becomes/"&gt; &lt;img alt="LLM-driven puzzle sandbox: anything you try becomes an action (Cosmic Egg)" src="https://external-preview.redd.it/dHZxczAzbTN0aDBnMTBAWoGHzmzPlCXmWH6RtU6SjIImLDmcCL43zhjlQgdI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb4fbdff352a105ed4a910befbf1d1449947eb83" title="LLM-driven puzzle sandbox: anything you try becomes an action (Cosmic Egg)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re using LLMs to generate actions in our upcoming puzzle game Cosmic Egg—so “anything you can think of” becomes a validated, in-world interaction.&lt;/p&gt; &lt;p&gt;The system works with local LLMs + smart caching + a bit of game-dev smoke &amp;amp; mirrors—while keeping the game deterministic so everyone shares a common action pool and outcomes are reproducible.&lt;/p&gt; &lt;p&gt;Still lots to do, right now we’re improving sprite generation and adding player inventory &amp;amp; items.&lt;/p&gt; &lt;p&gt;Feedback very welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VirtualJamesHarrison"&gt; /u/VirtualJamesHarrison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6i40e2m3th0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otpql6/llmdriven_puzzle_sandbox_anything_you_try_becomes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otpql6/llmdriven_puzzle_sandbox_anything_you_try_becomes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T20:56:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1otmamz</id>
    <title>When does RTX 6000 Pro make sense over a 5090?</title>
    <updated>2025-11-10T18:49:40+00:00</updated>
    <author>
      <name>/u/Herald_Of_Rivia</name>
      <uri>https://old.reddit.com/user/Herald_Of_Rivia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;Hey all—trying to sanity-check an upgrade. Current GPU: RTX 5090 Use cases: training mid-size LLMs, Stable Diffusion/ComfyUI, inferencing GPT-OSS-120B / GLM 4.5 Air Rig: 9950X3D / 96GB DDR5 / 1500W Corsair H1500i • OS: Win11 / Ubuntu 24.04 I’m eyeing the RTX 6000 Pro (Blackwell) mainly for: * More VRAM/ECC * Potential tensor/FP improvements for AI workloads Questions for folks who’ve used the 6000 Pro vs the RXT 5090: * In real projects, what speed/throughput gains did you see for general AI workload? * Did ECC + pro drivers measurably reduce crashes/corruption vs 5090? * Any gotchas (thermals, power, coil whine, chassis fit, Linux/Windows quirks, NVLink/virtualization)? * If you switched back, why? If my workloads are mainly for LLM inference / small training and SD, is the upgrade worth it, or is 5090 still the best value? Benchmarks and anecdotes welcome! Thanks. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Herald_Of_Rivia"&gt; /u/Herald_Of_Rivia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otmamz/when_does_rtx_6000_pro_make_sense_over_a_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otmamz/when_does_rtx_6000_pro_make_sense_over_a_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otmamz/when_does_rtx_6000_pro_make_sense_over_a_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T18:49:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1otnj2k</id>
    <title>Are any of you using local llms for "real" work?</title>
    <updated>2025-11-10T19:34:39+00:00</updated>
    <author>
      <name>/u/hmsenterprise</name>
      <uri>https://old.reddit.com/user/hmsenterprise</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am having fun personally tinkering with local models and workflows and such, but sometimes it feels like we're all still stuck in the &amp;quot;fun experimentation&amp;quot; phase with local LLMs and not actually producing any &amp;quot;production grade&amp;quot; outputs or using it in real workflows. &lt;/p&gt; &lt;p&gt;Idk if it's just the gap between what &amp;quot;personal&amp;quot; LLM-capable rigs can handle vs the compute needs of current best-in-class models or what.&lt;/p&gt; &lt;p&gt;Am I wrong here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hmsenterprise"&gt; /u/hmsenterprise &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otnj2k/are_any_of_you_using_local_llms_for_real_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otnj2k/are_any_of_you_using_local_llms_for_real_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otnj2k/are_any_of_you_using_local_llms_for_real_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T19:34:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1otwek3</id>
    <title>Full Replication of Google's Nested Learning Paper in PyTorch – code now live</title>
    <updated>2025-11-11T01:29:37+00:00</updated>
    <author>
      <name>/u/complains_constantly</name>
      <uri>https://old.reddit.com/user/complains_constantly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some of you may have seen Google Research’s &lt;a href="https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/"&gt;&lt;strong&gt;Nested Learning paper&lt;/strong&gt;&lt;/a&gt;. They introduced HOPE, a self-modifying TITAN variant with a Continuum Memory System (multi-frequency FFN chain) + deep optimizer stack. They published the research but no code (like always), so I rebuilt the architecture and infra in PyTorch over the weekend.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/kmccleary3301/nested_learning"&gt;https://github.com/kmccleary3301/nested_learning&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Highlights&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Level clock + CMS implementation (update-period gating, associative-memory optimizers).&lt;/li&gt; &lt;li&gt;HOPE block w/ attention, TITAN memory, self-modifier pathway.&lt;/li&gt; &lt;li&gt;Hydra configs for pilot/mid/target scales, uv-managed env, Deepspeed/FSDP launchers.&lt;/li&gt; &lt;li&gt;Data pipeline: filtered RefinedWeb + supplements (C4, RedPajama, code) with tokenizer/sharding scripts.&lt;/li&gt; &lt;li&gt;Evaluation: zero-shot harness covering PIQA, HellaSwag, WinoGrande, ARC-E/C, BoolQ, SIQA, CommonsenseQA, OpenBookQA + NIAH long-context script.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;What I need help with:&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Running larger training configs (760M+, 4–8k context) and reporting W&amp;amp;B benchmarks.&lt;/li&gt; &lt;li&gt;Stress-testing CMS/self-modifier stability + alternative attention backbones.&lt;/li&gt; &lt;li&gt;Continual-learning evaluation (streaming domains) &amp;amp; regression tests.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you try it, please file issues/PRs—especially around stability tricks, data pipelines, or eval scripts. Would love to see how it stacks up against these Qwen, DeepSeek, Minimax, and Kimi architectures.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/complains_constantly"&gt; /u/complains_constantly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otwek3/full_replication_of_googles_nested_learning_paper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otwek3/full_replication_of_googles_nested_learning_paper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otwek3/full_replication_of_googles_nested_learning_paper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T01:29:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ottmjb</id>
    <title>Meta drops new ASR models (up to 7B)</title>
    <updated>2025-11-10T23:28:10+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta just released a new kind of ASR models that are particularly useful to transcribe languages for which little training data is available.&lt;/p&gt; &lt;p&gt;Most interestingly, they seem to have implemented something like audio context, where you can provide some audio and the correct transcriptions and use that to improve ASR without needing a full fine-tune. It appears that the audio needed for this is very much doable without large scale transcription efforts you would normally have to do to run a fine-tune.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/facebookresearch/omnilingual-asr"&gt;https://github.com/facebookresearch/omnilingual-asr&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ottmjb/meta_drops_new_asr_models_up_to_7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ottmjb/meta_drops_new_asr_models_up_to_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ottmjb/meta_drops_new_asr_models_up_to_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T23:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1otj99f</id>
    <title>LinkedIn now tells you when you're looking at an AI-generated image, if you haven't noticed.</title>
    <updated>2025-11-10T17:01:12+00:00</updated>
    <author>
      <name>/u/MarketingNetMind</name>
      <uri>https://old.reddit.com/user/MarketingNetMind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otj99f/linkedin_now_tells_you_when_youre_looking_at_an/"&gt; &lt;img alt="LinkedIn now tells you when you're looking at an AI-generated image, if you haven't noticed." src="https://preview.redd.it/bl396lgsng0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d06e52cf2855e25cb75bab6e7f8d9e9a70cccd3" title="LinkedIn now tells you when you're looking at an AI-generated image, if you haven't noticed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the 1st image shows, the C2PA label is used.&lt;/p&gt; &lt;p&gt;Here's what's interesting.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The feature only applies to image platforms who join the C2PA.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Now there's only:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ChatGPT/DALL-E 3 images&lt;/li&gt; &lt;li&gt;Adobe Firefly images&lt;/li&gt; &lt;li&gt;Leica Camera images&lt;/li&gt; &lt;li&gt;BBC news images&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The 2nd image, generated by &lt;a href="https://www.netmind.ai/modelsLibrary/nano-banana"&gt;Google's Nano Banana&lt;/a&gt;, does not have the label.&lt;/p&gt; &lt;p&gt;What's even more interesting?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;It's easy to bypass this new rule.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;You just need to upload the screenshot of the AI-generated pic, as we did with the 3rd image, a screenshot of the 1st one.&lt;/p&gt; &lt;p&gt;Do you think more AI image platforms, like Google, will join C2PA?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarketingNetMind"&gt; /u/MarketingNetMind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bl396lgsng0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otj99f/linkedin_now_tells_you_when_youre_looking_at_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otj99f/linkedin_now_tells_you_when_youre_looking_at_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T17:01:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot95gj</id>
    <title>Qwen3-VL's perceptiveness is incredible.</title>
    <updated>2025-11-10T09:12:28+00:00</updated>
    <author>
      <name>/u/Trypocopris</name>
      <uri>https://old.reddit.com/user/Trypocopris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.imgur.com/liqVUJd.jpeg"&gt;I took a 4k image and scattered around 6 medium-length words.&lt;/a&gt; &lt;/p&gt; &lt;p&gt;With &lt;code&gt;Qwen3-VL-8B-Instruct-GGUF&lt;/code&gt; and a temperature of &lt;code&gt;0&lt;/code&gt;, an image token count of &lt;code&gt;2300&lt;/code&gt; (seems to be the sweet spot), and the prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Provide transcriptions and bounding boxes for the words in the image. Use JSON format.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This is the output:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;[ {&amp;quot;bbox_2d&amp;quot;: [160, 867, 181, 879], &amp;quot;text_content&amp;quot;: &amp;quot;steam&amp;quot;}, {&amp;quot;bbox_2d&amp;quot;: [146, 515, 168, 527], &amp;quot;text_content&amp;quot;: &amp;quot;queen&amp;quot;}, {&amp;quot;bbox_2d&amp;quot;: [565, 731, 589, 743], &amp;quot;text_content&amp;quot;: &amp;quot;satisfied&amp;quot;}, {&amp;quot;bbox_2d&amp;quot;: [760, 615, 784, 627], &amp;quot;text_content&amp;quot;: &amp;quot;feather&amp;quot;}, {&amp;quot;bbox_2d&amp;quot;: [335, 368, 364, 379], &amp;quot;text_content&amp;quot;: &amp;quot;mention&amp;quot;}, {&amp;quot;bbox_2d&amp;quot;: [515, 381, 538, 392], &amp;quot;text_content&amp;quot;: &amp;quot;cabinet&amp;quot;} ]&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Flawless. No notes. &lt;a href="https://i.imgur.com/5bejqK9.jpeg"&gt;It even got the bounding boxes correct.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How do other models compare?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gemini 2.5 pro: Hallucinates an answer.&lt;/li&gt; &lt;li&gt;Claude Opus 4: Correctly identifies 3/6 words.&lt;/li&gt; &lt;li&gt;ChatGPT 5: After 5 minutes (!!) of thinking, it finds all 6 words. The bounding boxes are wrong.&lt;/li&gt; &lt;li&gt;DeepSeekOCR: Produces garbage (possible PEBCAK)&lt;/li&gt; &lt;li&gt;PaddleOCR-VL-0.9B: Finds 3 words, hallucinates 2. Doesn't output bounding boxes.&lt;/li&gt; &lt;li&gt;GLM-4.5V: Also perfect results.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Very impressive that such as small model can get such good results, especially considering it's not tuned for OCR.&lt;/p&gt; &lt;p&gt;edit:&lt;/p&gt; &lt;p&gt;&lt;a href="https://gist.github.com/vapetrov/f5597628e77f4238ce25bd9a63e14af1"&gt;Here's the script I used to run it.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://files.catbox.moe/byex4n.jpg"&gt;The exact image I used.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct"&gt;The model.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trypocopris"&gt; /u/Trypocopris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot95gj/qwen3vls_perceptiveness_is_incredible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot95gj/qwen3vls_perceptiveness_is_incredible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot95gj/qwen3vls_perceptiveness_is_incredible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T09:12:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1otihl1</id>
    <title>Open-dLLM: Open Diffusion Large Language Models</title>
    <updated>2025-11-10T16:33:06+00:00</updated>
    <author>
      <name>/u/pengzhangzhi</name>
      <uri>https://old.reddit.com/user/pengzhangzhi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otihl1/opendllm_open_diffusion_large_language_models/"&gt; &lt;img alt="Open-dLLM: Open Diffusion Large Language Models" src="https://external-preview.redd.it/eHlpNXJmc3BpZzBnMbC2Q-rs9CfDNgw85akHP4ZCgTS81bEyqZb3k8CkqU2r.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fedc17befcb0ad76c15f3434bb58981727894c5" title="Open-dLLM: Open Diffusion Large Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the most open release of a diffusion-based large language model to date —&lt;br /&gt; including &lt;strong&gt;pretraining, evaluation, inference, and checkpoints&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/pengzhangzhi/Open-dLLM"&gt;https://github.com/pengzhangzhi/Open-dLLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://oval-shell-31c.notion.site/Open-dLLM-Open-Diffusion-Large-Language-Model-25e03bf6136480b7a4ebe3d53be9f68a"&gt;https://oval-shell-31c.notion.site/Open-dLLM-Open-Diffusion-Large-Language-Model-25e03bf6136480b7a4ebe3d53be9f68a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pengzhangzhi"&gt; /u/pengzhangzhi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qb62efspig0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otihl1/opendllm_open_diffusion_large_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otihl1/opendllm_open_diffusion_large_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T16:33:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1otscki</id>
    <title>Reflection AI reached human-level performance (85%) on ARC-AGI v1 for under $10k and within 12 hours. You can run this code yourself, it’s open source.</title>
    <updated>2025-11-10T22:37:40+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otscki/reflection_ai_reached_humanlevel_performance_85/"&gt; &lt;img alt="Reflection AI reached human-level performance (85%) on ARC-AGI v1 for under $10k and within 12 hours. You can run this code yourself, it’s open source." src="https://external-preview.redd.it/ARR7y9mlLeCC9oWmE5UREkOw8RADA8XOccGD021Q5lw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4be25bea8245c672919ac843febfe66e1da8de0" title="Reflection AI reached human-level performance (85%) on ARC-AGI v1 for under $10k and within 12 hours. You can run this code yourself, it’s open source." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jerber/arc-lang-public"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otscki/reflection_ai_reached_humanlevel_performance_85/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otscki/reflection_ai_reached_humanlevel_performance_85/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T22:37:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1otl8q8</id>
    <title>Omnilingual ASR: Advancing Automatic Speech Recognition for 1,600+ Languages</title>
    <updated>2025-11-10T18:12:13+00:00</updated>
    <author>
      <name>/u/jean-</name>
      <uri>https://old.reddit.com/user/jean-</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jean-"&gt; /u/jean- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otl8q8/omnilingual_asr_advancing_automatic_speech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otl8q8/omnilingual_asr_advancing_automatic_speech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T18:12:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1otveug</id>
    <title>A startup Olares is attempting to launch a small 3.5L MiniPC dedicated to local AI, with RTX 5090 Mobile (24GB VRAM) and 96GB of DDR5 RAM for $3K</title>
    <updated>2025-11-11T00:44:49+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otveug/a_startup_olares_is_attempting_to_launch_a_small/"&gt; &lt;img alt="A startup Olares is attempting to launch a small 3.5L MiniPC dedicated to local AI, with RTX 5090 Mobile (24GB VRAM) and 96GB of DDR5 RAM for $3K" src="https://external-preview.redd.it/j6x6Pm9GXcBDejuI8fZ_JaGjEF5FKmyowYdHbKM_k34.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f2207a39f85b0be48a03566c4c904bcc528405b" title="A startup Olares is attempting to launch a small 3.5L MiniPC dedicated to local AI, with RTX 5090 Mobile (24GB VRAM) and 96GB of DDR5 RAM for $3K" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techpowerup.com/342779/olares-to-launch-a-personal-ai-device-bringing-cloud-level-performance-home"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otveug/a_startup_olares_is_attempting_to_launch_a_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otveug/a_startup_olares_is_attempting_to_launch_a_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T00:44:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oth5pw</id>
    <title>AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model</title>
    <updated>2025-11-10T15:44:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt; &lt;img alt="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" src="https://b.thumbs.redditmedia.com/yz9_FpdLcHNiCkaH5fLEIoXS2f5u5twNBr7SQ9Go3AI.jpg" title="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Moonshot AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/ComfortableAsk4494"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/zxytim"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ppwwyyxx"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM – 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87"&gt;https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:44:10+00:00</published>
  </entry>
</feed>
