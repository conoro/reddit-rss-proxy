<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-10T16:50:15+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o31syu</id>
    <title>Dual Xeon 6 + 12 channel ram</title>
    <updated>2025-10-10T14:03:45+00:00</updated>
    <author>
      <name>/u/____vladrad</name>
      <uri>https://old.reddit.com/user/____vladrad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi wanted to get your thoughts on a new build I may be trying to put together. I sold my 4090 and am thinking about expanding my current workstation. I hear that xeon 6 have really good memory optimization for offloading matrix multiplications.&lt;/p&gt; &lt;p&gt;I found one motherboard that accepts a 1 slot Xeon 6. Then I saw this one GIGABYTE MS74-HB0 Motherboard - Intel¬Æ Xeon¬Æ 6 Processors. It‚Äôs dual slot but supports 12 channel ram up to 8000mhz. I was thinking about doing a two Xeon 6 - 6521P. With 12 channel ram I calculated around 700-800 Gb/s (if I did that right). With two TRX 6000 pros I assume the acceleration with the Xeon 6 cpus (48 cores + memory bandwidth + amx implementation) should be ok.&lt;/p&gt; &lt;p&gt;Is this overkill? Is this a bad idea. My original intent was to run Deepseek and GLM like this for dataset creations/distilling. &lt;/p&gt; &lt;p&gt;&lt;a href="https://lmsys.org/blog/2025-07-14-intel-xeon-optimization/"&gt;https://lmsys.org/blog/2025-07-14-intel-xeon-optimization/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Was going off this article and some llama.cpp stuff I saw. Is this a terrible idea? Thanks for the feedback&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/____vladrad"&gt; /u/____vladrad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o31syu/dual_xeon_6_12_channel_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o31syu/dual_xeon_6_12_channel_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o31syu/dual_xeon_6_12_channel_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T14:03:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2vnf2</id>
    <title>Temperatures for MI50 during inference? Anyone with experience re-pasting processor?</title>
    <updated>2025-10-10T08:51:07+00:00</updated>
    <author>
      <name>/u/EdenistTech</name>
      <uri>https://old.reddit.com/user/EdenistTech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As many others in here, I am experimenting with the MI50 at the moment due to the fantastic value-for-money relationship of this card (at least w.r.t. $ / GB VRAM). I am getting 80c-85c degrees on the edge sensor running full tilt with a &amp;quot;custom cooling solution&amp;quot;. The junction sensor shows &amp;gt;100c (which is high but acceptable, I am told). Decreasing the power limit with rocm-smi does not seem to affect temps much. Idle temps are 30c-40c. What is your experience with temperatures? Have any of you successfully re-pasted the processor?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EdenistTech"&gt; /u/EdenistTech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2vnf2/temperatures_for_mi50_during_inference_anyone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2vnf2/temperatures_for_mi50_during_inference_anyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2vnf2/temperatures_for_mi50_during_inference_anyone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T08:51:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1o32c6c</id>
    <title>is there any LLM App that can generate files for you?</title>
    <updated>2025-10-10T14:24:31+00:00</updated>
    <author>
      <name>/u/FatFigFresh</name>
      <uri>https://old.reddit.com/user/FatFigFresh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For old farts like me who are near their graves and want to skip DIY part of responds of LLMs and being an absolute bum by expecting LLM App take care of the DIY part of writing the notes( or programming codes or whatever) and saving them as files and then deliver it as the final product to you... is any app produced for this matter to satisfy the needs of clowns like me? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FatFigFresh"&gt; /u/FatFigFresh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o32c6c/is_there_any_llm_app_that_can_generate_files_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o32c6c/is_there_any_llm_app_that_can_generate_files_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o32c6c/is_there_any_llm_app_that_can_generate_files_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T14:24:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2vwh1</id>
    <title>Whats the best local model i can run with 16 GB VRAM and 96 GB RAM</title>
    <updated>2025-10-10T09:07:19+00:00</updated>
    <author>
      <name>/u/Tricky_Reflection_75</name>
      <uri>https://old.reddit.com/user/Tricky_Reflection_75</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;1 general model that has some intelligence with really good tool calling capabilties / (Would be good if it was uncensored to some capacity too, not for any specific purpose but just generally don't want it to turn down stuff cause of &amp;quot;Safety&amp;quot; or something.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tricky_Reflection_75"&gt; /u/Tricky_Reflection_75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2vwh1/whats_the_best_local_model_i_can_run_with_16_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2vwh1/whats_the_best_local_model_i_can_run_with_16_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2vwh1/whats_the_best_local_model_i_can_run_with_16_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T09:07:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2z7fz</id>
    <title>Experience with networked 2x128GB AI Max 395?</title>
    <updated>2025-10-10T12:13:32+00:00</updated>
    <author>
      <name>/u/Bird476Shed</name>
      <uri>https://old.reddit.com/user/Bird476Shed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are considering to buy two of these AI shoeboxes, for space and power efficiency. Run a large LLM during the day, use as CI/CD/test server over night.&lt;/p&gt; &lt;p&gt;Q: Anyone has experience with such a setup? Specifically, what's the expected performance of a large (GLM or Qwen235B) model that is split over these two with llama.cpp and RPC?&lt;/p&gt; &lt;p&gt;I have prototyped this setup already with 2x 96GB regular PCs/CPUs, it's quite slow but the answers are quite good. Faster ram and 5(?)GB network between the showboxes should provide faster performance? How much?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bird476Shed"&gt; /u/Bird476Shed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2z7fz/experience_with_networked_2x128gb_ai_max_395/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2z7fz/experience_with_networked_2x128gb_ai_max_395/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2z7fz/experience_with_networked_2x128gb_ai_max_395/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T12:13:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1o23vqf</id>
    <title>microsoft/UserLM-8b - ‚ÄúUnlike typical LLMs that are trained to play the role of the 'assistant' in conversation, we trained UserLM-8b to simulate the 'user' role‚Äù</title>
    <updated>2025-10-09T11:54:17+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o23vqf/microsoftuserlm8b_unlike_typical_llms_that_are/"&gt; &lt;img alt="microsoft/UserLM-8b - ‚ÄúUnlike typical LLMs that are trained to play the role of the 'assistant' in conversation, we trained UserLM-8b to simulate the 'user' role‚Äù" src="https://external-preview.redd.it/AHBKf2lyQkjC30uEVnsWIAOzEiogI0jy6tjwogMGZ5A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e0a4856ffac6d098a7303d1eafccab9d20ceaa1" title="microsoft/UserLM-8b - ‚ÄúUnlike typical LLMs that are trained to play the role of the 'assistant' in conversation, we trained UserLM-8b to simulate the 'user' role‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/UserLM-8b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o23vqf/microsoftuserlm8b_unlike_typical_llms_that_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o23vqf/microsoftuserlm8b_unlike_typical_llms_that_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T11:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2y43t</id>
    <title>LLaMA that plays chess</title>
    <updated>2025-10-10T11:17:54+00:00</updated>
    <author>
      <name>/u/neurocod</name>
      <uri>https://old.reddit.com/user/neurocod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2y43t/llama_that_plays_chess/"&gt; &lt;img alt="LLaMA that plays chess" src="https://external-preview.redd.it/geYQQAxqjX90o0mQq_3STTNJ2tR_-toI4zN0X160D4k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4a0e7749765f565b43f6f79308b17aa4c64b216" title="LLaMA that plays chess" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a &lt;a href="https://github.com/neurocod/llm-chess-hybrid"&gt;hybrid&lt;/a&gt; of LLaMA and several other neural networks that can play chess quite well. It‚Äôs part of my ongoing series of &lt;a href="https://www.linkedin.com/pulse/hybrid-neural-networks-inside-llms-konstantine-kozachuk-xquze/"&gt;articles&lt;/a&gt; about hybrid neural networks. The hippocampus model is still missing and outsourced to traditional C++ code.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z916pjlqp9uf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d470e2d693d723ffd1d3f1493df16faf36be7a2"&gt;https://preview.redd.it/z916pjlqp9uf1.png?width=3840&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d470e2d693d723ffd1d3f1493df16faf36be7a2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neurocod"&gt; /u/neurocod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2y43t/llama_that_plays_chess/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2y43t/llama_that_plays_chess/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2y43t/llama_that_plays_chess/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T11:17:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2k0cw</id>
    <title>Is there any way I can finetune the GrayWolf models faster? It currently takes 10,000 years to create a LoRA on my current GPU rig and I want to speed up the process.</title>
    <updated>2025-10-09T22:31:49+00:00</updated>
    <author>
      <name>/u/Arkhos-Winter</name>
      <uri>https://old.reddit.com/user/Arkhos-Winter</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2k0cw/is_there_any_way_i_can_finetune_the_graywolf/"&gt; &lt;img alt="Is there any way I can finetune the GrayWolf models faster? It currently takes 10,000 years to create a LoRA on my current GPU rig and I want to speed up the process." src="https://preview.redd.it/9aievo3cx5uf1.gif?width=320&amp;amp;crop=smart&amp;amp;s=0776eb0fcf74d851620f3d811dfc1d496ccc9441" title="Is there any way I can finetune the GrayWolf models faster? It currently takes 10,000 years to create a LoRA on my current GPU rig and I want to speed up the process." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arkhos-Winter"&gt; /u/Arkhos-Winter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9aievo3cx5uf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2k0cw/is_there_any_way_i_can_finetune_the_graywolf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2k0cw/is_there_any_way_i_can_finetune_the_graywolf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T22:31:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o32cs8</id>
    <title>chatllm.cpp supports Janus-Pro</title>
    <updated>2025-10-10T14:25:12+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o32cs8/chatllmcpp_supports_januspro/"&gt; &lt;img alt="chatllm.cpp supports Janus-Pro" src="https://external-preview.redd.it/RerkM_YaMvlrnabVjd6weUri30h-rvzJS7KreYcdnw8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f426236473f9cf99ef0faf88dd4229251ceabba" title="chatllm.cpp supports Janus-Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Janus-Pro is a novel autoregressive framework that unifies multimodal understanding and generation.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/Janus-Pro-1B"&gt;https://huggingface.co/deepseek-ai/Janus-Pro-1B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With &lt;a href="https://github.com/foldl/chatllm.cpp"&gt;chatllm.cpp&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6gp815annauf1.png?width=1217&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2dd884a934261226b096fc1f13da6dac53db1cb6"&gt;https://preview.redd.it/6gp815annauf1.png?width=1217&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2dd884a934261226b096fc1f13da6dac53db1cb6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o32cs8/chatllmcpp_supports_januspro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o32cs8/chatllmcpp_supports_januspro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o32cs8/chatllmcpp_supports_januspro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T14:25:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2lq5n</id>
    <title>Is there anything faster or smaller with equal quality to Qwen 30B A3B?</title>
    <updated>2025-10-09T23:49:17+00:00</updated>
    <author>
      <name>/u/WEREWOLF_BX13</name>
      <uri>https://old.reddit.com/user/WEREWOLF_BX13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Specs: RTX 3060 12GB - 4+8+16GB RAM - R5 4600G&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I've tried mistral small, instruct and nemo in 7b, 14b and 24b sizes but unfortunately 7b just can't handle much nothing except for those 200 tokens &lt;a href="http://c.ai"&gt;c.ai&lt;/a&gt; chatbots and they're thrice slower than Qwen.&lt;/p&gt; &lt;p&gt;Do you know anything smaller than &lt;strong&gt;Qwen A3B 30B&lt;/strong&gt; with at least same quality as the &lt;strong&gt;Q3_K_M quant (14,3GB)&lt;/strong&gt; and 28k context window? Not using for programming, but more complex reasoning tasks and super long story-writing/advanced character creation with amateur psychology knowledge. I saw that this model has different processing methods, that's why its faster.&lt;/p&gt; &lt;p&gt;&lt;em&gt;I'm planning on getting a 24GB VRAM gpu like RTX 3090, but it will be absolute pointless if there isn't anything noticeably better than Qwen or Video Generation models keep getting worse in optimization considering how slow it is even for the 4090.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WEREWOLF_BX13"&gt; /u/WEREWOLF_BX13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2lq5n/is_there_anything_faster_or_smaller_with_equal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2lq5n/is_there_anything_faster_or_smaller_with_equal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2lq5n/is_there_anything_faster_or_smaller_with_equal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-09T23:49:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2x0bv</id>
    <title>AMD MI50 32GB better buy than MI100?</title>
    <updated>2025-10-10T10:16:09+00:00</updated>
    <author>
      <name>/u/FriendlyRetriver</name>
      <uri>https://old.reddit.com/user/FriendlyRetriver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Plenty of people have the MI50 and performance seems to continuously improve.&lt;/p&gt; &lt;p&gt;While it's officially dropped from ROCm 7, we can still get it to work if we copy some files manually.. obviously this will sooner or later stop working but then we'll have Vulkan.. which (with llama.cpp at least) seems to be almost at a performance-parity with ROCm (or faster?).&lt;/p&gt; &lt;p&gt;Now my question, MI100 does not have Vulkan support AFAIK (from AMD specs). While it's still supported by ROCm 7, sooner or later AMD will drop it.. I realize all of this will be irrelevant as tech moves on and both these cards will be considered old relics, but doesn't Vulkan support make the MI50 the better long term buy, for homelabbers at least?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FriendlyRetriver"&gt; /u/FriendlyRetriver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2x0bv/amd_mi50_32gb_better_buy_than_mi100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2x0bv/amd_mi50_32gb_better_buy_than_mi100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2x0bv/amd_mi50_32gb_better_buy_than_mi100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T10:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1o34d0s</id>
    <title>üöÄ ToolNeuron Beta-4.5 ‚Äî Offline &amp; Privacy-First AI Hub for Android!</title>
    <updated>2025-10-10T15:39:37+00:00</updated>
    <author>
      <name>/u/DarkEngine774</name>
      <uri>https://old.reddit.com/user/DarkEngine774</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o34d0s/toolneuron_beta45_offline_privacyfirst_ai_hub_for/"&gt; &lt;img alt="üöÄ ToolNeuron Beta-4.5 ‚Äî Offline &amp;amp; Privacy-First AI Hub for Android!" src="https://b.thumbs.redditmedia.com/peLMfgbkrIUNrY6CzJRA-umyrCKrsWziFRSotHvuf_g.jpg" title="üöÄ ToolNeuron Beta-4.5 ‚Äî Offline &amp;amp; Privacy-First AI Hub for Android!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey&lt;/p&gt; &lt;p&gt;I'm excited to share &lt;strong&gt;ToolNeuron Beta-4.5&lt;/strong&gt;, my privacy-first AI hub for Android devices. It's designed to bring &lt;strong&gt;powerful AI to your pocket&lt;/strong&gt; ‚Äî fully offline, with plugin support, and the ability to tweak models on the fly.&lt;/p&gt; &lt;h1&gt;üß† What ToolNeuron Can Do:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Main Chat Screen&lt;/strong&gt;: Smooth, ready-to-use chat interface with runtime model switching.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Tweaking Screen&lt;/strong&gt;: Adjust any model‚Äôs parameters in real-time (GGUF or OpenRouter).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plugin Screen&lt;/strong&gt;: Browse, enable, or disable plugins; extend AI capabilities (Web Search, Web Scraper, Coding Canvas, etc.).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DataHub Screen&lt;/strong&gt;: Attach dynamic datasets to models for specialized knowledge (coding, medical, etc.).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Personal Data View Screen&lt;/strong&gt;: Inspect local data packs and manage conversation history.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Screen&lt;/strong&gt;: Import, manage, and switch between any installed models seamlessly.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üîß Why You‚Äôll Love It:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Fully offline (privacy-first) üõ°Ô∏è&lt;/li&gt; &lt;li&gt;Switch between models mid-chat without losing context üîÑ&lt;/li&gt; &lt;li&gt;Load custom models from your device üìÇ&lt;/li&gt; &lt;li&gt;Expandable via plugins and data packs üß©&lt;/li&gt; &lt;li&gt;Optimized for daily productivity &amp;amp; fun ‚ö°&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üì• Try It Now&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/Siddhesh2377/ToolNeuron/releases/tag/Beta-4.5"&gt;Download Beta-4.5 APK&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;üí¨ Let‚Äôs Make This Interactive:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Which AI model do you mostly use on mobile?&lt;/li&gt; &lt;li&gt;What plugin would you like to see next in ToolNeuron?&lt;/li&gt; &lt;li&gt;Any feature requests or UX improvements?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôd love to hear your feedback and ideas! I‚Äôm personally very active and plan to incorporate community suggestions quickly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Join our community&lt;/strong&gt;: &lt;a href="https://discord.gg/SvgGaxwk"&gt;Discord&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;GitHub &amp;amp; Releases&lt;/strong&gt;: &lt;a href="https://github.com/Siddhesh2377/ToolNeuron"&gt;GitHub Repo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkEngine774"&gt; /u/DarkEngine774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o34d0s"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o34d0s/toolneuron_beta45_offline_privacyfirst_ai_hub_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o34d0s/toolneuron_beta45_offline_privacyfirst_ai_hub_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T15:39:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o32utg</id>
    <title>My Deep Dive into Fine-Tuning: IBM Granite-4.0 with Python and Unsloth! üöÄ</title>
    <updated>2025-10-10T14:44:33+00:00</updated>
    <author>
      <name>/u/krishanndev</name>
      <uri>https://old.reddit.com/user/krishanndev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent this week getting hands-on with IBM‚Äôs Granite-4.0 LLM and the Unsloth library, honestly thinking it would just be another ‚Äúmeh‚Äù open-source fine-tuning project. Instead‚ÄîI ended up pretty excited, so wanted to share my take for anyone on the fence!&lt;/p&gt; &lt;p&gt;Personal hurdles? I‚Äôm used to LLM fine-tuning being a clunky, resource-heavy slog. But this time I actually got domain-level results (support-bot made way better recommendations!) with just a free Colab T4 and some Python. Seeing the model shift from bland, generic helpdesk answers to context-aware, on-point responses in only about 60 training steps was incredibly satisfying.&lt;/p&gt; &lt;p&gt;If you‚Äôre like me and always chasing practical, accessible AI upgrades, this is worth the experiment.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real custom fine-tuning, no expensive infra&lt;/li&gt; &lt;li&gt;Model is compact‚Äîruns smooth, even on free hardware&lt;/li&gt; &lt;li&gt;The workflow‚Äôs straightforward (and yes, I documented mistakes and fixes too)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Want to give it a spin?&lt;br /&gt; Here‚Äôs the full story and guide I wrote: &lt;a href="https://medium.com/@krishanw30/4fc11b529c1f?sk=dd2effadc55ce5ca7acce163bec66d9d"&gt;Medium Article&lt;/a&gt;&lt;br /&gt; Or dive right into my shared Hugging Face checkpoint: &lt;a href="https://huggingface.co/krishanwalia30/granite-4.0-h-micro_lora_model?source=post_page-----4fc11b529c1f---------------------------------------"&gt;Fine-tuned Model&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/krishanndev"&gt; /u/krishanndev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o32utg/my_deep_dive_into_finetuning_ibm_granite40_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o32utg/my_deep_dive_into_finetuning_ibm_granite40_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o32utg/my_deep_dive_into_finetuning_ibm_granite40_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T14:44:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1o30e9q</id>
    <title>"Google Gemini" but using a local model</title>
    <updated>2025-10-10T13:06:41+00:00</updated>
    <author>
      <name>/u/Tasty-Lobster-8915</name>
      <uri>https://old.reddit.com/user/Tasty-Lobster-8915</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1o30e9q/video/sii45b8z8auf1/player"&gt;https://reddit.com/link/1o30e9q/video/sii45b8z8auf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I built a local assistant app that can replace Google Gemini as your phone's default assistant. It works similar to Gemini: long press the power button to bring up Layla, and it will run a local model instead of Gemini.&lt;/p&gt; &lt;p&gt;It supports using local models (GGUF or PTE), connect to any OpenAI endpoint such as LMStudio running on your PC, or Layla Cloud.&lt;/p&gt; &lt;p&gt;Video is showing a 8B model (L3-Rhaenys) running on S25 Ultra. But if your phone is not powerful enough, you can choose to run 2B or 4B models.&lt;/p&gt; &lt;p&gt;It's still in early development; I'd love to hear what other tools/features you'd like to see integrated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tasty-Lobster-8915"&gt; /u/Tasty-Lobster-8915 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o30e9q/google_gemini_but_using_a_local_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o30e9q/google_gemini_but_using_a_local_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o30e9q/google_gemini_but_using_a_local_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T13:06:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o33iin</id>
    <title>Zero-Learn in ToolBrain ‚Äî Agents that write their own training data</title>
    <updated>2025-10-10T15:08:41+00:00</updated>
    <author>
      <name>/u/Excellent-Solid1865</name>
      <uri>https://old.reddit.com/user/Excellent-Solid1865</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One of the trickiest parts of training tool-using agents is collecting enough task data. What if your agent could generate its own curriculum instead?&lt;/p&gt; &lt;p&gt;That‚Äôs what we built in ToolBrain‚Äôs Zero-Learn feature ‚Äî a lightweight reinforcement-learning loop where an LLM agent bootstraps its own training queries directly from the tool definitions you give it.&lt;/p&gt; &lt;h2&gt;‚öôÔ∏è How Zero-Learn Works&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;You start with a few tools (from &lt;code&gt;smolagent&lt;/code&gt;), e.g.:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;```python from smolagent import tool&lt;/p&gt; &lt;p&gt;@tool def calculate_compound_interest(principal, rate, years): ... @tool def calculate_loan_payment(principal, rate, term): ... ```&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The Brain‚Äôs method &lt;code&gt;generate_training_examples&lt;/code&gt; prompts the model to invent realistic tasks that require using these tools. You can use the LLM of the agent or use external model, you can also add external tools.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;```python from toolbrain import Brain&lt;/p&gt; &lt;p&gt;brain = Brain(agent=agent) examples = brain.generate_training_examples( task_description=&amp;quot;Finance queries that use multiple tools&amp;quot;, num_examples=100, min_tool_calls=2, # hint to include multiple tool uses max_words=80, # keeps prompts short and realistic self_rank=True # optional: let the LLM rank them by quality ) ```&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Generated examples are auto-ranked and filtered, then used for RL fine-tuning (GRPO / DPO).&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;What happens inside:&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;ToolBrain builds a ‚Äútool card‚Äù (name + description + args).&lt;/li&gt; &lt;li&gt;The agent‚Äôs LLM writes user queries that should require those tools and provide realistic arguments for tools.&lt;/li&gt; &lt;li&gt;If &lt;code&gt;self_rank=True&lt;/code&gt;, the model re-ranks them based on relevance, argument realism, and concreteness.&lt;/li&gt; &lt;li&gt;You get back a list of plain text queries ‚Äî your new mini training set; then you can use them for training with &lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;üí° Example Outputs (Finance Tools)&lt;/h2&gt; &lt;p&gt;From a &lt;code&gt;Qwen-0.5B&lt;/code&gt; agent using simple finance functions:&lt;/p&gt; &lt;p&gt;&lt;code&gt; &amp;quot;Calculate the compound interest on $10,000 at an annual rate of 5% for 3 years.&amp;quot; &amp;quot;What is the formula for calculating compound interest?&amp;quot; &amp;quot;Compute the loan payment for a 7-year loan at 5% interest and $10,000 principal.&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Roughly two-thirds of the generated queries are directly executable ‚Äî the rest can be filtered or rewritten automatically.&lt;/p&gt; &lt;h2&gt;üîÅ Why it‚Äôs useful&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Bootstraps small, domain-specific datasets without human effort.&lt;/li&gt; &lt;li&gt;Perfect for teaching agents to use your custom tools (finance, bio-med, robotics, whatever).&lt;/li&gt; &lt;li&gt;Integrates directly with ToolBrain‚Äôs RL loop ‚Äî GRPO, DPO, knowledge distillation, etc.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;üìò Learn More&lt;/h2&gt; &lt;p&gt;üìÑ Paper ‚Üí ToolBrain: A Flexible Reinforcement Learning Framework for Agentic Tools (arXiv:2510.00023)&lt;/p&gt; &lt;p&gt;üåê Project ‚Üí toolbrain.org&lt;/p&gt; &lt;p&gt;Would love to hear from others experimenting with synthetic data generation for agents ‚Äî How are you teaching your models new tools without curated datasets?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent-Solid1865"&gt; /u/Excellent-Solid1865 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o33iin/zerolearn_in_toolbrain_agents_that_write_their/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o33iin/zerolearn_in_toolbrain_agents_that_write_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o33iin/zerolearn_in_toolbrain_agents_that_write_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T15:08:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2z88f</id>
    <title>GALAX Rolls Out Its Single-Slot GeForce RTX 5060 Ti GPU With 16 GB VRAM &amp; Blower-Fan</title>
    <updated>2025-10-10T12:14:32+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2z88f/galax_rolls_out_its_singleslot_geforce_rtx_5060/"&gt; &lt;img alt="GALAX Rolls Out Its Single-Slot GeForce RTX 5060 Ti GPU With 16 GB VRAM &amp;amp; Blower-Fan" src="https://external-preview.redd.it/y6J_OcAXJadfgqu4sGThqrpBh-QuwElkR48n9WojP7o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02f280419bde7e9970740ea47bdbd26403d64024" title="GALAX Rolls Out Its Single-Slot GeForce RTX 5060 Ti GPU With 16 GB VRAM &amp;amp; Blower-Fan" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/galax-single-slot-geforce-rtx-5060-ti-gpu-16-gb-vram-blower-fan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2z88f/galax_rolls_out_its_singleslot_geforce_rtx_5060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2z88f/galax_rolls_out_its_singleslot_geforce_rtx_5060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T12:14:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2x2xs</id>
    <title>Qwen team auto-closed all issues on Qwen2-VL repository</title>
    <updated>2025-10-10T10:20:21+00:00</updated>
    <author>
      <name>/u/CasualCapybara</name>
      <uri>https://old.reddit.com/user/CasualCapybara</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just noticed that the Qwen2-VL repository has been renamed to Qwen3-VL and that all issues on GitHub are being closed. It sits currently at 475 open issues/859 closed issues, and changing quickly: &lt;a href="https://github.com/QwenLM/Qwen3-VL/issues"&gt;https://github.com/QwenLM/Qwen3-VL/issues&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I think this is somewhat rude, because it ignores the effort of all the people that took time out of their day to report issues. They could just as easily have created a new repository.&lt;/p&gt; &lt;p&gt;Of course I hugely appreciate all the open models that the Qwen team gave us, but I still think that this could have been handled in a better way.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CasualCapybara"&gt; /u/CasualCapybara &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2x2xs/qwen_team_autoclosed_all_issues_on_qwen2vl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2x2xs/qwen_team_autoclosed_all_issues_on_qwen2vl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2x2xs/qwen_team_autoclosed_all_issues_on_qwen2vl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T10:20:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2wvvy</id>
    <title>China blacklists major chip research firm TechInsights following report on Huawei</title>
    <updated>2025-10-10T10:08:58+00:00</updated>
    <author>
      <name>/u/vancity-boi-in-tdot</name>
      <uri>https://old.reddit.com/user/vancity-boi-in-tdot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2wvvy/china_blacklists_major_chip_research_firm/"&gt; &lt;img alt="China blacklists major chip research firm TechInsights following report on Huawei" src="https://external-preview.redd.it/cXviE8IZIrPbvKrVn_b-IwEX0IhaM8bGQVFM7IF9Z1o.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ba6db197a71a303a6de965206d6fa42d8ec4ee3" title="China blacklists major chip research firm TechInsights following report on Huawei" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vancity-boi-in-tdot"&gt; /u/vancity-boi-in-tdot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/10/10/china-blacklists-major-chip-ai-research-firm-techinsights-analyzed-report-huawei.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2wvvy/china_blacklists_major_chip_research_firm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2wvvy/china_blacklists_major_chip_research_firm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T10:08:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2q5n6</id>
    <title>I made a multimodal local RAG system with LM Studio</title>
    <updated>2025-10-10T03:22:45+00:00</updated>
    <author>
      <name>/u/donotfire</name>
      <uri>https://old.reddit.com/user/donotfire</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2q5n6/i_made_a_multimodal_local_rag_system_with_lm/"&gt; &lt;img alt="I made a multimodal local RAG system with LM Studio" src="https://external-preview.redd.it/cnBqa3NkeWhkN3VmMQb9PvMVdq9S5Iwim0AbH7Nkf50d3AGCnXrbNmpKxt0b.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f129228772f39b10328979337e92d94e8f2fd7ab" title="I made a multimodal local RAG system with LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I couldn‚Äôt find a RAG system that worked with Google Docs and could have more than 10,000 synced files, so I made one myself. This thing is a beast, it works with Gemma 3 4B decently well but I think the results would be way better with a larger model and a larger dataset. I‚Äôll share the full code later on but I‚Äôm tired rn&lt;/p&gt; &lt;p&gt;Edit, here's the source: &lt;a href="https://github.com/henrydaum/second-brain"&gt;Second Brain&lt;/a&gt;. Sorry for the wait.&lt;/p&gt; &lt;p&gt;I haven't tested this on other machines so please leave a comment or dm me if you find bugs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/donotfire"&gt; /u/donotfire &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/09paso3id7uf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2q5n6/i_made_a_multimodal_local_rag_system_with_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2q5n6/i_made_a_multimodal_local_rag_system_with_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T03:22:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1o32j5s</id>
    <title>Beck, a small model for delicate life situations</title>
    <updated>2025-10-10T14:32:08+00:00</updated>
    <author>
      <name>/u/antcroca159</name>
      <uri>https://old.reddit.com/user/antcroca159</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A language model that handles delicate life situations and tries to really help you.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/gustavecortal/Beck-8B"&gt;Beck&lt;/a&gt; is based on my previous model &lt;a href="https://huggingface.co/gustavecortal/Piaget-8B"&gt;Piaget&lt;/a&gt; (a language model for psychological reasoning), and was aligned with &lt;a href="https://huggingface.co/datasets/Psychotherapy-LLM/PsychoCounsel-Preference"&gt;psychotherapeutic preferences&lt;/a&gt;. Available sizes are: &lt;a href="https://huggingface.co/gustavecortal/Beck-0.6B"&gt;0.6B&lt;/a&gt;, &lt;a href="https://huggingface.co/gustavecortal/Beck-1.7B"&gt;1.7B&lt;/a&gt;, &lt;a href="https://huggingface.co/gustavecortal/Beck-4B"&gt;4B&lt;/a&gt;, &lt;a href="https://huggingface.co/gustavecortal/Beck-8B"&gt;8B&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Beck was trained using preference optimization (ORPO) and LoRA. You can reproduce the results using &lt;a href="https://github.com/gustavecortal/preference-optimization-orpo-lora"&gt;my repo&lt;/a&gt; for lightweight preference optimization.&lt;/p&gt; &lt;p&gt;Beck was inspired by my position paper on emotion analysis: &lt;a href="https://aclanthology.org/2024.cmcl-1.23/"&gt;Improving Language Models for Emotion Analysis: Insights from Cognitive Science&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How does it compare to other language models?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For the given prompt:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Anxiety often strikes when I'm faced with making decisions. The fear of making the wrong choice or disappointing others paralyzes me, leaving me stuck in indecision. I want to learn how to trust myself and make confident choices. \no_think &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3-8B&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;It's completely natural to feel anxious when facing important decisions, especially when you're worried about making the wrong choice or letting others down. This internal conflict can create a sense of overwhelm that makes it hard to move forward. Learning to trust yourself is a powerful step toward overcoming this challenge. Start by reflecting on past decisions‚Äîwhat worked, what didn't, and what you learned from them. Over time, you'll begin to recognize your own values, strengths, and patterns of thinking. Practicing self-compassion is also key; remind yourself that no decision is perfect and that growth comes from experience, not perfection. Small steps, like setting aside time each day for reflection or journaling, can help build confidence. Surround yourself with people who support your journey, and don't hesitate to seek guidance if needed. Trusting yourself isn‚Äôt about having all the answers‚Äîit‚Äôs about believing in your ability to navigate uncertainty and grow through the process. With patience and practice, you‚Äôll find greater clarity and courage in your choices. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Beck-8B&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;It sounds like you're dealing with a lot of pressure regarding decision-making and it's causing you significant distress. This is quite common for many people, as we all have moments where choosing can feel overwhelming. Let‚Äôs explore what might be contributing to these feelings so that we can work on building your confidence in decision-making. Firstly, consider what specific fears arise when you think about making a decision. Is there a particular aspect of the choice that triggers anxiety? Sometimes identifying just one component can help in breaking down the process. Also, think about past decisions that went well‚Äîeven if they don‚Äôt seem big at first. Remembering successful outcomes can reinforce trust in yourself, reminding you that you have experience navigating through uncertainties successfully before. Building self-trust involves challenging any negative thoughts surrounding decision-making. It may also help to practice mindfulness‚Äîbeing fully present during each step of the decision process rather than anticipating worst-case scenarios. By cultivating patience and acceptance towards ambiguity, you might find that some anxieties lessen over time. Would you like us to brainstorm practical exercises or strategies tailored specifically for boosting your decision-making abilities and confidence? &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/antcroca159"&gt; /u/antcroca159 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o32j5s/beck_a_small_model_for_delicate_life_situations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o32j5s/beck_a_small_model_for_delicate_life_situations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o32j5s/beck_a_small_model_for_delicate_life_situations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T14:32:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1o33mui</id>
    <title>A list of models released or updated this week on this sub, in case you missed any (10 Oct).</title>
    <updated>2025-10-10T15:13:02+00:00</updated>
    <author>
      <name>/u/aifeed-fyi</name>
      <uri>https://old.reddit.com/user/aifeed-fyi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;Here is the list of models (releases and updates), I found mentioned on the LocalLlama for this week, Please update or let me know in the comments if there are any mistakes or misses. Enjoy !&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Reddit post&lt;/th&gt; &lt;th align="left"&gt;HF / GitHub&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Jamba 3B&lt;/td&gt; &lt;td align="left"&gt;tiny 3 B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1o1ac09"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;WEBGEN,UIGEN‚ÄëFX&lt;/td&gt; &lt;td align="left"&gt;research‚Äëpreview for UI/UX&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1nz20g2"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Tesslate/UIGENT-30B-3A-Preview"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;KAT-Dev-72B-Exp&lt;/td&gt; &lt;td align="left"&gt;Coding model&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o31rdl/kwaipilotkatdev72bexp_model_released"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Playable-GGUF&lt;/td&gt; &lt;td align="left"&gt;7b vibe coding retro games&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o27xsj/introducing_playable1gguf_by_far_the_worlds_best"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/playable/Playable1-GGUF"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UserLM-8b&lt;/td&gt; &lt;td align="left"&gt;8b LLM playing user role&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o23vqf/microsoftuserlm8b_unlike_typical_llms_that_are"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/microsoft/UserLM-8b"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CoDA‚Äëv0‚ÄëInstruct&lt;/td&gt; &lt;td align="left"&gt;language‚Äëdiffusion&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o1s7q8/an_open_sourced_language_diffusion_model_by_sf/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Salesforce/CoDA-v0-Instruct"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ColBERT Nano 250K&lt;/td&gt; &lt;td align="left"&gt;tiny‚Äëretrieval&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o1mpt5/introducing_the_colbert_nano_series_of_models_all/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/NeuML/"&gt;HF collection&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2‚Äë8B‚ÄëA1B&lt;/td&gt; &lt;td align="left"&gt;hybrid 8 B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o0zted/lfm28ba1b_quality_34b_dense_yet_faster_than/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-8B-A1B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3‚ÄëVL‚Äë30B‚ÄëA3B‚ÄëInstruct&lt;/td&gt; &lt;td align="left"&gt;vision‚ÄëLLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nxhfcq/qwen3vl30ba3binstruct_thinking_are_here/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BULaMU&lt;/td&gt; &lt;td align="left"&gt;Luganda LLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nykxfq/bulamuthe_first_luganda_large_language_model/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/datasets/mwebazarick/BULaMU"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;YanoljaNEXT‚ÄëRosetta‚Äë12B‚Äë2510&lt;/td&gt; &lt;td align="left"&gt;translation 12 B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o2bm3z/yanoljayanoljanextrosetta12b2510/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-12B-2510"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SDLM 32B&lt;/td&gt; &lt;td align="left"&gt;multimodal 32B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nwxje9/sdlm_32b4b_from_opengvlab/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/OpenGVLab/SDLM-32B-D4"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SDLM 4B&lt;/td&gt; &lt;td align="left"&gt;multimodal 4 B&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nwxje9/sdlm_32b4b_from_opengvlab/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/OpenGVLab/SDLM-3B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;üîß Notable resources&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Resource&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Reddit post&lt;/th&gt; &lt;th align="left"&gt;HF / GitHub&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;MLXSharp&lt;/td&gt; &lt;td align="left"&gt;.NET MLX wrapper&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nypq6q/made_the_first_net_wrapper_for_apple_mlx_looking/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/managedcode/MLXSharp"&gt;GH&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Video2X 6.x&lt;/td&gt; &lt;td align="left"&gt;upscaler + interpolation&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nykzv3/video2x_6x_opensource_upscaler_frame/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/k4yt3x/video2x"&gt;GH&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SurfSense&lt;/td&gt; &lt;td align="left"&gt;Perplexity alt.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o00ban/open_source_alternative_to_perplexity/"&gt;reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/MODSetter/SurfSense"&gt;GH&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aifeed-fyi"&gt; /u/aifeed-fyi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o33mui/a_list_of_models_released_or_updated_this_week_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o33mui/a_list_of_models_released_or_updated_this_week_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o33mui/a_list_of_models_released_or_updated_this_week_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T15:13:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2rppj</id>
    <title>Qwen3 VL 4B to be released?</title>
    <updated>2025-10-10T04:44:56+00:00</updated>
    <author>
      <name>/u/Signal-Run7450</name>
      <uri>https://old.reddit.com/user/Signal-Run7450</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2rppj/qwen3_vl_4b_to_be_released/"&gt; &lt;img alt="Qwen3 VL 4B to be released?" src="https://preview.redd.it/68mmis87s7uf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ab80165de7b5dc3e27595de513c638e5086c61d" title="Qwen3 VL 4B to be released?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen released cookbooks and in one of them this model Qwen3 VL 4B is present but I can't find it anywhere on huggingface. Link of the cookbook- &lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/long_document_understanding.ipynb"&gt;https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/long_document_understanding.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This would be quite amazing for OCR use cases. Qwen2.5/2 VL 3b/7b was foundation for many good OCR models&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Signal-Run7450"&gt; /u/Signal-Run7450 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/68mmis87s7uf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2rppj/qwen3_vl_4b_to_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2rppj/qwen3_vl_4b_to_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T04:44:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2wkyw</id>
    <title>We can now run wan or any heavy models even on a 6GB NVIDIA laptop GPU | Thanks to upcoming GDS integration in comfy</title>
    <updated>2025-10-10T09:50:47+00:00</updated>
    <author>
      <name>/u/maifee</name>
      <uri>https://old.reddit.com/user/maifee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2wkyw/we_can_now_run_wan_or_any_heavy_models_even_on_a/"&gt; &lt;img alt="We can now run wan or any heavy models even on a 6GB NVIDIA laptop GPU | Thanks to upcoming GDS integration in comfy" src="https://b.thumbs.redditmedia.com/p7rn0Ix6tXlo4gn9p81TJIfGWsn4o8QzjNEOzhKU77w.jpg" title="We can now run wan or any heavy models even on a 6GB NVIDIA laptop GPU | Thanks to upcoming GDS integration in comfy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello&lt;/p&gt; &lt;p&gt;I am Maifee. I am integrating GDS (GPU Direct Storage) in ComfyUI. And it's working, if you want to test, just do the following:&lt;/p&gt; &lt;p&gt;&lt;code&gt; git clone https://github.com/maifeeulasad/ComfyUI.git cd ComfyUI git checkout offloader-maifee python3 main.py --enable-gds --gds-stats # gds enabled run &lt;/code&gt;&lt;/p&gt; &lt;p&gt;And you no longer need custome offloader, or just be happy with quantized version. Or you don't even have to wait. Just run with GDS enabled flag and we are good to go. Everything will be handled for you. I have already created issue and raised MR, review is going on, hope this gets merged real quick.&lt;/p&gt; &lt;p&gt;If you have some suggestions or feedback, please let me know.&lt;/p&gt; &lt;p&gt;And thanks to these helpful sub reddits, where I got so many advices, and trust me it was always more than enough.&lt;/p&gt; &lt;p&gt;Enjoy your weekend!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maifee"&gt; /u/maifee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o2wkyw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2wkyw/we_can_now_run_wan_or_any_heavy_models_even_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2wkyw/we_can_now_run_wan_or_any_heavy_models_even_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T09:50:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o31rdl</id>
    <title>Kwaipilot/KAT-Dev-72B-Exp model released</title>
    <updated>2025-10-10T14:02:02+00:00</updated>
    <author>
      <name>/u/UltrMgns</name>
      <uri>https://old.reddit.com/user/UltrMgns</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The model makers claim it's second on coding only to Sonnet 4.5 at only 72B parameters.&lt;br /&gt; Could someone here who has the hardware to run it, validate this? &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp"&gt;https://huggingface.co/Kwaipilot/KAT-Dev-72B-Exp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UltrMgns"&gt; /u/UltrMgns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o31rdl/kwaipilotkatdev72bexp_model_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o31rdl/kwaipilotkatdev72bexp_model_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o31rdl/kwaipilotkatdev72bexp_model_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T14:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o2z1vt</id>
    <title>Reflection AI raises $2B to be America's open frontier AI lab, challenging DeepSeek | TechCrunch</title>
    <updated>2025-10-10T12:05:56+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2z1vt/reflection_ai_raises_2b_to_be_americas_open/"&gt; &lt;img alt="Reflection AI raises $2B to be America's open frontier AI lab, challenging DeepSeek | TechCrunch" src="https://external-preview.redd.it/J33KaNhavmUdkjELX9a4XHseBsKE9ltSNTN6hWuL1_c.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ecb75e4826fba31ddac14c79c56f0fceee48915" title="Reflection AI raises $2B to be America's open frontier AI lab, challenging DeepSeek | TechCrunch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Reflection AI: &lt;a href="https://reflection.ai/"&gt;https://reflection.ai/&lt;/a&gt;&lt;br /&gt; On ùïè: &lt;a href="https://x.com/reflection_ai/status/1976304405369520242"&gt;https://x.com/reflection_ai/status/1976304405369520242&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/10/09/reflection-raises-2b-to-be-americas-open-frontier-ai-lab-challenging-deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o2z1vt/reflection_ai_raises_2b_to_be_americas_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o2z1vt/reflection_ai_raises_2b_to_be_americas_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-10T12:05:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect ‚Äî Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect ‚Äî Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM ‚Äì 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
