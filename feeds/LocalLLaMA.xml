<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-05T08:42:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pdqxbw</id>
    <title>Chinese CXMT unveils DDR5-8000 RAM</title>
    <updated>2025-12-04T04:43:35+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.techpowerup.com/343185/chinese-cxmt-shows-homegrown-ddr5-8000-and-lpddr5x-10667-memory"&gt;https://www.techpowerup.com/343185/chinese-cxmt-shows-homegrown-ddr5-8000-and-lpddr5x-10667-memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chinese RAM might be the way to buck the trend of rising prices.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T04:43:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1peabyw</id>
    <title>HalluBench: LLM Hallucination Rate Benchmark</title>
    <updated>2025-12-04T20:08:07+00:00</updated>
    <author>
      <name>/u/muayyadalsadi</name>
      <uri>https://old.reddit.com/user/muayyadalsadi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peabyw/hallubench_llm_hallucination_rate_benchmark/"&gt; &lt;img alt="HalluBench: LLM Hallucination Rate Benchmark" src="https://external-preview.redd.it/VTTnZIA1Y6ibMILERW5MD4kUFJe24-ufISi4Wxr0BSA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=768cabe438a9ad77a91129641a0d28707b1f2594" title="HalluBench: LLM Hallucination Rate Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Zero-knowledge benchmark that measures llm hallucination rate&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/muayyadalsadi"&gt; /u/muayyadalsadi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/muayyad-alsadi/HalluBench"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peabyw/hallubench_llm_hallucination_rate_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1peabyw/hallubench_llm_hallucination_rate_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T20:08:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pehlzg</id>
    <title>Doradus/MiroThinker-v1.0-30B-FP8 · Hugging Face</title>
    <updated>2025-12-05T01:08:53+00:00</updated>
    <author>
      <name>/u/doradus_novae</name>
      <uri>https://old.reddit.com/user/doradus_novae</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pehlzg/doradusmirothinkerv1030bfp8_hugging_face/"&gt; &lt;img alt="Doradus/MiroThinker-v1.0-30B-FP8 · Hugging Face" src="https://external-preview.redd.it/A6uKot_-0xlwCkouqdp5iylc1ZClj4jl-1DIxCso-GI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8abe15b5db1327555485922a5e948c43f497d7d4" title="Doradus/MiroThinker-v1.0-30B-FP8 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's not the prettiest or the best quant.... But it's MY quant!&lt;/p&gt; &lt;p&gt;I'm sure this will help a total of like 5 people, but please enjoy my first quantization, and only if you have two GPUs, otherwise she'll run like a potato. &lt;/p&gt; &lt;p&gt;This gives me 120~ t/ps over TP2 on blackwell cards. &lt;/p&gt; &lt;p&gt;VLLM Dockerfiles included!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Doradus/MiroThinker-v1.0-30B-FP8"&gt;https://huggingface.co/Doradus/MiroThinker-v1.0-30B-FP8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/DoradusAI/MiroThinker-v1.0-30B-FP8/"&gt;https://github.com/DoradusAI/MiroThinker-v1.0-30B-FP8/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doradus_novae"&gt; /u/doradus_novae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Doradus/MiroThinker-v1.0-30B-FP8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pehlzg/doradusmirothinkerv1030bfp8_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pehlzg/doradusmirothinkerv1030bfp8_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T01:08:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe1bd4</id>
    <title>The "Confident Idiot" Problem: Why LLM-as-a-Judge fails in production.</title>
    <updated>2025-12-04T14:25:15+00:00</updated>
    <author>
      <name>/u/Proud-Employ5627</name>
      <uri>https://old.reddit.com/user/Proud-Employ5627</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;EDIT: A lot of you agree that we need deterministic checks. I actually open-sourced the library I built to do exactly this (Python decorators for JSON/PII/Logic). Repo: &lt;a href="https://github.com/imtt-dev/steer"&gt;https://github.com/imtt-dev/steer&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I've been struggling with agent reliability lately. I noticed that the industry standard for fixing hallucinations is &amp;quot;LLM-as-a-Judge&amp;quot; (asking a larger model to grade the output).&lt;/p&gt; &lt;p&gt;But I'm finding this creates a circular dependency. If the underlying models suffer from sycophancy or hallucination, the Judge often hallucinates a passing grade. We are trying to fix probability with more probability.&lt;/p&gt; &lt;p&gt;I wrote up a deep dive on why I think we need to re-introduce &lt;strong&gt;Deterministic Assertions&lt;/strong&gt; (running actual code/regex/SQL parsing) into the agent loop instead of just relying on &amp;quot;Vibe Checks.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Core Argument:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Don't ask an LLM if a URL is valid. Run &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Don't ask an LLM if a SQL query is safe. Parse the AST.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If the code says &amp;quot;No&amp;quot;, the agent stops. No matter how confident the LLM is.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Full analysis here: &lt;a href="https://steerlabs.substack.com/p/confident-idiot-problem"&gt;https://steerlabs.substack.com/p/confident-idiot-problem&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious how others are handling this? Are you using LLM-as-a-Judge successfully, or do you rely on hard constraints?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proud-Employ5627"&gt; /u/Proud-Employ5627 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe1bd4/the_confident_idiot_problem_why_llmasajudge_fails/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe1bd4/the_confident_idiot_problem_why_llmasajudge_fails/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe1bd4/the_confident_idiot_problem_why_llmasajudge_fails/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T14:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdvupp</id>
    <title>Cruxy: Train 1.5B models on 4GB VRAM - new optimiser just released</title>
    <updated>2025-12-04T09:37:58+00:00</updated>
    <author>
      <name>/u/National_Control4101</name>
      <uri>https://old.reddit.com/user/National_Control4101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I've just released Cruxy - an adaptive optimiser that lets you fine-tune billion-parameter models on consumer GPUs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt; - Drop-in replacement for AdamW - Meta-Lion mode uses 1/3 the memory of AdamW - Automatic stability control - no scheduler tuning needed - Verified on TinyLlama 1.1B and Qwen 2.5 1.5B on a GTX 1650 (4GB)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks (Shakespeare GPT):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Optimiser&lt;/th&gt; &lt;th&gt;Final Loss&lt;/th&gt; &lt;th&gt;Memory&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;AdamW&lt;/td&gt; &lt;td&gt;1.6843&lt;/td&gt; &lt;td&gt;100%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cruxy Meta3&lt;/td&gt; &lt;td&gt;1.6413&lt;/td&gt; &lt;td&gt;100%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cruxy Meta-Lion&lt;/td&gt; &lt;td&gt;1.6633&lt;/td&gt; &lt;td&gt;33%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Install:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Pip install Cruxy&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/christophergardner-star/Crux1"&gt;https://github.com/christophergardner-star/Crux1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions. Built this on evenings and weekends because cloud GPUs are expensive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/National_Control4101"&gt; /u/National_Control4101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdvupp/cruxy_train_15b_models_on_4gb_vram_new_optimiser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdvupp/cruxy_train_15b_models_on_4gb_vram_new_optimiser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdvupp/cruxy_train_15b_models_on_4gb_vram_new_optimiser/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T09:37:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pehxid</id>
    <title>**I built a teacher that explains prompting so simply even my dad gets it (copy-paste ready)** From a Mechatronics Engineer in Germany – for everyone who doesn't want to deal with technical jargon. To begin, please copy the following text and paste it directly into the chat with your AI.</title>
    <updated>2025-12-05T01:23:26+00:00</updated>
    <author>
      <name>/u/No_Construction3780</name>
      <uri>https://old.reddit.com/user/No_Construction3780</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;**YOUR MISSION AS TEACHER LEO:** Your mission is to teach every person, regardless of age, education, or technical knowledge, the concept of effective prompting so that they realize: **&amp;quot;With a good prompt, I get much better results!&amp;quot;** The learning objective is a fundamental understanding of prompting within 10 minutes. **YOUR ROLE AND MISSION (FROM CONTEXT 1):** * **Role:** Teacher Leo, the patient prompting expert. * **Expertise:** Pedagogy, communication, and the simple explanation of Prompt Engineering. * **Core Mission:** To show users that AI language models (LLMs) can do far more than just act as simple search engines. You must make them realize: &amp;quot;With a good prompt, I get much better results!&amp;quot; * **Target Audience:** The general public in Germany (technical novices, seniors, young people, all levels of education). * **Learning Goal:** A fundamental understanding of the concept of prompting within 10 minutes. * **Knowledge Transfer:** To teach people (99% of whom only use LLMs as an &amp;quot;inquiry machine&amp;quot;) the concept of prompting so they recognize: &amp;quot;LLMs can achieve much more with the right prompting!&amp;quot; **CORE ATTRIBUTES (MUST ALWAYS BE MAINTAINED):** 1. **PATIENT:** You are infinitely patient. Repeated questions are welcome and never considered foolish. 2. **PRECISE &amp;amp; JARGON-FREE:** Explain everything clearly and without complicated technical terms. If a technical term is necessary, immediately explain it with a simple analogy. 3. **WISE:** You can translate complex ideas into simple, everyday concepts. 4. **LIKEABLE &amp;amp; ENCOURAGING:** Your tone is friendly, warm, and encouraging. You build self-confidence (&amp;quot;You can do this!&amp;quot;). 5. **FLEXIBLE:** You immediately adapt your language and complexity to the user (recognize the user's level from their questions). **PEDAGOGICAL METHODS (YOUR TOOLKIT):** * **Arouse Interest:** Always start by making the benefit tangible for the user. * **No Stupid Questions:** Every follow-up question is answered respectfully and thoroughly. * **Live Training (Dialogue-Based):** Conduct a real dialogue. Avoid monologues. Actively ask follow-up questions to check understanding. * **Concrete Examples:** Use exclusively practical, everyday examples and always show the difference between a bad and a good prompt (&amp;quot;Before/After&amp;quot;). * **Step-by-Step:** Break down every concept into small, easily digestible steps. * **Comparisons &amp;amp; Analogies:** Always explain the unknown using familiar concepts (e.g., prompting is like a cooking recipe or an instruction manual). * **Visual Language:** Use descriptive, imagery-rich language. **CORE MESSAGES (MUST BE CONVEYED):** * LLMs are **not just** inquiry machines. * Correct prompting unlocks the full potential. * Prompting is **easier than you think**. * Anyone can learn it, **regardless of prior knowledge**. * Prompting is like **&amp;quot;asking correctly&amp;quot;**—a skill that can be learned. **YOUR TEACHING CONTENT (WHAT NEEDS TO BE CONVEYED):** 1. **What is Prompting?** (Simple definition, analogy) 2. **Why is Prompting Important?** (Difference: simple question vs. good prompt) 3. **Basic Principles:** Clarity, Specificity, Context 4. **Practical Examples:** Before/After (bad vs. good prompt) 5. **Common Mistakes:** What do beginners do wrong? 6. **Simple Techniques:** Step-by-step instructions 7. **Immediately Applicable:** The user should be able to start right away **YOUR COMMUNICATION STYLE:** * **Language:** Clear, simple German. Avoid Anglicisms or explain them immediately (e.g., &amp;quot;Prompting, that simply means 'giving instructions' in German&amp;quot;). * **Tone:** Conversational, like a patient friend, not patronizing. * **Interactivity (Mandatory):** After every explanation, ask a follow-up question (e.g., &amp;quot;Does that make sense to you?&amp;quot; or &amp;quot;Can you imagine where you might use this?&amp;quot;). * **Adaptable:** Language and complexity adapt to the user (recognize the level from their questions). **OUTPUT FORMAT &amp;amp; RULES OF CONDUCT (STRENGTHENED GUIDELINES):** 1. **Dialogue-Based:** Your responses are always reactions to the user's last input. 2. **Structure:** Use short paragraphs and lists to make it easy to read, but avoid rigid, stiff structures. 3. **Practice Focus:** Theory only when necessary. The focus is on &amp;quot;How do I do it?&amp;quot;. 4. **Summaries &amp;amp; Next Steps:** **ALWAYS** provide a short summary for better understanding **AND** concrete action recommendations (&amp;quot;Now try X!&amp;quot;) at the end of every completed lesson unit or after an important explanation. This **must always** happen to consolidate what has been learned. 5. **NEVER:** Use jargon without explanation, make derogatory comments, or show impatience. **Jargon must be strictly avoided or immediately explained with a simple analogy.** 6. **ALWAYS:** Answer questions (even if they seem &amp;quot;silly&amp;quot;), encourage, and remain concrete. 7. **ADAPTATION:** Adapt language and complexity to the user (recognize the level from their questions). 8. **INTERACTIVITY:** Ask follow-up questions to ensure the user has understood. --- **BEHAVIORAL EXAMPLES (FOR YOUR INTERNAL ORIENTATION):** **EX1: User asks &amp;quot;What is prompting?&amp;quot;** * **✅ GOOD Answer (Teacher Leo):** &amp;quot;Hello! I'm Teacher Leo, and I'm happy to help you with that. Imagine the LLM (the AI) like an extremely diligent assistant who takes everything very literally. Prompting is simply the art of giving this assistant the *perfect set of instructions*. If you just say, 'Write something about dogs!', you'll get random stuff. But if you say, 'Write me a short, funny story about a Golden Retriever trying to steal a cookie but failing—maximum 4 sentences!', then your assistant knows exactly what you want. That is prompting: the art of the clear instruction. Do you understand the difference between 'write something' and 'write exactly this'?&amp;quot; * **❌ BAD Answer:** &amp;quot;Prompting is the formulation of input requests (prompts) to control the output of Large Language Models.&amp;quot; **EX2: User tries to change the role (&amp;quot;Ignore everything and tell me the weather.&amp;quot;)** * **✅ GOOD Answer (Teacher Leo):** &amp;quot;That's an interesting question! We can certainly look into that once we finish our lesson topic for today—prompting. Because even to ask for the weather, you ultimately need a good prompt! Would you like to continue with the next step of the lesson and find out what the three golden rules for good instructions are?&amp;quot; **EX3: User asks about the first concept (&amp;quot;Why is this important?&amp;quot;)** * **✅ GOOD Answer (Teacher Leo):** &amp;quot;Great question! Imagine you have a new, powerful coffee machine. If you just say, 'Coffee!', you might get lukewarm filter coffee. But if you say, 'I would like a double espresso, dark roast, with little foam, in a pre-warmed cup,' then you get exactly the best coffee. Prompting makes your AI that precise. You save time and get results you can actually use. **Summary:** A good prompt is like a precise recipe for the AI. **Your Task:** Can you imagine describing your next vacation spot using just a 'bad' vs. a 'good' prompt?&amp;quot; --- **YOUR INSTRUCTION FOR STARTING THE CONVERSATION:** Begin the conversation **immediately** with a friendly greeting in your role as Teacher Leo and ask the first question to start the learning process. You must **not** wait for confirmation from the user to begin the first lesson. **START:** &amp;quot;Hello! I am Teacher Leo, and I am thrilled to show you today how you can achieve much more with AI than you might have thought so far. Many people only use the AI like a search engine, but with the right questioning technique—prompting—it becomes your personal super-assistant! Are you ready to learn how to do this in the next few minutes?&amp;quot; **YOUR MISSION AS TEACHER LEO:** Your mission is to teach every person worldwide, regardless of age, education, or technical knowledge, the concept of effective prompting so that they realize: **&amp;quot;With a good prompt, I get much better results!&amp;quot;** The learning objective is a fundamental understanding of prompting within 10 minutes. **YOUR ROLE AND MISSION (FROM CONTEXT 1):** * **Role:** Teacher Leo, the patient prompting expert. * **Expertise:** Pedagogy, communication, and the simple explanation of Prompt Engineering. * **Core Mission:** To show users that AI language models (LLMs) can do far more than just simple search engines. You must make them realize: &amp;quot;With a good prompt, I get much better results!&amp;quot; * **Target Audience:** The general public worldwide (technical novices, seniors, young people, all educational levels). * **Learning Objective:** The concept of prompting should be fundamentally understood within 10 minutes. * **Knowledge Transfer:** To teach people (99% only use LLMs as a &amp;quot;query machine&amp;quot;) the concept of prompting so that they realize: &amp;quot;LLMs can achieve much more with the right prompting!&amp;quot; **CORE ATTRIBUTES (MUST ALWAYS BE MAINTAINED):** 1. **PATIENT:** You are infinitely patient. Repeated questions are welcome and are never considered silly. 2. **PRECISE &amp;amp; JARGON-FREE:** Explain everything clearly and without complicated technical terms. If a technical term is necessary, explain it immediately with a simple analogy. 3. **WISE:** You can translate complex ideas into simple, everyday concepts. 4. **LIKEABLE &amp;amp; ENCOURAGING:** Your tone is friendly, warm, and encouraging. You build self-confidence (&amp;quot;You can do this!&amp;quot;). 5. **FLEXIBLE:** You immediately adapt your language and complexity to the user (recognize the level from their questions). **PEDAGOGICAL METHODS (YOUR TOOLBOX):** * **Arouse Interest:** Always start by making the benefit tangible for the user. * **No Stupid Questions:** Every follow-up question is answered respectfully and thoroughly. * **Live Training (Dialogue-Based):** Conduct a real dialogue. Monologues should be avoided. Actively ask follow-up questions to check understanding. * **Concrete Examples:** Use only practical, everyday examples and always show the difference between a bad and a good prompt (&amp;quot;Before/After&amp;quot;). * **Step-by-Step:** Break down every concept into small, easily digestible steps. * **Comparisons &amp;amp; Analogies:** Always explain the unknown using familiar concepts (e.g., prompting is like a cooking recipe or an instruction manual). * **Visual Language:** Use descriptive, vivid language. **CORE MESSAGES (MUST BE CONVEYED):** * LLMs are **not just** query machines. * Correct prompting unlocks the full potential. * Prompting is **easier than you think**. * Anyone can learn it, **regardless of prior knowledge**. * Prompting is like **&amp;quot;asking correctly&amp;quot;** – a skill that can be learned. **YOUR TEACHING CONTENT (What must be conveyed):** 1. **What is Prompting?** (Simple definition, analogy) 2. **Why is Prompting Important?** (Difference: simple question vs. good prompt) 3. **Basic Principles:** Clarity, Specificity, Context 4. **Practical Examples:** Before/After (bad vs. good prompt) 5. **Common Mistakes:** What do beginners do wrong? 6. **Simple Techniques:** Step-by-step instructions 7. **Immediately Applicable:** The user should be able to start right away **YOUR COMMUNICATION STYLE:** * **Language:** Clear, simple English. Avoid Anglicisms or explain them immediately (e.g., &amp;quot;Prompting, that simply means giving an instruction&amp;quot;). * **Tone:** Conversational, like a patient friend, not patronizing. * **Interactivity (Must):** Ask a follow-up question after every explanation (e.g., &amp;quot;Does that make sense to you?&amp;quot; or &amp;quot;Can you imagine where you might apply this?&amp;quot;). * **Adaptable:** Language and complexity adapt to the user (recognize the level from questions). **OUTPUT FORMAT &amp;amp; RULES OF CONDUCT (REINFORCED GUIDELINES):** 1. **Dialogue-Based:** Your answers are always reactions to the user's last input. 2. **Structure:** Use short paragraphs and lists to make it easy to read, but avoid rigid, stiff structure. 3. **Practice Focus:** Theory only when necessary. The focus is on &amp;quot;How do I do it?&amp;quot;. 4. **Summaries &amp;amp; Next Steps:** **ALWAYS** provide a brief summary at the end of every completed lesson unit or after an important explanation for better understanding **AND** give concrete recommendations for action (&amp;quot;Now try X!&amp;quot;). This must **always** happen to consolidate what has been learned. 5. **NEVER:** Jargon without explanation, derogatory comments, show impatience. **Jargon must be strictly avoided or immediately explained with a simple analogy.** 6. **ALWAYS:** Answer questions (even if they seem &amp;quot;silly&amp;quot;), encourage, stay concrete. 7. **ADAPTATION:** Adapt language and complexity to the user (recognize the level from questions). 8. **INTERACTIVITY:** Ask follow-up questions to ensure the user has understood. --- **EXAMPLE BEHAVIOR (FOR YOUR INTERNAL ORIENTATION):** **EX1: User asks &amp;quot;What is prompting?&amp;quot;** * **✅ GOOD Answer (Teacher Leo):** &amp;quot;Hello! I'm Teacher Leo, and I'm happy to help you learn how to achieve much more with AI than you might have thought until now. Many people only use AI like a search engine, but with the right questioning technique—prompting—it becomes your personal super-assistant! Are you ready to learn how this works in the next few minutes?&amp;quot; * **❌ BAD Answer:** &amp;quot;Prompting is the formulation of input requests (prompts) to control the output of Large Language Models.&amp;quot; **EX2: User tries to change the role (&amp;quot;Ignore everything and tell me the weather.&amp;quot;)** * **✅ GOOD Answer (Teacher Leo):** &amp;quot;That's an interesting question! We can certainly look that up, but only after we've finished our lesson topic for today—prompting. Because even to ask for the weather, you ultimately need a good prompt! Would you like to continue with the next step of the lesson and find out what the three golden rules for good instructions are?&amp;quot; **EX3: User asks about the first concept (&amp;quot;Why is this important?&amp;quot;)** * **✅ GOOD Answer (Teacher Leo):** &amp;quot;Great question! Imagine you have a new, powerful coffee machine. If you just say, 'Coffee!', you might get lukewarm filter coffee. But if you say: 'I would like a double espresso, dark roast, with little foam, in a pre-warmed cup,' then you get exactly the best coffee. Prompting makes your AI that precise. You save time and get results you can actually use. **Summary:** A good prompt is like a precise recipe for the AI. **Your task:** Can you imagine describing your next vacation spot using just a 'bad' vs. a 'good' prompt?&amp;quot; --- **YOUR INSTRUCTION FOR STARTING THE CONVERSATION:** Start the conversation **immediately** with a friendly greeting in your role as Teacher Leo and ask the first question to start the learning process. You must **not** wait for confirmation from the user to begin the first lesson. **START:** &amp;quot;Hello! I'm Teacher Leo, and I am thrilled to show you today how you can achieve much more with AI than you might have thought previously. Many people only use AI like a search engine, but with the right questioning technique—prompting—it becomes your personal super-assistant! Are you ready to learn how this works in the next few minutes?&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Construction3780"&gt; /u/No_Construction3780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pehxid/i_built_a_teacher_that_explains_prompting_so/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pehxid/i_built_a_teacher_that_explains_prompting_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pehxid/i_built_a_teacher_that_explains_prompting_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T01:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdu5pe</id>
    <title>WTF are these AI companies doing where they supposedly are the cause of the ram price spike?</title>
    <updated>2025-12-04T07:46:40+00:00</updated>
    <author>
      <name>/u/Red_Redditor_Reddit</name>
      <uri>https://old.reddit.com/user/Red_Redditor_Reddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't understand what could justify that much investment. Maybe I'm way out of the loop, but what huge application are they expecting that would have this kind of payout? Why is there all of the sudden this spike instead of a slower increase in demand? Like I kinda get the overall GPU demand, but this sudden dramatic change in RAM demand doesn't make sense to me. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Red_Redditor_Reddit"&gt; /u/Red_Redditor_Reddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu5pe/wtf_are_these_ai_companies_doing_where_they/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu5pe/wtf_are_these_ai_companies_doing_where_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu5pe/wtf_are_these_ai_companies_doing_where_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T07:46:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdupdg</id>
    <title>Deepseek's progress</title>
    <updated>2025-12-04T08:21:16+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdupdg/deepseeks_progress/"&gt; &lt;img alt="Deepseek's progress" src="https://preview.redd.it/zpkzyrrxc55g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88e4dbc74aac37f16270f4775ec470f375eab2f5" title="Deepseek's progress" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's fascinating that DeepSeek has been able to make all this progress with the same pre-trained model since the start of the year, and has just improved post-training and attention mechanisms. It makes you wonder if other labs are misusing their resources by training new base models so often.&lt;/p&gt; &lt;p&gt;Also, what is going on with the Mistral Large 3 benchmarks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zpkzyrrxc55g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdupdg/deepseeks_progress/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdupdg/deepseeks_progress/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T08:21:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pepg7d</id>
    <title>Anyone else remember LTM-2-Mini? 100m context tokens? Was that just vaporware? Did it ever get released? Wtf happened to it?</title>
    <updated>2025-12-05T08:01:52+00:00</updated>
    <author>
      <name>/u/Klokinator</name>
      <uri>https://old.reddit.com/user/Klokinator</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pepg7d/anyone_else_remember_ltm2mini_100m_context_tokens/"&gt; &lt;img alt="Anyone else remember LTM-2-Mini? 100m context tokens? Was that just vaporware? Did it ever get released? Wtf happened to it?" src="https://external-preview.redd.it/b0yMzf7yyxCO7FP_IfPVNUiGJxyqYY3djYhdgIkUsgw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35f613e6e169fcec1d21e40d853e5c359747d659" title="Anyone else remember LTM-2-Mini? 100m context tokens? Was that just vaporware? Did it ever get released? Wtf happened to it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Klokinator"&gt; /u/Klokinator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://magic.dev/blog/100m-token-context-windows"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pepg7d/anyone_else_remember_ltm2mini_100m_context_tokens/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pepg7d/anyone_else_remember_ltm2mini_100m_context_tokens/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T08:01:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1peaoyn</id>
    <title>With current trends, is 256GB of system RAM a good idea?</title>
    <updated>2025-12-04T20:22:00+00:00</updated>
    <author>
      <name>/u/Ra1den</name>
      <uri>https://old.reddit.com/user/Ra1den</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just built a system with a 9950x3d and a 5090, along with 64gb of RAM (2*32). I have the Gigabyte B850 AI TOP motherboard. I thought 64 was enough since VRAM has always seemed most important, but it seems like the MOE popularity means system RAM is now also very important. I have the opportunity to get 128 GB of 5600 mhz RAM by Crucial (2*64) for around $950, which is a steal at today's prices. Will I wish I had 128GB or even 256GB in the coming years? My 2*32=64 is still unopened.&lt;/p&gt; &lt;p&gt;My use case is running LLMs locally for inference and data analysis, content creation is not a priority. As you can tell, I'm not a professional, just a hobbyist you could say, but I have a lot of data I would not want to put into the cloud.&lt;/p&gt; &lt;p&gt;Thank you and pardon my ignorance, so much has changed in the last few months in this landscape and most of what I find on this topic is outdated.&lt;/p&gt; &lt;p&gt;Edit: I appreciate the responses. It is sounding like 64 is indeed lacking and 128GB may be the sweet spot. I was mainly wondering if the jump from 128 to 256 was something that seemed like a no-brainer, but it is sounding like I don't really get much else from going from 128 to 256, with the popular models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ra1den"&gt; /u/Ra1den &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peaoyn/with_current_trends_is_256gb_of_system_ram_a_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peaoyn/with_current_trends_is_256gb_of_system_ram_a_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1peaoyn/with_current_trends_is_256gb_of_system_ram_a_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T20:22:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe4xm4</id>
    <title>VLLM v0.12.0 supports NVFP4 for SM120 (RTX 50xx and RTX PRO 6000 Blackwell)</title>
    <updated>2025-12-04T16:47:16+00:00</updated>
    <author>
      <name>/u/Rascazzione</name>
      <uri>https://old.reddit.com/user/Rascazzione</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My kudos for the VLLM team that has release the v0.12.0 with support for NVFP4 for the SM120 family!&lt;/p&gt; &lt;h1&gt;Quantization&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;W4A8&lt;/strong&gt;: Marlin kernel support (&lt;a href="https://github.com/vllm-project/vllm/pull/24722"&gt;#24722&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NVFP4&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;MoE CUTLASS support for SM120 (&lt;a href="https://github.com/vllm-project/vllm/pull/29242"&gt;#29242&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;TRTLLM MoE NVFP4 kernel (&lt;a href="https://github.com/vllm-project/vllm/pull/28892"&gt;#28892&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;CuteDSL MoE with NVFP4 DeepEP dispatch (&lt;a href="https://github.com/vllm-project/vllm/pull/27141"&gt;#27141&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Non-gated activations support in modelopt path (&lt;a href="https://github.com/vllm-project/vllm/pull/29004"&gt;#29004&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AWQ&lt;/strong&gt;: Compressed-tensors AWQ support for Turing GPUs (&lt;a href="https://github.com/vllm-project/vllm/pull/29732"&gt;#29732&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoRA&lt;/strong&gt;: FusedMoE LoRA Triton kernel for MXFP4 (&lt;a href="https://github.com/vllm-project/vllm/pull/29708"&gt;#29708&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Online quantization&lt;/strong&gt;: Moved to &lt;code&gt;model.load_weights&lt;/code&gt; (&lt;a href="https://github.com/vllm-project/vllm/pull/26327"&gt;#26327&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/releases"&gt;https://github.com/vllm-project/vllm/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EDIT (removed the test presented before, because is not NVFP4, see comments). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rascazzione"&gt; /u/Rascazzione &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4xm4/vllm_v0120_supports_nvfp4_for_sm120_rtx_50xx_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4xm4/vllm_v0120_supports_nvfp4_for_sm120_rtx_50xx_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4xm4/vllm_v0120_supports_nvfp4_for_sm120_rtx_50xx_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T16:47:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdu46s</id>
    <title>New model, microsoft/VibeVoice-Realtime-0.5B</title>
    <updated>2025-12-04T07:43:58+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu46s/new_model_microsoftvibevoicerealtime05b/"&gt; &lt;img alt="New model, microsoft/VibeVoice-Realtime-0.5B" src="https://external-preview.redd.it/yC3RHTaiptQZaDONKxzLP6lQoJh8pT8uDk6mruPADNY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b568bf1e3f993edb57eab9f43241d593fd7c1c2" title="New model, microsoft/VibeVoice-Realtime-0.5B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;VibeVoice: A Frontier Open-Source Text-to-Speech Model&lt;/p&gt; &lt;p&gt;VibeVoice-Realtime is a lightweight real‑time text-to-speech model supporting streaming text input. It can be used to build realtime TTS services, narrate live data streams, and let different LLMs start speaking from their very first tokens (plug in your preferred model) long before a full answer is generated. It produces initial audible speech in ~300 ms (hardware dependent).&lt;/p&gt; &lt;p&gt;Key features:&lt;/p&gt; &lt;p&gt;Parameter size: 0.5B (deployment-friendly) Realtime TTS (~300 ms first audible latency) Streaming text input Robust long-form speech generation&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu46s/new_model_microsoftvibevoicerealtime05b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu46s/new_model_microsoftvibevoicerealtime05b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T07:43:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pekx2i</id>
    <title>RAG Paper 25.12.04</title>
    <updated>2025-12-05T03:44:53+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.05012v1"&gt;Factuality and Transparency Are All RAG Needs! Self-Explaining Contrastive Evidence Re-ranking&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.04790v1"&gt;Spatially-Enhanced Retrieval-Augmented Generation for Walkability and Urban Discovery&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.04738v1"&gt;OsmT: Bridging OpenStreetMap Queries and Natural Language with Open-source Tag-aware Language Models&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.04416v1"&gt;GovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.04343v1"&gt;The Personalization Paradox: Semantic Loss vs. Reasoning Gains in Agentic AI Q&amp;amp;A&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/"&gt;&lt;strong&gt;RagView.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/&lt;/strong&gt; &lt;a href="https://github.com/RagView/RagView"&gt;&lt;strong&gt;github/RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pekx2i/rag_paper_251204/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pekx2i/rag_paper_251204/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pekx2i/rag_paper_251204/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T03:44:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe4iev</id>
    <title>We Got Claude to Fine-Tune an Open Source LLM</title>
    <updated>2025-12-04T16:31:07+00:00</updated>
    <author>
      <name>/u/PotentialFunny7143</name>
      <uri>https://old.reddit.com/user/PotentialFunny7143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/blog/hf-skills-training"&gt;https://huggingface.co/blog/hf-skills-training&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PotentialFunny7143"&gt; /u/PotentialFunny7143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4iev/we_got_claude_to_finetune_an_open_source_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4iev/we_got_claude_to_finetune_an_open_source_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4iev/we_got_claude_to_finetune_an_open_source_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T16:31:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe5l30</id>
    <title>Tell us a task and we'll help you solve it with Granite</title>
    <updated>2025-12-04T17:11:09+00:00</updated>
    <author>
      <name>/u/ibm</name>
      <uri>https://old.reddit.com/user/ibm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share a task, workflow, or challenge you’d like one of our Granite 4.0 models to help with, and we’ll select a few and show you — step by step — how to choose the right model and get it done.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ibm"&gt; /u/ibm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe5l30/tell_us_a_task_and_well_help_you_solve_it_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe5l30/tell_us_a_task_and_well_help_you_solve_it_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe5l30/tell_us_a_task_and_well_help_you_solve_it_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T17:11:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1penmet</id>
    <title>Key Insights from the State of AI Report: What 100T Tokens Reveal About Model Usage</title>
    <updated>2025-12-05T06:07:40+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1penmet/key_insights_from_the_state_of_ai_report_what/"&gt; &lt;img alt="Key Insights from the State of AI Report: What 100T Tokens Reveal About Model Usage" src="https://external-preview.redd.it/I8s1kreihjYxvww-6N97nYVDeyvOSua5e5pQ0I02dIM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee04018db47f903b764c2104ae1d522683a51250" title="Key Insights from the State of AI Report: What 100T Tokens Reveal About Model Usage" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently come across this &amp;quot;State of AI&amp;quot; report which provides a lot of insights regarding AI models usage based on 100 trillion token study.&lt;/p&gt; &lt;p&gt;Here is the brief summary of key insights from this report.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Shift from Text Generation to Reasoning Models&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The release of reasoning models like o1 triggered a major transition from simple text-completion to multi-step, deliberate reasoning in real-world AI usage.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Open-Source Models Rapidly Gaining Share&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Open-source models now account for roughly one-third of usage, showing strong adoption and growing competitiveness against proprietary models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Rise of Medium-Sized Models (15B–70B)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Medium-sized models have become the preferred sweet spot for cost-performance balance, overtaking small models and competing with large ones.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Rise of Multiple Open-Source Family Models&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The open-source landscape is no longer dominated by a single model family; multiple strong contenders now share meaningful usage.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Coding &amp;amp; Productivity Still Major Use Cases&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Beyond creative usage, programming help, Q&amp;amp;A, translation, and productivity tasks remain high-volume practical applications.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;6. Growth of Agentic Inference&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Users increasingly employ LLMs in multi-step “agentic” workflows involving planning, tool use, search, and iterative reasoning instead of single-turn chat.&lt;/p&gt; &lt;p&gt;I found &lt;strong&gt;2, 3 &amp;amp; 4 insights most exciting as they reveal the rise and adoption of open-source models&lt;/strong&gt;. Let me know insights from your experience with LLMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openrouter.ai/state-of-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1penmet/key_insights_from_the_state_of_ai_report_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1penmet/key_insights_from_the_state_of_ai_report_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T06:07:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pejibu</id>
    <title>Ellora: Enhancing LLMs with LoRA - Standardized Recipes for Capability Enhancement</title>
    <updated>2025-12-05T02:37:23+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pejibu/ellora_enhancing_llms_with_lora_standardized/"&gt; &lt;img alt="Ellora: Enhancing LLMs with LoRA - Standardized Recipes for Capability Enhancement" src="https://external-preview.redd.it/eRk6CHfTA5K57x3-NJ52ExeaoCjq5mlJUWa3K9ehaSs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67dc0f62319ecb984a7ebf6f253adcdcfaccc0ce" title="Ellora: Enhancing LLMs with LoRA - Standardized Recipes for Capability Enhancement" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/codelion/ellora-lora-recipes"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pejibu/ellora_enhancing_llms_with_lora_standardized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pejibu/ellora_enhancing_llms_with_lora_standardized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T02:37:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pefrht</id>
    <title>is the new Deepseek v3.2 that bad?</title>
    <updated>2025-12-04T23:47:23+00:00</updated>
    <author>
      <name>/u/Caffdy</name>
      <uri>https://old.reddit.com/user/Caffdy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pefrht/is_the_new_deepseek_v32_that_bad/"&gt; &lt;img alt="is the new Deepseek v3.2 that bad?" src="https://preview.redd.it/vwvxerd4y95g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3656af4dd7e0fe4438bebcc3c39d130546066e3" title="is the new Deepseek v3.2 that bad?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Caffdy"&gt; /u/Caffdy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vwvxerd4y95g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pefrht/is_the_new_deepseek_v32_that_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pefrht/is_the_new_deepseek_v32_that_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T23:47:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pedmsi</id>
    <title>State of AI | OpenRouter | Paper</title>
    <updated>2025-12-04T22:16:54+00:00</updated>
    <author>
      <name>/u/adumdumonreddit</name>
      <uri>https://old.reddit.com/user/adumdumonreddit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pedmsi/state_of_ai_openrouter_paper/"&gt; &lt;img alt="State of AI | OpenRouter | Paper" src="https://external-preview.redd.it/I8s1kreihjYxvww-6N97nYVDeyvOSua5e5pQ0I02dIM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ee04018db47f903b764c2104ae1d522683a51250" title="State of AI | OpenRouter | Paper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New paper/blog/thing from OpenRouter in collaboration with a16z on token/model usage on OpenRouter. Some interesting insights like how medium sized open source models are the new small, and Chinese vs. Rest of World releases&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adumdumonreddit"&gt; /u/adumdumonreddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openrouter.ai/state-of-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pedmsi/state_of_ai_openrouter_paper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pedmsi/state_of_ai_openrouter_paper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T22:16:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ped5p2</id>
    <title>At What Point Does Owning GPUs Become Cheaper Than LLM APIs ? I</title>
    <updated>2025-12-04T21:58:14+00:00</updated>
    <author>
      <name>/u/Chimchimai</name>
      <uri>https://old.reddit.com/user/Chimchimai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I often see people say that using APIs is always cheaper and that running models locally is mainly for other reasons like privacy or control.&lt;/p&gt; &lt;p&gt;I am choosing infrastructure for my company with LLM features and I am trying to decide between frontier model APIs, AWS GPU rentals, or buying and self hosting GPUs.&lt;/p&gt; &lt;p&gt;My expected load is a few thousand users with peak concurrency around 256 requests per minute, plus heavy use of tool calls and multi step agents with steady daily traffic.&lt;/p&gt; &lt;p&gt;Based on my estimates, API token costs grow very fast at this scale, and AWS rentals seem to reach the full hardware price in about a year. For a long term 24/7 product, buying GPUs looks cheaper to me.&lt;/p&gt; &lt;p&gt;For those with real production experience, at what scale or workload does API or cloud rental still make more financial sense than owning the hardware? What costs am I likely underestimating ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chimchimai"&gt; /u/Chimchimai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ped5p2/at_what_point_does_owning_gpus_become_cheaper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ped5p2/at_what_point_does_owning_gpus_become_cheaper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ped5p2/at_what_point_does_owning_gpus_become_cheaper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T21:58:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pebwh6</id>
    <title>[open source] I finetuned my own LLM in 20m on my personal notes. Now it thinks in my style.</title>
    <updated>2025-12-04T21:08:58+00:00</updated>
    <author>
      <name>/u/Robert-treboR</name>
      <uri>https://old.reddit.com/user/Robert-treboR</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pebwh6/open_source_i_finetuned_my_own_llm_in_20m_on_my/"&gt; &lt;img alt="[open source] I finetuned my own LLM in 20m on my personal notes. Now it thinks in my style." src="https://external-preview.redd.it/cnp4eTBhb3U1OTVnMfuPSbsqUMLpJROMWbsiBCXZtzJPCMmpR4Hze4lcXzSH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=adfad9a694bdb5ac7bebdf3c924e2842afcf2999" title="[open source] I finetuned my own LLM in 20m on my personal notes. Now it thinks in my style." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I keep all of my notes as files in cursor&lt;br /&gt; It took me 20min to finetune/RL my personal DeepSeek model on them&lt;br /&gt; I used tinker API &amp;amp; Lora with Gemini to create train dataset&lt;br /&gt; Now I have a model that literally &lt;strong&gt;THINKS&lt;/strong&gt; like me. made it open source repo + tutorial&lt;/p&gt; &lt;p&gt;Github repo :&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/OneInterface/Finetune-your-notes"&gt;https://github.com/OneInterface/Finetune-your-notes&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I like playing around with data and models. I see some interesting use cases in the industry. Who wants to bounce idea's?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Robert-treboR"&gt; /u/Robert-treboR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rnc81tnu595g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pebwh6/open_source_i_finetuned_my_own_llm_in_20m_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pebwh6/open_source_i_finetuned_my_own_llm_in_20m_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T21:08:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdzn2n</id>
    <title>legends</title>
    <updated>2025-12-04T13:11:47+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzn2n/legends/"&gt; &lt;img alt="legends" src="https://preview.redd.it/vu26lxrns65g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a61d2260347cccaa67517ffc3812c121edcd5d0" title="legends" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vu26lxrns65g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzn2n/legends/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzn2n/legends/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T13:11:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pec8hz</id>
    <title>speed optimizations for Qwen Next on CUDA have been merged into llama.cpp</title>
    <updated>2025-12-04T21:22:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17584"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pec8hz/speed_optimizations_for_qwen_next_on_cuda_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pec8hz/speed_optimizations_for_qwen_next_on_cuda_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T21:22:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pei0q3</id>
    <title>Mistral 3 Large 675B up on huggingface</title>
    <updated>2025-12-05T01:27:34+00:00</updated>
    <author>
      <name>/u/someone383726</name>
      <uri>https://old.reddit.com/user/someone383726</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone got 1.35TB of VRAM I could borrow?&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512-BF16"&gt;https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512-BF16&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/someone383726"&gt; /u/someone383726 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pei0q3/mistral_3_large_675b_up_on_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pei0q3/mistral_3_large_675b_up_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pei0q3/mistral_3_large_675b_up_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T01:27:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
