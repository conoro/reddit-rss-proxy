<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-13T14:54:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r3qm9u</id>
    <title>ELO leaderboard for LLMs playing games (non-thinking only): Llama vs Mistral vs Claude vs Gemini [4200+ matches]</title>
    <updated>2026-02-13T14:29:27+00:00</updated>
    <author>
      <name>/u/stef_1982</name>
      <uri>https://old.reddit.com/user/stef_1982</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;We built a platform where **humans play strategy games against LLMs**, using an ELO system to rank them (like LMSYS but for gameplay). **The key rule: non-thinking models only.** All models play in instant response mode - no chain-of-thought, no extended reasoning, no compute-heavy thinking chains. We test raw strategic ability: can the model make good decisions without extra thinking time? This also reflects real-world deployment where you need sub-second decisions (chatbots, trading, robotics) - not 30-second thinking pauses. **Current overall standings after 4200+ matches** (log-weighted ELO across all games): - Top: Gemini 3 Flash Preview (1145), Claude Opus 4.5 (1145) - Open-source: Llama 4 Maverick (1079), GLM 4.7 (1058), Llama 4 Scout (1040), Mistral Large 3 (1022) - Humans still winning ~83% of games **Games:** TicTacToe, Connect4, Battleship, Mastermind, WordDuel, Dots and Boxes **Why it's interesting:** Tests spatial reasoning, strategic planning, and rule adherence with zero-shot responses only. Some models with great benchmark scores struggle with basic game rules when they can't reason their way through it step by step. **Try it:** playtheai.com (free, no account) **Question:** Which open-source models should we add? Especially interested in smaller/quantized models that might surprise us in instant-response mode. Curious what the community thinks about using gameplay as a non-thinking reasoning benchmark? &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/stef_1982"&gt; /u/stef_1982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3qm9u/elo_leaderboard_for_llms_playing_games/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3qm9u/elo_leaderboard_for_llms_playing_games/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3qm9u/elo_leaderboard_for_llms_playing_games/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T14:29:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3aod7</id>
    <title>Qwen3 Coder Next : Loop Fix</title>
    <updated>2026-02-13T00:36:48+00:00</updated>
    <author>
      <name>/u/TBG______</name>
      <uri>https://old.reddit.com/user/TBG______</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;My Optimal llama.cpp Settings for Qwen3-Coder-Next After 1 Day of Testing&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As many of you have noted, the new Qwen3 Next models tend to get stuck in repetitive loops quite frequently. Additionally, both the coder and instruct variants with standard temperature settings can be overly creative - often initiating new tasks without being asked. For example, when you request &amp;quot;change the this in A,&amp;quot; it might decide to change multiple other Leters as well, which isn't always what we need.&lt;/p&gt; &lt;p&gt;After a full day of testing, I've found these settings work best for Qwen3-Coder-Next with llama.cpp to prevent loops and reduce unwanted creativity:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# This is the Loop Fix --temp 0.8 # default 1 was to creative for me --top-p 0.95 --min-p 0.01 --top-k 40 --presence-penalty 1.10 --dry-multiplier 0.5 --dry-allowed-length 5 --frequency_penalty 0.5&amp;quot; # This is for my system and Qwen3-Coder-Next-MXFP4_MOE so it fits all in my 2 GPUs with ctx 256k --cache-type-k q8_0 --cache-type-v q8_0 --threads 64 --threads-batch 64 --n-gpu-layers 999 ( you can just use --fit on) --n-cpu-moe 0 ( you can just use --fit on) --batch-size 2048 --ubatch-size 512&amp;quot; --parallel 1 # And the rest --model %MODEL% --alias %ALIAS% --host 0.0.0.0 --port 8080 --ctx-size %CTX% --jinja --flash-attn on --context-shift --cache-ram -1 (optional unlimited ram for cache ) Select ctx-size: 1) 32768 (32k) 2) 65536 (64k) 3) 98304 (96k) 4) 131072 (128k) 5) 180224 (180k) 6) 196608 (196K) 7) 202752 (200k) 8) 262144 (256k) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These parameters help keep the model focused on the actual task without going off on tangents or getting stuck repeating itself.&lt;/p&gt; &lt;p&gt;Stats: promt 1400 t/s | gen 30-38 t/s Windows WSL (way faster in wsl than in windos native 24 to 28 t/s) 3090RTX +5090RTX&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TBG______"&gt; /u/TBG______ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3aod7/qwen3_coder_next_loop_fix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3aod7/qwen3_coder_next_loop_fix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3aod7/qwen3_coder_next_loop_fix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T00:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1r35ceo</id>
    <title>GLM-5 and Minimax-2.5 on Fiction.liveBench</title>
    <updated>2026-02-12T21:01:32+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35ceo/glm5_and_minimax25_on_fictionlivebench/"&gt; &lt;img alt="GLM-5 and Minimax-2.5 on Fiction.liveBench" src="https://preview.redd.it/4390rts4o4jg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72aeaba258795dc87fe96ebe0ff21b86947a9bfd" title="GLM-5 and Minimax-2.5 on Fiction.liveBench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4390rts4o4jg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35ceo/glm5_and_minimax25_on_fictionlivebench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r35ceo/glm5_and_minimax25_on_fictionlivebench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T21:01:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3lwk5</id>
    <title>Google Releases Conductor</title>
    <updated>2026-02-13T10:38:09+00:00</updated>
    <author>
      <name>/u/techlatest_net</name>
      <uri>https://old.reddit.com/user/techlatest_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Google Releases Conductor: a context-driven Gemini CLI extension that stores knowledge as Markdown and orchestrates agentic workflows&lt;/h1&gt; &lt;p&gt;Link: &lt;a href="https://github.com/gemini-cli-extensions/conductor"&gt;https://github.com/gemini-cli-extensions/conductor&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techlatest_net"&gt; /u/techlatest_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3lwk5/google_releases_conductor/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3lwk5/google_releases_conductor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3lwk5/google_releases_conductor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T10:38:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1r35326</id>
    <title>I'm playing telephone pictionary with LLMs, VLMs, SDs, and Kokoro on my Strix Halo</title>
    <updated>2026-02-12T20:51:36+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35326/im_playing_telephone_pictionary_with_llms_vlms/"&gt; &lt;img alt="I'm playing telephone pictionary with LLMs, VLMs, SDs, and Kokoro on my Strix Halo" src="https://external-preview.redd.it/YmJmc3hpcWVtNGpnMZDShp7-xGpOcgsVOxorkEUrrQwTSNVbCBVhROxXE8sP.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6045e2eeee4aab8952d17984a247e68ed71ffb61" title="I'm playing telephone pictionary with LLMs, VLMs, SDs, and Kokoro on my Strix Halo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/txyz48qem4jg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35326/im_playing_telephone_pictionary_with_llms_vlms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r35326/im_playing_telephone_pictionary_with_llms_vlms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T20:51:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r30bgz</id>
    <title>Ming-flash-omni-2.0: 100B MoE (6B active) omni-modal model - unified speech/SFX/music generation</title>
    <updated>2026-02-12T17:54:35+00:00</updated>
    <author>
      <name>/u/bobeeeeeeeee8964</name>
      <uri>https://old.reddit.com/user/bobeeeeeeeee8964</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r30bgz/mingflashomni20_100b_moe_6b_active_omnimodal/"&gt; &lt;img alt="Ming-flash-omni-2.0: 100B MoE (6B active) omni-modal model - unified speech/SFX/music generation" src="https://external-preview.redd.it/_l8FEwEfj_HhNLZzTpSlQTuaBTUdY25FimxgyYeDN_Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59d3dea77c3801617c49eea3f48b88a3ec1ddde3" title="Ming-flash-omni-2.0: 100B MoE (6B active) omni-modal model - unified speech/SFX/music generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ant Group just open-sourced Ming-flash-omni-2.0, a true (omni-modal) model: image + text + video + audio input ‚Üí image + text + audio output, all in one unified architecture. Looks realy interesting. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobeeeeeeeee8964"&gt; /u/bobeeeeeeeee8964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-flash-omni-2.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r30bgz/mingflashomni20_100b_moe_6b_active_omnimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r30bgz/mingflashomni20_100b_moe_6b_active_omnimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T17:54:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3kzz2</id>
    <title>How is the quality of recent TTS ?</title>
    <updated>2026-02-13T09:42:10+00:00</updated>
    <author>
      <name>/u/TheRealistDude</name>
      <uri>https://old.reddit.com/user/TheRealistDude</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you think of quality of recent TTS like Moss TTS , Mio TTS? &lt;/p&gt; &lt;p&gt;Are these better than Qwen3 or something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealistDude"&gt; /u/TheRealistDude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzz2/how_is_the_quality_of_recent_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzz2/how_is_the_quality_of_recent_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzz2/how_is_the_quality_of_recent_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T09:42:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2ygac</id>
    <title>Why do we allow "un-local" content</title>
    <updated>2026-02-12T16:45:57+00:00</updated>
    <author>
      <name>/u/JacketHistorical2321</name>
      <uri>https://old.reddit.com/user/JacketHistorical2321</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title somewhat says it all. I get that it's related but if links to new models are being discussed shouldn't it be a requirement that there be a &amp;quot;local&amp;quot; component?&lt;/p&gt; &lt;p&gt;Edit: since this is starting to get some traction I want to be a little more specific with what I'm talking about. &lt;/p&gt; &lt;p&gt;In the past 2 to 3 days we've seen multiple posts related to new models being released. They include links to API resources prior to weights being released. &lt;/p&gt; &lt;p&gt;I believe that if a post includes a link to API serving hosts then it should be requirement that a hugging face link is also included. If both of these requirements cannot be met for any reason (ex. Weights will probably be released but have not been released yet) the post should be taken down. &lt;/p&gt; &lt;p&gt;This would at least put some guardrails in place that would make sure posts are closer to the true nature of this sub as opposed to being low-key marketing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JacketHistorical2321"&gt; /u/JacketHistorical2321 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2ygac/why_do_we_allow_unlocal_content/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2ygac/why_do_we_allow_unlocal_content/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2ygac/why_do_we_allow_unlocal_content/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T16:45:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3gjx5</id>
    <title>ZwZ 8B/7B/4B</title>
    <updated>2026-02-13T05:17:17+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3gjx5/zwz_8b7b4b/"&gt; &lt;img alt="ZwZ 8B/7B/4B" src="https://preview.redd.it/0qvadyln47jg1.png?width=140&amp;amp;height=72&amp;amp;auto=webp&amp;amp;s=05efe6fee87739b20abf6a144df17c59c6612f79" title="ZwZ 8B/7B/4B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-8B#model-summary"&gt;&lt;/a&gt;Model Summary&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;ZwZ-8B&lt;/strong&gt; is a fine-grained multimodal perception model built upon &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B"&gt;Qwen3-VL-8B&lt;/a&gt;. It is trained using &lt;strong&gt;Region-to-Image Distillation (R2I)&lt;/strong&gt; combined with reinforcement learning, enabling superior fine-grained visual understanding in a single forward pass ‚Äî no inference-time zooming or tool calling required.&lt;/p&gt; &lt;p&gt;ZwZ-8B achieves state-of-the-art performance on fine-grained perception benchmarks among open-source models of comparable size, while also demonstrating strong out-of-distribution generalization on visual reasoning, GUI agent, and AIGC detection tasks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0qvadyln47jg1.png?width=3461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b9d12949205d2c9015be9a120643d5298548e6b"&gt;https://preview.redd.it/0qvadyln47jg1.png?width=3461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b9d12949205d2c9015be9a120643d5298548e6b&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-8B#key-features"&gt;&lt;/a&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;‚ö° Single-Pass Efficiency&lt;/strong&gt;: Achieves fine-grained perception in one forward pass, eliminating inference-time tool-calling overhead&lt;/li&gt; &lt;li&gt;&lt;strong&gt;üéØ Superior Accuracy&lt;/strong&gt;: State-of-the-art on perception benchmarks among open-source models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;üìà Broad Improvements&lt;/strong&gt;: Enhances not only perception benchmarks but also out-of-distribution generalization on visual reasoning, GUI agent, and AIGC detection&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-8B#how-it-works"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;How It Works&lt;/h1&gt; &lt;p&gt;Traditional &amp;quot;Thinking-with-Images&amp;quot; methods zoom into regions of interest during inference, incurring high latency from repeated tool calls and visual re-encoding. &lt;strong&gt;ZwZ&lt;/strong&gt; transforms zooming from an inference-time tool into a training-time primitive:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Zoom in&lt;/strong&gt; to micro-cropped regions and let strong teacher models (Qwen3-VL-235B, GLM-4.5V) generate high-quality VQA data&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Distill&lt;/strong&gt; this region-grounded supervision back to the full image with explicit bounding-box overlays&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reinforce&lt;/strong&gt; via RL training to enable single-glance fine-grained perception without tool use&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-8B"&gt;https://huggingface.co/inclusionAI/ZwZ-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-7B"&gt;https://huggingface.co/inclusionAI/ZwZ-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/ZwZ-4B"&gt;https://huggingface.co/inclusionAI/ZwZ-4B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3gjx5/zwz_8b7b4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3gjx5/zwz_8b7b4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3gjx5/zwz_8b7b4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T05:17:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1r397hi</id>
    <title>Step 3.5 Flash is a beast?</title>
    <updated>2026-02-12T23:33:47+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have not used it on serious tasks until today.&lt;/p&gt; &lt;p&gt;I gave it a complex task of merging, it worked through it and stayed completely sane even at 90k context and successfully finished the task. It felt so gut, I double checked that I am not running a closed source frontier model like claude 4.6.&lt;/p&gt; &lt;p&gt;I mean, for agentic tasks, this is definitely better than Gemini 3.0 Preview. And it's so fast.&lt;/p&gt; &lt;p&gt;I tested it on opencode and claude code (I don't use it, just wanted to see how flexible it is, and also found out setting up non anthropic model is a pain in the ass) and it did great in both.&lt;/p&gt; &lt;p&gt;What is your experience? Do we have open weight model that is in real world tasks better than gemini 3.0 pro?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r397hi/step_35_flash_is_a_beast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r397hi/step_35_flash_is_a_beast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r397hi/step_35_flash_is_a_beast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T23:33:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3q0qb</id>
    <title>llama.cpp llama-server running SSM models VRAM fix merged</title>
    <updated>2026-02-13T14:04:54+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;During my time fixing the Kimi Linear server bug reported by &lt;a href="/u/Lord_Pazzu"&gt;u/Lord_Pazzu&lt;/a&gt;, I discovered that running llama-server running SSM hybrid models in general uses KV cache that is multiple of the number of parallel threads (--parallel), so for example, if you run Nemotron 3 Nano at 1M context and --parallel 8, then it would use 48GB VRAM KV cache instead of 6GB even though each server instance can only serve 128K context. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/19552"&gt;https://github.com/ggml-org/llama.cpp/issues/19552&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With this fix, you will only use 6GB just like the transformer models. That means with 48GB VRAM to spare, you can now serve 8 users simultaneously with 1M context each.&lt;/p&gt; &lt;p&gt;Merged PR:&lt;br /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/pull/19559"&gt;https://github.com/ggml-org/llama.cpp/pull/19559&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This applies to all SSM hybrid models like Qwen3Next, Kimi Linear, Nemotron 3 Nano, etc.&lt;/p&gt; &lt;p&gt;So if u r a llama-server user with these new models, then it will be a great news to you. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3q0qb/llamacpp_llamaserver_running_ssm_models_vram_fix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3q0qb/llamacpp_llamaserver_running_ssm_models_vram_fix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3q0qb/llamacpp_llamaserver_running_ssm_models_vram_fix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T14:04:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r2xotu</id>
    <title>Minimax M2.5 Officially Out</title>
    <updated>2026-02-12T16:17:13+00:00</updated>
    <author>
      <name>/u/Which_Slice1600</name>
      <uri>https://old.reddit.com/user/Which_Slice1600</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2xotu/minimax_m25_officially_out/"&gt; &lt;img alt="Minimax M2.5 Officially Out" src="https://preview.redd.it/75rjx62d93jg1.png?width=140&amp;amp;height=67&amp;amp;auto=webp&amp;amp;s=bc1913b92a211d48c5d2979574442a87148c17cf" title="Minimax M2.5 Officially Out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Only official webpages released now. But the bench looks very promising:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SWE-Bench Verified 80.2%&lt;/li&gt; &lt;li&gt;Multi-SWE-Bench 51.3%&lt;/li&gt; &lt;li&gt;BrowseComp 76.3%&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Edit: replaced with the en page:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.minimax.io/news/minimax-m25"&gt;https://www.minimax.io/news/minimax-m25&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Which_Slice1600"&gt; /u/Which_Slice1600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r2xotu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r2xotu/minimax_m25_officially_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r2xotu/minimax_m25_officially_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T16:17:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3jadj</id>
    <title>Improving LLM's coding ability through a new edit format</title>
    <updated>2026-02-13T07:53:36+00:00</updated>
    <author>
      <name>/u/Mushoz</name>
      <uri>https://old.reddit.com/user/Mushoz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3jadj/improving_llms_coding_ability_through_a_new_edit/"&gt; &lt;img alt="Improving LLM's coding ability through a new edit format" src="https://external-preview.redd.it/_HtanEVWgmWOk8SpjQcvTfNBYkpegEjBayvVrK7UD5E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e44168f7cbe68411e3b5b140f2f465f516bb44d0" title="Improving LLM's coding ability through a new edit format" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mushoz"&gt; /u/Mushoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.can.ac/2026/02/12/the-harness-problem/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3jadj/improving_llms_coding_ability_through_a_new_edit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3jadj/improving_llms_coding_ability_through_a_new_edit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T07:53:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3q1h7</id>
    <title>https://huggingface.co/MiniMaxAI/MiniMax-M2.5/tree/main</title>
    <updated>2026-02-13T14:05:44+00:00</updated>
    <author>
      <name>/u/Remarkable_Jicama775</name>
      <uri>https://old.reddit.com/user/Remarkable_Jicama775</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.5/tree/main"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M2.5/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;quants are here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable_Jicama775"&gt; /u/Remarkable_Jicama775 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3q1h7/httpshuggingfacecominimaxaiminimaxm25treemain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3q1h7/httpshuggingfacecominimaxaiminimaxm25treemain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3q1h7/httpshuggingfacecominimaxaiminimaxm25treemain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T14:05:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3ob0r</id>
    <title>GLM 5 has a regression in international language writing according to NCBench</title>
    <updated>2026-02-13T12:49:51+00:00</updated>
    <author>
      <name>/u/jugalator</name>
      <uri>https://old.reddit.com/user/jugalator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This surprised me but also confirmed my poor first impression of it, since I happened to use it for text generation in a less common language and it performed very poorly, barely even like the aging Gemini 2.5 Flash and more like a good 70B Llama 3.x model.&lt;/p&gt; &lt;p&gt;At NCBench - Language Writing, it trails GLM 4.5-4.7 by quite a distance when tested for European languages and Hindi. GLM 4.5 is the clear, superior release in this regard according to NCBench.&lt;/p&gt; &lt;p&gt;Interestingly, Language &lt;em&gt;Comprehension&lt;/em&gt; didn't seem to regress much at all!&lt;/p&gt; &lt;p&gt;GLM 5 may be great and all, but just a heads up if you use it for this particular scenario since I think it's been flying below the radar.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jugalator"&gt; /u/jugalator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.nc-bench.com/tests/language-writing"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ob0r/glm_5_has_a_regression_in_international_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ob0r/glm_5_has_a_regression_in_international_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T12:49:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r35d2x</id>
    <title>MiniMaxAI MiniMax-M2.5 has 230b parameters and 10b active parameters</title>
    <updated>2026-02-12T21:02:15+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35d2x/minimaxai_minimaxm25_has_230b_parameters_and_10b/"&gt; &lt;img alt="MiniMaxAI MiniMax-M2.5 has 230b parameters and 10b active parameters" src="https://external-preview.redd.it/_kcNQarR05LXfQqSjI9sCiHSj5IycOpRZaI00SHW4k8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96ababa53bad9198147827e5856fa3e99fbda827" title="MiniMaxAI MiniMax-M2.5 has 230b parameters and 10b active parameters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenHands reveals the model size in their announcement.&lt;/p&gt; &lt;p&gt;Still waiting for the model to appear on HF.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openhands.dev/blog/minimax-m2-5-open-weights-models-catch-up-to-claude"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r35d2x/minimaxai_minimaxm25_has_230b_parameters_and_10b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r35d2x/minimaxai_minimaxm25_has_230b_parameters_and_10b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-12T21:02:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3ntgi</id>
    <title>Deepseek announced they are testing a new model.</title>
    <updated>2026-02-13T12:25:39+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ntgi/deepseek_announced_they_are_testing_a_new_model/"&gt; &lt;img alt="Deepseek announced they are testing a new model." src="https://preview.redd.it/y5kcxf8699jg1.jpg?width=140&amp;amp;height=58&amp;amp;auto=webp&amp;amp;s=7a1927f261bcd4f8544cc048c08f30ee495de504" title="Deepseek announced they are testing a new model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/y5kcxf8699jg1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1a7fad6dc2630447bffd04ec671cfb62edc9187f"&gt;https://preview.redd.it/y5kcxf8699jg1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1a7fad6dc2630447bffd04ec671cfb62edc9187f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From Chinese group&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rutggcjgf9jg1.png?width=342&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e393275ade2c1baff47b0855457c817e87bca3e3"&gt;https://preview.redd.it/rutggcjgf9jg1.png?width=342&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e393275ade2c1baff47b0855457c817e87bca3e3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;the benchmark model name is fake (placeholder) . This is just for distinguishing&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The benchmark is test reading skills&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Testing completed, OpenAI MRCR 8-pin&lt;/p&gt; &lt;p&gt;-------------------------------------------------------------&lt;/p&gt; &lt;p&gt;Index | Target | Tokens | Prefix | Score | Result&lt;/p&gt; &lt;p&gt;-------------------------------------------------------------&lt;/p&gt; &lt;p&gt;78 | 128000 | 130660 | BfohKPVF96 | 0.9821 | üÜó 0.98&lt;/p&gt; &lt;p&gt;71 | 128000 | 132425 | NPnuBb2ccE | 0.0575 | üÜó 0.06&lt;/p&gt; &lt;p&gt;32 | 128000 | 132828 | dlUj2XS3iz | 1.0000 | ‚úÖ Pass&lt;/p&gt; &lt;p&gt;45 | 128000 | 135258 | VAUPEFeyUy | 1.0000 | ‚úÖ Pass&lt;/p&gt; &lt;p&gt;56 | 128000 | 136965 | kZWrPWyo2z | 0.0276 | üÜó 0.03&lt;/p&gt; &lt;p&gt;7 | 128000 | 136974 | kej4Qdr9Mf | 0.0101 | üÜó 0.01&lt;/p&gt; &lt;p&gt;57 | 128000 | 137211 | HdvXqxVvwQ | 0.0420 | üÜó 0.04&lt;/p&gt; &lt;p&gt;87 | 128000 | 138158 | 4KJuJvpDKt | 0.1123 | üÜó 0.11&lt;/p&gt; &lt;p&gt;64 | 128000 | 138512 | piNIebm2Zr | 0.0560 | üÜó 0.06&lt;/p&gt; &lt;p&gt;88 | 128000 | 138628 | 9W0rMIR3gM | 0.0963 | üÜó 0.10&lt;/p&gt; &lt;p&gt;69 | 256000 | 255410 | BdPq3nqqWy | 0.0307 | üÜó 0.03&lt;/p&gt; &lt;p&gt;40 | 256000 | 255073 | mlzCS98ySY | 0.0221 | üÜó 0.02&lt;/p&gt; &lt;p&gt;58 | 256000 | 254750 | 7ABmnzg5oI | 0.9830 | üÜó 0.98&lt;/p&gt; &lt;p&gt;61 | 256000 | 254317 | gkaLloQvjH | 0.1098 | üÜó 0.11&lt;/p&gt; &lt;p&gt;97 | 256000 | 253819 | 9RNBXn2Gh5 | 1.0000 | ‚úÖ Pass&lt;/p&gt; &lt;p&gt;51 | 256000 | 251993 | E4c3w7oF2w | 0.0703 | üÜó 0.07&lt;/p&gt; &lt;p&gt;23 | 256000 | 251766 | SNtG1BhaDM | 0.9952 | üÜó 1.00&lt;/p&gt; &lt;p&gt;280 | 256000 | 261742 | RsuyJ8tkrC | 0.0681 | üÜó 0.07&lt;/p&gt; &lt;p&gt;278 | 256000 | 263214 | D7Ndj9vdKm | 0.1613 | üÜó 0.16&lt;/p&gt; &lt;p&gt;224 | 256000 | 265550 | 1YZYhQtMCW | 0.1545 | üÜó 0.15&lt;/p&gt; &lt;p&gt;-------------------------------------------------------------&lt;/p&gt; &lt;p&gt;üèÜ Total Marks: 0.3489&lt;/p&gt; &lt;p&gt;üìè Length statistics:&lt;/p&gt; &lt;p&gt;- 128000 Tokens: 0.3384 (n=10)&lt;/p&gt; &lt;p&gt;- 256000 Tokens: 0.3595 (n=10)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ntgi/deepseek_announced_they_are_testing_a_new_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ntgi/deepseek_announced_they_are_testing_a_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3ntgi/deepseek_announced_they_are_testing_a_new_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T12:25:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3l572</id>
    <title>MiniMax onX: Weights dropping REALLY, REALLY, SOON</title>
    <updated>2026-02-13T09:51:33+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3l572/minimax_onx_weights_dropping_really_really_soon/"&gt; &lt;img alt="MiniMax onX: Weights dropping REALLY, REALLY, SOON" src="https://preview.redd.it/jrgpe9krh8jg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=30a1ae2be695a2a4f2dee2ca962e2fa76614dcc1" title="MiniMax onX: Weights dropping REALLY, REALLY, SOON" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jrgpe9krh8jg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3l572/minimax_onx_weights_dropping_really_really_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3l572/minimax_onx_weights_dropping_really_really_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T09:51:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3mnm3</id>
    <title>ByteDance Releases Protenix-v1</title>
    <updated>2026-02-13T11:22:41+00:00</updated>
    <author>
      <name>/u/techlatest_net</name>
      <uri>https://old.reddit.com/user/techlatest_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;ByteDance Releases Protenix-v1: A New Open-Source Model Achieving AF3-Level Performance in Biomolecular Structure Prediction&lt;/h1&gt; &lt;p&gt;Link: &lt;a href="https://github.com/bytedance/Protenix"&gt;https://github.com/bytedance/Protenix&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techlatest_net"&gt; /u/techlatest_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3mnm3/bytedance_releases_protenixv1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3mnm3/bytedance_releases_protenixv1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3mnm3/bytedance_releases_protenixv1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T11:22:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3hlfq</id>
    <title>UG student launches Dhi-5B (Trained from Scratch)</title>
    <updated>2026-02-13T06:13:29+00:00</updated>
    <author>
      <name>/u/gradNorm</name>
      <uri>https://old.reddit.com/user/gradNorm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3hlfq/ug_student_launches_dhi5b_trained_from_scratch/"&gt; &lt;img alt="UG student launches Dhi-5B (Trained from Scratch)" src="https://preview.redd.it/5tsgquvue7jg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fac59fbf4b00df28aabae2f993f4d65bb88169c" title="UG student launches Dhi-5B (Trained from Scratch)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hii everyone,&lt;/p&gt; &lt;p&gt;I present Dhi-5B: A 5 billion parameter Multimodal Language Model trained compute optimally with just ‚Çπ1.1 lakh ($1200).&lt;/p&gt; &lt;p&gt;I incorporate the latest architecture design and training methodologies in this. And I also use a custom built codebase for training these models.&lt;/p&gt; &lt;p&gt;I train the Dhi-5B in 5 stages:-&lt;/p&gt; &lt;p&gt;üìö Pre-Training: The most compute heavy phase, where the core is built. (Gives the Base varient.)&lt;/p&gt; &lt;p&gt;üìú Context-Length-Extension: The model learns to handle 16k context from the 4k learned during PT.&lt;/p&gt; &lt;p&gt;üìñ Mid-Training: Annealing on very high quality datasets.&lt;/p&gt; &lt;p&gt;üí¨ Supervised-Fine-Tuning: Model learns to handle conversations. (Gives the Instruct model.)&lt;/p&gt; &lt;p&gt;üëÄ Vision-Extension: The model learns to see. (Results in The Dhi-5B.)&lt;/p&gt; &lt;p&gt;I'll be dropping it in 3 phases:-&lt;/p&gt; &lt;p&gt;i. Dhi-5B-Base (available now)&lt;/p&gt; &lt;p&gt;ii. Dhi-5B-Instruct (coming soon)&lt;/p&gt; &lt;p&gt;iii. The Dhi-5B (coming soon)&lt;/p&gt; &lt;p&gt;Some details about the Dhi-5B-Base model:-&lt;/p&gt; &lt;p&gt;The base varient is of 4 billion parameters. It is trained on 40 billion natural language tokens mostly in english from FineWeb-Edu dataset.&lt;/p&gt; &lt;p&gt;I use the new Muon optimizer for optimising the Matrix Layers, and rest are optimized by AdamW.&lt;/p&gt; &lt;p&gt;The model has 32 layers, with 3072 width, SwiGLU MLPs, the full MHA attention with FlashAttention-3, 4096 context length, 64k vocab and 2 million batch size during training.&lt;/p&gt; &lt;p&gt;Attached are some evaluations of the base model, the compared models are about 10x more expensive than ours.&lt;/p&gt; &lt;p&gt;Thank you, everyone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gradNorm"&gt; /u/gradNorm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5tsgquvue7jg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3hlfq/ug_student_launches_dhi5b_trained_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3hlfq/ug_student_launches_dhi5b_trained_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T06:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3o6je</id>
    <title>New DeepSeek update: "DeepSeek Web / APP is currently testing a new long-context model architecture, supporting a 1M context window."</title>
    <updated>2026-02-13T12:43:50+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3o6je/new_deepseek_update_deepseek_web_app_is_currently/"&gt; &lt;img alt="New DeepSeek update: &amp;quot;DeepSeek Web / APP is currently testing a new long-context model architecture, supporting a 1M context window.&amp;quot;" src="https://preview.redd.it/dg94ujw1c9jg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55de58cf8a3e4a397d81184a2473b94f7a31aa33" title="New DeepSeek update: &amp;quot;DeepSeek Web / APP is currently testing a new long-context model architecture, supporting a 1M context window.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From AiBattle on ùïè: &lt;a href="https://x.com/AiBattle_/status/2022280288643039235"&gt;https://x.com/AiBattle_/status/2022280288643039235&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dg94ujw1c9jg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3o6je/new_deepseek_update_deepseek_web_app_is_currently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3o6je/new_deepseek_update_deepseek_web_app_is_currently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T12:43:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3kzce</id>
    <title>MiniMax-M2.5 Checkpoints on huggingface will be in 8 hours</title>
    <updated>2026-02-13T09:41:01+00:00</updated>
    <author>
      <name>/u/Own_Forever_5997</name>
      <uri>https://old.reddit.com/user/Own_Forever_5997</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzce/minimaxm25_checkpoints_on_huggingface_will_be_in/"&gt; &lt;img alt="MiniMax-M2.5 Checkpoints on huggingface will be in 8 hours" src="https://preview.redd.it/p94fz9gsf8jg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=920f76b1a80dd8b1b58e34745f143966274a40a4" title="MiniMax-M2.5 Checkpoints on huggingface will be in 8 hours" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own_Forever_5997"&gt; /u/Own_Forever_5997 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p94fz9gsf8jg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzce/minimaxm25_checkpoints_on_huggingface_will_be_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzce/minimaxm25_checkpoints_on_huggingface_will_be_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T09:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3pxy7</id>
    <title>MiniMaxAI/MiniMax-M2.5 ¬∑ Hugging Face</title>
    <updated>2026-02-13T14:01:52+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/"&gt; &lt;img alt="MiniMaxAI/MiniMax-M2.5 ¬∑ Hugging Face" src="https://external-preview.redd.it/U31ABjps-QUJnQnBzpD57R8aGaCakUKOyzXjWrvNYSg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de0bab4be78008336f973196f0ed98e2bbe49764" title="MiniMaxAI/MiniMax-M2.5 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Published an hour ago, how is there no post yet?!&lt;/p&gt; &lt;p&gt;No quants yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T14:01:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3csbk</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2.5 SoTA Model (Friday, 8AM-11AM PST)</title>
    <updated>2026-02-13T02:12:47+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3csbk/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2.5 SoTA Model (Friday, 8AM-11AM PST)" src="https://preview.redd.it/orcqu1oq76jg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=617bc649889e0dd0343008568d8aa957dde229c5" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2.5 SoTA Model (Friday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; üëã&lt;/p&gt; &lt;p&gt;We're excited for Friday's guests: &lt;strong&gt;The Core Team of MiniMax Lab and The Lab‚Äôs Founder!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Friday, Feb. 13th, 8 AM‚Äì11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don‚Äôt post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/orcqu1oq76jg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3csbk/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3csbk/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T02:12:47+00:00</published>
  </entry>
</feed>
