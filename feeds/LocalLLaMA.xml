<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-07T17:23:56+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oqzedw</id>
    <title>New stealth model Polaris Alpha from Openrouter</title>
    <updated>2025-11-07T16:44:02+00:00</updated>
    <author>
      <name>/u/sirjoaco</name>
      <uri>https://old.reddit.com/user/sirjoaco</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqzedw/new_stealth_model_polaris_alpha_from_openrouter/"&gt; &lt;img alt="New stealth model Polaris Alpha from Openrouter" src="https://external-preview.redd.it/bTdzOWxtN3c1dnpmMWaLRFtlOaNnELcut79whFpV_-6R32Ak6GAiQHe4R0Dv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8465085c933b82084549ab7df219a7b594475a3e" title="New stealth model Polaris Alpha from Openrouter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New stealth model Polaris Alpha from Openrouter&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sirjoaco"&gt; /u/sirjoaco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/w4hp2m7w5vzf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqzedw/new_stealth_model_polaris_alpha_from_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqzedw/new_stealth_model_polaris_alpha_from_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T16:44:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqwloa</id>
    <title>Using Ray, Unsloth, Axolotl or GPUStack? We are looking for beta testers</title>
    <updated>2025-11-07T14:58:42+00:00</updated>
    <author>
      <name>/u/Good-Coconut3907</name>
      <uri>https://old.reddit.com/user/Good-Coconut3907</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are looking for beta testers to help us put the &lt;a href="https://github.com/kalavai-net/kalavai-client"&gt;Kalavai platform&lt;/a&gt; through its paces. &lt;/p&gt; &lt;p&gt;If you are using &lt;strong&gt;Ray&lt;/strong&gt; for distributed workloads, &lt;strong&gt;Unsloth/Axolotl&lt;/strong&gt; for fine tuning models or &lt;strong&gt;GPUStack&lt;/strong&gt; to manage your GPU cluster, we need you!&lt;/p&gt; &lt;p&gt;Sign up &lt;a href="https://kalavai.net/beta"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;PS: Are you an AI developer working on other frameworks? We'd love to support it too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Good-Coconut3907"&gt; /u/Good-Coconut3907 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqwloa/using_ray_unsloth_axolotl_or_gpustack_we_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqwloa/using_ray_unsloth_axolotl_or_gpustack_we_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqwloa/using_ray_unsloth_axolotl_or_gpustack_we_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T14:58:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqojnk</id>
    <title>Announcing: Hack the Edge by AMD √ó Liquid AI - San Francisco 15-16th November</title>
    <updated>2025-11-07T07:58:41+00:00</updated>
    <author>
      <name>/u/LiquidAI_Team</name>
      <uri>https://old.reddit.com/user/LiquidAI_Team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqojnk/announcing_hack_the_edge_by_amd_liquid_ai_san/"&gt; &lt;img alt="Announcing: Hack the Edge by AMD √ó Liquid AI - San Francisco 15-16th November" src="https://preview.redd.it/tb6azm0yjszf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fea13414021e95ebdcab649e10638f46da5615f" title="Announcing: Hack the Edge by AMD √ó Liquid AI - San Francisco 15-16th November" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;Join the AMD and Liquid teams at the Liquid AI Office in SF for an exclusive hackathon &lt;strong&gt;Nov 15-16th.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Over these two days you will build unique local, private, and efficient AI applications directly on AMD hardware ‚Äî with guidance from Liquid and AMD researchers.&lt;/p&gt; &lt;p&gt;The challenge will be revealed on site. &lt;/p&gt; &lt;p&gt;Winners receive their share of $5K.&lt;/p&gt; &lt;p&gt;Apply to Joinüëá&lt;br /&gt; &lt;a href="https://luma.com/smik3k94"&gt;https://luma.com/smik3k94&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiquidAI_Team"&gt; /u/LiquidAI_Team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tb6azm0yjszf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqojnk/announcing_hack_the_edge_by_amd_liquid_ai_san/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqojnk/announcing_hack_the_edge_by_amd_liquid_ai_san/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T07:58:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq1i9b</id>
    <title>Kimi K2 Thinking Huggingface</title>
    <updated>2025-11-06T15:12:59+00:00</updated>
    <author>
      <name>/u/DistanceSolar1449</name>
      <uri>https://old.reddit.com/user/DistanceSolar1449</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1i9b/kimi_k2_thinking_huggingface/"&gt; &lt;img alt="Kimi K2 Thinking Huggingface" src="https://external-preview.redd.it/H-gfQMTLwEzPYBcfO_Qq4uuh_Gu1NEE3y2PjVFhCwx0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73256a6e56665a31c845dbe43d4cf687ee6b4218" title="Kimi K2 Thinking Huggingface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DistanceSolar1449"&gt; /u/DistanceSolar1449 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Thinking"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1i9b/kimi_k2_thinking_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1i9b/kimi_k2_thinking_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T15:12:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqki9e</id>
    <title>128GB RAM costs ~$1000 &amp; Strix Halo costs $1600 in total</title>
    <updated>2025-11-07T04:04:41+00:00</updated>
    <author>
      <name>/u/johnnytshi</name>
      <uri>https://old.reddit.com/user/johnnytshi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We all know RAM has gone up quite a bit, like: &lt;a href="https://pcpartpicker.com/product/WTMMnQ/corsair-vengeance-rgb-64-gb-2-x-32-gb-ddr5-6000-cl30-memory-cmh64gx5m2b6000c30"&gt;https://pcpartpicker.com/product/WTMMnQ/corsair-vengeance-rgb-64-gb-2-x-32-gb-ddr5-6000-cl30-memory-cmh64gx5m2b6000c30&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How is it possible that Strix Halo with 128GB costs $1699? like &lt;a href="https://www.gmktec.com/products/amd-ryzen%E2%84%A2-ai-max-395-evo-x2-ai-mini-pc?srsltid=AfmBOopMa5dg-W23Ck2BDBNK2wWvPAnToenYsT16yQ-_mreQ8HR7gD9v"&gt;https://www.gmktec.com/products/amd-ryzen%E2%84%A2-ai-max-395-evo-x2-ai-mini-pc?srsltid=AfmBOopMa5dg-W23Ck2BDBNK2wWvPAnToenYsT16yQ-_mreQ8HR7gD9v&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LPDDR5X, 8000MHz &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johnnytshi"&gt; /u/johnnytshi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqki9e/128gb_ram_costs_1000_strix_halo_costs_1600_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqki9e/128gb_ram_costs_1000_strix_halo_costs_1600_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqki9e/128gb_ram_costs_1000_strix_halo_costs_1600_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T04:04:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqsf0j</id>
    <title>How do you evaluate the quality of your knowledge base?</title>
    <updated>2025-11-07T11:54:59+00:00</updated>
    <author>
      <name>/u/CapitalShake3085</name>
      <uri>https://old.reddit.com/user/CapitalShake3085</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Typically, in a RAG system, we measure metrics related to the retrieval pipeline ‚Äî such as retriever performance, reranker accuracy, and generation quality.&lt;/p&gt; &lt;p&gt;However, I believe it‚Äôs equally important to have metrics that assess the quality of the underlying knowledge base itself. For example:&lt;/p&gt; &lt;p&gt;Are there contradictory or outdated documents?&lt;/p&gt; &lt;p&gt;Are there duplicates or near-duplicates causing noise?&lt;/p&gt; &lt;p&gt;Is the content complete and consistent across topics?&lt;/p&gt; &lt;p&gt;How do you evaluate this? Are there existing frameworks or tools for assessing knowledge base quality? What approaches or best practices do you use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CapitalShake3085"&gt; /u/CapitalShake3085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqsf0j/how_do_you_evaluate_the_quality_of_your_knowledge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqsf0j/how_do_you_evaluate_the_quality_of_your_knowledge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqsf0j/how_do_you_evaluate_the_quality_of_your_knowledge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T11:54:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq8mmy</id>
    <title>Kimi K2 Thinking and DeepSeek R1 Architectures Side by Side</title>
    <updated>2025-11-06T19:37:54+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq8mmy/kimi_k2_thinking_and_deepseek_r1_architectures/"&gt; &lt;img alt="Kimi K2 Thinking and DeepSeek R1 Architectures Side by Side" src="https://b.thumbs.redditmedia.com/bLGXBa7gA85RfSO792H013zd_aNhH4CnZeSr7OxsZVc.jpg" title="Kimi K2 Thinking and DeepSeek R1 Architectures Side by Side" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2 is based on the DeepSeek V3/R1 architecture, and here's a side-by-side comparison.&lt;/p&gt; &lt;p&gt;- 2√ó fewer attention heads (64 vs. 128)&lt;br /&gt; - ~1.5√ó more experts per MoE layer (384 vs. 256)&lt;br /&gt; - Bigger vocabulary (160k vs. 129k)&lt;br /&gt; - K2 activates ~32B parameters per token (vs. 37B in DeepSeek R1)&lt;br /&gt; - Fewer dense FFN blocks before MoE&lt;br /&gt; - 2x longer supported context&lt;/p&gt; &lt;p&gt;In short, Kimi K2 is a slightly scaled DeepSeek V3/R1. And the gains are in the data and training recipes. Hopefully, we will see some details on those soon, too.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/98ghpsqn9pzf1.jpg?width=2200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8ca505a76cd4755ed0b557ac281e621eeac3da9e"&gt;https://preview.redd.it/98ghpsqn9pzf1.jpg?width=2200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8ca505a76cd4755ed0b557ac281e621eeac3da9e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq8mmy/kimi_k2_thinking_and_deepseek_r1_architectures/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq8mmy/kimi_k2_thinking_and_deepseek_r1_architectures/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq8mmy/kimi_k2_thinking_and_deepseek_r1_architectures/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T19:37:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqcd1y</id>
    <title>Just want to take a moment to express gratitude for this tech</title>
    <updated>2025-11-06T22:00:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What a time to be alive! &lt;/p&gt; &lt;p&gt;I was just randomly reflecting today - a single file with just a bunch of numbers can be used to make poems, apps, reports and so much more. And that's just LLMs.. But then this applies to image, video, speech, music, audio, 3D models and whatever else that can be expressed digitally&lt;/p&gt; &lt;p&gt;Anyone can do this with publicly available downloads and software. You dont need sophisticated computers or hardware.&lt;/p&gt; &lt;p&gt;Possibly most insane of all is that you can do all of this for free.&lt;/p&gt; &lt;p&gt;This is just utter insanity. If you had told me this would be the ecosystem before this wave happened, I would have never believed you. Regardless of how things evolve, I think we should be immensely grateful for all of this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqcd1y/just_want_to_take_a_moment_to_express_gratitude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqcd1y/just_want_to_take_a_moment_to_express_gratitude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqcd1y/just_want_to_take_a_moment_to_express_gratitude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T22:00:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1or08aq</id>
    <title>OpenAI Pushes to Label Datacenters as ‚ÄòAmerican Manufacturing‚Äô Seeking Federal Subsidies After Preaching Independence</title>
    <updated>2025-11-07T17:15:05+00:00</updated>
    <author>
      <name>/u/Ok-Breakfast-4676</name>
      <uri>https://old.reddit.com/user/Ok-Breakfast-4676</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"&gt; &lt;img alt="OpenAI Pushes to Label Datacenters as ‚ÄòAmerican Manufacturing‚Äô Seeking Federal Subsidies After Preaching Independence" src="https://preview.redd.it/jq1jrz6kbvzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=258e2c389f69a6ed7cdcda3ea33b5a80b38a0fb3" title="OpenAI Pushes to Label Datacenters as ‚ÄòAmerican Manufacturing‚Äô Seeking Federal Subsidies After Preaching Independence" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI is now lobbying to classify datacenter spending as ‚ÄúAmerican manufacturing.‚Äù&lt;/p&gt; &lt;p&gt;In their recent submission, they explicitly advocate for Federal loan guarantees the same kind used to subsidize large-scale industrial projects.&lt;/p&gt; &lt;p&gt;So after all the talk about independence and no need for government help‚Ä¶ Sam lied. Again.Ôøº&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Breakfast-4676"&gt; /u/Ok-Breakfast-4676 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jq1jrz6kbvzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T17:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqxx8w</id>
    <title>Vulnerability Inception: How AI Code Assistants Replicate and Amplify Security Flaws</title>
    <updated>2025-11-07T15:48:49+00:00</updated>
    <author>
      <name>/u/ortegaalfredo</name>
      <uri>https://old.reddit.com/user/ortegaalfredo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqxx8w/vulnerability_inception_how_ai_code_assistants/"&gt; &lt;img alt="Vulnerability Inception: How AI Code Assistants Replicate and Amplify Security Flaws" src="https://external-preview.redd.it/5sm3GRNGqkHPJJHTtLe5heJ60MTx9qDDMNLNZZnboms.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81efb413a983424e2fb3cecba428a0affa68289f" title="Vulnerability Inception: How AI Code Assistants Replicate and Amplify Security Flaws" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm sharing an article about prompt injection in Large Language Models (LLMs), specifically regarding coding and coding agents. The research shows that it's easy to manipulate LLMs into injecting backdoors and vulnerabilities into code, simply by embedding instructions in a comment, as the LLM will follow any instructions it finds in the original source code.&lt;/p&gt; &lt;p&gt;This is relevant to the localLlama community because only one open-weights model, Deepseek 3.2 Exp, appears to resistant (but not immune) to this vulnerability. It seems to have received specialized training to avoid introducing security flaws. I think this is a significant finding and hope you find it useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ortegaalfredo"&gt; /u/ortegaalfredo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ortegaalfredo/aiweaknesses/blob/main/ai_vulnerabilities_article.pdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqxx8w/vulnerability_inception_how_ai_code_assistants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqxx8w/vulnerability_inception_how_ai_code_assistants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:48:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq9ui3</id>
    <title>Microsoft‚Äôs AI Scientist</title>
    <updated>2025-11-06T20:23:52+00:00</updated>
    <author>
      <name>/u/Ok-Breakfast-4676</name>
      <uri>https://old.reddit.com/user/Ok-Breakfast-4676</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9ui3/microsofts_ai_scientist/"&gt; &lt;img alt="Microsoft‚Äôs AI Scientist" src="https://preview.redd.it/jbv9rmub4pzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7b040b383a3c04d5034fca2fe81396d6e5d57a9" title="Microsoft‚Äôs AI Scientist" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft literally just dropped the first AI scientist&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Breakfast-4676"&gt; /u/Ok-Breakfast-4676 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jbv9rmub4pzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9ui3/microsofts_ai_scientist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9ui3/microsofts_ai_scientist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T20:23:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqx4dj</id>
    <title>Emergent Occam's Razor: Teaching qwen2.5:7b to learn through journaling (51%‚Üí78%) [Full code + paper]</title>
    <updated>2025-11-07T15:18:06+00:00</updated>
    <author>
      <name>/u/Next_Bid_8339</name>
      <uri>https://old.reddit.com/user/Next_Bid_8339</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just finished an experiment where a 7B model learns through reflection and self-critique - no weight updates, no training data, just journaling about mistakes.&lt;/p&gt; &lt;p&gt;**The surprising part: the model discovered Occam's Razor on its own.**&lt;/p&gt; &lt;p&gt;## The Setup&lt;/p&gt; &lt;p&gt;- Model: qwen2.5:7b (local, via Ollama)&lt;/p&gt; &lt;p&gt;- Task: Meeting room scheduling (constraint satisfaction)&lt;/p&gt; &lt;p&gt;- Method: After each batch, model writes reflective journal and distills strategy&lt;/p&gt; &lt;p&gt;- Hardware: Consumer laptop, no GPU needed&lt;/p&gt; &lt;p&gt;- Runtime: ~40 minutes total&lt;/p&gt; &lt;p&gt;## The Results&lt;/p&gt; &lt;p&gt;| Stage | Accuracy | What Happened |&lt;/p&gt; &lt;p&gt;|-------|----------|---------------|&lt;/p&gt; &lt;p&gt;| Baseline | 51.3% | Zero-shot, weak |&lt;/p&gt; &lt;p&gt;| Bootstrap | 66.0% | Learning phase (messy) |&lt;/p&gt; &lt;p&gt;| Test w/ LRL | 78.0% | **+26.7% improvement!** |&lt;/p&gt; &lt;p&gt;## The Learning Journey (This is the cool part)&lt;/p&gt; &lt;p&gt;**Batches 1-5: &amp;quot;The Over-Engineer&amp;quot;**&lt;/p&gt; &lt;p&gt;Model confidently proposes complex solutions:&lt;/p&gt; &lt;p&gt;- &amp;quot;Implement interval trees!&amp;quot;&lt;/p&gt; &lt;p&gt;- &amp;quot;Apply dynamic programming!&amp;quot;&lt;/p&gt; &lt;p&gt;- &amp;quot;Use graph theory approaches!&amp;quot;&lt;/p&gt; &lt;p&gt;Result: ~35% accuracy. Sophisticated nonsense.&lt;/p&gt; &lt;p&gt;**Batches 6-8: &amp;quot;Seeds of Doubt&amp;quot;**&lt;/p&gt; &lt;p&gt;Journal entries start showing conflict:&lt;/p&gt; &lt;p&gt;&amp;gt; &amp;quot;Since the problem is straightforward, focusing on basic interval checking...&amp;quot;&lt;/p&gt; &lt;p&gt;First time admitting simplicity might be the answer.&lt;/p&gt; &lt;p&gt;**Batches 9-10: &amp;quot;The Awakening&amp;quot;**&lt;/p&gt; &lt;p&gt;The breakthrough journal entry:&lt;/p&gt; &lt;p&gt;&amp;gt; &amp;quot;This suggests a **fundamental misunderstanding** of how to handle overlapping intervals.&amp;quot;&lt;/p&gt; &lt;p&gt;The model admitted it was wrong. Everything changed from there.&lt;/p&gt; &lt;p&gt;## Why This Matters for Local LLMs&lt;/p&gt; &lt;p&gt;‚úÖ **Interpretable** - Read the complete thought process in journals &lt;/p&gt; &lt;p&gt;‚úÖ **Efficient** - No GPU training, pure inference &lt;/p&gt; &lt;p&gt;‚úÖ **Transferable** - Strategies are text files you can share &lt;/p&gt; &lt;p&gt;‚úÖ **Safe** - Models that learn to doubt themselves &lt;/p&gt; &lt;p&gt;The distillation process acts like evolution: ideas that work (simple counting) survive, ideas that fail (graph theory) get filtered out.&lt;/p&gt; &lt;p&gt;## Try It Yourself&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;p&gt;git clone &lt;a href="https://github.com/DRawson5570/linguistic-rl-scheduling"&gt;https://github.com/DRawson5570/linguistic-rl-scheduling&lt;/a&gt;&lt;/p&gt; &lt;p&gt;cd linguistic-rl-scheduling&lt;/p&gt; &lt;p&gt;ollama pull qwen2.5:7b&lt;/p&gt; &lt;p&gt;python3 scheduling_lrl_paper.py&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Next_Bid_8339"&gt; /u/Next_Bid_8339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqx4dj/emergent_occams_razor_teaching_qwen257b_to_learn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqx4dj/emergent_occams_razor_teaching_qwen257b_to_learn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqx4dj/emergent_occams_razor_teaching_qwen257b_to_learn/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:18:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqtose</id>
    <title>Sparse Attention MoE - a test repo for a novel swappable attention mechanism</title>
    <updated>2025-11-07T12:56:53+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw someone talking about using a MoE for Attention a few weeks back. At the time, it seemed like nonsense, but something about the post made me fiddle around with it a bit, and I was surprised to find it... worked? Crazier still... it seems to beat regular attention while radically reducing the amount of time and compute needed to train a model in my testing.&lt;/p&gt; &lt;p&gt;This is an experiment I put together for testing Sparse Attention MoE, a novel attention mechanism that reduces self-attention computational complexity. The idea is to create a new drop-in attention mechanism that should work in existing AI training pipelines while radically reducing the amount of compute required (allowing larger models to be trained on smaller devices, for example). Faster training, lower use of resources, and in my testing so far it trains models that outperforms regular dense attention (at least on my small toy model tests).&lt;/p&gt; &lt;p&gt;Normally, MoE routes feed-forward experts. This concept routes attention sparsity levels. By training Attention we are able to get it to identify easy, medium, and hard tokens, allowing it to route them in a way that reduces how much compute is required as a whole.&lt;/p&gt; &lt;p&gt;I've built a small end-to-end test model and provided all the code to train one yourself at this github repo. This demonstrates &lt;strong&gt;O(N¬∑k) attention&lt;/strong&gt; (vs. O(N¬≤)) attention, and allows efficient training since you don't have quadratic blowup on attention. I test-trained a small LLM to see how it would go and saw similar improvement: The adaptive model achieved **12.03% perplexity improvement** over the non-adaptive baseline with **balanced expert usage** (47%/34%/19%) and was **1.7√ó faster to train**. This directly replicates the vision model's success pattern in a different domain, proving the mechanism is **task-general, not vision-specific**.&lt;/p&gt; &lt;p&gt;For now I'm sharing the diffusion version (it's doing a denoise job on cifar data since that's a simplistic task that can be trained in a few minutes on a 4090).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Deveraux-Parker/Adaptive_Sparse_Attention_MoE/tree/main"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqtose/sparse_attention_moe_a_test_repo_for_a_novel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqtose/sparse_attention_moe_a_test_repo_for_a_novel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T12:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq9b7e</id>
    <title>Nvidia's Jensen Huang: 'China is going to win the AI race,' FT reports</title>
    <updated>2025-11-06T20:03:28+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9b7e/nvidias_jensen_huang_china_is_going_to_win_the_ai/"&gt; &lt;img alt="Nvidia's Jensen Huang: 'China is going to win the AI race,' FT reports" src="https://external-preview.redd.it/B5kYZqF-LXs8_vBUF8bfaXMktkNYepX59paDPfYv7go.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04b0e3c2929dde65c0820bb5e348487a3bb39955" title="Nvidia's Jensen Huang: 'China is going to win the AI race,' FT reports" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reuters.com/world/asia-pacific/nvidias-jensen-huang-says-china-will-win-ai-race-with-us-ft-reports-2025-11-05/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9b7e/nvidias_jensen_huang_china_is_going_to_win_the_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9b7e/nvidias_jensen_huang_china_is_going_to_win_the_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T20:03:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqpt4w</id>
    <title>Minimax will launch a coding package on November 14th</title>
    <updated>2025-11-07T09:20:02+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqpt4w/minimax_will_launch_a_coding_package_on_november/"&gt; &lt;img alt="Minimax will launch a coding package on November 14th" src="https://b.thumbs.redditmedia.com/vtguBl-F1WEoWi4Kor5zaXQYJwCg2LdpS7SCl2DWLrM.jpg" title="Minimax will launch a coding package on November 14th" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oqpt4w"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqpt4w/minimax_will_launch_a_coding_package_on_november/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqpt4w/minimax_will_launch_a_coding_package_on_november/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T09:20:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqxuqs</id>
    <title>From your experience for text only, how is Qwen3VL compared to Qwen3, does having a Visual module penalize the text-only capacities ?</title>
    <updated>2025-11-07T15:46:05+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title.&lt;/p&gt; &lt;p&gt;Let's say &lt;code&gt;Qwen3-30B-A3B-Instruct-2507&lt;/code&gt; excels at text only and long context.&lt;/p&gt; &lt;p&gt;What about &lt;code&gt;Qwen3-VL-30B-A3B-Instruct&lt;/code&gt; if you use it as a text only model ? have you seen any quality loss ?&lt;/p&gt; &lt;p&gt;We're wondering if it make sense to have in one gpu Qwen3 VL and on another gpu Qwen3.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqxuqs/from_your_experience_for_text_only_how_is_qwen3vl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqxuqs/from_your_experience_for_text_only_how_is_qwen3vl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqxuqs/from_your_experience_for_text_only_how_is_qwen3vl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:46:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq1arc</id>
    <title>Kimi released Kimi K2 Thinking, an open-source trillion-parameter reasoning model</title>
    <updated>2025-11-06T15:04:59+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"&gt; &lt;img alt="Kimi released Kimi K2 Thinking, an open-source trillion-parameter reasoning model" src="https://b.thumbs.redditmedia.com/NupD3tHHs6sXvqucL46py-jFU7OPNJHTwiCDt_n7fGc.jpg" title="Kimi released Kimi K2 Thinking, an open-source trillion-parameter reasoning model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d01vorgfjnzf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a8f26127a8125731e93b25522a7bcdc28637d6f"&gt;https://preview.redd.it/d01vorgfjnzf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a8f26127a8125731e93b25522a7bcdc28637d6f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech blog:&lt;/strong&gt; &lt;a href="https://moonshotai.github.io/Kimi-K2/thinking.html"&gt;https://moonshotai.github.io/Kimi-K2/thinking.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Weights &amp;amp; code:&lt;/strong&gt; &lt;a href="https://huggingface.co/moonshotai"&gt;https://huggingface.co/moonshotai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T15:04:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqu4i3</id>
    <title>Intel Arc Pro B50 GPU Review: An Affordable, Low-Power Workstation GPU</title>
    <updated>2025-11-07T13:16:02+00:00</updated>
    <author>
      <name>/u/brand_momentum</name>
      <uri>https://old.reddit.com/user/brand_momentum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqu4i3/intel_arc_pro_b50_gpu_review_an_affordable/"&gt; &lt;img alt="Intel Arc Pro B50 GPU Review: An Affordable, Low-Power Workstation GPU" src="https://external-preview.redd.it/3Egk-w2HASlmi68QUUwWJyPFPlaRWFVtyMPx2j34DHg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0db3e975c5d76b36da431c0de3d9c2859f58e5a6" title="Intel Arc Pro B50 GPU Review: An Affordable, Low-Power Workstation GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brand_momentum"&gt; /u/brand_momentum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.storagereview.com/review/intel-arc-pro-b50-gpu-review-an-affordable-low-power-workstation-gpu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqu4i3/intel_arc_pro_b50_gpu_review_an_affordable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqu4i3/intel_arc_pro_b50_gpu_review_an_affordable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T13:16:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqmder</id>
    <title>Co-authored a book called "Build DeepSeek from Scratch" | Live Now</title>
    <updated>2025-11-07T05:45:39+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqmder/coauthored_a_book_called_build_deepseek_from/"&gt; &lt;img alt="Co-authored a book called &amp;quot;Build DeepSeek from Scratch&amp;quot; | Live Now" src="https://preview.redd.it/1felu4y3wrzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd71bff3abb06e132c57c36de21150381fe19207" title="Co-authored a book called &amp;quot;Build DeepSeek from Scratch&amp;quot; | Live Now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Book link: &lt;a href="https://hubs.la/Q03Rl_lh0"&gt;https://hubs.la/Q03Rl_lh0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github repository: &lt;a href="https://github.com/VizuaraAI/DeepSeek-From-Scratch"&gt;https://github.com/VizuaraAI/DeepSeek-From-Scratch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Published by Manning Publications. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1felu4y3wrzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqmder/coauthored_a_book_called_build_deepseek_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqmder/coauthored_a_book_called_build_deepseek_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T05:45:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqjgnh</id>
    <title>30 days to become AI engineer</title>
    <updated>2025-11-07T03:12:00+00:00</updated>
    <author>
      <name>/u/CayleneKole</name>
      <uri>https://old.reddit.com/user/CayleneKole</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm moving from 12 years in cybersecurity (big tech) into a Staff AI Engineer role.&lt;br /&gt; I have 30 days (~16h/day) to get production-ready, prioritizing context engineering, RAG, and reliable agents.&lt;br /&gt; I need a focused path: the few resources, habits, and pitfalls that matter most.&lt;br /&gt; If you‚Äôve done this or ship real LLM systems, how would you spend the 30 days?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CayleneKole"&gt; /u/CayleneKole &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqjgnh/30_days_to_become_ai_engineer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqjgnh/30_days_to_become_ai_engineer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqjgnh/30_days_to_become_ai_engineer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T03:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqttg0</id>
    <title>Can someone explain what a Mixture-of-Experts model really is?</title>
    <updated>2025-11-07T13:02:36+00:00</updated>
    <author>
      <name>/u/Weebviir</name>
      <uri>https://old.reddit.com/user/Weebviir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I've been aware of MoE since Deepseek dropped in the beginning of the year but I never really delved deep into what it is and how it helps in things like local AI inferencing. This sub's been very helpful with my local AI related questions so I wanted to learn from the people here.&lt;/p&gt; &lt;p&gt;Here are some more questions:&lt;br /&gt; - How does a model know when an expert is to be used?&lt;br /&gt; - Are MoE models really easier to run than traditional models?&lt;br /&gt; - How do Activation parameters really work? Do they affect fine tuning processes later?&lt;br /&gt; - Why do MoE models work better than traditional models?&lt;br /&gt; - What are ‚Äúsparse‚Äù vs ‚Äúdense‚Äù MoE architectures?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weebviir"&gt; /u/Weebviir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqttg0/can_someone_explain_what_a_mixtureofexperts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqttg0/can_someone_explain_what_a_mixtureofexperts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqttg0/can_someone_explain_what_a_mixtureofexperts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T13:02:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqo57j</id>
    <title>ubergarm/Kimi-K2-Thinking-GGUF ¬∑ Hugging Face</title>
    <updated>2025-11-07T07:32:32+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqo57j/ubergarmkimik2thinkinggguf_hugging_face/"&gt; &lt;img alt="ubergarm/Kimi-K2-Thinking-GGUF ¬∑ Hugging Face" src="https://external-preview.redd.it/-6vnf_3yTWf3TtVUA6a-SCJQHQSGAkjtdxEpaCd4oLc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1be9cda0b5e2a3434209c5a9d38f045a106ba74" title="ubergarm/Kimi-K2-Thinking-GGUF ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Great job ngxson, compilade, DevQuasar, Bartowski, AesSedai, and more folks who pulled together hacking on this one today! ü´∂&lt;/p&gt; &lt;p&gt;Only one quant released so far which is &lt;code&gt;q4_0&lt;/code&gt; for the routed experts and &lt;code&gt;q8_0&lt;/code&gt; for everything else. This is because the original model is released in roughly this size at &amp;quot;full quality&amp;quot;.&lt;/p&gt; &lt;p&gt;I've tested the quant on both ik_llama.cpp and mainline llama.cpp and it inferences fine. Though it wasn't giving me any &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; or &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; tags so you might have to fiddle with the template or something (model card shows how to just load whatever you want).&lt;/p&gt; &lt;p&gt;I may try some smaller quants for ik_llama.cpp to see if they hold up despite original model being QAT'd to ~4bpw. The &amp;quot;full size&amp;quot; weighs in at 543.617 GiB (4.549 BPW).&lt;/p&gt; &lt;p&gt;Have fun!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqo57j/ubergarmkimik2thinkinggguf_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqo57j/ubergarmkimik2thinkinggguf_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T07:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqiduq</id>
    <title>Kimi 2 is the #1 creative writing AI right now. better than sonnet 4.5</title>
    <updated>2025-11-07T02:20:29+00:00</updated>
    <author>
      <name>/u/Excellent-Run7265</name>
      <uri>https://old.reddit.com/user/Excellent-Run7265</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just tried Kimi 2 and I'm genuinely impressed. It's the best creative writer AI I've used‚Äîbetter than Sonnet 4.5, better than anything else out there. And it's dirt cheap compared to Sonnet.&lt;/p&gt; &lt;p&gt;I never thought a cheap, open model would beat Anthropic at writing. don't do coding as much, but its understanding is so strong that it's probably capable there too. This is amazing for us consumers.&lt;/p&gt; &lt;p&gt;The giants now have to slash prices significantly or lose to China. At this pace, we'll see locally-run LLMs outperforming current top models in months. That's terrible for big companies like OpenAI and Anthropic‚Äîthey'll need AGI or something massively better to justify their cost difference or cut the price down to half at least for now.&lt;/p&gt; &lt;p&gt;This market is unpredictable and wild. With the US and Chinese companies pushing each other like this and not holding back, AI will become so powerful so fast that we won't have to do anything ourselves anymore.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent-Run7265"&gt; /u/Excellent-Run7265 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqiduq/kimi_2_is_the_1_creative_writing_ai_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqiduq/kimi_2_is_the_1_creative_writing_ai_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqiduq/kimi_2_is_the_1_creative_writing_ai_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T02:20:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1oquezp</id>
    <title>Kimi K2 Thinking with sglang and mixed GPU / ktransformers CPU inference @ 31 tokens/sec</title>
    <updated>2025-11-07T13:28:44+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got Kimi K2 Thinking running locally and I'm blown away how fast it runs in simple chat tests: approximately ~ 30 tokens/sec with 4000 tokens in the context. Obviously a lot more testing to be done, but wow... a trillion parameter model running at 30 tokens/sec. &lt;/p&gt; &lt;p&gt;I'll whip up some tests around batching and available context lengths soon, but for now here's the recipe to get it running should you have the necessary hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: it looks like only the first API request works. Subsequent requests always cause sglang to crash and require a restart, regardless of how I configure things:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; File &amp;quot;/home/carl/ktransformers/ktransformers/.venv/lib/python3.11/site-packages/triton/compiler/compiler.py&amp;quot;, line 498, in __getattribute__ self._init_handles() File &amp;quot;/home/carl/ktransformers/ktransformers/.venv/lib/python3.11/site-packages/triton/compiler/compiler.py&amp;quot;, line 483, in _init_handles raise OutOfResources(self.metadata.shared, max_shared, &amp;quot;shared memory&amp;quot;) triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 106496, Hardware limit: 101376. Reducing block sizes or `num_stages` may help. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;System&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;EPYC &lt;del&gt;7B45&lt;/del&gt; 9B45 (128-core, 256 thread) CPU&lt;/li&gt; &lt;li&gt;768GB DDR5 6400 MT/s&lt;/li&gt; &lt;li&gt;4x RTX 6000 Pro Workstation 96GB GPUs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup virtual python environment&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mkdir sglang-ktransformers cd sglang-ktransformers uv venv --python 3.11 --seed . .venv/bin/activate &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Install sglang&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uv pip install &amp;quot;sglang&amp;quot; --prerelease=allow &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Download and initialize ktransformers repo&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/kvcache-ai/ktransformers cd ktransformers git submodule update --init --recursive &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Install ktransformers CPU kernel for sglang&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd kt-kernel export CPUINFER_CPU_INSTRUCT=AVX512 export CPUINFER_ENABLE_AMX=OFF uv pip install . cd .. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Download Kimi K2 Thinking GPU &amp;amp; CPU parts&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uv pip install -U hf hf_transfer hf download moonshotai/Kimi-K2-Thinking hf download KVCache-ai/Kimi-K2-Thinking-CPU-weight &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Run k2&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 python -m sglang.launch_server \ --host 0.0.0.0 --port 8080 \ --model ~/.cache/huggingface/hub/models--moonshotai--Kimi-K2-Thinking/snapshots/357b94aee9d50ec88e5e6dd9550fd7f957cb1baa \ --kt-amx-weight-path ~/.cache/huggingface/hub/models--KVCache-ai--Kimi-K2-Thinking-CPU-weight/snapshots/690ffacb9203d3b5e05ee8167ff1f5d4ae027c83 \ --kt-cpuinfer 252 \ --kt-threadpool-count 2 \ --kt-num-gpu-experts 238 \ --kt-amx-method AMXINT4 \ --attention-backend triton --trust-remote-code \ --mem-fraction-static 0.98 \ --chunked-prefill-size 4096 \ --max-running-requests 1 \ --max-total-tokens 32768 \ --enable-mixed-chunk \ --tensor-parallel-size 4 \ --enable-p2p-check \ --disable-shared-experts-fusion &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oquezp/kimi_k2_thinking_with_sglang_and_mixed_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oquezp/kimi_k2_thinking_with_sglang_and_mixed_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oquezp/kimi_k2_thinking_with_sglang_and_mixed_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T13:28:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqebr3</id>
    <title>World's strongest agentic model is now open source</title>
    <updated>2025-11-06T23:20:15+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqebr3/worlds_strongest_agentic_model_is_now_open_source/"&gt; &lt;img alt="World's strongest agentic model is now open source" src="https://preview.redd.it/jd607rvrzpzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f84c70ace26fdbd5db78313787e58d2403961e38" title="World's strongest agentic model is now open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jd607rvrzpzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqebr3/worlds_strongest_agentic_model_is_now_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqebr3/worlds_strongest_agentic_model_is_now_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T23:20:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqy1k7</id>
    <title>AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)</title>
    <updated>2025-11-07T15:53:33+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt; &lt;img alt="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" src="https://preview.redd.it/8v2luf5owuzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9bc34ec8dddd94422397eaa91e0310250da5ba3" title="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8v2luf5owuzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:53:33+00:00</published>
  </entry>
</feed>
