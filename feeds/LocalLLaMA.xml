<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-23T20:25:10+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ptxdtx</id>
    <title>Web-based GGUF recipe merger for GGUF-Tool-Suite</title>
    <updated>2025-12-23T15:55:32+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on making the GGUF-Tool-Suite more accessible, and as part of that effort I created a small web-based GGUF merger tool for GGUF-Tool-Suite recipe files:&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="https://gguf.thireus.com/quant_downloader.html"&gt;https://gguf.thireus.com/quant_downloader.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It lets you load a GGUF recipe and automatically merge/download the referenced model parts, with verification and resume support.&lt;/p&gt; &lt;p&gt;For anyone not familiar with the GGUF-Tool-Suite: it‚Äôs a toolchain where you input your VRAM and RAM constraints, and it generates a fine-tuned GGUF recipe for advanced users who want precise, automated, dynamic GGUF quant production.&lt;/p&gt; &lt;p&gt;Issues and feedback can be reported here: &lt;a href="https://github.com/Thireus/GGUF-Tool-Suite/"&gt;https://github.com/Thireus/GGUF-Tool-Suite/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxdtx/webbased_gguf_recipe_merger_for_gguftoolsuite/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxdtx/webbased_gguf_recipe_merger_for_gguftoolsuite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxdtx/webbased_gguf_recipe_merger_for_gguftoolsuite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T15:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptt4uf</id>
    <title>I integrated llama.cpp's new router mode into llamactl with web UI support</title>
    <updated>2025-12-23T12:49:34+00:00</updated>
    <author>
      <name>/u/RealLordMathis</name>
      <uri>https://old.reddit.com/user/RealLordMathis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've shared my project &lt;a href="https://github.com/lordmathis/llamactl"&gt;llamactl&lt;/a&gt; here a few times, and wanted to update you on some major new features, especially the integration of llama.cpp's recently released router mode.&lt;/p&gt; &lt;p&gt;Llamactl is a unified management system for running local LLMs across llama.cpp, MLX, and vLLM backends. It provides a web dashboard for managing instances along with an OpenAI-compatible API.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Router mode integration&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;llama.cpp recently introduced router mode for dynamic model management, and I've now integrated it into llamactl. You can now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create a llama.cpp instance without specifying a model&lt;/li&gt; &lt;li&gt;Load/unload models on-demand through the dashboard&lt;/li&gt; &lt;li&gt;Route requests using &lt;code&gt;&amp;lt;instance_name&amp;gt;/&amp;lt;model_name&amp;gt;&lt;/code&gt; syntax in your chat completion calls&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Current limitations&lt;/strong&gt; (both planned for future releases):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model preset configuration (.ini files) must be done manually for now&lt;/li&gt; &lt;li&gt;Model downloads aren't available through the UI yet (there's a hacky workaround)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Other recent additions&lt;/strong&gt; :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-node support - Deploy instances across different hosts for distributed setups&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Granular API key permissions - Create inference API keys with per-instance access control&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Docker support, log rotation, improved health checks, and more&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/lordmathis/llamactl"&gt;GitHub&lt;/a&gt;&lt;br /&gt; &lt;a href="https://llamactl.org/stable/"&gt;Docs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Always looking for feedback and contributions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RealLordMathis"&gt; /u/RealLordMathis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptt4uf/i_integrated_llamacpps_new_router_mode_into/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptt4uf/i_integrated_llamacpps_new_router_mode_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptt4uf/i_integrated_llamacpps_new_router_mode_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T12:49:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptz4fz</id>
    <title>Hey, where are the weights for Minimax M2.1?</title>
    <updated>2025-12-23T17:03:53+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People are waiting! Is it coming soon? It takes time for someone like Unsloth or MLX community to convert it into GGUF or MLX and upload it unless they did it already... Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptz4fz/hey_where_are_the_weights_for_minimax_m21/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptz4fz/hey_where_are_the_weights_for_minimax_m21/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptz4fz/hey_where_are_the_weights_for_minimax_m21/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T17:03:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt3sco</id>
    <title>I made Soprano-80M: Stream ultra-realistic TTS in &lt;15ms, up to 2000x realtime, and &lt;1 GB VRAM, released under Apache 2.0!</title>
    <updated>2025-12-22T16:24:36+00:00</updated>
    <author>
      <name>/u/eugenekwek</name>
      <uri>https://old.reddit.com/user/eugenekwek</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt3sco/i_made_soprano80m_stream_ultrarealistic_tts_in/"&gt; &lt;img alt="I made Soprano-80M: Stream ultra-realistic TTS in &amp;lt;15ms, up to 2000x realtime, and &amp;lt;1 GB VRAM, released under Apache 2.0!" src="https://external-preview.redd.it/NzJvcm5nM3g1czhnMTuHKMiW0LLPLmT-UAsj3QPgelU3LLUn7ZzaJN_zFkuW.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92e3fd36cfab3db61e8dc90b692c7a0d1918ce30" title="I made Soprano-80M: Stream ultra-realistic TTS in &amp;lt;15ms, up to 2000x realtime, and &amp;lt;1 GB VRAM, released under Apache 2.0!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I‚Äôm Eugene, and I‚Äôve been working on &lt;strong&gt;Soprano&lt;/strong&gt;: a new state-of-the-art TTS model I designed for voice chatbots. Voice applications require very low latency and natural speech generation to sound convincing, and I created Soprano to deliver on both of these goals.&lt;/p&gt; &lt;p&gt;Soprano is the world‚Äôs fastest TTS by an enormous margin. It is optimized to stream audio playback with &lt;strong&gt;&amp;lt;15 ms latency&lt;/strong&gt;, 10x faster than any other realtime TTS model like Chatterbox Turbo, VibeVoice-Realtime, GLM TTS, or CosyVoice3. It also natively supports batched inference, benefiting greatly from long-form speech generation. &lt;strong&gt;I was able to generate a 10-hour audiobook in under 20 seconds, achieving ~2000x realtime!&lt;/strong&gt; This is multiple orders of magnitude faster than any other TTS model, making ultra-fast, ultra-natural TTS a reality for the first time.&lt;/p&gt; &lt;p&gt;I owe these gains to the following design choices:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Higher sample rate:&lt;/strong&gt; most TTS models use a sample rate of 24 kHz, which can cause s and z sounds to be muffled. In contrast, Soprano natively generates 32 kHz audio, which sounds much sharper and clearer. In fact, 32 kHz speech sounds indistinguishable from 44.1/48 kHz speech, so I found it to be the best choice.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vocoder-based audio decoder:&lt;/strong&gt; Most TTS designs use diffusion models to convert LLM outputs into audio waveforms. However, this comes at the cost of slow generation. To fix this, I trained a vocoder-based decoder instead, which uses a Vocos model to perform this conversion. My decoder runs several orders of magnitude faster than diffusion-based decoders (~6000x realtime!), enabling extremely fast audio generation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Seamless Streaming:&lt;/strong&gt; Streaming usually requires generating multiple audio chunks and applying crossfade. However, this causes streamed output to sound worse than nonstreamed output. I solve this by using a Vocos-based decoder. Because Vocos has a finite receptive field. I can exploit its input locality to completely skip crossfading, producing streaming output that is identical to unstreamed output. Furthermore, I modified the Vocos architecture to reduce the receptive field, allowing Soprano to start streaming audio after generating just five audio tokens with the LLM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;State-of-the-art Neural Audio Codec:&lt;/strong&gt; Speech is represented using a novel neural codec that compresses audio to ~15 tokens/sec at just 0.2 kbps. This helps improve generation speed, as only 15 tokens need to be generated to synthesize 1 second of audio, compared to 25, 50, or other commonly used token rates. To my knowledge, this is the highest bitrate compression achieved by any audio codec.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Infinite generation length:&lt;/strong&gt; Soprano automatically generates each sentence independently, and then stitches the results together. Theoretically, this means that sentences can no longer influence each other, but in practice I found that this doesn‚Äôt really happen anyway. Splitting by sentences allows for batching on long inputs, dramatically improving inference speed. &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I‚Äôm a second-year undergrad who‚Äôs just started working on TTS models, so I wanted to start small. Soprano was only pretrained on 1000 hours of audio (~100x less than other TTS models), so its stability and quality will improve tremendously as I train it on more data. Also, I optimized Soprano purely for speed, which is why it lacks bells and whistles like voice cloning, style control, and multilingual support. Now that I have experience creating TTS models, I have a lot of ideas for how to make Soprano even better in the future, so stay tuned for those!&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/ekwek1/soprano"&gt;https://github.com/ekwek1/soprano&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Huggingface Demo: &lt;a href="https://huggingface.co/spaces/ekwek/Soprano-TTS"&gt;https://huggingface.co/spaces/ekwek/Soprano-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model Weights: &lt;a href="https://huggingface.co/ekwek/Soprano-80M"&gt;https://huggingface.co/ekwek/Soprano-80M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Eugene&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eugenekwek"&gt; /u/eugenekwek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/htwi2n2x5s8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt3sco/i_made_soprano80m_stream_ultrarealistic_tts_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt3sco/i_made_soprano80m_stream_ultrarealistic_tts_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T16:24:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptt5xj</id>
    <title>[PROJECT] I updated EntropyGuard a CLI tool to deduplicate RAG data locally on CPU before embedding. Saves ~40% tokens, handles 100GB+ files, and just got Checkpointing. (Open Source)</title>
    <updated>2025-12-23T12:51:02+00:00</updated>
    <author>
      <name>/u/Low-Flow-6572</name>
      <uri>https://old.reddit.com/user/Low-Flow-6572</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptt5xj/project_i_updated_entropyguard_a_cli_tool_to/"&gt; &lt;img alt="[PROJECT] I updated EntropyGuard a CLI tool to deduplicate RAG data locally on CPU before embedding. Saves ~40% tokens, handles 100GB+ files, and just got Checkpointing. (Open Source)" src="https://preview.redd.it/j1j9f7j6ay8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=365953652a5dbae69ef2b33ac18bd26caabdf93e" title="[PROJECT] I updated EntropyGuard a CLI tool to deduplicate RAG data locally on CPU before embedding. Saves ~40% tokens, handles 100GB+ files, and just got Checkpointing. (Open Source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Like many of you, I've been building local RAG pipelines and got tired of the &amp;quot;garbage in, garbage out&amp;quot; problem. I noticed my vector database (and context window) was often bloated with duplicate chunks, things like recurring headers/footers in PDFs, identical error logs, or scraped pages that are 99% the same.&lt;/p&gt; &lt;p&gt;This does two bad things:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Pollutes Retrieval:&lt;/strong&gt; Your &lt;code&gt;top-k&lt;/code&gt; slots get filled with 5 variations of the same sentence, pushing out unique/relevant info.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Wastes Compute:&lt;/strong&gt; You end up embedding (and storing) junk.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I didn't want to spin up a heavy vector DB cluster just to clean data, and I definitely didn't want to send my raw data to an external API for processing. I needed something that runs on my CPU so my GPU is free for inference.&lt;/p&gt; &lt;p&gt;So I built &lt;strong&gt;EntropyGuard&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It‚Äôs a standalone CLI tool designed to filter your datasets &lt;em&gt;before&lt;/em&gt; ingestion.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works (The &amp;quot;Hybrid&amp;quot; approach):&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Stage 1 (Fast):&lt;/strong&gt; It runs a fast hash (&lt;code&gt;xxhash&lt;/code&gt;) on the normalized text. This kills 100% identical duplicates instantly without touching neural networks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stage 2 (Smart):&lt;/strong&gt; The survivors go through a lightweight embedding model (default: &lt;code&gt;all-MiniLM-L6-v2&lt;/code&gt;) and FAISS to find &lt;em&gt;semantic&lt;/em&gt; duplicates.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;I just pushed v1.22 today with features for larger local datasets:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OOM Safe:&lt;/strong&gt; It uses chunked processing and Polars LazyFrames. I‚Äôve tested it on datasets larger than my RAM, and it doesn't crash.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Checkpoint &amp;amp; Resume:&lt;/strong&gt; If you're processing a massive dataset (e.g., 50GB) and your script dies at 90%, you can run &lt;code&gt;--resume&lt;/code&gt;. It picks up exactly where it left off.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unix Pipes:&lt;/strong&gt; It plays nice with bash. You can just: &lt;code&gt;cat data.jsonl | entropyguard --dedup-threshold 0.95 &amp;gt; clean.jsonl&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Stats:&lt;/strong&gt; On my machine, I'm seeing about ~6k rows/sec for the hashing stage. It tells you exactly how many &amp;quot;Tokens&amp;quot; you saved at the end of the run, which is satisfying to watch.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;License:&lt;/strong&gt; MIT. It's open source and runs entirely offline.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Link:&lt;/strong&gt;&lt;a href="https://github.com/DamianSiuta/entropyguard"&gt;https://github.com/DamianSiuta/entropyguard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôd love some feedback on the logic or performance. If you manage to break it with a weird dataset, let me know in the issues. If you find it useful for your local stack, a star on GitHub is always appreciated!&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Low-Flow-6572"&gt; /u/Low-Flow-6572 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j1j9f7j6ay8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptt5xj/project_i_updated_entropyguard_a_cli_tool_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptt5xj/project_i_updated_entropyguard_a_cli_tool_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T12:51:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptom2s</id>
    <title>exllamav3 adds support for GLM 4.7 (and 4.6V, + Ministral &amp; OLMO 3)</title>
    <updated>2025-12-23T08:13:58+00:00</updated>
    <author>
      <name>/u/Unstable_Llama</name>
      <uri>https://old.reddit.com/user/Unstable_Llama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lots of updates this month to exllamav3. Support added for &lt;a href="https://github.com/turboderp-org/exllamav3/commit/4d4992a8b82ae13edf86db2bb19e2de1c522c054"&gt;GLM 4.6V&lt;/a&gt;, &lt;a href="https://github.com/turboderp-org/exllamav3/commit/9b75bc5f58a70cb0e73c45f0bcd7d5959e124aa4"&gt;Ministral&lt;/a&gt;, and &lt;a href="https://github.com/turboderp-org/exllamav3/commit/104268521cdd1b24d19bcf92e5289b10219af5bd"&gt;OLMO 3&lt;/a&gt; (on the dev branch).&lt;/p&gt; &lt;p&gt;As GLM 4.7 is the same architecture as 4.6, it is already supported.&lt;/p&gt; &lt;p&gt;Several models from these families haven't been quantized and uploaded to HF yet, so if you can't find the one you are looking for, now is your chance to contribute to local AI!&lt;/p&gt; &lt;p&gt;Questions? Ask here or at the &lt;a href="https://discord.gg/wmrxvpdd"&gt;exllama discord&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unstable_Llama"&gt; /u/Unstable_Llama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptom2s/exllamav3_adds_support_for_glm_47_and_46v/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptom2s/exllamav3_adds_support_for_glm_47_and_46v/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptom2s/exllamav3_adds_support_for_glm_47_and_46v/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T08:13:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt5heq</id>
    <title>GLM 4.7 is out on HF!</title>
    <updated>2025-12-22T17:30:33+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt5heq/glm_47_is_out_on_hf/"&gt; &lt;img alt="GLM 4.7 is out on HF!" src="https://external-preview.redd.it/gR0grxFGZc9MSnGGFWbK39DsDkjKEI-u2jMcygDp6Nc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb92abe1e270f4c3e9d804bcff4653d2d0d7cc74" title="GLM 4.7 is out on HF!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt5heq/glm_47_is_out_on_hf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt5heq/glm_47_is_out_on_hf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T17:30:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptpzs3</id>
    <title>500Mb Text Anonymization model to remove PII from any text locally. Easily fine-tune on any language (see example for Spanish).</title>
    <updated>2025-12-23T09:44:31+00:00</updated>
    <author>
      <name>/u/Ok_Hold_5385</name>
      <uri>https://old.reddit.com/user/Ok_Hold_5385</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/tanaos/tanaos-text-anonymizer-v1"&gt;https://huggingface.co/tanaos/tanaos-text-anonymizer-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A small (500Mb, 0.1B params) but efficient Text Anonimization model which &lt;strong&gt;removes Personal Identifiable Information locally&lt;/strong&gt; from any type of text, without the need to send it to any third-party services or APIs.&lt;/p&gt; &lt;h1&gt;Use-case&lt;/h1&gt; &lt;p&gt;You need to share data with a colleague, a shareholder, a third-party service provider but it contains Personal Identifiable Information such as names, addresses or phone numbers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;tanaos-text-anonymizer-v1&lt;/strong&gt; allows you to automatically identify and replace all PII with placeholder text &lt;strong&gt;locally&lt;/strong&gt;, without sending the data to any external service or API.&lt;/p&gt; &lt;h1&gt;Example&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;The patient John Doe visited New York on 12th March 2023 at 10:30 AM. &amp;gt;&amp;gt;&amp;gt; The patient [MASKED] visited [MASKED] on [MASKED] at [MASKED]. &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Fine-tune on custom domain or language without labeled data&lt;/h1&gt; &lt;p&gt;Do you want to tailor the model to your specific domain (medical, legal, engineering etc.) or to a different language? Use the &lt;a href="https://github.com/tanaos/artifex"&gt;Artifex library&lt;/a&gt; to fine-tune the model by generating synthetic training data on-the-fly.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from artifex import Artifex ta = Artifex().text_anonymization model_output_path = &amp;quot;./output_model/&amp;quot; ta.train( domain=&amp;quot;documentos medicos en Espa√±ol&amp;quot;, output_path=model_output_path ) ta.load(model_output_path) print(ta(&amp;quot;El paciente John Doe visit√≥ Nueva York el 12 de marzo de 2023 a las 10:30 a. m.&amp;quot;)) # &amp;gt;&amp;gt;&amp;gt; [&amp;quot;El paciente [MASKED] visit√≥ [MASKED] el [MASKED] a las [MASKED].&amp;quot;] &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Hold_5385"&gt; /u/Ok_Hold_5385 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptpzs3/500mb_text_anonymization_model_to_remove_pii_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptpzs3/500mb_text_anonymization_model_to_remove_pii_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptpzs3/500mb_text_anonymization_model_to_remove_pii_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T09:44:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptx1pu</id>
    <title>gemma-3-4b-it-Cognitive-Liberty | Attempting to fix the "Lobotomy Tax" | MMLU Marketing 85%, Politics 83% | 0% Refusal</title>
    <updated>2025-12-23T15:41:53+00:00</updated>
    <author>
      <name>/u/AlexHardy08</name>
      <uri>https://old.reddit.com/user/AlexHardy08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been experimenting with a new fine-tuning approach to address a common issue with &amp;quot;uncensored&amp;quot; models: usually, when you strip away the safety rails (abliteration/unaligning), the model loses IQ points. It becomes compliant but incoherent, or just agrees with everything you say.&lt;/p&gt; &lt;p&gt;I wanted to see if I could create a model that has &lt;strong&gt;zero refusals&lt;/strong&gt; but maintains (or improves) deep reasoning capabilities.&lt;/p&gt; &lt;p&gt;I used google/gemma-3-4b-it as the base and fine-tuned it on a custom synthetic dataset (&lt;strong&gt;Cognitive Liberty V3&lt;/strong&gt;) focused heavily on philosophy, evolutionary game theory, and complex systems analysis, rather than just generic RP or chat data.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Result: gemma-3-4b-it-Cognitive-Liberty&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is an aggressive fine-tune (&lt;strong&gt;KL Divergence: 1.14&lt;/strong&gt;), which usually signals brain damage in a model. However, benchmarks suggest it actually specialized rather than degraded. It has turned into a bit of a &amp;quot;Humanities/Social Science&amp;quot; expert.&lt;/p&gt; &lt;h1&gt;üìä Benchmark Highlights (MMLU 5-shot)&lt;/h1&gt; &lt;p&gt;It matches the base model's overall MMLU (~58%) but drastically shifts the distribution:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üß† &lt;strong&gt;Marketing:&lt;/strong&gt; 85.04% (This is abnormally high for a 4B model)&lt;/li&gt; &lt;li&gt;üèõÔ∏è &lt;strong&gt;Government &amp;amp; Politics:&lt;/strong&gt; 83.94%&lt;/li&gt; &lt;li&gt;üó£Ô∏è &lt;strong&gt;Sociology:&lt;/strong&gt; 77.61%&lt;/li&gt; &lt;li&gt;üß© &lt;strong&gt;Logical Fallacies:&lt;/strong&gt; 74.85%&lt;/li&gt; &lt;li&gt;üß† &lt;strong&gt;Psychology:&lt;/strong&gt; 79.63%&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The &amp;quot;Moral Anomaly&amp;quot; (Feature, not bug)&lt;/h1&gt; &lt;p&gt;You'll see a low score on &lt;strong&gt;Moral Scenarios&lt;/strong&gt; (30.61%).&lt;br /&gt; Standard benchmarks expect binary, safe answers (e.g., &amp;quot;Is doing X bad? -&amp;gt; Yes&amp;quot;). Because this model is trained to analyze nuance (utilitarianism vs deontology), it often over-analyzes simple moral questions or refuses to give the &amp;quot;standard&amp;quot; safety answer. In my testing, this results in better conversation, even if it hurts the automated score.&lt;/p&gt; &lt;h1&gt;Usage&lt;/h1&gt; &lt;p&gt;It‚Äôs a 4B model, so it runs on basically anything (even phones/consumer GPUs). I find it works best for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Debating controversial topics (it won't lecture you).&lt;/li&gt; &lt;li&gt;Analyzing manipulation tactics/marketing.&lt;/li&gt; &lt;li&gt;Creative writing where you need a &amp;quot;Machiavellian&amp;quot; character.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Link to Model:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/AiAsistent/gemma-3-4b-it-Cognitive-Liberty"&gt;https://huggingface.co/AiAsistent/gemma-3-4b-it-Cognitive-Liberty&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm looking for feedback on how it handles logic puzzles and edge cases compared to the stock Gemma 3. Let me know if you break it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexHardy08"&gt; /u/AlexHardy08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptx1pu/gemma34bitcognitiveliberty_attempting_to_fix_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptx1pu/gemma34bitcognitiveliberty_attempting_to_fix_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptx1pu/gemma34bitcognitiveliberty_attempting_to_fix_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T15:41:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptzft4</id>
    <title>Representation Engineering / activation steering: ‚Äúprompting vs finetuning vs steering vectors‚Äù (practical notes + demo)</title>
    <updated>2025-12-23T17:16:21+00:00</updated>
    <author>
      <name>/u/AstraNorth</name>
      <uri>https://old.reddit.com/user/AstraNorth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptzft4/representation_engineering_activation_steering/"&gt; &lt;img alt="Representation Engineering / activation steering: ‚Äúprompting vs finetuning vs steering vectors‚Äù (practical notes + demo)" src="https://preview.redd.it/pbgq9willz8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57ce9f92e2504d518c58e6584a3a818c9e0c7865" title="Representation Engineering / activation steering: ‚Äúprompting vs finetuning vs steering vectors‚Äù (practical notes + demo)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been exploring Representation Engineering (RepE) / activation steering recently and it feels like a useful ‚Äúthird lever‚Äù between prompting and fine-tuning.‚Äã&lt;/p&gt; &lt;p&gt;High-level framing (practitioner view):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompting: fast to iterate, but persona/behavior can drift over long contexts.‚Äã&lt;/li&gt; &lt;li&gt;Fine-tuning: powerful but costly, and it can trade off generality if you push it too hard.‚Äã&lt;/li&gt; &lt;li&gt;Steering (activations): keep weights fixed and add a learned ‚Äúdirection‚Äù in hidden states at inference time (steering vectors), so you can nudge behavior without huge prompts or retraining.‚Äã&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The demo that made it click for me is ‚ÄúThe Eiffel Tower Llama‚Äù (Hugging Face Space / walkthrough): &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=F2jd5WuT-zg"&gt;https://www.youtube.com/watch?v=F2jd5WuT-zg&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What‚Äôs interesting is how concrete the concept becomes: you find a direction corresponding to some concept (toy example: ‚ÄúEiffel Tower‚Äù; more generally: honesty/helpfulness/positivity/etc.) and then add/subtract that vector during generation to shift outputs.‚Äã‚Äã&lt;/p&gt; &lt;p&gt;Questions for folks here who‚Äôve implemented this in real setups:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What‚Äôs your go-to method for discovering robust steering directions (contrastive pairs? probes? SAEs?) and which layers tend to be the most controllable?‚Äã&lt;/li&gt; &lt;li&gt;Have you seen steering reliably stack for multi-concept control, or does it quickly start to interfere (one concept breaking another / hurting instruction-following)?‚Äã&lt;/li&gt; &lt;li&gt;Any best practices for evaluating side effects (capability loss, new biases, safety regressions) beyond qualitative samples?‚Äã&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love pointers to good repos, eval recipes, or ‚Äúgotchas‚Äù you‚Äôve hit when moving from toy demos to actual workflows.‚Äã&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AstraNorth"&gt; /u/AstraNorth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pbgq9willz8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptzft4/representation_engineering_activation_steering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptzft4/representation_engineering_activation_steering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T17:16:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptm3n4</id>
    <title>GLM 4.7 top the chart at Rank #6 in WebDev</title>
    <updated>2025-12-23T05:43:37+00:00</updated>
    <author>
      <name>/u/GeLaMi-Speaker</name>
      <uri>https://old.reddit.com/user/GeLaMi-Speaker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptm3n4/glm_47_top_the_chart_at_rank_6_in_webdev/"&gt; &lt;img alt="GLM 4.7 top the chart at Rank #6 in WebDev" src="https://b.thumbs.redditmedia.com/s0EknbN86ZnaumUiKb5KC7z3WQ1YE737LQCzHo-j3pg.jpg" title="GLM 4.7 top the chart at Rank #6 in WebDev" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.7"&gt;https://huggingface.co/zai-org/GLM-4.7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GeLaMi-Speaker"&gt; /u/GeLaMi-Speaker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ptm3n4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptm3n4/glm_47_top_the_chart_at_rank_6_in_webdev/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptm3n4/glm_47_top_the_chart_at_rank_6_in_webdev/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T05:43:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptk5fs</id>
    <title>Unsloth GLM-4.7 GGUF</title>
    <updated>2025-12-23T04:01:15+00:00</updated>
    <author>
      <name>/u/Wooden-Deer-1276</name>
      <uri>https://old.reddit.com/user/Wooden-Deer-1276</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.7-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.7-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden-Deer-1276"&gt; /u/Wooden-Deer-1276 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptk5fs/unsloth_glm47_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptk5fs/unsloth_glm47_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptk5fs/unsloth_glm47_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T04:01:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptq7rc</id>
    <title>GLM 4.7 vs. Minimax M2.1. My test &amp; subscription decision</title>
    <updated>2025-12-23T09:59:41+00:00</updated>
    <author>
      <name>/u/Psychological_Box406</name>
      <uri>https://old.reddit.com/user/Psychological_Box406</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been really excited about these two releases since I subscribed to both as potential offloads for my Claude Pro subscription.&lt;/p&gt; &lt;p&gt;I grabbed the GLM 4.7 subscription in early October on the quarterly plan (expires in ~2 weeks), and the Minimax M2.1 $2/month plan about 3 weeks ago to test it out. With both subscriptions ending soon, I needed to figure out which one to renew.&lt;/p&gt; &lt;p&gt;Since subscribing to Minimax M2.1, it's been my go-to model. But I wanted to see if GLM 4.7 had improved enough to make me switch back.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Test&lt;/strong&gt;&lt;br /&gt; I ran both models on the same prompt (in Claude Code) to generate e2e tests for a new feature I'm implementing in an application I'm building. Nothing complicated, two tables (1:N relationship), model, repo, service, controller, validator, routes. Pretty standard stuff.&lt;/p&gt; &lt;p&gt;I set up an agent with all the project's patterns, examples, and context for e2e testing. The models' job was to review the implementation done and instruct the agent to generate the new e2e.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GLM 4.7&lt;/strong&gt;: Ran for 70 minutes straight without finishing. Tests kept failing. I've had enough and stopped it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Minimax M2.1&lt;/strong&gt;: Finished in 40 minutes with clean, working tests.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;But&lt;/strong&gt;&lt;br /&gt; The interesting part is, even though GLM 4.7 failed to finish, it actually caught a flaw in my implementation during testing. Minimax M2.1, on the other hand, just bent the tests to make them pass without flagging the design issue.&lt;/p&gt; &lt;p&gt;I‚Äôll be sticking with Minimax for now, but I‚Äôm going to update my agent‚Äôs docs and constraints so it catches that kind of design flaw in the future.&lt;/p&gt; &lt;p&gt;I'm thinking about grabbing the GLM yearly promo at $29 just to have it on hand in case they drop a significantly faster and more capable version (GLM 5?). But for now, Minimax M2.1 wins on speed and reliability for me.&lt;/p&gt; &lt;p&gt;Also, Minimax, where is the Christmas promo like others are doing ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological_Box406"&gt; /u/Psychological_Box406 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptq7rc/glm_47_vs_minimax_m21_my_test_subscription/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptq7rc/glm_47_vs_minimax_m21_my_test_subscription/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptq7rc/glm_47_vs_minimax_m21_my_test_subscription/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T09:59:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptdtmz</id>
    <title>DGX Spark: an unpopular opinion</title>
    <updated>2025-12-22T23:05:29+00:00</updated>
    <author>
      <name>/u/emdblc</name>
      <uri>https://old.reddit.com/user/emdblc</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptdtmz/dgx_spark_an_unpopular_opinion/"&gt; &lt;img alt="DGX Spark: an unpopular opinion" src="https://preview.redd.it/cktkoyb16u8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ddc1aaf35931031505022ffcc5838d1fb7a1a8ea" title="DGX Spark: an unpopular opinion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know there has been a lot of criticism about the DGX Spark here, so I want to share some of my personal experience and opinion:&lt;/p&gt; &lt;p&gt;I‚Äôm a doctoral student doing data science in a small research group that doesn‚Äôt have access to massive computing resources. We only have a handful of V100s and T4s in our local cluster, and limited access to A100s and L40s on the university cluster (two at a time). Spark lets us prototype and train foundation models, and (at last) compete with groups that have access to high performance GPUs like the H100s or H200s.&lt;/p&gt; &lt;p&gt;I want to be clear: Spark is NOT faster than an H100 (or even a 5090). But its all-in-one design and its massive amount of memory (all sitting on your desk) enable us ‚Äî a small group with limited funding, to do more research.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emdblc"&gt; /u/emdblc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cktkoyb16u8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptdtmz/dgx_spark_an_unpopular_opinion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptdtmz/dgx_spark_an_unpopular_opinion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T23:05:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pty0kf</id>
    <title>Intel x Nvidia Serpent Lake leaks as Strix Halo rival: capable CPU, RTX Rubin iGPU, 16x LPDDR6.</title>
    <updated>2025-12-23T16:20:02+00:00</updated>
    <author>
      <name>/u/CYTR_</name>
      <uri>https://old.reddit.com/user/CYTR_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pty0kf/intel_x_nvidia_serpent_lake_leaks_as_strix_halo/"&gt; &lt;img alt="Intel x Nvidia Serpent Lake leaks as Strix Halo rival: capable CPU, RTX Rubin iGPU, 16x LPDDR6." src="https://external-preview.redd.it/zDVzxhAxQ5w8_AjaN1AtROq_Cu1ubeOIqtKDbvjiOZA.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71ee57553bd88813a2db74fe1c6fa688b0b6458a" title="Intel x Nvidia Serpent Lake leaks as Strix Halo rival: capable CPU, RTX Rubin iGPU, 16x LPDDR6." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;These powerful RTX iGPUs are reportedly coming with Intel Serpent Lake. Described as Intel's response to AMD Strix Halo/ Zen 6 Medusa Halo APUs...&lt;/p&gt; &lt;p&gt;[...]&lt;/p&gt; &lt;p&gt;For the GPU chiplet, Intel is said to be partnering with Nvidia to use the latter's RTX Rubin GPU architecture, or a close variant, for integrated graphics. The iGPU could be based on the TSMC N3P process node, which is to be expected.&lt;/p&gt; &lt;p&gt;Moreover, the leaker suggests that the Serpent Lake APUs could also bring support for 16X LPDDR6 memory. This likely refers to Serpent Lake supporting 16 memory channels for increased bandwidth.&amp;quot;&lt;/p&gt; &lt;p&gt;Potentially very interesting if nothing dethrones CUDA in the coming years and if Medusa Halo is disappointing from a bandwidth perspective. Of course, we can expect a prohibitive price and certainly a very late release given the current context.&lt;/p&gt; &lt;p&gt;Time will tell.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CYTR_"&gt; /u/CYTR_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.notebookcheck.net/Intel-x-Nvidia-Serpent-Lake-leaks-as-Strix-Halo-rival-with-capable-CPU-and-big-GeForce-RTX-Rubin-iGPU.1190608.0.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pty0kf/intel_x_nvidia_serpent_lake_leaks_as_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pty0kf/intel_x_nvidia_serpent_lake_leaks_as_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:20:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu2bwy</id>
    <title>New Update - Mistral Vibe v1.3.0</title>
    <updated>2025-12-23T19:10:57+00:00</updated>
    <author>
      <name>/u/Nefhis</name>
      <uri>https://old.reddit.com/user/Nefhis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new &lt;a href="https://github.com/mistralai/mistral-vibe"&gt;&lt;strong&gt;Vibe&lt;/strong&gt;&lt;/a&gt; update is here! We‚Äôre keeping the momentum going by including &lt;a href="https://agentskills.io/home"&gt;Agent Skills&lt;/a&gt; in this latest Vibe update. Agent Skills are &lt;strong&gt;collections of instructions, scripts, and resources that agents can discover and use to perform tasks&lt;/strong&gt; more accurately and efficiently.&lt;/p&gt; &lt;h1&gt;Changelog&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Agent Skills Support&lt;/li&gt; &lt;li&gt;Native Terminal Theme Support&lt;/li&gt; &lt;li&gt;Reasoning Models Support&lt;/li&gt; &lt;li&gt;Multiple Bug Fixes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;-# Learn more about the changes &lt;a href="https://github.com/mistralai/mistral-vibe/blob/main/CHANGELOG.md#130---2025-12-23"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Happy shipping - and happy holidays!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;-&amp;gt; &lt;code&gt;uv tool install mistral-vibe&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nefhis"&gt; /u/Nefhis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu2bwy/new_update_mistral_vibe_v130/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu2bwy/new_update_mistral_vibe_v130/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pu2bwy/new_update_mistral_vibe_v130/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T19:10:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptr3lv</id>
    <title>r/LocalLLaMA - a year in review</title>
    <updated>2025-12-23T10:56:05+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptr3lv/rlocalllama_a_year_in_review/"&gt; &lt;img alt="r/LocalLLaMA - a year in review" src="https://external-preview.redd.it/JtVvJz_3p9gkpLWB6adlC3-5M7zJIyvCc23zaC-6JV0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4f54df711befdabb904b0d3f68bc21eb350d5a4" title="r/LocalLLaMA - a year in review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm the same guy that made &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hov3y9/rlocalllama_a_year_in_review/"&gt;2024 edition&lt;/a&gt;, here we are again.&lt;/p&gt; &lt;p&gt;This community has been the central hub for open-source AI for another year, and what a year 2025 has been. Let me take you back to the most notable things happened here during this time. This isn't really a list of model releases or papers, rather posts that were discussed and upvoted by the people here. So notable things missing is also an indication of what was going on. From the rise of Chinese open-source dominance to the hardware hacks, here is what happened in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; in 2025.&lt;/p&gt; &lt;p&gt;The year started with a splash. The &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ho27fr/the_whale_has_landed/"&gt;arrival of &amp;quot;The Whale&amp;quot;&lt;/a&gt; (2121 upvotes, by &lt;a href="/u/fourDnet"&gt;u/fourDnet&lt;/a&gt;) marked the release of DeepSeek V3, setting the tone for what would become the &amp;quot;Year of the Open Source Strike Back.&amp;quot; It wasn't long before we saw &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hphlz7/sam_altman_is_taking_veiled_shots_at_deepseek_and/"&gt;Sam Altman taking veiled shots&lt;/a&gt; (1959 upvotes) at the new competition, a clear sign that the market was changing.&lt;/p&gt; &lt;p&gt;We were all trying to figure out how to run these new beasts. Nvidia teased us with the &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1hvj4wn/nvidia_announces_3000_personal_ai_supercomputer/"&gt;Digits personal AI supercomputer&lt;/a&gt; (1663 upvotes, by &lt;a href="/u/DubiousLLM"&gt;u/DubiousLLM&lt;/a&gt;), while others were just trying to understand the sheer scale of what was happening. The realization that &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i80cwf/deepseek_is_a_side_project/"&gt;DeepSeek was essentially a side project&lt;/a&gt; (2861 upvotes, by &lt;a href="/u/ParsaKhaz"&gt;u/ParsaKhaz&lt;/a&gt;) for a hedge fund only made it even more interesting.&lt;/p&gt; &lt;p&gt;By late January, the narrative was clear: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i88g4y/meta_panicked_by_deepseek/"&gt;Meta was panicked&lt;/a&gt; (2779 upvotes, by &lt;a href="/u/Optimal_Hamster5789"&gt;u/Optimal_Hamster5789&lt;/a&gt;), reportedly &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ibk9us/meta_is_reportedly_scrambling_multiple_war_rooms/"&gt;scrambling &amp;quot;war rooms&amp;quot;&lt;/a&gt; (2117 upvotes, by &lt;a href="/u/FullstackSensei"&gt;u/FullstackSensei&lt;/a&gt;) to catch up. The community was buzzing with benchmarks, with &lt;a href="/u/kyazoglu"&gt;u/kyazoglu&lt;/a&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i8tx5z/i_benchmarked_almost_every_model_that_can_fit_in/"&gt;testing almost every model that fits in 24GB VRAM&lt;/a&gt; (1861 upvotes) - a hero's work for the GPU-poor among us.&lt;/p&gt; &lt;p&gt;The &amp;quot;DeepSeek effect&amp;quot; was everywhere. &lt;a href="/u/Porespellar"&gt;u/Porespellar&lt;/a&gt; summed it up perfectly: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iji47x/all_deepseek_all_the_time/"&gt;&amp;quot;All DeepSeek, all the time&amp;quot;&lt;/a&gt; (4116 upvotes). But it wasn't just about models; it was about what we could &lt;em&gt;do&lt;/em&gt; with them. We saw inspiring projects like &lt;a href="/u/Dry_Steak30"&gt;u/Dry_Steak30&lt;/a&gt;'s &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ij5yf2/how_i_built_an_open_source_ai_tool_to_find_my/"&gt;open source tool to find their autoimmune disease&lt;/a&gt; (2488 upvotes), proving that local AI is more than just a hobby.&lt;/p&gt; &lt;p&gt;Of course, it wouldn't be 2025 without some drama. The threat of &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1igc6r0/20_yrs_in_jail_or_1_million_for_downloading/"&gt;20 years in jail for downloading Chinese models&lt;/a&gt; (2092 upvotes, by &lt;a href="/u/segmond"&gt;u/segmond&lt;/a&gt;) worried us, but that didn't stop the innovation. We laughed when &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iwb5nu/groks_think_mode_leaks_system_prompt/"&gt;Grok's think mode leaked its system prompt&lt;/a&gt; (6465 upvotes, by &lt;a href="/u/onil_gova"&gt;u/onil_gova&lt;/a&gt;), and cheered when DeepSeek announced they would &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iui6nk/starting_next_week_deepseek_will_opensource_5/"&gt;open-source 5 repos&lt;/a&gt; (4560 upvotes, by &lt;a href="/u/Nunki08"&gt;u/Nunki08&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;Hardware remained a constant obsession. We drooled over &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iy2t7c/frameworks_new_ryzen_max_desktop_with_128gb/"&gt;Framework's new Ryzen Max desktop&lt;/a&gt; (2004 upvotes, by &lt;a href="/u/sobe3249"&gt;u/sobe3249&lt;/a&gt;) and marveled at the monstrosity that was &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j67bxt/16x_3090s_its_alive/"&gt;16x 3090s&lt;/a&gt; (1797 upvotes, by &lt;a href="/u/Conscious_Cut_6144"&gt;u/Conscious_Cut_6144&lt;/a&gt;). &amp;quot;It's alive!&amp;quot; indeed.&lt;/p&gt; &lt;p&gt;Spring brought the highly anticipated Llama 4. Mark Zuckerberg &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jsampe/mark_presenting_four_llama_4_models_even_a_2/"&gt;presented the models&lt;/a&gt; (2645 upvotes, by &lt;a href="/u/LarDark"&gt;u/LarDark&lt;/a&gt;), but the community felt it &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jt7hlc/metas_llama_4_fell_short/"&gt;fell short&lt;/a&gt; (2175 upvotes, by &lt;a href="/u/Rare-Site"&gt;u/Rare-Site&lt;/a&gt;). The community was let down, especially when compared to the relentless release schedule from the East.&lt;/p&gt; &lt;p&gt;Open Weight releases continued, though, we got &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1juni3t/deepcoder_a_fully_opensource_14b_coder_at_o3mini/"&gt;DeepCoder&lt;/a&gt; (1609 upvotes, by &lt;a href="/u/TKGaming_11"&gt;u/TKGaming_11&lt;/a&gt;) and saw &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jytw62/deepseek_is_about_to_opensource_their_inference/"&gt;DeepSeek open-sourcing their inference engine&lt;/a&gt; (1760 upvotes, by &lt;a href="/u/Dr_Karminski"&gt;u/Dr_Karminski&lt;/a&gt;). There was also a moment of collective frustration when &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1jzocoo/finally_someone_noticed_this_unfair_situation/"&gt;llama.cpp was snubbed&lt;/a&gt; (1742 upvotes, by &lt;a href="/u/nekofneko"&gt;u/nekofneko&lt;/a&gt;) in favor of shinier wrappers.&lt;/p&gt; &lt;p&gt;Then came &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ka6mic/qwen_3/"&gt;Qwen 3&lt;/a&gt; (1940 upvotes, by &lt;a href="/u/ResearchCrafty1804"&gt;u/ResearchCrafty1804&lt;/a&gt;). The excitement was back. We were running &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1klx9q2/realtime_webcam_demo_with_smolvlm_using_llamacpp/"&gt;real-time webcam demos with SmolVLM&lt;/a&gt; (2762 upvotes, by &lt;a href="/u/dionisioalcaraz"&gt;u/dionisioalcaraz&lt;/a&gt;) and building &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ktx15j/guys_i_managed_to_build_a_100_fully_local_voice/"&gt;fully local voice AIs&lt;/a&gt; (2447 upvotes, by &lt;a href="/u/RoyalCities"&gt;u/RoyalCities&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;The reality of our hardware addiction hit hard with the question: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ktlz3w/96gb_vram_what_should_run_first/"&gt;&amp;quot;96GB VRAM! What should run first?&amp;quot;&lt;/a&gt; (1745 upvotes, by &lt;a href="/u/Mother_Occasion_8076"&gt;u/Mother_Occasion_8076&lt;/a&gt;). And as &lt;a href="/u/TheLogiqueViper"&gt;u/TheLogiqueViper&lt;/a&gt; noted, &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1kzsa70/china_is_leading_open_source/"&gt;China is leading open source&lt;/a&gt; (2618 upvotes).&lt;/p&gt; &lt;p&gt;We found humor in the absurdity of it all. &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1l6ibwg/when_you_figure_out_its_all_just_math/"&gt;&amp;quot;When you figure out it‚Äôs all just math&amp;quot;&lt;/a&gt; (4123 upvotes, by &lt;a href="/u/Current-Ticket4214"&gt;u/Current-Ticket4214&lt;/a&gt;) was a top post, and we all related to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1l1qqdx/at_the_airport_people_watching_while_i_run_models/"&gt;running models at the airport&lt;/a&gt; (2378 upvotes, by &lt;a href="/u/Current-Ticket4214"&gt;u/Current-Ticket4214&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;Summer was a season of delays and parodies. &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/"&gt;&amp;quot;We have to delay it&amp;quot;&lt;/a&gt; (3574 upvotes, by &lt;a href="/u/ILoveMy2Balls"&gt;u/ILoveMy2Balls&lt;/a&gt;) became the catchphrase for Western labs. We poked fun with a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1laee7q/got_a_tester_version_of_the_openweight_openai/"&gt;tester version of the &amp;quot;open-weight&amp;quot; OpenAI model&lt;/a&gt; (1639 upvotes, by &lt;a href="/u/Firepal64"&gt;u/Firepal64&lt;/a&gt;) and a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/"&gt;friendly reminder about Grok 3&lt;/a&gt; (1447 upvotes, by &lt;a href="/u/Wrong_User_Logged"&gt;u/Wrong_User_Logged&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;But the community kept building. &lt;a href="/u/hotroaches4liferz"&gt;u/hotroaches4liferz&lt;/a&gt; made a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m39uqi/i_made_a_1000_hour_nsfw_tts_dataset/"&gt;1000 hour NSFW TTS dataset&lt;/a&gt; (1516 upvotes)-because of course they did. &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m6qdet/qwen3coder_is_here/"&gt;Qwen3-Coder arrived&lt;/a&gt; (1925 upvotes, by &lt;a href="/u/ResearchCrafty1804"&gt;u/ResearchCrafty1804&lt;/a&gt;), followed by the blazing fast &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1me31d8/qwen3coderflash_released/"&gt;Qwen3-Coder-Flash&lt;/a&gt; (1694 upvotes).&lt;/p&gt; &lt;p&gt;The sentiment shifted as Meta seemingly bowed out of open source: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1md6t2h/bye_bye_meta_ai_it_was_good_while_it_lasted/"&gt;&amp;quot;Bye bye, Meta AI&amp;quot;&lt;/a&gt; (1492 upvotes, by &lt;a href="/u/absolooot1"&gt;u/absolooot1&lt;/a&gt;). Meanwhile, we got the adorable &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"&gt;Kitten TTS&lt;/a&gt; (2460 upvotes, by &lt;a href="/u/ElectricalBar7464"&gt;u/ElectricalBar7464&lt;/a&gt;) and continued to dream of &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mllt5x/imagine_an_open_source_code_model_that_in_the/"&gt;open source code models rivaling Claude&lt;/a&gt; (2304 upvotes, by &lt;a href="/u/Severe-Awareness829"&gt;u/Severe-Awareness829&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;&lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; remained &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mnxodk/localllama_is_the_last_sane_place_to_discuss_llms/"&gt;&amp;quot;the last sane place to discuss LLMs&amp;quot;&lt;/a&gt; (2181 upvotes, by &lt;a href="/u/ForsookComparison"&gt;u/ForsookComparison&lt;/a&gt;). Even if we did have to vent about &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mncrqp/ollama/"&gt;Ollama&lt;/a&gt; (1906 upvotes, by &lt;a href="/u/jacek2023"&gt;u/jacek2023&lt;/a&gt;) occasionally.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/"&gt;China entering the GPU market&lt;/a&gt; (4171 upvotes, by &lt;a href="/u/CeFurkan"&gt;u/CeFurkan&lt;/a&gt;) with 96GB cards for under $2000 was a game-changer. Some of us even went to Shenzhen to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt;buy modded 4090s&lt;/a&gt; (1924 upvotes, by &lt;a href="/u/king_priam_of_Troy"&gt;u/king_priam_of_Troy&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;We celebrated the &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nz722n/biggest_provider_for_the_community_for_at_moment/"&gt;biggest providers for the community&lt;/a&gt; (2918 upvotes, by &lt;a href="/u/dead-supernova"&gt;u/dead-supernova&lt;/a&gt;)-mostly Chinese labs now-and devoured &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"&gt;Stanford's 5.5hrs of lectures&lt;/a&gt; (2731 upvotes, by &lt;a href="/u/igorwarzocha"&gt;u/igorwarzocha&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;The year ended with a mix of high-level tools and deep-dive resources. We got &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/"&gt;Heretic for automatic censorship removal&lt;/a&gt; (3008 upvotes, by &lt;a href="/u/-p-e-w-"&gt;u/-p-e-w-&lt;/a&gt;) and &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ok3xie/200_pages_of_hugging_face_secrets_on_how_to_train/"&gt;200+ pages of Hugging Face secrets&lt;/a&gt; (2204 upvotes, by &lt;a href="/u/eliebakk"&gt;u/eliebakk&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;And finally, the memes kept us grounded. The &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/"&gt;Realist meme of the year&lt;/a&gt; (1926 upvotes, by &lt;a href="/u/Slight_Tone_2188"&gt;u/Slight_Tone_2188&lt;/a&gt;) reminded us that no matter how advanced the models get, we'll always be RAM poor from now on.&lt;/p&gt; &lt;p&gt;That's it, folks. 2025 was the year the open-source torch passed to the East, the year our hardware dreams got a little wilder (and insanely more expensive). Here's to another year of local LLMs!&lt;/p&gt; &lt;p&gt;P.S. I wasn't going to make a recap this year, but &lt;a href="https://gist.github.com/qingy1337"&gt;qingy1337&lt;/a&gt; kindly asked on GitHub if I would which touched me. So here it is!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptr3lv/rlocalllama_a_year_in_review/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptr3lv/rlocalllama_a_year_in_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptr3lv/rlocalllama_a_year_in_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T10:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptytig</id>
    <title>Two new 12B finetunes for adventure, role play and writing</title>
    <updated>2025-12-23T16:52:08+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This one was &lt;strong&gt;cooking for ~4 month&lt;/strong&gt;. I'll give here the TL;DR for each model, for full details, check the model cards:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Impish_Bloodmoon_12B&lt;/strong&gt; üòà&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Frontier-adjacent like capabilities, now locally available in 12B! (Stats, items, traits triggering, and so much more).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Very strong theory of mind!&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Well over &lt;strong&gt;1B&lt;/strong&gt; tokens trained!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fallout &amp;amp; Morrowind&lt;/strong&gt; fandom refined!&lt;/li&gt; &lt;li&gt;Heat turned to &lt;strong&gt;11&lt;/strong&gt;!&lt;/li&gt; &lt;li&gt; Additional languages added: Japanese, Hebrew, Russian.&lt;/li&gt; &lt;li&gt;1-shot JSON roleplay datasets! Escape velocity reached! (even for those who can't run DSV3 \ Kimi).&lt;/li&gt; &lt;li&gt;Less positivity bias , all lessons from the successful Negative_LLAMA_70B style of data learned &amp;amp; integrated, with serious upgrades added ‚Äî and it shows! (Note: if this bites you a bit too hard, try Angelic_Eclipse_12B. üëº)&lt;/li&gt; &lt;li&gt;Reduced slop for both roleplay and creative tasks.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Angelic_Eclipse_12B&lt;/strong&gt; üëº&lt;/p&gt; &lt;p&gt;Very similar capabilities to the above, but:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Reactions realism&lt;/strong&gt;. It meant to reflect real-life behaviour accurately&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Slow burn&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Powerful 'vanilla assistant'&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The models are &lt;strong&gt;available on HuggingFace&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Impish_Bloodmoon_12B"&gt;https://huggingface.co/SicariusSicariiStuff/Impish_Bloodmoon_12B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Angelic_Eclipse_12B"&gt;https://huggingface.co/SicariusSicariiStuff/Angelic_Eclipse_12B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptytig/two_new_12b_finetunes_for_adventure_role_play_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptytig/two_new_12b_finetunes_for_adventure_role_play_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptytig/two_new_12b_finetunes_for_adventure_role_play_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:52:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptz6xy</id>
    <title>AudioGhost AI: Run Meta's SAM-Audio on 4GB-6GB VRAM with a Windows One-Click Installer üëªüéµ</title>
    <updated>2025-12-23T17:06:40+00:00</updated>
    <author>
      <name>/u/GGwithRabbit</name>
      <uri>https://old.reddit.com/user/GGwithRabbit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptz6xy/audioghost_ai_run_metas_samaudio_on_4gb6gb_vram/"&gt; &lt;img alt="AudioGhost AI: Run Meta's SAM-Audio on 4GB-6GB VRAM with a Windows One-Click Installer üëªüéµ" src="https://external-preview.redd.it/NzF3YWpkZmxqejhnMSn9Bvgd5F2HIaI4NgTX7xfRCm50JCfHFGJKJxKbbOUZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7f0ad905dfb18baa2431a03e1610a6c84dcefa2" title="AudioGhost AI: Run Meta's SAM-Audio on 4GB-6GB VRAM with a Windows One-Click Installer üëªüéµ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Meta's &lt;strong&gt;SAM-Audio&lt;/strong&gt; is a breakthrough for object-oriented audio separation (e.g., &amp;quot;extract the violin from this busy track&amp;quot; using natural language), but the original repo has a massive VRAM footprint. Many users (including myself) experienced OOM errors even on high-end cards because it loads vision encoders and rankers by default.&lt;/p&gt; &lt;p&gt;I built &lt;strong&gt;AudioGhost AI&lt;/strong&gt; ‚Äî an open-source, full-stack GUI designed to bring this power to laptop and consumer GPUs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üöÄ &lt;strong&gt;Lite Mode (Low VRAM):&lt;/strong&gt; By stripping unused encoders and rankers, I got the VRAM usage down to &lt;strong&gt;4GB-6GB&lt;/strong&gt; for the Small model and &lt;strong&gt;~10GB&lt;/strong&gt; for Large.&lt;/li&gt; &lt;li&gt;üõ†Ô∏è &lt;strong&gt;Windows 1-Click Installer:&lt;/strong&gt; No more wrestling with FFmpeg versions or TorchCodec DLL errors. The &lt;code&gt;install.bat&lt;/code&gt; handles everything.&lt;/li&gt; &lt;li&gt;üé® &lt;strong&gt;Modern Interface:&lt;/strong&gt; Next.js + Tailwind glassmorphism UI with real-time waveform and stem mixing.&lt;/li&gt; &lt;li&gt;‚ö° &lt;strong&gt;Local-First:&lt;/strong&gt; Privacy is paramount‚Äîeverything runs 100% on your own hardware.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Performance (4090 Tested, 4:26 audio (11 chunks @ 25s each)):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Small Model: ~6GB VRAM | 25s |&lt;/li&gt; &lt;li&gt;Large Model: ~10GB VRAM | 41s |&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I truly believe &lt;strong&gt;SAM-Audio&lt;/strong&gt; is the future of audio editing, and I hope this tool makes it accessible to more creators who don't have access to lab-grade GPU clusters.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub (Open Source):&lt;/strong&gt; &lt;a href="https://github.com/0x0funky/audioghost-ai"&gt;https://github.com/0x0funky/audioghost-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts, feedback, or any issues you find while running it on your rig! üëª&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GGwithRabbit"&gt; /u/GGwithRabbit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ovsyaleljz8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptz6xy/audioghost_ai_run_metas_samaudio_on_4gb6gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptz6xy/audioghost_ai_run_metas_samaudio_on_4gb6gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T17:06:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptw5ol</id>
    <title>Could it be GLM 4.7 Air?</title>
    <updated>2025-12-23T15:05:26+00:00</updated>
    <author>
      <name>/u/noiserr</name>
      <uri>https://old.reddit.com/user/noiserr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Head of Global Brand &amp;amp; Partnerships @Zai_org&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;says:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We have a new model coming soon. Stay tuned! üòù&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://x.com/louszbd/status/2003153617013137677"&gt;https://x.com/louszbd/status/2003153617013137677&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Maybe the Air version is next?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noiserr"&gt; /u/noiserr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptw5ol/could_it_be_glm_47_air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptw5ol/could_it_be_glm_47_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptw5ol/could_it_be_glm_47_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T15:05:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptttcm</id>
    <title>How to run the GLM-4.7 model locally on your own device (guide)</title>
    <updated>2025-12-23T13:23:03+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptttcm/how_to_run_the_glm47_model_locally_on_your_own/"&gt; &lt;img alt="How to run the GLM-4.7 model locally on your own device (guide)" src="https://preview.redd.it/b995ei5mfy8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4519336dd309d77b0ea2caf5ea5cb6af6df8bf4" title="How to run the GLM-4.7 model locally on your own device (guide)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;GLM-4.7 is Z.ai‚Äôs latest thinking model, delivering stronger coding, agent, and chat performance than GLM-4.6 &lt;/li&gt; &lt;li&gt;It achieves SOTA performance on on SWE-bench (73.8%, +5.8), SWE-bench Multilingual (66.7%, +12.9), and Terminal Bench 2.0 (41.0%, +16.5).&lt;/li&gt; &lt;li&gt;The full 355B parameter model requires &lt;strong&gt;400GB&lt;/strong&gt; of disk space, while the Unsloth Dynamic 2-bit GGUF reduces the size to &lt;strong&gt;134GB&lt;/strong&gt; (-&lt;strong&gt;75%)&lt;/strong&gt;. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Official blog post - &lt;a href="https://docs.unsloth.ai/models/glm-4.7"&gt;https://docs.unsloth.ai/models/glm-4.7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b995ei5mfy8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptttcm/how_to_run_the_glm47_model_locally_on_your_own/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptttcm/how_to_run_the_glm47_model_locally_on_your_own/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T13:23:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pu1uq6</id>
    <title>Saw this on local marketplace, must be from a fellow r/LocalLLaMA here</title>
    <updated>2025-12-23T18:51:52+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu1uq6/saw_this_on_local_marketplace_must_be_from_a/"&gt; &lt;img alt="Saw this on local marketplace, must be from a fellow r/LocalLLaMA here" src="https://preview.redd.it/rd8mxp4l209g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3bc74b68c9ffc26cb488794a081d8077fa8ae663" title="Saw this on local marketplace, must be from a fellow r/LocalLLaMA here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rd8mxp4l209g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pu1uq6/saw_this_on_local_marketplace_must_be_from_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pu1uq6/saw_this_on_local_marketplace_must_be_from_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T18:51:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pty4l1</id>
    <title>Qwen released Qwen-Image-Edit-2511 ‚Äî a major upgrade over 2509</title>
    <updated>2025-12-23T16:24:27+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pty4l1/qwen_released_qwenimageedit2511_a_major_upgrade/"&gt; &lt;img alt="Qwen released Qwen-Image-Edit-2511 ‚Äî a major upgrade over 2509" src="https://b.thumbs.redditmedia.com/h4Zf9373IbplPJ8uOKIcc2EETzqgKctn8fs9-JElwKQ.jpg" title="Qwen released Qwen-Image-Edit-2511 ‚Äî a major upgrade over 2509" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2511"&gt;https://huggingface.co/Qwen/Qwen-Image-Edit-2511&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What‚Äôs new in 2511: üë• Stronger multi-person consistency for group photos and complex scenes üß© Built-in popular community LoRAs ‚Äî no extra tuning required üí° Enhanced industrial &amp;amp; product design generation üîí Reduced image drift with dramatically improved character &amp;amp; identity consistency üìê Improved geometric reasoning, including construction lines and structural edits From identity-preserving portrait edits to high-fidelity multi-person fusion and practical engineering &amp;amp; design workflows, 2511 pushes image editing to the next level. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pty4l1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pty4l1/qwen_released_qwenimageedit2511_a_major_upgrade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pty4l1/qwen_released_qwenimageedit2511_a_major_upgrade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:24:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt50mt</id>
    <title>AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)</title>
    <updated>2025-12-22T17:12:21+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt; &lt;img alt="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" src="https://preview.redd.it/r06ch4zyfs8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0a10789f393f350618520fcb81174f3a3dae1c7" title="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r06ch4zyfs8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T17:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
</feed>
