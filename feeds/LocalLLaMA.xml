<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-26T09:14:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qn0dtg</id>
    <title>REAP experiences</title>
    <updated>2026-01-26T00:17:08+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The title means Router-weighted Expert Activation Pruning by Cerebras&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/cerebras/cerebras-reap"&gt;https://huggingface.co/collections/cerebras/cerebras-reap&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It has been out for a bit now.&lt;/p&gt; &lt;p&gt;What is your assessment of the quality of REAP models? How have they performed in practice? Are they over-hyped or is it a useful method for production?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn0dtg/reap_experiences/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn0dtg/reap_experiences/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn0dtg/reap_experiences/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T00:17:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn02w8</id>
    <title>I put an RTX PRO 4000 Blackwell SFF in my MS-S1 Max (Strix Halo), some benchmarks</title>
    <updated>2026-01-26T00:04:13+00:00</updated>
    <author>
      <name>/u/Grouchy-Bed-7942</name>
      <uri>https://old.reddit.com/user/Grouchy-Bed-7942</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn02w8/i_put_an_rtx_pro_4000_blackwell_sff_in_my_mss1/"&gt; &lt;img alt="I put an RTX PRO 4000 Blackwell SFF in my MS-S1 Max (Strix Halo), some benchmarks" src="https://b.thumbs.redditmedia.com/yc5Vei7cC84KoR0--JRWZS5c4cSpiTakofT-9ZG1GIw.jpg" title="I put an RTX PRO 4000 Blackwell SFF in my MS-S1 Max (Strix Halo), some benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Translated/formatted with gpt-oss-120b. After all, we’re on &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;.)&lt;/p&gt; &lt;p&gt;I received an RTX PRO 4000 Blackwell SFF, which I installed in an MS-S1 Max (AMD Strix Halo – Minisforum) via the PCIe 4.0 x4 slot, mechanically extended to x16 inside the case. The card draws 70 W.&lt;/p&gt; &lt;p&gt;The chassis is still open for now: I’m waiting for a 1-slot cooler like n3rdware to appear so I can close it neatly.&lt;/p&gt; &lt;p&gt;With the extra VRAM I was able to push the tests a bit further, notably running CUDA + Vulkan in the same container, and loading heavier quantizations.&lt;/p&gt; &lt;p&gt;On MiniMax M2.1 Q4_K_XL, I get roughly 170–200 tokens/s in prompt processing without context, and 25–30 tokens/s in generation, also without context. llama-bench crashes as soon as it tries to allocate the full context for this model, but the server stays stable with the following configuration:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash llama-server \ -m ~/.cache/llama.cpp/unsloth_MiniMax-M2.1-GGUF_UD-Q4_K_XL_MiniMax-M2.1-UD-Q4_K_XL-00001-of-00003.gguf \ --fit 1 \ --jinja \ -c 40000 \ -fa 1 \ --no-mmap \ --cache-type-k q8_0 \ --cache-type-v q8_0 \ -dev Cuda0,Vulkan1 \ -sm layer \ -ts 2/10 \ -ngl 999 \ --host 0.0.0.0 &lt;/code&gt;&lt;/p&gt; &lt;h1&gt;Benchmarks (llama.cpp)&lt;/h1&gt; &lt;h2&gt;Environment&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;GPU CUDA: NVIDIA RTX PRO 4000 Blackwell SFF (compute capability 12.0, VMM enabled)&lt;/li&gt; &lt;li&gt;GPU ROCm / Vulkan: Radeon 8060S (gfx1151)&lt;/li&gt; &lt;li&gt;Flash Attention enabled&lt;/li&gt; &lt;li&gt;ngl=999, mmp=0&lt;/li&gt; &lt;li&gt;ROCm containers: I use the containers from kyuz0/amd-strix-halo-toolboxes for ROCm workloads.&lt;/li&gt; &lt;li&gt;Vulkan + CUDA containers: custom-built containers I created myself.&lt;/li&gt; &lt;li&gt;Host OS: Fedora 43, kernel 6.17.1-300.fc43.x86_64&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Tests&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;pp512 : short-prompt processing&lt;/li&gt; &lt;li&gt;pp32768: long-context prompt processing&lt;/li&gt; &lt;li&gt;tg128 : generation&lt;/li&gt; &lt;li&gt;3 runs per test&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;GPT-OSS-20B – MXFP4 MoE&lt;/h1&gt; &lt;h2&gt;CUDA&lt;/h2&gt; &lt;p&gt;llama.cpp build: 0bf5636&lt;/p&gt; &lt;p&gt;&lt;code&gt; | model | size | params | backend | ngl | fa | test | t/s | |-----------------------|-----------|---------|---------|-----|----|-----------|-------------------| | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | CUDA | 999 | 1 | pp512 | 4826.07 ± 45.77 | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | CUDA | 999 | 1 | pp32768 | 3355.12 ± 34.28 | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | CUDA | 999 | 1 | tg128 | 117.47 ± 0.63 | &lt;/code&gt;&lt;/p&gt; &lt;h2&gt;ROCm 7.1.1&lt;/h2&gt; &lt;p&gt;(ROCm 6.4.4 no longer works with recent llama.cpp updates) llama.cpp build: 8f91ca54e (7822)&lt;/p&gt; &lt;p&gt;&lt;code&gt; | model | size | params | backend | ngl | fa | test | t/s | |-----------------------|-----------|---------|---------|-----|----|-----------|-------------------| | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | ROCm | 999 | 1 | pp512 | 1669.38 ± 5.53 | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | ROCm | 999 | 1 | pp32768 | 822.84 ± 3.97 | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | ROCm | 999 | 1 | tg128 | 71.47 ± 0.03 | &lt;/code&gt;&lt;/p&gt; &lt;h1&gt;GPT-OSS-120B – MXFP4 MoE&lt;/h1&gt; &lt;h2&gt;CUDA + Vulkan (split per layer, ts 5 / 10)&lt;/h2&gt; &lt;p&gt;llama.cpp build: 0bf5636&lt;/p&gt; &lt;p&gt;&lt;code&gt; | model | size | params | backend | ngl | fa | dev | ts | test | t/s | |------------------------|-----------|----------|-------------|-----|----|---------------|-------------|---------|-----------------| | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | CUDA,Vulkan | 999 | 1 | CUDA0/Vulkan1 | 5.00/10.00 | pp512 | 808.29 ± 2.68 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | CUDA,Vulkan | 999 | 1 | CUDA0/Vulkan1 | 5.00/10.00 | pp32768 | 407.10 ± 1.61 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | CUDA,Vulkan | 999 | 1 | CUDA0/Vulkan1 | 5.00/10.00 | tg128 | 58.84 ± 0.02 | &lt;/code&gt;&lt;/p&gt; &lt;h2&gt;ROCm 7.1.1&lt;/h2&gt; &lt;p&gt;llama.cpp build: 8f91ca54e (7822)&lt;/p&gt; &lt;p&gt;&lt;code&gt; | model | size | params | backend | ngl | fa | test | t/s | |------------------------|-----------|----------|---------|-----|----|---------|-----------------| | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm | 999 | 1 | pp512 | 643.95 ± 2.49 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm | 999 | 1 | pp32768 | 396.67 ± 1.21 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm | 999 | 1 | tg128 | 49.84 ± 0.01 | &lt;/code&gt;&lt;/p&gt; &lt;h1&gt;Qwen3-VL-30B-A3B – Q8_K_XL&lt;/h1&gt; &lt;h2&gt;CUDA + Vulkan (ts 10 / 6.5)&lt;/h2&gt; &lt;p&gt;llama.cpp build: 0bf5636&lt;/p&gt; &lt;p&gt;&lt;code&gt; | model | size | params | backend | ngl | fa | dev | ts | test | t/s | |-----------------------|-----------|---------|-------------|-----|----|---------------|------------|---------|-----------------| | qwen3vlmoe 30B.A3B Q8 | 33.51 GiB | 30.53 B | CUDA,Vulkan | 999 | 1 | CUDA0/Vulkan1 | 10.00/6.50 | pp512 | 1515.69 ± 12.07 | | qwen3vlmoe 30B.A3B Q8 | 33.51 GiB | 30.53 B | CUDA,Vulkan | 999 | 1 | CUDA0/Vulkan1 | 10.00/6.50 | pp32768 | 390.71 ± 2.89 | | qwen3vlmoe 30B.A3B Q8 | 33.51 GiB | 30.53 B | CUDA,Vulkan | 999 | 1 | CUDA0/Vulkan1 | 10.00/6.50 | tg128 | 49.94 ± 0.02 | &lt;/code&gt;&lt;/p&gt; &lt;h2&gt;ROCm 7.1.1&lt;/h2&gt; &lt;p&gt;llama.cpp build: 8f91ca54e (7822)&lt;/p&gt; &lt;p&gt;&lt;code&gt; | model | size | params | backend | ngl | fa | test | t/s | |-----------------------|-----------|---------|---------|-----|----|---------|-----------------| | qwen3vlmoe 30B.A3B Q8 | 33.51 GiB | 30.53 B | ROCm | 999 | 1 | pp512 | 1078.12 ± 8.81 | | qwen3vlmoe 30B.A3B Q8 | 33.51 GiB | 30.53 B | ROCm | 999 | 1 | pp32768 | 377.29 ± 0.15 | | qwen3vlmoe 30B.A3B Q8 | 33.51 GiB | 30.53 B | ROCm | 999 | 1 | tg128 | 53.66 ± 0.01 | &lt;/code&gt;&lt;/p&gt; &lt;h1&gt;Qwen3-Next-80B-A3B – Q8_K_XL&lt;/h1&gt; &lt;h2&gt;CUDA + Vulkan (ts 3.5 / 10)&lt;/h2&gt; &lt;p&gt;llama.cpp build: 0bf5636&lt;/p&gt; &lt;p&gt;&lt;code&gt; | model | size | params | backend | ngl | fa | dev | ts | test | t/s | |------------------------|-----------|---------|-------------|-----|----|---------------|------------|---------|-----------------| | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA,Vulkan | 999 | 1 | CUDA0/Vulkan1 | 3.50/10.00 | pp512 | 590.23 ± 3.38 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA,Vulkan | 999 | 1 | CUDA0/Vulkan1 | 3.50/10.00 | pp32768 | 324.88 ± 0.74 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | CUDA,Vulkan | 999 | 1 | CUDA0/Vulkan1 | 3.50/10.00 | tg128 | 34.83 ± 0.04 | &lt;/code&gt;&lt;/p&gt; &lt;h2&gt;ROCm 7.1.1&lt;/h2&gt; &lt;p&gt;llama.cpp build: 8f91ca54e (7822)&lt;/p&gt; &lt;p&gt;&lt;code&gt; | model | size | params | backend | ngl | fa | test | t/s | |------------------------|-----------|---------|---------|-----|----|---------|------------------| | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | ROCm | 999 | 1 | pp512 | 587.93 ± 19.98 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | ROCm | 999 | 1 | pp32768 | 473.05 ± 0.33 | | qwen3next 80B.A3B Q8_0 | 78.98 GiB | 79.67 B | ROCm | 999 | 1 | tg128 | 29.47 ± 0.08 | &lt;/code&gt;&lt;/p&gt; &lt;p&gt;If you have any relevant tests to run with this hybrid (CUDA + Vulkan, CUDA-only, large models) setup, or even just optimisation suggestions, I’m all ears.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Grouchy-Bed-7942"&gt; /u/Grouchy-Bed-7942 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn02w8/i_put_an_rtx_pro_4000_blackwell_sff_in_my_mss1/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn02w8/i_put_an_rtx_pro_4000_blackwell_sff_in_my_mss1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn02w8/i_put_an_rtx_pro_4000_blackwell_sff_in_my_mss1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T00:04:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnauyj</id>
    <title>Cost Analysis: Subscription vs Token-Based Pricing for Long-Form RP &amp; Character Workloads</title>
    <updated>2026-01-26T08:50:29+00:00</updated>
    <author>
      <name>/u/Forward_Reaction6744</name>
      <uri>https://old.reddit.com/user/Forward_Reaction6744</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A lot of recent threads here discuss inference cost, token pricing, and whether subscriptions actually make sense for long-form usage. I wanted to share a cost-oriented perspective specifically for &lt;strong&gt;roleplay and character-driven workloads&lt;/strong&gt;, which behave very differently from short Q&amp;amp;A.&lt;/p&gt; &lt;p&gt;This isn’t a recommendation post. Just observations from testing different setups.&lt;/p&gt; &lt;h1&gt;Why roleplay workloads are expensive&lt;/h1&gt; &lt;p&gt;Roleplay and persistent character chats tend to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;run for hundreds or thousands of turns&lt;/li&gt; &lt;li&gt;resend large context windows&lt;/li&gt; &lt;li&gt;generate high output-to-input ratios&lt;/li&gt; &lt;li&gt;favor consistency over raw reasoning&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This means cost doesn’t scale linearly. Small inefficiencies multiply quickly.&lt;/p&gt; &lt;p&gt;Most “chat-first” platforms aren’t optimized for this pattern.&lt;/p&gt; &lt;h1&gt;Subscription pricing hides real cost&lt;/h1&gt; &lt;p&gt;Flat subscriptions look attractive at first, but they blur important details:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;how much compute you’re actually using&lt;/li&gt; &lt;li&gt;how load affects latency&lt;/li&gt; &lt;li&gt;whether you’re paying for priority or for tokens&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In practice, many heavy users are paying to avoid queues rather than paying for actual model usage. Performance degradation during peak hours is effectively a hidden cost.&lt;/p&gt; &lt;p&gt;This matters more the longer your conversations run.&lt;/p&gt; &lt;h1&gt;Token-based pricing behaves differently&lt;/h1&gt; &lt;p&gt;With token-based setups:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;cost is proportional to usage&lt;/li&gt; &lt;li&gt;long sessions are predictable&lt;/li&gt; &lt;li&gt;you can downgrade models for routine dialogue&lt;/li&gt; &lt;li&gt;expensive models are used selectively&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This makes it easier to control burn rate in character-heavy scenarios, especially when context length dominates cost.&lt;/p&gt; &lt;h1&gt;Model choice matters more than people think&lt;/h1&gt; &lt;p&gt;For RP workloads, raw benchmark scores matter less than:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;coherence under repetition&lt;/li&gt; &lt;li&gt;tone stability&lt;/li&gt; &lt;li&gt;output efficiency&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In testing, mid-sized models with good dialogue tuning often outperform larger reasoning models on cost-per-quality for RP. Using a single “best” model everywhere is usually inefficient.&lt;/p&gt; &lt;h1&gt;Character iteration has hidden costs&lt;/h1&gt; &lt;p&gt;Another overlooked factor is &lt;strong&gt;creator iteration time&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Platforms that don’t allow export or reuse force repeated setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;rewriting personalities&lt;/li&gt; &lt;li&gt;rebuilding prompts&lt;/li&gt; &lt;li&gt;re-testing behavior after changes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;From a cost perspective, lost iteration time matters just as much as token spend, especially for creators or testers.&lt;/p&gt; &lt;h1&gt;When subscriptions still make sense&lt;/h1&gt; &lt;p&gt;Subscriptions are still reasonable if you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;chat casually&lt;/li&gt; &lt;li&gt;don’t run long sessions&lt;/li&gt; &lt;li&gt;don’t care about model choice&lt;/li&gt; &lt;li&gt;value simplicity over control&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;They just scale poorly for sustained RP or multi-character workflows.&lt;/p&gt; &lt;h1&gt;Takeaway&lt;/h1&gt; &lt;p&gt;For long-form character and RP use:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;pricing transparency matters&lt;/li&gt; &lt;li&gt;model flexibility reduces cost&lt;/li&gt; &lt;li&gt;subscriptions hide performance tax&lt;/li&gt; &lt;li&gt;creator workflows affect total cost more than advertised&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious how others here approach cost control for RP or long-context usage. Happy to compare notes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Forward_Reaction6744"&gt; /u/Forward_Reaction6744 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnauyj/cost_analysis_subscription_vs_tokenbased_pricing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnauyj/cost_analysis_subscription_vs_tokenbased_pricing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qnauyj/cost_analysis_subscription_vs_tokenbased_pricing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T08:50:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn68ih</id>
    <title>Suggestion Needed: Large Context Model For Summarizing Text</title>
    <updated>2026-01-26T04:37:02+00:00</updated>
    <author>
      <name>/u/Professional-Yak4359</name>
      <uri>https://old.reddit.com/user/Professional-Yak4359</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to summarize very long, somewhat technical papers, and I am wondering if anyone has any good suggestions? I do not need the model to be super smart; I just want it to be able to chew through 200 pages or so at a time, in context, so I can ask questions. &lt;/p&gt; &lt;p&gt;In terms of hardware, I am rocking 8 x 5070 Ti under Ubuntu in a headless box where I serve VLLM to myself on another desktop. Ideally, I would love to have something 256k or even 512k context that fits fully in VRAM. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Yak4359"&gt; /u/Professional-Yak4359 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn68ih/suggestion_needed_large_context_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn68ih/suggestion_needed_large_context_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn68ih/suggestion_needed_large_context_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T04:37:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmxexe</id>
    <title>How are people actually learning/building real-world AI agents (money, legal, business), not demos?</title>
    <updated>2026-01-25T22:19:32+00:00</updated>
    <author>
      <name>/u/Altruistic-Law-4750</name>
      <uri>https://old.reddit.com/user/Altruistic-Law-4750</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;#x200b;&lt;/p&gt; &lt;p&gt;I’m trying to understand how people are actually learning and building *real-world* AI agents — the kind that integrate into businesses, touch money, workflows, contracts, and carry real responsibility.&lt;/p&gt; &lt;p&gt;Not chat demos, not toy copilots, not “LLM + tools” weekend projects.&lt;/p&gt; &lt;p&gt;What I’m struggling with:&lt;/p&gt; &lt;p&gt;- There are almost no reference repos for serious agents&lt;/p&gt; &lt;p&gt;- Most content is either shallow, fragmented, or stops at orchestration&lt;/p&gt; &lt;p&gt;- Blogs talk about “agents” but avoid accountability, rollback, audit, or failure&lt;/p&gt; &lt;p&gt;- Anything real seems locked behind IP, internal systems, or closed companies&lt;/p&gt; &lt;p&gt;I get *why* — this stuff is risky and not something people open-source casually.&lt;/p&gt; &lt;p&gt;But clearly people are building these systems.&lt;/p&gt; &lt;p&gt;So I’m trying to understand from those closer to the work:&lt;/p&gt; &lt;p&gt;- How did you personally learn this layer?&lt;/p&gt; &lt;p&gt;- What should someone study first: infra, systems design, distributed systems, product, legal constraints?&lt;/p&gt; &lt;p&gt;- Are most teams just building traditional software systems with LLMs embedded (and “agent” is mostly a label)?&lt;/p&gt; &lt;p&gt;- How are responsibility, human-in-the-loop, and failure handled in production?&lt;/p&gt; &lt;p&gt;- Where do serious discussions about this actually happen?&lt;/p&gt; &lt;p&gt;I’m not looking for shortcuts or magic repos.&lt;/p&gt; &lt;p&gt;I’m trying to build the correct **mental model and learning path** for production-grade systems, not demos.&lt;/p&gt; &lt;p&gt;If you’ve worked on this, studied it deeply, or know where real practitioners share knowledge — I’d really appreciate guidance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altruistic-Law-4750"&gt; /u/Altruistic-Law-4750 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmxexe/how_are_people_actually_learningbuilding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmxexe/how_are_people_actually_learningbuilding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmxexe/how_are_people_actually_learningbuilding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T22:19:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn9jbc</id>
    <title>GLM flash and MLA</title>
    <updated>2026-01-26T07:31:50+00:00</updated>
    <author>
      <name>/u/blahbhrowawayblahaha</name>
      <uri>https://old.reddit.com/user/blahbhrowawayblahaha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;does the new glm 4.5 flash use MLA à la Deepseek?&lt;/p&gt; &lt;p&gt;if so, is it the only small (&amp;lt;70B) model we have available that uses MLA? When DS described MLA I assumed everyone would start using it bc it seemed like a free lunch. so I’m curious why it’s taken so long for it to appear in other models (especially smaller ones)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blahbhrowawayblahaha"&gt; /u/blahbhrowawayblahaha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn9jbc/glm_flash_and_mla/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn9jbc/glm_flash_and_mla/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn9jbc/glm_flash_and_mla/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T07:31:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmsk9w</id>
    <title>LLM Reasoning Efficiency - lineage-bench accuracy vs generated tokens</title>
    <updated>2026-01-25T19:22:14+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmsk9w/llm_reasoning_efficiency_lineagebench_accuracy_vs/"&gt; &lt;img alt="LLM Reasoning Efficiency - lineage-bench accuracy vs generated tokens" src="https://preview.redd.it/gai51kz2pjfg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f44d218e6b2a5dc8982ffb434c4c01e0cf195277" title="LLM Reasoning Efficiency - lineage-bench accuracy vs generated tokens" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Generated from lineage-128 and lineage-192 &lt;a href="http://github.com/fairydreaming/lineage-bench"&gt;lineage-bench&lt;/a&gt; &lt;a href="https://github.com/fairydreaming/lineage-bench-results/tree/main/lineage-8_64_128_192"&gt;benchmark results&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Sorry for overlapping labels.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gai51kz2pjfg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmsk9w/llm_reasoning_efficiency_lineagebench_accuracy_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmsk9w/llm_reasoning_efficiency_lineagebench_accuracy_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T19:22:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn7mmh</id>
    <title>I built a local "Cognitive IDE" to manage multi-agent workflows</title>
    <updated>2026-01-26T05:46:55+00:00</updated>
    <author>
      <name>/u/Healthy-Basil-7521</name>
      <uri>https://old.reddit.com/user/Healthy-Basil-7521</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7mmh/i_built_a_local_cognitive_ide_to_manage/"&gt; &lt;img alt="I built a local &amp;quot;Cognitive IDE&amp;quot; to manage multi-agent workflows" src="https://b.thumbs.redditmedia.com/kN_DAjYgrrfSfACl7J_j_VELjorkGTe4hpFMMfGDYzc.jpg" title="I built a local &amp;quot;Cognitive IDE&amp;quot; to manage multi-agent workflows" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After months of using llms for a research project and personal use , I hit a wall. I needed to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Maintain separate &amp;quot;expert&amp;quot; agents that remember their domain&lt;/li&gt; &lt;li&gt;See how ideas flowed between conversations&lt;/li&gt; &lt;li&gt;Pull context from multiple chats into a single synthesis&lt;/li&gt; &lt;li&gt;A quick way to build detailed system personas&lt;/li&gt; &lt;li&gt;Search by concept not by chat name&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I built &lt;strong&gt;Cognitive OS&lt;/strong&gt; - a local-first desktop environment for managing AI workflows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Core Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Persistent State:&lt;/strong&gt; Agents are treated as files, not temporary sessions. They remember everything across reloads. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Knowledge Graph:&lt;/strong&gt; Visualizes the &amp;quot;lineage of thought.&amp;quot; You can see exactly how an insight flowed from Agent A to Agent B.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Context Forwarding (MCF):&lt;/strong&gt; Select specific messages from multiple different agents and bundle them into a payload to pipe into a &amp;quot;Synthesis Bot.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;JIT (Just-In-Time) Injection:&lt;/strong&gt; Instead of dumping a whole chat history, you can query an agent to generate a specific summary of its knowledge on the fly, and inject that summary into another agent's context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Integrated Prompter Bot:&lt;/strong&gt; A built-in meta-agent dedicated to interviewing you and crafting high-fidelity system prompts to spin up new experts quickly.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic Search:&lt;/strong&gt; A global memory search that finds insights by concept, not just keyword.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Librarian Bot:&lt;/strong&gt; I have initial deterministic labels based on how the chat was created, and also overtime a dynamic labeling that uses the JIT to give more nuanced labels for chats.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Python Backend (Logic &amp;amp; State Management)&lt;/li&gt; &lt;li&gt;Frontend (The UI in the screenshot is hosted on ViteJs, but I will add it to the source code)&lt;/li&gt; &lt;li&gt;Model Agnostic (Currently running on Gemini Flash, but architected to swap easily)&lt;/li&gt; &lt;li&gt;100% Local Storage (JSON filesystem + Vector DB)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking for feedback from other users hitting the same walls. What workflows would you want supported?&lt;/p&gt; &lt;p&gt;&lt;a href="https://stackblitz.com/edit/vitejs-vite-ske72rwt?file=src%2FApp.tsx"&gt;Link for demo seen in image&lt;/a&gt; (Not every tab mentioned is in the demo, I just wanted to see if a larger audience than me is interested in the idea)&lt;br /&gt; &lt;a href="https://github.com/8lak/Cognitive_OS"&gt;Repo &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nx0ko55jtmfg1.png?width=1917&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfbce46e34e8bef9d49b34c3be126f41816b35f9"&gt;https://preview.redd.it/nx0ko55jtmfg1.png?width=1917&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfbce46e34e8bef9d49b34c3be126f41816b35f9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Basil-7521"&gt; /u/Healthy-Basil-7521 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7mmh/i_built_a_local_cognitive_ide_to_manage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7mmh/i_built_a_local_cognitive_ide_to_manage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7mmh/i_built_a_local_cognitive_ide_to_manage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T05:46:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn132c</id>
    <title>How to use plugins in LM Studio?</title>
    <updated>2026-01-26T00:47:14+00:00</updated>
    <author>
      <name>/u/tri_idias</name>
      <uri>https://old.reddit.com/user/tri_idias</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was going through this forum and I just discovered the various plugins for LM Studio. DuckDuckGo, Visit websites, Dice, and Wikipedia.&lt;/p&gt; &lt;p&gt;According to LM studio, the model that I'm using should be capable for tool use as well (There's the hammer icon). However, I'm not able to trigger any of those plugins through the chat screen.&lt;/p&gt; &lt;p&gt;Do I need something else?&lt;/p&gt; &lt;p&gt;To be exact, I'm using Drummer's Cydonia 24B 4.3 model.&lt;br /&gt; I've all those plugins installed and enabled as well. But I just can't seems to get it to work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tri_idias"&gt; /u/tri_idias &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn132c/how_to_use_plugins_in_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn132c/how_to_use_plugins_in_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn132c/how_to_use_plugins_in_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T00:47:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn7ew9</id>
    <title>RAG Paper 26.1.22</title>
    <updated>2026-01-26T05:35:56+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.16027v1"&gt;Deja Vu in Plots: Leveraging Cross-Session Evidence with Retrieval-Augmented LLMs for Live Streaming Risk Assessment&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.15849v1"&gt;CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.15820v1"&gt;ExDR: Explanation-driven Dynamic Retrieval Enhancement for Multimodal Fake News Detection&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.15816v1"&gt;Virtual Traffic Police: Large Language Model-Augmented Traffic Signal Control for Unforeseen Incidents&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.15678v1"&gt;Connect the Dots: Knowledge Graph-Guided Crawler Attack on Retrieval-Augmented Generation Systems&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2601.15645v1"&gt;Towards Reliable Medical LLMs: Benchmarking and Enhancing Confidence Estimation of Large Language Models in Medical Consultation&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/components/arena"&gt;&lt;strong&gt;RagView.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/&lt;/strong&gt; &lt;a href="https://github.com/RagView/RagView"&gt;&lt;strong&gt;github/RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7ew9/rag_paper_26122/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7ew9/rag_paper_26122/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7ew9/rag_paper_26122/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T05:35:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn1uux</id>
    <title>On-device tool calling with Llama 3.2 3B on iPhone - made it suggest sushi restaurants [Open Source, React Native]</title>
    <updated>2026-01-26T01:20:05+00:00</updated>
    <author>
      <name>/u/New_Inflation_6927</name>
      <uri>https://old.reddit.com/user/New_Inflation_6927</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just built a tool calling POC - Llama 3.2 3B doing tool calls entirely on-device (iPhone 16 Pro Max).&lt;/p&gt; &lt;p&gt;Demo: DoorDash-style food ordering app where you chat with a local LLM that searches restaurants and helps you order.&lt;/p&gt; &lt;p&gt;On-device: LLM inference + Tool call decisions + Response parsing&lt;br /&gt; API: Foursquare for restaurant places info&lt;/p&gt; &lt;p&gt;No cloud AI. The brain is local, it just reaches out for data when needed.&lt;/p&gt; &lt;p&gt;Stack: React Native, RunAnywhere SDK (open source), Llama 3.2 3B&lt;/p&gt; &lt;p&gt;Source code in comments.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qn1uux/video/sugg6e6ehlfg1/player"&gt;https://reddit.com/link/1qn1uux/video/sugg6e6ehlfg1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Inflation_6927"&gt; /u/New_Inflation_6927 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn1uux/ondevice_tool_calling_with_llama_32_3b_on_iphone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn1uux/ondevice_tool_calling_with_llama_32_3b_on_iphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn1uux/ondevice_tool_calling_with_llama_32_3b_on_iphone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T01:20:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmu1a1</id>
    <title>GLM-4.7-Flash context slowdown</title>
    <updated>2026-01-25T20:15:06+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmu1a1/glm47flash_context_slowdown/"&gt; &lt;img alt="GLM-4.7-Flash context slowdown" src="https://b.thumbs.redditmedia.com/1XaF-XYj4di3wcMyTbjLQ43nM77xN3jdiZdZndwBcDM.jpg" title="GLM-4.7-Flash context slowdown" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;UPDATE &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;to check on your setup, run:&lt;br /&gt; (you can use higher -p and -n and modify -d to your needs)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;jacek@AI-SuperComputer:~$ CUDA_VISIBLE_DEVICES=0,1,2 llama-bench -m /mnt/models1/GLM/GLM-4.7-Flash-Q8_0.gguf -d 0,5000,10000,15000,20000,25000,30000,35000,40000,45000,50000 -p 200 -n 200 -fa 1 ggml_cuda_init: found 3 CUDA devices: Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes Device 2: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes | model | size | params | backend | ngl | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 | 1985.41 ± 11.02 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 | 95.65 ± 0.44 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d5000 | 1392.15 ± 12.63 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d5000 | 81.83 ± 0.67 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d10000 | 1027.56 ± 13.50 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d10000 | 72.60 ± 0.07 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d15000 | 824.05 ± 8.08 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d15000 | 64.24 ± 0.46 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d20000 | 637.06 ± 79.79 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d20000 | 58.46 ± 0.14 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d25000 | 596.69 ± 11.13 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d25000 | 53.31 ± 0.18 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d30000 | 518.71 ± 5.25 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d30000 | 49.41 ± 0.02 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d35000 | 465.65 ± 2.69 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d35000 | 45.80 ± 0.04 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d40000 | 417.97 ± 1.67 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d40000 | 42.65 ± 0.05 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d45000 | 385.33 ± 1.80 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d45000 | 40.01 ± 0.03 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | pp200 @ d50000 | 350.91 ± 2.17 | | deepseek2 ?B Q8_0 | 29.65 GiB | 29.94 B | CUDA | 99 | 1 | tg200 @ d50000 | 37.63 ± 0.02 | build: 8f91ca54e (7822) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;real usage of opencode (with 200000 context):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;slot launch_slot_: id 0 | task 2495 | processing task, is_child = 0 slot update_slots: id 0 | task 2495 | new prompt, n_ctx_slot = 200192, n_keep = 0, task.n_tokens = 66276 slot update_slots: id 0 | task 2495 | n_tokens = 63140, memory_seq_rm [63140, end) slot update_slots: id 0 | task 2495 | prompt processing progress, n_tokens = 65188, batch.n_tokens = 2048, progress = 0.983584 slot update_slots: id 0 | task 2495 | n_tokens = 65188, memory_seq_rm [65188, end) slot update_slots: id 0 | task 2495 | prompt processing progress, n_tokens = 66276, batch.n_tokens = 1088, progress = 1.000000 slot update_slots: id 0 | task 2495 | prompt done, n_tokens = 66276, batch.n_tokens = 1088 slot init_sampler: id 0 | task 2495 | init sampler, took 8.09 ms, tokens: text = 66276, total = 66276 slot print_timing: id 0 | task 2495 | prompt eval time = 10238.44 ms / 3136 tokens ( 3.26 ms per token, 306.30 tokens per second) eval time = 11570.90 ms / 355 tokens ( 32.59 ms per token, 30.68 tokens per second) total time = 21809.34 ms / 3491 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;n_tokens = 66276, 306.30t/s, 30.68t/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qmu1a1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmu1a1/glm47flash_context_slowdown/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmu1a1/glm47flash_context_slowdown/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T20:15:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn7zhi</id>
    <title>GLM 4.7: Why does explicit "--threads -1" ruin my t/s in llama-server?</title>
    <updated>2026-01-26T06:05:39+00:00</updated>
    <author>
      <name>/u/phwlarxoc</name>
      <uri>https://old.reddit.com/user/phwlarxoc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using unsloth GLM-4.7 UD-Q8_K_XL quants on a dual RTX 5090 machine with 512 GB of system RAM and a 32C Zen5 Threadripper Pro. I run llama-server like so:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1 llama.cpp/build/bin/llama-server --model ./GLM-4.7-UD-Q8_K_XL-00001-of-00009.gguf \ --cache-type-k q8_0 \ --cache-type-v q8_0 \ --jinja \ --ctx-size 40000 \ --temp 1.0 \ --top-p 0.95 \ --top-k 40 \ --min-p 0.0 \ --fit on &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This yields about 9 t/s where CPU load is constantly 51% and GPU load varies between 6 and 20%. However, if I add &amp;quot;--threads -1&amp;quot; with the idea to increase idling CPU core usage, the CPU is indeed used at nearly 100%, but t/s plummets to about 2.5 t/s. Why is that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phwlarxoc"&gt; /u/phwlarxoc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7zhi/glm_47_why_does_explicit_threads_1_ruin_my_ts_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7zhi/glm_47_why_does_explicit_threads_1_ruin_my_ts_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn7zhi/glm_47_why_does_explicit_threads_1_ruin_my_ts_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T06:05:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmir5d</id>
    <title>What do you actually want from a private AI chat on your phone?</title>
    <updated>2026-01-25T13:12:00+00:00</updated>
    <author>
      <name>/u/AppDeveloperAsdf</name>
      <uri>https://old.reddit.com/user/AppDeveloperAsdf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"&gt; &lt;img alt="What do you actually want from a private AI chat on your phone?" src="https://external-preview.redd.it/b2k1d3JkaHV0aGZnMbzKSbNJeiRdJL3Vv6uz8BgUY-ES1g_l6yTqUuzYy_d7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f45f222f3ee31c2716f286b9cf0998d79f80e6f" title="What do you actually want from a private AI chat on your phone?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey friends. We are building zerotap - an Android app where AI can control your phone like a human (taps, scrolls, reads screen). It supports Ollama, proxies like OpenRouter and Straico and models directly such as OpenAI, Claude, Gemini and DeepSeek.&lt;/p&gt; &lt;p&gt;Recently we added a chat interface, so now it works like a regular AI chat that can take over your device when needed.&lt;/p&gt; &lt;p&gt;Now we are planning what to focus on next and we'd love your input. Some options we're considering:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MCP servers&lt;/strong&gt; - connect your chat to external tools and services&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deep research&lt;/strong&gt; - letting the AI browse and gather information for you&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-modality&lt;/strong&gt; — image read &amp;amp; write (generation)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;On-device models&lt;/strong&gt; — we are working on Gemma 3n and Qwen support, but small context windows are hurting performance so much&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Speaking of which - for those of you running Ollama: do you expose your instance to the internet or keep it local network only?&lt;/p&gt; &lt;p&gt;Honest question: what would make an AI chat on your phone actually useful for you on a daily basis? Not as a toy, but as something you would rely on - what's missing from current mobile AI apps (that supports ollama) that annoys you the most?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppDeveloperAsdf"&gt; /u/AppDeveloperAsdf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/em3174huthfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T13:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn217z</id>
    <title>Practical use of local AI: Get a daily postcard with an anime girl inviting you to a local event based on your interests</title>
    <updated>2026-01-26T01:27:42+00:00</updated>
    <author>
      <name>/u/catplusplusok</name>
      <uri>https://old.reddit.com/user/catplusplusok</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/catplusplus/vibecheck/"&gt;https://github.com/catplusplus/vibecheck/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unique use case should run well on a good desktop or Apple laptop, cloud APIs would have real costs or at least discourage me from burning tokens with abandon for cosmetic improvements. Feel free to laugh at the anime girls, I am sure nobody else on this forum has similar AI use cases! The bottom line is that the app is for self improvement, encouraging me to get out of the house, go to events, learn new things and meet new people. &lt;/p&gt; &lt;p&gt;I have another even more compute intensive projects that involves mass describing my entire photo library, so local is not always just for the sake of it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/catplusplusok"&gt; /u/catplusplusok &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn217z/practical_use_of_local_ai_get_a_daily_postcard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn217z/practical_use_of_local_ai_get_a_daily_postcard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn217z/practical_use_of_local_ai_get_a_daily_postcard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T01:27:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnan7u</id>
    <title>How Did We Get Here? The largest companies are replacing their already cheap outsourced support staff with AI chatbots,</title>
    <updated>2026-01-26T08:37:40+00:00</updated>
    <author>
      <name>/u/MelodicRecognition7</name>
      <uri>https://old.reddit.com/user/MelodicRecognition7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;and they hallucinate back completely irrelevant responses. I had to choose the flair but this is not funny, especially given that a magic phrase &amp;quot;chat with human&amp;quot; does not work anymore.&lt;/p&gt; &lt;p&gt;Personal experience with Ebay: &amp;quot;I completely understand your frustration with $something&amp;quot; (the question was about a very different thing), &amp;quot;After &lt;strong&gt;thoroughly reviewing&lt;/strong&gt; the details of your transaction, I can confirm that it occurred on Mar 2025&amp;quot; (the transaction was just 2 weeks ago in Jan 2026), and so on.&lt;/p&gt; &lt;p&gt;Personal experience with Payoneer: &amp;quot;Please reply with the reason why you want to block your card.&amp;quot; (the support request was about Payoneer website returning an error when withdrawing funds to a bank account), &amp;quot;Please provide A video or A screenshot of the page that leads to the error and a screenshot of the error itself&amp;quot; (detailed screenshots were already provided in the previous message), and so on.&lt;/p&gt; &lt;p&gt;which other companies have also fired their live human support staff? Share your horror stories.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MelodicRecognition7"&gt; /u/MelodicRecognition7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnan7u/how_did_we_get_here_the_largest_companies_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnan7u/how_did_we_get_here_the_largest_companies_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qnan7u/how_did_we_get_here_the_largest_companies_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T08:37:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmjzx1</id>
    <title>KV cache fix for GLM 4.7 Flash</title>
    <updated>2026-01-25T14:06:55+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"&gt; &lt;img alt="KV cache fix for GLM 4.7 Flash" src="https://external-preview.redd.it/Yd6yP0tYXhTq7c3g8_wDa0Z1Zijr0IAXDTPXGjQc7ts.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=697845700fcf489c797d62fb0a23359703d41821" title="KV cache fix for GLM 4.7 Flash" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tl;dr: remove Air from GLM 4.7 Flash&lt;/p&gt; &lt;p&gt;KV cache uses a lot of VRAM. GLM 4.7 Flash doesn’t even use V in the KV cache. With long contexts, this means gigabytes of VRAM saved, so you can run much longer context on the same setup.&lt;/p&gt; &lt;p&gt;UPDATE &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19067"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T14:06:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn0dl8</id>
    <title>Backporting FP8 to the RTX 3090 (No H100 Required)</title>
    <updated>2026-01-26T00:16:50+00:00</updated>
    <author>
      <name>/u/one_does_not_just</name>
      <uri>https://old.reddit.com/user/one_does_not_just</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Worked on this project over the weekend, was curious if I can get fp8 compute going without decoding to fp16 in global memory or storing fp16 intermediates. Sacrificed some compute perf, but did achieve the intended VRAM savings. I did add a torch extension, if you wanna try it in your workflow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/one_does_not_just"&gt; /u/one_does_not_just &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://amohan.dev/blog/2026/fp8-as-storage-imma-ampere/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn0dl8/backporting_fp8_to_the_rtx_3090_no_h100_required/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn0dl8/backporting_fp8_to_the_rtx_3090_no_h100_required/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T00:16:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn9jg3</id>
    <title>Reflow Studio v0.5: A fully local, portable Neural Dubbing Workstation (RVC + Wav2Lip + GFPGAN). No Python install required.</title>
    <updated>2026-01-26T07:32:05+00:00</updated>
    <author>
      <name>/u/MeanManagement834</name>
      <uri>https://old.reddit.com/user/MeanManagement834</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn9jg3/reflow_studio_v05_a_fully_local_portable_neural/"&gt; &lt;img alt="Reflow Studio v0.5: A fully local, portable Neural Dubbing Workstation (RVC + Wav2Lip + GFPGAN). No Python install required." src="https://external-preview.redd.it/ajZpMWRzZGRjbmZnMd4x9B9VcVNKweplxa8BtHpj-my1OgtVfslok1CAkfL6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=918eeb6d7bdf677a3dd2d221ac8a00a14d7aa3b3" title="Reflow Studio v0.5: A fully local, portable Neural Dubbing Workstation (RVC + Wav2Lip + GFPGAN). No Python install required." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;I got tired of relying on cloud services or setting up complex Python environments just to run basic AI dubbing workflows. I wanted something that felt like a proper &amp;quot;app&amp;quot;—offline, private, and cool to look at.&lt;/p&gt; &lt;h1&gt;The Solution: Reflow Studio v0.5&lt;/h1&gt; &lt;p&gt;I built a fully portable, local workstation that combines &lt;strong&gt;RVC&lt;/strong&gt; (Voice Cloning) and &lt;strong&gt;Wav2Lip&lt;/strong&gt; (Lip Sync) into a single Cyberpunk-themed interface.&lt;/p&gt; &lt;h2&gt;Features in v0.5:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;🤖 Neural Voice Cloning:&lt;/strong&gt; Integrated RVC for instant, high-quality voice cloning.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;👄 Wav2Lip Sync:&lt;/strong&gt; Automatically matches the video mouth movements to the dubbed audio.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;👁️ Face Enhancement:&lt;/strong&gt; Built-in GFPGAN to fix the blurry mouth issues common with Wav2Lip.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;🛡️ Vision Meter:&lt;/strong&gt; Real-time content filtering.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;🚀 Portable:&lt;/strong&gt; No Python/CUDA installation needed. Download the zip, extract, and run the &lt;code&gt;.bat&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Tech Stack&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; Gradio (Heavily customized CSS)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; PyTorch, FFmpeg&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Models:&lt;/strong&gt; RVC v2, Wav2Lip-GAN, GFPGAN&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Try it out&lt;/h2&gt; &lt;p&gt;It's open source and available now. I'd love feedback on the UI and performance on different GPUs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub &amp;amp; Download:&lt;/strong&gt; &lt;a href="https://github.com/ananta-sj/ReFlow-Studio"&gt;https://github.com/ananta-sj/ReFlow-Studio&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MeanManagement834"&gt; /u/MeanManagement834 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/sesj5xfdcnfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn9jg3/reflow_studio_v05_a_fully_local_portable_neural/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn9jg3/reflow_studio_v05_a_fully_local_portable_neural/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T07:32:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmlpjp</id>
    <title>Internet blackout and Local LLMs</title>
    <updated>2026-01-25T15:15:05+00:00</updated>
    <author>
      <name>/u/DunderSunder</name>
      <uri>https://old.reddit.com/user/DunderSunder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Due to protests and massacre in Iran we are facing severe internet blackout which has been ongoing for 400 HOURS. only after a few days 3 websites got white-listed: google, chatgpt, deepseek. everything else is blocked even subdomains like Gmail. at the very least few people have Starlink (which is illegal) and share their connection. Finding a working vpn is really hard (I busted my ass to load reddit).&lt;/p&gt; &lt;p&gt;Meanwhile, I've been using my local uncensored Gemma3 12B and Qwen3 8B (on 8gb VRAM with llama.cpp). Then we got access to chatgpt which was pretty good since we could ask it to read contents of some pages or get latest news. But still chatgpt is VERY unhelpful in terms of finding solutions to circumvent internet censorship even if I explain the truly fucked up situation it refuses, and deepseek is worse. This is where a large uncensored local LLM could be very helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DunderSunder"&gt; /u/DunderSunder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T15:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn3evg</id>
    <title>~60GB models on coding: GLM 4.7 Flash vs. GPT OSS 120B vs. Qwen3 Coder 30B -- your comparisons?</title>
    <updated>2026-01-26T02:28:31+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All three of the models seem really strong. Qwen is the oldest, being from 2025 July, while we have about a week of experience with the GLM model now. They're all on the same class, taking ~60GB storage.&lt;/p&gt; &lt;p&gt;So just out of curiosity, what have your experiences been between the three models? What do you think the pros/cons are for each of the models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn3evg/60gb_models_on_coding_glm_47_flash_vs_gpt_oss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn3evg/60gb_models_on_coding_glm_47_flash_vs_gpt_oss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn3evg/60gb_models_on_coding_glm_47_flash_vs_gpt_oss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T02:28:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn2n4p</id>
    <title>I reverse-engineered Microsoft AutoGen’s reasoning loop and cut agent latency by 85% (13.4s → 1.6s). Here is the architecture.</title>
    <updated>2026-01-26T01:54:35+00:00</updated>
    <author>
      <name>/u/New_Care3681</name>
      <uri>https://old.reddit.com/user/New_Care3681</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’ve been building voice agents using AutoGen, and the &amp;quot;awkward silence&amp;quot; during the Chain-of-Thought (CoT) phase was killing the UX. The standard sequential loop (Think → Wait → Execute Tool → Wait → Speak) just doesn't work for real-time interaction.&lt;/p&gt; &lt;p&gt;Instead of waiting for a v2 update, I dug into the ConversableAgent class and implemented a module for Speculative Reasoning Execution (SRE).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Core Idea:&lt;/strong&gt;&lt;br /&gt; Standard Speculative Decoding predicts tokens. I adapted this to predict Tool Calls.&lt;br /&gt; While the LLM is still generating its &amp;quot;Reasoning&amp;quot; text (e.g., &amp;quot;I need to search for weather...&amp;quot;), my module regex-sniffs the stream for intent. If it detects a high-confidence tool pattern, it executes the tool asynchronously in a background thread before the LLM finishes the sentence.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Benchmarks (NVIDIA A100):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Baseline: 13.4s Time-to-Action (Sequential)&lt;/li&gt; &lt;li&gt;With SRE: 1.6s Time-to-Action (Parallel)&lt;/li&gt; &lt;li&gt;Reduction: ~85%&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The PR is currently approved by the AutoGen core team:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fautogen%2Fpull%2F7179"&gt;https://github.com/microsoft/autogen/pull/7179&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I also built a distributed training rig for Whisper on Ray (SpeechLab):&lt;/strong&gt;&lt;br /&gt; To verify if my infra skills scaled, I built a fault-tolerant training engine for Whisper using Ray Train + PyTorch DDP. It handles streaming audio ingestion (so no OOM on Terabyte datasets) and hit 94% scaling efficiency on 4x A100s.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Demo (Vimeo): &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fvimeo.com%2F1156797116"&gt;https://vimeo.com/1156797116&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Repo: &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2FYash3561%2Fspeechlab"&gt;https://github.com/Yash3561/speechlab&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Looking for Feedback:&lt;/strong&gt;&lt;br /&gt; I built this to solve the &amp;quot;awkward silence&amp;quot; bottleneck in my own voice agents, but I'm curious how others are handling CoT latency in production.&lt;/p&gt; &lt;p&gt;If you are running agentic runtimes or distributed training platforms, I’d love to roast your architecture (or have you roast mine). Happy to answer questions about the regex sniffing logic or Ray actor pool management in the comments!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New_Care3681"&gt; /u/New_Care3681 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn2n4p/i_reverseengineered_microsoft_autogens_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn2n4p/i_reverseengineered_microsoft_autogens_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn2n4p/i_reverseengineered_microsoft_autogens_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T01:54:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmvny5</id>
    <title>GLM-4.7-Flash is even faster now</title>
    <updated>2026-01-25T21:14:50+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/"&gt; &lt;img alt="GLM-4.7-Flash is even faster now" src="https://external-preview.redd.it/y3hK5MFwhoK-QUOGog7BpAan8RKjGCnfL7Xowe9Lb4E.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=880502683b1e21f9efe3ec41ebe19f6a59040622" title="GLM-4.7-Flash is even faster now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19092"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T21:14:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qn3xig</id>
    <title>I just won an Nvidia DGX Spark GB10 at an Nvidia hackathon. What do I do with it?</title>
    <updated>2026-01-26T02:51:42+00:00</updated>
    <author>
      <name>/u/brandon-i</name>
      <uri>https://old.reddit.com/user/brandon-i</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn3xig/i_just_won_an_nvidia_dgx_spark_gb10_at_an_nvidia/"&gt; &lt;img alt="I just won an Nvidia DGX Spark GB10 at an Nvidia hackathon. What do I do with it?" src="https://preview.redd.it/wky8vuufylfg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce8114a0cbc29adcc5dff5d6dd9ef4259bf40636" title="I just won an Nvidia DGX Spark GB10 at an Nvidia hackathon. What do I do with it?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;Noob here. I just won an Nvidia Hackathon and the prize was a Dell DGX Spark GB10.&lt;/p&gt; &lt;p&gt;I’ve never fine tuned a model before and I was just using it for inferencing a nemotron 30B with vLLM that took 100+ GB of memory.&lt;/p&gt; &lt;p&gt;Anything you all would recommend me doing with it first?&lt;/p&gt; &lt;p&gt;NextJS was using around 60GB+ at one point so maybe I can run 2 nextJS apps at the same time potentially.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brandon-i"&gt; /u/brandon-i &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wky8vuufylfg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qn3xig/i_just_won_an_nvidia_dgx_spark_gb10_at_an_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qn3xig/i_just_won_an_nvidia_dgx_spark_gb10_at_an_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T02:51:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
