<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-07T19:23:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lu1z10</id>
    <title>Learning triton &amp; cuda: How far can colab + nsight-compute take me?</title>
    <updated>2025-07-07T18:29:55+00:00</updated>
    <author>
      <name>/u/Zealousideal_Elk109</name>
      <uri>https://old.reddit.com/user/Zealousideal_Elk109</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks!&lt;/p&gt; &lt;p&gt;I've recently been learning Triton and CUDA, writing my own kernels and optimizing them using a lot of great tricks I‚Äôve picked up from blog-posts and docs. However, I currently don‚Äôt have access to any local GPUs.&lt;/p&gt; &lt;p&gt;Right now, I‚Äôm using Google Colab with T4 GPUs to run my kernels. I collect telemetry and kernel stats using nsight-compute, then download the reports and inspect them locally using the GUI.&lt;/p&gt; &lt;p&gt;It‚Äôs been workable thus far, but I‚Äôm wondering: how far can I realistically go with this workflow? I‚Äôm also a bit concerned about optimizing against the T4, since it‚Äôs now three generations behind the latest architecture and I‚Äôm not sure how transferable performance insights will be.&lt;/p&gt; &lt;p&gt;Also, I‚Äôd love to hear how you are writing and profiling your kernels, especially if you're doing inference-time optimizations. Any tips or suggestions would be much appreciated.&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal_Elk109"&gt; /u/Zealousideal_Elk109 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu1z10/learning_triton_cuda_how_far_can_colab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu1z10/learning_triton_cuda_how_far_can_colab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lu1z10/learning_triton_cuda_how_far_can_colab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T18:29:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltbg2s</id>
    <title>Narrative Beam Search workflow in Open WebUI</title>
    <updated>2025-07-06T20:39:40+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltbg2s/narrative_beam_search_workflow_in_open_webui/"&gt; &lt;img alt="Narrative Beam Search workflow in Open WebUI" src="https://external-preview.redd.it/cGprbTl2dDhlYmJmMfNv2qQc5KO5fB6gHC38B4rcVAB-vfM2l6tq4JjQRNK2.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88fed0ead8ff149a552dc79fcd54b637dc557727" title="Narrative Beam Search workflow in Open WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What is this?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A variant of beam search which runs from the point of view of different system prompts. The workflow runs in an optimising LLM proxy that sends an artifact back to Open WebUI that listens to the data from the pending completion.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/av/harbor/blob/main/boost/src/modules/nbs.py"&gt;Code&lt;/a&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/067r3vt8ebbf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltbg2s/narrative_beam_search_workflow_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltbg2s/narrative_beam_search_workflow_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T20:39:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltwuga</id>
    <title>Trouble setting up conda environment for unsloth finetuning</title>
    <updated>2025-07-07T15:16:33+00:00</updated>
    <author>
      <name>/u/No-Mud-1902</name>
      <uri>https://old.reddit.com/user/No-Mud-1902</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can you please help me find a clean way to set up a conda environment correctly to finetune a model from huggingface using unsloth. I keep getting dependency issues and am losing my mind. this is what am doing now:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;conda create --name unsloth_env python=3.10 -y conda activate unsloth_env conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y pip install bitsandbytes pip install git+https://github.com/unslothai/unsloth.git &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Mud-1902"&gt; /u/No-Mud-1902 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltwuga/trouble_setting_up_conda_environment_for_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltwuga/trouble_setting_up_conda_environment_for_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltwuga/trouble_setting_up_conda_environment_for_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T15:16:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lu35pf</id>
    <title>Why does Copilot sound like a 60 year old CEO / politician</title>
    <updated>2025-07-07T19:15:06+00:00</updated>
    <author>
      <name>/u/Anto444_</name>
      <uri>https://old.reddit.com/user/Anto444_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu35pf/why_does_copilot_sound_like_a_60_year_old_ceo/"&gt; &lt;img alt="Why does Copilot sound like a 60 year old CEO / politician" src="https://preview.redd.it/tas86nsu4ibf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9ec7a5b34e1c1d4a27828482beb605b8f5b980a7" title="Why does Copilot sound like a 60 year old CEO / politician" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anto444_"&gt; /u/Anto444_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tas86nsu4ibf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lu35pf/why_does_copilot_sound_like_a_60_year_old_ceo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lu35pf/why_does_copilot_sound_like_a_60_year_old_ceo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T19:15:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lts4q2</id>
    <title>Video Dubbing: TTS + Speaker Detection + Auto-Length Adjustments?</title>
    <updated>2025-07-07T11:51:33+00:00</updated>
    <author>
      <name>/u/Initial_Designer_802</name>
      <uri>https://old.reddit.com/user/Initial_Designer_802</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, the company I work for is currently using an online service for dubbing. And I gotta say, they're pretty good; You upload a video and translated subtitles, make some minor tweaks, and the video is automatically dubbed for you. &lt;/p&gt; &lt;p&gt;Are there any local LLM models that can do something similar to this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Initial_Designer_802"&gt; /u/Initial_Designer_802 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lts4q2/video_dubbing_tts_speaker_detection_autolength/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lts4q2/video_dubbing_tts_speaker_detection_autolength/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lts4q2/video_dubbing_tts_speaker_detection_autolength/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T11:51:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lts4y9</id>
    <title>eGPU Setup: Legion Laptop + RTX 5060 Ti</title>
    <updated>2025-07-07T11:51:55+00:00</updated>
    <author>
      <name>/u/Few-Welcome3297</name>
      <uri>https://old.reddit.com/user/Few-Welcome3297</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing it here in case it's helpful for anyone&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few-Welcome3297"&gt; /u/Few-Welcome3297 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://shb777.dev/blog/egpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lts4y9/egpu_setup_legion_laptop_rtx_5060_ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lts4y9/egpu_setup_legion_laptop_rtx_5060_ti/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T11:51:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltyc9k</id>
    <title>Has anyone here tried to augment text data using local domain specific LLMs ?</title>
    <updated>2025-07-07T16:14:16+00:00</updated>
    <author>
      <name>/u/skillmaker</name>
      <uri>https://old.reddit.com/user/skillmaker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did any of you guys try to augment text data uaing an LLM? For example augmenting medical symptoms using MedGemma, by telling the LLM to generate 3 different phrases similar to the original phrase and then repeating this for every row until all the dataset is augmented.&lt;/p&gt; &lt;p&gt;What do you think about this approach, and would it be better than using a bert model or other augmentation techniques like synonyms replacement, translation....&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skillmaker"&gt; /u/skillmaker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltyc9k/has_anyone_here_tried_to_augment_text_data_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltyc9k/has_anyone_here_tried_to_augment_text_data_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltyc9k/has_anyone_here_tried_to_augment_text_data_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T16:14:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lts8ai</id>
    <title>Would you pay for a service that uses your localLLM to power the app</title>
    <updated>2025-07-07T11:56:58+00:00</updated>
    <author>
      <name>/u/numinouslymusing</name>
      <uri>https://old.reddit.com/user/numinouslymusing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Whether LLMs have any useful applications past summarization and basic tasks is another debate, but if you found a useful service but it used a local LLM would you still pay for it? or rather find a way to run it locally. Or you prefer hosted models if your paying for it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/numinouslymusing"&gt; /u/numinouslymusing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lts8ai/would_you_pay_for_a_service_that_uses_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lts8ai/would_you_pay_for_a_service_that_uses_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lts8ai/would_you_pay_for_a_service_that_uses_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T11:56:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltw5lh</id>
    <title>Radeon Pro Duo or AMD Instinct Mi50?</title>
    <updated>2025-07-07T14:49:50+00:00</updated>
    <author>
      <name>/u/nathan22211</name>
      <uri>https://old.reddit.com/user/nathan22211</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been trying to decide between the two for my AI rig, which since I mostly use GPT4all for LLama models, I'd imagine I'm stuck with Vulcan. I bought a ARC A770 prior but intel cards are blacklisted by GPT4All currently, which is why I've been looking at one of these two cards. I already have a dual haswell xeon rig for this with linux so I'm covered for the Mi50.&lt;/p&gt; &lt;p&gt;radeon pro duo is about 370 USD for me: &lt;a href="https://www.ebay.com/itm/226829831304"&gt;https://www.ebay.com/itm/226829831304&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the Mi50 is about 100 USD cheaper: &lt;a href="https://www.ebay.com/itm/146669335049"&gt;https://www.ebay.com/itm/146669335049&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nathan22211"&gt; /u/nathan22211 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltw5lh/radeon_pro_duo_or_amd_instinct_mi50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltw5lh/radeon_pro_duo_or_amd_instinct_mi50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltw5lh/radeon_pro_duo_or_amd_instinct_mi50/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T14:49:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltnpsl</id>
    <title>How good is Qwen3-14B for local use? Any benchmarks vs other models?</title>
    <updated>2025-07-07T07:14:32+00:00</updated>
    <author>
      <name>/u/abubakkar_s</name>
      <uri>https://old.reddit.com/user/abubakkar_s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I'm looking into running a larger language model locally and came across Qwen3-14B (or Qwen3\_14B depending on naming). I know it's been getting some hype lately, but I wanted to hear from people who‚Äôve actually used it.&lt;/p&gt; &lt;p&gt;* How does it perform compared to other 13B/14B class models like Gemma, Mistral, LLaMA 2/3, Yi, etc.?&lt;/p&gt; &lt;p&gt;* Any real-world performance/benchmark comparisons in terms of speed, context handling, or reasoning?&lt;/p&gt; &lt;p&gt;* How‚Äôs the quantization support (GGUF/ExLlama/AutoGPTQ)? Is it efficient enough to run on a single GPU (e.g. 24GB VRAM of Macmini m4, token/secs)?&lt;/p&gt; &lt;p&gt;* How does it do with coding, long-context tasks, or general instruction following?&lt;/p&gt; &lt;p&gt;Would like to hear your experience, whether it‚Äôs through serious benchmarking or just specific use. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abubakkar_s"&gt; /u/abubakkar_s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltnpsl/how_good_is_qwen314b_for_local_use_any_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltnpsl/how_good_is_qwen314b_for_local_use_any_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltnpsl/how_good_is_qwen314b_for_local_use_any_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T07:14:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltxiy4</id>
    <title>LangChain/Crew/AutoGen made it easy to build agents, but operating them is a joke</title>
    <updated>2025-07-07T15:43:04+00:00</updated>
    <author>
      <name>/u/ImmuneCoder</name>
      <uri>https://old.reddit.com/user/ImmuneCoder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built an internal support agent using LangChain + OpenAI + some simple tool calls.&lt;/p&gt; &lt;p&gt;Getting to a working prototype took 3 days with Cursor and just messing around. Great.&lt;/p&gt; &lt;p&gt;But actually trying to operate that agent across multiple teams was absolute chaos.&lt;/p&gt; &lt;p&gt;‚Äì No structured logs of intermediate reasoning&lt;/p&gt; &lt;p&gt;‚Äì No persistent memory or traceability&lt;/p&gt; &lt;p&gt;‚Äì No access control (anyone could run/modify it)&lt;/p&gt; &lt;p&gt;‚Äì No ability to validate outputs at scale&lt;/p&gt; &lt;p&gt;It‚Äôs like deploying a microservice with no logs, no auth, and no monitoring. The frameworks are designed for demos, not real workflows. And everyone I know is duct-taping together JSON dumps + Slack logs to stay afloat.&lt;/p&gt; &lt;p&gt;So, what does agent infra actually look like after the first prototype for you guys?&lt;/p&gt; &lt;p&gt;Would love to hear real setups. Especially if you‚Äôve gone past the LangChain happy path.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ImmuneCoder"&gt; /u/ImmuneCoder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxiy4/langchaincrewautogen_made_it_easy_to_build_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxiy4/langchaincrewautogen_made_it_easy_to_build_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxiy4/langchaincrewautogen_made_it_easy_to_build_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T15:43:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltt72w</id>
    <title>Free context tool that runs local</title>
    <updated>2025-07-07T12:44:12+00:00</updated>
    <author>
      <name>/u/wuu73</name>
      <uri>https://old.reddit.com/user/wuu73</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I believe my tool is unique even though there are like 40 different similar tools for giving LLMs context of lots of code files. Different for:&lt;/p&gt; &lt;p&gt;Saving the state of which files you include for next time you use it in that same directory,&lt;/p&gt; &lt;p&gt;The User Interface (works anywhere python and Qt can run) can just type ‚Äòaicp + enter‚Äô. Option to install right click menu on any OS for finder, file explorer, nautilus. &lt;/p&gt; &lt;p&gt;Prompt on top and/or bottom (both can enhance response from LLM)&lt;/p&gt; &lt;p&gt;Preset buttons, can add your own bits of text you find yourself asking often, like ‚Äúwrite solution in single code tag to paste into Cline or Cursor‚Äù. &lt;/p&gt; &lt;p&gt;I posted here cuz it runs local and does not need GitHub like some of the similar tools. I get some great feedback and there is a thing in the help menu to complain or send your thoughts about it anonymously. Easy install with pipx.&lt;/p&gt; &lt;p&gt;&lt;a href="https://wuu73.org/aicp"&gt;https://wuu73.org/aicp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I hate those tech bro phrases so I really hate to even say this but ‚Äúcontext engineering‚Äù does seem appropriate lol that is what the tool does basically &lt;/p&gt; &lt;p&gt;Shaves off seconds every time you have to IDE &amp;lt;‚Äî‚Äî&amp;gt; tabs of web chat interfaces&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wuu73"&gt; /u/wuu73 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltt72w/free_context_tool_that_runs_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltt72w/free_context_tool_that_runs_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltt72w/free_context_tool_that_runs_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T12:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltbrlf</id>
    <title>üéß Listen and Compare 12 Open-Source Text-to-Speech Models (Hugging Face Space)</title>
    <updated>2025-07-06T20:53:01+00:00</updated>
    <author>
      <name>/u/rbgo404</name>
      <uri>https://old.reddit.com/user/rbgo404</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltbrlf/listen_and_compare_12_opensource_texttospeech/"&gt; &lt;img alt="üéß Listen and Compare 12 Open-Source Text-to-Speech Models (Hugging Face Space)" src="https://preview.redd.it/bwd1gqkrfbbf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57eba9e11159f3d51759d5ca917254faf9332203" title="üéß Listen and Compare 12 Open-Source Text-to-Speech Models (Hugging Face Space)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;We have been exploring various open-source Text-to-Speech (TTS) models, and decided to create a Hugging Face demo space that makes it easy to compare their quality side-by-side.&lt;/p&gt; &lt;p&gt;The demo features &lt;strong&gt;12 popular TTS models&lt;/strong&gt;, all tested using a consistent prompt, so you can quickly hear and compare their synthesized speech and choose the best one for your audio projects.&lt;/p&gt; &lt;p&gt;Would love to get feedback or suggestions!&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="https://huggingface.co/spaces/Inferless/Open-Source-TTS-Gallary"&gt;Check out the demo space and detailed comparison here!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="https://www.inferless.com/learn/comparing-different-text-to-speech---tts--models-part-2"&gt;Check out the blog: Choosing the Right Text-to-Speech Model: Part 2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Share your use-case and we will update this space as required! &lt;/p&gt; &lt;p&gt;Which TTS model sounds most natural to you?&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rbgo404"&gt; /u/rbgo404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bwd1gqkrfbbf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltbrlf/listen_and_compare_12_opensource_texttospeech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltbrlf/listen_and_compare_12_opensource_texttospeech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T20:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltstdt</id>
    <title>[PAPER] Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs</title>
    <updated>2025-07-07T12:25:56+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The thought progress bar looks cool.&lt;/p&gt; &lt;p&gt;Unfortunately, this needs to train something to modify hidden state.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://royeisen.github.io/OverclockingLLMReasoning-paper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltstdt/paper_overclocking_llm_reasoning_monitoring_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltstdt/paper_overclocking_llm_reasoning_monitoring_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T12:25:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltxzad</id>
    <title>Hardware recommendations? Mac Mini, NVIDIA Orin, Ryzen AI... ?</title>
    <updated>2025-07-07T16:00:41+00:00</updated>
    <author>
      <name>/u/lizard121n6</name>
      <uri>https://old.reddit.com/user/lizard121n6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there! I recently started being interested in getting an &amp;quot;affordable&amp;quot; Mini PC type machine that can run LLMs without being too power hungry. &lt;/p&gt; &lt;p&gt;The first challenge is to try and understand what is required for this. What I have gathered so far:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RAM is important (double the model size in billions and leave room for some overhead, e.g. 7B*2 = 14 =&amp;gt; 16GB should work)&lt;/li&gt; &lt;li&gt;Memory Bandwidth is another very important factor, which is why graphics cards with enough VRAM work better than CPUs with much more RAM&lt;/li&gt; &lt;li&gt;There are options with shared/unified RAM, especially the Apple Silicon ones&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That being said, I just don't know how to find out what to get. So many options, so little information. No LLM benchmarks.&lt;/p&gt; &lt;p&gt;The Apple Silicon Chips are doing a good job with their high RAM configurations and unified RAM and good bandwidth. So what about Ryzen AI, e.g. AMD Ryzen AI 9 HX370. It has a CPU, GPU, NPU; where would the LLM run, can it run on the NPU? Ho do I know how the performance compares with e.g. a Mac Mini M2 Pro? And then there are dedicated AI options like the NVIDIA Orin NX, which come with &amp;quot;only&amp;quot; 16GB of RAM max. I also tried running LLama 3.1 7B on my 2060 Super and the result was satisfactory.. So some Mini-PC with a decent graphics card might also work?&lt;/p&gt; &lt;p&gt;I just don't know where to start, what to buy, how do I find out? &lt;/p&gt; &lt;p&gt;What I really want is the best option for 500-800‚Ç¨. A setup with a full sized (external) graphics card is not an option. I would love for it to be upgradeable. I started with just wanting to tinker with a RasPI-AI Hat and then everything grew from there. I don't have huge demands, running a 7B model on an (upgradeable) Mini-PC would make me happy. &lt;/p&gt; &lt;p&gt;Some examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GMtec Evo X1 (AMD Ryzen AI 9 HX370 with unified memory (?))&lt;/li&gt; &lt;li&gt;Mac Mini M2 Pro&lt;/li&gt; &lt;li&gt;Mac Mini M4&lt;/li&gt; &lt;li&gt; MINISFORUM AI X1 370&lt;/li&gt; &lt;li&gt;NVIDIA Orin NX 8/16GB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am very thankful for any advice!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lizard121n6"&gt; /u/lizard121n6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxzad/hardware_recommendations_mac_mini_nvidia_orin/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxzad/hardware_recommendations_mac_mini_nvidia_orin/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxzad/hardware_recommendations_mac_mini_nvidia_orin/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T16:00:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltamap</id>
    <title>Cheapest way to stack VRAM in 2025?</title>
    <updated>2025-07-06T20:04:49+00:00</updated>
    <author>
      <name>/u/gnad</name>
      <uri>https://old.reddit.com/user/gnad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking to get a total of at least 140 GB RAM/VRAM combined to run Qwen 235B Q4. Current i have 96 GB RAM so next step is to get some cheap VRAM. After some research i found the following options at around 1000$ each: &lt;/p&gt; &lt;ol&gt; &lt;li&gt;4x RTX 3060 (48 GB)&lt;/li&gt; &lt;li&gt;4x P100 (64 GB)&lt;/li&gt; &lt;li&gt;3x P40 (72 GB)&lt;/li&gt; &lt;li&gt;3x RX 9060 (48 GB)&lt;/li&gt; &lt;li&gt;4x MI50 32GB (128GB)&lt;/li&gt; &lt;li&gt;3x RTX 4060 ti/5060 ti (48 GB)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Edit: add more suggestion from comments. &lt;/p&gt; &lt;p&gt;Which GPU do you recommend or is there anything else better? I know 3090 is king here but cost per GB is around double the above GPU. Any suggestion is appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gnad"&gt; /u/gnad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltamap/cheapest_way_to_stack_vram_in_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T20:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltvkqq</id>
    <title>(Kramer UI for Ollama) I was tired of dealing with Docker, so I built a simple, portable Windows UI for Ollama.</title>
    <updated>2025-07-07T14:26:54+00:00</updated>
    <author>
      <name>/u/DanielKramer_</name>
      <uri>https://old.reddit.com/user/DanielKramer_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltvkqq/kramer_ui_for_ollama_i_was_tired_of_dealing_with/"&gt; &lt;img alt="(Kramer UI for Ollama) I was tired of dealing with Docker, so I built a simple, portable Windows UI for Ollama." src="https://external-preview.redd.it/9sqw7guDNkr_lh4DzQzAQ3_oGbJPe0qHLVbjofkhPuc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d5b553bbacb3aa769ebe7746d6025ee8190093ba" title="(Kramer UI for Ollama) I was tired of dealing with Docker, so I built a simple, portable Windows UI for Ollama." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share a small project I built for my own purposes: Kramer UI for Ollama.&lt;/p&gt; &lt;p&gt;I love Ollama for its simplicity and its model management, but setting up a UI for it has always been a pain point. I used to use OpenWebUI and it was great, but I'd rather not have to set up docker. And using Ollama through the CLI makes me feel like a simpleton because I can't even edit my messages.&lt;/p&gt; &lt;p&gt;I wanted a UI as simple as Ollama to accompany it. So I built it. Kramer UI is a single, portable executable file for Windows. There's no installer. You just run the .exe and you're ready to start chatting.&lt;/p&gt; &lt;p&gt;My goal was to make interacting with your local models as frictionless as possible.&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Uses 45mb of ram&lt;/li&gt; &lt;li&gt;Edit your messages&lt;/li&gt; &lt;li&gt;Models' thoughts are hidden behind dropdown&lt;/li&gt; &lt;li&gt;Model selector&lt;/li&gt; &lt;li&gt;Currently no support for conversation history&lt;/li&gt; &lt;li&gt;You can probably compile it for Linux and Mac too&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can download the executable directly from the GitHub releases page [here.] (&lt;a href="https://github.com/dvkramer/kramer-ui/releases/"&gt;https://github.com/dvkramer/kramer-ui/releases/&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wn2nw8zjrgbf1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca6f0470fa54f4ae06a9a19a88cf9c3fbbe8632e"&gt;https://preview.redd.it/wn2nw8zjrgbf1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca6f0470fa54f4ae06a9a19a88cf9c3fbbe8632e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All feedback, suggestions, and ideas are welcome! Let me know what you think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanielKramer_"&gt; /u/DanielKramer_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltvkqq/kramer_ui_for_ollama_i_was_tired_of_dealing_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltvkqq/kramer_ui_for_ollama_i_was_tired_of_dealing_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltvkqq/kramer_ui_for_ollama_i_was_tired_of_dealing_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T14:26:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltgayn</id>
    <title>Fused Qwen3 MoE layer for faster training Qwen3-30B-A3B LoRA</title>
    <updated>2025-07-07T00:18:28+00:00</updated>
    <author>
      <name>/u/woct0rdho</name>
      <uri>https://old.reddit.com/user/woct0rdho</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/"&gt; &lt;img alt="Fused Qwen3 MoE layer for faster training Qwen3-30B-A3B LoRA" src="https://external-preview.redd.it/HSpEkOVbwqk3BvJQ60pY-PTeM7wLxNZ1qE0Ade0f5cc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=094a72eeef2838a876e8835f533b8146c2ead2a9" title="Fused Qwen3 MoE layer for faster training Qwen3-30B-A3B LoRA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Qwen3 MoE model (and all other MoE models) in HF Transformers is notoriously slow, because it uses a for loop to access the experts, resulting in &amp;lt; 20% GPU usage. It's been two months and there are still very few LoRAs of Qwen3-30B-A3B in the public. (If you search 'qwen3 30b a3b lora' on HuggingFace, that's... interesting)&lt;/p&gt; &lt;p&gt;This should be made easier. I've made a fused version of Qwen3 MoE Layer that's much faster, while being compatible with the HF Transformers ecosystem, such as LoRA, bitsandbytes 4-bit quantization, and Unsloth. On a single GPU with 24GB VRAM, it reaches 100% GPU usage and 5x speedup of training compared to the unfused model.&lt;/p&gt; &lt;p&gt;There is still room for further optimization, but you can try it now and train your own LoRA.&lt;/p&gt; &lt;p&gt;Also, please help if you know how to upstream this to Transformers or Unsloth. (Transformers itself never includes Triton or CUDA kernels in the package, but they have a HuggingFace Kernels project to do so.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/woct0rdho"&gt; /u/woct0rdho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/woct0rdho/transformers-qwen3-moe-fused"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltgayn/fused_qwen3_moe_layer_for_faster_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T00:18:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltze9d</id>
    <title>Do you use prompt caching to save chat history in your LLM apps?</title>
    <updated>2025-07-07T16:53:44+00:00</updated>
    <author>
      <name>/u/Physical_Ad9040</name>
      <uri>https://old.reddit.com/user/Physical_Ad9040</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious to hear from others building LLM-based chat apps: Do you implement &lt;strong&gt;prompt caching&lt;/strong&gt; to store chat history or previous responses? Or do you send the chat history with each user's prompt?&lt;/p&gt; &lt;p&gt;Caching is more expensive to write, but the costs are then net positive if the conversation becomes long, no?&lt;/p&gt; &lt;p&gt;Would appreciate your insights ‚Äî thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Physical_Ad9040"&gt; /u/Physical_Ad9040 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T16:53:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltfgoy</id>
    <title>I drew a silly comic about Llama model</title>
    <updated>2025-07-06T23:37:41+00:00</updated>
    <author>
      <name>/u/Organic-Mechanic-435</name>
      <uri>https://old.reddit.com/user/Organic-Mechanic-435</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltfgoy/i_drew_a_silly_comic_about_llama_model/"&gt; &lt;img alt="I drew a silly comic about Llama model" src="https://a.thumbs.redditmedia.com/ntblmHJuZXo-K_j-B7phe4Ko7b3I1mCMnzblLD25_K8.jpg" title="I drew a silly comic about Llama model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a roleplayer using SillyTavern. Llama models are often used as 'base' for fine tunes in Huggingface. Seeing what people can do with local models also fascinate me. &lt;sup&gt;^&lt;/sup&gt; Hello!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Organic-Mechanic-435"&gt; /u/Organic-Mechanic-435 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ltfgoy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltfgoy/i_drew_a_silly_comic_about_llama_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltfgoy/i_drew_a_silly_comic_about_llama_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T23:37:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lt4y1z</id>
    <title>Self-hosted AI coding that just works</title>
    <updated>2025-07-06T16:09:28+00:00</updated>
    <author>
      <name>/u/send_me_a_ticket</name>
      <uri>https://old.reddit.com/user/send_me_a_ticket</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;TLDR&lt;/em&gt;&lt;/strong&gt;: VSCode + RooCode + LM Studio + Devstral + snowflake-arctic-embed2 + docs-mcp-server. A fast, cost-free, self-hosted AI coding assistant setup supports lesser-used languages and minimizes hallucinations on less powerful hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Long Post:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hello everyone, sharing my findings on trying to find a self-hosted agentic AI coding assistant that:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Responds reasonably well on a variety of hardware.&lt;/li&gt; &lt;li&gt;Doesn‚Äôt hallucinate outdated syntax.&lt;/li&gt; &lt;li&gt;Costs $0 (except electricity).&lt;/li&gt; &lt;li&gt;Understands less common languages, e.g., KQL, Flutter, etc.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;After experimenting with several setups, here‚Äôs the combo I found that actually works.&lt;br /&gt; Please forgive any mistakes and feel free to let me know of any improvements you are aware of.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware&lt;/strong&gt;&lt;br /&gt; Tested on a Ryzen 5700 + RTX 3080 (10GB VRAM), 48GB RAM.&lt;br /&gt; Should work on both low, and high-end setups, your mileage may vary.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Stack&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;VSCode +(with) RooCode +(connected to) LM Studio +(running both) Devstral +(and) snowflake-arctic-embed2 +(supported by) docs-mcp-server&lt;/code&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit 1:&lt;/strong&gt; Setup Process for users saying this is too complicated&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Install &lt;code&gt;VSCode&lt;/code&gt; then get &lt;code&gt;RooCode&lt;/code&gt; Extension&lt;/li&gt; &lt;li&gt;Install &lt;code&gt;LMStudio&lt;/code&gt; and pull &lt;code&gt;snowflake-arctic-embed2&lt;/code&gt; embeddings model, as well as &lt;code&gt;Devstral&lt;/code&gt; large language model which suits your computer. Start LM Studio server and load both models from &amp;quot;Power User&amp;quot; tab.&lt;/li&gt; &lt;li&gt;Install &lt;code&gt;Docker&lt;/code&gt; or &lt;code&gt;NodeJS&lt;/code&gt;, depending on which config you prefer &lt;em&gt;(recommend Docker)&lt;/em&gt;&lt;/li&gt; &lt;li&gt;Include &lt;code&gt;docs-mcp-server&lt;/code&gt; in your RooCode MCP configuration &lt;em&gt;(see json below)&lt;/em&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Edit 2&lt;/strong&gt;: I had been &lt;a href="https://docs.useanything.com/setup/embedder-configuration/local/lmstudio"&gt;misinformed&lt;/a&gt; that running embeddings and LLM together via LM Studio is not possible, it certainly is! I have updated this guide to remove Ollama altogether and only use LM Studio.&lt;/p&gt; &lt;p&gt;LM Studio made it slightly confusing because you cannot load embeddings model from &amp;quot;Chat&amp;quot; tab, you must load it from &amp;quot;Developer&amp;quot; tab.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;VSCode + RooCode&lt;/strong&gt;&lt;br /&gt; RooCode is a VS Code extension that enables agentic coding and has MCP support.&lt;/p&gt; &lt;p&gt;VS Code: &lt;a href="https://code.visualstudio.com/download"&gt;https://code.visualstudio.com/download&lt;/a&gt;&lt;br /&gt; Alternative - VSCodium: &lt;a href="https://github.com/VSCodium/vscodium/releases"&gt;https://github.com/VSCodium/vscodium/releases&lt;/a&gt; - No telemetry&lt;/p&gt; &lt;p&gt;RooCode: &lt;a href="https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline"&gt;https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Alternative to this setup is Zed Editor: &lt;a href="https://zed.dev/download"&gt;https://zed.dev/download&lt;/a&gt;&lt;/p&gt; &lt;p&gt;( Zed is nice, but you cannot yet pass problems as context. Released only for MacOS and Linux, coming soon for windows. Unofficial windows nightly here: &lt;a href="https://github.com/send-me-a-ticket/zedforwindows"&gt;github.com/send-me-a-ticket/zedforwindows&lt;/a&gt; )&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LM Studio&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://lmstudio.ai/download"&gt;https://lmstudio.ai/download&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nice UI with real-time logs&lt;/li&gt; &lt;li&gt;GPU offloading is too simple. Changing AI model parameters is a breeze. You can achieve same effect in ollama by creating custom models with changed num_gpu and num_ctx parameters&lt;/li&gt; &lt;li&gt;Good (better?) OpenAI-compatible API&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Devstral (Unsloth finetune)&lt;/strong&gt;&lt;br /&gt; Solid coding model with good tool usage.&lt;/p&gt; &lt;p&gt;I use &lt;code&gt;devstral-small-2505@iq2_m&lt;/code&gt;, which fully fits within 10GB VRAM. token context 32768.&lt;br /&gt; Other variants &amp;amp; parameters may work depending on your hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;snowflake-arctic-embed2&lt;/strong&gt;&lt;br /&gt; Tiny embeddings model used with docs-mcp-server. Feel free to substitute for any better ones.&lt;br /&gt; I use &lt;code&gt;text-embedding-snowflake-arctic-embed-l-v2.0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://www.docker.com/products/docker-desktop/"&gt;https://www.docker.com/products/docker-desktop/&lt;/a&gt;&lt;br /&gt; Recommend Docker use instead of NPX, for security and ease of use.&lt;/p&gt; &lt;p&gt;Portainer is my recommended extension for ease of use:&lt;br /&gt; &lt;a href="https://hub.docker.com/extensions/portainer/portainer-docker-extension"&gt;https://hub.docker.com/extensions/portainer/portainer-docker-extension&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;docs-mcp-server&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://github.com/arabold/docs-mcp-server"&gt;https://github.com/arabold/docs-mcp-server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is what makes it all click. MCP server scrapes documentation (with versioning) so the AI can look up the &lt;em&gt;correct&lt;/em&gt; syntax for &lt;em&gt;your&lt;/em&gt; version of language implementation, and avoid hallucinations.&lt;/p&gt; &lt;p&gt;You &lt;em&gt;should&lt;/em&gt; also be able to run &lt;code&gt;localhost:6281&lt;/code&gt; to open web UI for the &lt;code&gt;docs-mcp-server&lt;/code&gt;, however web UI doesn't seem to be working for me, which I can ignore because AI is managing that anyway.&lt;/p&gt; &lt;p&gt;You can implement this MCP server as following -&lt;/p&gt; &lt;p&gt;&lt;em&gt;Docker version (needs Docker Installed)&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;mcpServers&amp;quot;: { &amp;quot;docs-mcp-server&amp;quot;: { &amp;quot;command&amp;quot;: &amp;quot;docker&amp;quot;, &amp;quot;args&amp;quot;: [ &amp;quot;run&amp;quot;, &amp;quot;-i&amp;quot;, &amp;quot;--rm&amp;quot;, &amp;quot;-p&amp;quot;, &amp;quot;6280:6280&amp;quot;, &amp;quot;-p&amp;quot;, &amp;quot;6281:6281&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;OPENAI_API_KEY&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;OPENAI_API_BASE&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;, &amp;quot;-v&amp;quot;, &amp;quot;docs-mcp-data:/data&amp;quot;, &amp;quot;ghcr.io/arabold/docs-mcp-server:latest&amp;quot; ], &amp;quot;env&amp;quot;: { &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;ollama&amp;quot;, &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://host.docker.internal:1234/v1&amp;quot;, &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;text-embedding-snowflake-arctic-embed-l-v2.0&amp;quot; } } } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;NPX version (needs NodeJS installed)&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;mcpServers&amp;quot;: { &amp;quot;docs-mcp-server&amp;quot;: { &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;, &amp;quot;args&amp;quot;: [ &amp;quot;@arabold/docs-mcp-server@latest&amp;quot; ], &amp;quot;env&amp;quot;: { &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;ollama&amp;quot;, &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://host.docker.internal:1234/v1&amp;quot;, &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;text-embedding-snowflake-arctic-embed-l-v2.0&amp;quot; } } } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Adding documentation for your language&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Ask AI to use the &lt;code&gt;scrape_docs&lt;/code&gt; tool with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;url&lt;/strong&gt; (link to the documentation),&lt;/li&gt; &lt;li&gt;&lt;strong&gt;library&lt;/strong&gt; (name of the documentation/programming language),&lt;/li&gt; &lt;li&gt;&lt;strong&gt;version&lt;/strong&gt; (version of the documentation)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;you can also provide (optional):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;maxPages&lt;/strong&gt; (maximum number of pages to scrape, default is 1000).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;maxDepth&lt;/strong&gt; (maximum navigation depth, default is 3).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;scope&lt;/strong&gt; (crawling boundary, which can be 'subpages', 'hostname', or 'domain', default is 'subpages').&lt;/li&gt; &lt;li&gt;&lt;strong&gt;followRedirects&lt;/strong&gt; (whether to follow HTTP 3xx redirects, default is true).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can ask AI to use &lt;strong&gt;search_docs&lt;/strong&gt; tool any time you want to make sure the syntax or code implementation is correct. It should also check docs automatically if it is smart enough.&lt;/p&gt; &lt;p&gt;This stack isn‚Äôt limited to coding, Devstral handles logical, non-coding tasks well too.&lt;br /&gt; The MCP setup helps reduce hallucinations by grounding the AI in real documentation, making this a flexible and reliable solution for a variety of tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Thanks for reading... If you have used and/or improved on this, I‚Äôd love to hear about it..!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/send_me_a_ticket"&gt; /u/send_me_a_ticket &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-06T16:09:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1lthtbn</id>
    <title>8.5K people voted on which AI models create the best website, games, and visualizations. Both Llama Models came almost dead last. Claude comes up on top.</title>
    <updated>2025-07-07T01:35:48+00:00</updated>
    <author>
      <name>/u/adviceguru25</name>
      <uri>https://old.reddit.com/user/adviceguru25</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lthtbn/85k_people_voted_on_which_ai_models_create_the/"&gt; &lt;img alt="8.5K people voted on which AI models create the best website, games, and visualizations. Both Llama Models came almost dead last. Claude comes up on top." src="https://b.thumbs.redditmedia.com/YH48KR3uSeLFmipFEDX0ai8FZ4TwQmmccEKSaaUx3Fk.jpg" title="8.5K people voted on which AI models create the best website, games, and visualizations. Both Llama Models came almost dead last. Claude comes up on top." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was working on a &lt;a href="https://www.designarena.ai/"&gt;research&lt;/a&gt; project (note that the votes and data is completely free and open, so not profiting off this, but just showing research as context) where users write a prompt, and then vote on content generated (e.g. websites, games, 3D visualizations) from 4 randomly generated models each. Note that when &lt;a href="https://www.designarena.ai/vote"&gt;voting&lt;/a&gt;, model names are hidden, so people don't immediately know which models generated what. &lt;/p&gt; &lt;p&gt;From the data collected so far, Llama 4 Maverick is 19th and Llama 4 Scout is 23rd. On the other extreme, Claude and Deepseek are taking up most of the spots in the top 10 while Mistral and Grok have been surprising dark horses. &lt;/p&gt; &lt;p&gt;Anything surprise you here? What models have you noticed been the best for UI/UX and frontend development? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adviceguru25"&gt; /u/adviceguru25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lthtbn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lthtbn/85k_people_voted_on_which_ai_models_create_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lthtbn/85k_people_voted_on_which_ai_models_create_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T01:35:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1lts4wd</id>
    <title>Inside Google Gemma 3n: my PyTorch Profiler insights</title>
    <updated>2025-07-07T11:51:50+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lts4wd/inside_google_gemma_3n_my_pytorch_profiler/"&gt; &lt;img alt="Inside Google Gemma 3n: my PyTorch Profiler insights" src="https://external-preview.redd.it/iyG6eCUPhSylmQBjhmsazmQyUh0CUb3n-N54OyLJmm0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25644e360804c2abbce6ff1e1b66b61cd330cee0" title="Inside Google Gemma 3n: my PyTorch Profiler insights" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;If you‚Äôve ever wondered what really happens inside modern vision-language models, here‚Äôs a hands-on look. I profiled the Google Gemma 3n model on an NVIDIA GPU using PyTorch Profiler, asking it to describe a &lt;a href="https://cdn-lfs.hf.co/datasets/huggingface/documentation-images/8b21ba78250f852ca5990063866b1ace6432521d0251bde7f8de783b22c99a6d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27bee.jpg%3B+filename%3D%22bee.jpg%22%3B&amp;amp;response-content-type=image%2Fjpeg&amp;amp;Expires=1751892238&amp;amp;Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTg5MjIzOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9kYXRhc2V0cy9odWdnaW5nZmFjZS9kb2N1bWVudGF0aW9uLWltYWdlcy84YjIxYmE3ODI1MGY4NTJjYTU5OTAwNjM4NjZiMWFjZTY0MzI1MjFkMDI1MWJkZTdmOGRlNzgzYjIyYzk5YTZkP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&amp;amp;Signature=FWMAYJoqhsk9AHs1%7EyIoOHBmh53A16J6Xyj-vhFVXTW%7EFkL2tRptgpALUSWppQKXjCnJZsnMXtDFcZAvDm-PFgQaK3UycJD%7ElNShdj5yopPA2F5U2gT4wEvXc-AibMF5mUrzeNKxfY56CjsiFWCfKczLZKzV-kfrXZu7t60d4o5ZdY6jmkdeMHMkYmLROTFE-tmPiKqmN7jVcMIdW43xmaEvova9oA4akIqKphaQUUvvVTToqPjILfn2LLhqwH5BgnbAE5OZ9DtreQirvzS75Xhkgi8GN7LEyrX2nt7LSYtS2vv1SfeSmWca8MY0eO7KEqF71jyA5DquPofRkEEesQ__&amp;amp;Key-Pair-Id=K3RPWS32NSSJCE"&gt;bee image&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I visualized the profiling results using &lt;a href="https://ui.perfetto.dev/"&gt;https://ui.perfetto.dev/&lt;/a&gt;, as shown in the animated GIF below:&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/frlijkwkwfbf1.gif"&gt;https://i.redd.it/frlijkwkwfbf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Along the way, I captured and analyzed the key inference phases, including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Image feature extraction&lt;/strong&gt; with MobileNetV5 (74 msec) - the trace shows the &lt;code&gt;get_image_features&lt;/code&gt; function of Gemma3n (&lt;a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma3n/modular_gemma3n.py#L2253"&gt;source&lt;/a&gt;), which then calls &lt;code&gt;forward_features&lt;/code&gt; in MobileNetV5 (&lt;a href="https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/mobilenetv5.py#L535"&gt;source&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/afzke1tdxfbf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=899a055b776818546205514b3d9e29fe7dee38cd"&gt;https://preview.redd.it/afzke1tdxfbf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=899a055b776818546205514b3d9e29fe7dee38cd&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Text decoding&lt;/strong&gt; through a stack of Gemma3nTextDecoderLayer layers (142 msec) - a series of &lt;code&gt;Gemma3nTextDecoderLayer&lt;/code&gt; (&lt;a href="https://github.com/huggingface/transformers/blob/ca7e1a3756c022bf31429c452b2f313f043f32de/src/transformers/models/gemma3n/modular_gemma3n.py#L1829"&gt;source&lt;/a&gt;) calls. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6hlcdthfxfbf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=833ae582e5eb759a1eba9adbca1841deeba07195"&gt;https://preview.redd.it/6hlcdthfxfbf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=833ae582e5eb759a1eba9adbca1841deeba07195&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Token generation&lt;/strong&gt; with per-token execution broken down to kernel launches and synchronizations (244 msec total for 10 tokens, ~24 msec per token) &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xzoilykgxfbf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16f504610e8821d686d63aa83e255a4feb8dfd60"&gt;https://preview.redd.it/xzoilykgxfbf1.png?width=2880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16f504610e8821d686d63aa83e255a4feb8dfd60&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôve shared the full code, profiling scripts, and raw trace data, so you can dive in, reproduce the results, and explore the model‚Äôs internals for yourself.&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="https://github.com/sbnb-io/gemma3n-profiling/"&gt;https://github.com/sbnb-io/gemma3n-profiling/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you‚Äôre looking to better understand how these models run under the hood, this is a solid place to start. Happy to hear your thoughts or suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lts4wd/inside_google_gemma_3n_my_pytorch_profiler/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lts4wd/inside_google_gemma_3n_my_pytorch_profiler/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lts4wd/inside_google_gemma_3n_my_pytorch_profiler/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T11:51:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltubvs</id>
    <title>Jamba 1.7 - a ai21labs Collection</title>
    <updated>2025-07-07T13:35:12+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltubvs/jamba_17_a_ai21labs_collection/"&gt; &lt;img alt="Jamba 1.7 - a ai21labs Collection" src="https://external-preview.redd.it/T-WGV8JGl5ddvynFCnHkV0GApDuiD0OUmPGVN858nB8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35b8dfb877220fc5dfb06c711e11e9b9d474f083" title="Jamba 1.7 - a ai21labs Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ai21labs/jamba-17-68653e9be386dc69b1f30828"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltubvs/jamba_17_a_ai21labs_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltubvs/jamba_17_a_ai21labs_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T13:35:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ltxsqh</id>
    <title>Qwen3-8B-BitNet</title>
    <updated>2025-07-07T15:53:44+00:00</updated>
    <author>
      <name>/u/codys12</name>
      <uri>https://old.reddit.com/user/codys12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a decent Qwen3 BitNet model I trained with ~1B tokens using SYNTHETIC-1 data. BitNet Hunyuan A13B is training this week.&lt;br /&gt; &lt;a href="https://huggingface.co/codys12/Qwen3-8B-BitNet"&gt;model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://colab.research.google.com/drive/1GT0GEyjzOQUiOI0tphvhiFDwUw-F6v7l?usp=sharing"&gt;notebook&lt;/a&gt; to try out the model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codys12"&gt; /u/codys12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxsqh/qwen38bbitnet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxsqh/qwen38bbitnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ltxsqh/qwen38bbitnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-07T15:53:44+00:00</published>
  </entry>
</feed>
