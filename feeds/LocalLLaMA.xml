<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-19T02:15:54+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nka2cl</id>
    <title>Am I the first one to run a full multi-agent workflow on an edge device?</title>
    <updated>2025-09-18T14:38:09+00:00</updated>
    <author>
      <name>/u/Abit_Anonymous</name>
      <uri>https://old.reddit.com/user/Abit_Anonymous</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nka2cl/am_i_the_first_one_to_run_a_full_multiagent/"&gt; &lt;img alt="Am I the first one to run a full multi-agent workflow on an edge device?" src="https://external-preview.redd.it/cnpxNWU2MmRueHBmMYp0QtsqZN4801amkhsrr7Zpcr6-upwl98kHrYEp8oGI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43b8d55e77ae6ba3f231b599f323a6bef27f6908" title="Am I the first one to run a full multi-agent workflow on an edge device?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I‚Äôve been messing with Jetson boards for quiet a while, but this was my first time trying to push a real multi-agent stack onto one. Instead of cloud or desktop, I wanted to see if I could get a Multi Agent AI Workflow to run end-to-end on a Jetson Orin Nano 8GB.&lt;/p&gt; &lt;p&gt;The goal: talk to the device, have it generate a PowerPoint, all locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt; ‚Ä¢ Jetson Orin Nano 8GB ‚Ä¢ CAMEL-AI framework for agent orchestration ‚Ä¢ Whisper for STT ‚Ä¢ CAMEL PPTXToolkit for slide generation ‚Ä¢ Models tested: Mistral 7B Q4, Llama 3.1 8B Q4, Qwen 2.5 7B Q4&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What actually happened&lt;/strong&gt; ‚Ä¢ Whisper crushed it. 95%+ accuracy even with noise. ‚Ä¢ CAMEL‚Äôs agent split made sense. One agent handled chat, another handled slide creation. Felt natural, no duct tape. ‚Ä¢ Jetson held up way better than I expected. 7B inference + Whisper at the same time on 8GB is wild. ‚Ä¢ The slides? Actually useful, not just generic bullets.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What broke my flow (Learnings for future too.)&lt;/strong&gt; ‚Ä¢ TTS was slooow. 15‚Äì25s per reply ‚Ä¢ Totally ruins the convo feel. ‚Ä¢ Mistral kept breaking function calls with bad JSON. ‚Ä¢ Llama 3.1 was too chunky for 8GB, constant OOM. ‚Ä¢ Qwen 2.5 7B ended up being the sweet spot.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Takeaways&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Model fit &amp;gt; model hype.&lt;/li&gt; &lt;li&gt;TTS on edge is the real bottleneck.&lt;/li&gt; &lt;li&gt;8GB is just enough, but you‚Äôre cutting it close.&lt;/li&gt; &lt;li&gt;Edge optimization is very different from cloud.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So yeah, it worked. Multi-agent on edge is possible. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full pipeline&lt;/strong&gt; Whisper ‚Üí CAMEL agents ‚Üí PPTXToolkit ‚Üí TTS. &lt;/p&gt; &lt;p&gt;Curious if anyone else here has tried running Agentic Workflows or any other multi-agent frameworks on edge hardware? Or am I actually the first to get this running?‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Abit_Anonymous"&gt; /u/Abit_Anonymous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/af1mqlrenxpf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nka2cl/am_i_the_first_one_to_run_a_full_multiagent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nka2cl/am_i_the_first_one_to_run_a_full_multiagent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T14:38:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkcb7n</id>
    <title>I open-sourced a text2SQL RAG for all your databases and local models</title>
    <updated>2025-09-18T16:02:36+00:00</updated>
    <author>
      <name>/u/Durovilla</name>
      <uri>https://old.reddit.com/user/Durovilla</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkcb7n/i_opensourced_a_text2sql_rag_for_all_your/"&gt; &lt;img alt="I open-sourced a text2SQL RAG for all your databases and local models" src="https://preview.redd.it/kolx2nwb3ypf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a50db6350de24146879cc0b1d205373350ff3287" title="I open-sourced a text2SQL RAG for all your databases and local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey r/LocalLLama üëã&lt;/p&gt; &lt;p&gt;I‚Äôve spent most of my career working with databases, and one thing that‚Äôs always bugged me is how hard it is for AI agents to work with them. Whenever I ask Claude, GPT, or Llama about my data, it either invents schemas or hallucinates details. To fix that, I built &lt;a href="https://docs.toolfront.ai/"&gt;ToolFront&lt;/a&gt;. It's a free and open-source Python library for creating lightweight but powerful retrieval agents, giving them a safe, smart way to actually understand and query your database schemas.&lt;/p&gt; &lt;h1&gt;So, how does it work?&lt;/h1&gt; &lt;p&gt;ToolFront gives your custom/local models two read-only database tools so they can explore your data and quickly find answers. You can also add business context to help the AI better understand your databases. Check out our &lt;a href="https://docs.toolfront.ai/documentation/ai_models/#custom-model-providers"&gt;model documentation page&lt;/a&gt; for more info on how to use your own models.&lt;/p&gt; &lt;h1&gt;Connects to everything&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;15+ databases and warehouses, including: Snowflake, BigQuery, PostgreSQL &amp;amp; more!&lt;/li&gt; &lt;li&gt;Data files like CSVs, Parquets, JSONs, and even Excel files.&lt;/li&gt; &lt;li&gt;Any API with an OpenAPI/Swagger spec (e.g. GitHub, Stripe, Discord, and even internal APIs)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why you'll love it&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Zero configuration:&lt;/strong&gt; Skip config files and infrastructure setup. ToolFront works out of the box with all your data and models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Predictable results&lt;/strong&gt;: Data is messy. ToolFront returns structured, type-safe responses that match exactly what you want e.g. &lt;ul&gt; &lt;li&gt;&lt;code&gt;answer: list[int] = db.ask(...)&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Use it anywhere&lt;/strong&gt;: Avoid migrations. Run ToolFront directly, as an MCP server, or build custom tools for your favorite AI framework.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you‚Äôre building AI agents for databases (or APIs!), I really think ToolFront could make your life easier. Your feedback last time was incredibly helpful for improving the project. Please keep it coming!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Docs:&lt;/strong&gt; &lt;a href="https://docs.toolfront.ai/"&gt;https://docs.toolfront.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo&lt;/strong&gt;: &lt;a href="https://github.com/kruskal-labs/toolfront"&gt;https://github.com/kruskal-labs/toolfront&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Discord:&lt;/strong&gt; &lt;a href="https://discord.com/invite/rRyM7zkZTf"&gt;https://discord.com/invite/rRyM7zkZTf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A ‚≠ê on GitHub really helps with visibility!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Durovilla"&gt; /u/Durovilla &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kolx2nwb3ypf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkcb7n/i_opensourced_a_text2sql_rag_for_all_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkcb7n/i_opensourced_a_text2sql_rag_for_all_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T16:02:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkqyyi</id>
    <title>How can I get an LLM to talk with the humor/style of transcripts?</title>
    <updated>2025-09-19T01:57:12+00:00</updated>
    <author>
      <name>/u/mohalibou</name>
      <uri>https://old.reddit.com/user/mohalibou</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am still relatively new to all this, so go easy on me with the replies, but there's been something that I've been thinking about for a while.&lt;/p&gt; &lt;p&gt;Let's say I saved multiple transcripts in the txt file format. Would I be able to use those transcripts as a dataset to finetune an LLM?&lt;/p&gt; &lt;p&gt;I am essentially trying to recreate the rhetoric, speaking style, and vocabulary that is being used in those transcripts. &lt;/p&gt; &lt;p&gt;So far, I‚Äôve tried prompting ChatGPT while feeding it several transcripts for context, but it never really nails down the style in the same manner. &lt;/p&gt; &lt;p&gt;At this point, I‚Äôm starting to think that my best bet would be to resort to finetuning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mohalibou"&gt; /u/mohalibou &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkqyyi/how_can_i_get_an_llm_to_talk_with_the_humorstyle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkqyyi/how_can_i_get_an_llm_to_talk_with_the_humorstyle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkqyyi/how_can_i_get_an_llm_to_talk_with_the_humorstyle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T01:57:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1njqt5s</id>
    <title>once China is able to produce its own GPU for datacenters (which they are forced to due to both import and export bans by both China and USA), there will be less reason to release their models open weight?</title>
    <updated>2025-09-17T22:07:52+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njqt5s/once_china_is_able_to_produce_its_own_gpu_for/"&gt; &lt;img alt="once China is able to produce its own GPU for datacenters (which they are forced to due to both import and export bans by both China and USA), there will be less reason to release their models open weight?" src="https://preview.redd.it/s4cols18tspf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ff443b6756ab190d26356690b7694db6efda4f6" title="once China is able to produce its own GPU for datacenters (which they are forced to due to both import and export bans by both China and USA), there will be less reason to release their models open weight?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s4cols18tspf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njqt5s/once_china_is_able_to_produce_its_own_gpu_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njqt5s/once_china_is_able_to_produce_its_own_gpu_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T22:07:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk1jbc</id>
    <title>I just made VRAM approximation tool for LLM</title>
    <updated>2025-09-18T07:09:12+00:00</updated>
    <author>
      <name>/u/SmilingGen</name>
      <uri>https://old.reddit.com/user/SmilingGen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a simple tool to estimate how much memory is needed to run GGUF models locally, based on your desired maximum context size.&lt;/p&gt; &lt;p&gt;You just paste the direct download URL of a GGUF model (for example, from Hugging Face), enter the context length you plan to use, and it will give you an approximate memory requirement.&lt;/p&gt; &lt;p&gt;It‚Äôs especially useful if you're trying to figure out whether a model will fit in your available VRAM or RAM, or when comparing different quantization levels like Q4_K_M vs Q8_0.&lt;/p&gt; &lt;p&gt;The tool is completely free and open-source. You can try it here: &lt;a href="https://www.kolosal.ai/memory-calculator"&gt;https://www.kolosal.ai/memory-calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And check out the code on GitHub: &lt;a href="https://github.com/KolosalAI/model-memory-calculator"&gt;https://github.com/KolosalAI/model-memory-calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd really appreciate any feedback, suggestions, or bug reports if you decide to give it a try.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SmilingGen"&gt; /u/SmilingGen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk1jbc/i_just_made_vram_approximation_tool_for_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk1jbc/i_just_made_vram_approximation_tool_for_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nk1jbc/i_just_made_vram_approximation_tool_for_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T07:09:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkigg6</id>
    <title>New stealth model Golden Capybara?</title>
    <updated>2025-09-18T19:52:29+00:00</updated>
    <author>
      <name>/u/Adept_Photograph_796</name>
      <uri>https://old.reddit.com/user/Adept_Photograph_796</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkigg6/new_stealth_model_golden_capybara/"&gt; &lt;img alt="New stealth model Golden Capybara?" src="https://b.thumbs.redditmedia.com/iMYu_qRqMZ8AGfGw81KMO1yjddk0Rr7udWRjyvqT90k.jpg" title="New stealth model Golden Capybara?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/2yjkolxa9zpf1.png?width=2806&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bea303a1394a6c099d644ecb55f33b9e30da15aa"&gt;https://preview.redd.it/2yjkolxa9zpf1.png?width=2806&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bea303a1394a6c099d644ecb55f33b9e30da15aa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Golden Capybara has been popping up in a lot of tournaments but I can't find anything about it online... thinking it's another stealth model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adept_Photograph_796"&gt; /u/Adept_Photograph_796 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkigg6/new_stealth_model_golden_capybara/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkigg6/new_stealth_model_golden_capybara/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkigg6/new_stealth_model_golden_capybara/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T19:52:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk5df9</id>
    <title>Ryzen 6800H iGPU 680M Vulkan benchmarks llama.cpp</title>
    <updated>2025-09-18T11:11:32+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I continue to be impressed on how well iGPU perform. Here are some updated LLM benchmarks. &lt;/p&gt; &lt;p&gt;Llama.cpp with Vulkan for Ubuntu is running pretty fast especially when you throw a &lt;a href="https://www.tensorops.ai/post/what-is-mixture-of-experts-llm"&gt;MoE&lt;/a&gt; model at it.&lt;/p&gt; &lt;p&gt;AMD Ryzen 7 &lt;a href="https://www.techpowerup.com/cpu-specs/ryzen-7-6800h.c2527"&gt;6800H&lt;/a&gt; CPU with Radeon Graphics &lt;a href="https://www.techpowerup.com/gpu-specs/radeon-680m.c3871"&gt;680M&lt;/a&gt; with 64GB &lt;a href="https://en.wikipedia.org/wiki/DDR5_SDRAM"&gt;DDR5&lt;/a&gt; 4800 system RAM and &lt;a href="https://www.reddit.com/r/ollama/comments/1lbpqln/minipc_ryzen_7_6800h_cpu_and_igpu_680m/"&gt;16GB for iGPU&lt;/a&gt;. System running &lt;a href="https://kubuntu.org/"&gt;Kubuntu&lt;/a&gt; 25.10 and &lt;a href="https://docs.mesa3d.org/relnotes/25.1.7.html"&gt;Mesa 25.1.7&lt;/a&gt;-1ubuntu1.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;Release&lt;/a&gt; llama.cpp &lt;a href="https://github.com/ggml-org/llama.cpp/releases/download/b6501/llama-b6478-bin-ubuntu-vulkan-x64.zip"&gt;Vulkan&lt;/a&gt; build: 28c39da7 (6478)&lt;/p&gt; &lt;p&gt;Using &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/7195"&gt;llama-bench&lt;/a&gt; sorted by &lt;a href="https://web.dev/articles/llm-sizes"&gt;Parameter size&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size GiB&lt;/th&gt; &lt;th align="left"&gt;Params B&lt;/th&gt; &lt;th align="left"&gt;pp512 t/s&lt;/th&gt; &lt;th align="left"&gt;tg128 t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Phi-3.5-MoE-instruct-IQ4_NL.gguf&lt;/td&gt; &lt;td align="left"&gt;21.99&lt;/td&gt; &lt;td align="left"&gt;41.87&lt;/td&gt; &lt;td align="left"&gt;95.58&lt;/td&gt; &lt;td align="left"&gt;16.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EXAONE-4.0-32B-Q4_K_M.gguf&lt;/td&gt; &lt;td align="left"&gt;18.01&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;30.4&lt;/td&gt; &lt;td align="left"&gt;2.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Coder-30B-A3B-Instruct-IQ4_NL.gguf&lt;/td&gt; &lt;td align="left"&gt;16.12&lt;/td&gt; &lt;td align="left"&gt;30.53&lt;/td&gt; &lt;td align="left"&gt;150.73&lt;/td&gt; &lt;td align="left"&gt;30.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Coder-30B-A3B-Instruct-IQ4_XS.gguf&lt;/td&gt; &lt;td align="left"&gt;15.25&lt;/td&gt; &lt;td align="left"&gt;30.53&lt;/td&gt; &lt;td align="left"&gt;140.24&lt;/td&gt; &lt;td align="left"&gt;28.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Coder-30B-A3B-Instruct-UD-Q5_K_XL.gguf&lt;/td&gt; &lt;td align="left"&gt;20.24&lt;/td&gt; &lt;td align="left"&gt;30.53&lt;/td&gt; &lt;td align="left"&gt;120.68&lt;/td&gt; &lt;td align="left"&gt;25.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q4_k_m.gguf&lt;/td&gt; &lt;td align="left"&gt;13.65&lt;/td&gt; &lt;td align="left"&gt;24.15&lt;/td&gt; &lt;td align="left"&gt;35.81&lt;/td&gt; &lt;td align="left"&gt;4.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ERNIE-4.5-21B-A3B-PT.i1-IQ4_XS.gguf&lt;/td&gt; &lt;td align="left"&gt;10.89&lt;/td&gt; &lt;td align="left"&gt;21.83&lt;/td&gt; &lt;td align="left"&gt;176.99&lt;/td&gt; &lt;td align="left"&gt;30.29&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ERNIE-4.5-21B-A3B-PT-IQ4_NL.gguf&lt;/td&gt; &lt;td align="left"&gt;11.52&lt;/td&gt; &lt;td align="left"&gt;21.83&lt;/td&gt; &lt;td align="left"&gt;196.39&lt;/td&gt; &lt;td align="left"&gt;29.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SmallThinker-21B-A3B-Instruct.IQ4_XS.imatrix.gguf&lt;/td&gt; &lt;td align="left"&gt;10.78&lt;/td&gt; &lt;td align="left"&gt;21.51&lt;/td&gt; &lt;td align="left"&gt;155.94&lt;/td&gt; &lt;td align="left"&gt;26.12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EuroLLM-9B-Instruct-IQ4_XS.gguf&lt;/td&gt; &lt;td align="left"&gt;4.7&lt;/td&gt; &lt;td align="left"&gt;9.15&lt;/td&gt; &lt;td align="left"&gt;116.78&lt;/td&gt; &lt;td align="left"&gt;12.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EuroLLM-9B-Instruct-Q4_K_M.gguf&lt;/td&gt; &lt;td align="left"&gt;5.2&lt;/td&gt; &lt;td align="left"&gt;9.15&lt;/td&gt; &lt;td align="left"&gt;113.45&lt;/td&gt; &lt;td align="left"&gt;12.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EuroLLM-9B-Instruct-Q6_K_L.gguf&lt;/td&gt; &lt;td align="left"&gt;7.23&lt;/td&gt; &lt;td align="left"&gt;9.15&lt;/td&gt; &lt;td align="left"&gt;110.87&lt;/td&gt; &lt;td align="left"&gt;9.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1-0528-Qwen3-8B-IQ4_XS.gguf&lt;/td&gt; &lt;td align="left"&gt;4.26&lt;/td&gt; &lt;td align="left"&gt;8.19&lt;/td&gt; &lt;td align="left"&gt;136.77&lt;/td&gt; &lt;td align="left"&gt;14.58&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Phi-mini-MoE-instruct-IQ2_XS.gguf&lt;/td&gt; &lt;td align="left"&gt;2.67&lt;/td&gt; &lt;td align="left"&gt;7.65&lt;/td&gt; &lt;td align="left"&gt;347.45&lt;/td&gt; &lt;td align="left"&gt;61.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Phi-mini-MoE-instruct-Q4_K_M.gguf&lt;/td&gt; &lt;td align="left"&gt;4.65&lt;/td&gt; &lt;td align="left"&gt;7.65&lt;/td&gt; &lt;td align="left"&gt;294.85&lt;/td&gt; &lt;td align="left"&gt;40.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5-7B-Instruct.Q8_0.gguf&lt;/td&gt; &lt;td align="left"&gt;7.54&lt;/td&gt; &lt;td align="left"&gt;7.62&lt;/td&gt; &lt;td align="left"&gt;256.57&lt;/td&gt; &lt;td align="left"&gt;8.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama-2-7b.Q4_0.gguf&lt;/td&gt; &lt;td align="left"&gt;3.56&lt;/td&gt; &lt;td align="left"&gt;6.74&lt;/td&gt; &lt;td align="left"&gt;279.81&lt;/td&gt; &lt;td align="left"&gt;16.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Phi-4-mini-instruct-Q4_K_M.gguf&lt;/td&gt; &lt;td align="left"&gt;2.31&lt;/td&gt; &lt;td align="left"&gt;3.84&lt;/td&gt; &lt;td align="left"&gt;275.75&lt;/td&gt; &lt;td align="left"&gt;25.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-3.1-3b-a800m-instruct_f16.gguf&lt;/td&gt; &lt;td align="left"&gt;6.15&lt;/td&gt; &lt;td align="left"&gt;3.3&lt;/td&gt; &lt;td align="left"&gt;654.88&lt;/td&gt; &lt;td align="left"&gt;34.39&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk5df9/ryzen_6800h_igpu_680m_vulkan_benchmarks_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk5df9/ryzen_6800h_igpu_680m_vulkan_benchmarks_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nk5df9/ryzen_6800h_igpu_680m_vulkan_benchmarks_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T11:11:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nklnqi</id>
    <title>Local real-time assistant that remembers convo + drafts a doc</title>
    <updated>2025-09-18T21:56:53+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nklnqi/local_realtime_assistant_that_remembers_convo/"&gt; &lt;img alt="Local real-time assistant that remembers convo + drafts a doc" src="https://external-preview.redd.it/cDVmYTE2a2l0enBmMX1sjDttlx_mQ45DeDKL-DubvjOdCDAQ-7LC5FNwPoBD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=885c1074982440415a834257790222ab70d2d1f5" title="Local real-time assistant that remembers convo + drafts a doc" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wired up a local ‚Äúbrainstorming assistant‚Äù that keeps memory of our chat and then writes a Google doc based on what we talked about.&lt;/p&gt; &lt;p&gt;Demo was simple:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Talked with it about cats.&lt;/li&gt; &lt;li&gt;Asked it to generate a doc with what we discussed.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Results: it dropped a few details, but it captured the main points surprisingly well. Not bad for a first pass. Next step is wiring it up with an MCP so the doc gets written continuously while we talk instead of at the end.&lt;/p&gt; &lt;p&gt;Excited to test this on a longer conversation. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/g7qj66kitzpf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nklnqi/local_realtime_assistant_that_remembers_convo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nklnqi/local_realtime_assistant_that_remembers_convo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T21:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkb0yq</id>
    <title>Latest Open-Source AMD Improvements Allowing For Better Llama.cpp AI Performance Against Windows 11</title>
    <updated>2025-09-18T15:14:48+00:00</updated>
    <author>
      <name>/u/NewtMurky</name>
      <uri>https://old.reddit.com/user/NewtMurky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I was checking out the recent llama.cpp benchmarks and the data in this link shows that llama.cpp runs significantly faster on Windows 11 (25H2) than on Ubuntu for AMD GPUs. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NewtMurky"&gt; /u/NewtMurky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/review/llama-cpp-windows-linux/3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkb0yq/latest_opensource_amd_improvements_allowing_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkb0yq/latest_opensource_amd_improvements_allowing_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T15:14:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nknkyr</id>
    <title>[Research] Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens</title>
    <updated>2025-09-18T23:19:10+00:00</updated>
    <author>
      <name>/u/Confident-Honeydew66</name>
      <uri>https://old.reddit.com/user/Confident-Honeydew66</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I thought this would be relevant for us here in local llama, since reasoning models are coming into fashion for local inference, with the new GPT OSS models and friends (and that reflexion fiasco; for those that remember)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Confident-Honeydew66"&gt; /u/Confident-Honeydew66 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2508.01191"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nknkyr/research_is_chainofthought_reasoning_of_llms_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nknkyr/research_is_chainofthought_reasoning_of_llms_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T23:19:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk87rk</id>
    <title>Qwen3-next-80b-a3b hits 1400 elo (also longcat-flash)</title>
    <updated>2025-09-18T13:24:45+00:00</updated>
    <author>
      <name>/u/GabryIta</name>
      <uri>https://old.reddit.com/user/GabryIta</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk87rk/qwen3next80ba3b_hits_1400_elo_also_longcatflash/"&gt; &lt;img alt="Qwen3-next-80b-a3b hits 1400 elo (also longcat-flash)" src="https://a.thumbs.redditmedia.com/nVKYZwK_64TwYIgHazAhHI7N38wg6aI4LwIqXOVqdq4.jpg" title="Qwen3-next-80b-a3b hits 1400 elo (also longcat-flash)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/48ne8xrrcxpf1.png?width=2244&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d1e166212d7f9a2f5361e5a8ee35fead714d1a7a"&gt;https://preview.redd.it/48ne8xrrcxpf1.png?width=2244&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d1e166212d7f9a2f5361e5a8ee35fead714d1a7a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I just noticed the Lmarena leaderboard has been updated, even though there‚Äôs been no announcement on social media. (lately they only post updates for major models. kind of a shame)&lt;/p&gt; &lt;p&gt;The new Qwen3-next-80b-a3b reaches 1400 ELO with just 3B active parameters&lt;br /&gt; According to the benchmark, its performance is on par with qwen3-235b-a22b and qwen3-235b-a22b-thinking-2507&lt;/p&gt; &lt;p&gt;Anyone tried it yet? Is it actually that good in real-world use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GabryIta"&gt; /u/GabryIta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk87rk/qwen3next80ba3b_hits_1400_elo_also_longcatflash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk87rk/qwen3next80ba3b_hits_1400_elo_also_longcatflash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nk87rk/qwen3next80ba3b_hits_1400_elo_also_longcatflash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T13:24:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkpohe</id>
    <title>I can can get GPUs as a tax write off. Thinking of doubling down on my LLM/ML learning adventure by buying one or two RTX 6000 pros.</title>
    <updated>2025-09-19T00:56:21+00:00</updated>
    <author>
      <name>/u/Tired__Dev</name>
      <uri>https://old.reddit.com/user/Tired__Dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was having a lot of fun a few months back learning graph/vector based RAG. Then work unloaded a ridiculous level of work. I started by trying to use my ASUS M16 with a 4090 for local 3b models. It didn't work as I hoped. Now I'll probably sell the thing to build a local desktop rig that I can remotely use across the world (original reason I got the M16). &lt;/p&gt; &lt;p&gt;Reason I want it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Over the last two years I've taken it upon myself to start future proofing my career. I've learn IoT, game development, and now mostly LLMs. I want to also learn how to do things like object detection. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It's a tax write off.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If I'm jobless I don't have to pay cloud costs and I have something I can liquidate if need be. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It would expand what I could do startup wise. &lt;strong&gt;(Most important reason)&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So my question is, what's the limit of one or two RTX 6000 Pro Blackwells? Would I be able to essentially do any RAG, Object detection, or ML like start up? What type of accuracy could I hope to accomplish with a good RAG pipeline and the open source models that'd be able to run on one or two of these GPUs? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tired__Dev"&gt; /u/Tired__Dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkpohe/i_can_can_get_gpus_as_a_tax_write_off_thinking_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkpohe/i_can_can_get_gpus_as_a_tax_write_off_thinking_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkpohe/i_can_can_get_gpus_as_a_tax_write_off_thinking_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T00:56:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkgh1u</id>
    <title>RX 7700 launched with 2560 cores (relatively few) and 16GB memory with 624 GB/s bandwidth (relatively high)</title>
    <updated>2025-09-18T18:37:07+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkgh1u/rx_7700_launched_with_2560_cores_relatively_few/"&gt; &lt;img alt="RX 7700 launched with 2560 cores (relatively few) and 16GB memory with 624 GB/s bandwidth (relatively high)" src="https://external-preview.redd.it/ZtmeYdNQChVt_XKulmUOe_WPIvyPIS1JhHlC4A0Lp38.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb90e99fc8917d6295abd4272b2623d002b47934" title="RX 7700 launched with 2560 cores (relatively few) and 16GB memory with 624 GB/s bandwidth (relatively high)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This seems like an LLM GPU. Lot‚Äôs of bandwidth compared to compute.&lt;/p&gt; &lt;p&gt;See &lt;a href="https://www.amd.com/en/products/graphics/desktops/radeon/7000-series/amd-radeon-rx-7700.html"&gt;https://www.amd.com/en/products/graphics/desktops/radeon/7000-series/amd-radeon-rx-7700.html&lt;/a&gt; for the full specs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/amd-launches-radeon-rx-7700-with-2560-cores-and-16gb-memory"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkgh1u/rx_7700_launched_with_2560_cores_relatively_few/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkgh1u/rx_7700_launched_with_2560_cores_relatively_few/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T18:37:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkdnf2</id>
    <title>A dialogue where god tries (and fails) to prove to satan that humans can reason</title>
    <updated>2025-09-18T16:52:06+00:00</updated>
    <author>
      <name>/u/FinnFarrow</name>
      <uri>https://old.reddit.com/user/FinnFarrow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkdnf2/a_dialogue_where_god_tries_and_fails_to_prove_to/"&gt; &lt;img alt="A dialogue where god tries (and fails) to prove to satan that humans can reason" src="https://preview.redd.it/fqm6nmw8dypf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d353499d3cd5e9c56a4af4ceac147c01524c7fe1" title="A dialogue where god tries (and fails) to prove to satan that humans can reason" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.astralcodexten.com/p/what-is-man-that-thou-art-mindful"&gt;Full article here&lt;/a&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FinnFarrow"&gt; /u/FinnFarrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fqm6nmw8dypf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkdnf2/a_dialogue_where_god_tries_and_fails_to_prove_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkdnf2/a_dialogue_where_god_tries_and_fails_to_prove_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T16:52:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkcbwp</id>
    <title>GLM 4.5 Air - Jinja Template Modification (Based on Unsloth's) - No thinking by default - straight quick answers, need thinking? simple activation with "/think" command anywhere in the system prompt.</title>
    <updated>2025-09-18T16:03:16+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkcbwp/glm_45_air_jinja_template_modification_based_on/"&gt; &lt;img alt="GLM 4.5 Air - Jinja Template Modification (Based on Unsloth's) - No thinking by default - straight quick answers, need thinking? simple activation with &amp;quot;/think&amp;quot; command anywhere in the system prompt." src="https://b.thumbs.redditmedia.com/-CO-B8aIjArUbhh-kSj2q3zmOt03NEOLl8XW32plQVQ.jpg" title="GLM 4.5 Air - Jinja Template Modification (Based on Unsloth's) - No thinking by default - straight quick answers, need thinking? simple activation with &amp;quot;/think&amp;quot; command anywhere in the system prompt." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nkcbwp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkcbwp/glm_45_air_jinja_template_modification_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkcbwp/glm_45_air_jinja_template_modification_based_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T16:03:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkh88k</id>
    <title>Can you guess what model you're talking to in 5 prompts?</title>
    <updated>2025-09-18T19:05:38+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkh88k/can_you_guess_what_model_youre_talking_to_in_5/"&gt; &lt;img alt="Can you guess what model you're talking to in 5 prompts?" src="https://external-preview.redd.it/NTh4aG80cW8xenBmMfQ6ULqGkcZNtZeiwHOodBaY1uWCovO-Ocod72xeRKh_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb846ec47ccac331a0bed323c431ac8221cc38d8" title="Can you guess what model you're talking to in 5 prompts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a &lt;a href="https://whichllama.com"&gt;web version&lt;/a&gt; of the WhichLlama? bot in our Discord server (you should &lt;a href="https://discord.gg/bNQP7DcQ"&gt;join&lt;/a&gt;!) to share here. I think my own &amp;quot;LLM palate&amp;quot; isn't refined enough to tell models apart (drawing an analogy to coffee and wine tasting).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y7dajeso1zpf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkh88k/can_you_guess_what_model_youre_talking_to_in_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkh88k/can_you_guess_what_model_youre_talking_to_in_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T19:05:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk706v</id>
    <title>Qwen Next is my new go to model</title>
    <updated>2025-09-18T12:32:30+00:00</updated>
    <author>
      <name>/u/Miserable-Dare5090</name>
      <uri>https://old.reddit.com/user/Miserable-Dare5090</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is blazing fast, made 25 back to back tool calls with no errors, both as mxfp4 and qx86hi quants. I had been unable to test until now, and previously OSS-120B had become my main model due to speed/tool calling efficiency. Qwen delivered! &lt;/p&gt; &lt;p&gt;Have not tested coding, or RP (I am not interested in RP, my use is as a true assistant, running tasks). what are the issues that people have found? i prefer it to Qwen 235 which I can run at 6 bits atm. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Miserable-Dare5090"&gt; /u/Miserable-Dare5090 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk706v/qwen_next_is_my_new_go_to_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk706v/qwen_next_is_my_new_go_to_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nk706v/qwen_next_is_my_new_go_to_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T12:32:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nknpgd</id>
    <title>System prompt to make a model help users guess its name?</title>
    <updated>2025-09-18T23:24:54+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nknpgd/system_prompt_to_make_a_model_help_users_guess/"&gt; &lt;img alt="System prompt to make a model help users guess its name?" src="https://preview.redd.it/shq50qtyb0qf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dcdc6bd12cac219bcc5a1cf9ba84a06b87fe4e3d" title="System prompt to make a model help users guess its name?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm working on this bot (you can find it in the &lt;a href="/r/LocalLLaMa"&gt;/r/LocalLLaMa&lt;/a&gt; Discord server) that plays a game asking users to guess which model it is. My system prompt asks the model to switch to riddles if the user directly asks for its identity, because that‚Äôs how some users may choose to play the game. But what I‚Äôm finding is that the riddles are often useless because the model doesn‚Äôt know its own identity (or it is intentionally lying).&lt;/p&gt; &lt;p&gt;&lt;em&gt;Note: I know asking directly for identity is a bad strategy, I just want to make it less bad for users who try it!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Case in point, Mistral designing an elaborate riddle about itself being made by Google: &lt;a href="https://whichllama.com/?share=SMJXbCovucr8AVqy"&gt;https://whichllama.com/?share=SMJXbCovucr8AVqy&lt;/a&gt; (why?!)&lt;/p&gt; &lt;p&gt;Now, I can plug the true model name into the system prompt myself, but that is either ignored by the model or used in a way that makes it too easy to guess. Any tips on how I can design the system prompt to balance between being too easy and difficult?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/shq50qtyb0qf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nknpgd/system_prompt_to_make_a_model_help_users_guess/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nknpgd/system_prompt_to_make_a_model_help_users_guess/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T23:24:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkfvrl</id>
    <title>Local LLM Coding Stack (24GB minimum, ideal 36GB)</title>
    <updated>2025-09-18T18:14:56+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkfvrl/local_llm_coding_stack_24gb_minimum_ideal_36gb/"&gt; &lt;img alt="Local LLM Coding Stack (24GB minimum, ideal 36GB)" src="https://preview.redd.it/ia5muohupypf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1148b1bf986af2a5825964a50e7d6bdf8dc5dc16" title="Local LLM Coding Stack (24GB minimum, ideal 36GB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perhaps this could be useful to someone trying to get his/her own local AI coding stack. I do scientific coding stuff, not web or application development related stuff, so the needs might be different. &lt;/p&gt; &lt;p&gt;Deployed on a 48gb Mac, but this should work on 32GB, and maybe even 24GB setups:&lt;/p&gt; &lt;p&gt;General Tasks, used 90% of the time: Cline on top of Qwen3Coder-30b-a3b. Served by LM Studio in MLX format for maximum speed. This is the backbone of everything else...&lt;/p&gt; &lt;p&gt;Difficult single script tasks, 5% of the time: QwenCode on top of GPT-OSS 20b (Reasoning effort: High). Served by LM Studio. This cannot be served at the same time of Qwen3Coder due to lack of RAM. The problem cracker. GPT-OSS can be swept with other reasoning models with tool use capabilities (Magistral, DeepSeek, ERNIE-thinking, EXAONE, etc... lot of options here)&lt;/p&gt; &lt;p&gt;Experimental, hand-made prototyping: Continue doing auto-complete work on top of Qwen2.5-Coder 7b. Served by Ollama to be always available together with the model served by LM Studio. When you need to be in the loop of creativity this is the one.&lt;/p&gt; &lt;p&gt;IDE for data exploration: Spyder&lt;/p&gt; &lt;p&gt;Long Live to Local LLM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ia5muohupypf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkfvrl/local_llm_coding_stack_24gb_minimum_ideal_36gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkfvrl/local_llm_coding_stack_24gb_minimum_ideal_36gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T18:14:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkkghp</id>
    <title>Decart-AI releases ‚ÄúOpen Source Nano Banana for Video‚Äù</title>
    <updated>2025-09-18T21:09:20+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkkghp/decartai_releases_open_source_nano_banana_for/"&gt; &lt;img alt="Decart-AI releases ‚ÄúOpen Source Nano Banana for Video‚Äù" src="https://preview.redd.it/eisyod0snzpf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b42197a4268b3f80790f0506f12d7d6cfc5a4bb" title="Decart-AI releases ‚ÄúOpen Source Nano Banana for Video‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are building ‚ÄúOpen Source Nano Banana for Video‚Äù - here is open source demo v0.1&lt;/p&gt; &lt;p&gt;We are open sourcing Lucy Edit, the first foundation model for text-guided video editing!&lt;/p&gt; &lt;p&gt;Lucy Edit lets you prompt to try on uniforms or costumes - with motion, face, and identity staying perfectly preserved&lt;/p&gt; &lt;p&gt;Get the model on @huggingface ü§ó, API on @FAL, and nodes on @ComfyUI üßµ&lt;/p&gt; &lt;p&gt;X post: &lt;a href="https://x.com/decartai/status/1968769793567207528?s=46"&gt;https://x.com/decartai/status/1968769793567207528?s=46&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/decart-ai/Lucy-Edit-Dev"&gt;https://huggingface.co/decart-ai/Lucy-Edit-Dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Lucy Edit Node on ComfyUI: &lt;a href="https://github.com/decartAI/lucy-edit-comfyui"&gt;https://github.com/decartAI/lucy-edit-comfyui&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eisyod0snzpf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkkghp/decartai_releases_open_source_nano_banana_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkkghp/decartai_releases_open_source_nano_banana_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T21:09:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkmc7z</id>
    <title>Moondream 3 (Preview) -- hybrid reasoning vision language model</title>
    <updated>2025-09-18T22:24:59+00:00</updated>
    <author>
      <name>/u/radiiquark</name>
      <uri>https://old.reddit.com/user/radiiquark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkmc7z/moondream_3_preview_hybrid_reasoning_vision/"&gt; &lt;img alt="Moondream 3 (Preview) -- hybrid reasoning vision language model" src="https://external-preview.redd.it/4djziNvQ2zvOfJv3_xVajpCMtf-Z4Exi5Qyi8qcyMmc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=690d6b125016267d773d6fd42ebb4a21aff8aca7" title="Moondream 3 (Preview) -- hybrid reasoning vision language model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/radiiquark"&gt; /u/radiiquark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/moondream/moondream3-preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkmc7z/moondream_3_preview_hybrid_reasoning_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkmc7z/moondream_3_preview_hybrid_reasoning_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T22:24:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkjpu3</id>
    <title>Model: Qwen3 Next Pull Request llama.cpp</title>
    <updated>2025-09-18T20:40:40+00:00</updated>
    <author>
      <name>/u/Loskas2025</name>
      <uri>https://old.reddit.com/user/Loskas2025</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkjpu3/model_qwen3_next_pull_request_llamacpp/"&gt; &lt;img alt="Model: Qwen3 Next Pull Request llama.cpp" src="https://a.thumbs.redditmedia.com/_7Yokv2mOxidJMZ0PGNDJ03EhjWToQTSVgNpJKxH3R0.jpg" title="Model: Qwen3 Next Pull Request llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/yy1fvsujizpf1.png?width=424&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fabb8a9874c6f2d8ac968673d1a4d84bf1f4eec0"&gt;https://preview.redd.it/yy1fvsujizpf1.png?width=424&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fabb8a9874c6f2d8ac968673d1a4d84bf1f4eec0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're fighting with you guys! Maximum support!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loskas2025"&gt; /u/Loskas2025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkjpu3/model_qwen3_next_pull_request_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkjpu3/model_qwen3_next_pull_request_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkjpu3/model_qwen3_next_pull_request_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T20:40:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk7jbi</id>
    <title>NVIDIA invests 5 billions $ into Intel</title>
    <updated>2025-09-18T12:56:04+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk7jbi/nvidia_invests_5_billions_into_intel/"&gt; &lt;img alt="NVIDIA invests 5 billions $ into Intel" src="https://external-preview.redd.it/n5kP5NRletQy7r254iQxj6sHk25NybFeHBeqLvZxjz8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ddf7ad07021e40c3004f38a19f49697e1cd4cc6" title="NVIDIA invests 5 billions $ into Intel" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bizarre news, so NVIDIA is like 99% of the market now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/09/18/intel-nvidia-investment.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk7jbi/nvidia_invests_5_billions_into_intel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nk7jbi/nvidia_invests_5_billions_into_intel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T12:56:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkbrk1</id>
    <title>Local Suno just dropped</title>
    <updated>2025-09-18T15:42:25+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/fredconex/SongBloom-Safetensors"&gt;https://huggingface.co/fredconex/SongBloom-Safetensors&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/fredconex/ComfyUI-SongBloom"&gt;https://github.com/fredconex/ComfyUI-SongBloom&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Examples:&lt;br /&gt; &lt;a href="https://files.catbox.moe/i0iple.flac"&gt;https://files.catbox.moe/i0iple.flac&lt;/a&gt;&lt;br /&gt; &lt;a href="https://files.catbox.moe/96i90x.flac"&gt;https://files.catbox.moe/96i90x.flac&lt;/a&gt;&lt;br /&gt; &lt;a href="https://files.catbox.moe/zot9nu.flac"&gt;https://files.catbox.moe/zot9nu.flac&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There is a DPO trained one that just came out &lt;a href="https://huggingface.co/fredconex/SongBloom-Safetensors/blob/main/songbloom_full_150s_dpo.safetensors"&gt;https://huggingface.co/fredconex/SongBloom-Safetensors/blob/main/songbloom_full_150s_dpo.safetensors&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Using the DPO one this was feeding it the start of Metallica fade to black and some claude generated lyrics&lt;br /&gt; &lt;a href="https://files.catbox.moe/sopv2f.flac"&gt;https://files.catbox.moe/sopv2f.flac&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This was higher cfg / lower temp / another seed: &lt;a href="https://files.catbox.moe/olajtj.flac"&gt;https://files.catbox.moe/olajtj.flac&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Crazy leap for local&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkbrk1/local_suno_just_dropped/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkbrk1/local_suno_just_dropped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkbrk1/local_suno_just_dropped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T15:42:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkieo3</id>
    <title>PSA it costs authors $12,690 to make a Nature article Open Access</title>
    <updated>2025-09-18T19:50:42+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkieo3/psa_it_costs_authors_12690_to_make_a_nature/"&gt; &lt;img alt="PSA it costs authors $12,690 to make a Nature article Open Access" src="https://preview.redd.it/xkcal9zq9zpf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07dcfaf4df77e0f86644296480de4064b0f6ca22" title="PSA it costs authors $12,690 to make a Nature article Open Access" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And the DeepSeek folks paid up so we can read their work without hitting a paywall. Massive respect for absorbing the costs so the public benefits.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xkcal9zq9zpf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkieo3/psa_it_costs_authors_12690_to_make_a_nature/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkieo3/psa_it_costs_authors_12690_to_make_a_nature/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T19:50:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1njjz7j</id>
    <title>Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)</title>
    <updated>2025-09-17T17:44:17+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt; &lt;img alt="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" src="https://preview.redd.it/4xt9enbairpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cc1ac16a9a2b96134934fcb2f81a9f3d4916b31" title="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4xt9enbairpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T17:44:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkft9l</id>
    <title>AMA with the LM Studio team</title>
    <updated>2025-09-18T18:12:24+00:00</updated>
    <author>
      <name>/u/yags-lms</name>
      <uri>https://old.reddit.com/user/yags-lms</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We're excited for this AMA. Thank you for having us here today. We got a full house from the LM Studio team: &lt;/p&gt; &lt;p&gt;- Yags &lt;a href="https://t.co/ERfA4NrR96"&gt;https://reddit.com/user/yags-lms/&lt;/a&gt; (founder)&lt;br /&gt; - Neil &lt;a href="https://t.co/KyiHVfv0QG"&gt;https://reddit.com/user/neilmehta24/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Will &lt;a href="https://t.co/IjAZJL2JMK"&gt;https://reddit.com/user/will-lms/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Matt &lt;a href="https://t.co/6MNkItPYnI"&gt;https://reddit.com/user/matt-lms/&lt;/a&gt; (LLM engines, runtime, and APIs)&lt;br /&gt; - Ryan &lt;a href="https://t.co/0snuNUPizo"&gt;https://reddit.com/user/ryan-lms/&lt;/a&gt; (Core system and APIs)&lt;br /&gt; - Rugved &lt;a href="https://t.co/xGtYHsJZI3"&gt;https://reddit.com/user/rugved_lms/&lt;/a&gt; (CLI and SDKs)&lt;br /&gt; - Alex &lt;a href="https://t.co/wtT2IFf0z6"&gt;https://reddit.com/user/alex-lms/&lt;/a&gt; (App)&lt;br /&gt; - Julian &lt;a href="https://www.reddit.com/user/julian-lms/"&gt;https://www.reddit.com/user/julian-lms/&lt;/a&gt; (Ops) &lt;/p&gt; &lt;p&gt;Excited to chat about: the latest local models, UX for local models, steering local models effectively, LM Studio SDK and APIs, how we support multiple LLM engines (llama.cpp, MLX, and more), privacy philosophy, why local AI matters, our open source projects (mlx-engine, lms, lmstudio-js, lmstudio-python, venvstacks), why ggerganov and Awni are the GOATs, where is TheBloke, and more. &lt;/p&gt; &lt;p&gt;Would love to hear about people's setup, which models you use, use cases that really work, how you got into local AI, what needs to improve in LM Studio and the ecosystem as a whole, how you use LM Studio, and anything in between!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Everyone: it was awesome to see your questions here today and share replies! Thanks a lot for the welcoming AMA. We will continue to monitor this post for more questions over the next couple of days, but for now we're signing off to continue building üî®&lt;/p&gt; &lt;p&gt;We have several marquee features we've been working on for a loong time coming out later this month that we hope you'll love and find lots of value in. And don't worry, UI for n cpu moe is on the way too :)&lt;/p&gt; &lt;p&gt;Special shoutout and thanks to ggerganov, Awni Hannun, TheBloke, Hugging Face, and all the rest of the open source AI community!&lt;/p&gt; &lt;p&gt;Thank you and see you around! - Team LM Studio üëæ&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yags-lms"&gt; /u/yags-lms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T18:12:24+00:00</published>
  </entry>
</feed>
