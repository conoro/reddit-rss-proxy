<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-24T19:34:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nouiqj</id>
    <title>Qwen3-Omni thinking model running on local H100 (major leap over 2.5)</title>
    <updated>2025-09-23T21:49:43+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nouiqj/qwen3omni_thinking_model_running_on_local_h100/"&gt; &lt;img alt="Qwen3-Omni thinking model running on local H100 (major leap over 2.5)" src="https://external-preview.redd.it/ZG5qNW92cXRoenFmMQidY-VedNK5oWhNvWMcKBJGzqCaGjB2dyVwW_xfHksA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49e20215e9b1e3d03e6f379d9005a12d480bacc6" title="Qwen3-Omni thinking model running on local H100 (major leap over 2.5)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just gave the new Qwen3-Omni (thinking model) a run on my local H100.&lt;/p&gt; &lt;p&gt;Running FP8 dynamic quant with a 32k context size, enough room for 11x concurrency without issue. Latency is higher (which is expected) since thinking is enabled and it's streaming reasoning tokens.&lt;/p&gt; &lt;p&gt;But the output is sharp, and it's clearly smarter than Qwen 2.5 with better reasoning, memory, and real-world awareness.&lt;/p&gt; &lt;p&gt;It consistently understands what I’m saying, and even picked up when I was “singing” (just made some boop boop sounds lol).&lt;/p&gt; &lt;p&gt;Tool calling works too, which is huge. More on that + load testing soon!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hsp0mvqthzqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nouiqj/qwen3omni_thinking_model_running_on_local_h100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nouiqj/qwen3omni_thinking_model_running_on_local_h100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T21:49:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1npdnok</id>
    <title>How do you know which contributors’ quantisation to trust on huggingface?</title>
    <updated>2025-09-24T14:29:38+00:00</updated>
    <author>
      <name>/u/AllSystemsFragile</name>
      <uri>https://old.reddit.com/user/AllSystemsFragile</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New to the local llm scene and trying to experiment a bit with running models on my phone, but confused about how to pick which version to download. E.g. I’d like to run Qweb 3 4b Instruction 2507, but then need to rely on a contributors version of this - not directly the Qwen page? How do you pick who to trust here (and is there even a big risk?). I kind of get go with the one with the most downloads, but seems a bit random - seeing names like bartowski, unsloth, maziyar panahi. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AllSystemsFragile"&gt; /u/AllSystemsFragile &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npdnok/how_do_you_know_which_contributors_quantisation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npdnok/how_do_you_know_which_contributors_quantisation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npdnok/how_do_you_know_which_contributors_quantisation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T14:29:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1not4up</id>
    <title>Qwen3-VL-235B-A22B-Thinking and Qwen3-VL-235B-A22B-Instruct</title>
    <updated>2025-09-23T20:55:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1not4up/qwen3vl235ba22bthinking_and/"&gt; &lt;img alt="Qwen3-VL-235B-A22B-Thinking and Qwen3-VL-235B-A22B-Instruct" src="https://external-preview.redd.it/buohQYfptNXWK_RjSglwO9z3swviJ-ly59KfrJBuCDs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7b84536da0880f9dbcad1c25d46560ac5263a67" title="Qwen3-VL-235B-A22B-Thinking and Qwen3-VL-235B-A22B-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Meet Qwen3-VL — the most powerful vision-language model in the Qwen series to date.&lt;/p&gt; &lt;p&gt;This generation delivers comprehensive upgrades across the board: superior text understanding &amp;amp; generation, deeper visual perception &amp;amp; reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.&lt;/p&gt; &lt;p&gt;Available in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‑enhanced Thinking editions for flexible, on‑demand deployment.&lt;/p&gt; &lt;h1&gt;Key Enhancements:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Visual Agent&lt;/strong&gt;: Operates PC/mobile GUIs—recognizes elements, understands functions, invokes tools, completes tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual Coding Boost&lt;/strong&gt;: Generates &lt;a href="http://Draw.io/HTML/CSS/JS"&gt;Draw.io/HTML/CSS/JS&lt;/a&gt; from images/videos.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced Spatial Perception&lt;/strong&gt;: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long Context &amp;amp; Video Understanding&lt;/strong&gt;: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced Multimodal Reasoning&lt;/strong&gt;: Excels in STEM/Math—causal analysis and logical, evidence-based answers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Upgraded Visual Recognition&lt;/strong&gt;: Broader, higher-quality pretraining is able to “recognize everything”—celebrities, anime, products, landmarks, flora/fauna, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Expanded OCR&lt;/strong&gt;: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text Understanding on par with pure LLMs&lt;/strong&gt;: Seamless text–vision fusion for lossless, unified comprehension.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yx8zvj9z9zqf1.jpg?width=4583&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3dd9ee51e1ed2ee5b561d613720f88406ce22a14"&gt;https://preview.redd.it/yx8zvj9z9zqf1.jpg?width=4583&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3dd9ee51e1ed2ee5b561d613720f88406ce22a14&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k2lvsypz9zqf1.jpg?width=4583&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2dff5af92d79eb12f9b7ac684373c589fa924d18"&gt;https://preview.redd.it/k2lvsypz9zqf1.jpg?width=4583&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2dff5af92d79eb12f9b7ac684373c589fa924d18&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1not4up/qwen3vl235ba22bthinking_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1not4up/qwen3vl235ba22bthinking_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1not4up/qwen3vl235ba22bthinking_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T20:55:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nosdxy</id>
    <title>Qwen3-VL: Sharper Vision, Deeper Thought, Broader Action</title>
    <updated>2025-09-23T20:26:06+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nosdxy/qwen3vl_sharper_vision_deeper_thought_broader/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nosdxy/qwen3vl_sharper_vision_deeper_thought_broader/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T20:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1npi8lt</id>
    <title>Any good resources to learn llama.cpp tool and its parameters and settings?</title>
    <updated>2025-09-24T17:22:22+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been using llama.cpp instead of LM Studio but I’ve been a script kid and copy pasting or using flags blindly. I want to know what I’m doing and I’d like to ask the community that where do I learn everything about llama.cpp in good detail. &lt;/p&gt; &lt;p&gt;Multiple resources that you have learned from, please drop them like Qwen drops new models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npi8lt/any_good_resources_to_learn_llamacpp_tool_and_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npi8lt/any_good_resources_to_learn_llamacpp_tool_and_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npi8lt/any_good_resources_to_learn_llamacpp_tool_and_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T17:22:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1npkqb5</id>
    <title>What's the consensus on Qwen3-Max vs Qwen3 235b Instruct model? How much better do you perceive Max to be?</title>
    <updated>2025-09-24T18:57:04+00:00</updated>
    <author>
      <name>/u/Striking_Wedding_461</name>
      <uri>https://old.reddit.com/user/Striking_Wedding_461</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Obviously one is more based (open-weight) while the other is proprietary BUT considering Qwen3-Max has over a trillion parameters it should be at least 10% better than 235b right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking_Wedding_461"&gt; /u/Striking_Wedding_461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npkqb5/whats_the_consensus_on_qwen3max_vs_qwen3_235b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npkqb5/whats_the_consensus_on_qwen3max_vs_qwen3_235b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npkqb5/whats_the_consensus_on_qwen3max_vs_qwen3_235b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T18:57:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1npcetj</id>
    <title>Local AI Agent | Open Source</title>
    <updated>2025-09-24T13:39:24+00:00</updated>
    <author>
      <name>/u/cride20</name>
      <uri>https://old.reddit.com/user/cride20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm happily announcing my Agent CLI program!&lt;br /&gt; It supports most APIs, example configs are provided for popular LLM Providers&lt;/p&gt; &lt;p&gt;I've been stress-testing it for days with a series of increasingly difficult tasks, and I wanted to share the final result.&lt;/p&gt; &lt;p&gt;The &amp;quot;final exam&amp;quot; was to build a configurable quiz generator from scratch. The rules were brutal: it had to use a specific, less-common JS library (Alpine.js) for reactivity, manage a complex two-stage UI, and follow a strict design system—all in a single HTML file.&lt;/p&gt; &lt;p&gt;After 30 minutes of generation on my laptop (running a Qwen3-Instruct-30B-Q8 MoE model), it produced a fully functional, single-file web app.&lt;/p&gt; &lt;p&gt;The repository: &lt;a href="https://github.com/cride9/AISlop"&gt;AISlop Agent Github&lt;/a&gt;&lt;br /&gt; The outcome: &lt;a href="https://cride9.github.io/slopquiz.html"&gt;Configurable Quiz Generator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The most fascinating part was watching different models fail in unique ways before this one finally succeeded. It really pushed the boundaries of what I thought was possible with local models. Happy to answer any questions about the setup or the agent's instructions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cride20"&gt; /u/cride20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npcetj/local_ai_agent_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npcetj/local_ai_agent_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npcetj/local_ai_agent_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T13:39:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1npky3c</id>
    <title>Anyone tried Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010 on HF ?</title>
    <updated>2025-09-24T19:04:52+00:00</updated>
    <author>
      <name>/u/cztothehead</name>
      <uri>https://old.reddit.com/user/cztothehead</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This being a thinking, instruction mix gives me high hopes that is could perform very well, has anyone found good settings for it (top_k) etc?&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010"&gt;https://huggingface.co/huihui-ai/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q4_K_M.gguf"&gt;https://huggingface.co/mradermacher/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010-i1-GGUF/resolve/main/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q4_K_M.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PC: 12gb VRAM 4070, 32GB DDR4 3200 RAM, 5700x3d, don't mind if its slow but would prefer over a token per second if possible.&lt;br /&gt; Lastly, for best use in longer context / inputs, are there any tricks I can do in LM Studio ? I just want to make it as performant as possible but don't think anything below K4_K_M will be accurate enough&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cztothehead"&gt; /u/cztothehead &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npky3c/anyone_tried/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npky3c/anyone_tried/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npky3c/anyone_tried/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T19:04:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1npgqfe</id>
    <title>Memory Enhanced Adapter for Reasoning</title>
    <updated>2025-09-24T16:25:31+00:00</updated>
    <author>
      <name>/u/arcco96</name>
      <uri>https://old.reddit.com/user/arcco96</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npgqfe/memory_enhanced_adapter_for_reasoning/"&gt; &lt;img alt="Memory Enhanced Adapter for Reasoning" src="https://external-preview.redd.it/nkhh65ujo5BznFJFojoMPaKjGuLSpPj6KGhRov-ykOg.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e2f90964c81a1de52938be6bcb08665605293f2" title="Memory Enhanced Adapter for Reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tldr; 74% performance on 500 train samples 50 test samples of gsm8k using llama 3 8b&lt;/p&gt; &lt;p&gt;Building from the idea that working memory is a strong correlate of general intelligence I created a &amp;quot;working memory adapter&amp;quot; technique that equips llms which typically have a linear memory with a graph attention powered global memory. Via the usage of a special &amp;lt;memory&amp;gt; tag and direction injection via LORA the llm receives an input summarizing all previous model hidden states. The technique works for any dataset but I imagine its best suited for reasoning tasks. &lt;/p&gt; &lt;p&gt;Theres a slight problem with stepping the COT where the steps are not terminated correctly and therefore parsed incorrectly producing an empty string for second step parsed but including all reasoning steps in the first parsed step output. I'm not sure what the conventional way of fixing this problem is. Does COT training usually include special &amp;lt;beginning\_of\_thought&amp;gt;, &amp;lt;end\_of\_thought&amp;gt; tokens?&lt;/p&gt; &lt;p&gt;I was hoping to get everyone's opinion about where to go from here. The performance on an abbreviated dataset trained for few epochs was pretty good which you can see in the linked colab notebook. What should I change if anything regarding hyperparameters and model architecture? I've attempted multiple different enhanced architectures all of which fail except for a multi layer LORA integration which performs on par with the single LORA layer integration. Multi layer GAT failed as well as multi &amp;quot;arm&amp;quot; gat which had specialized arms fused with a GAT. &lt;/p&gt; &lt;p&gt;Last does anybody know of similar GNN techniques applied to llm/ llm reasoning? What about working memory esque augmentations for llms... everyone seems to be excited about long term memory for llms and not at all working/short term.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arcco96"&gt; /u/arcco96 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://colab.research.google.com/drive/1UphKe8gValkFeYFjUc6SFrSq_aPWTJdu?usp=sharing"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npgqfe/memory_enhanced_adapter_for_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npgqfe/memory_enhanced_adapter_for_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T16:25:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1np9vz0</id>
    <title>Qwen3-14B-ARPO-DeepSearch feedback</title>
    <updated>2025-09-24T11:43:59+00:00</updated>
    <author>
      <name>/u/Temporary-Roof2867</name>
      <uri>https://old.reddit.com/user/Temporary-Roof2867</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, hoping not to be intrusive, has anyone ever tried the dongguanting/Qwen3-14B-ARPO-DeepSearch version? How do you like it? Not as an agent model, but just as a model that responds to prompts. What's your experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary-Roof2867"&gt; /u/Temporary-Roof2867 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np9vz0/qwen314barpodeepsearch_feedback/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np9vz0/qwen314barpodeepsearch_feedback/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np9vz0/qwen314barpodeepsearch_feedback/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T11:43:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1np75y4</id>
    <title>[Rant] Magistral-Small-2509 &gt; Claude4</title>
    <updated>2025-09-24T08:58:16+00:00</updated>
    <author>
      <name>/u/OsakaSeafoodConcrn</name>
      <uri>https://old.reddit.com/user/OsakaSeafoodConcrn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So unsure if many of you use Claude4 for non-coding stuff...but it's been turned into a blithering idiot thanks to Anthropic giving us a dumb quant that cannot follow simple writing instructions (professional writing about such exciting topics as science/etc).&lt;/p&gt; &lt;p&gt;Claude4 is amazing for 3-4 business days after they come out with a new release. I believe this is due to them giving the public the full precision model for a few days to generate publicity and buzz...then forcing everyone onto a dumbed-down quant to save money on compute/etc. &lt;/p&gt; &lt;p&gt;That said...&lt;/p&gt; &lt;p&gt;I recall some guy on here saying his wife felt that Magistral-Small-2509 was better than Claude. Based on this random lady mentioned in a random anecdote, I downloaded Magistral-Small-2509-Q6_K.gguf from Bartowski and was able to fit it on my 3060 and 64GB DDR4 RAM.&lt;/p&gt; &lt;p&gt;Loaded up Oobabooga, set &amp;quot;cache type&amp;quot; to Q6 (assuming that's the right setting), and set &amp;quot;enable thinking&amp;quot; to &amp;quot;high.&amp;quot;&lt;/p&gt; &lt;p&gt;Magistral, even at a Q6 quant on my shitty 3060 and 64GB of RAM was better able to adhere to a prompt and follow a list of grammar rules WAY better than Claude4. &lt;/p&gt; &lt;p&gt;The tokens per second are surprisingly fast (I know that is subjective...but it types at the speed of a competent human typer).&lt;/p&gt; &lt;p&gt;While full precision Claude4 would blow anything local out of the water and dance the Irish jig on its rotting corpse....for some reason the major AI companies are giving us dumbed-down quants. Not talking shit about Magistral, nor all their hard work. &lt;/p&gt; &lt;p&gt;But one would expect a Q6 SMALL model to be a pile of shit compared to the billion-dollar AI models from Anthropic and their ilk. So, I'm absolutely blown away at how this little model that can is punching WELL above its weight class.&lt;/p&gt; &lt;p&gt;Thank you to Magistral. You have saved me hours of productivity lost by constantly forcing Claude4 to fix its fuckups and errors. For the most part, Magistral gives me what I need on the first or 2nd prompt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OsakaSeafoodConcrn"&gt; /u/OsakaSeafoodConcrn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np75y4/rant_magistralsmall2509_claude4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np75y4/rant_magistralsmall2509_claude4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np75y4/rant_magistralsmall2509_claude4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T08:58:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nor65d</id>
    <title>Qwen 3 max released</title>
    <updated>2025-09-23T19:40:02+00:00</updated>
    <author>
      <name>/u/clem844</name>
      <uri>https://old.reddit.com/user/clem844</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://qwen.ai/blog?id=241398b9cd6353de490b0f82806c7848c5d2777d&amp;amp;from=research.latest-advancements-list"&gt;https://qwen.ai/blog?id=241398b9cd6353de490b0f82806c7848c5d2777d&amp;amp;from=research.latest-advancements-list&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Following the release of the Qwen3-2507 series, we are thrilled to introduce Qwen3-Max — our largest and most capable model to date. The preview version of Qwen3-Max-Instruct currently ranks third on the Text Arena leaderboard, surpassing GPT-5-Chat. The official release further enhances performance in coding and agent capabilities, achieving state-of-the-art results across a comprehensive suite of benchmarks — including knowledge, reasoning, coding, instruction following, human preference alignment, agent tasks, and multilingual understanding. We invite you to try Qwen3-Max-Instruct via its API on Alibaba Cloud or explore it directly on Qwen Chat. Meanwhile, Qwen3-Max-Thinking — still under active training — is already demonstrating remarkable potential. When augmented with tool usage and scaled test-time compute, the Thinking variant has achieved 100% on challenging reasoning benchmarks such as AIME 25 and HMMT. We look forward to releasing it publicly in the near future.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clem844"&gt; /u/clem844 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nor65d/qwen_3_max_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nor65d/qwen_3_max_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nor65d/qwen_3_max_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T19:40:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1nphn86</id>
    <title>Qwen3-30B-A3B for role-playing</title>
    <updated>2025-09-24T17:00:05+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My favorite model for roleplaying, using a good detailed prompt, has been Gemma 3, until today when I decided to try something unusual: Qwen3-30B-A3B. Well, that thing is incredible! It seems to follow the prompt much better than Gemma, interactions and scenes are really vivid, original, filled with sensory details.&lt;/p&gt; &lt;p&gt;The only problem is, it really likes to write (often 15-20 lines per reply) and sometimes it keeps expanding the dialogue in the same reply (so it becomes twice longer...) I'm using the recommended &amp;quot;official&amp;quot; settings for Qwen. Any idea how I can reduce this behaviour?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nphn86/qwen330ba3b_for_roleplaying/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nphn86/qwen330ba3b_for_roleplaying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nphn86/qwen330ba3b_for_roleplaying/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T17:00:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nozz23</id>
    <title>The Ryzen AI MAX+ 395 is a true unicorn (In a good way)</title>
    <updated>2025-09-24T01:57:05+00:00</updated>
    <author>
      <name>/u/simracerman</name>
      <uri>https://old.reddit.com/user/simracerman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I put an order for the &lt;a href="https://frame.work/products/framework-desktop-mainboard-amd-ryzen-ai-max-300-series?v=FRAFMK0006"&gt;128GB version of the Framework Desktop Board&lt;/a&gt; for AI inference mainly, and while I've been waiting patiently for it to ship, I had doubts recently about the cost to benefit/future upgrade-ability since the RAM, CPU/iGPU are soldered into the motherboard.&lt;/p&gt; &lt;p&gt;So I decided to do a quick exercise of PC part picking to match the specs Framework is offering in their 128GB Board. I started looking at Motherboards offering 4 Channels, and thought I'd find something cheap.. wrong! &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cheapest consumer level MB offering DDR5 at a high speed (8000 MT/s) with more than 2 channels is $600+. &lt;/li&gt; &lt;li&gt;CPU equivalent to the 395 MAX+ in benchmarks is the &lt;a href="https://www.amazon.com/AMD-Ryzen-9950X3D-16-Core-Processor/dp/B0DVZSG8D5"&gt;9955HX3d&lt;/a&gt;, which runs about ~$660 from Amazon. A quiet heat sink with dual fans from &lt;a href="https://www.amazon.com/Noctua-NH-D15-heatpipe-NF-A15-140mm/dp/B00L7UZMAK?s=electronics"&gt;Noctua&lt;/a&gt; is $130&lt;/li&gt; &lt;li&gt;RAM from &lt;a href="https://www.amazon.com/G-SKILL-Trident-CL38-48-48-128-Desktop-Computer/dp/B0F4M6C65N"&gt;G.Skill 4x24&lt;/a&gt; (128GB total) at 8000 MT/s runs you closer to $450. &lt;/li&gt; &lt;li&gt;The 8060s iGPU is similar in performance to the RTX 4060 or &lt;a href="https://www.amazon.com/MSI-Gaming-GeForce-GDRR6-Boost/dp/B0D3KGNMXP"&gt;4060 Ti 16gb&lt;/a&gt;, runs about $400.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Total for this build is ~&lt;strong&gt;$2240. I&lt;/strong&gt;t's obviously a good $500+ more than Framework's board. Cost aside, the speed is compromised as the GPU in this setup will access most of the system RAM at some a loss since it lives outside the GPU chip, and has to traverse the PCIE 5 to access the Memory directly. Total power draw out the wall at full system load at least double the 395's setup. More power = More fan noise = &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nogrv2/computer_literally_warms_my_room_by_5_degrees/"&gt;More heat&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To compare, the M4 Pro/Max offer higher memory bandwidth, but suck at running diffusion models, also runs at 2X the cost at the same RAM/GPU specs. The 395 runs Linux/Windows, more flexibility and versatility (Games on Windows, Inference on Linux). Nvidia is so far out in the cost alone it makes no sense to compare it. The closest equivalent (but at much higher inference speed) is 4x 3090 which costs more, consumes multiple times the power, and generates a ton more heat.&lt;/p&gt; &lt;p&gt;AMD has a true unicorn here. For tinkers and hobbyists looking to develop, test, and gain more knowledge in this field, the MAX+ 395 is pretty much the only viable option at this $$ amount, with this low power draw. I decided to continue on with my order, but wondering if anyone else went down this rabbit hole seeking similar answers..!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simracerman"&gt; /u/simracerman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nozz23/the_ryzen_ai_max_395_is_a_true_unicorn_in_a_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nozz23/the_ryzen_ai_max_395_is_a_true_unicorn_in_a_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nozz23/the_ryzen_ai_max_395_is_a_true_unicorn_in_a_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T01:57:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1np2v1i</id>
    <title>Large Language Model Performance Doubles Every 7 Months</title>
    <updated>2025-09-24T04:24:24+00:00</updated>
    <author>
      <name>/u/Aralknight</name>
      <uri>https://old.reddit.com/user/Aralknight</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np2v1i/large_language_model_performance_doubles_every_7/"&gt; &lt;img alt="Large Language Model Performance Doubles Every 7 Months" src="https://external-preview.redd.it/FIe2X4pB5JIPoblqtKC-Psg0C0IDm1Mq5ljjHekoesw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74fd271c0f36614a182e5a476492961d5ccd453d" title="Large Language Model Performance Doubles Every 7 Months" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aralknight"&gt; /u/Aralknight &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://spectrum.ieee.org/large-language-model-performance"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np2v1i/large_language_model_performance_doubles_every_7/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np2v1i/large_language_model_performance_doubles_every_7/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T04:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1npbxpw</id>
    <title>Reproducing GPT-2 (124M) from scratch - results &amp; notes</title>
    <updated>2025-09-24T13:19:20+00:00</updated>
    <author>
      <name>/u/garg-aayush</name>
      <uri>https://old.reddit.com/user/garg-aayush</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npbxpw/reproducing_gpt2_124m_from_scratch_results_notes/"&gt; &lt;img alt="Reproducing GPT-2 (124M) from scratch - results &amp;amp; notes" src="https://external-preview.redd.it/Vt6iMmXcWe78znjNZqJ9nEufupk1m_LVEHGIsA87b3o.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f19f06e9fc779ca6468a6c1b10abcff2a43181b" title="Reproducing GPT-2 (124M) from scratch - results &amp;amp; notes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the last couple of weeks, I followed karpathy’s &lt;a href="https://www.youtube.com/watch?v=l8pRSuU81PU"&gt;‘Let’s Reproduce GPT-2’&lt;/a&gt; video religiously—making notes, implementing the logic line by line, and completing a re-implementation of GPT-2 from scratch.&lt;/p&gt; &lt;p&gt;I went a few steps further by implementing some of the improvements suggested by &lt;a href="/u/karpathy"&gt;u/karpathy&lt;/a&gt; (such as learning rate adjustments and data loader fixes), along with modern enhancements like RoPE and SwiGLU-FFN.&lt;/p&gt; &lt;p&gt;My best-performing experiment &lt;code&gt;gpt2-rope&lt;/code&gt;, achieved a validation loss of &lt;strong&gt;2.987&lt;/strong&gt; and a HellaSwag accuracy of &lt;strong&gt;0.320.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ttfrzk1j54rf1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b0d9dfec4a39f29885293d8c592373bdc23f36bd"&gt;https://preview.redd.it/ttfrzk1j54rf1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b0d9dfec4a39f29885293d8c592373bdc23f36bd&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Experiment&lt;/th&gt; &lt;th align="left"&gt;Min Validation Loss&lt;/th&gt; &lt;th align="left"&gt;Max HellaSwag Acc&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt2-baseline&lt;/td&gt; &lt;td align="left"&gt;3.065753&lt;/td&gt; &lt;td align="left"&gt;0.303724&lt;/td&gt; &lt;td align="left"&gt;Original GPT-2 architecture&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt2-periodicity-fix&lt;/td&gt; &lt;td align="left"&gt;3.063873&lt;/td&gt; &lt;td align="left"&gt;0.305517&lt;/td&gt; &lt;td align="left"&gt;Fixed data loading periodicity&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt2-lr-inc&lt;/td&gt; &lt;td align="left"&gt;3.021046&lt;/td&gt; &lt;td align="left"&gt;0.315475&lt;/td&gt; &lt;td align="left"&gt;Increased learning rate by 3x and reduced warmup steps&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt2-global-datafix&lt;/td&gt; &lt;td align="left"&gt;3.004503&lt;/td&gt; &lt;td align="left"&gt;0.316869&lt;/td&gt; &lt;td align="left"&gt;Used global shuffling with better indexing&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt2-rope&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;2.987392&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.320155&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Replaced learned embeddings with RoPE&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt2-swiglu&lt;/td&gt; &lt;td align="left"&gt;3.031061&lt;/td&gt; &lt;td align="left"&gt;0.317467&lt;/td&gt; &lt;td align="left"&gt;Replaced FFN with SwiGLU-FFN activation&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I really loved the whole process of writing the code, running multiple trainings and gradually seeing the losses improve. I learnt so much about LLMs pre-training from this single video. Honestly, the $200 I spent on compute over these two weeks was the best money I’ve spent lately. Learned a ton and had fun.&lt;/p&gt; &lt;p&gt;I have made sure to log everything, the code, training runs, checkpoints, notes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Repo: &lt;a href="https://github.com/garg-aayush/building-from-scratch/blob/main/gpt-2/"&gt;https://github.com/garg-aayush/building-from-scratch/blob/main/gpt-2/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Notes: &lt;a href="https://github.com/garg-aayush/building-from-scratch/blob/main/gpt-2/notes/lecture_notes.md"&gt;https://github.com/garg-aayush/building-from-scratch/blob/main/gpt-2/notes/lecture_notes.md&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Runs: &lt;a href="https://wandb.ai/garg-aayush/pre-training"&gt;https://wandb.ai/garg-aayush/pre-training&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Dataset (training and validation): &lt;a href="https://drive.google.com/drive/folders/1FGHKpY0_jJmSR_j7ki4oyoxK-fJgldgG?usp=sharing"&gt;Google Drive&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Best checkpoints for each experiment: &lt;a href="https://drive.google.com/drive/folders/1S9mFDMG3ZPjA-JGdx_814T_NCVSjUJO-?usp=sharing"&gt;Google Drive&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/garg-aayush"&gt; /u/garg-aayush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npbxpw/reproducing_gpt2_124m_from_scratch_results_notes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npbxpw/reproducing_gpt2_124m_from_scratch_results_notes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npbxpw/reproducing_gpt2_124m_from_scratch_results_notes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T13:19:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1npb1vd</id>
    <title>LongCat-Flash-Thinking, MOE, that activates 18.6B∼31.3B parameters</title>
    <updated>2025-09-24T12:40:20+00:00</updated>
    <author>
      <name>/u/Trilogix</name>
      <uri>https://old.reddit.com/user/Trilogix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npb1vd/longcatflashthinking_moe_that_activates_186b313b/"&gt; &lt;img alt="LongCat-Flash-Thinking, MOE, that activates 18.6B∼31.3B parameters" src="https://preview.redd.it/oswqrfovx3rf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d27c1d79f8ef45bbf03cf184beed1a1a5da19925" title="LongCat-Flash-Thinking, MOE, that activates 18.6B∼31.3B parameters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is happening, can this one be so good? &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/meituan-longcat"&gt;https://huggingface.co/meituan-longcat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trilogix"&gt; /u/Trilogix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/oswqrfovx3rf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npb1vd/longcatflashthinking_moe_that_activates_186b313b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npb1vd/longcatflashthinking_moe_that_activates_186b313b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T12:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1np8uv6</id>
    <title>InclusionAI published GGUFs for the Ring-mini and Ling-mini models (MoE 16B A1.4B)</title>
    <updated>2025-09-24T10:46:45+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-mini-2.0-GGUF"&gt;https://huggingface.co/inclusionAI/Ring-mini-2.0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-mini-2.0-GGUF"&gt;https://huggingface.co/inclusionAI/Ling-mini-2.0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;!!! warning !!! PRs are still not merged (read the discussions) you must use their version of llama.cpp&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16063"&gt;https://github.com/ggml-org/llama.cpp/pull/16063&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16028"&gt;https://github.com/ggml-org/llama.cpp/pull/16028&lt;/a&gt;&lt;/p&gt; &lt;p&gt;models:&lt;/p&gt; &lt;p&gt;Today, we are excited to announce the open-sourcing of &lt;strong&gt;Ling 2.0&lt;/strong&gt; — a family of MoE-based large language models that combine &lt;strong&gt;SOTA performance&lt;/strong&gt; with &lt;strong&gt;high efficiency&lt;/strong&gt;. The first released version, Ling-mini-2.0, is compact yet powerful. It has &lt;strong&gt;16B total parameters&lt;/strong&gt;, but only &lt;strong&gt;1.4B&lt;/strong&gt; are activated per input token (non-embedding 789M). Trained on more than &lt;strong&gt;20T tokens&lt;/strong&gt; of high-quality data and enhanced through multi-stage supervised fine-tuning and reinforcement learning, Ling-mini-2.0 achieves remarkable improvements in complex reasoning and instruction following. With just 1.4B activated parameters, it still reaches the top-tier level of sub-10B dense LLMs and even matches or surpasses much larger MoE models.&lt;/p&gt; &lt;p&gt;Ring is a reasoning and Ling is an instruct model (thanks &lt;a href="/u/Obvious-Ad-2454"&gt;u/Obvious-Ad-2454&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;UPDATE&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-flash-2.0-GGUF"&gt;https://huggingface.co/inclusionAI/Ling-flash-2.0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today, &lt;strong&gt;Ling-flash-2.0&lt;/strong&gt; is officially open-sourced! 🚀 Following the release of the &lt;strong&gt;language model&lt;/strong&gt; &lt;a href="https://huggingface.co/inclusionAI/Ling-mini-2.0"&gt;&lt;strong&gt;Ling-mini-2.0&lt;/strong&gt;&lt;/a&gt; and the &lt;strong&gt;thinking model&lt;/strong&gt; &lt;a href="https://huggingface.co/inclusionAI/Ring-mini-2.0"&gt;&lt;strong&gt;Ring-mini-2.0&lt;/strong&gt;&lt;/a&gt;, we are now open-sourcing the third MoE LLM under the &lt;strong&gt;Ling 2.0 architecture: Ling-flash-2.0&lt;/strong&gt;, a language model with &lt;strong&gt;100B total parameters&lt;/strong&gt; and &lt;strong&gt;6.1B activated parameters (4.8B non-embedding)&lt;/strong&gt;. Trained on &lt;strong&gt;20T+ tokens of high-quality data&lt;/strong&gt;, together with &lt;strong&gt;supervised fine-tuning&lt;/strong&gt; and &lt;strong&gt;multi-stage reinforcement learning&lt;/strong&gt;, Ling-flash-2.0 achieves &lt;strong&gt;SOTA performance among dense models under 40B parameters&lt;/strong&gt;, despite activating only ~6B parameters. Compared to MoE models with larger activation/total parameters, it also demonstrates strong competitiveness. Notably, it delivers outstanding performance in &lt;strong&gt;complex reasoning, code generation, and frontend development&lt;/strong&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np8uv6/inclusionai_published_ggufs_for_the_ringmini_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np8uv6/inclusionai_published_ggufs_for_the_ringmini_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np8uv6/inclusionai_published_ggufs_for_the_ringmini_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T10:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1np5ey8</id>
    <title>MiniModel-200M-Base</title>
    <updated>2025-09-24T06:58:12+00:00</updated>
    <author>
      <name>/u/Wooden-Deer-1276</name>
      <uri>https://old.reddit.com/user/Wooden-Deer-1276</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np5ey8/minimodel200mbase/"&gt; &lt;img alt="MiniModel-200M-Base" src="https://preview.redd.it/clbzeq0i82rf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=056ef5c77a2001c2a6d5509cbdcb9173566b1c52" title="MiniModel-200M-Base" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most “efficient” small models still need days of training or massive clusters. &lt;strong&gt;MiniModel-200M-Base&lt;/strong&gt; was trained &lt;strong&gt;from scratch on just 10B tokens&lt;/strong&gt; in &lt;strong&gt;110k steps (≈1 day)&lt;/strong&gt; on a &lt;strong&gt;single RTX 5090&lt;/strong&gt;, using &lt;strong&gt;no gradient accumulation&lt;/strong&gt; yet still achieving a &lt;strong&gt;batch size of 64 x 2048 tokens&lt;/strong&gt; and with peak memory &lt;strong&gt;&amp;lt;30 GB VRAM&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Key efficiency techniques:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Adaptive Muon optimizer&lt;/strong&gt;: 2.1× more data-efficient than AdamW&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Float8 pretraining&lt;/strong&gt;: ~30% less VRAM, ~20% higher throughput (attention kept in bf16)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ReLU² activation&lt;/strong&gt; (from Google’s &lt;em&gt;Primer&lt;/em&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bin-packing&lt;/strong&gt;: reduced padding from &amp;gt;70% → &amp;lt;5%&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full attention + QK-norm without scalars&lt;/strong&gt; for stability&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Despite its size, it shows surprising competence:&lt;/p&gt; &lt;p&gt;✅ &lt;strong&gt;Fibonacci (temp=0.0001)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def fibonacci(n: int): if n &amp;lt; 2: return n return fibonacci(n - 1) + fibonacci(n - 2) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;✅ &lt;strong&gt;Digits of π (temp=0.0001)&lt;/strong&gt;&lt;br /&gt; Recites &lt;strong&gt;3.14159265358979323846…&lt;/strong&gt; correctly — the first 20+ digits.&lt;/p&gt; &lt;p&gt;It’s &lt;strong&gt;Apache 2.0 licensed&lt;/strong&gt;, with public config, tokenizer, and safetensors weights. No instruct-tuning yet, as this is pure pretraining on educational data (Ultra-FineWeb, Python tutorials, math).&lt;/p&gt; &lt;p&gt;Not perfect (it thinks Earth’s radius is 375,000 miles), but for a 200M model trained in a day it’s a solid base for experimentation, distillation, or local prototyping.&lt;/p&gt; &lt;p&gt;🔗 &lt;a href="https://huggingface.co/xTimeCrystal/MiniModel-200M-Base"&gt;Hugging Face: MiniModel-200M-Base&lt;/a&gt;&lt;br /&gt; 🧠 200M | 🌐 en/zh/Python | 📜 Apache 2.0&lt;/p&gt; &lt;p&gt;Any feedback is welcome, especially on replicating the training setup or improving data efficiency!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden-Deer-1276"&gt; /u/Wooden-Deer-1276 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/clbzeq0i82rf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np5ey8/minimodel200mbase/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np5ey8/minimodel200mbase/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T06:58:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1npa8yt</id>
    <title>Be cautious of GPU modification posts. And do not send anyone money. DYI if you can.</title>
    <updated>2025-09-24T12:02:25+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just a precautionary post and a reminder that this is Reddit. People can make a good looking legit website and scam you into sending them an advance payment for your 48GB 4090 or 20 GB 3080 but be cautious and stay safe. &lt;/p&gt; &lt;p&gt;Thanks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npa8yt/be_cautious_of_gpu_modification_posts_and_do_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npa8yt/be_cautious_of_gpu_modification_posts_and_do_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npa8yt/be_cautious_of_gpu_modification_posts_and_do_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T12:02:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1npgjpv</id>
    <title>China's latest GPU arrives with claims of CUDA compatibility and RT support — Fenghua No.3 also boasts 112GB+ of HBM memory for AI</title>
    <updated>2025-09-24T16:18:29+00:00</updated>
    <author>
      <name>/u/Battle-Chimp</name>
      <uri>https://old.reddit.com/user/Battle-Chimp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npgjpv/chinas_latest_gpu_arrives_with_claims_of_cuda/"&gt; &lt;img alt="China's latest GPU arrives with claims of CUDA compatibility and RT support — Fenghua No.3 also boasts 112GB+ of HBM memory for AI" src="https://external-preview.redd.it/WMoFSM_ESNhbjEkciy-Rd0SY0RIYgPT1B4RqqWSMj-g.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e6a5a5393e6e1773fbc67615613b404ec6ab6f7" title="China's latest GPU arrives with claims of CUDA compatibility and RT support — Fenghua No.3 also boasts 112GB+ of HBM memory for AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Battle-Chimp"&gt; /u/Battle-Chimp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/chinas-latest-gpu-arrives-with-claims-of-cuda-compatibility-and-rt-support-fenghua-no-3-also-boasts-112gb-of-hbm-memory-for-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npgjpv/chinas_latest_gpu_arrives_with_claims_of_cuda/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npgjpv/chinas_latest_gpu_arrives_with_claims_of_cuda/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T16:18:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1npfnvw</id>
    <title>Chinese modified 3080 20GB performance..</title>
    <updated>2025-09-24T15:45:55+00:00</updated>
    <author>
      <name>/u/sub_RedditTor</name>
      <uri>https://old.reddit.com/user/sub_RedditTor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npfnvw/chinese_modified_3080_20gb_performance/"&gt; &lt;img alt="Chinese modified 3080 20GB performance.." src="https://a.thumbs.redditmedia.com/GXzMwuLrsR20Cc_j5wp_lZeVn_BM6JL-8wxZrG9D7z4.jpg" title="Chinese modified 3080 20GB performance.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm quite surprised to see it beat 3080TI &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sub_RedditTor"&gt; /u/sub_RedditTor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1npfnvw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npfnvw/chinese_modified_3080_20gb_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npfnvw/chinese_modified_3080_20gb_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T15:45:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nph3az</id>
    <title>New Agent benchmark from Meta Super Intelligence Lab and Hugging Face</title>
    <updated>2025-09-24T16:39:07+00:00</updated>
    <author>
      <name>/u/clem59480</name>
      <uri>https://old.reddit.com/user/clem59480</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nph3az/new_agent_benchmark_from_meta_super_intelligence/"&gt; &lt;img alt="New Agent benchmark from Meta Super Intelligence Lab and Hugging Face" src="https://preview.redd.it/fjardl7x45rf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b3257ce7506ae29e607d203aa3899a1ddf43031" title="New Agent benchmark from Meta Super Intelligence Lab and Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/blog/gaia2"&gt;https://huggingface.co/blog/gaia2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clem59480"&gt; /u/clem59480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fjardl7x45rf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nph3az/new_agent_benchmark_from_meta_super_intelligence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nph3az/new_agent_benchmark_from_meta_super_intelligence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T16:39:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1np9rav</id>
    <title>My second modified 3080 20GB from China , for local Ai inference , video and image generation..</title>
    <updated>2025-09-24T11:36:56+00:00</updated>
    <author>
      <name>/u/sub_RedditTor</name>
      <uri>https://old.reddit.com/user/sub_RedditTor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np9rav/my_second_modified_3080_20gb_from_china_for_local/"&gt; &lt;img alt="My second modified 3080 20GB from China , for local Ai inference , video and image generation.." src="https://b.thumbs.redditmedia.com/PgWSwkWNNiXl6B2p4tLoG8a-8KyV8YynoMLgVVg97bI.jpg" title="My second modified 3080 20GB from China , for local Ai inference , video and image generation.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got this triple fan version instead of server - blower style card because of fan noise. It's also slightly bigger in size than the blower card . Teps are quite good and manageable , staying below 75°C , even when stress testing @ 300W . And it's a 2½ slot card ..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sub_RedditTor"&gt; /u/sub_RedditTor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1np9rav"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np9rav/my_second_modified_3080_20gb_from_china_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np9rav/my_second_modified_3080_20gb_from_china_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T11:36:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1np5te1</id>
    <title>Oh my God, what a monster is this?</title>
    <updated>2025-09-24T07:24:01+00:00</updated>
    <author>
      <name>/u/NearbyBig3383</name>
      <uri>https://old.reddit.com/user/NearbyBig3383</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np5te1/oh_my_god_what_a_monster_is_this/"&gt; &lt;img alt="Oh my God, what a monster is this?" src="https://preview.redd.it/1pxmwf50e2rf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9d1eb3d320d1305fe702c08f9c69cd841db5fd1" title="Oh my God, what a monster is this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NearbyBig3383"&gt; /u/NearbyBig3383 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1pxmwf50e2rf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np5te1/oh_my_god_what_a_monster_is_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np5te1/oh_my_god_what_a_monster_is_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T07:24:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
