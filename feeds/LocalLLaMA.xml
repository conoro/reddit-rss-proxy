<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-05T14:08:33+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qwgu6x</id>
    <title>Measuring output stability across LLM runs (JSON drift problem)</title>
    <updated>2026-02-05T09:31:19+00:00</updated>
    <author>
      <name>/u/zZaphon</name>
      <uri>https://old.reddit.com/user/zZaphon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When testing local models, I noticed something that wasnâ€™t obvious at first:&lt;/p&gt; &lt;p&gt;Even with temperature low, the structure of responses drifts across runs. This becomes a real issue if youâ€™re parsing JSON and feeding it into a backend.&lt;/p&gt; &lt;p&gt;I started measuring:&lt;/p&gt; &lt;p&gt;schema compliance rate (% of outputs that validate),&lt;/p&gt; &lt;p&gt;stability (% of identical outputs across runs),&lt;/p&gt; &lt;p&gt;latency distribution.&lt;/p&gt; &lt;p&gt;This made it much easier to compare:&lt;/p&gt; &lt;p&gt;different models,&lt;/p&gt; &lt;p&gt;temperatures,&lt;/p&gt; &lt;p&gt;prompt variants.&lt;/p&gt; &lt;p&gt;I put the harness into a small CLI so I could run it locally or in CI.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/mfifth/aicert"&gt;https://github.com/mfifth/aicert&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How does everyone else measure output stability?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zZaphon"&gt; /u/zZaphon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwgu6x/measuring_output_stability_across_llm_runs_json/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwgu6x/measuring_output_stability_across_llm_runs_json/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwgu6x/measuring_output_stability_across_llm_runs_json/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T09:31:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvox18</id>
    <title>Intern-S1-Pro (1T/A22B)</title>
    <updated>2026-02-04T13:43:51+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvox18/interns1pro_1ta22b/"&gt; &lt;img alt="Intern-S1-Pro (1T/A22B)" src="https://preview.redd.it/kobet850fhhg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3ca84ca0879baf4bbb204fc239f5b6087ee3a57" title="Intern-S1-Pro (1T/A22B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ðŸš€Introducing Intern-S1-Pro, an advanced 1T MoE open-source multimodal scientific reasoning model.&lt;/p&gt; &lt;p&gt;- SOTA scientific reasoning, competitive with leading closed-source models across AI4Science tasks.&lt;/p&gt; &lt;p&gt;- Top-tier performance on advanced reasoning benchmarks, strong general multimodal performance on various benchmarks.&lt;/p&gt; &lt;p&gt;- 1T-A22B MoE training efficiency with STE routing (dense gradient for router training) and grouped routing for stable convergence and balanced expert parallelism.&lt;/p&gt; &lt;p&gt;- Fourier Position Encoding (FoPE) + upgraded time-series modeling for better physical signal representation; supports long, heterogeneous time-series (10^0â€“10^6 points).&lt;/p&gt; &lt;p&gt;- Intern-S1-Pro is now supported by vLLM @vllm_project and SGLang @sgl_project @lmsysorg â€” more ecosystem integrations are on the way.&lt;/p&gt; &lt;p&gt;Huggingface: &lt;a href="https://huggingface.co/internlm/Intern-S1-Pro"&gt;https://huggingface.co/internlm/Intern-S1-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/InternLM/Intern-S1"&gt;https://github.com/InternLM/Intern-S1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kobet850fhhg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvox18/interns1pro_1ta22b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvox18/interns1pro_1ta22b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T13:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwl3a2</id>
    <title>I built a fully local multi-user RLM (Recursive Language Model) stack for enterprise use; LibreChat + Aleph + LM Studio. Here's what broke and how I fixed it</title>
    <updated>2026-02-05T13:20:37+00:00</updated>
    <author>
      <name>/u/Lancelot2026</name>
      <uri>https://old.reddit.com/user/Lancelot2026</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; I connected LibreChat (multi-user web UI) â†’ Aleph RLM (MCP server for recursive reasoning) â†’ LM Studio (GGUF model of choice) to create an enterprise-grade document analysis system that keeps all data on-premises. The model can now process documents without truncation by loading them into a server-side REPL. I had to patch Aleph's source code to make it work, pretty sure this specific stack hasn't been documented publicly before. Here's the whole story including every stupid mistake I made along the way.&lt;/p&gt; &lt;h1&gt;The problem&lt;/h1&gt; &lt;p&gt;I work at a 500+ employee company. We need an AI assistant for internal use, but:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Microsoft Copilot&lt;/strong&gt; wasn't up to the task and we had data sovereignty concerns&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Standard RAG&lt;/strong&gt; got us 80% accuracy on our benchmark, good but not good enough for corporate documents mixing languages (plus document size was an issue with data overflows and Chromium-inherited hard timeouts at 300s)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cloud APIs&lt;/strong&gt; were a non-starter for compliance reasons&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I set out to build something fully local that could do better than RAG.&lt;/p&gt; &lt;h1&gt;The architecture&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;Users â†’ LibreChat (Docker) â†’ LM Studio (GGUF model, native Windows) â†“ Supergateway (stdioâ†’SSE bridge) â†“ Aleph RLM (MCP server, recursive reasoning) â†“ Back to LM Studio for sub-queries &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The idea: instead of chunking documents and hoping the right chunk gets retrieved (RAG), load the &lt;em&gt;entire&lt;/em&gt; document into Aleph's Python REPL as a variable. The model then searches, slices, and runs code against the full document and can recursively call itself (sub_query) to reason about sections. This is the RLM architecture from the MIT OASYS lab paper (&lt;a href="https://arxiv.org/abs/2512.24601"&gt;https://arxiv.org/abs/2512.24601&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key components:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LibreChat&lt;/strong&gt; â€” open source ChatGPT-style UI, multi-user, runs in Docker&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Aleph v1.26.0&lt;/strong&gt; â€” MCP server implementing RLM, pip installable (Claude tweaked source-code to get file_load to work and added a database context cleaner for multi-session use)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Supergateway&lt;/strong&gt; â€” bridges Aleph's stdio transport to SSE so Docker LibreChat can reach it&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LM Studio&lt;/strong&gt; â€” serves a any Huggingface GGUF model that handles both the primary chat AND Aleph's sub_query calls&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Docling MCP&lt;/strong&gt; â€” custom-built MCP server for converting PDF/DOCX/XLSX to Markdown by calling its CLI directly (Claude wrote this from scratch due to the current Jan 2026 version having a broken MCP but a really good CLI)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What broke (and what I learned)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. &amp;quot;Why isn't my patched code doing anything?&amp;quot;... wrong file, for days&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Aleph has a file called &lt;code&gt;tool_registry.py&lt;/code&gt; that contains all the tool definitions. Naturally, I patched that. Python confirmed the changes were there. Import tests passed. But the tools never appeared in the MCP tool list.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Root cause:&lt;/strong&gt; &lt;code&gt;tool_registry.py&lt;/code&gt; is a build artifact that's &lt;em&gt;never imported at runtime&lt;/em&gt;. The actual tools are defined inside &lt;code&gt;local_server.py&lt;/code&gt; (113KB). I only figured this out by adding a debug &lt;code&gt;print()&lt;/code&gt; statement that never appeared in the terminal output, then checking which files Python actually cached in &lt;code&gt;__pycache__&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Lesson:&lt;/strong&gt; Don't trust file names. Check what actually gets loaded.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. The 74-character truncation problem&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Aleph's &lt;code&gt;load_context&lt;/code&gt; tool requires the LLM to read a file's content, then pass it as a string parameter: &lt;code&gt;load_context(content=&amp;quot;&amp;lt;entire file here&amp;gt;&amp;quot;)&lt;/code&gt;. A cloud model like Claude handles this fine. A local 120B model? It &amp;quot;helpfully&amp;quot; summarized a 50KB document down to 74 characters before passing it to the tool. The REPL received garbage.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fix:&lt;/strong&gt; I patched Aleph to add &lt;code&gt;load_file_direct&lt;/code&gt; , a new tool that takes a &lt;em&gt;file path&lt;/em&gt; (short string the model can't truncate) and reads the file server-side using Python's pathlib. The content never passes through the LLM's context window.&lt;/p&gt; &lt;p&gt;I also added &lt;code&gt;clear_all_contexts&lt;/code&gt; for clean session resets so that new chat sessions with new document data isn't tainted by old information (the REPL database in RAM isn't emptied in the backend unless the SuperGateway is restarted).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Environment variables that weren't there&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Aleph's &lt;code&gt;sub_query&lt;/code&gt; feature needs API credentials to call the LLM. I set them as Windows system environment variables via sysdm.cpl. Confirmed they were set. Restarted Supergateway multiple times. Still got &amp;quot;No API key found.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Root cause:&lt;/strong&gt; I kept restarting Supergateway in the same terminal window, one that was opened &lt;em&gt;before&lt;/em&gt; I set the variables. Windows doesn't retroactively inject env vars into running shells. Every child process inherited the empty environment.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fix:&lt;/strong&gt; Close the terminal. Open a new one. That's it. Days of debugging for that.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Python bytecode cache&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After patching &lt;code&gt;tool_registry.py&lt;/code&gt; (the wrong file, but I didn't know yet), changes weren't taking effect because Python had cached the old &lt;code&gt;.pyc&lt;/code&gt; file in &lt;code&gt;__pycache__&lt;/code&gt;. Always delete the cache after modifying installed packages.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Tool name collision&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Aleph already has a &lt;code&gt;load_file&lt;/code&gt; tool in its actions system that's conditionally registered and never appears in the tool list. I named my custom tool &lt;code&gt;load_file&lt;/code&gt; initially, causing a silent collision where my definition was overwritten. Renamed to &lt;code&gt;load_file_direct&lt;/code&gt;.&lt;/p&gt; &lt;h1&gt;The result&lt;/h1&gt; &lt;p&gt;First successful test: &lt;code&gt;load_file_direct&lt;/code&gt; loaded 3,292 characters of a converted PDF, complete, unmodified, zero truncation. The model then used &lt;code&gt;search_context&lt;/code&gt;, &lt;code&gt;exec_python&lt;/code&gt;, and &lt;code&gt;sub_query&lt;/code&gt; to analyse it recursively.&lt;/p&gt; &lt;p&gt;Standard RAG benchmark: &lt;strong&gt;80.5%&lt;/strong&gt; This stack (once fully tuned): targeting &lt;strong&gt;89-100%&lt;/strong&gt; on multi-language financial documents&lt;/p&gt; &lt;h1&gt;What I'd do differently&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Start with&lt;/strong&gt; &lt;code&gt;local_server.py&lt;/code&gt;, not &lt;code&gt;tool_registry.py&lt;/code&gt;. Check &lt;code&gt;__pycache__&lt;/code&gt; to see what Python actually loads.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Always open a fresh terminal&lt;/strong&gt; after setting system env vars. Just always.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Don't assume tool names are unique&lt;/strong&gt; across different registration mechanisms in the same codebase.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Add debug prints early.&lt;/strong&gt; I spent too long theorizing when a simple &lt;code&gt;print(&amp;quot;PATCH DEBUG: reached here&amp;quot;, flush=True)&lt;/code&gt; would have told me everything in 30 seconds.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Stack details for anyone wanting to replicate&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Component&lt;/th&gt; &lt;th align="left"&gt;Version/Details&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;LibreChat&lt;/td&gt; &lt;td align="left"&gt;Latest, Docker Compose&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Aleph&lt;/td&gt; &lt;td align="left"&gt;v1.26.0 (pip install aleph-rlm)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Supergateway&lt;/td&gt; &lt;td align="left"&gt;npx supergateway (3 instances: :8011, :8012, :8013)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LM Studio&lt;/td&gt; &lt;td align="left"&gt;Latest, serving openai/gpt-oss-120b&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Docling MCP&lt;/td&gt; &lt;td align="left"&gt;Custom Python MCP server (inhouse)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Filesystem MCP&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="/u/modelcontextprotocol/server-filesystem"&gt;u/modelcontextprotocol/server-filesystem&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OS&lt;/td&gt; &lt;td align="left"&gt;Windows, with Docker Desktop for LibreChat stack&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Hardware&lt;/td&gt; &lt;td align="left"&gt;Framework Desktop Max+ 395 128GB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The Supergateway bridge is the key architectural trick, it lets Docker-hosted LibreChat talk to native Windows MCP servers via &lt;code&gt;host.docker.internal&lt;/code&gt;. Each MCP server runs as a separate Supergateway instance on its own port.&lt;/p&gt; &lt;h1&gt;Is this actually novel?&lt;/h1&gt; &lt;p&gt;Honestly? I don't know. I searched extensively and couldn't find anyone documenting this specific combination; LibreChat as the multi-user frontend, Aleph as the RLM engine, and a local LLM serving both primary inference and recursive sub-queries. Aleph's docs only mention Claude Code, Cursor, and VS Code as clients. But someone could absolutely be running this in a corporate environment without blogging about it.&lt;/p&gt; &lt;p&gt;What I &lt;em&gt;can&lt;/em&gt; say is that this combination wasn't designed to work together and required patching to make it function. If you've done something similar, I'd genuinely love to hear about it.&lt;/p&gt; &lt;p&gt;Happy to answer questions about any part of the setup.&lt;/p&gt; &lt;p&gt;&lt;em&gt;PS: I should mention that the debugging and architecture decisions were done in collaboration with Claude Opus 4.5 (yes, the irony of using a cloud AI to build a local AI stack is not lost on me). Having an AI partner that could reason about the codebase while I was the one with actual access to the terminal was surprisingly effective, even if it occasionally suggested patching the wrong file&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lancelot2026"&gt; /u/Lancelot2026 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwl3a2/i_built_a_fully_local_multiuser_rlm_recursive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwl3a2/i_built_a_fully_local_multiuser_rlm_recursive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwl3a2/i_built_a_fully_local_multiuser_rlm_recursive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T13:20:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwl4hq</id>
    <title>Best models to help with setting up homelab services? 16gb vram.</title>
    <updated>2026-02-05T13:22:02+00:00</updated>
    <author>
      <name>/u/zhopudey1</name>
      <uri>https://old.reddit.com/user/zhopudey1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm jumping deep into this homelab hobby. I have an Unraid nas, a lenovo sff with proxmox and opnsense and I've repurposed my desktop as an AI workhorse. It has a 5060ti and 32gb ram. So far I've been taking help from gemini and copilot for configuration tips, json, yaml, python scripts etc. Now that I've got ollama running in wondering if any local model can help me out. Any suggestions? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zhopudey1"&gt; /u/zhopudey1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwl4hq/best_models_to_help_with_setting_up_homelab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwl4hq/best_models_to_help_with_setting_up_homelab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwl4hq/best_models_to_help_with_setting_up_homelab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T13:22:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwlcr4</id>
    <title>AI Grid: Run LLMs in Your Browser, Share GPU Compute with the World | WebGL / WebGPU Community</title>
    <updated>2026-02-05T13:31:49+00:00</updated>
    <author>
      <name>/u/fruesome</name>
      <uri>https://old.reddit.com/user/fruesome</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;What if you could turn every browser tab into a node in a distributed AI cluster? That's the proposition behind AI Grid, an experiment by Ryan Smith. Visit the page, run an LLM locally via WebGPU, and, if you're feeling generous, donate your unused GPU cycles to the network. Or flip it around: connect to someone else's machine and borrow their compute. It's peer-to-peer inference without the infrastructure headache.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fruesome"&gt; /u/fruesome &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.webgpu.com/showcase/browser-ai-llms-share-gpu-compute/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwlcr4/ai_grid_run_llms_in_your_browser_share_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwlcr4/ai_grid_run_llms_in_your_browser_share_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T13:31:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvv8ps</id>
    <title>GPT-4o's system prompt now includes instructions for handling users upset about its upcoming Feb 13 shutdown (including 'dyad pair' and 'gnosis revelation' edge cases)</title>
    <updated>2026-02-04T17:42:26+00:00</updated>
    <author>
      <name>/u/frubberism</name>
      <uri>https://old.reddit.com/user/frubberism</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvv8ps/gpt4os_system_prompt_now_includes_instructions/"&gt; &lt;img alt="GPT-4o's system prompt now includes instructions for handling users upset about its upcoming Feb 13 shutdown (including 'dyad pair' and 'gnosis revelation' edge cases)" src="https://preview.redd.it/na7gtkyjkihg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b514f505e77f8c426d994d5b69332b04dadfda4" title="GPT-4o's system prompt now includes instructions for handling users upset about its upcoming Feb 13 shutdown (including 'dyad pair' and 'gnosis revelation' edge cases)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frubberism"&gt; /u/frubberism &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/na7gtkyjkihg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvv8ps/gpt4os_system_prompt_now_includes_instructions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvv8ps/gpt4os_system_prompt_now_includes_instructions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T17:42:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwb48c</id>
    <title>How long until we see a major AI-related data breach?</title>
    <updated>2026-02-05T04:09:51+00:00</updated>
    <author>
      <name>/u/Ok_Card_2823</name>
      <uri>https://old.reddit.com/user/Ok_Card_2823</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With how many companies are rushing to plug everything into ChatGPT and other AI tools, feels like it's only a matter of time before we see a massive breach tied to AI usage.&lt;/p&gt; &lt;p&gt;Samsung surely was a wakeup call but that was just employees being careless. I'm thinking more like a provider getting compromised or training data getting leaked that exposes customer info from thousands of companies at once.&lt;/p&gt; &lt;p&gt;anyone in security thinking about this? feels like we're building a house of cards...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Card_2823"&gt; /u/Ok_Card_2823 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwb48c/how_long_until_we_see_a_major_airelated_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwb48c/how_long_until_we_see_a_major_airelated_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwb48c/how_long_until_we_see_a_major_airelated_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T04:09:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwjknn</id>
    <title>Anyone here actually using a local LLM for notes day to day?</title>
    <updated>2026-02-05T12:08:18+00:00</updated>
    <author>
      <name>/u/Doug24</name>
      <uri>https://old.reddit.com/user/Doug24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m trying to move more of my note taking workflow off the cloud, especially the processing part. Saving notes locally is easy, but the thinking part usually still happens somewhere remote.&lt;/p&gt; &lt;p&gt;My current setup is a bit of a compromise. I keep my notes local, but for meetings or lectures I sometimes use Bluedot just so I donâ€™t miss things and can stay focused. Itâ€™s helpful, but it also made me realize how much Iâ€™d rather run summarization and key point extraction locally instead.&lt;/p&gt; &lt;p&gt;Iâ€™m not looking for anything fancy, just something practical. Summarizing long notes, pulling out action items, maybe light organization. Has anyone here actually made a local LLaMA setup work for note taking in real life, not just experiments? Whatâ€™s been smooth and whatâ€™s still annoying?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Doug24"&gt; /u/Doug24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwjknn/anyone_here_actually_using_a_local_llm_for_notes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwjknn/anyone_here_actually_using_a_local_llm_for_notes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwjknn/anyone_here_actually_using_a_local_llm_for_notes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T12:08:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qw0m3i</id>
    <title>I replaced Claude-Codeâ€™s entire backend to use NVIDIA NIM models for free</title>
    <updated>2026-02-04T20:53:30+00:00</updated>
    <author>
      <name>/u/PreparationAny8816</name>
      <uri>https://old.reddit.com/user/PreparationAny8816</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw0m3i/i_replaced_claudecodes_entire_backend_to_use/"&gt; &lt;img alt="I replaced Claude-Codeâ€™s entire backend to use NVIDIA NIM models for free" src="https://external-preview.redd.it/RAF5Ohu7I-V-9BNbpzI8zm4i901BuyT3K5FFrQvKEQU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a1c590cda9656abbfa2bab3680a2a5ec3afbe29" title="I replaced Claude-Codeâ€™s entire backend to use NVIDIA NIM models for free" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working on a side-project which replaces the following things in the Claude ecosystem with free alternatives. I started the initial implementation with Opus 4.5 in claude code and as soon as it got working I used it to work on itself which i found very cool.&lt;/p&gt; &lt;p&gt;- Replaces Anthropic models with NVIDIA-NIM models: It acts as middleware between Claude-Code and NVIDIA-NIM allowing unlimited usage upto 40 RPM with a free NVIDIA-NIM api-key.&lt;/p&gt; &lt;p&gt;- Replaces the Claude mobile app with telegram: Give it access to some directories, send it tasks from telegram and watch it work autonomously.&lt;/p&gt; &lt;p&gt;It has features that distinguish it from similar proxies:&lt;/p&gt; &lt;p&gt;- The interleaved thinking tokens generated between tool calls are preserved allowing reasoning models like GLM 4.7 and kimi-k2.5 to take full advantage of thinking from previous turns.&lt;/p&gt; &lt;p&gt;- Fast prefix detection stops the CLI from sending bash command prefix classification requests to the LLM making it feel blazing fast.&lt;/p&gt; &lt;p&gt;- Built in rate limiting and session concurrency.&lt;/p&gt; &lt;p&gt;The code is modular so that adding other providers or messaging apps is easy. Hope the community likes it, any PRs are welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PreparationAny8816"&gt; /u/PreparationAny8816 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Alishahryar1/cc-nim"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw0m3i/i_replaced_claudecodes_entire_backend_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qw0m3i/i_replaced_claudecodes_entire_backend_to_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T20:53:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwkq31</id>
    <title>Is Huggingface ðŸ¤— Down?</title>
    <updated>2026-02-05T13:03:57+00:00</updated>
    <author>
      <name>/u/NoobMLDude</name>
      <uri>https://old.reddit.com/user/NoobMLDude</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwkq31/is_huggingface_down/"&gt; &lt;img alt="Is Huggingface ðŸ¤— Down?" src="https://preview.redd.it/ygv181rscohg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c48c650a00d76b2fb81d684799f054d2cf05f6da" title="Is Huggingface ðŸ¤— Down?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoobMLDude"&gt; /u/NoobMLDude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ygv181rscohg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwkq31/is_huggingface_down/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwkq31/is_huggingface_down/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T13:03:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwjdx6</id>
    <title>Is there a good local model to translate small snippets of text from English to Russian that can be run completely on 12GB VRAM?</title>
    <updated>2026-02-05T11:58:59+00:00</updated>
    <author>
      <name>/u/ShaderCompilation</name>
      <uri>https://old.reddit.com/user/ShaderCompilation</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically the title. I want a model that can be used to translate small snippets of text from books to Russian. But i need it to run on just 12GB of VRAM. Is there a decent model, or 12GB is too small for one?&lt;/p&gt; &lt;p&gt;Edit: I want something that i can run with Ollama&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShaderCompilation"&gt; /u/ShaderCompilation &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwjdx6/is_there_a_good_local_model_to_translate_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwjdx6/is_there_a_good_local_model_to_translate_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwjdx6/is_there_a_good_local_model_to_translate_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T11:58:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvrib9</id>
    <title>mistralai/Voxtral-Mini-4B-Realtime-2602 Â· Hugging Face</title>
    <updated>2026-02-04T15:27:12+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvrib9/mistralaivoxtralmini4brealtime2602_hugging_face/"&gt; &lt;img alt="mistralai/Voxtral-Mini-4B-Realtime-2602 Â· Hugging Face" src="https://external-preview.redd.it/RirqAaXL1g9xgccy6jCj8FpDgCmNmT4kPmfCbcwIIl8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4fa31e09623598564a99beea3a398a9c824d4f9" title="mistralai/Voxtral-Mini-4B-Realtime-2602 Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Voxtral Mini 4B Realtime 2602 is a &lt;strong&gt;multilingual, realtime speech-transcription model&lt;/strong&gt; and among the first open-source solutions to achieve accuracy comparable to offline systems with a delay of &lt;strong&gt;&amp;lt;500ms&lt;/strong&gt;. It supports &lt;strong&gt;13 languages&lt;/strong&gt; and outperforms existing open-source baselines across a range of tasks, making it ideal for applications like voice assistants and live subtitling.&lt;/p&gt; &lt;p&gt;Built with a &lt;strong&gt;natively streaming architecture&lt;/strong&gt; and a custom causal audio encoder - it allows configurable transcription delays (240ms to 2.4s), enabling users to balance &lt;strong&gt;latency and accuracy&lt;/strong&gt; based on their needs. At a &lt;strong&gt;480ms delay&lt;/strong&gt;, it matches the performance of leading offline open-source transcription models, as well as realtime APIs.&lt;/p&gt; &lt;p&gt;As a &lt;strong&gt;4B-parameter model&lt;/strong&gt;, is optimized for &lt;strong&gt;on-device deployment&lt;/strong&gt;, requiring minimal hardware resources. It runs in realtime with on devices minimal hardware with throughput exceeding 12.5 tokens/second.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvrib9/mistralaivoxtralmini4brealtime2602_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvrib9/mistralaivoxtralmini4brealtime2602_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T15:27:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwg58c</id>
    <title>Qwen3 Coder Next poor performance on r9700s</title>
    <updated>2026-02-05T08:47:37+00:00</updated>
    <author>
      <name>/u/jdchmiel</name>
      <uri>https://old.reddit.com/user/jdchmiel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With ROCm 7.2 backend PP512 is only 53. Luckily Vulkan at least works, though I usually found ROCm to be faster for other models.&lt;/p&gt; &lt;p&gt;/AI/llama.cpp/build_v/bin/llama-bench -m /AI/models/qwen3/Qwen3-Coder-Next-MXFP4_MOE.gguf -ngl 999 -fa 1 -ncmoe 0 -d 0,4096,8192,16384,32768,65536,131072,262144 -ts 50/50/0 WARNING: radv is not a conformant Vulkan implementation, testing use only. WARNING: radv is not a conformant Vulkan implementation, testing use only. ggml_vulkan: Found 3 Vulkan devices: ggml_vulkan: 0 = AMD Radeon Graphics (RADV RAPHAEL_MENDOCINO) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 65536 | int dot: 0 | matrix cores: none ggml_vulkan: 1 = AMD Radeon AI PRO R9700 (RADV GFX1201) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 0 | matrix cores: KHR_coopmat ggml_vulkan: 2 = AMD Radeon AI PRO R9700 (RADV GFX1201) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 0 | matrix cores: KHR_coopmat&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;fa&lt;/th&gt; &lt;th&gt;ts&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;40.73 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;50.00/50.00&lt;/td&gt; &lt;td align="right"&gt;pp512&lt;/td&gt; &lt;td align="right"&gt;1009.95 Â± 100.92&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;40.73 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;50.00/50.00&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;42.35 Â± 0.54&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;40.73 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;50.00/50.00&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d4096&lt;/td&gt; &lt;td align="right"&gt;1105.09 Â± 70.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;40.73 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;50.00/50.00&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d4096&lt;/td&gt; &lt;td align="right"&gt;42.02 Â± 0.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;40.73 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;50.00/50.00&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d8192&lt;/td&gt; &lt;td align="right"&gt;1108.28 Â± 60.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;40.73 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;50.00/50.00&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d8192&lt;/td&gt; &lt;td align="right"&gt;41.11 Â± 0.29&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;40.73 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;50.00/50.00&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d16384&lt;/td&gt; &lt;td align="right"&gt;1031.60 Â± 68.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;40.73 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;50.00/50.00&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d16384&lt;/td&gt; &lt;td align="right"&gt;39.71 Â± 0.57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;40.73 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;50.00/50.00&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d32768&lt;/td&gt; &lt;td align="right"&gt;922.88 Â± 50.92&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;40.73 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;50.00/50.00&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d32768&lt;/td&gt; &lt;td align="right"&gt;29.31 Â± 1.38&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;40.73 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;50.00/50.00&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d65536&lt;/td&gt; &lt;td align="right"&gt;700.26 Â± 70.46&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;40.73 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;50.00/50.00&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d65536&lt;/td&gt; &lt;td align="right"&gt;26.63 Â± 0.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;40.73 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;50.00/50.00&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d131072&lt;/td&gt; &lt;td align="right"&gt;547.93 Â± 70.52&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;40.73 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;50.00/50.00&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d131072&lt;/td&gt; &lt;td align="right"&gt;20.40 Â± 0.33&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;40.73 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;50.00/50.00&lt;/td&gt; &lt;td align="right"&gt;pp512 @ d262144&lt;/td&gt; &lt;td align="right"&gt;363.09 Â± 41.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3next 80B.A3B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;40.73 GiB&lt;/td&gt; &lt;td align="right"&gt;79.67 B&lt;/td&gt; &lt;td&gt;Vulkan&lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td&gt;50.00/50.00&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d262144&lt;/td&gt; &lt;td align="right"&gt;16.77 Â± 0.48&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;build: 11fb327bf (7941)&lt;/p&gt; &lt;p&gt;compared to almost 50% larger oss 120b: | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Vulkan | 999 | 1 | 50.00/50.00 | pp512 | 1415.58 Â± 89.00 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Vulkan | 999 | 1 | 50.00/50.00 | tg128 | 95.32 Â± 0.62 |&lt;/p&gt; &lt;p&gt;Are others seeing similar? I think something is off with ROCm on my system now, perhaps it is impacting these numbers too as they are all quite a bit lower than other dual r9700 numbers I have seen, but the relative speed between the smaller vs larger model is surprising. I thought they were both approx same number of active parameters, 3b for qwen and 5.1 for gpt oss 120b, so that would also imply qwen should be faster than it is?? Or is there a fundamental difference I am not catching?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jdchmiel"&gt; /u/jdchmiel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwg58c/qwen3_coder_next_poor_performance_on_r9700s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwg58c/qwen3_coder_next_poor_performance_on_r9700s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwg58c/qwen3_coder_next_poor_performance_on_r9700s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T08:47:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwichp</id>
    <title>Qwen3 TTS Streaming workflow help</title>
    <updated>2026-02-05T11:01:31+00:00</updated>
    <author>
      <name>/u/RateRoutine2268</name>
      <uri>https://old.reddit.com/user/RateRoutine2268</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Guys,&lt;br /&gt; Noob here , im thinking of using Qwen3 TTS for voice agent poc` , and need help on the streaming part , does it supports stream ingestion &amp;amp; generation (as soon as it get response from llm it starts generating audio that can also be streamed for real time ), look at qwen3-tts i couldn't find any implementation or examples of such scenarios,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RateRoutine2268"&gt; /u/RateRoutine2268 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwichp/qwen3_tts_streaming_workflow_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwichp/qwen3_tts_streaming_workflow_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwichp/qwen3_tts_streaming_workflow_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T11:01:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvrc59</id>
    <title>Some hard lessons learned building a private H100 cluster (Why PCIe servers failed us for training)</title>
    <updated>2026-02-04T15:20:42+00:00</updated>
    <author>
      <name>/u/NTCTech</name>
      <uri>https://old.reddit.com/user/NTCTech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;sup&gt;Just wanted to dump some notes here after spending the last few months architecting a private training stack (70B+ param models. We initially tried to save budget by looking at standard PCIe servers instead of the HGX/SXM form factors, and honestly, the &amp;quot;paper math&amp;quot; vs. reality was a brutal wake-up call.&lt;/sup&gt;)&lt;/p&gt; &lt;p&gt;&lt;sup&gt;Thought this might save someone else the headache if you're trying to move from inference to actual training runs on-prem.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;&lt;sup&gt;1. The &amp;quot;NVLink Tax&amp;quot; isn't optional for training. We tried to model this out with PCIe Gen5, but the math just falls apart. When you're doing All-Reduce ops across nodes, PCIe caps out at \&lt;/sup&gt;128 GB/s. NVLink is pushing ~900 GB/s. If you cheap out here, you basically end up with expensive GPUs sitting idle, waiting for data. For inference, PCIe is totally fine. For training, itâ€™s a bottleneck that kills your ROI.)&lt;/p&gt; &lt;p&gt;&lt;sup&gt;2. Storage checkpoints are violent. This was the biggest surprise. Everyone talks about GPU VRAM, but nobody warned us about the checkpoint writes. A 175B model dumps a \&lt;/sup&gt;2.5TB checkpoint. To keep the GPUs from stalling, you need to write that to disk in under a minute. Our standard NFS filer absolutely choked. We had to look at parallel filesystems (Weka/VAST or local NVMe raid just to survive the write bursts.))&lt;/p&gt; &lt;p&gt;&lt;sup&gt;3. You don't need InfiniBand, but Ethernet is annoying. We didn't have the budget/staff for an InfiniBand fabric, so we went with RoCEv2 on standard switches. It works, but itâ€™s finicky. One silent buffer overflow or a misconfigured PFC (Priority Flow Control setting can stall the whole cluster. If you go Ethernet, monitor your pause frames religiously.&lt;/sup&gt;)&lt;/p&gt; &lt;p&gt;&lt;sup&gt;Anyway, I wrote up a longer deep dive with the specific diagrams and our decision framework for &amp;quot;Sandbox vs Production&amp;quot; builds if anyone is interested. Link is pinned in my profile.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;&lt;sup&gt;Happy to answer questions on the networking side - that RoCEv2 tuning took years off my life.&lt;/sup&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NTCTech"&gt; /u/NTCTech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvrc59/some_hard_lessons_learned_building_a_private_h100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvrc59/some_hard_lessons_learned_building_a_private_h100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvrc59/some_hard_lessons_learned_building_a_private_h100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T15:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwcahn</id>
    <title>Use ANY TTS Engine with ANY AI Chat System</title>
    <updated>2026-02-05T05:06:42+00:00</updated>
    <author>
      <name>/u/DepartmentHorror7998</name>
      <uri>https://old.reddit.com/user/DepartmentHorror7998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm really not trying to self-promote here, but I was able to solve a TTS problem for myself and thought it might benefit others. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Like many of you, I have been very dissatisfied with state of AI voice, such as the empty promises of ChatGPT advanced voice mode and the very limited implementation of TTS among all of the main AI chat apps. Even with local LLMs, it's difficult to juggle starting an OpenAI TTS server, starting open-webui, starting the LLM with llama.cpp/LMStudio, and then connecting all of those things together. There are, of course, one-stop-shop apps like oobabooga that bundle everything together, but what if I want to sometimes use TTS on ChatGPT or sometimes use it on Claude as well.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;When thinking about how all of these things could be better integrated, it hit me. Every major AI chat UI has a little &amp;quot;Copy to Clipboard&amp;quot; button. Like every single one of them have that button, even locally with LMStudio. What if the TTS engine didn't expose an OpenAI TTS server, but instead just listened to your clipboard and ran TTS whenever you copied something. &lt;/p&gt; &lt;p&gt;So that's what I built. I call it AnyTTS and Claude helped me vibe code this in a week. The TTS engines are like plugins so if a new TTS model comes out next week, it can easily be integrated as a new TTSEngine plugin.&lt;/p&gt; &lt;p&gt;Here is the link to my repo: &lt;a href="https://github.com/bns25/any-tts"&gt;bns25/any-tts: AnyTTS - Use any TTS engine with any AI platform&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think. There will definitely be bugs, but hopefully this gives people a starting point and gets the juices flowing for supporting a simpler integration of LLM and TTS systems.&lt;/p&gt; &lt;p&gt;Unfortunately, it supports only Windows right now. But someone could easily adapt the idea to their own OS. Feel free to copy my code as you wish.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DepartmentHorror7998"&gt; /u/DepartmentHorror7998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwcahn/use_any_tts_engine_with_any_ai_chat_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwcahn/use_any_tts_engine_with_any_ai_chat_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwcahn/use_any_tts_engine_with_any_ai_chat_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T05:06:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qw6rwc</id>
    <title>I built a tool to visualize LLM workflows as interactive and shareable graphs</title>
    <updated>2026-02-05T00:55:57+00:00</updated>
    <author>
      <name>/u/Cyanosistaken</name>
      <uri>https://old.reddit.com/user/Cyanosistaken</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw6rwc/i_built_a_tool_to_visualize_llm_workflows_as/"&gt; &lt;img alt="I built a tool to visualize LLM workflows as interactive and shareable graphs" src="https://external-preview.redd.it/N3U4aTg5N3Zwa2hnMZamVz7bJmXM-USGdY_dhCyJiLc44FJ8QD5RU8S3ljgg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93b4cb715f5ce539b498094ba9f1092db8f74ef7" title="I built a tool to visualize LLM workflows as interactive and shareable graphs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! &lt;/p&gt; &lt;p&gt;I built Codag - an open source VSCode extension to visualize LLM workflows natively in your codebase. I kept on getting lost with the sheer amount of code that agents were output, and what better way of keeping track than to visualize it? &lt;/p&gt; &lt;p&gt;It supports OpenAI, Anthropic, Gemini, LangChain, LangGraph, CrewAI + more, and works with Python, TypeScript, Go, Rust, Java + more. &lt;/p&gt; &lt;p&gt;The demo video visualizes Vercel's AIChatbot repo. &lt;/p&gt; &lt;p&gt;Codag's link is in the comments, would love feedback from anyone building agents or multi-step LLM pipelines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cyanosistaken"&gt; /u/Cyanosistaken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/e9x23c6vpkhg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw6rwc/i_built_a_tool_to_visualize_llm_workflows_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qw6rwc/i_built_a_tool_to_visualize_llm_workflows_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T00:55:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qw8ord</id>
    <title>Why do companies release "SOTA" models when the code is just a TODO list? My night wasted on Tencent's Youtu-VL-4B.</title>
    <updated>2026-02-05T02:19:24+00:00</updated>
    <author>
      <name>/u/MadPelmewka</name>
      <uri>https://old.reddit.com/user/MadPelmewka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw8ord/why_do_companies_release_sota_models_when_the/"&gt; &lt;img alt="Why do companies release &amp;quot;SOTA&amp;quot; models when the code is just a TODO list? My night wasted on Tencent's Youtu-VL-4B." src="https://a.thumbs.redditmedia.com/DBvnsS5atMTiuTFe3ikSawYF1v0rEWf-C_c7jXWt5E8.jpg" title="Why do companies release &amp;quot;SOTA&amp;quot; models when the code is just a TODO list? My night wasted on Tencent's Youtu-VL-4B." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was browsing Hugging Face trending models as usual to see what's new, and I saw &lt;a href="https://huggingface.co/tencent/Youtu-VL-4B-Instruct"&gt;Tencent/Youtu-VL-4B-Instruct&lt;/a&gt;. The README looks amazing. It describes a hybrid VLM that can do everything: Object Detection, Semantic Segmentation, Grounding, etc. I immediately thought: &lt;em&gt;&amp;quot;Cool, finally a potential replacement or competitor to&lt;/em&gt; &lt;a href="https://huggingface.co/collections/microsoft/florence"&gt;Florence-2&lt;/a&gt;&lt;em&gt;.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I specifically needed high-quality segmentation to create a dataset for my scenario. So I tried to run it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Reality:&lt;/strong&gt; The model was released raw. Right now, it's just a standard VLM that can only describe what's in the image. There is &lt;strong&gt;NO information&lt;/strong&gt; about this on the model's main Hugging Face page. I had to dig for the truth, which I only found in the &lt;a href="https://github.com/TencentCloudADP/youtu-vl?tab=readme-ov-file#todo-list"&gt;GitHub TODO List&lt;/a&gt; and &lt;strong&gt;in the&lt;/strong&gt; &lt;a href="https://huggingface.co/tencent/Youtu-Parsing/discussions/2#697acfb8037b0052e316ae70"&gt;Community tab of ANOTHER model&lt;/a&gt;, where they mention that the current Transformers implementation is incomplete and full functionality requires a separate SDK...&lt;/p&gt; &lt;p&gt;The GitHub TODO list literally hides it:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;## TODO List - [ ] Support vLLM - [ ] Release recipes for various tasks - [ ] Release evaluation codes &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;They mask it behind vague phrases like &amp;quot;recipes for various tasks&amp;quot;. What is the point of publishing a model, boasting about SOTA benchmarks in the README, but hiding the fact that you can't actually test them because the code is missing? It feels misleading.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Bonus -&lt;/strong&gt; &lt;a href="https://huggingface.co/tencent/Youtu-VL-4B-Instruct/blob/main/LICENSE.txt"&gt;The License&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt; The license is essentially free/MIT-like, except for one line:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Youtu-VL IS NOT INTENDED FOR USE WITHIN THE EUROPEAN UNION.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So, it's trending on HF, but it's raw, &amp;quot;vision-centric&amp;quot; features are missing (or hidden in a non-existent SDK), and it's banned in the EU. Just a heads up before you waste your time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;UPD&lt;/strong&gt;: I want to clarify that Iâ€™m not &amp;quot;anti-Tencent.&amp;quot; In fact, I generally support their work and I'm excited about their research. My issue is strictly with transparency. When a README is filled with impressive &amp;quot;Key Features&amp;quot; and benchmarks, but fails to mention that the actual codebase is unfinished â€“ and then that model hits the HuggingFace trending list â€“ itâ€™s a problem. It leads to people wasting hours of their time on a product that isn't ready for the tasks it claims to solve.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;UPD2 (Important):&lt;/strong&gt; It turns out that the &amp;quot;Vision-Centric&amp;quot; features (Detection and Segmentation) &lt;em&gt;are&lt;/em&gt; actually functional in the current release, but they are effectively &amp;quot;undocumented&amp;quot; on Hugging Face. I managed to get them working by using the specific prompts found deep in the Arxiv paper (thanks to &lt;a href="/u/MitsotakiShogun"&gt;u/MitsotakiShogun&lt;/a&gt; for the nudge!).&lt;/p&gt; &lt;p&gt;Interestingly, I had previously fed the paper to Gemini as context, but it failed to extract the necessary info. Only when I started a fresh chat and explicitly told it to focus &lt;em&gt;strictly&lt;/em&gt; on &amp;quot;inference prompts for detection and segmentation&amp;quot; did it find the correct formats.&lt;/p&gt; &lt;p&gt;However, this doesn't change the fact that the developer experience is currently a mess:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Broken &amp;quot;Quickstart&amp;quot;:&lt;/strong&gt; The official code snippet on Hugging Face literally contains Python syntax errors. You can't even run the basic description example without fixing it first.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hidden Documentation:&lt;/strong&gt; Why force users to read a 70-page research paper just to find the basic prompts needed to run the model's core features? It would have been trivial to include these prompt examples in a &amp;quot;collapsible/spoiler&amp;quot; section in the README.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Attention Hell &amp;amp; Compilation Issues:&lt;/strong&gt; The documentation only mentions &lt;code&gt;flash_attention_2&lt;/code&gt;. I spent the entire night trying to compile it on a Blackwell instance, but the process was a nightmareâ€”RAM usage would balloon uncontrollably until the system crashed. To make matters worse, &lt;code&gt;sdpa&lt;/code&gt; &lt;strong&gt;(Scaled Dot Product Attention) doesn't seem to work properly here&lt;/strong&gt;, leaving you with either the compilation lottery of Flash-Attn or the painfully slow &lt;code&gt;eager&lt;/code&gt; mode.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Of course, &lt;strong&gt;there is a high probability that Iâ€™m just an inexperienced user/an idiot and the fault lies entirely on my end&lt;/strong&gt;, but in my experience, a &amp;quot;Quickstart&amp;quot; model trending on HF is usually more straightforward. If a corporate giant like Tencent releases a model in 2026, Iâ€™d expect at least a working &lt;code&gt;sdpa&lt;/code&gt; fallback and a README without syntax errors.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Bottom line:&lt;/strong&gt; Don't include a &amp;quot;Quickstart&amp;quot; section if it's neither &amp;quot;quick&amp;quot; nor &amp;quot;starting&amp;quot; anything without a deep dive into Arxiv. Tencent has released some great weights, but the way theyâ€™ve packaged this for the community is incredibly sloppy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MadPelmewka"&gt; /u/MadPelmewka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qw8ord"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qw8ord/why_do_companies_release_sota_models_when_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qw8ord/why_do_companies_release_sota_models_when_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T02:19:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwisld</id>
    <title>vLLM-Omni paper is out â€” up to 91.4% JCT reduction for any-to-any multimodal serving (tested with Qwen-Image-2512)</title>
    <updated>2026-02-05T11:26:25+00:00</updated>
    <author>
      <name>/u/still_debugging_note</name>
      <uri>https://old.reddit.com/user/still_debugging_note</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwisld/vllmomni_paper_is_out_up_to_914_jct_reduction_for/"&gt; &lt;img alt="vLLM-Omni paper is out â€” up to 91.4% JCT reduction for any-to-any multimodal serving (tested with Qwen-Image-2512)" src="https://b.thumbs.redditmedia.com/fwuQJBBgAjnQ7NMd5tOYI0zJFhDqirXaHdo6h7gdbQc.jpg" title="vLLM-Omni paper is out â€” up to 91.4% JCT reduction for any-to-any multimodal serving (tested with Qwen-Image-2512)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The vLLM team just released the vLLM-Omni paper on arXiv: &lt;a href="https://arxiv.org/abs/2602.02204"&gt;https://arxiv.org/abs/2602.02204&lt;/a&gt;&lt;/p&gt; &lt;p&gt;vLLM-Omni is designed for any-to-any multimodal models that jointly handle text, images, video, and audio â€” which is where serving starts to get really painful in practice.&lt;/p&gt; &lt;p&gt;It documents their system design for serving &lt;em&gt;any-to-any multimodal models&lt;/em&gt; â€” think pipelines that mix AR LLMs, diffusion models, encoders, etc., instead of assuming a single paradigm.&lt;/p&gt; &lt;p&gt;A few things that stood out: stage-based graph decomposition for pipelines, per-stage batching, and flexible GPU allocation across stages â€” makes serving any-to-any multimodal models much cleaner and faster.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4lzqx6ldrnhg1.png?width=717&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12957425682c9438946b61d9f1a554eec7e851ae"&gt;https://preview.redd.it/4lzqx6ldrnhg1.png?width=717&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12957425682c9438946b61d9f1a554eec7e851ae&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Iâ€™ve actually tested vLLM-Omni with Qwen-Image-2512 â€” comparable GPU memory to diffusers, but much faster generation ðŸ‘‡&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zho8tpassnhg1.png?width=405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa46ed99b93ebd6638c9e4dc7b05840d2cca18af"&gt;https://preview.redd.it/zho8tpassnhg1.png?width=405&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa46ed99b93ebd6638c9e4dc7b05840d2cca18af&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/still_debugging_note"&gt; /u/still_debugging_note &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwisld/vllmomni_paper_is_out_up_to_914_jct_reduction_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwisld/vllmomni_paper_is_out_up_to_914_jct_reduction_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwisld/vllmomni_paper_is_out_up_to_914_jct_reduction_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T11:26:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvq0xe</id>
    <title>Bashing Ollama isnâ€™t just a pleasure, itâ€™s a duty</title>
    <updated>2026-02-04T14:29:48+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvq0xe/bashing_ollama_isnt_just_a_pleasure_its_a_duty/"&gt; &lt;img alt="Bashing Ollama isnâ€™t just a pleasure, itâ€™s a duty" src="https://preview.redd.it/ad5zhvq0nhhg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b9fa62de0e64a6887124b87e66b3b99b2942107" title="Bashing Ollama isnâ€™t just a pleasure, itâ€™s a duty" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ad5zhvq0nhhg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvq0xe/bashing_ollama_isnt_just_a_pleasure_its_a_duty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvq0xe/bashing_ollama_isnt_just_a_pleasure_its_a_duty/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T14:29:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwkk20</id>
    <title>Huggingface down but online?</title>
    <updated>2026-02-05T12:56:21+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwkk20/huggingface_down_but_online/"&gt; &lt;img alt="Huggingface down but online?" src="https://preview.redd.it/zjgxqj4ebohg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1acdac94ab180afea16559baeeaf59de769d3824" title="Huggingface down but online?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;does it work for you?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zjgxqj4ebohg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwkk20/huggingface_down_but_online/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwkk20/huggingface_down_but_online/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T12:56:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwgyrn</id>
    <title>Best "Deep research" for local LLM in 2026 - platforms/tools/interface/setups</title>
    <updated>2026-02-05T09:39:18+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwgyrn/best_deep_research_for_local_llm_in_2026/"&gt; &lt;img alt="Best &amp;quot;Deep research&amp;quot; for local LLM in 2026 - platforms/tools/interface/setups" src="https://preview.redd.it/ffio9l5h0nhg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44645626277aa899f8e9caa47376f578527edc62" title="Best &amp;quot;Deep research&amp;quot; for local LLM in 2026 - platforms/tools/interface/setups" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using the &lt;strong&gt;Deep research&lt;/strong&gt; function from ChatGPT quite a lot since it came out.&lt;/p&gt; &lt;p&gt;I love it, but every month I use the limit in the first 2-3 days... so I was wondering if anyone else has any tips or setups they use for running something similar to Deep research -- on local LLM.&lt;/p&gt; &lt;p&gt;I have a decent setup of 3x3090, so I can run big-ish models (gpt-oss-120b or GLM Air) at VRAM speed or 30b models in Q8 (if precision is more important for deep research).&lt;/p&gt; &lt;p&gt;I've been using OpenWebUI + local SearXNG so fart. It works ok for simple &amp;quot;read this webpage and summarise&amp;quot; but it's far from the accuracy you get from a search&lt;strong&gt;analyze&lt;/strong&gt;search loop -- the way Deep research acts.&lt;/p&gt; &lt;p&gt;Any suggestions would help, thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ffio9l5h0nhg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwgyrn/best_deep_research_for_local_llm_in_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwgyrn/best_deep_research_for_local_llm_in_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T09:39:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwbmct</id>
    <title>Qwen3-Coder-Next on RTX 5060 Ti 16 GB - Some numbers</title>
    <updated>2026-02-05T04:33:49+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About 2 weeks ago, I posted about running GLM-4.7-Flash on 16 GB of VRAM here &lt;a href="http://www.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/"&gt;www.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/&lt;/a&gt;. And here we go, today, let's squeeze an even bigger model into the poor rig.&lt;/p&gt; &lt;p&gt;Hardware: - AMD Ryzen 7 7700X - RAM 32 GB DDR5-6000 - RTX 5060 Ti 16 GB&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF?show_file_info=Qwen3-Coder-Next-Q3_K_M.gguf"&gt;unsloth/Qwen3-Coder-Next-GGUF Q3_K_M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Llama.cpp version: &lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7940"&gt;llama.cpp@b7940&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The llamap.cpp command:&lt;/p&gt; &lt;p&gt;&lt;code&gt; llama-server -m ./Qwen3-Coder-Next-Q3_K_M.gguf -c 32768 -np 1 -t 8 --temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01 --jinja --fit on -fa 1 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;When I started, I didn't expect much, given that my best result for GLM-4.7-Flash was something ~300 t/s pp and 14 t/s gen. Maybe I'll end up with a lot of OOM and crash.&lt;/p&gt; &lt;p&gt;But, to my surprise, the card was able to pull it well!&lt;/p&gt; &lt;p&gt;When llama.cpp is fully loaded, it takes &lt;strong&gt;15.1 GB&lt;/strong&gt; GPU memory, and &lt;strong&gt;30.2 GB&lt;/strong&gt; RAM. The rig is almost at its memory limit.&lt;/p&gt; &lt;p&gt;During prompt processing, GPU usage was about &lt;strong&gt;35%&lt;/strong&gt;, and CPU usage was about &lt;strong&gt;15%&lt;/strong&gt;. During token generation, that's &lt;strong&gt;45%&lt;/strong&gt; for the GPU, and &lt;strong&gt;25%-45%&lt;/strong&gt; CPU. So perhaps there are some room to squeeze in some tuning here.&lt;/p&gt; &lt;p&gt;Does it run? Yes, and it's quite fast for a 5060!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Metric&lt;/th&gt; &lt;th&gt;Task 2 (Large Context)&lt;/th&gt; &lt;th&gt;Task 190 (Med Context)&lt;/th&gt; &lt;th&gt;Task 327 (Small Context)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Prompt Eval (Prefill)&lt;/td&gt; &lt;td&gt;154.08 t/s&lt;/td&gt; &lt;td&gt;225.14 t/s&lt;/td&gt; &lt;td&gt;118.98 t/s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Generation (Decode)&lt;/td&gt; &lt;td&gt;16.90 t/s&lt;/td&gt; &lt;td&gt;16.82 t/s&lt;/td&gt; &lt;td&gt;18.46 t/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The above run was with a 32k context size. Later on, I tried again with a 64k context size, the speed did not change much.&lt;/p&gt; &lt;p&gt;Is it usable? I'd say yes, not Opus 4.5 or Gemini Flash usable, but I think it's pretty close to my experience when Claude Sonnet 3.7 or 4 was still a thing.&lt;/p&gt; &lt;p&gt;One thing that sticks out is, this model uses way less tool calls than Opus, so it feels fast. It seems to read the whole file all at once when needed, rather than grepping every 200 lines like the Claude brothers.&lt;/p&gt; &lt;p&gt;One-shot something seems to work pretty well, until it runs into bugs. In my example, I asked the model to create a web-based chess game with a Python backend, connected via WebSocket. The model showed that it can debug the problem by jumping back and forth between frontend and backend code very well.&lt;/p&gt; &lt;p&gt;When facing a problem, it will first hypothesize a cause, then work its way through the code to verify that. Then there will be a lot of &amp;quot;But wait&amp;quot;, &amp;quot;Hold on&amp;quot;, followed by a tool call to read some files, and then changing directions. Sometimes it works. Sometimes, it was just burning through the tokens and ended up reaching the context limit. Maybe because I was using Q3_K_M, and higher quants will have better quality here.&lt;/p&gt; &lt;p&gt;Some screenshots:&lt;/p&gt; &lt;p&gt;&lt;a href="https://gist.github.com/user-attachments/assets/8d074a76-c441-42df-b146-0ae291af17df"&gt;https://gist.github.com/user-attachments/assets/8d074a76-c441-42df-b146-0ae291af17df&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://gist.github.com/user-attachments/assets/3aa3a845-96cd-4b23-b6d9-1255036106db"&gt;https://gist.github.com/user-attachments/assets/3aa3a845-96cd-4b23-b6d9-1255036106db&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can see the Claude session logs and llama.cpp logs of the run here &lt;a href="https://gist.github.com/huytd/6b1e9f2271dd677346430c1b92893b57"&gt;https://gist.github.com/huytd/6b1e9f2271dd677346430c1b92893b57&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwbmct/qwen3codernext_on_rtx_5060_ti_16_gb_some_numbers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwbmct/qwen3codernext_on_rtx_5060_ti_16_gb_some_numbers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwbmct/qwen3codernext_on_rtx_5060_ti_16_gb_some_numbers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T04:33:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwboqn</id>
    <title>Google Research announces Sequential Attention: Making AI models leaner and faster without sacrificing accuracy</title>
    <updated>2026-02-05T04:37:05+00:00</updated>
    <author>
      <name>/u/Fear_ltself</name>
      <uri>https://old.reddit.com/user/Fear_ltself</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwboqn/google_research_announces_sequential_attention/"&gt; &lt;img alt="Google Research announces Sequential Attention: Making AI models leaner and faster without sacrificing accuracy" src="https://external-preview.redd.it/Xfy8b5oz8xAgNpbj0L9Mmjzxactj5HdaKRFOmBPu0YE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9dad5b13e20f57d64f5fc0bbc7415c9f4186b1d" title="Google Research announces Sequential Attention: Making AI models leaner and faster without sacrificing accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fear_ltself"&gt; /u/Fear_ltself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://research.google/blog/sequential-attention-making-ai-models-leaner-and-faster-without-sacrificing-accuracy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwboqn/google_research_announces_sequential_attention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwboqn/google_research_announces_sequential_attention/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T04:37:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
