<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-20T16:39:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1prei1a</id>
    <title>[Research] Help us quantify "Vibe Check" - How we actually evaluate models!</title>
    <updated>2025-12-20T13:35:22+00:00</updated>
    <author>
      <name>/u/Fickle-Medium-3751</name>
      <uri>https://old.reddit.com/user/Fickle-Medium-3751</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, PhD student here!&lt;/p&gt; &lt;p&gt;We all know the pattern - a model tops the leaderboard, but when you run it locally, it feels.. off. We all rely on our own (and other users) &amp;quot;vibe checks&amp;quot;.&lt;/p&gt; &lt;p&gt;Our lab is working on a paper to formalize these &amp;quot;vibe checks&amp;quot;. We aren't selling a tool or a new model. We are trying to scientifically map the signals you look for when you decide if a model is actually good or bad.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How can you help?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We need ground-truth data from the people who actually use these models (you!). We‚Äôve put together a short 5-10 min survey to capture your evaluation intuition.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Link to Survey:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://forms.gle/HqE6R9Vevq9zzk3c6"&gt;https://forms.gle/HqE6R9Vevq9zzk3c6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We promise to post the results here once the study is done so the community can use it too!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fickle-Medium-3751"&gt; /u/Fickle-Medium-3751 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prei1a/research_help_us_quantify_vibe_check_how_we/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prei1a/research_help_us_quantify_vibe_check_how_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prei1a/research_help_us_quantify_vibe_check_how_we/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T13:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1prhoeq</id>
    <title>Strix Halo with eGPU</title>
    <updated>2025-12-20T16:00:51+00:00</updated>
    <author>
      <name>/u/Miserable-Dare5090</name>
      <uri>https://old.reddit.com/user/Miserable-Dare5090</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got a strix halo and I was hoping to link an eGPU but I have a concern. i‚Äôm looking for advice from others who have tried to improve the prompt processing in the strix halo this way. &lt;/p&gt; &lt;p&gt;At the moment, I have a 3090ti Founders. I already use it via oculink with a standard PC tower that has a 4060ti 16gb, and layer splitting with Llama allows me to run Nemotron 3 or Qwen3 30b at 50 tokens per second with very decent pp speeds. &lt;/p&gt; &lt;p&gt;but obviously this is Nvidia. I‚Äôm not sure how much harder it would be to get it running in the Ryzen with an oculink.&lt;/p&gt; &lt;p&gt;Has anyone tried eGPU set ups in the strix halo, and would an AMD card be easier to configure and use? The 7900 xtx is at a decent price right now, and I am sure the price will jump very soon. &lt;/p&gt; &lt;p&gt;Any suggestions welcome. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Miserable-Dare5090"&gt; /u/Miserable-Dare5090 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prhoeq/strix_halo_with_egpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prhoeq/strix_halo_with_egpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prhoeq/strix_halo_with_egpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T16:00:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqyphk</id>
    <title>Framework says that a single AI datacenter consumes enough memory for millions of laptops</title>
    <updated>2025-12-19T22:46:46+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quote: the boom in AI data center construction and server manufacturing is consuming immense amounts of memory. A single rack of NVIDIA‚Äôs GB300 solution uses 20TB of HBM3E and 17TB of LPDDR5X. That‚Äôs enough LPDDR5x for a thousand laptops, and an &lt;a href="https://en.wikipedia.org/wiki/AI_datacenter"&gt;AI-focused datacenter&lt;/a&gt; is loaded with thousands of these racks!&lt;/p&gt; &lt;p&gt;/end quote&lt;/p&gt; &lt;p&gt;thousand * thousands = millions&lt;/p&gt; &lt;p&gt;&lt;a href="https://frame.work/pl/en/blog/updates-on-memory-pricing-and-navigating-the-volatile-memory-market"&gt;https://frame.work/pl/en/blog/updates-on-memory-pricing-and-navigating-the-volatile-memory-market&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The good news: there hasn't been new recent price increase for strix halo systems, but there was some 8 weeks in response to U.S. tariff increases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqyphk/framework_says_that_a_single_ai_datacenter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqyphk/framework_says_that_a_single_ai_datacenter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqyphk/framework_says_that_a_single_ai_datacenter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T22:46:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqoldt</id>
    <title>Chinese researchers unveil "LightGen": An all-optical chip that outperforms Nvidia‚Äôs A100 by 100x</title>
    <updated>2025-12-19T15:55:23+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New research from SJTU and Tsinghua (these are top tier labs, not slopmonsters like East China Normal University etc.).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.science.org/doi/10.1126/science.adv7434"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoldt/chinese_researchers_unveil_lightgen_an_alloptical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoldt/chinese_researchers_unveil_lightgen_an_alloptical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T15:55:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1prctcc</id>
    <title>RAG Re-Ranking</title>
    <updated>2025-12-20T12:03:13+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the classic RAG setup you have a retrieval stage followed by a re-ranking stage. The retrieval stage usually consists of an embedding model which takes in chunks and outputs vectors, followed by a nearest neighbour search on those vectors to select perhaps 50-200 chunks (from a corpus that could be 10,000 chunks or more.) Classic text search algorithms such as BM25 also get thrown in to propose more chunks as a sort of hybrid RAG. Sometimes a graph database query will be used, with the main example being Cypher for Neo4j, to propose more chunks, in so-called ‚Äúgraph-RAG‚Äù. There is also the late-interaction ColBERT method which is beyond the scope of this post.&lt;/p&gt; &lt;p&gt;But what about the re-ranking stage?&lt;/p&gt; &lt;p&gt;We have 50-200 curated chunks selected by the retrieval step, what can we do to ‚Äúre-rank‚Äù them or increase their quality to help our LLMs?&lt;/p&gt; &lt;p&gt;The main paradigm seems to be point-wise scoring between chunk and query, and sometimes pair-wise scoring between two chunks and a query, followed by quicksort/bubblesort etc.&lt;/p&gt; &lt;p&gt;The re-ranking models used to be encoder-only Bert-likes such as Roberta and Deberta, sometimes literally Bert, partly due to the popularity of the Sentence Transformers library. I have seen the encoder-decoder model T5 used also. After this era decoder-only specialist re-ranking models appeared, in a similar way to how decoder-only models have taken over most other areas of NLP. After that era there has now been some moves into so-called ‚Äúagentic re-ranking‚Äù.&lt;/p&gt; &lt;p&gt;What do you think about the development of re-ranking so far?&lt;/p&gt; &lt;p&gt;What models and methods do you think are good?&lt;/p&gt; &lt;p&gt;Have you seen any interesting developments, articles or github libraries on this topic lately?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prctcc/rag_reranking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prctcc/rag_reranking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prctcc/rag_reranking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T12:03:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqn0vq</id>
    <title>GLM 4.7 is Coming?</title>
    <updated>2025-12-19T14:52:18+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqn0vq/glm_47_is_coming/"&gt; &lt;img alt="GLM 4.7 is Coming?" src="https://preview.redd.it/206mfj3dc68g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f01e8b54d3347827d6980eb1b0cbc7453cfd2d9c" title="GLM 4.7 is Coming?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/30876"&gt;https://github.com/vllm-project/vllm/pull/30876&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/206mfj3dc68g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqn0vq/glm_47_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqn0vq/glm_47_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T14:52:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqpj29</id>
    <title>Career Advice in AI ‚Äî Notes from an Andrew Ng Lecture</title>
    <updated>2025-12-19T16:31:46+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqpj29/career_advice_in_ai_notes_from_an_andrew_ng/"&gt; &lt;img alt="Career Advice in AI ‚Äî Notes from an Andrew Ng Lecture" src="https://preview.redd.it/cu5vt8lnt68g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=efe9182c042a4013c8b0a1760f4ff5a1ac022efd" title="Career Advice in AI ‚Äî Notes from an Andrew Ng Lecture" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;[1] A Golden Age for AI Careers&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Andrew Ng emphasizes that this is the best time ever to build a career in AI. He notes that the complexity of tasks AI can handle is doubling approximately every seven months, meaning progress is accelerating, not slowing down.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[2] The Power of AI Coding Tools&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Staying on the ‚Äúfrontier‚Äù of coding tools (like Cursor, Claude, and Gemini) is crucial. Being even half a generation behind in your tooling makes you significantly less productive in the current market.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[3] The ‚ÄúProduct Management Bottleneck‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Because AI has made writing code so much cheaper and faster, the bottleneck has shifted to deciding what to build. Engineers who can talk to users, develop empathy, and handle product management (PM) tasks are the fastest-moving individuals in Silicon Valley today.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[4] Surround Yourself with the Right People&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Success is highly predicted by the people you surround yourself with. Ng encourages building a ‚Äúrich connective tissue‚Äù of friends and colleagues to share insights that aren‚Äôt yet published on the internet.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[5] Team Over Brand&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;When job hunting, the specific team and people you work with day-to-day are more important than the company‚Äôs ‚Äúhot brand.‚Äù Avoid companies that refuse to tell you which team you will join before you sign.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[6] Go and Build Stuff&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Andrew Ng‚Äôs number one piece of advice is to simply &lt;strong&gt;go and build stuff&lt;/strong&gt;. The cost of failure is low (losing a weekend), but the learning and demonstration of skill are invaluable.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[7] The Value of Hard Work&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Andrew Ng encourages working hard, defining it not just by hours but by output and passion for building.&lt;/p&gt; &lt;p&gt;Video - &lt;a href="https://www.youtube.com/watch?v=AuZoDsNmG_s"&gt;https://www.youtube.com/watch?v=AuZoDsNmG_s&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cu5vt8lnt68g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqpj29/career_advice_in_ai_notes_from_an_andrew_ng/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqpj29/career_advice_in_ai_notes_from_an_andrew_ng/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T16:31:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqegcr</id>
    <title>Realist meme of the year!</title>
    <updated>2025-12-19T06:49:54+00:00</updated>
    <author>
      <name>/u/Slight_Tone_2188</name>
      <uri>https://old.reddit.com/user/Slight_Tone_2188</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/"&gt; &lt;img alt="Realist meme of the year!" src="https://preview.redd.it/8oge3a2by38g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4697e9a87c50f3f170db7e87eccd27363c505dc" title="Realist meme of the year!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slight_Tone_2188"&gt; /u/Slight_Tone_2188 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8oge3a2by38g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T06:49:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqui9l</id>
    <title>FlashHead: Up to 50% faster token generation on top of other techniques like quantization</title>
    <updated>2025-12-19T19:49:00+00:00</updated>
    <author>
      <name>/u/Any_Frame9721</name>
      <uri>https://old.reddit.com/user/Any_Frame9721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqui9l/flashhead_up_to_50_faster_token_generation_on_top/"&gt; &lt;img alt="FlashHead: Up to 50% faster token generation on top of other techniques like quantization" src="https://external-preview.redd.it/_fiYnJOiLFMjXWAHyOC4PQuzh6t1PbyOv347pBjd4tc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79b959a9401497de2b2ba109df8da0de73c9e45f" title="FlashHead: Up to 50% faster token generation on top of other techniques like quantization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;We have developed FlashHead, an architectural innovation for SLMs offering up to 50% more tokens per second &lt;strong&gt;on top&lt;/strong&gt; of other techniques like quantization. It is a drop-in replacement for the language model head. It works by replacing the expensive lm head with the FlashHead layer that uses information retrieval to identify the next token efficiently with perfect accuracy compared to the baseline model.&lt;/p&gt; &lt;p&gt;Try it with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install embedl-models python -m embedl.models.vllm.demo \ --model embedl/Llama-3.2-3B-Instruct-FlashHead-W4A16 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Llama 3.2 1B Instruct benchmark on Ada Gen 3500 GPU (batch size = 1)&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/embedl/Llama-3.2-1B-Instruct-FlashHead#token-generation-speed-rtx-3500-ada-batch-size--1"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Precision&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Tokens/sec&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Speedup vs BF16&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;BF16 baseline&lt;/td&gt; &lt;td align="left"&gt;130&lt;/td&gt; &lt;td align="left"&gt;1.0√ó&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;FlashHead (Embedl)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;163&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1.25√ó&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;W4A16 baseline&lt;/td&gt; &lt;td align="left"&gt;278&lt;/td&gt; &lt;td align="left"&gt;2.14√ó&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;FlashHead W4A16 (Embedl)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;485&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3.73√ó&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The models perform as their original counterparts, but faster. We have tried to make it as friction-less as possible to use via our vLLM integration, we would love to hear feedback. The GitHub repo is &lt;a href="https://github.com/embedl/embedl-models"&gt;https://github.com/embedl/embedl-models&lt;/a&gt;, &lt;/p&gt; &lt;p&gt;We are a Swedish startup working on efficient AI. We also have a free Edge AI Hub that allows users to run models on mobile devices (Android, iOS) &lt;a href="https://hub.embedl.com"&gt;https://hub.embedl.com&lt;/a&gt; , feel free to join our Slack (#llm channel) for discussions or open an issue on GitHub&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Frame9721"&gt; /u/Any_Frame9721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/embedl/models"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqui9l/flashhead_up_to_50_faster_token_generation_on_top/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqui9l/flashhead_up_to_50_faster_token_generation_on_top/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T19:49:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqy2bq</id>
    <title>Devstral 2 (with Mistral's Vibe) vs Sonnet 4.5 (Claude Code) on SWE-bench: 37.6% vs 39.8% (within statistical error)</title>
    <updated>2025-12-19T22:18:46+00:00</updated>
    <author>
      <name>/u/Constant_Branch282</name>
      <uri>https://old.reddit.com/user/Constant_Branch282</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Update: Just discovered my script wasn't passing the --model flag correctly. Claude Code was using automatic model selection (typically Opus), not Sonnet 4.5 as I stated. This actually makes the results more significant - Devstral 2 matched Anthropic's best model in my test, not just Sonnet&lt;/p&gt; &lt;p&gt;I ran Mistral's Vibe (Devstral 2) against Claude Code (Sonnet 4.5) on SWE-bench-verified-mini - 45 real GitHub issues, 10 attempts each, 900 total runs.&lt;/p&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;p&gt;Claude Code (Sonnet 4.5) : 39.8% (37.3% - 42.2%)&lt;/p&gt; &lt;p&gt;Vibe (Devstral 2): 37.6% (35.1% - 40.0%)&lt;/p&gt; &lt;p&gt;The gap is within statistical error. An open-weight model I can run on my Strix Halo is matching Anthropic's recent model.&lt;/p&gt; &lt;p&gt;Vibe was also faster - 296s mean vs Claude's 357s.&lt;/p&gt; &lt;p&gt;The variance finding (applies to both): about 40% of test cases were inconsistent across runs. Same agent, same bug, different outcomes. Even on cases solved 10/10, patch sizes varied up to 8x.&lt;/p&gt; &lt;p&gt;Full writeup with charts and methodology: &lt;a href="https://blog.kvit.app/posts/variance-claude-vibe/"&gt;https://blog.kvit.app/posts/variance-claude-vibe/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Constant_Branch282"&gt; /u/Constant_Branch282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqy2bq/devstral_2_with_mistrals_vibe_vs_sonnet_45_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqy2bq/devstral_2_with_mistrals_vibe_vs_sonnet_45_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqy2bq/devstral_2_with_mistrals_vibe_vs_sonnet_45_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T22:18:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pre9a8</id>
    <title>I built a 2.2MB transformer that learns First-Order Logic (662-symbol vocab, runs on a Pi)</title>
    <updated>2025-12-20T13:23:26+00:00</updated>
    <author>
      <name>/u/No_Corgi1789</name>
      <uri>https://old.reddit.com/user/No_Corgi1789</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/aiprojects/?f=flair_name%3A%22%3Atrophy%3A%20Project%20Showcase%22"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôve been experimenting with whether tiny transformers can learn useful structure in &lt;strong&gt;formal logic&lt;/strong&gt; without the usual ‚Äújust scale it‚Äù approach.&lt;/p&gt; &lt;p&gt;This repo trains a small transformer (&lt;strong&gt;566K params / ~2.2MB FP32&lt;/strong&gt;) on a &lt;strong&gt;next-symbol prediction&lt;/strong&gt; task over &lt;strong&gt;First-Order Logic&lt;/strong&gt; sequences using a &lt;strong&gt;662-symbol vocabulary&lt;/strong&gt; (625 numerals + FOL operators + category tokens). The main idea is &lt;strong&gt;compositional tokens&lt;/strong&gt; for indexed entities (e.g. &lt;code&gt;VAR 42 ‚Üí [VAR, 4, 2]&lt;/code&gt;) so the model doesn‚Äôt need a separate embedding for every variable/predicate ID.&lt;/p&gt; &lt;p&gt;It‚Äôs &lt;strong&gt;not a theorem prover&lt;/strong&gt; and it‚Äôs not trying to replace grammars ‚Äî the aim is learning &lt;em&gt;preferences&lt;/em&gt; among valid continuations (and generalising under shifts like unseen indices / longer formulas), with something small enough to run on constrained devices.&lt;/p&gt; &lt;p&gt;If anyone‚Äôs interested, I‚Äôd love feedback on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;whether the token design makes sense / obvious improvements&lt;/li&gt; &lt;li&gt;what baselines or benchmarks you‚Äôd expect&lt;/li&gt; &lt;li&gt;what would make this genuinely useful (e.g. premise‚Üíconclusion, solver-in-the-loop, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;article explainer: &lt;a href="https://medium.com/@trippitytrip/the-2-2mb-transformer-that-learns-logic-402da6b0e4f2"&gt;https://medium.com/@trippitytrip/the-2-2mb-transformer-that-learns-logic-402da6b0e4f2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;github: &lt;a href="https://github.com/tripptytrip/Symbolic-Transformers"&gt;https://github.com/tripptytrip/Symbolic-Transformers&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Corgi1789"&gt; /u/No_Corgi1789 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pre9a8/i_built_a_22mb_transformer_that_learns_firstorder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pre9a8/i_built_a_22mb_transformer_that_learns_firstorder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pre9a8/i_built_a_22mb_transformer_that_learns_firstorder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T13:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1prf3iz</id>
    <title>How does a 'reasoning' model reason</title>
    <updated>2025-12-20T14:04:39+00:00</updated>
    <author>
      <name>/u/El_90</name>
      <uri>https://old.reddit.com/user/El_90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thanks for reading, I'm new to the field&lt;/p&gt; &lt;p&gt;If a local LLM is just a statistics model, how can it be described as reasoning or 'following instructions'&lt;/p&gt; &lt;p&gt;I had assume COT, or validation would be handled by logic, which I would have assumed is the LLM loader (e.g. Ollama)&lt;/p&gt; &lt;p&gt;Many thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/El_90"&gt; /u/El_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prf3iz/how_does_a_reasoning_model_reason/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prf3iz/how_does_a_reasoning_model_reason/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prf3iz/how_does_a_reasoning_model_reason/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T14:04:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pr3sxi</id>
    <title>Nine US lawmakers urge DoD to add DeepSeek to list of companies aligned with China's military</title>
    <updated>2025-12-20T02:59:56+00:00</updated>
    <author>
      <name>/u/PortlandPoly</name>
      <uri>https://old.reddit.com/user/PortlandPoly</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PortlandPoly"&gt; /u/PortlandPoly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://eposnix.com/us-lawmakers-deepseek-xiaomi-pentagon-1260h-list/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pr3sxi/nine_us_lawmakers_urge_dod_to_add_deepseek_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pr3sxi/nine_us_lawmakers_urge_dod_to_add_deepseek_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T02:59:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqoi6i</id>
    <title>Qwen released Qwen-Image-Layered on Hugging face.</title>
    <updated>2025-12-19T15:51:45+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoi6i/qwen_released_qwenimagelayered_on_hugging_face/"&gt; &lt;img alt="Qwen released Qwen-Image-Layered on Hugging face." src="https://b.thumbs.redditmedia.com/WT_uezmugp_bMYr9okz4OYqH1W02XtM64SzwTE-NCms.jpg" title="Qwen released Qwen-Image-Layered on Hugging face." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Layered"&gt;https://huggingface.co/Qwen/Qwen-Image-Layered&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Photoshop-grade layering Physically isolated RGBA layers with true native editability Prompt-controlled structure Explicitly specify 3‚Äì10 layers ‚Äî from coarse layouts to fine-grained details Infinite decomposition Keep drilling down: layers within layers, to any depth of detail&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pqoi6i"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoi6i/qwen_released_qwenimagelayered_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoi6i/qwen_released_qwenimagelayered_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T15:51:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pr54as</id>
    <title>Just pushed M2.1 through a 3D particle system. InsaneÔºÅ</title>
    <updated>2025-12-20T04:07:57+00:00</updated>
    <author>
      <name>/u/srtng</name>
      <uri>https://old.reddit.com/user/srtng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pr54as/just_pushed_m21_through_a_3d_particle_system/"&gt; &lt;img alt="Just pushed M2.1 through a 3D particle system. InsaneÔºÅ" src="https://external-preview.redd.it/YmR1ZjY4bWFhYThnMUdT_NEOfC8ECakI0ZIQpaGOzpZqq9FHGjXFcuqPFEGL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6affb174619dc33c19c34f4cec3f6cd14cbb391b" title="Just pushed M2.1 through a 3D particle system. InsaneÔºÅ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just tested an interactive 3D particle system with MiniMax M2.1.&lt;/p&gt; &lt;p&gt;Yeah‚Ä¶ this is insane. üî•&lt;/p&gt; &lt;p&gt;And I know you‚Äôre gonna ask ‚Äî M2.1 is coming soooooon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/srtng"&gt; /u/srtng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dsz04yuaaa8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pr54as/just_pushed_m21_through_a_3d_particle_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pr54as/just_pushed_m21_through_a_3d_particle_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T04:07:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1prgi41</id>
    <title>AMD Radeon AI PRO R9700 benchmarks with ROCm and Vulkan and llama.cpp</title>
    <updated>2025-12-20T15:10:10+00:00</updated>
    <author>
      <name>/u/Finguili</name>
      <uri>https://old.reddit.com/user/Finguili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prgi41/amd_radeon_ai_pro_r9700_benchmarks_with_rocm_and/"&gt; &lt;img alt="AMD Radeon AI PRO R9700 benchmarks with ROCm and Vulkan and llama.cpp" src="https://b.thumbs.redditmedia.com/XbjSf-XY5RQvB4Yp042RgNWFOniLVUqUixGoKSlUncc.jpg" title="AMD Radeon AI PRO R9700 benchmarks with ROCm and Vulkan and llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently in comments to various posts about R9700 many people asked for benchmarks, so I took some of my time to run them.&lt;/p&gt; &lt;p&gt;Spec: AMD Ryzen 7 5800X (16) @ 5.363 GHz, 64 GiB DDR4 RAM @ 3600 MHz, AMD Radeon AI PRO R9700.&lt;/p&gt; &lt;p&gt;Software is running on Arch Linux with ROCm 7.1.1 (my Comfy install is still using a slightly older PyTorch nightly release with ROCm 7.0).&lt;/p&gt; &lt;p&gt;Disclaimer: I was lazy and instructed the LLM to generate Python scripts for plots. It‚Äôs possible that it hallucinated some values while copying tables into the script.&lt;/p&gt; &lt;h1&gt;Novel summarisation&lt;/h1&gt; &lt;p&gt;Let‚Äôs start with a practical task to see how it performs in the real world. The LLM is instructed to summarise each chapter of a 120k-word novel individually, with a script parallelising calls to the local API to take advantage of batched inference. The batch size was selected so that there is at least 15k ctx per request.&lt;/p&gt; &lt;p&gt;Mistral Small: batch=3; 479s total time; ~14k output words&lt;/p&gt; &lt;p&gt;gpt-oss 20B: batch=32; 113s; 18k output words (exluding reasoning)&lt;/p&gt; &lt;p&gt;Below are detailed benchmarks per model, with some diffusion models at the end. I run them with logical batch size (`-b` flag) set to 1024, as I noticed that prompt processing slowed much more with default value 2048, though I only measured in for Mistral Small, so it might not be optimal for every model.&lt;/p&gt; &lt;p&gt;TLDR is that ROCm usually has slightly faster prompt processing and takes less performance hit from long context, while Vulkan usually has slightly faster tg.&lt;/p&gt; &lt;h1&gt;gpt-oss 20B MXFP4&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3qwo0a77jd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c0f02f7d3832d70457663b8692f16e8b4846d8"&gt;https://preview.redd.it/3qwo0a77jd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c0f02f7d3832d70457663b8692f16e8b4846d8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Batched ROCm (&lt;code&gt;llama-batched-bench -m ~/Pobrane/gpt-oss-20b-mxfp4.gguf -ngl 99 --ctx-size 262144 -fa 1 -npp 1024 -ntg 512 -npl 1,2,4,8,16,32 -b 1024&lt;/code&gt;):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;PP&lt;/th&gt; &lt;th align="left"&gt;TG&lt;/th&gt; &lt;th align="left"&gt;B&lt;/th&gt; &lt;th align="left"&gt;N_KV&lt;/th&gt; &lt;th align="left"&gt;T_PP s&lt;/th&gt; &lt;th align="left"&gt;S_PP t/s&lt;/th&gt; &lt;th align="left"&gt;T_TG s&lt;/th&gt; &lt;th align="left"&gt;S_TG t/s&lt;/th&gt; &lt;th align="left"&gt;T s&lt;/th&gt; &lt;th align="left"&gt;S t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1536&lt;/td&gt; &lt;td align="left"&gt;0.356&lt;/td&gt; &lt;td align="left"&gt;2873.01&lt;/td&gt; &lt;td align="left"&gt;3.695&lt;/td&gt; &lt;td align="left"&gt;138.55&lt;/td&gt; &lt;td align="left"&gt;4.052&lt;/td&gt; &lt;td align="left"&gt;379.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;3072&lt;/td&gt; &lt;td align="left"&gt;0.439&lt;/td&gt; &lt;td align="left"&gt;4662.19&lt;/td&gt; &lt;td align="left"&gt;6.181&lt;/td&gt; &lt;td align="left"&gt;165.67&lt;/td&gt; &lt;td align="left"&gt;6.620&lt;/td&gt; &lt;td align="left"&gt;464.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;6144&lt;/td&gt; &lt;td align="left"&gt;0.879&lt;/td&gt; &lt;td align="left"&gt;4658.93&lt;/td&gt; &lt;td align="left"&gt;7.316&lt;/td&gt; &lt;td align="left"&gt;279.92&lt;/td&gt; &lt;td align="left"&gt;8.196&lt;/td&gt; &lt;td align="left"&gt;749.67&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;12288&lt;/td&gt; &lt;td align="left"&gt;1.784&lt;/td&gt; &lt;td align="left"&gt;4592.69&lt;/td&gt; &lt;td align="left"&gt;8.943&lt;/td&gt; &lt;td align="left"&gt;458.02&lt;/td&gt; &lt;td align="left"&gt;10.727&lt;/td&gt; &lt;td align="left"&gt;1145.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;24576&lt;/td&gt; &lt;td align="left"&gt;3.584&lt;/td&gt; &lt;td align="left"&gt;4571.87&lt;/td&gt; &lt;td align="left"&gt;12.954&lt;/td&gt; &lt;td align="left"&gt;632.37&lt;/td&gt; &lt;td align="left"&gt;16.538&lt;/td&gt; &lt;td align="left"&gt;1486.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;49152&lt;/td&gt; &lt;td align="left"&gt;7.211&lt;/td&gt; &lt;td align="left"&gt;4544.13&lt;/td&gt; &lt;td align="left"&gt;19.088&lt;/td&gt; &lt;td align="left"&gt;858.36&lt;/td&gt; &lt;td align="left"&gt;26.299&lt;/td&gt; &lt;td align="left"&gt;1869.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Batched Vulkan:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;PP&lt;/th&gt; &lt;th align="left"&gt;TG&lt;/th&gt; &lt;th align="left"&gt;B&lt;/th&gt; &lt;th align="left"&gt;N_KV&lt;/th&gt; &lt;th align="left"&gt;T_PP s&lt;/th&gt; &lt;th align="left"&gt;S_PP t/s&lt;/th&gt; &lt;th align="left"&gt;T_TG s&lt;/th&gt; &lt;th align="left"&gt;S_TG t/s&lt;/th&gt; &lt;th align="left"&gt;T s&lt;/th&gt; &lt;th align="left"&gt;S t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1536&lt;/td&gt; &lt;td align="left"&gt;0.415&lt;/td&gt; &lt;td align="left"&gt;2465.21&lt;/td&gt; &lt;td align="left"&gt;2.997&lt;/td&gt; &lt;td align="left"&gt;170.84&lt;/td&gt; &lt;td align="left"&gt;3.412&lt;/td&gt; &lt;td align="left"&gt;450.12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;3072&lt;/td&gt; &lt;td align="left"&gt;0.504&lt;/td&gt; &lt;td align="left"&gt;4059.63&lt;/td&gt; &lt;td align="left"&gt;8.555&lt;/td&gt; &lt;td align="left"&gt;119.70&lt;/td&gt; &lt;td align="left"&gt;9.059&lt;/td&gt; &lt;td align="left"&gt;339.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;6144&lt;/td&gt; &lt;td align="left"&gt;1.009&lt;/td&gt; &lt;td align="left"&gt;4059.83&lt;/td&gt; &lt;td align="left"&gt;10.528&lt;/td&gt; &lt;td align="left"&gt;194.53&lt;/td&gt; &lt;td align="left"&gt;11.537&lt;/td&gt; &lt;td align="left"&gt;532.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;12288&lt;/td&gt; &lt;td align="left"&gt;2.042&lt;/td&gt; &lt;td align="left"&gt;4011.59&lt;/td&gt; &lt;td align="left"&gt;13.553&lt;/td&gt; &lt;td align="left"&gt;302.22&lt;/td&gt; &lt;td align="left"&gt;15.595&lt;/td&gt; &lt;td align="left"&gt;787.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;24576&lt;/td&gt; &lt;td align="left"&gt;4.102&lt;/td&gt; &lt;td align="left"&gt;3994.08&lt;/td&gt; &lt;td align="left"&gt;16.222&lt;/td&gt; &lt;td align="left"&gt;505.01&lt;/td&gt; &lt;td align="left"&gt;20.324&lt;/td&gt; &lt;td align="left"&gt;1209.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;49152&lt;/td&gt; &lt;td align="left"&gt;8.265&lt;/td&gt; &lt;td align="left"&gt;3964.67&lt;/td&gt; &lt;td align="left"&gt;19.416&lt;/td&gt; &lt;td align="left"&gt;843.85&lt;/td&gt; &lt;td align="left"&gt;27.681&lt;/td&gt; &lt;td align="left"&gt;1775.67&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w3ebchfajd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c56e5c540b4463088b24d0521b8493056c52ca7a"&gt;https://preview.redd.it/w3ebchfajd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c56e5c540b4463088b24d0521b8493056c52ca7a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/87a0hmgajd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=be7b601a45118f3dde71c061c6cda8d1ad27a9bb"&gt;https://preview.redd.it/87a0hmgajd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=be7b601a45118f3dde71c061c6cda8d1ad27a9bb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Long context ROCm:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;3859.15 ¬± 370.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;142.62 ¬± 1.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;3344.57 ¬± 15.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;134.42 ¬± 0.83&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;2617.02 ¬± 17.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;127.62 ¬± 1.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;1819.82 ¬± 36.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;119.04 ¬± 0.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;999.01 ¬± 72.31&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;101.80 ¬± 0.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d48000&lt;/td&gt; &lt;td align="left"&gt;680.86 ¬± 83.60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="left"&gt;89.82 ¬± 0.67&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Long context Vulkan:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;2648.20 ¬± 201.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;173.13 ¬± 3.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;3012.69 ¬± 12.39&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;167.87 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;2295.56 ¬± 13.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;159.13 ¬± 0.63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;1566.27 ¬± 25.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;148.42 ¬± 0.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;919.79 ¬± 5.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;129.22 ¬± 0.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d48000&lt;/td&gt; &lt;td align="left"&gt;518.21 ¬± 1.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;11.27 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="left"&gt;114.46 ¬± 1.20&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;gpt-oss 120B MXFP4&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l8npf7xbjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ac8c09710db6e2020521d8f9385ac5ac587c80c"&gt;https://preview.redd.it/l8npf7xbjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ac8c09710db6e2020521d8f9385ac5ac587c80c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3c4y66wbjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92dbf2bbbf3fddaf2f09d4c2558d12b36961cc2c"&gt;https://preview.redd.it/3c4y66wbjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92dbf2bbbf3fddaf2f09d4c2558d12b36961cc2c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Long context ROCm (&lt;code&gt;llama-bench -m ~/Pobrane/gpt-oss-120b-mxfp4-00001-of-00003.gguf --n-cpu-moe 21 -ngl 99 -fa 1 -r 2 -d 0,4000,8000,16000,32000,48000 -b 1024&lt;/code&gt;)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;279.07 ¬± 133.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;26.79 ¬± 0.20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;498.33 ¬± 6.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;26.47 ¬± 0.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;479.48 ¬± 4.16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;25.97 ¬± 0.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;425.65 ¬± 2.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;25.31 ¬± 0.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;339.71 ¬± 10.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;23.86 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d48000&lt;/td&gt; &lt;td align="left"&gt;277.79 ¬± 12.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="left"&gt;22.53 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Long context Vulkan:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;211.64 ¬± 7.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;26.80 ¬± 0.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;220.63 ¬± 7.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;26.54 ¬± 0.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;203.32 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;26.10 ¬± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;187.31 ¬± 4.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;25.37 ¬± 0.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;163.22 ¬± 5.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;24.06 ¬± 0.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d48000&lt;/td&gt; &lt;td align="left"&gt;137.56 ¬± 2.33&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;59.02 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="left"&gt;22.83 ¬± 0.08&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Mistral Small 3.2 24B Q8&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f9q4ocndjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a7ff45c2062852fd2c2ab9b08c1ec39904ee5ea1"&gt;https://preview.redd.it/f9q4ocndjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a7ff45c2062852fd2c2ab9b08c1ec39904ee5ea1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h6w60gndjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09680b3edb2f48de6924984f2ef78d25b2240e0a"&gt;https://preview.redd.it/h6w60gndjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09680b3edb2f48de6924984f2ef78d25b2240e0a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Long context (&lt;code&gt;llama-bench -m mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf -ngl 99 -fa 1 -r 2 -d 0,4000,8000,16000,32000,48000 -b 1024&lt;/code&gt;):&lt;/p&gt; &lt;p&gt;ROCm:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1563.27 ¬± 0.78&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;23.59 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;1146.39 ¬± 0.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;23.03 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;852.24 ¬± 55.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;22.41 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;557.38 ¬± 79.97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;21.38 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;351.07 ¬± 31.77&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;19.48 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d48000&lt;/td&gt; &lt;td align="left"&gt;256.75 ¬± 16.98&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="left"&gt;17.90 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Vulkan:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1033.43 ¬± 0.92&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;24.47 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;705.07 ¬± 84.33&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;23.69 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;558.55 ¬± 58.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;22.94 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;404.23 ¬± 35.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;21.66 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;257.74 ¬± 12.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;11.25 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d48000&lt;/td&gt; &lt;td align="left"&gt;167.42 ¬± 6.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 13B Q8_0&lt;/td&gt; &lt;td align="left"&gt;23.33 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="left"&gt;10.93 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qnvl209fjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d5d1872bd290d3b31a32e9e0940dc9f4306947f"&gt;https://preview.redd.it/qnvl209fjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d5d1872bd290d3b31a32e9e0940dc9f4306947f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Batched ROCm (&lt;code&gt;llama-batched-bench -m ~/Pobrane/mistralai_Mistral-Small-3.2-24B-Instruct-2506-Q8_0.gguf -ngl 99 --ctx-size 32798 -fa 1 -npp 1024 -ntg 512 -npl 1,2,4,8 -b 1024&lt;/code&gt;):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;PP&lt;/th&gt; &lt;th align="left"&gt;TG&lt;/th&gt; &lt;th align="left"&gt;B&lt;/th&gt; &lt;th align="left"&gt;N_KV&lt;/th&gt; &lt;th align="left"&gt;T_PP s&lt;/th&gt; &lt;th align="left"&gt;S_PP t/s&lt;/th&gt; &lt;th align="left"&gt;T_TG s&lt;/th&gt; &lt;th align="left"&gt;S_TG t/s&lt;/th&gt; &lt;th align="left"&gt;T s&lt;/th&gt; &lt;th align="left"&gt;S t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1536&lt;/td&gt; &lt;td align="left"&gt;0.719&lt;/td&gt; &lt;td align="left"&gt;1423.41&lt;/td&gt; &lt;td align="left"&gt;21.891&lt;/td&gt; &lt;td align="left"&gt;23.39&lt;/td&gt; &lt;td align="left"&gt;22.610&lt;/td&gt; &lt;td align="left"&gt;67.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;3072&lt;/td&gt; &lt;td align="left"&gt;1.350&lt;/td&gt; &lt;td align="left"&gt;1516.62&lt;/td&gt; &lt;td align="left"&gt;24.193&lt;/td&gt; &lt;td align="left"&gt;42.33&lt;/td&gt; &lt;td align="left"&gt;25.544&lt;/td&gt; &lt;td align="left"&gt;120.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;6144&lt;/td&gt; &lt;td align="left"&gt;2.728&lt;/td&gt; &lt;td align="left"&gt;1501.73&lt;/td&gt; &lt;td align="left"&gt;25.139&lt;/td&gt; &lt;td align="left"&gt;81.47&lt;/td&gt; &lt;td align="left"&gt;27.867&lt;/td&gt; &lt;td align="left"&gt;220.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;12288&lt;/td&gt; &lt;td align="left"&gt;5.468&lt;/td&gt; &lt;td align="left"&gt;1498.09&lt;/td&gt; &lt;td align="left"&gt;33.595&lt;/td&gt; &lt;td align="left"&gt;121.92&lt;/td&gt; &lt;td align="left"&gt;39.063&lt;/td&gt; &lt;td align="left"&gt;314.57&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Batched Vulkan:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;PP&lt;/th&gt; &lt;th align="left"&gt;TG&lt;/th&gt; &lt;th align="left"&gt;B&lt;/th&gt; &lt;th align="left"&gt;N_KV&lt;/th&gt; &lt;th align="left"&gt;T_PP s&lt;/th&gt; &lt;th align="left"&gt;S_PP t/s&lt;/th&gt; &lt;th align="left"&gt;T_TG s&lt;/th&gt; &lt;th align="left"&gt;S_TG t/s&lt;/th&gt; &lt;th align="left"&gt;T s&lt;/th&gt; &lt;th align="left"&gt;S t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1536&lt;/td&gt; &lt;td align="left"&gt;1.126&lt;/td&gt; &lt;td align="left"&gt;909.50&lt;/td&gt; &lt;td align="left"&gt;21.095&lt;/td&gt; &lt;td align="left"&gt;24.27&lt;/td&gt; &lt;td align="left"&gt;22.221&lt;/td&gt; &lt;td align="left"&gt;69.12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;3072&lt;/td&gt; &lt;td align="left"&gt;2.031&lt;/td&gt; &lt;td align="left"&gt;1008.54&lt;/td&gt; &lt;td align="left"&gt;21.961&lt;/td&gt; &lt;td align="left"&gt;46.63&lt;/td&gt; &lt;td align="left"&gt;23.992&lt;/td&gt; &lt;td align="left"&gt;128.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;6144&lt;/td&gt; &lt;td align="left"&gt;4.089&lt;/td&gt; &lt;td align="left"&gt;1001.70&lt;/td&gt; &lt;td align="left"&gt;23.051&lt;/td&gt; &lt;td align="left"&gt;88.85&lt;/td&gt; &lt;td align="left"&gt;27.140&lt;/td&gt; &lt;td align="left"&gt;226.38&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;12288&lt;/td&gt; &lt;td align="left"&gt;8.196&lt;/td&gt; &lt;td align="left"&gt;999.45&lt;/td&gt; &lt;td align="left"&gt;29.695&lt;/td&gt; &lt;td align="left"&gt;137.94&lt;/td&gt; &lt;td align="left"&gt;37.891&lt;/td&gt; &lt;td align="left"&gt;324.30&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Qwen3 VL 32B Q5_K_L&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lg0s9f4gjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00da017f55bfd0ec1ac9477b2dc61bc907c591a4"&gt;https://preview.redd.it/lg0s9f4gjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00da017f55bfd0ec1ac9477b2dc61bc907c591a4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qivjlf4gjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52e7caf0e45a2a523d86eb22709b9f3a47807cba"&gt;https://preview.redd.it/qivjlf4gjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52e7caf0e45a2a523d86eb22709b9f3a47807cba&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Long context ROCm (&lt;code&gt;llama-bench -m ~/Pobrane/Qwen_Qwen3-VL-32B-Instruct-Q5_K_L.gguf -ngl 99 -fa 1 -r 2 -d 0,4000,8000,16000,32000,48000 -b 1024&lt;/code&gt;)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;796.33 ¬± 0.84&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;22.56 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;425.83 ¬± 128.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;21.11 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;354.85 ¬± 34.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;20.14 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;228.75 ¬± 14.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;18.46 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;134.29 ¬± 5.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;15.75 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Note: 48k doesn‚Äôt fit.&lt;/p&gt; &lt;p&gt;Long context Vulkan:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;424.14 ¬± 1.45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;23.93 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;300.68 ¬± 9.66&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;22.69 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;226.81 ¬± 11.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;21.65 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;152.41 ¬± 0.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;19.78 ¬± 0.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;80.38 ¬± 0.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3vl 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;22.06 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;10.39 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Gemma 3 27B Q6_K_L&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tcabncnhjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7765dc64ada8bf70e7603ceec0da012d6430dcbd"&gt;https://preview.redd.it/tcabncnhjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7765dc64ada8bf70e7603ceec0da012d6430dcbd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vll62lnhjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b42b7237c72fe36638a1550531e1aa0e76ac49ea"&gt;https://preview.redd.it/vll62lnhjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b42b7237c72fe36638a1550531e1aa0e76ac49ea&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Long context ROCm (&lt;code&gt;llama-bench -m ~/Pobrane/google_gemma-3-27b-it-Q6_K_L.gguf -ngl 99 -fa 1 -r 2 -d 0,4000,8000,16000,32000,48000 -b 1024&lt;/code&gt;)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;659.05 ¬± 0.33&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;23.25 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;582.29 ¬± 10.16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;21.04 ¬± 2.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;531.76 ¬± 40.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;22.20 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;478.30 ¬± 58.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;21.67 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;418.48 ¬± 51.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;20.71 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d48000&lt;/td&gt; &lt;td align="left"&gt;373.22 ¬± 40.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="left"&gt;19.78 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Long context Vulkan:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_batch&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;664.79 ¬± 0.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;24.63 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d4000&lt;/td&gt; &lt;td align="left"&gt;593.41 ¬± 12.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d4000&lt;/td&gt; &lt;td align="left"&gt;23.70 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d8000&lt;/td&gt; &lt;td align="left"&gt;518.78 ¬± 58.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d8000&lt;/td&gt; &lt;td align="left"&gt;23.18 ¬± 0.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d16000&lt;/td&gt; &lt;td align="left"&gt;492.78 ¬± 19.97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d16000&lt;/td&gt; &lt;td align="left"&gt;22.61 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d32000&lt;/td&gt; &lt;td align="left"&gt;372.34 ¬± 1.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d32000&lt;/td&gt; &lt;td align="left"&gt;21.26 ¬± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512 @ d48000&lt;/td&gt; &lt;td align="left"&gt;336.42 ¬± 19.47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3 27B Q6_K&lt;/td&gt; &lt;td align="left"&gt;20.96 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="left"&gt;20.15 ¬± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Gemma 2 9B BF16&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lnnp237jjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a386123bf9af1aaaa3e26680cb480d6f6debd7a"&gt;https://preview.redd.it/lnnp237jjd8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a386123bf9af1aaaa3e26680cb480d6f6debd7a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Batched ROCm (&lt;code&gt;llama-batched-bench -m ~/Pobrane/gemma2-test-bf16_0.gguf -ngl 99 --ctx-size 32798 -fa 1 -npp 1024 -ntg 512 -npl 1,2,4,8 -b 1024&lt;/code&gt;)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;PP&lt;/th&gt; &lt;th align="left"&gt;TG&lt;/th&gt; &lt;th align="left"&gt;B&lt;/th&gt; &lt;th align="left"&gt;N_KV&lt;/th&gt; &lt;th align="left"&gt;T_PP s&lt;/th&gt; &lt;th align="left"&gt;S_PP t/s&lt;/th&gt; &lt;th align="left"&gt;T_TG s&lt;/th&gt; &lt;th align="left"&gt;S_TG t/s&lt;/th&gt; &lt;th align="left"&gt;T s&lt;/th&gt; &lt;th align="left"&gt;S t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1536&lt;/td&gt; &lt;td align="left"&gt;2.145&lt;/td&gt; &lt;td align="left"&gt;477.39&lt;/td&gt; &lt;td align="left"&gt;17.676&lt;/td&gt; &lt;td align="left"&gt;28.97&lt;/td&gt; &lt;td align="left"&gt;19.821&lt;/td&gt; &lt;td align="left"&gt;77.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;3072&lt;/td&gt; &lt;td align="left"&gt;3.948&lt;/td&gt; &lt;td align="left"&gt;518.70&lt;/td&gt; &lt;td align="left"&gt;19.190&lt;/td&gt; &lt;td align="left"&gt;53.36&lt;/td&gt; &lt;td align="left"&gt;23.139&lt;/td&gt; &lt;td align="left"&gt;132.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;6144&lt;/td&gt; &lt;td align="left"&gt;7.992&lt;/td&gt; &lt;td align="left"&gt;512.50&lt;/td&gt; &lt;td align="left"&gt;25.012&lt;/td&gt; &lt;td align="left"&gt;81.88&lt;/td&gt; &lt;td align="left"&gt;33.004&lt;/td&gt; &lt;td align="left"&gt;186.16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;12288&lt;/td&gt; &lt;td align="left"&gt;16.025&lt;/td&gt; &lt;td align="left"&gt;511.20&lt;/td&gt; &lt;td align="left"&gt;27.818&lt;/td&gt; &lt;td align="left"&gt;147.24&lt;/td&gt; &lt;td align="left"&gt;43.844&lt;/td&gt; &lt;td align="left"&gt;280.27&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;For some reason this one has terribly slow prompt processing on ROCm.&lt;/p&gt; &lt;p&gt;Batched Vulkan:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;PP&lt;/th&gt; &lt;th align="left"&gt;TG&lt;/th&gt; &lt;th align="left"&gt;B&lt;/th&gt; &lt;th align="left"&gt;N_KV&lt;/th&gt; &lt;th align="left"&gt;T_PP s&lt;/th&gt; &lt;th align="left"&gt;S_PP t/s&lt;/th&gt; &lt;th align="left"&gt;T_TG s&lt;/th&gt; &lt;th align="left"&gt;S_TG t/s&lt;/th&gt; &lt;th align="left"&gt;T s&lt;/th&gt; &lt;th align="left"&gt;S t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1536&lt;/td&gt; &lt;td align="left"&gt;0.815&lt;/td&gt; &lt;td align="left"&gt;1256.70&lt;/td&gt; &lt;td align="left"&gt;18.187&lt;/td&gt; &lt;td align="left"&gt;28.15&lt;/td&gt; &lt;td align="left"&gt;19.001&lt;/td&gt; &lt;td align="left"&gt;80.84&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;3072&lt;/td&gt; &lt;td align="left"&gt;1.294&lt;/td&gt; &lt;td align="left"&gt;1582.42&lt;/td&gt; &lt;td align="left"&gt;19.690&lt;/td&gt; &lt;td align="left"&gt;52.01&lt;/td&gt; &lt;td align="left"&gt;20.984&lt;/td&gt; &lt;td align="left"&gt;146.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;6144&lt;/td&gt; &lt;td align="left"&gt;2.602&lt;/td&gt; &lt;td align="left"&gt;1574.33&lt;/td&gt; &lt;td align="left"&gt;23.380&lt;/td&gt; &lt;td align="left"&gt;87.60&lt;/td&gt; &lt;td align="left"&gt;25.982&lt;/td&gt; &lt;td align="left"&gt;236.47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1024&lt;/td&gt; &lt;td align="left"&gt;512&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;12288&lt;/td&gt; &lt;td align="left"&gt;5.220&lt;/td&gt; &lt;td align="left"&gt;1569.29&lt;/td&gt; &lt;td align="left"&gt;30.615&lt;/td&gt; &lt;td align="left"&gt;133.79&lt;/td&gt; &lt;td align="left"&gt;35.835&lt;/td&gt; &lt;td align="left"&gt;342.90&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Diffusion&lt;/h1&gt; &lt;p&gt;All using ComfyUI.&lt;/p&gt; &lt;p&gt;Z-image, prompt cached, 9 steps, 1024√ó1024: 7.5 s (6.3 s with torch compile), ~8.1 s with prompt processing.&lt;/p&gt; &lt;p&gt;SDXL, v-pred model, 1024√ó1024, 50 steps, Euler ancestral cfg++, batch 4: 44.5 s (Comfy shows 1.18 it/s, so 4.72 it/s after normalising for batch size and without counting VAE decode). With torch compile I get 41.2 s and 5 it/s after normalising for batch count.&lt;/p&gt; &lt;p&gt;Flux 2 dev fp8. Keep in mind that Comfy is unoptimised regarding RAM usage, and 64 GiB is simply not enough for such a large model ‚Äî without &lt;code&gt;--no-cache&lt;/code&gt; it tried to load Flux weights for half an hour, using most of my swap, until I gave up. With the aforementioned flag it works, but everything has to be re-executed each time you run the workflow, including loading from disk, which slows things down. This is the only benchmark where I include weight loading in the total time.&lt;/p&gt; &lt;p&gt;1024√ó1024, 30 steps, no reference image: 126.2 s, 2.58 s/it for diffusion. With one reference image it‚Äôs 220 s and 5.73 s/it.&lt;/p&gt; &lt;h1&gt;Various notes&lt;/h1&gt; &lt;p&gt;I also successfully finished full LoRA training of Gemma 2 9B using Unsloth. It was surprisingly quick, but perhaps that should be expected given the small dataset (about 70 samples and 4 epochs). While I don‚Äôt remember exactly how long it took, it was definitely measured in minutes rather than hours. The process was also smooth, although Unsloth warns that 4-bit QLoRA training is broken if you want to train something larger.&lt;/p&gt; &lt;p&gt;Temperatures are stable; memory can reach 90 ¬∞C, but I have yet to see the fans spinning at 100%. The card is also not as loud as some might suggest based on the blower fan design. It‚Äôs hard to judge exactly how loud it is, but it doesn‚Äôt feel much louder than my old RX 6700 XT, and you don‚Äôt really hear it outside the room.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Finguili"&gt; /u/Finguili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prgi41/amd_radeon_ai_pro_r9700_benchmarks_with_rocm_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prgi41/amd_radeon_ai_pro_r9700_benchmarks_with_rocm_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prgi41/amd_radeon_ai_pro_r9700_benchmarks_with_rocm_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T15:10:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pr20el</id>
    <title>Japan's Rakuten is going to release a 700B open weight model in Spring 2026</title>
    <updated>2025-12-20T01:29:37+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://news.yahoo.co.jp/articles/0fc312ec3386f87d65e797ab073db56c230757e1"&gt;https://news.yahoo.co.jp/articles/0fc312ec3386f87d65e797ab073db56c230757e1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope it works well in real life. Then it can not only be an alternative to the Chinese models. but also prompt the US companies to release big models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pr20el/japans_rakuten_is_going_to_release_a_700b_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pr20el/japans_rakuten_is_going_to_release_a_700b_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pr20el/japans_rakuten_is_going_to_release_a_700b_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T01:29:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1prh5jp</id>
    <title>A Raspberry Pi + eGPU isn't as dumb as I thought</title>
    <updated>2025-12-20T15:38:20+00:00</updated>
    <author>
      <name>/u/geerlingguy</name>
      <uri>https://old.reddit.com/user/geerlingguy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prh5jp/a_raspberry_pi_egpu_isnt_as_dumb_as_i_thought/"&gt; &lt;img alt="A Raspberry Pi + eGPU isn't as dumb as I thought" src="https://b.thumbs.redditmedia.com/6loQqYyrEG88VaZ_nKHTgCav_dnTkH81E4pJ_NvuO0w.jpg" title="A Raspberry Pi + eGPU isn't as dumb as I thought" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's a small selection of benchmarks from my &lt;a href="https://www.jeffgeerling.com/blog/2025/big-gpus-dont-need-big-pcs"&gt;blog post&lt;/a&gt;, I tested a variety of AMD and Nvidia cards on a Raspberry Pi CM5 using an eGPU dock (total system cost, cards excluded, around $350).&lt;/p&gt; &lt;p&gt;For larger models, the performance delta between the Pi and an Intel Core Ultra 265K PC build with 64GB of DDR5 RAM and PCIe Gen 5 was less than 5%. For llama 2 13B, the Pi was even faster for many Nvidia cards (why is that?).&lt;/p&gt; &lt;p&gt;For AMD, the Pi was much slower‚Äîto the point I'm pretty sure there's a driver issue or something the AMD drivers expect that the Pi isn't providing (yet... like a large BAR).&lt;/p&gt; &lt;p&gt;I publish all the llama-bench data in &lt;a href="https://github.com/geerlingguy/ai-benchmarks/issues?q=is%3Aissue%20state%3Aclosed"&gt;https://github.com/geerlingguy/ai-benchmarks/issues?q=is%3Aissue%20state%3Aclosed&lt;/a&gt; and multi-GPU benchmarks in &lt;a href="https://github.com/geerlingguy/ai-benchmarks/issues/44"&gt;https://github.com/geerlingguy/ai-benchmarks/issues/44&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geerlingguy"&gt; /u/geerlingguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1prh5jp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prh5jp/a_raspberry_pi_egpu_isnt_as_dumb_as_i_thought/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prh5jp/a_raspberry_pi_egpu_isnt_as_dumb_as_i_thought/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T15:38:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1prc2xb</id>
    <title>MiniMax M2.1 is Coming??</title>
    <updated>2025-12-20T11:17:31+00:00</updated>
    <author>
      <name>/u/BlackRice_hmz</name>
      <uri>https://old.reddit.com/user/BlackRice_hmz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prc2xb/minimax_m21_is_coming/"&gt; &lt;img alt="MiniMax M2.1 is Coming??" src="https://preview.redd.it/fdf74pqsec8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e9feb40e7806d93bd8a1075005bf3f47b34a4d6b" title="MiniMax M2.1 is Coming??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was checking vLLM recipes and saw they just added MiniMax M2.1. Thoughts?&lt;br /&gt; &lt;a href="https://github.com/vllm-project/recipes/pull/174"&gt;https://github.com/vllm-project/recipes/pull/174&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlackRice_hmz"&gt; /u/BlackRice_hmz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fdf74pqsec8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prc2xb/minimax_m21_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prc2xb/minimax_m21_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T11:17:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pr48qm</id>
    <title>Key Highlights of NVIDIA‚Äôs New Open-Source Vision-to-Action Model: NitroGen</title>
    <updated>2025-12-20T03:22:27+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pr48qm/key_highlights_of_nvidias_new_opensource/"&gt; &lt;img alt="Key Highlights of NVIDIA‚Äôs New Open-Source Vision-to-Action Model: NitroGen" src="https://external-preview.redd.it/dDloZG9jMTIxYThnMYYenHrEFODP-kaUVO32HiooUq-dF7OihjJxUG2oEn5A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85cfa0c43c9a2ca54b190aa4ad8ee4cd2217243e" title="Key Highlights of NVIDIA‚Äôs New Open-Source Vision-to-Action Model: NitroGen" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;NitroGen is a unified vision-to-action model designed to play video games directly from raw frames. It takes video game footage as input and outputs gamepad actions. &lt;/li&gt; &lt;li&gt;NitroGen is trained purely through large-scale imitation learning on videos of human gameplay. &lt;/li&gt; &lt;li&gt;NitroGen works best on games designed for gamepad controls (e.g., action, platformer, and racing games) and is less effective on games that rely heavily on mouse and keyboard (e.g., RTS, MOBA).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How this model works?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RGB frames are processed through a pre-trained vision transformer (SigLip2).&lt;/li&gt; &lt;li&gt;A diffusion matching transformer (DiT) then generates actions, conditioned on SigLip output.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model - &lt;a href="https://huggingface.co/nvidia/NitroGen"&gt;https://huggingface.co/nvidia/NitroGen&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9om52w021a8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pr48qm/key_highlights_of_nvidias_new_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pr48qm/key_highlights_of_nvidias_new_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T03:22:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1prcu0t</id>
    <title>Of course it works, in case you are wondering... and it's quite faster.</title>
    <updated>2025-12-20T12:04:22+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prcu0t/of_course_it_works_in_case_you_are_wondering_and/"&gt; &lt;img alt="Of course it works, in case you are wondering... and it's quite faster." src="https://preview.redd.it/p9tf12m7nc8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e07a1563bfc8d54447cad9ed61107030cf7aff50" title="Of course it works, in case you are wondering... and it's quite faster." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p9tf12m7nc8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1prcu0t/of_course_it_works_in_case_you_are_wondering_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1prcu0t/of_course_it_works_in_case_you_are_wondering_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T12:04:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pragtf</id>
    <title>Open source LLM tooling is getting eaten by big tech</title>
    <updated>2025-12-20T09:29:03+00:00</updated>
    <author>
      <name>/u/Inevitable_Wear_9107</name>
      <uri>https://old.reddit.com/user/Inevitable_Wear_9107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was using TGI for inference six months ago. Migrated to vLLM last month. Thought it was just me chasing better performance, then I read the LLM Landscape 2.0 report. Turns out 35% of projects from just three months ago already got replaced. This isn't just my stack. The whole ecosystem is churning.&lt;/p&gt; &lt;p&gt;The deeper I read, the crazier it gets. Manus blew up in March, OpenManus and OWL launched within weeks as open source alternatives, both are basically dead now. TensorFlow has been declining since 2019 and still hasn't hit bottom. The median project age in this space is 30 months.&lt;/p&gt; &lt;p&gt;Then I looked at what's gaining momentum. NVIDIA drops Dynamo, optimized for NVIDIA hardware. Google releases Gemini CLI with Google Cloud baked in. OpenAI ships Codex CLI that funnels you into their API. That's when it clicked.&lt;/p&gt; &lt;p&gt;Two years ago this space was chaotic but independent. Now the open source layer is becoming the customer acquisition layer. We're not choosing tools anymore. We're being sorted into ecosystems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable_Wear_9107"&gt; /u/Inevitable_Wear_9107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pragtf/open_source_llm_tooling_is_getting_eaten_by_big/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pragtf/open_source_llm_tooling_is_getting_eaten_by_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pragtf/open_source_llm_tooling_is_getting_eaten_by_big/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T09:29:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pre225</id>
    <title>1 gram of RAM die is more expensive than 1 gram of 16 karat gold rn</title>
    <updated>2025-12-20T13:13:37+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pre225/1_gram_of_ram_die_is_more_expensive_than_1_gram/"&gt; &lt;img alt="1 gram of RAM die is more expensive than 1 gram of 16 karat gold rn" src="https://external-preview.redd.it/OHRyMG44Nmd6YzhnMepdlrUNzDYlu9mVv7LdXRWHP9OWTLwciJpO2ibgF6zt.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9432c4b22b9b76b0a1b7f61e9560da759fe707c" title="1 gram of RAM die is more expensive than 1 gram of 16 karat gold rn" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xtvxfy5gzc8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pre225/1_gram_of_ram_die_is_more_expensive_than_1_gram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pre225/1_gram_of_ram_die_is_more_expensive_than_1_gram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-20T13:13:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp9w31</id>
    <title>AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio</title>
    <updated>2025-12-17T22:18:01+00:00</updated>
    <author>
      <name>/u/AIatMeta</name>
      <uri>https://old.reddit.com/user/AIatMeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We‚Äôre the research team behind the newest members of the Segment Anything collection of models: SAM 3 + SAM 3D + SAM Audio.&lt;/p&gt; &lt;p&gt;We‚Äôre excited to be here to talk all things SAM (sorry, we can‚Äôt share details on other projects or future work) and have members from across our team participating:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SAM 3 (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/segment-anything-model-3/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nikhila Ravi&lt;/li&gt; &lt;li&gt;Pengchuan Zhang&lt;/li&gt; &lt;li&gt;Shoubhik Debnath&lt;/li&gt; &lt;li&gt;Chay Ryali&lt;/li&gt; &lt;li&gt;Yuan-Ting Hu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM 3D (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-3d/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weiyao Wang&lt;/li&gt; &lt;li&gt;Sasha Sax&lt;/li&gt; &lt;li&gt;Xitong Yang&lt;/li&gt; &lt;li&gt;Jinkun Cao&lt;/li&gt; &lt;li&gt;Michelle Guo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM Audio (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-audio/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bowen Shi&lt;/li&gt; &lt;li&gt;Andros Tjandra&lt;/li&gt; &lt;li&gt;John Hoffman&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try SAM Audio, SAM 3D, and SAM 3 in the Segment Anything Playground: &lt;a href="https://go.meta.me/87b53b"&gt;https://go.meta.me/87b53b&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PROOF: &lt;a href="https://x.com/AIatMeta/status/2001429429898407977"&gt;https://x.com/AIatMeta/status/2001429429898407977&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT: Thanks to everyone who joined the AMA and for all the great conversation. We look forward to the next one!&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIatMeta"&gt; /u/AIatMeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T22:18:01+00:00</published>
  </entry>
</feed>
