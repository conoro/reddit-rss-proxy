<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-03T19:05:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q2rqom</id>
    <title>Local programming vs cloud</title>
    <updated>2026-01-03T10:50:17+00:00</updated>
    <author>
      <name>/u/Photo_Sad</name>
      <uri>https://old.reddit.com/user/Photo_Sad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm personally torn.&lt;br /&gt; Not sure if going 1 or 2 NV 96GB cards is even worth it. Seems that having 96 or 192 doesn't change much effectively compared to 32GB if one wants to run a local model for coding to avoid cloud - cloud being so much better in quality and speed.&lt;br /&gt; Going for 1TB local RAM and do CPU inference might pay-off, but also not sure about model quality.&lt;/p&gt; &lt;p&gt;Any experience by anyone here doing actual pro use at job with os models?&lt;br /&gt; Do 96 or 192 GB VRAM change anything meaningfully?&lt;br /&gt; Is 1TB CPU inference viable?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Photo_Sad"&gt; /u/Photo_Sad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2rqom/local_programming_vs_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2rqom/local_programming_vs_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2rqom/local_programming_vs_cloud/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T10:50:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2seed</id>
    <title>Production Hybrid Retrieval: 48% better accuracy with BM25 + FAISS on a single t3.medium</title>
    <updated>2026-01-03T11:29:08+00:00</updated>
    <author>
      <name>/u/Ok-Blacksmith-8257</name>
      <uri>https://old.reddit.com/user/Ok-Blacksmith-8257</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;Sharing our hybrid retrieval system that serves 127k+ queries on a single AWS Lightsail instance (no GPU needed for embeddings, optional for reranking). **Stack**: - Embeddings: all-MiniLM-L6-v2 (22M params, CPU-friendly) - Reranker: ms-marco-MiniLM-L-6-v2 (cross-encoder) - Infrastructure: t3.medium (4GB RAM, 2 vCPU) - Cost: ~$50/month **Performance**: - Retrieval: 75ms (BM25 + FAISS + RRF + rerank) - Throughput: 50 queries/min - Accuracy: 91% (vs 62% dense-only) **Why hybrid?** Dense-only failed on &amp;quot;kenteken AB-123-CD&amp;quot; (license plate). Semantic similarity understood the concept but missed the exact entity. Solution: 4-stage cascade combining keyword precision (BM25) + semantic understanding (FAISS). **Latency breakdown**: - BM25: 8ms - FAISS: 15ms (runs parallel with BM25) - RRF fusion: 2ms - Cross-encoder rerank: 50ms (bottleneck but +12% accuracy) **Optimizations**: - Async parallel retrieval - Batch reranking (size 32) - GPU optional (3x speedup for reranker) **Code**: https://github.com/Eva-iq/E.V.A.-Cascading-Retrieval **Write-up**: https://medium.com/@pbronck/better-rag-accuracy-with-hybrid-bm25-dense-vector-search-ea99d48cba93 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Blacksmith-8257"&gt; /u/Ok-Blacksmith-8257 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2seed/production_hybrid_retrieval_48_better_accuracy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2seed/production_hybrid_retrieval_48_better_accuracy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2seed/production_hybrid_retrieval_48_better_accuracy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T11:29:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2dcje</id>
    <title>ASUS officially announces price hikes from January 5, right before CES 2026</title>
    <updated>2026-01-02T22:53:19+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2dcje/asus_officially_announces_price_hikes_from/"&gt; &lt;img alt="ASUS officially announces price hikes from January 5, right before CES 2026" src="https://external-preview.redd.it/e0KouFF887lm3A9dV6mq_44cYZmQvCh1I3h_7LTZz8c.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c1b6efd1e001ac6907d2df9de1ac4165e4b086a" title="ASUS officially announces price hikes from January 5, right before CES 2026" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/asus-officially-announces-price-hikes-from-january-5-right-before-ces-2026"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2dcje/asus_officially_announces_price_hikes_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2dcje/asus_officially_announces_price_hikes_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T22:53:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q30q4x</id>
    <title>RTX 5060Ti vs RX 9060 XT (Both 16GB)</title>
    <updated>2026-01-03T17:38:29+00:00</updated>
    <author>
      <name>/u/Clean-Market5761</name>
      <uri>https://old.reddit.com/user/Clean-Market5761</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just a dev building his first PC, kind of interesting on AI and local LLMs, so NVIDIA seems like the right choice even if it's a bit more expensive, I notice the AMD just drops and is a complete mess and has a lot of support issues with anything AI related. Just trying to get some honest feedback&lt;/p&gt; &lt;p&gt;For now, my PC is looking like this&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; AMD Ryzen 7 5700X&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU Cooler:&lt;/strong&gt; Cooler Master Hyper 212 Black&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; GIGABYTE B550 Eagle WIFI6&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; &lt;em&gt;Any of those two cards&lt;/em&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Case:&lt;/strong&gt; Corsair 4000D Airflow (Includes 3x Corsair RS fans)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PSU:&lt;/strong&gt; Corsair RM850e (850W)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; Corsair Vengeance LPX 32 GB (2x 16 GB) DDR4 3600 MHz&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Clean-Market5761"&gt; /u/Clean-Market5761 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q30q4x/rtx_5060ti_vs_rx_9060_xt_both_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q30q4x/rtx_5060ti_vs_rx_9060_xt_both_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q30q4x/rtx_5060ti_vs_rx_9060_xt_both_16gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T17:38:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2wyz7</id>
    <title>What are the best ultrasmall LLMs / best datasets to train them?</title>
    <updated>2026-01-03T15:12:46+00:00</updated>
    <author>
      <name>/u/cpldcpu</name>
      <uri>https://old.reddit.com/user/cpldcpu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems that there is more activity now to train ultra-small LLMS with &amp;lt;100M parameters. I was wondering about the general activity in that space? What is currently the best &amp;quot;tiny&amp;quot; model? Are there good synthetic datasets to train these models? (Tinystories is getting a bit boring)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cpldcpu"&gt; /u/cpldcpu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2wyz7/what_are_the_best_ultrasmall_llms_best_datasets/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2wyz7/what_are_the_best_ultrasmall_llms_best_datasets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2wyz7/what_are_the_best_ultrasmall_llms_best_datasets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T15:12:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1q32au2</id>
    <title>Support for Maincode/Maincoder-1B has been merged into llama.cpp</title>
    <updated>2026-01-03T18:37:44+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q32au2/support_for_maincodemaincoder1b_has_been_merged/"&gt; &lt;img alt="Support for Maincode/Maincoder-1B has been merged into llama.cpp" src="https://external-preview.redd.it/J1TYMrJiMUgIT8dFS1ce2lKxyzmTEOBCMeDbgQQxD4A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ce81ed4107dce8e34a266879521b2bbde40d194" title="Support for Maincode/Maincoder-1B has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1puf614/new_1b_parameter_opensource_coding_model_getting/"&gt;Here&lt;/a&gt; is previous thread from model creator/team for more details.&lt;/p&gt; &lt;p&gt;Model&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Maincode/Maincoder-1B"&gt;https://huggingface.co/Maincode/Maincoder-1B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF (from model creator/team)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Maincode/Maincoder-1B-GGUF"&gt;https://huggingface.co/Maincode/Maincoder-1B-GGUF&lt;/a&gt; &lt;/p&gt; &lt;p&gt;(Thought &lt;a href="/u/jacek2023"&gt;u/jacek2023&lt;/a&gt; posted this already)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7614"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q32au2/support_for_maincodemaincoder1b_has_been_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q32au2/support_for_maincodemaincoder1b_has_been_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T18:37:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2p2wa</id>
    <title>nanbeige4 is an incredible model for running locally</title>
    <updated>2026-01-03T08:06:54+00:00</updated>
    <author>
      <name>/u/Revolutionalredstone</name>
      <uri>https://old.reddit.com/user/Revolutionalredstone</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Feels like a deepseek moment might have slipped a few people by&lt;/p&gt; &lt;p&gt;nanbeige (weird name- apparently chosen to be bland/uninteresting)&lt;/p&gt; &lt;p&gt;..It's very interesting! basically 3 invalidating most 30B models.&lt;/p&gt; &lt;p&gt;(you can find it up ridiculously high on this chart: for a 3B model)&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/creative_writing.html"&gt;https://eqbench.com/creative_writing.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm stoked to have intelligence like this at home, but I'd love to know how to push this into super fast interference territory! (I've heard about diffusion based conversion etc and am super keen!)&lt;/p&gt; &lt;p&gt;Has anyone else seen something newer (this is a few weeks old now)? Seems like various charts show this one to be an outlier.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Revolutionalredstone"&gt; /u/Revolutionalredstone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2p2wa/nanbeige4_is_an_incredible_model_for_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2p2wa/nanbeige4_is_an_incredible_model_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2p2wa/nanbeige4_is_an_incredible_model_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T08:06:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2yse3</id>
    <title>50M param PGN-only transformer plays coherent chess without search: Is small-LLM generalization is underrated?</title>
    <updated>2026-01-03T16:24:50+00:00</updated>
    <author>
      <name>/u/Tasty_Share_1357</name>
      <uri>https://old.reddit.com/user/Tasty_Share_1357</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2yse3/50m_param_pgnonly_transformer_plays_coherent/"&gt; &lt;img alt="50M param PGN-only transformer plays coherent chess without search: Is small-LLM generalization is underrated?" src="https://b.thumbs.redditmedia.com/cqk0fjtKNqL9lspX_eRxoCqF6Z2rVcXeGLzb5oTCsIs.jpg" title="50M param PGN-only transformer plays coherent chess without search: Is small-LLM generalization is underrated?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all — been poking at Adam Karvonen’s 50 M-param &lt;strong&gt;Chess GPT&lt;/strong&gt; (nanoGPT architecture, plain PGN in/out, no board tensor, no engine search) and wrapped a tiny UI so you can try it out.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick takeaways&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Surprisingly legal / coherent&lt;/strong&gt; — far better than frontier chat models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Feels human:&lt;/strong&gt; samples a move distribution instead of crunching Stockfish lines.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hit me with a castle-mate (O-O-O#) in ~25 moves&lt;/strong&gt; — vanishingly rare in real games.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;“Stockfish-trained”&lt;/strong&gt; = tuned to &lt;em&gt;imitate&lt;/em&gt; Stockfish’s choices; the engine itself isn’t inside.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Temp sweet-spots:&lt;/strong&gt; T ≈ 0.3 for the Stockfish-style model, T = 0 for the Lichess-style one.&lt;/li&gt; &lt;li&gt;Nice micro-case study of how small, domain-trained LLMs show sharp &lt;em&gt;in-distribution&lt;/em&gt; generalization while giant general models still hallucinate elsewhere.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Write-up (context): &lt;a href="https://chinmaysnotebook.substack.com/p/chessllm-what-a-50m-transformer-says"&gt;https://chinmaysnotebook.substack.com/p/chessllm-what-a-50m-transformer-says&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Live demo: &lt;a href="https://chess-llm-316391656470.us-central1.run.app"&gt;https://chess-llm-316391656470.us-central1.run.app&lt;/a&gt;&lt;/li&gt; &lt;li&gt;HF models: &lt;a href="https://huggingface.co/adamkarvonen/chess_llms/tree/main"&gt;https://huggingface.co/adamkarvonen/chess_llms/tree/main&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Original blog / paper (Karvonen, 2024): &lt;a href="https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html?utm_source=chatgpt.com"&gt;https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious what the &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; crowd thinks—feedback welcome!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bkqdqkh5c6bg1.png?width=1684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9764256359eb3e8c59d4cf0a1c025e8ecdbe63e0"&gt;https://preview.redd.it/bkqdqkh5c6bg1.png?width=1684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9764256359eb3e8c59d4cf0a1c025e8ecdbe63e0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tasty_Share_1357"&gt; /u/Tasty_Share_1357 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2yse3/50m_param_pgnonly_transformer_plays_coherent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2yse3/50m_param_pgnonly_transformer_plays_coherent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2yse3/50m_param_pgnonly_transformer_plays_coherent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T16:24:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1q25070</id>
    <title>LeCun Says Llama 4 results "were fudged a little bit"</title>
    <updated>2026-01-02T17:38:01+00:00</updated>
    <author>
      <name>/u/MrPecunius</name>
      <uri>https://old.reddit.com/user/MrPecunius</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There was speculation in this sub about suspicious Llama 4 benchmarks some time back, and now LeCun confirms it on his way out. Best I can do is a Slashdot link since the FT article is paywalled:&lt;/p&gt; &lt;p&gt;&lt;a href="https://tech.slashdot.org/story/26/01/02/1449227/results-were-fudged-departing-meta-ai-chief-confirms-llama-4-benchmark-manipulation"&gt;'Results Were Fudged': Departing Meta AI Chief Confirms Llama 4 Benchmark Manipulation &lt;/a&gt;&lt;/p&gt; &lt;p&gt;This bit jumped out at me:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Zuckerberg subsequently &amp;quot;sidelined the entire GenAI organisation,&amp;quot; according to LeCun. &amp;quot;A lot of people have left, a lot of people who haven't yet left will leave.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This explains a lot, if true: we never saw the promised huge Llama 4 model, and there hasn't been any followup since the other releases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrPecunius"&gt; /u/MrPecunius &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T17:38:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2qo2u</id>
    <title>WhisperNote — a simple local Whisper-based transcription app (Windows)</title>
    <updated>2026-01-03T09:44:46+00:00</updated>
    <author>
      <name>/u/_fortexe</name>
      <uri>https://old.reddit.com/user/_fortexe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2qo2u/whispernote_a_simple_local_whisperbased/"&gt; &lt;img alt="WhisperNote — a simple local Whisper-based transcription app (Windows)" src="https://preview.redd.it/wcbalo8cu3bg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0328ac2605fcb9e788498a1ee7e1ae8c3f63b484" title="WhisperNote — a simple local Whisper-based transcription app (Windows)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone.&lt;/p&gt; &lt;p&gt;I’ve been working on a small personal project called WhisperNote. It’s a simple Windows desktop app for local audio transcription using OpenAI Whisper.&lt;/p&gt; &lt;p&gt;The main goal was not to build “the best” tool, but a clean and straightforward one: press record or drop an audio file — get text.&lt;/p&gt; &lt;p&gt;All processing happens locally on your machine. No cloud, no accounts. It’s intentionally minimal and focused on doing one thing well. Models are downloaded once, then everything runs offline.&lt;/p&gt; &lt;p&gt;I’m sharing it here in case someone values simplicity and local-first tools as much as I do. If it’s useful to you — that’s great. Note: the Windows build is ~4 GB because it bundles Python, PyTorch with CUDA, and FFmpeg for a fully offline, out-of-the-box experience.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/LokiSkardina/WhisperNote"&gt;https://github.com/LokiSkardina/WhisperNote&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_fortexe"&gt; /u/_fortexe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wcbalo8cu3bg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2qo2u/whispernote_a_simple_local_whisperbased/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2qo2u/whispernote_a_simple_local_whisperbased/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T09:44:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2p5dh</id>
    <title>Glm4.7 + CC not bad</title>
    <updated>2026-01-03T08:10:55+00:00</updated>
    <author>
      <name>/u/Federal_Spend2412</name>
      <uri>https://old.reddit.com/user/Federal_Spend2412</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I genuinely think it's pretty good this time - GLM4.7 + CC is actually somewhat close to 4.5 Sonnet, or more accurately I'd say it's on par with 4 Sonnet. I'm subscribed to the middle-tier plan.&lt;/p&gt; &lt;p&gt;I tested it with a project that has a Python backend and TypeScript frontend, asking it to add a feature that involved both backend and frontend work. It handled everything smoothly, and the MCP calls all went through without getting stuck (which used to be a problem before).&lt;/p&gt; &lt;p&gt;Of course, to be completely honest, there's still a massive gap between this and 4.5 Opus - Opus is on a completely insane level&lt;/p&gt; &lt;p&gt;So I'm still keeping my $10/month GitHub Copilot subscription. For the really tough problems, I'll use 4.5 Opus, but for regular stuff, GLM4.7 + CC basically handles everything. GLM4.7 costs me $100/month now, plus the $10 for Copilot - that's less than around $13 per month total(bigmodel.cn coding plan), which feels pretty good.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal_Spend2412"&gt; /u/Federal_Spend2412 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2p5dh/glm47_cc_not_bad/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2p5dh/glm47_cc_not_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2p5dh/glm47_cc_not_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T08:10:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2xbjc</id>
    <title>[Experimental] "Temporal LoRA": A dynamic adapter router that switches context (Code vs. Lit) with 100% accuracy. Proof of concept on GPT-2.</title>
    <updated>2026-01-03T15:27:02+00:00</updated>
    <author>
      <name>/u/Waste-Persimmon-4735</name>
      <uri>https://old.reddit.com/user/Waste-Persimmon-4735</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2xbjc/experimental_temporal_lora_a_dynamic_adapter/"&gt; &lt;img alt="[Experimental] &amp;quot;Temporal LoRA&amp;quot;: A dynamic adapter router that switches context (Code vs. Lit) with 100% accuracy. Proof of concept on GPT-2." src="https://b.thumbs.redditmedia.com/cyIcaaq4CP1oV64NwqtjnSjlIEa8OaHxkVW3eaqL8Is.jpg" title="[Experimental] &amp;quot;Temporal LoRA&amp;quot;: A dynamic adapter router that switches context (Code vs. Lit) with 100% accuracy. Proof of concept on GPT-2." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/9hlxzha8k5bg1.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a4700705ee17523749e4e0f9034808223007a533"&gt;https://preview.redd.it/9hlxzha8k5bg1.png?width=1800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a4700705ee17523749e4e0f9034808223007a533&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I’ve been working on a project called &lt;strong&gt;Stability-First AI&lt;/strong&gt;, exploring ways to prevent catastrophic forgetting and handle multi-tasking better.&lt;/p&gt; &lt;p&gt;I wanted to share one specific experiment (&lt;strong&gt;Project 02&lt;/strong&gt;) that I think is relevant to this sub: &lt;strong&gt;Temporal LoRA&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt; We often have multiple LoRAs (e.g., one for coding, one for roleplay), but merging them degrades performance, and manually loading/unloading them is slow. We need a way for the model to &amp;quot;know&amp;quot; which adapter to use per token or per prompt.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Experiment:&lt;/strong&gt; I used a GPT-2 baseline and trained two distinct LoRA adapters:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Shakespeare Adapter&lt;/strong&gt; (Literature style)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Python Adapter&lt;/strong&gt; (Coding style)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I then implemented a &amp;quot;Time Mixer&amp;quot; — a lightweight gating network (router) that dynamically activates the correct adapter based on the input context.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Results:&lt;/strong&gt; The router achieved &lt;strong&gt;100% accuracy&lt;/strong&gt; in distinguishing between coding prompts (e.g., &lt;code&gt;import torch&lt;/code&gt;) and literary prompts (e.g., &lt;code&gt;To be or not to be&lt;/code&gt;).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It routes &amp;quot;Code&amp;quot; prompts -&amp;gt; Python Adapter&lt;/li&gt; &lt;li&gt;It routes &amp;quot;Prose&amp;quot; prompts -&amp;gt; Shakespeare Adapter&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This effectively creates a modular, reversible learning system where the backbone stays stable, but the &amp;quot;interface&amp;quot; (adapters) is fluid.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this matters:&lt;/strong&gt; While this demo is on GPT-2, the architecture suggests a clean way to implement &lt;strong&gt;Mixture of Experts (MoE)&lt;/strong&gt; using LoRAs on larger local models (Llama 3, Mistral, etc.) without training a massive MoE from scratch. It allows for &amp;quot;hot-swapping&amp;quot; skills without degrading the base model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo &amp;amp; Code:&lt;/strong&gt; The code is open source. You can check the &lt;code&gt;02-temporal-lora-gpt2&lt;/code&gt; folder to see the router implementation:&lt;a href="https://github.com/vitali-sialedchyk/stability-first-ai"&gt;https://github.com/vitali-sialedchyk/stability-first-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m looking for feedback or anyone interested in testing this routing logic on larger architectures (Llama-3-8B or similar).&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Waste-Persimmon-4735"&gt; /u/Waste-Persimmon-4735 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2xbjc/experimental_temporal_lora_a_dynamic_adapter/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2xbjc/experimental_temporal_lora_a_dynamic_adapter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2xbjc/experimental_temporal_lora_a_dynamic_adapter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T15:27:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2jwsn</id>
    <title>How is Cloud Inference so cheap</title>
    <updated>2026-01-03T03:37:13+00:00</updated>
    <author>
      <name>/u/VolkoTheWorst</name>
      <uri>https://old.reddit.com/user/VolkoTheWorst</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How do cloud inference companies like DeepInfra, Together, Chutes, Novita etc manage to be in profit regarding to the price of the GPUs/electricity and the fact that I guess it's difficult to have always someone to serve ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VolkoTheWorst"&gt; /u/VolkoTheWorst &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jwsn/how_is_cloud_inference_so_cheap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jwsn/how_is_cloud_inference_so_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2jwsn/how_is_cloud_inference_so_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T03:37:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2onpg</id>
    <title>Built a fully local AI assistant with long-term memory, tool orchestration, and a 3D UI (runs on a GTX 1650)</title>
    <updated>2026-01-03T07:42:14+00:00</updated>
    <author>
      <name>/u/atif_dev</name>
      <uri>https://old.reddit.com/user/atif_dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2onpg/built_a_fully_local_ai_assistant_with_longterm/"&gt; &lt;img alt="Built a fully local AI assistant with long-term memory, tool orchestration, and a 3D UI (runs on a GTX 1650)" src="https://b.thumbs.redditmedia.com/27iIaYMlD6cCAk7xURWf9QOU6i1_KjiShnklurpG_mo.jpg" title="Built a fully local AI assistant with long-term memory, tool orchestration, and a 3D UI (runs on a GTX 1650)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been working on a personal project called ATOM — a fully local AI assistant designed more like an operating system for intelligence than a chatbot.&lt;/p&gt; &lt;p&gt;Everything runs locally. No cloud inference.&lt;/p&gt; &lt;p&gt;Key components: - Local LLM via LM Studio (currently Qwen3-VL-4B, vision + tool calling) - Tool orchestration (system info, web search via self-hosted SearXNG, file/PDF generation, Home Assistant, robotics) - Long-term memory with ChromaDB - Async memory saving via a smaller “judge” model Semantic retrieval + periodic RAG-style injection - Dedicated local embedding server (OpenAI-style API) - Real hardware control (robotic arm, sensors) - JSON logging + test harness for reproducible scenarios&lt;/p&gt; &lt;p&gt;On the UI side, I built a React + React Three Fiber interface using Firebase Studio that visualizes tool usage as orbiting “planets” around a central core. It’s mostly for observability and debugging, but it turned out pretty fun.&lt;/p&gt; &lt;p&gt;Constraints: Hardware is limited (GTX 1650), so performance tradeoffs were necessary System is experimental and some components are still evolving&lt;/p&gt; &lt;p&gt;This is not a product, just a personal engineering project exploring: - long-term memory consolidation - tool-centric reasoning - fully local personal AI systems&lt;/p&gt; &lt;p&gt;Would appreciate feedback, especially from others running local setups or experimenting with memory/tool architectures.&lt;/p&gt; &lt;p&gt;GitHub (backend): &lt;a href="https://github.com/AtifUsmani/A.T.O.M"&gt;https://github.com/AtifUsmani/A.T.O.M&lt;/a&gt; UI repo: &lt;a href="https://github.com/AtifUsmani/ATOM-UI"&gt;https://github.com/AtifUsmani/ATOM-UI&lt;/a&gt; Demo videos linked in the README.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atif_dev"&gt; /u/atif_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q2onpg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2onpg/built_a_fully_local_ai_assistant_with_longterm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2onpg/built_a_fully_local_ai_assistant_with_longterm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T07:42:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2wm33</id>
    <title>How capable is GPT-OSS-120b, and what are your predictions for smaller models in 2026?</title>
    <updated>2026-01-03T14:58:01+00:00</updated>
    <author>
      <name>/u/Apart_Paramedic_7767</name>
      <uri>https://old.reddit.com/user/Apart_Paramedic_7767</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an RTX 3090 and I’m considering getting another one so I can run OSS-120b. I’m mainly interested in chatting with it about private documents, statistical analysis, STEM knowledge/analysis and some coding.&lt;/p&gt; &lt;p&gt;Is it a worthwhile investment? I don’t mind speculation in this post - what do you think is possible for smaller models in this frame that I could run with two RTX 3090s this year?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apart_Paramedic_7767"&gt; /u/Apart_Paramedic_7767 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2wm33/how_capable_is_gptoss120b_and_what_are_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2wm33/how_capable_is_gptoss120b_and_what_are_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2wm33/how_capable_is_gptoss120b_and_what_are_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T14:58:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2ppkb</id>
    <title>MiniMax-M2.1 Uncensored: PRISM Advanced Abliteration</title>
    <updated>2026-01-03T08:45:29+00:00</updated>
    <author>
      <name>/u/Maxious</name>
      <uri>https://old.reddit.com/user/Maxious</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2ppkb/minimaxm21_uncensored_prism_advanced_abliteration/"&gt; &lt;img alt="MiniMax-M2.1 Uncensored: PRISM Advanced Abliteration" src="https://external-preview.redd.it/AAFIDX32Yo3yBOTXBXkwbpmtKeh886wBWSOhkOds4Pc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bc2cf3d74a648b569c2bd55f6d299c6f5f27ea9" title="MiniMax-M2.1 Uncensored: PRISM Advanced Abliteration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxious"&gt; /u/Maxious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Ex0bit/MiniMax-M2.1-PRISM"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2ppkb/minimaxm21_uncensored_prism_advanced_abliteration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2ppkb/minimaxm21_uncensored_prism_advanced_abliteration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T08:45:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2ubre</id>
    <title>[R] Understanding DeepSeek-V3's "Hydra" Architecture: How mHC prevents signal explosion</title>
    <updated>2026-01-03T13:13:45+00:00</updated>
    <author>
      <name>/u/Leading_Wrangler_708</name>
      <uri>https://old.reddit.com/user/Leading_Wrangler_708</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2ubre/r_understanding_deepseekv3s_hydra_architecture/"&gt; &lt;img alt="[R] Understanding DeepSeek-V3's &amp;quot;Hydra&amp;quot; Architecture: How mHC prevents signal explosion" src="https://b.thumbs.redditmedia.com/-9DU6w_0AEFPBH8wh4HFMYV3GfpPIvdn1P3CRZC4etg.jpg" title="[R] Understanding DeepSeek-V3's &amp;quot;Hydra&amp;quot; Architecture: How mHC prevents signal explosion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;​I spent some time deconstructing the DeepSeek-V3 paper to understand how they managed to split the residual stream without destabilizing the network. I created a visual guide (attached) to explain the engineering behind the &amp;quot;Hydra&amp;quot; architecture. ​Here is the breakdown of the slides:&lt;/p&gt; &lt;p&gt;​1. The Bottleneck ​Standard Transformers (like Llama 3) operate on a &amp;quot;Single Lane&amp;quot; highway. No matter how large the embedding dimension is, features (Syntax, Logic, Tone) effectively compete for space in the same vector. &lt;/p&gt; &lt;p&gt;​2. The &amp;quot;Hydra&amp;quot; Concept &amp;amp; The Crash ​DeepSeek proposed splitting this into N parallel streams (Hyper-Connections).&lt;br /&gt; ​The Problem: When they allowed these lanes to talk to each other via mixing matrices, the signal energy exploded. ​The Stat: In their experiments, signal energy increased by 3000x, causing gradients to hit NaN almost immediately. &lt;/p&gt; &lt;p&gt;​3. The Physics Fix: Sinkhorn-Knopp ​They solved this by enforcing Conservation of Energy. The mixing matrix must be a Doubly Stochastic Matrix (rows sum to 1, columns sum to 1).&lt;br /&gt; ​The Analogy (Slide 6): I used a &amp;quot;Dinner Party&amp;quot; analogy. If Guests are Rows and Chairs are Columns, the Sinkhorn algorithm acts as a referee, iteratively scaling demands until every guest has exactly one chair and every chair has exactly one guest. &lt;/p&gt; &lt;p&gt;​4. The Engineering: TileLang &amp;amp; Recomputation ​The math worked, but it was too slow (running an iterative algo 20 times per layer hits the memory wall).&lt;br /&gt; ​Kernel Fusion: They wrote custom kernels to keep data in the GPU cache (SRAM) during the iterative steps, avoiding VRAM round-trips.&lt;br /&gt; ​Recomputation: Instead of storing the states of 4 parallel lanes (which would OOM), they re-calculate the matrices from scratch during the backward pass. &lt;/p&gt; &lt;p&gt;​TL;DR: DeepSeek-V3 essentially widens the &amp;quot;intelligence highway&amp;quot; by using parallel lanes, but keeps it stable by enforcing physics constraints (energy conservation) via a custom implementation of the Sinkhorn-Knopp algorithm.&lt;/p&gt; &lt;p&gt;​Let me know if you have questions about the visualization!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leading_Wrangler_708"&gt; /u/Leading_Wrangler_708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q2ubre"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2ubre/r_understanding_deepseekv3s_hydra_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2ubre/r_understanding_deepseekv3s_hydra_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T13:13:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2o033</id>
    <title>What is the smartest uncensored nsfw LLM you can run with 20GB VRAM and 24GB RAM</title>
    <updated>2026-01-03T07:04:18+00:00</updated>
    <author>
      <name>/u/Death_12_35_taken</name>
      <uri>https://old.reddit.com/user/Death_12_35_taken</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking for something that can stay in character and be fast but also creative. I am looking for models that i can run locally and at decent speed. Just need something that is smart and uncensored. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Death_12_35_taken"&gt; /u/Death_12_35_taken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2o033/what_is_the_smartest_uncensored_nsfw_llm_you_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2o033/what_is_the_smartest_uncensored_nsfw_llm_you_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2o033/what_is_the_smartest_uncensored_nsfw_llm_you_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T07:04:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1q31ltd</id>
    <title>Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched</title>
    <updated>2026-01-03T18:11:26+00:00</updated>
    <author>
      <name>/u/ubrtnk</name>
      <uri>https://old.reddit.com/user/ubrtnk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"&gt; &lt;img alt="Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched" src="https://b.thumbs.redditmedia.com/q10c8h1nk9ulWsUdfgzAsIflFbSIgTra6fA9fSqXqqQ.jpg" title="Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share my experiences this morning, in the wake of the US attacking Venezuela and capturing Maduro and his wife&lt;/p&gt; &lt;p&gt;It started with asking Qwen Research (Qwen Long 1.5-30B-A3B) about the attacks that we all woke up to this morning:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/086yb5lj76bg1.png?width=2047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de920b95fac7b93215f1516105c5536eb1eeb6c1"&gt;https://preview.redd.it/086yb5lj76bg1.png?width=2047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de920b95fac7b93215f1516105c5536eb1eeb6c1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It got to the information but I had questions about why it thought for 5 minutes to find information about breaking news. Started looking at and tightening system prompts to reduce thinking time. However, the events this morning were so extreme and unlikely, from the LLM's perspective, that Qwen Research continued to classify the event as a hoax/misinformation multiple times, reframed the query as hypothetical/fictional and suggested that the whole environment it was operating in a simulation, despite having links from Reuters, AP, BBC, MSN, NYTimes etc. all saying the same thing. It was so &amp;quot;outlandish&amp;quot; that the model was actively choosing to ignore the proof that it had pulled. &lt;/p&gt; &lt;p&gt;I added:&lt;/p&gt; &lt;p&gt;Evidence Authority Rules, Hoax Classification Rules, Reality Frame Rules, Meta Reasoning Rules and Reasoning Limit/Budget rules and it Qwen Long fought me the entire way. &lt;/p&gt; &lt;p&gt;So then I thought lets go talk to Spark, my trusty default model that never lets me down. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6tbh4km376bg1.png?width=2265&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fee098c46a18daa03c80acc8394cd85e84335ca"&gt;https://preview.redd.it/6tbh4km376bg1.png?width=2265&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fee098c46a18daa03c80acc8394cd85e84335ca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Spark 4.0 is GPT-OSS:20B that is always loaded for the family and runs on a dedicated 4080 Super. &lt;/p&gt; &lt;p&gt;Spark just flat out said, nope cant help you and then said it didnt have any credible sources. It wasn't until I gave it the links from BBC, Reuters, NYT etc that I gave Qwen that it finally acknowledged that the event was real.&lt;/p&gt; &lt;p&gt;I'm testing with GPT-OSS:120B now and its working thru the process of &amp;quot;skeptical but verify&amp;quot; much faster than the smaller models. Thor (GPT-OSS:120B) also thought it was fake news &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o1bdoqsqc6bg1.png?width=2269&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a981f0a1247daf50497f284cf5d59dccf88a412b"&gt;https://preview.redd.it/o1bdoqsqc6bg1.png?width=2269&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a981f0a1247daf50497f284cf5d59dccf88a412b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But he powered thru and did a bunch of research and gave me a good answer. I just wanted to share the experience that I had with trying to get details about the event. When the LLMs say &amp;quot;Nah, that CAN'T be real, that's too ridiculous&amp;quot;, the event must be really bad. But it does shine a light on knowledge cut offs, &amp;quot;fake news&amp;quot; threshold, how models handle global/international events and the smaller models we daily drive. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ubrtnk"&gt; /u/ubrtnk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T18:11:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2sfwx</id>
    <title>ElevenLabs is killing my budget. What are the best "hidden gem" alternatives for documentary style TTS?</title>
    <updated>2026-01-03T11:31:31+00:00</updated>
    <author>
      <name>/u/Ancient_Routine8576</name>
      <uri>https://old.reddit.com/user/Ancient_Routine8576</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I'm running a YouTube channel focused on &amp;quot;War Economics&amp;quot; and &amp;quot;History&amp;quot;. I've been using ElevenLabs (Marcus voice) and the quality is amazing, but the pricing is unsustainable for long-form content (8-10 min videos).&lt;/p&gt; &lt;p&gt;I've tried the usual suspects (Murf, Play.ht) but they sound too robotic or corporate.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I am looking for:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Something with a dark, authoritative, documentary-style tone.&lt;/li&gt; &lt;li&gt;Either a cheaper paid alternative OR a high-quality GitHub/Local solution (I have a decent GPU if needed, like RVC or Tortoise).&lt;/li&gt; &lt;li&gt;Has anyone tried tools like &lt;strong&gt;Fish Audio&lt;/strong&gt; or &lt;strong&gt;OpenAI TTS API&lt;/strong&gt; wrappers?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any &amp;quot;underground&amp;quot; or lesser-known recommendations would be appreciated. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ancient_Routine8576"&gt; /u/Ancient_Routine8576 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2sfwx/elevenlabs_is_killing_my_budget_what_are_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2sfwx/elevenlabs_is_killing_my_budget_what_are_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2sfwx/elevenlabs_is_killing_my_budget_what_are_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T11:31:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2s3hp</id>
    <title>Don't sleep on granite 4 small if you got an 8+32+ system</title>
    <updated>2026-01-03T11:11:06+00:00</updated>
    <author>
      <name>/u/Zestyclose-Shift710</name>
      <uri>https://old.reddit.com/user/Zestyclose-Shift710</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2s3hp/dont_sleep_on_granite_4_small_if_you_got_an_832/"&gt; &lt;img alt="Don't sleep on granite 4 small if you got an 8+32+ system" src="https://b.thumbs.redditmedia.com/iwvWuoXjX_5rS64X-c0qbwyBZ9jpLqp-5OJdBXmYjto.jpg" title="Don't sleep on granite 4 small if you got an 8+32+ system" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My device: a thinkpad p15 with 32gb of ram and a 8gb quadro. Usually only really good enough for the 7-8b class.&lt;/p&gt; &lt;p&gt;The setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use a MoE;&lt;/li&gt; &lt;li&gt;Keep all experts in CPU (llama.cpp parameter);&lt;/li&gt; &lt;li&gt;This leaves you with VRAM to spare. Set the context length so it ~fills it up&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The result:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~200k context (f16 kv cache)&lt;/li&gt; &lt;li&gt;~30b MoE model&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;~10 tkps generation speed&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;But this is where granite 4 comes in: due to being a hybrid transformer+mamba model, it stays fast as context fills&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As such, using Granite 4.0 Small (32B total / 9B activated) with a 50 page (~50.5k tokens) paper in context, it stays at ~7 tkps, which is very usable!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nqpvxiu9a4bg1.png?width=1055&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4fd830b29fb3bf890136793590665cf3ceec979b"&gt;Screenshot is from Jan (https://www.jan.ai/), a sort of FOSS LM Studio alternative that I really like&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quite possibly this is all very obvious but I just found this out experimentally and it would probably be useful to others like me&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zestyclose-Shift710"&gt; /u/Zestyclose-Shift710 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2s3hp/dont_sleep_on_granite_4_small_if_you_got_an_832/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2s3hp/dont_sleep_on_granite_4_small_if_you_got_an_832/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2s3hp/dont_sleep_on_granite_4_small_if_you_got_an_832/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T11:11:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2wvsj</id>
    <title>Llama.cpp running on Android with Snapdragon 888 and 8GB of ram. Compiled/Built on device. [Guide/Tutorial]</title>
    <updated>2026-01-03T15:09:03+00:00</updated>
    <author>
      <name>/u/hackiv</name>
      <uri>https://old.reddit.com/user/hackiv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2wvsj/llamacpp_running_on_android_with_snapdragon_888/"&gt; &lt;img alt="Llama.cpp running on Android with Snapdragon 888 and 8GB of ram. Compiled/Built on device. [Guide/Tutorial]" src="https://b.thumbs.redditmedia.com/5qo0k-2a-bDFaK_FwoCOc6N5D0Imvs6jWuthPslr82Q.jpg" title="Llama.cpp running on Android with Snapdragon 888 and 8GB of ram. Compiled/Built on device. [Guide/Tutorial]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;1: Download Termux from F-droid (older version available on Google Playstore or Aurora)&lt;/p&gt; &lt;p&gt;2: Open Termux and run &amp;quot;&lt;a href="https://github.com/ggml-org/llama.cpp.git"&gt;https://github.com/ggml-org/llama.cpp.git&lt;/a&gt;&amp;quot; and then &amp;quot;cd llama.cpp&amp;quot; run &amp;quot;pkg install cmake&amp;quot; &lt;/p&gt; &lt;p&gt;3: run &amp;quot;cmake -B build&amp;quot; and then &amp;quot;cmake --build build --config Release&amp;quot; &lt;/p&gt; &lt;p&gt;4: find desired model from HuggingFace, then choose its quantized version (preferably 4-bit)&lt;/p&gt; &lt;p&gt;5: when pressing '4-bit' choose 'Use this model' and select 'llama.cpp' afterwards copy command which starts with &amp;quot;llama-server&amp;quot; &lt;/p&gt; &lt;p&gt;6: paste command in Termux and put &amp;quot;./&amp;quot; in front of &amp;quot;llama-server&amp;quot; so it's adjacent.&lt;/p&gt; &lt;p&gt;7: After model's downloaded, server is immediately launched. Model is saved in '.cache' so you can run this command again to start the server without all re-downloading ordeal. &lt;/p&gt; &lt;p&gt;8: open web browser and input 'localhost:8080' then press enter &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Enjoy. Any questions? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackiv"&gt; /u/hackiv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q2wvsj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2wvsj/llamacpp_running_on_android_with_snapdragon_888/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2wvsj/llamacpp_running_on_android_with_snapdragon_888/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T15:09:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2pons</id>
    <title>GLM-4.7-REAP-50-W4A16: 50% Expert-Pruned + INT4 Quantized GLM-4 (179B params, ~92GB)</title>
    <updated>2026-01-03T08:43:56+00:00</updated>
    <author>
      <name>/u/Maxious</name>
      <uri>https://old.reddit.com/user/Maxious</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/"&gt; &lt;img alt="GLM-4.7-REAP-50-W4A16: 50% Expert-Pruned + INT4 Quantized GLM-4 (179B params, ~92GB)" src="https://external-preview.redd.it/RT6xZIQ5U8h3GMBsKzEeqHJyXy63I2_XP8TVKTT_Hvg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7c7ca25d885be26a9f257d4e17e2b038061773a" title="GLM-4.7-REAP-50-W4A16: 50% Expert-Pruned + INT4 Quantized GLM-4 (179B params, ~92GB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxious"&gt; /u/Maxious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/0xSero/GLM-4.7-REAP-50-W4A16"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T08:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM – 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
