<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-25T16:25:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p66w91</id>
    <title>Novel Relational Cross-Attention appears to best Transformers in spatial reasoning tasks</title>
    <updated>2025-11-25T08:06:26+00:00</updated>
    <author>
      <name>/u/CommunityTough1</name>
      <uri>https://old.reddit.com/user/CommunityTough1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Repo (MIT): &lt;a href="https://github.com/clowerweb/relational-cross-attention"&gt;https://github.com/clowerweb/relational-cross-attention&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quick rundown:&lt;/p&gt; &lt;p&gt;A novel neural architecture for few-shot learning of transformations that outperforms standard transformers by &lt;strong&gt;30% relative improvement&lt;/strong&gt; while being &lt;strong&gt;17% faster&lt;/strong&gt;.&lt;/p&gt; &lt;h2&gt;Key Results&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Unseen Accuracy&lt;/th&gt; &lt;th&gt;Speed&lt;/th&gt; &lt;th&gt;Gap vs Standard&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Relational (Ours)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;16.12%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;24.8s&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;+3.76%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Standard Transformer&lt;/td&gt; &lt;td&gt;12.36%&lt;/td&gt; &lt;td&gt;29.7s&lt;/td&gt; &lt;td&gt;baseline&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Per-Transform Breakdown (Unseen)&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Transform&lt;/th&gt; &lt;th&gt;Standard&lt;/th&gt; &lt;th&gt;Relational&lt;/th&gt; &lt;th&gt;Improvement&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;flip_vertical&lt;/td&gt; &lt;td&gt;10.14%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;16.12%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;+5.98%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;rotate_180&lt;/td&gt; &lt;td&gt;10.33%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;15.91%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;+5.58%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;translate_down&lt;/td&gt; &lt;td&gt;9.95%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;16.20%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;+6.25%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;invert_colors&lt;/td&gt; &lt;td&gt;20.07%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;20.35%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;+0.28%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;The relational model excels at spatial reasoning while maintaining strong color transform performance.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;7M params model scores 2.5% on epoch 1 and 2.8% in 5 epochs on ARC-AGI. After 5 epochs, performance starts to slip, likely due to overfitting (I think the model is just too small, and I don't have the hardware to run ARC-AGI with a bigger one). I'd also love to see what this algorithm might do for LLMs, so I may train a TinyStories SLM over the weekend (it'll probably take several days on my hardware). Welcoming any feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CommunityTough1"&gt; /u/CommunityTough1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p66w91/novel_relational_crossattention_appears_to_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p66w91/novel_relational_crossattention_appears_to_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p66w91/novel_relational_crossattention_appears_to_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T08:06:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1p68b5v</id>
    <title>Data sandboxing for AI agents [Guide]</title>
    <updated>2025-11-25T09:39:47+00:00</updated>
    <author>
      <name>/u/Better-Department662</name>
      <uri>https://old.reddit.com/user/Better-Department662</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p68b5v/data_sandboxing_for_ai_agents_guide/"&gt; &lt;img alt="Data sandboxing for AI agents [Guide]" src="https://external-preview.redd.it/MKQuFdTBgb36LtxvPhfsDBrigq5T7WGuK5c1dyDAQUw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b28563ed9073873da3cea8a18f5c29789f99932a" title="Data sandboxing for AI agents [Guide]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most teams give AI agents database credentials and hope they only access the right data. But here's what I've learned: hope isn't a security strategy. Agents can query anything they have access to—and without proper boundaries, they will.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Data sandboxing&lt;/strong&gt; is the practice of creating isolated, controlled environments where agents can only access the data they're supposed to. It's not about restricting agents - it's about giving them safe, governed access that prevents security incidents, compliance violations, and costly mistakes.&lt;/p&gt; &lt;p&gt;I've seen teams deploy agents without sandboxing, then discover agents accessing sensitive customer data, querying production databases during peak hours, or violating compliance requirements. The fix is always harder than building it right from the start.&lt;/p&gt; &lt;p&gt;This guide explains what data sandboxing is, why it's essential for AI agents, and how to implement it with modern architecture patterns. Whether you're building your first agent or scaling to dozens, sandboxing is the foundation of secure agent data access.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Better-Department662"&gt; /u/Better-Department662 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.pylar.ai/blog/data-sandboxing-for-ai-agents-modern-architecture-guide"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p68b5v/data_sandboxing_for_ai_agents_guide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p68b5v/data_sandboxing_for_ai_agents_guide/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T09:39:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5fe9u</id>
    <title>Kimi: Wait... I beat Gemini 3? For real?</title>
    <updated>2025-11-24T12:09:46+00:00</updated>
    <author>
      <name>/u/xiaoruhao</name>
      <uri>https://old.reddit.com/user/xiaoruhao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5fe9u/kimi_wait_i_beat_gemini_3_for_real/"&gt; &lt;img alt="Kimi: Wait... I beat Gemini 3? For real?" src="https://b.thumbs.redditmedia.com/vHmqwt4RbzyHVTmK5Rpwoiu8qk8apNZx0Pd1Q5Gdc7w.jpg" title="Kimi: Wait... I beat Gemini 3? For real?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/3d2q76ci473g1.jpg?width=1194&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f00ae8be20af807202bfbb40d8cd9e4e18f0a736"&gt;https://preview.redd.it/3d2q76ci473g1.jpg?width=1194&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f00ae8be20af807202bfbb40d8cd9e4e18f0a736&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gguf when&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xiaoruhao"&gt; /u/xiaoruhao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5fe9u/kimi_wait_i_beat_gemini_3_for_real/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5fe9u/kimi_wait_i_beat_gemini_3_for_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5fe9u/kimi_wait_i_beat_gemini_3_for_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T12:09:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5r6vp</id>
    <title>Supertonic WebGPU: blazingly fast text-to-speech running 100% locally in your browser.</title>
    <updated>2025-11-24T19:51:48+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5r6vp/supertonic_webgpu_blazingly_fast_texttospeech/"&gt; &lt;img alt="Supertonic WebGPU: blazingly fast text-to-speech running 100% locally in your browser." src="https://external-preview.redd.it/bnE1bWMxM3RiOTNnMecSo005PIULOnLc1HMBGahwp1rxPwmS_uFA5SEw8lvq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7cd248206ce70028c5841b56f2a00da414e62be4" title="Supertonic WebGPU: blazingly fast text-to-speech running 100% locally in your browser." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last week, the Supertone team released Supertonic, an extremely fast and high-quality text-to-speech model. So, I created a demo for it that uses Transformers.js and ONNX Runtime Web to run the model 100% locally in the browser on WebGPU. The original authors made a web demo too, and I did my best to optimize the model as much as possible (up to ~40% faster in my tests, see below).&lt;/p&gt; &lt;p&gt;I was even able to generate a ~5 hour audiobook in under 3 minutes. Amazing, right?!&lt;/p&gt; &lt;p&gt;Link to demo (+ source code): &lt;a href="https://huggingface.co/spaces/webml-community/Supertonic-TTS-WebGPU"&gt;https://huggingface.co/spaces/webml-community/Supertonic-TTS-WebGPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* From my testing, for the same 226-character paragraph (on the same device): the &lt;a href="https://huggingface.co/onnx-community/Supertonic-TTS-ONNX"&gt;newly-optimized model&lt;/a&gt; ran at ~1750.6 characters per second, while the original ran at ~1255.6 characters per second.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b12eez2tb93g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5r6vp/supertonic_webgpu_blazingly_fast_texttospeech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5r6vp/supertonic_webgpu_blazingly_fast_texttospeech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T19:51:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5jh9l</id>
    <title>Universal LLM Memory Doesn't Exist</title>
    <updated>2025-11-24T15:09:08+00:00</updated>
    <author>
      <name>/u/selund1</name>
      <uri>https://old.reddit.com/user/selund1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5jh9l/universal_llm_memory_doesnt_exist/"&gt; &lt;img alt="Universal LLM Memory Doesn't Exist" src="https://preview.redd.it/z9mcdq37z73g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0117b1e8971316aa60731086907017a976288589" title="Universal LLM Memory Doesn't Exist" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing a write-up I just published and would love local / self-hosted perspectives.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; I benchmarked Mem0 and Zep as “universal memory” layers for agents on MemBench (4,000 conversational QA cases with reflective memory), using gpt-5-nano and comparing them to a plain long-context baseline.&lt;/p&gt; &lt;p&gt;Both memory systems were * &lt;strong&gt;14–77× more expensive&lt;/strong&gt; over a full conversation * &lt;strong&gt;~30% less accurate&lt;/strong&gt; at recalling facts than just passing the full history as context &lt;/p&gt; &lt;p&gt;The shared “LLM-on-write” pattern (running background LLMs to extract/normalise facts on every message) is a poor fit for working memory / execution state, even though it can be useful for long-term semantic memory.&lt;/p&gt; &lt;p&gt;I tried running the test locally and it was even worse: prompt processing completely blew up latency because of the N+1 effect from all the extra “memory” calls. On a single box, every one of those calls competes with the main model for compute.&lt;/p&gt; &lt;p&gt;My takeaway:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Working memory / execution state (tool outputs, logs, file paths, variables) wants simple, lossless storage (KV, append-only logs, sqlite, etc.).&lt;/li&gt; &lt;li&gt;Semantic memory (user prefs, long-term profile) can be a fuzzy vector/graph layer, but probably shouldn’t sit in the critical path of every message.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Write-up and harness:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blog post: &lt;a href="https://fastpaca.com/blog/memory-isnt-one-thing"&gt;https://fastpaca.com/blog/memory-isnt-one-thing&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Benchmark tool: &lt;a href="https://github.com/fastpaca/pacabench"&gt;https://github.com/fastpaca/pacabench&lt;/a&gt; (see &lt;code&gt;examples/membench_qa_test&lt;/code&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What are you doing for &lt;strong&gt;local&lt;/strong&gt; dev?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Are you using any “universal memory” libraries with local models?&lt;/li&gt; &lt;li&gt;Have you found a setup where an LLM-driven memory layer actually beats long context end to end?&lt;/li&gt; &lt;li&gt;Is anyone explicitly separating semantic vs working memory in their local stack?&lt;/li&gt; &lt;li&gt;Is there a better way I can benchmark this quicker locally? Using SLMs ruin fact extraction efficacy and feels &amp;quot;unfair&amp;quot;, but prompt processing in lm studio (on my mac studio m3 ultra) is too slow&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/selund1"&gt; /u/selund1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z9mcdq37z73g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5jh9l/universal_llm_memory_doesnt_exist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5jh9l/universal_llm_memory_doesnt_exist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T15:09:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1p67qaj</id>
    <title>DocFinder: Local Semantic Search for PDFs (Embeddings + SQLite)</title>
    <updated>2025-11-25T09:01:11+00:00</updated>
    <author>
      <name>/u/notagoodtradooor</name>
      <uri>https://old.reddit.com/user/notagoodtradooor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;What does DocFinder do?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Runs entirely offline: indexes PDFs using sentence-transformers and ONNX for fast embedding generation, stores data in plain SQLite BLOBs.&lt;/li&gt; &lt;li&gt;Supports top-k semantic search via cosine similarity directly on your machine.&lt;/li&gt; &lt;li&gt;Hardware autodetection: optimizes for Apple Silicon, NVIDIA &amp;amp; AMD GPUs, or CPU.&lt;/li&gt; &lt;li&gt;Desktop and web interfaces available, making document search and preview easy.&lt;/li&gt; &lt;li&gt;Simple installation for macOS, Windows, and Linux—with options to install as a Python package if you prefer.&lt;/li&gt; &lt;li&gt;Offline-first philosophy means data remains private, with flexible integration options.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm sharing this here specifically because this community focuses on running AI models locally with privacy and control in mind.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I'm open to feedback and suggestions!&lt;/strong&gt; If anyone has ideas for improving embedding models, optimizing for specific hardware configurations, or integrating with existing local LLM tools, I'd love to hear them. Thank you! &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/filippostanghellini/DocFinder"&gt;https://github.com/filippostanghellini/DocFinder&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notagoodtradooor"&gt; /u/notagoodtradooor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p67qaj/docfinder_local_semantic_search_for_pdfs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p67qaj/docfinder_local_semantic_search_for_pdfs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p67qaj/docfinder_local_semantic_search_for_pdfs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T09:01:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5epot</id>
    <title>The most objectively correct way to abliterate so far - ArliAI/GLM-4.5-Air-Derestricted</title>
    <updated>2025-11-24T11:32:45+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/"&gt; &lt;img alt="The most objectively correct way to abliterate so far - ArliAI/GLM-4.5-Air-Derestricted" src="https://external-preview.redd.it/8n5MhbkzXEcl9NlvZMbb8GGre-k1VjQ0kDAKe7qQtQM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9334318d3d29cfd953050dfdf981bc10db9cc00b" title="The most objectively correct way to abliterate so far - ArliAI/GLM-4.5-Air-Derestricted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, this is Owen Arli from &lt;strong&gt;&lt;a href="https://www.arliai.com"&gt;Arli AI&lt;/a&gt;&lt;/strong&gt; and this is the first model release we created in a while. We previously created models finetuned for more creativity with our &lt;a href="https://huggingface.co/collections/ArliAI/rpr-models"&gt;RpR&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/ArliAI/rpmax-v1-models"&gt;RPMax&lt;/a&gt; models.&lt;/p&gt; &lt;p&gt;After seeing the post by Jim Lai on Norm-Preserving Biprojected Abliteration &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oypwa7/a_more_surgical_approach_to_abliteration/"&gt;here&lt;/a&gt;, I immediately thought that no one has done abliteration this way and that the &amp;quot;norm-preserving&amp;quot; part was a brilliant improvement in the method to abliterate models, and appears to me like it is objectively the best way to abliterate models. You can find the full technical details in his post, but I will explain the gist of it here.&lt;/p&gt; &lt;h1&gt;The problem:&lt;/h1&gt; &lt;p&gt;Typical abliteration methods finds the refusal vector and simply subtracts it from the weights, this causes the &amp;quot;length&amp;quot; (Norm) of the weight vectors to be altered. This is a problem because this &amp;quot;length&amp;quot; usually dictates how &amp;quot;important&amp;quot; a neuron is and how much it contributes, so changing it will cause damage to the model's general intelligence.&lt;/p&gt; &lt;h1&gt;The solution:&lt;/h1&gt; &lt;p&gt;This Norm-Preserving technique modifies the direction the weights point in, but forces them to keep their original length.&lt;/p&gt; &lt;p&gt;Essentially, by removing the refusal in this way you can potentially also improve the model's performance instead of diminishing it. &lt;/p&gt; &lt;p&gt;Trying out the &lt;a href="https://huggingface.co/grimjim/gemma-3-12b-it-norm-preserved-biprojected-abliterated"&gt;Gemma 3 12B&lt;/a&gt; model example, it clearly works extremely well compared to regular abliteration methods that often leaves the model broken until further finetuning. Which explains why the model ranks so high in the UGI leaderboard even though its base was Gemma 3 12B which is a notoriously censored model.&lt;/p&gt; &lt;h1&gt;The result:&lt;/h1&gt; &lt;p&gt;Armed with a new 2xRTX Pro 6000 server I just built for Arli AI model experimentation, I set out to try and apply this abliteration technique to the much larger and smarter GLM-4.5-Air. Which ended up in what I think is undoubtedly one of the most interesting model I have ever used.&lt;/p&gt; &lt;p&gt;Its not that GLM-4.5-Air is usually plagued with refusals, but using this &amp;quot;Derestricted&amp;quot; version feels like the model suddenly becomes free to do anything it wants without trying to &amp;quot;align&amp;quot; to a non-existent guideline either visibly or subconsciously. It's hard to explain without trying it out yourself.&lt;/p&gt; &lt;p&gt;For an visible example, I bet that those of you running models locally or through an API will definitely have tried to add a system prompt that says &amp;quot;You are a person and not an AI&amp;quot; or something along those lines. Usually even with such a system prompt and nothing in the context that suggests it is an AI, the model will stubbornly still insist that it is an AI and it is unable to do &amp;quot;human-like&amp;quot; things. With this model, just adding that prompt immediately allows the model to pretend to act like a human in its response. No hesitation or any coaxing needed. &lt;/p&gt; &lt;p&gt;The most impressive part about this abliteration technique is definitely the fact that it has somehow made the model a better instruction follower instead of just a braindead NSFW-capable model from typical abliteration. As for it's intelligence, it has not been benchmarked but I believe that just using the model and feeling it out to see if it has degraded in capabilities is better than just checking benchmarks. Which in this case, the model does feel like it is just as smart if not better than the original GLM-4.5-Air.&lt;/p&gt; &lt;p&gt;You can find the model available on our API, or you can download them yourself from the HF links below!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model downloads:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Original: &lt;a href="https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted"&gt;https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted&lt;/a&gt;&lt;/li&gt; &lt;li&gt;FP8: &lt;a href="https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-FP8"&gt;https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-FP8&lt;/a&gt;&lt;/li&gt; &lt;li&gt;INT8: &lt;a href="https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-W8A8-INT8"&gt;https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-W8A8-INT8&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We will be working to create more of these Derestricted models, along with many new finetuned models too!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T11:32:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6gsjh</id>
    <title>LLaDA2.0 (103B/16B) has been released</title>
    <updated>2025-11-25T16:21:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;LLaDA2.0-flash&lt;/strong&gt; is a diffusion language model featuring a 100BA6B Mixture-of-Experts (MoE) architecture. As an enhanced, instruction-tuned iteration of the LLaDA2.0 series, it is optimized for practical applications.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LLaDA2.0-mini&lt;/strong&gt; is a diffusion language model featuring a 16BA1B Mixture-of-Experts (MoE) architecture. As an enhanced, instruction-tuned iteration of the LLaDA series, it is optimized for practical applications.&lt;/p&gt; &lt;p&gt;llama.cpp support in progress &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17454"&gt;https://github.com/ggml-org/llama.cpp/pull/17454&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gsjh/llada20_103b16b_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gsjh/llada20_103b16b_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gsjh/llada20_103b16b_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T16:21:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1p64kh6</id>
    <title>Qwen-3-Omni-30b-A3B Thinking on a 4090 vs on an AIMAX 395 with 128gb DDR5? Whats the better setup and ideal quantisation?</title>
    <updated>2025-11-25T05:44:12+00:00</updated>
    <author>
      <name>/u/Melodic-Muffin</name>
      <uri>https://old.reddit.com/user/Melodic-Muffin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen-3-Omni-30b-A3B Thinking takes around 70GB of VRAM to run unquantised. Would it be better to run it quantised on a 4090 or unquantised on an AIMAX 395? I don't care about how fast it is but 5-15tps would be great although I'm not too fused on speed as long as it's not so slow it takes minutes to generate one text reply. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Melodic-Muffin"&gt; /u/Melodic-Muffin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p64kh6/qwen3omni30ba3b_thinking_on_a_4090_vs_on_an_aimax/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p64kh6/qwen3omni30ba3b_thinking_on_a_4090_vs_on_an_aimax/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p64kh6/qwen3omni30ba3b_thinking_on_a_4090_vs_on_an_aimax/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T05:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5op7z</id>
    <title>From Microsoft, Fara-7B: An Efficient Agentic Model for Computer Use</title>
    <updated>2025-11-24T18:20:05+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5op7z/from_microsoft_fara7b_an_efficient_agentic_model/"&gt; &lt;img alt="From Microsoft, Fara-7B: An Efficient Agentic Model for Computer Use" src="https://external-preview.redd.it/SMZl7m5fVyqBxt4sbd8f4qjQFqlJCLo1Gazp1bcHVhM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dcf106148dae72548d6b9683c27e007b4306c1b9" title="From Microsoft, Fara-7B: An Efficient Agentic Model for Computer Use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fara-7B is Microsoft's first agentic small language model (SLM) designed specifically for computer use. With only 7 billion parameters, Fara-7B is an ultra-compact Computer Use Agent (CUA) that achieves state-of-the-art performance within its size class and is competitive with larger, more resource-intensive agentic systems.&lt;/p&gt; &lt;p&gt;Multimodal decoder-only language model that takes an image (screenshot) + text context. It directly predicts thoughts and actions with grounded arguments. Current production baselines leverage Qwen 2.5-VL (7B).&lt;/p&gt; &lt;p&gt;Parameters: 7 Billion&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/Fara-7B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5op7z/from_microsoft_fara7b_an_efficient_agentic_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5op7z/from_microsoft_fara7b_an_efficient_agentic_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T18:20:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6gruv</id>
    <title>I tested a few local hosted coding models with VSCode / cline so that you don't have to</title>
    <updated>2025-11-25T16:21:10+00:00</updated>
    <author>
      <name>/u/DrMicrobit</name>
      <uri>https://old.reddit.com/user/DrMicrobit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been running a bunch of &amp;quot;can I actually code with a local model in VS Code?&amp;quot; experiments over the last weeks, focused on task with moderate complexity. I chose simple, well known games as they help to visualise strengths and shortcomings of the results quite easily, also to a layperson. The tasks at hand: Space Invaders &amp;amp; Galaga in a single HTML file. I also did a more serious run with a ~2.3k- word design doc.&lt;/p&gt; &lt;p&gt;Sharing the main takeaways here for anyone trying to use local models with Cline/Ollama for real coding work, not just completions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt; Ubuntu 24.04, 2x 4060 Ti 16 GB (32 GB total VRAM), VS Code + Cline, models served via Ollama / GGUF. Context for local models was usually ~96k tokens (anything much bigger spilled into RAM and became 7-20x slower). Tasks ranged from YOLO prompts (&amp;quot;Write a Space Invaders game in a single HTML file&amp;quot;) to a moderately detailed spec for a modernized Space Invaders.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Headline result:&lt;/strong&gt; Qwen 3 Coder 30B is the only family I tested that consistently worked well with Cline and produced usable games. At 4-bit it's already solid; quality drops noticeably at 3-bit and 2-bit (more logic bugs, more broken runs). With 4-bit and 32 GB VRAM you can keep ~ 100k context and still be reasorably fast. If you can spare more VRAM or live with reduced context, higher-bit Qwen 3 Coder (e.g. 6-bit) does help. But 4-bit is the practical sweet spot for 32 GiB VRAM.&lt;/p&gt; &lt;p&gt;Merges/prunes of Qwen 3 Coder generally underperformed the original. The cerebras REAP 25B prune and YOYO merges were noticeably buggier and less reliable than vanilla Qwen 3 Coder 30B, even at higher bit widths. They sometimes produced runnable code, but with a much higher &amp;quot;Cline has to rerun / you have to hand-debug or giveup&amp;quot; rate. TL;DR: for coding, the unmodified coder models beat their fancy descendants.&lt;/p&gt; &lt;p&gt;Non-coder 30B models and &amp;quot;hot&amp;quot; general models mostly disappointed in this setup. Qwen 3 30B (base/instruct from various sources), devstral 24B, Skyfall 31B v4, Nemotron Nano 9B v2, and Olmo 3 32B either: (a) fought with Cline (rambling, overwriting their own code, breaking the project), or (b) produced very broken game logic that wasn't fixable in one or two debug rounds. Some also forced me to shrink context so much they stopped being interesting for larger tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Guiding the models:&lt;/strong&gt; I wanted to demonstrate, with examples that can be shown to people without much insights, what development means: YOLO prompts (&amp;quot;Make me a Space Invaders / Galaga game&amp;quot;) will produce widely varying results even for big online models, and doubly so for locals. See &lt;a href="https://drmicrobit.github.io/lllm_suit/tests/01_SpaceInvaders_yolo/online/GPT5/t1/space_invaders.html"&gt;this example&lt;/a&gt; for an interesting YOLO from GPT-5, and &lt;a href="https://drmicrobit.github.io/lllm_suit/tests/01_SpaceInvaders_yolo/online/Opus41/t2/space_invaders.html"&gt;this example&lt;/a&gt; for a barebone one from Opus 4.1. Models differ a lot in what they think &amp;quot;Space Invaders&amp;quot; or &amp;quot;Galaga&amp;quot; is, and leave out key features (bunkers, UFO, proper alien movement, etc.).&lt;/p&gt; &lt;p&gt;With a moderately detailed design doc, Qwen 3 Coder 30B can stick reasonably well to spec: &lt;a href="https://drmicrobit.github.io/lllm_suit/tests/03_SpaceInvaders_ddoc01/local/qwen3-coder-30B-unsloth/6bitUD_t1/space_invaders.html"&gt;Example 1&lt;/a&gt;, &lt;a href="https://drmicrobit.github.io/lllm_suit/tests/03_SpaceInvaders_ddoc01/local/qwen3-coder-30B-unsloth/4bitUD_t1/space_invaders.html"&gt;Example 2&lt;/a&gt;, &lt;a href="https://drmicrobit.github.io/lllm_suit/tests/03_SpaceInvaders_ddoc01/local/qwen3-coder-30B-unsloth/4bit_t2/space_invaders.html"&gt;Example 3&lt;/a&gt;. They still tend to repeat certain logic errors (e.g., invader formation movement, missing config entries) and often can't fix them from a high-level bug description without human help.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My current working hypothesis:&lt;/strong&gt; to do enthusiast-level Al-assisted coding in VS Code with Cline, one really needs to have at least 32 GB VRAM for usable models. Preferably use an untampered Qwen 3 Coder 30B (Ollama's default 4-bit, or an unsloth GGUF at 4-6 bits). Avoid going below 4-bit for coding, be wary of fancy merges/prunes, and don't expect miracles without a decent spec. &lt;/p&gt; &lt;p&gt;I documented all runs (code + notes) in a repo on GitHub (&lt;a href="https://github.com/DrMicrobit/lllm_suit"&gt;https://github.com/DrMicrobit/lllm_suit&lt;/a&gt;) if anyone's interested in. The docs there are linked and, going down the experiments, give an idea of what the results looked like with an image and have direct links runnable HTML files, configs, and model variants.&lt;/p&gt; &lt;p&gt;I'd be happy to hear what others think of this kind of simple experimental evaluation, or what other models I could test.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrMicrobit"&gt; /u/DrMicrobit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gruv/i_tested_a_few_local_hosted_coding_models_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gruv/i_tested_a_few_local_hosted_coding_models_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gruv/i_tested_a_few_local_hosted_coding_models_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T16:21:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5wjia</id>
    <title>Opus 4.5 only narrowly reclaims #1 on official SWE-bench leaderboard (independent evaluation); cheaper than previous versions, but still more expensive than others</title>
    <updated>2025-11-24T23:18:13+00:00</updated>
    <author>
      <name>/u/klieret</name>
      <uri>https://old.reddit.com/user/klieret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5wjia/opus_45_only_narrowly_reclaims_1_on_official/"&gt; &lt;img alt="Opus 4.5 only narrowly reclaims #1 on official SWE-bench leaderboard (independent evaluation); cheaper than previous versions, but still more expensive than others" src="https://b.thumbs.redditmedia.com/jl1e-X5a7OxM7ptJPHbpLZFTeFQd-7kAhlFa8Y3ToLA.jpg" title="Opus 4.5 only narrowly reclaims #1 on official SWE-bench leaderboard (independent evaluation); cheaper than previous versions, but still more expensive than others" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm from the SWE-bench team. We maintain a leaderboard where we evaluate all models with the exact same agent and prompts so that we can compare models apple-to-apple.&lt;/p&gt; &lt;p&gt;We just finished evaluating Opus 4.5 and it's back at #1 on the leaderboard. However, it's by quite a small margin (only 0.2%pts ahead of Gemini 3, i.e., just a single task) and it's clearly more expensive than the other models that achieve top scores.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/svt1p1b9fa3g1.png?width=3160&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f4ea5388eebbc540d03bdfa101614411dcb55a62"&gt;https://preview.redd.it/svt1p1b9fa3g1.png?width=3160&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f4ea5388eebbc540d03bdfa101614411dcb55a62&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Interestingly, Opus 4.5 takes fewer steps than Sonnet 4.5. About as many as Gemini 3 Pro, but much more than the GPT-5.1 models.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sx5o0e9cfa3g1.png?width=2251&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=68dd5df936d150ef8b697f150ddcd365f50f909e"&gt;https://preview.redd.it/sx5o0e9cfa3g1.png?width=2251&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=68dd5df936d150ef8b697f150ddcd365f50f909e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you want to get maximum performance, you should set the step limit to at least 100:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/52gyo5pefa3g1.png?width=2009&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfdaf2b849abe875e0693beb08da4d1e9e0a5678"&gt;https://preview.redd.it/52gyo5pefa3g1.png?width=2009&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfdaf2b849abe875e0693beb08da4d1e9e0a5678&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Limiting the max number of steps also allows you to balance avg cost vs performance (interestingly Opus 4.5 can be more cost-efficient than Sonnet 4.5 for lower step limits). &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gymvl4hffa3g1.png?width=2009&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c6cfd7a42eec0c88d8401fad5cfcef8a2f3a693"&gt;https://preview.redd.it/gymvl4hffa3g1.png?width=2009&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c6cfd7a42eec0c88d8401fad5cfcef8a2f3a693&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can find all other models at &lt;a href="http://swebench.com"&gt;swebench.com&lt;/a&gt; (will be updated in the next hour with the new results). You can also reproduce the numbers by using &lt;a href="https://github.com/SWE-agent/mini-swe-agent/"&gt;https://github.com/SWE-agent/mini-swe-agent/&lt;/a&gt; [MIT license]. There is a tutorial in the documentation on how to evaluate on SWE-bench (it's a 1-liner).&lt;/p&gt; &lt;p&gt;We're also currently evaluating minimax-m2 and other open source models and will be back with a comparison of the most open source models soon (we tend to take a bit longer at evaluating these because it often has more infra/logistics hiccups)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klieret"&gt; /u/klieret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5wjia/opus_45_only_narrowly_reclaims_1_on_official/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5wjia/opus_45_only_narrowly_reclaims_1_on_official/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5wjia/opus_45_only_narrowly_reclaims_1_on_official/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T23:18:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1p683yz</id>
    <title>Thank you all for your contribution with tools and stepping up to help maintain the Epstein 20K dataset</title>
    <updated>2025-11-25T09:26:44+00:00</updated>
    <author>
      <name>/u/tensonaut</name>
      <uri>https://old.reddit.com/user/tensonaut</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are keeping track of any RAG based tools that would help investigative journalists uncover hidden details from the Epstein Files. We got our Github setup earlier today with all your contributions listed: &lt;a href="https://github.com/EF20K/Projects"&gt;https://github.com/EF20K/Projects&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our &lt;a href="https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K"&gt;dataset&lt;/a&gt; is also currently featured on the front page of Hugging Face, so we expect more projects along the way. If you are interested in contributing feel free to reach out - no matter how small it is. Once again we would like to thank all the members of the sub for your support in keeping everything open source!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tensonaut"&gt; /u/tensonaut &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p683yz/thank_you_all_for_your_contribution_with_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p683yz/thank_you_all_for_your_contribution_with_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p683yz/thank_you_all_for_your_contribution_with_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T09:26:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6eb24</id>
    <title>cyankiwi AWQ v1.0</title>
    <updated>2025-11-25T14:47:56+00:00</updated>
    <author>
      <name>/u/_cpatonn</name>
      <uri>https://old.reddit.com/user/_cpatonn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thank you for using my model from my personal account cpatonn so far. I am happy to introduce cyankiwi AWQ v1.0 with 4bit quantized model achieving accuracy degradation of less than 1%, an improvement from my AWQ quants on my personal account cpatonn. cyankiwi AWQ v1.0 models will be labelled in our modelcards.&lt;/p&gt; &lt;p&gt;The following table compares wikitext byte perplexity (lower is better) of some cyankiwi AWQ v1.0 quantized models. Perplexity increases range from negatives (decreases) to 0.6%!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;/th&gt; &lt;th align="left"&gt;Base Model&lt;/th&gt; &lt;th align="left"&gt;cyankiwi AWQ 8bit&lt;/th&gt; &lt;th align="left"&gt;cyankiwi AWQ 4bit&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3-Next-80B-A3B-Instruct&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1.48256&lt;/td&gt; &lt;td align="left"&gt;1.48258&lt;/td&gt; &lt;td align="left"&gt;1.48602&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Kimi-Linear-48B-A3B-Instruct&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1.54038&lt;/td&gt; &lt;td align="left"&gt;1.54041&lt;/td&gt; &lt;td align="left"&gt;1.54194&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MiniMax-M2&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1.54984&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;1.54743&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1.80803&lt;/td&gt; &lt;td align="left"&gt;1.80776&lt;/td&gt; &lt;td align="left"&gt;1.79795&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Please, please and please let me know your thoughts on my prior quants, and what you expect in the future, as I always aim to improve my products! For more complex queries or feedback, please get in touch with me at &lt;a href="mailto:ton@cyan.kiwi"&gt;ton@cyan.kiwi&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_cpatonn"&gt; /u/_cpatonn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6eb24/cyankiwi_awq_v10/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6eb24/cyankiwi_awq_v10/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6eb24/cyankiwi_awq_v10/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T14:47:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5ynpr</id>
    <title>Qwen3-235B-A22B achieves SOTA in EsoBench, Claude 4.5 Opus places 7th. EsoBench tests how well models learn and use a private esolang.</title>
    <updated>2025-11-25T00:50:54+00:00</updated>
    <author>
      <name>/u/neat_space</name>
      <uri>https://old.reddit.com/user/neat_space</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5ynpr/qwen3235ba22b_achieves_sota_in_esobench_claude_45/"&gt; &lt;img alt="Qwen3-235B-A22B achieves SOTA in EsoBench, Claude 4.5 Opus places 7th. EsoBench tests how well models learn and use a private esolang." src="https://b.thumbs.redditmedia.com/OBtMIflKSvaUYAFVz4VdaUGn7N4bjfxcn6myRC9AdSs.jpg" title="Qwen3-235B-A22B achieves SOTA in EsoBench, Claude 4.5 Opus places 7th. EsoBench tests how well models learn and use a private esolang." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is &lt;a href="https://caseys-evals.com/"&gt;my own benchmark.&lt;/a&gt; (Apologies mobile users, I still need to fix the site on mobile D:)&lt;/p&gt; &lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Esoteric_programming_language"&gt;Esolang definition&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I've tested 3 open weights models, and of the course the shiny new Claude 4.5 Opus. New additions:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1)&lt;/strong&gt; Qwen3-235B-A22B thinking, scores 29.4&lt;/p&gt; &lt;p&gt;&lt;strong&gt;7)&lt;/strong&gt; Claude 4.5 Opus, scoring 20.9&lt;/p&gt; &lt;p&gt;&lt;strong&gt;16)&lt;/strong&gt; Deepseek v3.2 exp, scoring 16.2&lt;/p&gt; &lt;p&gt;&lt;strong&gt;17)&lt;/strong&gt; Kimi k2 thinking, scoring 16.1&lt;/p&gt; &lt;p&gt;I was pretty surpised by all results here. Qwen for doing so incredibly well, and the other 3 for underperforming. The Claude models are all run without thinking which kinda handicaps them, so you could argue 4.5 Opus actually did quite well.&lt;/p&gt; &lt;p&gt;The fact that, of the the models I've tested, an open weights model is the current SOTA has really taken me by surprise! Qwen took ages to test though, boy does that model think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neat_space"&gt; /u/neat_space &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p5ynpr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5ynpr/qwen3235ba22b_achieves_sota_in_esobench_claude_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5ynpr/qwen3235ba22b_achieves_sota_in_esobench_claude_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T00:50:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1p66m28</id>
    <title>PipesHub - The Open Source, Self-Hostable Alternative to Microsoft 365 Copilot</title>
    <updated>2025-11-25T07:48:44+00:00</updated>
    <author>
      <name>/u/Effective-Ad2060</name>
      <uri>https://old.reddit.com/user/Effective-Ad2060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I’m excited to share something we’ve been building for the past few months - &lt;strong&gt;PipesHub&lt;/strong&gt;, a &lt;strong&gt;fully open-source alternative to Microsoft 365 Copilot&lt;/strong&gt; designed to bring powerful Enterprise Search, Agent Builders to every team, without vendor lock-in. The platform brings all your business data together and makes it searchable. It connects with apps like Google Drive, Gmail, Slack, Notion, Confluence, Jira, Outlook, SharePoint, Dropbox, and even local file uploads. You can deploy it and run it with just one docker compose command.&lt;/p&gt; &lt;p&gt;The entire system is built on a &lt;strong&gt;fully event-streaming architecture powered by Kafka&lt;/strong&gt;, making indexing and retrieval scalable, fault-tolerant, and real-time across large volumes of data. PipesHub combines a vector database with a knowledge graph and uses Agentic RAG to deliver highly accurate results. We constrain the LLM to ground truth. Provides Visual citations, reasoning and confidence score. Our implementation says Information not found rather than hallucinating.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep understanding of user, organization and teams with enterprise knowledge graph&lt;/li&gt; &lt;li&gt;Connect to any AI model of your choice including OpenAI, Gemini, Claude, or Ollama (works well with gpt-oss or qwen3 vl)&lt;/li&gt; &lt;li&gt;Use any other provider that supports OpenAI compatible endpoints&lt;/li&gt; &lt;li&gt;Vision-Language Models and OCR for visual or scanned docs&lt;/li&gt; &lt;li&gt;Login with Google, Microsoft, OAuth, or SSO&lt;/li&gt; &lt;li&gt;Rich REST APIs for developers&lt;/li&gt; &lt;li&gt;All major file types support including pdfs with images, diagrams and charts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Features releasing this month&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agent Builder - Perform actions like Sending mails, Schedule Meetings, etc along with Search, Deep research, Internet search and more&lt;/li&gt; &lt;li&gt;Reasoning Agent that plans before executing tasks&lt;/li&gt; &lt;li&gt;40+ Connectors allowing you to connect to your entire business apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check it out and share your thoughts or feedback. Your feedback is immensely valuable and is much appreciated:&lt;br /&gt; &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo Video:&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=xA9m3pwOgz8"&gt;https://www.youtube.com/watch?v=xA9m3pwOgz8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective-Ad2060"&gt; /u/Effective-Ad2060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p66m28/pipeshub_the_open_source_selfhostable_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p66m28/pipeshub_the_open_source_selfhostable_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p66m28/pipeshub_the_open_source_selfhostable_alternative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T07:48:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5zz11</id>
    <title>Best Coding LLM as of Nov'25</title>
    <updated>2025-11-25T01:51:36+00:00</updated>
    <author>
      <name>/u/PhysicsPast8286</name>
      <uri>https://old.reddit.com/user/PhysicsPast8286</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Folks, &lt;/p&gt; &lt;p&gt;I have a NVIDIA H100 and have been tasked to find a replacement for Qwen3 32B (non-quantized) model currenly hosted on it. &lt;/p&gt; &lt;p&gt;I’m looking it to use primarily for Java coding tasks and want the LLM to support atleast 100K context window (input + output). It would be used in a corporate environment so censored models like GPT OSS are also okay if they are good at Java programming.&lt;/p&gt; &lt;p&gt;Can anyone recommend an alternative LLM that would be more suitable for this kind of work?&lt;/p&gt; &lt;p&gt;Appreciate any suggestions or insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhysicsPast8286"&gt; /u/PhysicsPast8286 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5zz11/best_coding_llm_as_of_nov25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5zz11/best_coding_llm_as_of_nov25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5zz11/best_coding_llm_as_of_nov25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T01:51:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5qzft</id>
    <title>Coursera Founder And AI Pioneer Andrew Ng Just Dropped An AI Reviewer That Performs At Human Level</title>
    <updated>2025-11-24T19:44:03+00:00</updated>
    <author>
      <name>/u/AskGpts</name>
      <uri>https://old.reddit.com/user/AskGpts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5qzft/coursera_founder_and_ai_pioneer_andrew_ng_just/"&gt; &lt;img alt="Coursera Founder And AI Pioneer Andrew Ng Just Dropped An AI Reviewer That Performs At Human Level" src="https://preview.redd.it/xslefnsmd93g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d03c76fdecbb27a88d2b15a9cd2eaa3d225b151c" title="Coursera Founder And AI Pioneer Andrew Ng Just Dropped An AI Reviewer That Performs At Human Level" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Andrew Ng just announced a new Agentic Reviewer that gives research feedback approaching human-level performance.&lt;/p&gt; &lt;p&gt;It was trained on ICLR 2025 reviews and scored:&lt;/p&gt; &lt;p&gt;0.41 correlation between two human reviewers&lt;/p&gt; &lt;p&gt;0.42 correlation between the AI and a human reviewer&lt;/p&gt; &lt;p&gt;Meaning: The AI reviewer is now effectively as reliable as a human reviewer. And it can potentially replace the 6-month feedback loop researchers normally suffer through when submitting papers.&lt;/p&gt; &lt;p&gt;It searches arXiv for context, analyzes your paper, and returns structured review comments instantly.&lt;/p&gt; &lt;p&gt;For anyone who’s had a paper rejected multiple times and waited months each round… this could be game-changing.&lt;/p&gt; &lt;p&gt;Try the tool here:&lt;/p&gt; &lt;p&gt;👉 &lt;a href="https://paperreview.ai"&gt;https://paperreview.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AskGpts"&gt; /u/AskGpts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xslefnsmd93g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5qzft/coursera_founder_and_ai_pioneer_andrew_ng_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5qzft/coursera_founder_and_ai_pioneer_andrew_ng_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T19:44:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p69bea</id>
    <title>GLiNER2: Unified Schema-Based Information Extraction</title>
    <updated>2025-11-25T10:43:25+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p69bea/gliner2_unified_schemabased_information_extraction/"&gt; &lt;img alt="GLiNER2: Unified Schema-Based Information Extraction" src="https://b.thumbs.redditmedia.com/jbxiujDqa50qKCnpeAe_kYWcLq_ws_q5F6bgTLhSOkM.jpg" title="GLiNER2: Unified Schema-Based Information Extraction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLiNER2 is an efficient, unified information extraction system that combines named entity recognition, text classification, and hierarchical structured data extraction into a single 205M-parameter model. Built on a pretrained transformer encoder architecture and trained on 254,334 examples of real and synthetic data, it achieves competitive performance with large language models while running efficiently on CPU hardware without requiring GPUs or external APIs.&lt;/p&gt; &lt;p&gt;The system uses a schema-based interface where users can define extraction tasks declaratively through simple Python API calls, supporting features like entity descriptions, multi-label classification, nested structures, and multi-task composition in a single forward pass.&lt;/p&gt; &lt;p&gt;Released as an open-source pip-installable library under Apache 2.0 license with pre-trained models on Hugging Face, GLiNER2 demonstrates strong zero-shot performance across benchmarks—achieving 0.72 average accuracy on classification tasks and 0.590 F1 on the CrossNER benchmark—while maintaining approximately 2.6× speedup over GPT-4o on CPU.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2507.18546"&gt;https://arxiv.org/abs/2507.18546&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code repo: &lt;a href="https://github.com/fastino-ai/GLiNER2"&gt;https://github.com/fastino-ai/GLiNER2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Install: &lt;a href="https://pypi.org/project/gliner2"&gt;https://pypi.org/project/gliner2&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p69bea"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p69bea/gliner2_unified_schemabased_information_extraction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p69bea/gliner2_unified_schemabased_information_extraction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T10:43:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1p61ch2</id>
    <title>NVIDIA RTX PRO 6000 Blackwell desktop GPU drops to $7,999</title>
    <updated>2025-11-25T02:56:35+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p61ch2/nvidia_rtx_pro_6000_blackwell_desktop_gpu_drops/"&gt; &lt;img alt="NVIDIA RTX PRO 6000 Blackwell desktop GPU drops to $7,999" src="https://external-preview.redd.it/YCPQesYDmOPQ_XkQN8p_ciK514B0FKoU6bNyhy9mcvg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc94dfb5840da6920f1ea749a1fc15f8c8d11b76" title="NVIDIA RTX PRO 6000 Blackwell desktop GPU drops to $7,999" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you guys think that a RTX Quadro 8000 situation could happen again?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/nvidia-flagship-rtx-pro-6000-is-now-rtx-5080-cheaper-as-card-price-drops-to-7999"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p61ch2/nvidia_rtx_pro_6000_blackwell_desktop_gpu_drops/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p61ch2/nvidia_rtx_pro_6000_blackwell_desktop_gpu_drops/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T02:56:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5u44r</id>
    <title>That's why local models are better</title>
    <updated>2025-11-24T21:42:13+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5u44r/thats_why_local_models_are_better/"&gt; &lt;img alt="That's why local models are better" src="https://preview.redd.it/7s5e59vpy93g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91d4c29a99283e56fcfd8614cc10c6d72a0af91a" title="That's why local models are better" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That is why the local ones are better than the private ones in addition to this model is still expensive, I will be surprised when the US models reach an optimized price like those in China, the price reflects the optimization of the model, did you know ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7s5e59vpy93g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5u44r/thats_why_local_models_are_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5u44r/thats_why_local_models_are_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T21:42:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1p68sjf</id>
    <title>tencent/HunyuanOCR-1B</title>
    <updated>2025-11-25T10:10:33+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p68sjf/tencenthunyuanocr1b/"&gt; &lt;img alt="tencent/HunyuanOCR-1B" src="https://external-preview.redd.it/euNO2VS0UsDEnKIxd8MnYm5CABYmnLN8JLKug1m_WZw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6451703bfbd1fab35e662fdb90c099a069b6d25b" title="tencent/HunyuanOCR-1B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/HunyuanOCR"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p68sjf/tencenthunyuanocr1b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p68sjf/tencenthunyuanocr1b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T10:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6cf2p</id>
    <title>How are Chinese AI models claiming such low training costs? Did some research</title>
    <updated>2025-11-25T13:27:51+00:00</updated>
    <author>
      <name>/u/Acrobatic_Solid6023</name>
      <uri>https://old.reddit.com/user/Acrobatic_Solid6023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Doing my little assignment on model cost. deepseek claims $6M training cost. Everyones losing their minds cause ChatGPT-4 cost $40-80M and Gemini Ultra hit $190M.&lt;/p&gt; &lt;p&gt;Got curious if other Chinese models show similar patterns or if deepseeks just marketing bs.&lt;/p&gt; &lt;p&gt;What I found on training costs:&lt;/p&gt; &lt;p&gt;glm-4.6: $8-12M estimated&lt;/p&gt; &lt;ul&gt; &lt;li&gt;357B parameters (thats model size)&lt;/li&gt; &lt;li&gt;More believable than deepseeks $6M but still way under Western models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Kimi K2-0905: $25-35M estimated&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1T parameters total (MoE architecture, only ~32B active at once)&lt;/li&gt; &lt;li&gt;Closer to Western costs but still cheaper&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;MiniMax: $15-20M estimated&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mid-range model, mid-range cost&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;deepseek V3.2: $6M (their claim)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Seems impossibly low for GPU rental + training time&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why the difference?&lt;/p&gt; &lt;p&gt;Training cost = GPU hours × GPU price + electricity + data costs.&lt;/p&gt; &lt;p&gt;Chinese models might be cheaper because:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cheaper GPU access (domestic chips or bulk deals)&lt;/li&gt; &lt;li&gt;Lower electricity costs in China&lt;/li&gt; &lt;li&gt;More efficient training methods (though this is speculation)&lt;/li&gt; &lt;li&gt;Or theyre just lying about the real numbers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;deepseeks $6M feels like marketing. You cant rent enough H100s for months and only spend $6M unless youre getting massive subsidies or cutting major corners.&lt;/p&gt; &lt;p&gt;glms $8-12M is more realistic. Still cheap compared to Western models but not suspiciously fake-cheap.&lt;/p&gt; &lt;p&gt;Kimi at $25-35M shows you CAN build competitive models for less than $100M+ but probably not for $6M.&lt;/p&gt; &lt;p&gt;Are these real training costs or are they hiding infrastructure subsidies and compute deals that Western companies dont get?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acrobatic_Solid6023"&gt; /u/Acrobatic_Solid6023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6cf2p/how_are_chinese_ai_models_claiming_such_low/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6cf2p/how_are_chinese_ai_models_claiming_such_low/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6cf2p/how_are_chinese_ai_models_claiming_such_low/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T13:27:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax — Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I’m Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; — Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; — Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; — LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5retd</id>
    <title>Best Local VLMs - November 2025</title>
    <updated>2025-11-24T20:00:04+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite models are right now and &lt;strong&gt;&lt;em&gt;why&lt;/em&gt;&lt;/strong&gt;. Given the nature of the beast in evaluating VLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (what applications, how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T20:00:04+00:00</published>
  </entry>
</feed>
