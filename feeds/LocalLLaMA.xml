<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-05T23:24:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mifaqv</id>
    <title>gpt-oss-120b and 20b GGUFs</title>
    <updated>2025-08-05T17:20:53+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mifaqv/gptoss120b_and_20b_ggufs/"&gt; &lt;img alt="gpt-oss-120b and 20b GGUFs" src="https://external-preview.redd.it/INENbEwV6ABsrDTZjJHvECpyAUiCQ1gcEo-8476Qbvk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c691d4c1f3d07ee3536b3daa6084ba562a0cf9cf" title="gpt-oss-120b and 20b GGUFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/ggml-org/gpt-oss-20b-GGUF"&gt;https://huggingface.co/ggml-org/gpt-oss-20b-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ggml-org/gpt-oss-120b-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mifaqv/gptoss120b_and_20b_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mifaqv/gptoss120b_and_20b_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T17:20:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1milm9t</id>
    <title>Just wanna say : Kudos to llama cpp our unsung heroes ü´°</title>
    <updated>2025-08-05T21:15:05+00:00</updated>
    <author>
      <name>/u/dreamai87</name>
      <uri>https://old.reddit.com/user/dreamai87</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kudos to you guys&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dreamai87"&gt; /u/dreamai87 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1milm9t/just_wanna_say_kudos_to_llama_cpp_our_unsung/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1milm9t/just_wanna_say_kudos_to_llama_cpp_our_unsung/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1milm9t/just_wanna_say_kudos_to_llama_cpp_our_unsung/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T21:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi6brm</id>
    <title>Fast and local open source TTS engine. 20+ languages, multiple voices. Model size 25MB to 65MB. Can train on new voices.</title>
    <updated>2025-08-05T11:13:52+00:00</updated>
    <author>
      <name>/u/phone_radio_tv</name>
      <uri>https://old.reddit.com/user/phone_radio_tv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6brm/fast_and_local_open_source_tts_engine_20/"&gt; &lt;img alt="Fast and local open source TTS engine. 20+ languages, multiple voices. Model size 25MB to 65MB. Can train on new voices." src="https://external-preview.redd.it/Y3B1eTg4N2FwNmhmMcdJr-o9P4ZouvxhN_0BwF4rV8WaxRXMUy0jmPSG-wmF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c02f020e4a288dbe74ee37d8070bfab6a8b6d543" title="Fast and local open source TTS engine. 20+ languages, multiple voices. Model size 25MB to 65MB. Can train on new voices." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fast and local TTS engine. 20+ languages, multiple voices. Model size 25MB to 65MB (based on the language). Can train on new voices.&lt;/p&gt; &lt;p&gt;Github Link: &lt;a href="https://github.com/OHF-Voice/piper1-gpl"&gt;https://github.com/OHF-Voice/piper1-gpl&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phone_radio_tv"&gt; /u/phone_radio_tv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4f9mf37ap6hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6brm/fast_and_local_open_source_tts_engine_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi6brm/fast_and_local_open_source_tts_engine_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T11:13:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1minpqr</id>
    <title>Finally, a model that's SAFE</title>
    <updated>2025-08-05T22:39:10+00:00</updated>
    <author>
      <name>/u/RandumbRedditor1000</name>
      <uri>https://old.reddit.com/user/RandumbRedditor1000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1minpqr/finally_a_model_thats_safe/"&gt; &lt;img alt="Finally, a model that's SAFE" src="https://a.thumbs.redditmedia.com/vPXQi6mxUBYl7zt9fZCD3LWOB6PaZGcjaDHZr2r1u18.jpg" title="Finally, a model that's SAFE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/elpfx70g3ahf1.png?width=996&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09af507acfa40063fa0bed3df990bca01d097c81"&gt;https://preview.redd.it/elpfx70g3ahf1.png?width=996&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09af507acfa40063fa0bed3df990bca01d097c81&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks openai, you're really contributing to the open-source LLM community&lt;/p&gt; &lt;p&gt;I haven't been this blown away by a model since Llama 4!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandumbRedditor1000"&gt; /u/RandumbRedditor1000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1minpqr/finally_a_model_thats_safe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1minpqr/finally_a_model_thats_safe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1minpqr/finally_a_model_thats_safe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T22:39:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1milfbi</id>
    <title>Qwen3 dense instruct/coder/thinking models tomorrow?</title>
    <updated>2025-08-05T21:07:30+00:00</updated>
    <author>
      <name>/u/MR_-_501</name>
      <uri>https://old.reddit.com/user/MR_-_501</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1milfbi/qwen3_dense_instructcoderthinking_models_tomorrow/"&gt; &lt;img alt="Qwen3 dense instruct/coder/thinking models tomorrow?" src="https://preview.redd.it/pbi1dcacn9hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71abb79aa684b3ff0cd618c4a35f701067ce38ca" title="Qwen3 dense instruct/coder/thinking models tomorrow?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MR_-_501"&gt; /u/MR_-_501 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pbi1dcacn9hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1milfbi/qwen3_dense_instructcoderthinking_models_tomorrow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1milfbi/qwen3_dense_instructcoderthinking_models_tomorrow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T21:07:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mifzqz</id>
    <title>GPT-OSS-120B vs GLM 4.5 Air...</title>
    <updated>2025-08-05T17:45:59+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mifzqz/gptoss120b_vs_glm_45_air/"&gt; &lt;img alt="GPT-OSS-120B vs GLM 4.5 Air..." src="https://preview.redd.it/w52pmzpcn8hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e464e743e2abb8f3b1740018d19c8f245bf5d459" title="GPT-OSS-120B vs GLM 4.5 Air..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w52pmzpcn8hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mifzqz/gptoss120b_vs_glm_45_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mifzqz/gptoss120b_vs_glm_45_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T17:45:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi8lbl</id>
    <title>Qwen3 Coder vs. Kimi K2 vs. Sonnet 4 Coding Comparison (Tested on Qwen CLI)</title>
    <updated>2025-08-05T13:02:08+00:00</updated>
    <author>
      <name>/u/shricodev</name>
      <uri>https://old.reddit.com/user/shricodev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alibaba released Qwen3‚ÄëCoder (480B ‚Üí 35B active) alongside Qwen Code CLI, a complete fork of Gemini CLI for agentic coding workflows specifically adapted for Qwen3 Coder. I tested it head-to-head with Kimi K2 and Claude Sonnet 4 in practical coding tasks using the same CLI via &lt;strong&gt;OpenRouter&lt;/strong&gt; to keep things consistent for all models. The results surprised me.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;‚ÑπÔ∏è &lt;strong&gt;Note:&lt;/strong&gt; All test timings are based on the OpenRouter providers.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I've done some real-world coding tests for all three, not just regular prompts. Here are the three questions I asked all three models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CLI Chat MCP Client in Python:&lt;/strong&gt; Build a CLI chat MCP client in Python. More like a chat room. Integrate Composio integration for tool calls (Gmail, Slack, etc.).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Geometry Dash WebApp Simulation:&lt;/strong&gt; Build a web version of Geometry Dash.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Typing Test WebApp:&lt;/strong&gt; Build a monkeytype-like typing test app with a theme switcher (Catppuccin theme) and animations (typing trail).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Claude Sonnet 4 was the most reliable across all tasks, with complete, production-ready outputs. It was also the fastest, usually taking 5‚Äì7 minutes.&lt;/li&gt; &lt;li&gt;Qwen3-Coder surprised me with solid results, much faster than Kimi, though not quite on Claude‚Äôs level.&lt;/li&gt; &lt;li&gt;Kimi K2 writes good UI and follows standards well, but it is slow (20+ minutes on some tasks) and sometimes non-functional.&lt;/li&gt; &lt;li&gt;On tool-heavy prompts like MCP + Composio, Claude was the only one to get it right in one try.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Verdict&lt;/h1&gt; &lt;p&gt;Honestly, Qwen3-Coder feels like the best middle ground if you want budget-friendly coding without massive compromises. But for real coding speed, Claude still dominates all these recent models.&lt;/p&gt; &lt;p&gt;I can't see much hype around Kimi K2, to be honest. It's just painfully slow and not really as great as they say it is in coding. It's mid! (Keep in mind, timings are noted based on the OpenRouter providers.)&lt;/p&gt; &lt;p&gt;Here's a complete blog post with timings for all the tasks for each model and a nice demo here: &lt;a href="https://composio.dev/blog/qwen-3-coder-vs-kimi-k2-vs-claude-4-sonnet-coding-comparison"&gt;Qwen 3 Coder vs. Kimi K2 vs. Claude 4 Sonnet: Coding comparison&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear if anyone else has benchmarked these models with real coding projects.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shricodev"&gt; /u/shricodev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi8lbl/qwen3_coder_vs_kimi_k2_vs_sonnet_4_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi8lbl/qwen3_coder_vs_kimi_k2_vs_sonnet_4_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi8lbl/qwen3_coder_vs_kimi_k2_vs_sonnet_4_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T13:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mij7fh</id>
    <title>List of open-weight models with unmodified permissive licenses</title>
    <updated>2025-08-05T19:43:41+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just starting a community-driven thread.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;License&lt;/th&gt; &lt;th align="left"&gt;Commercial Use&lt;/th&gt; &lt;th align="left"&gt;Link to License&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 3&lt;/td&gt; &lt;td align="left"&gt;Apache 2.0 &lt;em&gt;unmodified&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;Allowed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3"&gt;LICENSE&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 2.5 excl. 3B &amp;amp; 72B&lt;/td&gt; &lt;td align="left"&gt;Apache 2.0 &lt;em&gt;unmodified&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;Allowed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-14B/blob/main/LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss-120b&lt;/td&gt; &lt;td align="left"&gt;Apache 2.0 &lt;em&gt;unmodified&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;Allowed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/openai/gpt-oss-120b/blob/main/LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss-20b&lt;/td&gt; &lt;td align="left"&gt;Apache 2.0 &lt;em&gt;unmodified&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;Allowed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/openai/gpt-oss-20b/blob/main/LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OLMo series (all)&lt;/td&gt; &lt;td align="left"&gt;Apache 2.0 &lt;em&gt;unmodified&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;Allowed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/allenai/OLMo-7B"&gt;LICENSE&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral and Magistral Small 3&lt;/td&gt; &lt;td align="left"&gt;Apache 2.0 &lt;em&gt;unmodified&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;Allowed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://mistral.ai/news/mistral-small-3"&gt;LICENSE&lt;/a&gt;, &lt;a href="https://mistral.ai/news/magistral"&gt;LICENSE&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek r1&lt;/td&gt; &lt;td align="left"&gt;MIT &lt;em&gt;unmodified&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;Allowed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1/blob/main/LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek v3-0324 (not 0528)&lt;/td&gt; &lt;td align="left"&gt;MIT &lt;em&gt;unmodified&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;Allowed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-0324/blob/main/LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4 0414&lt;/td&gt; &lt;td align="left"&gt;MIT &lt;em&gt;unmodified&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;Allowed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4-32B-0414/blob/main/LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5&lt;/td&gt; &lt;td align="left"&gt;MIT &lt;em&gt;unmodified&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;Allowed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.5"&gt;LICENSE&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IBM Granite&lt;/td&gt; &lt;td align="left"&gt;Apache &lt;em&gt;unmodified&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;Allowed&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/ibm-granite/granite-code-models/blob/main/LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Any others? Surprisingly small list.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mij7fh/list_of_openweight_models_with_unmodified/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mij7fh/list_of_openweight_models_with_unmodified/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mij7fh/list_of_openweight_models_with_unmodified/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T19:43:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1milmrl</id>
    <title>OpenAI gpt-oss-120b &amp; 20b EQ-Bench &amp; creative writing results</title>
    <updated>2025-08-05T21:15:36+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1milmrl/openai_gptoss120b_20b_eqbench_creative_writing/"&gt; &lt;img alt="OpenAI gpt-oss-120b &amp;amp; 20b EQ-Bench &amp;amp; creative writing results" src="https://b.thumbs.redditmedia.com/ddG4iHe_QohGbzMrf1QVWE9bWoVRavxmRobwbx0Do3Y.jpg" title="OpenAI gpt-oss-120b &amp;amp; 20b EQ-Bench &amp;amp; creative writing results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://eqbench.com/"&gt;https://eqbench.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;gpt-oss-120b:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Creative writing&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/openai__gpt-oss-120b.html"&gt;https://eqbench.com/results/creative-writing-v3/openai__gpt-oss-120b.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Longform writing:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-longform/openai__gpt-oss-120b_longform_report.html"&gt;https://eqbench.com/results/creative-writing-longform/openai__gpt-oss-120b_longform_report.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EQ-Bench:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/eqbench3_reports/openai__gpt-oss-120b.html"&gt;https://eqbench.com/results/eqbench3_reports/openai__gpt-oss-120b.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;gpt-oss-20b:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Creative writing&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-v3/openai__gpt-oss-20b.html"&gt;https://eqbench.com/results/creative-writing-v3/openai__gpt-oss-20b.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Longform writing:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/creative-writing-longform/openai__gpt-oss-20b_longform_report.html"&gt;https://eqbench.com/results/creative-writing-longform/openai__gpt-oss-20b_longform_report.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EQ-Bench:&lt;/p&gt; &lt;p&gt;&lt;a href="https://eqbench.com/results/eqbench3_reports/openai__gpt-oss-20b.html"&gt;https://eqbench.com/results/eqbench3_reports/openai__gpt-oss-20b.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1milmrl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1milmrl/openai_gptoss120b_20b_eqbench_creative_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1milmrl/openai_gptoss120b_20b_eqbench_creative_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T21:15:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mieyrn</id>
    <title>gpt-oss Benchmarks</title>
    <updated>2025-08-05T17:08:55+00:00</updated>
    <author>
      <name>/u/Ill-Association-8410</name>
      <uri>https://old.reddit.com/user/Ill-Association-8410</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mieyrn/gptoss_benchmarks/"&gt; &lt;img alt="gpt-oss Benchmarks" src="https://preview.redd.it/rzxruvqmg8hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ad0296b5e45c1e8edae93cf3af9a01c66d76d7d" title="gpt-oss Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill-Association-8410"&gt; /u/Ill-Association-8410 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rzxruvqmg8hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mieyrn/gptoss_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mieyrn/gptoss_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T17:08:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mihyr9</id>
    <title>Dang. I did not expect that. Nice job OpenAI.</title>
    <updated>2025-08-05T18:57:18+00:00</updated>
    <author>
      <name>/u/Crierlon</name>
      <uri>https://old.reddit.com/user/Crierlon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mihyr9/dang_i_did_not_expect_that_nice_job_openai/"&gt; &lt;img alt="Dang. I did not expect that. Nice job OpenAI." src="https://preview.redd.it/m3ailyyqz8hf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d34437a86ae378885d8ac0db8b6d0c61daec7668" title="Dang. I did not expect that. Nice job OpenAI." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta is done for if they don't go full FOSS. No wonder Zuck was so desperate to poach OpenAI employees. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Crierlon"&gt; /u/Crierlon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m3ailyyqz8hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mihyr9/dang_i_did_not_expect_that_nice_job_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mihyr9/dang_i_did_not_expect_that_nice_job_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T18:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mifc2l</id>
    <title>GPT OSS 120b and 20b is Apache 2.0!</title>
    <updated>2025-08-05T17:22:13+00:00</updated>
    <author>
      <name>/u/Synaps3</name>
      <uri>https://old.reddit.com/user/Synaps3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://openai.com/index/introducing-gpt-oss/"&gt;https://openai.com/index/introducing-gpt-oss/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Synaps3"&gt; /u/Synaps3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mifc2l/gpt_oss_120b_and_20b_is_apache_20/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mifc2l/gpt_oss_120b_and_20b_is_apache_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mifc2l/gpt_oss_120b_and_20b_is_apache_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T17:22:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mijza6</id>
    <title>vLLM latency/throughput benchmarks for gpt-oss-120b</title>
    <updated>2025-08-05T20:12:32+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mijza6/vllm_latencythroughput_benchmarks_for_gptoss120b/"&gt; &lt;img alt="vLLM latency/throughput benchmarks for gpt-oss-120b" src="https://preview.redd.it/bz9j2b92d9hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c7db73fb89602975462166e965bca3fd42eb685" title="vLLM latency/throughput benchmarks for gpt-oss-120b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran the &lt;a href="https://github.com/vllm-project/vllm/tree/main/benchmarks"&gt;vLLM provided benchmarks&lt;/a&gt; &lt;code&gt;serve&lt;/code&gt; (online serving throughput) and &lt;code&gt;throughput&lt;/code&gt; (offline serving throughput) for &lt;code&gt;gpt-oss-120b&lt;/code&gt; on my H100 96GB with the ShareGPT benchmark data.&lt;/p&gt; &lt;p&gt;Can confirm it fits snugly in 96GB. Numbers below.&lt;/p&gt; &lt;h1&gt;Throughput Benchmark (offline serving throughput)&lt;/h1&gt; &lt;p&gt;Command: &lt;code&gt;vllm bench serve --model &amp;quot;openai/gpt-oss-120b&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 1000 Benchmark duration (s): 47.81 Total input tokens: 1022745 Total generated tokens: 48223 Request throughput (req/s): 20.92 Output token throughput (tok/s): 1008.61 Total Token throughput (tok/s): 22399.88 ---------------Time to First Token---------------- Mean TTFT (ms): 18806.63 Median TTFT (ms): 18631.45 P99 TTFT (ms): 36522.62 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 283.85 Median TPOT (ms): 271.48 P99 TPOT (ms): 801.98 ---------------Inter-token Latency---------------- Mean ITL (ms): 231.50 Median ITL (ms): 267.02 P99 ITL (ms): 678.42 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Serve Benchmark (online serving throughput)&lt;/h1&gt; &lt;p&gt;Command: &lt;code&gt;vllm bench latency --model &amp;quot;openai/gpt-oss-120b&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Avg latency: 1.3391752537339925 seconds 10% percentile latency: 1.277150624152273 seconds 25% percentile latency: 1.30161597346887 seconds 50% percentile latency: 1.3404422830790281 seconds 75% percentile latency: 1.3767581032589078 seconds 90% percentile latency: 1.393262314144522 seconds 99% percentile latency: 1.4468831585347652 seconds &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bz9j2b92d9hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mijza6/vllm_latencythroughput_benchmarks_for_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mijza6/vllm_latencythroughput_benchmarks_for_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T20:12:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1miec9u</id>
    <title>Release v4.55.0: New openai GPT OSS model! ¬∑ huggingface/transformers</title>
    <updated>2025-08-05T16:46:13+00:00</updated>
    <author>
      <name>/u/lomero</name>
      <uri>https://old.reddit.com/user/lomero</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miec9u/release_v4550_new_openai_gpt_oss_model/"&gt; &lt;img alt="Release v4.55.0: New openai GPT OSS model! ¬∑ huggingface/transformers" src="https://external-preview.redd.it/o3wB6ioZOZhpNvqAVXe5Ffp8Gi7bbUI44EsDB2_JzvY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b72a9d22224f12c017f5e0c04a52db64d195c9b" title="Release v4.55.0: New openai GPT OSS model! ¬∑ huggingface/transformers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lomero"&gt; /u/lomero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/releases/tag/v4.55.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miec9u/release_v4550_new_openai_gpt_oss_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miec9u/release_v4550_new_openai_gpt_oss_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T16:46:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mi7bem</id>
    <title>New llama.cpp options make MoE offloading trivial: `--n-cpu-moe`</title>
    <updated>2025-08-05T12:04:17+00:00</updated>
    <author>
      <name>/u/Pristine-Woodpecker</name>
      <uri>https://old.reddit.com/user/Pristine-Woodpecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi7bem/new_llamacpp_options_make_moe_offloading_trivial/"&gt; &lt;img alt="New llama.cpp options make MoE offloading trivial: `--n-cpu-moe`" src="https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc3c355c6e5a0c2867654d9ea9c2e7c8ed9c618b" title="New llama.cpp options make MoE offloading trivial: `--n-cpu-moe`" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No more need for super-complex regular expression in the -ot option! Just do &lt;code&gt;--cpu-moe&lt;/code&gt; or &lt;code&gt;--n-cpu-moe #&lt;/code&gt; and reduce the number until the model no longer fits on the GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pristine-Woodpecker"&gt; /u/Pristine-Woodpecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15077"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mi7bem/new_llamacpp_options_make_moe_offloading_trivial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mi7bem/new_llamacpp_options_make_moe_offloading_trivial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T12:04:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1midu35</id>
    <title>GPT-OSS today!</title>
    <updated>2025-08-05T16:27:22+00:00</updated>
    <author>
      <name>/u/Jawshoeadan</name>
      <uri>https://old.reddit.com/user/Jawshoeadan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Keep an eye on these links! &lt;a href="https://github.com/openai/harmony"&gt;https://github.com/openai/harmony&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://openai.com/open-models"&gt;https://openai.com/open-models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://gpt-oss.com"&gt;https://gpt-oss.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: also this &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15091"&gt;https://github.com/ggml-org/llama.cpp/pull/15091&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jawshoeadan"&gt; /u/Jawshoeadan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1midu35/gptoss_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1midu35/gptoss_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1midu35/gptoss_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T16:27:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1milkqp</id>
    <title>Run gpt-oss locally with Unsloth GGUFs + Fixes!</title>
    <updated>2025-08-05T21:13:26+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1milkqp/run_gptoss_locally_with_unsloth_ggufs_fixes/"&gt; &lt;img alt="Run gpt-oss locally with Unsloth GGUFs + Fixes!" src="https://preview.redd.it/6s62jsx2o9hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4a03b38836e71df4373dc670859d4fca8398ff1" title="Run gpt-oss locally with Unsloth GGUFs + Fixes!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! You can now run OpenAI's gpt-oss-120b &amp;amp; 20b open models locally with our &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; GGUFs! ü¶•&lt;/p&gt; &lt;p&gt;The uploads includes some of our chat template fixes including casing errors and other fixes. We also reuploaded the quants to facilitate OpenAI's recent change to their chat template and our new fixes.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;20b GGUF: &lt;a href="https://huggingface.co/unsloth/gpt-oss-20b-GGUF"&gt;https://huggingface.co/unsloth/gpt-oss-20b-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;120b GGUF: &lt;a href="https://huggingface.co/unsloth/gpt-oss-120b-GGUF"&gt;https://huggingface.co/unsloth/gpt-oss-120b-GGUF&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can run both of the models in original precision with the GGUFs. The 120b model fits on 66GB RAM/unified mem &amp;amp; 20b model on 14GB RAM/unified mem. Both will run at &amp;gt;6 token/s. The original model were in f4 but we renamed it to bf16 for easier navigation.&lt;/p&gt; &lt;p&gt;Guide to run model: &lt;a href="https://docs.unsloth.ai/basics/gpt-oss"&gt;https://docs.unsloth.ai/basics/gpt-oss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Instructions&lt;/strong&gt;: You must build llama.cpp from source. Update llama.cpp, Ollama, LM Studio etc. to run&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama.cpp/llama-cli \ -hf unsloth/gpt-oss-20b-GGUF:F16 \ --jinja -ngl 99 --threads -1 --ctx-size 16384 \ --temp 0.6 --top-p 1.0 --top-k 0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Or Ollama:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ollama run hf.co/unsloth/gpt-oss-20b-GGUF &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To run the &lt;strong&gt;120B model&lt;/strong&gt; via llama.cpp:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama.cpp/llama-cli \ --model unsloth/gpt-oss-120b-GGUF/gpt-oss-120b-F16.gguf \ --threads -1 \ --ctx-size 16384 \ --n-gpu-layers 99 \ -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; \ --temp 0.6 \ --min-p 0.0 \ --top-p 1.0 \ --top-k 0.0 \ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Thanks for the support guys and happy running. ü•∞&lt;/p&gt; &lt;p&gt;Finetuning support coming soon (likely tomorrow)!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6s62jsx2o9hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1milkqp/run_gptoss_locally_with_unsloth_ggufs_fixes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1milkqp/run_gptoss_locally_with_unsloth_ggufs_fixes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T21:13:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mifuqk</id>
    <title>gpt-oss-120b outperforms DeepSeek-R1-0528 in benchmarks</title>
    <updated>2025-08-05T17:40:56+00:00</updated>
    <author>
      <name>/u/oobabooga4</name>
      <uri>https://old.reddit.com/user/oobabooga4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a table I put together:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Benchmark&lt;/th&gt; &lt;th&gt;DeepSeek-R1&lt;/th&gt; &lt;th&gt;DeepSeek-R1-0528&lt;/th&gt; &lt;th&gt;GPT-OSS-20B&lt;/th&gt; &lt;th&gt;GPT-OSS-120B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GPQA Diamond&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;71.5&lt;/td&gt; &lt;td&gt;81.0&lt;/td&gt; &lt;td&gt;71.5&lt;/td&gt; &lt;td&gt;80.1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Humanity's Last Exam&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;8.5&lt;/td&gt; &lt;td&gt;17.7&lt;/td&gt; &lt;td&gt;17.3&lt;/td&gt; &lt;td&gt;19.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;AIME 2024&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;79.8&lt;/td&gt; &lt;td&gt;91.4&lt;/td&gt; &lt;td&gt;96.0&lt;/td&gt; &lt;td&gt;96.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;AIME 2025&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;70.0&lt;/td&gt; &lt;td&gt;87.5&lt;/td&gt; &lt;td&gt;98.7&lt;/td&gt; &lt;td&gt;97.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Average&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;57.5&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;69.4&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;70.9&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;73.4&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;based on&lt;/p&gt; &lt;p&gt;&lt;a href="https://openai.com/open-models/"&gt;https://openai.com/open-models/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528"&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-0528&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Here is the table without AIME, as some have pointed out the GPT-OSS benchmarks used tools while the DeepSeek ones did not:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Benchmark&lt;/th&gt; &lt;th&gt;DeepSeek-R1&lt;/th&gt; &lt;th&gt;DeepSeek-R1-0528&lt;/th&gt; &lt;th&gt;GPT-OSS-20B&lt;/th&gt; &lt;th&gt;GPT-OSS-120B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GPQA Diamond&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;71.5&lt;/td&gt; &lt;td&gt;81.0&lt;/td&gt; &lt;td&gt;71.5&lt;/td&gt; &lt;td&gt;80.1&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Humanity's Last Exam&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;8.5&lt;/td&gt; &lt;td&gt;17.7&lt;/td&gt; &lt;td&gt;17.3&lt;/td&gt; &lt;td&gt;19.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Average&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;40.0&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;49.4&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;44.4&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;49.6&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oobabooga4"&gt; /u/oobabooga4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mifuqk/gptoss120b_outperforms_deepseekr10528_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mifuqk/gptoss120b_outperforms_deepseekr10528_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mifuqk/gptoss120b_outperforms_deepseekr10528_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T17:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mhyzp7</id>
    <title>Kitten TTS : SOTA Super-tiny TTS Model (Less than 25 MB)</title>
    <updated>2025-08-05T03:52:26+00:00</updated>
    <author>
      <name>/u/ElectricalBar7464</name>
      <uri>https://old.reddit.com/user/ElectricalBar7464</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"&gt; &lt;img alt="Kitten TTS : SOTA Super-tiny TTS Model (Less than 25 MB)" src="https://external-preview.redd.it/ajlvMGd2aWhpNGhmMUpar5lWZvhVHx9_BWGYhGbOyuld4cLO275_Q90LHrwX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc51726f166f8072e9a0767410a3b105af874a6d" title="Kitten TTS : SOTA Super-tiny TTS Model (Less than 25 MB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Model introduction:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Kitten ML has released open source code and weights of their new TTS model's preview.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/KittenML/KittenTTS"&gt;https://github.com/KittenML/KittenTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Huggingface: &lt;a href="https://huggingface.co/KittenML/kitten-tts-nano-0.1"&gt;https://huggingface.co/KittenML/kitten-tts-nano-0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model is less than 25 MB, around 15M parameters. The full release next week will include another open source ~80M parameter model with these same 8 voices, that can also run on CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features and Advantages&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Eight Different Expressive voices - 4 female and 4 male voices. For a tiny model, the expressivity sounds pretty impressive. This release will support TTS in English and multilingual support expected in future releases.&lt;/li&gt; &lt;li&gt;Super-small in size: The two text to speech models will be ~15M and ~80M parameters .&lt;/li&gt; &lt;li&gt;Can literally run anywhere lol : Forget ‚ÄúNo gpu required.‚Äù - this thing can even run on raspberry pi‚Äôs and phones. Great news for gpu-poor folks like me.&lt;/li&gt; &lt;li&gt;Open source (hell yeah!): the model can used for free.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElectricalBar7464"&gt; /u/ElectricalBar7464 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vdfv5uihi4hf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mhyzp7/kitten_tts_sota_supertiny_tts_model_less_than_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T03:52:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mic8kf</id>
    <title>Llama.cpp: Add GPT-OSS</title>
    <updated>2025-08-05T15:25:57+00:00</updated>
    <author>
      <name>/u/atgctg</name>
      <uri>https://old.reddit.com/user/atgctg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mic8kf/llamacpp_add_gptoss/"&gt; &lt;img alt="Llama.cpp: Add GPT-OSS" src="https://external-preview.redd.it/SMmA2lbsQDUuflgGCV0_YBw5k-KcfZS-9iAMN58tb_s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=95a9e3e605eab496c9ba148173f1d7e68a1d7e9f" title="Llama.cpp: Add GPT-OSS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atgctg"&gt; /u/atgctg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15091"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mic8kf/llamacpp_add_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mic8kf/llamacpp_add_gptoss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T15:25:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1midi67</id>
    <title>GPT-OSS today?</title>
    <updated>2025-08-05T16:14:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1midi67/gptoss_today/"&gt; &lt;img alt="GPT-OSS today?" src="https://preview.redd.it/2br9oi8178hf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=20cd517e2220fa7745b9e909a9f4bfcf589d5f03" title="GPT-OSS today?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;because this is almost merged &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15091"&gt;https://github.com/ggml-org/llama.cpp/pull/15091&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2br9oi8178hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1midi67/gptoss_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1midi67/gptoss_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T16:14:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1migo6d</id>
    <title>I FEEL SO SAFE! THANK YOU SO MUCH OPENAI!</title>
    <updated>2025-08-05T18:10:18+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1migo6d/i_feel_so_safe_thank_you_so_much_openai/"&gt; &lt;img alt="I FEEL SO SAFE! THANK YOU SO MUCH OPENAI!" src="https://preview.redd.it/7e3v67opr8hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c78bd2d594d80d839e43d136cedfee6e05b2b464" title="I FEEL SO SAFE! THANK YOU SO MUCH OPENAI!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It also lacks all general knowledge and is terrible at coding compared to the same sized GLM air, what is the use case here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7e3v67opr8hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1migo6d/i_feel_so_safe_thank_you_so_much_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1migo6d/i_feel_so_safe_thank_you_so_much_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T18:10:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mieqcb</id>
    <title>openai/gpt-oss-120b ¬∑ Hugging Face</title>
    <updated>2025-08-05T17:00:37+00:00</updated>
    <author>
      <name>/u/ShreckAndDonkey123</name>
      <uri>https://old.reddit.com/user/ShreckAndDonkey123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mieqcb/openaigptoss120b_hugging_face/"&gt; &lt;img alt="openai/gpt-oss-120b ¬∑ Hugging Face" src="https://external-preview.redd.it/12ojQ9khZuJRm7jqdMaOtnKaFtBC6Yo7dfwq4qKZ3jA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ae7c659a21f868f6dba51b958c810a90c5bfe24" title="openai/gpt-oss-120b ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShreckAndDonkey123"&gt; /u/ShreckAndDonkey123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/openai/gpt-oss-120b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mieqcb/openaigptoss120b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mieqcb/openaigptoss120b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T17:00:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1migl0k</id>
    <title>gpt-oss-120b is safetymaxxed (cw: explicit safety)</title>
    <updated>2025-08-05T18:07:05+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o893aealq8hf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1migl0k/gptoss120b_is_safetymaxxed_cw_explicit_safety/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1migl0k/gptoss120b_is_safetymaxxed_cw_explicit_safety/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T18:07:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1miezct</id>
    <title>üöÄ OpenAI released their open-weight models!!!</title>
    <updated>2025-08-05T17:09:35+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miezct/openai_released_their_openweight_models/"&gt; &lt;img alt="üöÄ OpenAI released their open-weight models!!!" src="https://preview.redd.it/1yckal6wg8hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc6b586f5d511d8c0e30969100e707e6e00a1815" title="üöÄ OpenAI released their open-weight models!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Welcome to the gpt-oss series, OpenAI‚Äôs open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.&lt;/p&gt; &lt;p&gt;We‚Äôre releasing two flavors of the open models:&lt;/p&gt; &lt;p&gt;gpt-oss-120b ‚Äî for production, general purpose, high reasoning use cases that fits into a single H100 GPU (117B parameters with 5.1B active parameters)&lt;/p&gt; &lt;p&gt;gpt-oss-20b ‚Äî for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/openai/gpt-oss-120b"&gt;https://huggingface.co/openai/gpt-oss-120b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1yckal6wg8hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miezct/openai_released_their_openweight_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miezct/openai_released_their_openweight_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T17:09:35+00:00</published>
  </entry>
</feed>
