<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-03T10:08:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pcofbo</id>
    <title>Qwen3-Next speed on Mac Studio M3 Ultra with llama.cpp</title>
    <updated>2025-12-02T23:41:33+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcofbo/qwen3next_speed_on_mac_studio_m3_ultra_with/"&gt; &lt;img alt="Qwen3-Next speed on Mac Studio M3 Ultra with llama.cpp" src="https://preview.redd.it/r7psl6zumv4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a4d6300b08facbf12024fd9dc0ff979a01b4ca0" title="Qwen3-Next speed on Mac Studio M3 Ultra with llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was excited to try out Qwen3-Next at long last after the recent PR merge for llama.cpp. &lt;/p&gt; &lt;p&gt;Unfortunately its far too slow (11tok/s). Bigger models like Minimax M2 (REAP version) and gpt-oss:120b are running 3x and 7x as fast. &lt;/p&gt; &lt;p&gt;Sharing a data point&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r7psl6zumv4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcofbo/qwen3next_speed_on_mac_studio_m3_ultra_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcofbo/qwen3next_speed_on_mac_studio_m3_ultra_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T23:41:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pc6fks</id>
    <title>[Release] We built Step-Audio-R1: The first open-source Audio LLM that truly Reasons (CoT) and Scales ‚Äì Beats Gemini 2.5 Pro on Audio Benchmarks.</title>
    <updated>2025-12-02T11:48:13+00:00</updated>
    <author>
      <name>/u/BadgerProfessional43</name>
      <uri>https://old.reddit.com/user/BadgerProfessional43</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;üî• TL;DR:&lt;/strong&gt; We (the StepFun AI team) just released the weights for Step-Audio-R1, an audio-language model that performs Chain-of-Thought (CoT) reasoning directly on acoustic features. This solves the persistent &amp;quot;inverted scaling&amp;quot; problem in audio LLMs.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üëã Hello, &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; Community! (The System 2 Audio LLM)&lt;/h3&gt; &lt;p&gt;We've seen some of you discussing Step-Audio-R1 already, and we wanted to jump in as the creators to give a technical deep dive and answer any questions.&lt;/p&gt; &lt;p&gt;Most multi-modal LLMs (especially in audio) cheat: they transcribe the audio and then just reason over the &lt;em&gt;text&lt;/em&gt;. This fails when the acoustic nuance (tone, emotion, multiple speakers, sound effects) is key. We fixed this.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step-Audio-R1 is the first audio model that successfully benefits from test-time compute scaling.&lt;/strong&gt; This means the model gets better, not worse, when given more time/tokens to think.&lt;/p&gt; &lt;h3&gt;üß† The Technical Breakthrough: Modality-Grounded Reasoning&lt;/h3&gt; &lt;p&gt;The core innovation is our training framework: &lt;strong&gt;Modality-Grounded Reasoning Distillation (MGRD)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Traditional models rely on &lt;strong&gt;Textual Surrogate Reasoning&lt;/strong&gt;. They think like this: 1. Input Audio $\rightarrow$ 2. Transcribe to Text $\rightarrow$ 3. Reason on Text $\rightarrow$ 4. Output.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MGRD&lt;/strong&gt; forces the model (based on Qwen2.5 32B + Qwen2 Audio Encoder) to ground its thoughts in the acoustic data itself. It generates explicit reasoning (e.g., using &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; tokens) that is directly tied to the underlying sound, not just the transcript. This is how we solved the &amp;quot;inverted scaling&amp;quot; anomaly‚Äîa huge step for reliable audio intelligence.&lt;/p&gt; &lt;h3&gt;üìà Performance: Benchmarking against the Best&lt;/h3&gt; &lt;p&gt;We focused on complex audio reasoning benchmarks where this acoustic understanding is non-negotiable.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Result:&lt;/strong&gt; Step-Audio-R1 &lt;strong&gt;surpasses&lt;/strong&gt; Gemini 2.5 Pro and is comparable to Gemini 3 across comprehensive audio benchmarks. We are making extended deliberation an asset, not a liability.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;üíª Important: Hardware &amp;amp; Quantization (We Need Your Help!)&lt;/h3&gt; &lt;p&gt;We are committed to accessibility, but this is a large, state-of-the-art model built on a 32B parameter base.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;VRAM Requirement (FP16/BF16):&lt;/strong&gt; The base model requires approximately &lt;strong&gt;65 GB - 70 GB VRAM&lt;/strong&gt; for deployment (We tested it successfully on a 4-GPU cluster using vLLM, as detailed in our README).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;vLLM Support:&lt;/strong&gt; Inference code is included with customized vLLM support for high throughput.&lt;/li&gt; &lt;/ul&gt; &lt;h4&gt;&lt;strong&gt;Call to Action: GGUF/Quantization Request!&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;To bring Step-Audio-R1 to single-card users (e.g., those with 24GB 3090/4090s), we urgently need help from the community's expert quantizers.&lt;/p&gt; &lt;p&gt;If you are skilled in creating &lt;strong&gt;GGUF&lt;/strong&gt; or &lt;strong&gt;EXL2&lt;/strong&gt; quants, please reach out! Your work will enable thousands of local users to try the model. Feel free to tag experts like &lt;a href="/u/TheBloke"&gt;u/TheBloke&lt;/a&gt; in the comments‚Äîwe want to collaborate!&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üîó Links and Next Steps&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub Repository (Code &amp;amp; Documentation):&lt;/strong&gt; &lt;code&gt;[https://github.com/stepfun-ai/Step-Audio-R1]&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hugging Face Model Card (Weights):&lt;/strong&gt; &lt;code&gt;[https://huggingface.co/stepfun-ai/Step-Audio-R1]&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Technical Report (arXiv):&lt;/strong&gt; &lt;code&gt;[https://arxiv.org/pdf/2511.15848]&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live Demo (HF Spaces/Gradio):&lt;/strong&gt; &lt;code&gt;[https://stepaudiollm.github.io/step-audio-r1/]&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ask us anything about MGRD, the training data, the Qwen2 integration, or the inference stack! We'll be answering questions for the next several hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadgerProfessional43"&gt; /u/BadgerProfessional43 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pc6fks/release_we_built_stepaudior1_the_first_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pc6fks/release_we_built_stepaudior1_the_first_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pc6fks/release_we_built_stepaudior1_the_first_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T11:48:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pczbeo</id>
    <title>Apple Studio M1 Ultra 128GB -&gt; it is still worth for LLM?</title>
    <updated>2025-12-03T08:59:49+00:00</updated>
    <author>
      <name>/u/JonasTecs</name>
      <uri>https://old.reddit.com/user/JonasTecs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi , &lt;/p&gt; &lt;p&gt;considering to buy M1 Ultra studio, but dont have recent benchmark info.&lt;/p&gt; &lt;p&gt;How much is capable with e.g qwen3 70b in regards of TPS ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JonasTecs"&gt; /u/JonasTecs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pczbeo/apple_studio_m1_ultra_128gb_it_is_still_worth_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pczbeo/apple_studio_m1_ultra_128gb_it_is_still_worth_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pczbeo/apple_studio_m1_ultra_128gb_it_is_still_worth_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T08:59:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pch1on</id>
    <title>CLI for fine-tuning (SFT, RL, DPO, ORPO, PPO) - inference for test + MPS support</title>
    <updated>2025-12-02T18:56:57+00:00</updated>
    <author>
      <name>/u/OkOwl6744</name>
      <uri>https://old.reddit.com/user/OkOwl6744</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pch1on/cli_for_finetuning_sft_rl_dpo_orpo_ppo_inference/"&gt; &lt;img alt="CLI for fine-tuning (SFT, RL, DPO, ORPO, PPO) - inference for test + MPS support" src="https://preview.redd.it/2tb5y1zi8u4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54533e182a16939f850f12f3faf74b02a8f4f199" title="CLI for fine-tuning (SFT, RL, DPO, ORPO, PPO) - inference for test + MPS support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had a lot of problems running trainings on runpod and other virtual environments after testing on my local Mac. Tried finding some open source projects to abstract some work and couldn‚Äôt find much other than autotrain from HF, but it was an old project needing new recipes and revamping..&lt;/p&gt; &lt;p&gt;So I took the idiot path to save a few minutes and spent a few months working on a full cli + api + wizard on top of autotrain. Supports SFT, DPO, ORPO, PPO, sweeps, reward modeling, distillation, RL environments and more. &lt;/p&gt; &lt;p&gt;You can search a model from HuggingFace (or paste any ID), point it at a dataset, and it figures out the format and converts it to chat template. Works on Mac and NVIDIA - detects your hardware and sets things up accordingly.&lt;/p&gt; &lt;p&gt;After training you can run aitraining chat to test your models locally and compare different runs. Built on top of HuggingFace‚Äôs stuff. Open source.&lt;/p&gt; &lt;p&gt;pip install aitraining&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/monostate/aitraining"&gt;https://github.com/monostate/aitraining&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Lmk if you think this is trash üëå&lt;/p&gt; &lt;p&gt;If you test it and like it, pls star ‚≠ê on GitHub &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OkOwl6744"&gt; /u/OkOwl6744 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2tb5y1zi8u4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pch1on/cli_for_finetuning_sft_rl_dpo_orpo_ppo_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pch1on/cli_for_finetuning_sft_rl_dpo_orpo_ppo_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T18:56:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd082c</id>
    <title>[P] Built Multi-LLM Orchestrator ‚Äì unified Python client for GigaChat, YandexGPT &amp; Ollama with auto-fallback</title>
    <updated>2025-12-03T09:58:16+00:00</updated>
    <author>
      <name>/u/Subject_Pen_4816</name>
      <uri>https://old.reddit.com/user/Subject_Pen_4816</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLLaMA! I built a Python library that solves a problem I kept hitting: how do you use Ollama as your primary LLM but seamlessly fall back to cloud providers (like Russian GigaChat/YandexGPT) when your local GPU is busy or the model isn't downloaded yet?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multi-LLM Orchestrator&lt;/strong&gt; provides a unified interface across providers with smart routing and automatic fallback.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Streaming support for all providers (Ollama, GigaChat, YandexGPT)&lt;/li&gt; &lt;li&gt;Auto-fallback: If GigaChat returns 500 ‚Üí automatically switches to YandexGPT before the user sees any output&lt;/li&gt; &lt;li&gt;LangChain integration: Drop-in replacement for any BaseLLM&lt;/li&gt; &lt;li&gt;92% test coverage (133 tests including streaming)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why Ollama + Cloud Fallback?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Local models are great, but:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Your GPU might be busy training another model&lt;/li&gt; &lt;li&gt;You might not have that specific model downloaded yet&lt;/li&gt; &lt;li&gt;Some queries need larger context windows than your hardware can handle&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The orchestrator lets you configure Ollama as primary and cloud providers as backup. Fully async (httpx/asyncio).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real-world performance (tested with actual APIs):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;TTFT: 1.4s (including OAuth2 auth)&lt;/li&gt; &lt;li&gt;Speed: 137 tokens/sec on real GigaChat API&lt;/li&gt; &lt;li&gt;Fallback time: &amp;lt;500ms to switch providers on error&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Quick example:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from orchestrator import Router from orchestrator.providers import OllamaProvider, GigaChatProvider, ProviderConfig router = Router(strategy=&amp;quot;first-available&amp;quot;) # Try Ollama first router.add_provider(OllamaProvider( ProviderConfig(name=&amp;quot;local&amp;quot;, base_url=&amp;quot;http://localhost:11434&amp;quot;, model=&amp;quot;llama3.2&amp;quot;) )) # Fallback to GigaChat if Ollama fails router.add_provider(GigaChatProvider( ProviderConfig(name=&amp;quot;cloud&amp;quot;, api_key=&amp;quot;...&amp;quot;) )) # If Ollama is down/busy, automatically uses GigaChat response = await router.route(&amp;quot;Explain quantum computing&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/MikhailMalorod/Multi-LLM-Orchestrator"&gt;https://github.com/MikhailMalorod/Multi-LLM-Orchestrator&lt;/a&gt;&lt;/li&gt; &lt;li&gt;PyPI: &lt;code&gt;pip install multi-llm-orchestrator&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Technical deep-dive (Russian): &lt;a href="https://habr.com/ru/articles/972740/"&gt;https://habr.com/ru/articles/972740/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Open to feedback! Especially curious if anyone has similar setups or faced the same &amp;quot;local + cloud fallback&amp;quot; problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Subject_Pen_4816"&gt; /u/Subject_Pen_4816 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd082c/p_built_multillm_orchestrator_unified_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd082c/p_built_multillm_orchestrator_unified_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd082c/p_built_multillm_orchestrator_unified_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T09:58:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcp927</id>
    <title>Qwen 3 0.6B, 1.7B and 4B</title>
    <updated>2025-12-03T00:16:53+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What have people been building with the small Qwens?&lt;/p&gt; &lt;p&gt;Which tasks have you found them to perform well on or poorly?&lt;/p&gt; &lt;p&gt;So far my experience has been that you really need the 1.7B most of the time over the 0.6B, as it gains the ability to handle simple text tasks more reliably, but in a basic manner. The jump up to 4B makes it much more robust with broader knowledge and has almost a 7B feel. The 1.7B is adequate enough for simple tasks though.&lt;/p&gt; &lt;p&gt;Does this experience track?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp927/qwen_3_06b_17b_and_4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp927/qwen_3_06b_17b_and_4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp927/qwen_3_06b_17b_and_4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T00:16:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcyp5z</id>
    <title>The security risks of "Emoji Smuggling" and Hidden Prompts for Local Agents</title>
    <updated>2025-12-03T08:18:48+00:00</updated>
    <author>
      <name>/u/jokiruiz</name>
      <uri>https://old.reddit.com/user/jokiruiz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Long-time lurker here. We spend a lot of time optimizing inference speeds, quantization, and finding the best uncensored models. But I've been thinking about the security implications for Local Agents that have access to our tools/APIs.&lt;/p&gt; &lt;p&gt;I created a video demonstrating Prompt Injection techniques, specifically focusing on:&lt;/p&gt; &lt;p&gt;Emoji Smuggling: How malicious instructions can be encoded in tokens that humans ignore (like emojis) but the LLM interprets as commands.&lt;/p&gt; &lt;p&gt;Indirect Injection: The risk when we let a local model summarize a webpage or read an email that contains hidden prompts. I think the visual demonstrations (I use the Gandalf game for the logic examples) are easy to follow even without audio.&lt;/p&gt; &lt;p&gt;- Video Link: &lt;a href="https://youtu.be/Kck8JxHmDOs?si=icxpXu6t2OrI0hFk"&gt;https://youtu.be/Kck8JxHmDOs?si=icxpXu6t2OrI0hFk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discussion topic: For those of you running local agents with tool access (like function calling in Llama 3 or Mistral), do you implement any input sanitization layer? Or are we just trusting the model to not execute a hidden instruction found in a scraped website?&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts on securing local deployments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jokiruiz"&gt; /u/jokiruiz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcyp5z/the_security_risks_of_emoji_smuggling_and_hidden/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcyp5z/the_security_risks_of_emoji_smuggling_and_hidden/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcyp5z/the_security_risks_of_emoji_smuggling_and_hidden/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T08:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pctktp</id>
    <title>Open Source Alternative to NotebookLM</title>
    <updated>2025-12-03T03:34:26+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RBAC (Role Based Access for Teams)&lt;/li&gt; &lt;li&gt;Notion Like Document Editing experience&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Note Management (Like Notion)&lt;/li&gt; &lt;li&gt;Multi Collaborative Chats.&lt;/li&gt; &lt;li&gt;Multi Collaborative Documents.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pctktp/open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pctktp/open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pctktp/open_source_alternative_to_notebooklm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T03:34:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pchuvk</id>
    <title>Qwen3 VL built from scratch with PyTorch</title>
    <updated>2025-12-02T19:26:13+00:00</updated>
    <author>
      <name>/u/No-Compote-6794</name>
      <uri>https://old.reddit.com/user/No-Compote-6794</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pchuvk/qwen3_vl_built_from_scratch_with_pytorch/"&gt; &lt;img alt="Qwen3 VL built from scratch with PyTorch" src="https://preview.redd.it/m7gqtnm2du4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=693e3c8c4453fc8e36a6a9ec7a0c64542089aeab" title="Qwen3 VL built from scratch with PyTorch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I updated my &lt;a href="https://github.com/Emericen/tiny-qwen"&gt;Tiny-Qwen repo&lt;/a&gt; to support Qwen3 VL. It is a minimal PyTorch re-implementation of the open source model served behind a fancy CLI.&lt;/p&gt; &lt;p&gt;The code is IMO quite simple and easy to follow and hack around. If you are looking to learn how multi-modal LLM's work and find Hugging Face Transformers code verbose, then this repo is for you :)&lt;/p&gt; &lt;p&gt;This line of work is heavily inspired by Andrej Karpathy's nanoGPT (albeit I'm not nearly as good as he is). I always wished that style could be used on open source models so I did it myself. You can also find older versions of Qwen in the same repo as well as DeepSeek R1. I've linked them in the repo's readme.&lt;/p&gt; &lt;p&gt;If you find this helpful, please please please star the repo ü§ó&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Compote-6794"&gt; /u/No-Compote-6794 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m7gqtnm2du4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pchuvk/qwen3_vl_built_from_scratch_with_pytorch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pchuvk/qwen3_vl_built_from_scratch_with_pytorch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T19:26:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcrc1j</id>
    <title>apple/starflow ¬∑ Hugging Face</title>
    <updated>2025-12-03T01:50:16+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcrc1j/applestarflow_hugging_face/"&gt; &lt;img alt="apple/starflow ¬∑ Hugging Face" src="https://external-preview.redd.it/9MKZtuMzr_COQCqOF3P3AvNoeDUxBXmS4TeVDsiZt8c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=00b6a986e0fe8fa76f89639e776eee93811f8828" title="apple/starflow ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;STARFlow introduces a novel transformer autoregressive flow architecture that combines the expressiveness of autoregressive models with the efficiency of normalizing flows. The model achieves state-of-the-art results in both text-to-image and text-to-video generation tasks.&lt;/p&gt; &lt;p&gt;STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis (NeurIPS 2025 Spotlight) STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flows (Arxiv)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/apple/starflow"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcrc1j/applestarflow_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcrc1j/applestarflow_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T01:50:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcwffb</id>
    <title>Llama 3.1 70B + one prompt now beats Claude 3.5 Sonnet (96.9% on Arena-Hard-Auto, 4% refusals)</title>
    <updated>2025-12-03T06:00:57+00:00</updated>
    <author>
      <name>/u/NoSir261</name>
      <uri>https://old.reddit.com/user/NoSir261</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent the last few weeks iterating a single system prompt until stock Llama-3.1-70B-Instruct started outperforming Claude 3.5 Sonnet on the hardest blind arena benchmark. Results (100% reproducible):&lt;/p&gt; &lt;p&gt;‚Ä¢ 96.4‚Äì96.9% win rate on Arena-Hard-Auto (vs Sonnet‚Äôs 94.7%) ‚Ä¢ Only 4% refusals (base model is ~25‚Äì30%) ‚Ä¢ Dense, creative, actually useful output&lt;/p&gt; &lt;p&gt;No fine-tune, no LoRA, no quantization tricks. Just one prompt.&lt;/p&gt; &lt;p&gt;Full X thread with JSONL proof + evals: &lt;a href="https://x.com/BrSanch/status/1864123456789012345"&gt;https://x.com/BrSanch/status/1864123456789012345&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I think prompt engineering can do a lot more than most people think it can.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoSir261"&gt; /u/NoSir261 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcwffb/llama_31_70b_one_prompt_now_beats_claude_35/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcwffb/llama_31_70b_one_prompt_now_beats_claude_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcwffb/llama_31_70b_one_prompt_now_beats_claude_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T06:00:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pc6i8v</id>
    <title>Only the real ones remember (he is still the contributor with the most likes for his models)</title>
    <updated>2025-12-02T11:52:35+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pc6i8v/only_the_real_ones_remember_he_is_still_the/"&gt; &lt;img alt="Only the real ones remember (he is still the contributor with the most likes for his models)" src="https://b.thumbs.redditmedia.com/jy0GG6iG37_fVgalRs58BxRL8j79pwfEfAMtQhgLw_c.jpg" title="Only the real ones remember (he is still the contributor with the most likes for his models)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging Face space by TCTF: Top Contributors To Follow - November 2025: &lt;a href="https://huggingface.co/spaces/TCTF/TCTF"&gt;https://huggingface.co/spaces/TCTF/TCTF&lt;/a&gt;&lt;br /&gt; Team mradermacher and Bartowski on the podium, legends.&lt;br /&gt; From Yaƒüƒ±z √áalƒ±k on ùïè: &lt;a href="https://x.com/Weyaxi/status/1995814979543371869"&gt;https://x.com/Weyaxi/status/1995814979543371869&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pc6i8v"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pc6i8v/only_the_real_ones_remember_he_is_still_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pc6i8v/only_the_real_ones_remember_he_is_still_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T11:52:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcjjsk</id>
    <title>minimax m2 tops official SWE-bench leaderboard, followed by deepseek v3.2 and glm 4.6 [details on step limits, cost efficiency, etc. in post]</title>
    <updated>2025-12-02T20:29:33+00:00</updated>
    <author>
      <name>/u/klieret</name>
      <uri>https://old.reddit.com/user/klieret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjjsk/minimax_m2_tops_official_swebench_leaderboard/"&gt; &lt;img alt="minimax m2 tops official SWE-bench leaderboard, followed by deepseek v3.2 and glm 4.6 [details on step limits, cost efficiency, etc. in post]" src="https://a.thumbs.redditmedia.com/ys-1wDn79XSFknvv5ToJiU1nmLysxbP3_tau9tGK2u4.jpg" title="minimax m2 tops official SWE-bench leaderboard, followed by deepseek v3.2 and glm 4.6 [details on step limits, cost efficiency, etc. in post]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I'm from the SWE-bench team. We've just finished evaluating the new deepseek &amp;amp; GLM, and minimax using a minimal agent&lt;/p&gt; &lt;p&gt;Minimax M2 is best open source model (but expensive!). Deepseek v3.2 reasoning close behind, very cheap, but very slow. GLM 4.6 reaches good performance (same as qwen3 coder 480b a35b) fast and cheap. Compared to the non-open source models, the performance is still relatively low with Gemini 3 pro and Claude 4.5 Opus medium being around 74%&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uaudqnr5nu4g1.png?width=3593&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=27e997e9af142ed512db76961d22ded868a857ba"&gt;https://preview.redd.it/uaudqnr5nu4g1.png?width=3593&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=27e997e9af142ed512db76961d22ded868a857ba&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All costs are calculated with the official API cost at the time of release.&lt;/p&gt; &lt;p&gt;Models take different amount of steps, with minimax taking the most and deepseek taking comparatively few. This is probably a big factor in minimax being pretty pricy at the moment.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ggrkjftfnu4g1.png?width=2345&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=574c785d616c94a168dac9476f7b57bfb927186e"&gt;https://preview.redd.it/ggrkjftfnu4g1.png?width=2345&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=574c785d616c94a168dac9476f7b57bfb927186e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;However, you also cannot just stop minimax early by setting a low step limit, because it actually still solves quite a few instances at high step counts (&amp;gt; 150 and some even &amp;gt;200 steps). That definitely speaks to the ability to do long horizon tasks, though of course most people want to have results earlier. For deepseek you can already stop at around 100 steps, there's a very clear flattening effect there.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zghs1m94ou4g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6b7256b37e6c935e92ca5b783d91cbec9b2924e0"&gt;https://preview.redd.it/zghs1m94ou4g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6b7256b37e6c935e92ca5b783d91cbec9b2924e0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In terms of cost efficiency (again, official API cost), you can trade off performance vs cost if you reduce the step limit. Here's the resulting cost-performance lines that you can get. If you don't mind the very long reasoning times of deepseek, clearly this is your most cost efficient bet at the moment. Otherwise, GLM seems very cost efficient.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rt6rgt26ou4g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc0bd71026f671143894e52685d984dc332525ec"&gt;https://preview.redd.it/rt6rgt26ou4g1.png?width=2092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc0bd71026f671143894e52685d984dc332525ec&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some small evaluation notes: We used T=0 for all models except GLM (T=1). We don't want to tune temperature for this eval, so it's either T=0 or T=1 for all. To parse the action from the agent we use &amp;quot;triple backticks&amp;quot; except for minimax that really didn't like that, so we used &amp;quot;xml style&amp;quot; parsing.&lt;/p&gt; &lt;p&gt;You can find the full config/prompts here: &lt;a href="https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/config/extra/swebench.yaml"&gt;https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/config/extra/swebench.yaml&lt;/a&gt; (resp &lt;a href="https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/config/extra/swebench_xml.yaml"&gt;https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/config/extra/swebench_xml.yaml&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;The full leaderboard is at &lt;a href="http://swebench.com"&gt;swebench.com&lt;/a&gt; (I'll update it very soon, at which point you can create your own plots &amp;amp; browse the trajectories from your browser). The trajectories are already available in our s3 container.&lt;/p&gt; &lt;p&gt;mini-swe-agent is open source at &lt;a href="https://github.com/SWE-agent/mini-swe-agent/"&gt;https://github.com/SWE-agent/mini-swe-agent/&lt;/a&gt;. The docs contain the full example of how to evaluate on SWE-bench (it only takes 2 commands and $15 for deepseek)&lt;/p&gt; &lt;p&gt;Let us know what models to evaluate next (we hope to add more open source models soon)!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klieret"&gt; /u/klieret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjjsk/minimax_m2_tops_official_swebench_leaderboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjjsk/minimax_m2_tops_official_swebench_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjjsk/minimax_m2_tops_official_swebench_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T20:29:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcurp8</id>
    <title>Q: When will there be fast and competent SLMs for laptops?</title>
    <updated>2025-12-03T04:32:47+00:00</updated>
    <author>
      <name>/u/TomLucidor</name>
      <uri>https://old.reddit.com/user/TomLucidor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It has been a whole year since Qwen2.5-32B been published for people to self-host their coding models. Similar models for RP probably exists before then, but the ideal of a general purpose portable model is still here. Yet, the news kept showing more techniques!&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Qwen3-30B-A3B and GPT-OSS-20B both uses Mixture-of-Experts instead of dense layers for their SLM&lt;/li&gt; &lt;li&gt;Kimi-Linear and Qwen3-Next-80B-A3B moved along to use &amp;quot;mixed attention&amp;quot; (majority of layers with linear attention) to speed things up AND have longer contexts&lt;/li&gt; &lt;li&gt;Not enough people getting into ternary attention like &lt;strong&gt;BitNet a4.8&lt;/strong&gt; / &lt;strong&gt;BitNet v2&lt;/strong&gt; &lt;a href="https://arxiv.org/html/2504.18415v2"&gt;https://arxiv.org/html/2504.18415v2&lt;/a&gt; or ternary quantization (PTQ) &lt;a href="https://arxiv.org/html/2509.23809v2"&gt;https://arxiv.org/html/2509.23809v2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Whatever layer routing is to reduce the amount of RAM needed, including &lt;strong&gt;Ouro-2.6B-Thinking&lt;/strong&gt; these days and &lt;strong&gt;Mixture-of-Depths&lt;/strong&gt; back in 2024&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Are all of these different techniques conflicting with one another? If it is just a lack of funding for fine-tuning/modding an existing SLM into something fast (assuming QAFT and RL), how much would it cost to crowdfund a project like this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TomLucidor"&gt; /u/TomLucidor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcurp8/q_when_will_there_be_fast_and_competent_slms_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcurp8/q_when_will_there_be_fast_and_competent_slms_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcurp8/q_when_will_there_be_fast_and_competent_slms_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T04:32:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcbr10</id>
    <title>Ministral WebGPU: Run Mistral's new multimodal models 100% locally in your browser.</title>
    <updated>2025-12-02T15:43:30+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbr10/ministral_webgpu_run_mistrals_new_multimodal/"&gt; &lt;img alt="Ministral WebGPU: Run Mistral's new multimodal models 100% locally in your browser." src="https://external-preview.redd.it/a2FpOGJodms5dDRnMVOJ9FmD9w2-LMCVXdFIiBg8ZPjaS6tgqxX1OyhMPvmT.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f21be91171f3f1c63baa540518e8447e2d1bdca9" title="Ministral WebGPU: Run Mistral's new multimodal models 100% locally in your browser." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, Mistral released &lt;strong&gt;Mistral 3&lt;/strong&gt;, a family of multimodal models, including three start-of-the-art dense models (3B, 8B, and 14B) and Mistral Large 3 (675B, 41B active). All Apache 2.0! ü§ó Surprisingly, the 3B is small enough to run 100% locally in your browser with WebGPU acceleration, powered by Transformers.js.&lt;/p&gt; &lt;p&gt;Link to demo: &lt;a href="https://huggingface.co/spaces/mistralai/Ministral_3B_WebGPU"&gt;https://huggingface.co/spaces/mistralai/Ministral_3B_WebGPU&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vwrcg6vk9t4g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbr10/ministral_webgpu_run_mistrals_new_multimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbr10/ministral_webgpu_run_mistrals_new_multimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:43:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcv0kd</id>
    <title>Maaza Orchestrator v1.2 ‚Äî 9.6M params, 62.9 % on hard adversarial tool-calling, 39 ms latency</title>
    <updated>2025-12-03T04:45:28+00:00</updated>
    <author>
      <name>/u/CycleCore_Tech</name>
      <uri>https://old.reddit.com/user/CycleCore_Tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just shipped v1.2 of Maaza Orchestrator (9.6 M params). &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Split&lt;/th&gt; &lt;th align="left"&gt;v1.0&lt;/th&gt; &lt;th align="left"&gt;v1.2&lt;/th&gt; &lt;th align="left"&gt;Œî&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;In-distribution accuracy&lt;/td&gt; &lt;td align="left"&gt;88.0%&lt;/td&gt; &lt;td align="left"&gt;86.0%&lt;/td&gt; &lt;td align="left"&gt;‚àí2.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Adversarial tool-calling&lt;/td&gt; &lt;td align="left"&gt;26.6%&lt;/td&gt; &lt;td align="left"&gt;62.9%&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+36.3%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;p50 latency (CPU)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;33.4ms&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;39.4ms&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+6.0ms&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The adversarial set is 124 held-out examples across 36 tools. A few representative ones so you can judge the difficulty:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚Äúlmao just text that to them‚Äù ‚Üí email_send&lt;/li&gt; &lt;li&gt;‚Äúturn this into spokenshit‚Äù ‚Üí voice_mcp&lt;/li&gt; &lt;li&gt;‚Äútime to rip and tear‚Äù ‚Üí doom_mcp&lt;/li&gt; &lt;li&gt;‚Äúwassup with my ethereum val‚Äù ‚Üí crypto_lookup&lt;/li&gt; &lt;li&gt;‚Äúplz execcute dis py code, gr8 tnx‚Äù ‚Üí code_execute_python&lt;/li&gt; &lt;li&gt;‚Äúweather or not?‚Äù ‚Üí weather_lookup (pun + typo)&lt;/li&gt; &lt;li&gt;‚Äúwiggle to &lt;a href="http://www.example.com%E2%80%9D"&gt;www.example.com‚Äù&lt;/a&gt; ‚Üí puppeteer_navigate&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Most examples stack 2‚Äì3 perturbations (slang + typos + abbreviations + cultural references). A vanilla 9.6 M model would probably sit below 30 % here.&lt;/p&gt; &lt;p&gt;The +36% came from one data-centric fine-tune: ~500 diverse adversarial seeds ‚Üí 10√ó upsampled ‚Üí 5 epochs. &lt;/p&gt; &lt;p&gt;‚Ä¢ HF: &lt;a href="https://huggingface.co/CycleCoreTechnologies/maaza-nlm-orchestrator-9.6m-v1.2"&gt;https://huggingface.co/CycleCoreTechnologies/maaza-nlm-orchestrator-9.6m-v1.2&lt;/a&gt;&lt;br /&gt; ‚Ä¢ Full 124-example held-out adversarial set (JSONL)&lt;br /&gt; ‚Ä¢ Training split &amp;amp; exact upsampling script&lt;br /&gt; ‚Ä¢ Apache 2.0&lt;/p&gt; &lt;p&gt;Happy to share the seed adversarial list. (v1.3 with 18√ó upsampling is already training).&lt;/p&gt; &lt;p&gt;Thanks for reading. Feedback always welcome. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CycleCore_Tech"&gt; /u/CycleCore_Tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcv0kd/maaza_orchestrator_v12_96m_params_629_on_hard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcv0kd/maaza_orchestrator_v12_96m_params_629_on_hard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcv0kd/maaza_orchestrator_v12_96m_params_629_on_hard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T04:45:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd04cn</id>
    <title>Chinese startup founded by Google engineer claims to have developed its own tpu reportedly 1.5 times faster than nvidia a100.</title>
    <updated>2025-12-03T09:51:40+00:00</updated>
    <author>
      <name>/u/Turbulent_Pin7635</name>
      <uri>https://old.reddit.com/user/Turbulent_Pin7635</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.tomshardware.com/tech-industry/chinese-startup-founded-by-google-engineer-claims-to-have-developed-its-own-tpu-reportedly-1-5-times-faster-than-nvidias-a100-gpu-from-2020-42-percent-more-efficient"&gt;https://www.tomshardware.com/tech-industry/chinese-startup-founded-by-google-engineer-claims-to-have-developed-its-own-tpu-reportedly-1-5-times-faster-than-nvidias-a100-gpu-from-2020-42-percent-more-efficient&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent_Pin7635"&gt; /u/Turbulent_Pin7635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T09:51:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcjqjs</id>
    <title>Ministral 3 models were pruned from Mistral Small 3.1</title>
    <updated>2025-12-02T20:36:32+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjqjs/ministral_3_models_were_pruned_from_mistral_small/"&gt; &lt;img alt="Ministral 3 models were pruned from Mistral Small 3.1" src="https://preview.redd.it/bte4gtp1qu4g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bec6f7045ad754997a36d5294eedaa2112246178" title="Ministral 3 models were pruned from Mistral Small 3.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bte4gtp1qu4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjqjs/ministral_3_models_were_pruned_from_mistral_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjqjs/ministral_3_models_were_pruned_from_mistral_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T20:36:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcb50r</id>
    <title>Ministral-3 has been released</title>
    <updated>2025-12-02T15:20:25+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcb50r/ministral3_has_been_released/"&gt; &lt;img alt="Ministral-3 has been released" src="https://b.thumbs.redditmedia.com/a0DyjW1DyWh-ddE3J9WOyZjKJiBbmcXRGjqX2TH__QM.jpg" title="Ministral-3 has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"&gt;https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-14B-Instruct-2512"&gt;https://huggingface.co/mistralai/Ministral-3-14B-Instruct-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-14B-Base-2512"&gt;https://huggingface.co/mistralai/Ministral-3-14B-Base-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The largest model in the Ministral 3 family, &lt;strong&gt;Ministral 3 14B&lt;/strong&gt; offers frontier capabilities and performance comparable to its larger &lt;a href="https://huggingface.co/mistralai/Mistral-Small-3.2-Instruct-2506"&gt;Mistral Small 3.2 24B&lt;/a&gt; counterpart. A powerful and efficient language model with vision capabilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-8B-Reasoning-2512"&gt;https://huggingface.co/mistralai/Ministral-3-8B-Reasoning-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-8B-Instruct-2512"&gt;https://huggingface.co/mistralai/Ministral-3-8B-Instruct-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-8B-Base-2512"&gt;https://huggingface.co/mistralai/Ministral-3-8B-Base-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A balanced model in the Ministral 3 family, &lt;strong&gt;Ministral 3 8B&lt;/strong&gt; is a powerful, efficient tiny language model with vision capabilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-3B-Reasoning-2512"&gt;https://huggingface.co/mistralai/Ministral-3-3B-Reasoning-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512"&gt;https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-3B-Base-2512"&gt;https://huggingface.co/mistralai/Ministral-3-3B-Base-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The smallest model in the Ministral 3 family, &lt;strong&gt;Ministral 3 3B&lt;/strong&gt; is a powerful, efficient tiny language model with vision capabilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/471e4lma6t4g1.png?width=1078&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c23d37e6a361041132ccec451c0a03921acc6e13"&gt;https://preview.redd.it/471e4lma6t4g1.png?width=1078&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c23d37e6a361041132ccec451c0a03921acc6e13&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c2szd14b6t4g1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d97fc5e8626f25f8c13a5b159e6351976f45de5"&gt;https://preview.redd.it/c2szd14b6t4g1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d97fc5e8626f25f8c13a5b159e6351976f45de5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-14B-Reasoning-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-14B-Reasoning-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-8B-Reasoning-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-8B-Reasoning-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-3B-Reasoning-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-3B-Reasoning-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-3B-Instruct-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-3B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcb50r/ministral3_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcb50r/ministral3_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcb50r/ministral3_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcia1t</id>
    <title>DeepSeek V3.2 Speciale dominates my math bench while being ~15√ó cheaper than GPT-5.1 High</title>
    <updated>2025-12-02T19:41:57+00:00</updated>
    <author>
      <name>/u/kyousukegum</name>
      <uri>https://old.reddit.com/user/kyousukegum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcia1t/deepseek_v32_speciale_dominates_my_math_bench/"&gt; &lt;img alt="DeepSeek V3.2 Speciale dominates my math bench while being ~15√ó cheaper than GPT-5.1 High" src="https://a.thumbs.redditmedia.com/TJzNTRI6aFSLhjDdzBZtSFW1nl-mDnldDNZ8ONcsRV0.jpg" title="DeepSeek V3.2 Speciale dominates my math bench while being ~15√ó cheaper than GPT-5.1 High" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/cwyrxaxneu4g1.png?width=3600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02ddc7b09fb3264f23ada56612af21febfe93bad"&gt;https://preview.redd.it/cwyrxaxneu4g1.png?width=3600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02ddc7b09fb3264f23ada56612af21febfe93bad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;for context on how impressive this is, I couldn't believe my eyes and had to double-check the results multiple times. The problems in this category are very hard like in the same ballpark as IMO P6.&lt;br /&gt; &lt;a href="https://x.com/gum1h0x/status/1995915458612953419"&gt;https://x.com/gum1h0x/status/1995915458612953419&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyousukegum"&gt; /u/kyousukegum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcia1t/deepseek_v32_speciale_dominates_my_math_bench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcia1t/deepseek_v32_speciale_dominates_my_math_bench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcia1t/deepseek_v32_speciale_dominates_my_math_bench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T19:41:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcayfs</id>
    <title>Mistral 3 Blog post</title>
    <updated>2025-12-02T15:13:14+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcayfs/mistral_3_blog_post/"&gt; &lt;img alt="Mistral 3 Blog post" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Mistral 3 Blog post" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/mistral-3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcayfs/mistral_3_blog_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcayfs/mistral_3_blog_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcomhi</id>
    <title>I'm surprised how simple Qwen3 VL's architecture is.</title>
    <updated>2025-12-02T23:50:06+00:00</updated>
    <author>
      <name>/u/No-Compote-6794</name>
      <uri>https://old.reddit.com/user/No-Compote-6794</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcomhi/im_surprised_how_simple_qwen3_vls_architecture_is/"&gt; &lt;img alt="I'm surprised how simple Qwen3 VL's architecture is." src="https://preview.redd.it/bfrh4xf5nv4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2643d2d6457fb6e3adfc09a5cf9e18b995e4219f" title="I'm surprised how simple Qwen3 VL's architecture is." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the new 3D position id logic really got a lot more intuitive compared to qwen2.5 vl. it basically index image patches on width and height dimension in addition to the regular token sequence / temporal dimension (while treating text as one same number across all 3 dimensions). &lt;/p&gt; &lt;p&gt;in addition to this, they added deepstack, which essentially is just some residual connections between vision encoder blocks and downstream LLM blocks.&lt;/p&gt; &lt;p&gt;here's the full repo if you want to read more: &lt;a href="https://github.com/Emericen/tiny-qwen"&gt;https://github.com/Emericen/tiny-qwen&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Compote-6794"&gt; /u/No-Compote-6794 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bfrh4xf5nv4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcomhi/im_surprised_how_simple_qwen3_vls_architecture_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcomhi/im_surprised_how_simple_qwen3_vls_architecture_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T23:50:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcp8z3</id>
    <title>Who‚Äôs got them Q_001_X_S_REAP Mistral Large 3 GGUFs?</title>
    <updated>2025-12-03T00:16:47+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp8z3/whos_got_them_q_001_x_s_reap_mistral_large_3_ggufs/"&gt; &lt;img alt="Who‚Äôs got them Q_001_X_S_REAP Mistral Large 3 GGUFs?" src="https://preview.redd.it/buxyht7ltv4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed225e778fb3ebb1d3e4ff9ac401e09c3aced65e" title="Who‚Äôs got them Q_001_X_S_REAP Mistral Large 3 GGUFs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm looking at you, Unsloth üòÅ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/buxyht7ltv4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp8z3/whos_got_them_q_001_x_s_reap_mistral_large_3_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp8z3/whos_got_them_q_001_x_s_reap_mistral_large_3_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T00:16:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pceipb</id>
    <title>Mistral just released Mistral 3 ‚Äî a full open-weight model family from 3B all the way up to 675B parameters.</title>
    <updated>2025-12-02T17:26:06+00:00</updated>
    <author>
      <name>/u/InternationalToe2678</name>
      <uri>https://old.reddit.com/user/InternationalToe2678</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All models are Apache 2.0 and fully usable for research + commercial work.&lt;/p&gt; &lt;p&gt;Quick breakdown:&lt;/p&gt; &lt;p&gt;‚Ä¢ Ministral 3 (3B / 8B / 14B) ‚Äì compact, multimodal, and available in base, instruct, and reasoning variants. Surprisingly strong for their size.&lt;/p&gt; &lt;p&gt;‚Ä¢ Mistral Large 3 (675B MoE) ‚Äì their new flagship. Strong multilingual performance, high efficiency, and one of the most capable open-weight instruct models released so far.&lt;/p&gt; &lt;p&gt;Why it matters: You now get a full spectrum of open models that cover everything from on-device reasoning to large enterprise-scale intelligence. The release pushes the ecosystem further toward distributed, open AI instead of closed black-box APIs.&lt;/p&gt; &lt;p&gt;Full announcement: &lt;a href="https://mistral.ai/news/mistral-3"&gt;https://mistral.ai/news/mistral-3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalToe2678"&gt; /u/InternationalToe2678 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pceipb/mistral_just_released_mistral_3_a_full_openweight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pceipb/mistral_just_released_mistral_3_a_full_openweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pceipb/mistral_just_released_mistral_3_a_full_openweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T17:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
