<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-16T08:38:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oxb9zp</id>
    <title>Local models handle tools way better when you give them a code sandbox instead of individual tools</title>
    <updated>2025-11-14T22:54:49+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxb9zp/local_models_handle_tools_way_better_when_you/"&gt; &lt;img alt="Local models handle tools way better when you give them a code sandbox instead of individual tools" src="https://preview.redd.it/83hx5w1txa1g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e2fbc834e05621ee050a05b0ee016fd280ff683" title="Local models handle tools way better when you give them a code sandbox instead of individual tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/83hx5w1txa1g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxb9zp/local_models_handle_tools_way_better_when_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxb9zp/local_models_handle_tools_way_better_when_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T22:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxl3ju</id>
    <title>What makes closed source models good? Data, Architecture, Size?</title>
    <updated>2025-11-15T07:00:06+00:00</updated>
    <author>
      <name>/u/Bitter-College8786</name>
      <uri>https://old.reddit.com/user/Bitter-College8786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know Kimi K2, Minimax M2 and Deepseek R1 are strong, but I asked myself: what makes the closed source models like Sonnet 4.5 or GPT-5 so strong? Do they have better training data? Or are their models even bigger, e.g. 2T, or do their models have some really good secret architecture (what I assume for Gemini 2.5 with its 1M context)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bitter-College8786"&gt; /u/Bitter-College8786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxl3ju/what_makes_closed_source_models_good_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxl3ju/what_makes_closed_source_models_good_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxl3ju/what_makes_closed_source_models_good_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T07:00:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxsnbv</id>
    <title>I have a friend who as 21 3060Tis from his mining times. Can this be, in any way be used for inference?</title>
    <updated>2025-11-15T14:10:03+00:00</updated>
    <author>
      <name>/u/puru991</name>
      <uri>https://old.reddit.com/user/puru991</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just the title. Is there any way to put that Vram to anything usable? He is open to adding ram, cpu and other things that might help the setup be usable. Any directions or advice appreciated.&lt;/p&gt; &lt;p&gt;Edit: so it seems the answer is - it is a bad idea. Sell&amp;gt;buy fewer more vram cards&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/puru991"&gt; /u/puru991 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxsnbv/i_have_a_friend_who_as_21_3060tis_from_his_mining/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxsnbv/i_have_a_friend_who_as_21_3060tis_from_his_mining/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxsnbv/i_have_a_friend_who_as_21_3060tis_from_his_mining/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T14:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy0x1j</id>
    <title>Mi50 Prices Nov 2025</title>
    <updated>2025-11-15T19:43:01+00:00</updated>
    <author>
      <name>/u/Success-Dependent</name>
      <uri>https://old.reddit.com/user/Success-Dependent</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The best prices on alibaba for small order quantities I'm seeing is $106 for the 16gb (with turbo fan) and $320 for the 32gb.&lt;/p&gt; &lt;p&gt;The 32gb are mostly sold out.&lt;/p&gt; &lt;p&gt;What prices are you paying?&lt;/p&gt; &lt;p&gt;Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Success-Dependent"&gt; /u/Success-Dependent &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy0x1j/mi50_prices_nov_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy0x1j/mi50_prices_nov_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy0x1j/mi50_prices_nov_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T19:43:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy7ane</id>
    <title>I just discovered something about LM Studio I had no idea it had..</title>
    <updated>2025-11-16T00:17:54+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had no idea that LM Studio had a cli. Had no freaking clue. And in Linux no less. I usually stay away from cli, because half the time they're not well put together, unnecessarily hard or hard's sake, and never gave me the output I wanted. But I was reading through the docs and found out it has one. and it's actually fairly good, and very user friendly. If it can't find a model you're asking for, it will give you a list of models you have, you type what you want, and it will fuzzy search for the model, and give you the ability to arrow key through the models you have, and let you select it and load it. I'm very impressed. So is the cli part of it more powerful than the gui part? Are there any LM Studio nerds in this sub that can expand on all the features it actually has that are user friendly for the cli? I'd love to hear more if anyone can expand on it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy7ane/i_just_discovered_something_about_lm_studio_i_had/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy7ane/i_just_discovered_something_about_lm_studio_i_had/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy7ane/i_just_discovered_something_about_lm_studio_i_had/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T00:17:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy1sos</id>
    <title>The highest Quality of Qwen Coder FP32</title>
    <updated>2025-11-15T20:18:46+00:00</updated>
    <author>
      <name>/u/Trilogix</name>
      <uri>https://old.reddit.com/user/Trilogix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1sos/the_highest_quality_of_qwen_coder_fp32/"&gt; &lt;img alt="The highest Quality of Qwen Coder FP32" src="https://preview.redd.it/9avsfezkbh1g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7cddb567d3f9491609115441b66a42f6cf02ce2e" title="The highest Quality of Qwen Coder FP32" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quantized from Hugston Team.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Trilogix1/Qwen_Coder_F32"&gt;https://huggingface.co/Trilogix1/Qwen_Coder_F32&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trilogix"&gt; /u/Trilogix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9avsfezkbh1g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1sos/the_highest_quality_of_qwen_coder_fp32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1sos/the_highest_quality_of_qwen_coder_fp32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T20:18:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1oybkdx</id>
    <title>How do you test new models?</title>
    <updated>2025-11-16T03:45:30+00:00</updated>
    <author>
      <name>/u/Borkato</name>
      <uri>https://old.reddit.com/user/Borkato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same prompt every time? Random prompts? Full blown testing setup? Just vibes?&lt;/p&gt; &lt;p&gt;Trying to figure out what to do with my 1TB drive full of models, I feel like if I just delete them for more I‚Äôll learn nothing!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Borkato"&gt; /u/Borkato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oybkdx/how_do_you_test_new_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oybkdx/how_do_you_test_new_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oybkdx/how_do_you_test_new_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T03:45:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1oydkjm</id>
    <title>Does an AI tool to control your desktop exist</title>
    <updated>2025-11-16T05:33:02+00:00</updated>
    <author>
      <name>/u/dumb_questions_alt</name>
      <uri>https://old.reddit.com/user/dumb_questions_alt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've read about some demos for this, and some hack'y tools that aren't ready yet, but I'm curious if I'm missing something of if this idea sounds silly. Or please let me know if there is a better way to do this, but I want to test some software totally autonomously by creating a total sandbox. Fresh OS install. PC unconnected to the internet. &lt;/p&gt; &lt;p&gt;I'm working on pretty limited PC resources. A single 3090 to be specific, so I'm curious if I can create an overarching agent that can run other agents. For example, it could be a small 4-8B LLM, and act as something like a conductor of other agents. &lt;/p&gt; &lt;p&gt;For example, it would load something like gpt-oss-20B to create a plan to follow. Save that away for context, then unload gpt-oss, and load Qwen Coder and ask it to code the plan. Then create a test plan and execute it to see if things work, create it's own vector db entries or RAG, and repeat the process.&lt;/p&gt; &lt;p&gt;Basically like a LLM doing things that I could do using the desktop. Is that a silly idea? Is there a better way to accomplish this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dumb_questions_alt"&gt; /u/dumb_questions_alt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oydkjm/does_an_ai_tool_to_control_your_desktop_exist/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oydkjm/does_an_ai_tool_to_control_your_desktop_exist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oydkjm/does_an_ai_tool_to_control_your_desktop_exist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T05:33:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyasdz</id>
    <title>Does anyone have a description of the general model families and their strengths and weaknesses?</title>
    <updated>2025-11-16T03:05:15+00:00</updated>
    <author>
      <name>/u/Borkato</name>
      <uri>https://old.reddit.com/user/Borkato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used to play with models like Erosumika and am in the process of setting up mistral and all that, but I don‚Äôt have much of a sense of how the families compare. &lt;/p&gt; &lt;p&gt;Obviously I can just use them, I‚Äôm just wondering what the general consensus is! Some people would say ‚Äúnever use x, it sucks because‚Ä¶‚Äù etc so I‚Äôm just curious what you all think. &lt;/p&gt; &lt;p&gt;So far the families I know of are llama 2, llama 3, mistral, MoE, Gemma, qwen, and I‚Äôm sure there‚Äôs a bunch more I‚Äôm forgetting, but I don‚Äôt know anything about the family‚Äôs quirks in particular so I just wanted to start a dialogue!&lt;/p&gt; &lt;p&gt;I‚Äôve been using models for quite a while but now it‚Äôs time for me to get serious haha. I do also wonder about exl3 vs gguf‚Ä¶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Borkato"&gt; /u/Borkato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyasdz/does_anyone_have_a_description_of_the_general/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyasdz/does_anyone_have_a_description_of_the_general/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyasdz/does_anyone_have_a_description_of_the_general/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T03:05:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyc1nr</id>
    <title>Ik_llamacpp's llama-server supports vision models btw</title>
    <updated>2025-11-16T04:10:12+00:00</updated>
    <author>
      <name>/u/Betadoggo_</name>
      <uri>https://old.reddit.com/user/Betadoggo_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been supported for the last 2 weeks, but I didn't notice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Betadoggo_"&gt; /u/Betadoggo_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ikawrakow/ik_llama.cpp/pull/901"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyc1nr/ik_llamacpps_llamaserver_supports_vision_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyc1nr/ik_llamacpps_llamaserver_supports_vision_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T04:10:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy1v7q</id>
    <title>Model recommendations for 128GB Strix Halo and other big unified RAM machines?</title>
    <updated>2025-11-15T20:21:44+00:00</updated>
    <author>
      <name>/u/blbd</name>
      <uri>https://old.reddit.com/user/blbd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In recent weeks I just powered up a 128GB unified memory Strix Halo box (Beelink GTR9) with latest Debian stable. I was seeing some NIC reliability issues with unstable's extremely new kernels and the ixgbe driver code couldn't handle some driver API changes that happened there and that's one of the required points for stabilizing the NICs. &lt;/p&gt; &lt;p&gt;I have done some burn-in basic testing with ROCM, llama.cpp, and PyTorch (and some of its examples and test cases) to make sure everything works OK, and partially stabilized the glitchy NICs with the NIC firmware update though they still have some issues.&lt;/p&gt; &lt;p&gt;I configured the kernel boot options to unleash the full unified memory capacity for the GPUs with the 512MB GART as the initial size. I set the BIOS to the higher performance mode and tweaked the fan curves. Are there other BIOS or kernel settings worth tweaking?&lt;/p&gt; &lt;p&gt;After that I tried a few classic models people have mentioned (GPT OSS 120B, NeuralDaredevil's uncensored one, etc.) and played around with the promptfoo test suites just a little bit to get a feel for launching the various models and utilities and MCP servers etc. I made sure the popular core tools can run right and the compute load feeds through the GPUs in radeontop and the like. &lt;/p&gt; &lt;p&gt;Since then I have been looking at all of the different recommendations of models to try by searching on here and on the Internet. I was running into some challenges because most of the advice centers around smaller models that don't make full use of the huge VRAM because this gear is very new. Can anybody with more experience on these new boxes recommend their favorites for putting the VRAM to best use?&lt;/p&gt; &lt;p&gt;I am curious about the following use cases: less flowery more practical and technical output for prompts (like a no-BS chat use case), the coding use case (advice about what IDEs to hook up and how very welcome), and I would like to learn about the process of creating and testing your own custom agents and how to QA test them against all of the numerous security problems we all know about and talk about all the time.&lt;/p&gt; &lt;p&gt;But I am also happy to hear any input about any other use cases. I just want to get some feedback and start building a good mental model of how all of this works and what to do for understanding things properly and fully wrapping my head around it all. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blbd"&gt; /u/blbd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1v7q/model_recommendations_for_128gb_strix_halo_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1v7q/model_recommendations_for_128gb_strix_halo_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1v7q/model_recommendations_for_128gb_strix_halo_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T20:21:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyawkl</id>
    <title>Why is vLLM Outperforming TensorRT-LLM (Nvidia's deployment library)? My Shocking Benchmarks on GPT-OSS-120B with H100</title>
    <updated>2025-11-16T03:11:23+00:00</updated>
    <author>
      <name>/u/kev_11_1</name>
      <uri>https://old.reddit.com/user/kev_11_1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyawkl/why_is_vllm_outperforming_tensorrtllm_nvidias/"&gt; &lt;img alt="Why is vLLM Outperforming TensorRT-LLM (Nvidia's deployment library)? My Shocking Benchmarks on GPT-OSS-120B with H100" src="https://b.thumbs.redditmedia.com/azcV9gzGHWFgHLriW95UukIsXwE7hRjbjMfh4llKrYs.jpg" title="Why is vLLM Outperforming TensorRT-LLM (Nvidia's deployment library)? My Shocking Benchmarks on GPT-OSS-120B with H100" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I tested TensorRT LLM with &lt;strong&gt;vLLM and results were shocking. I ran GPT OSS 120b on the same machine. Vllm was beating&lt;/strong&gt; TensorRT LLM in most scenarios, so i tested it two times with but the results were same.&lt;/p&gt; &lt;p&gt;Do any of you guys can possibely give reason for this because i heard that in Raw Power you cant beat TensorRT LLM.&lt;/p&gt; &lt;p&gt;My cloud has an H100 Pcle machine with 85 GB VRAM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TensorRT LLM setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;docker pull &lt;a href="http://nvcr.io/nvidia/tensorrt-llm/devel:1.2.0rc2"&gt;nvcr.io/nvidia/tensorrt-llm/devel:1.2.0rc2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;docker run --rm -it --gpus all --ipc=host \&lt;/p&gt; &lt;p&gt; -p 8000:8000 \&lt;/p&gt; &lt;p&gt; --ulimit memlock=-1 --ulimit stack=67108864 \&lt;/p&gt; &lt;p&gt; -v $(pwd):/workspace -w /workspace \&lt;/p&gt; &lt;p&gt; &lt;a href="http://nvcr.io/nvidia/tensorrt-llm/release:1.2.0rc2"&gt;nvcr.io/nvidia/tensorrt-llm/release:1.2.0rc2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;trtllm-serve serve --model &amp;quot;openai/gpt-oss-120b&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;vLLM setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;docker pull vllm/vllm-openai:nightly&lt;/p&gt; &lt;p&gt;docker run --rm -it --gpus all --ipc=host \&lt;/p&gt; &lt;p&gt;-p 8000:8000 \&lt;/p&gt; &lt;p&gt;--ulimit memlock=-1 --ulimit stack=67108864 \&lt;/p&gt; &lt;p&gt;-v $(pwd):/workspace -w /workspace \&lt;/p&gt; &lt;p&gt;--entrypoint /bin/bash \&lt;/p&gt; &lt;p&gt;vllm/vllm-openai:nightly&lt;/p&gt; &lt;p&gt;python3 -m vllm.entrypoints.openai.api_server \&lt;/p&gt; &lt;p&gt;--model &amp;quot;openai/gpt-oss-120b&amp;quot; \&lt;/p&gt; &lt;p&gt;--host &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; \&lt;/p&gt; &lt;p&gt;--trust-remote-code \&lt;/p&gt; &lt;p&gt;--max-model-len 16384&lt;/p&gt; &lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I've been benchmarking TensorRT-LLM against vLLM on an H100, and my results are shocking and the complete opposite of what I expected. I've always heard that for raw inference performance, nothing beats TensorRT-LLM.&lt;/p&gt; &lt;p&gt;However, in my tests, vLLM is significantly faster in almost every single scenario. I ran the benchmarks twice just to be sure, and the results were identical.&lt;/p&gt; &lt;h1&gt;üìä The Results&lt;/h1&gt; &lt;p&gt;I've attached the full benchmark charts (for 512 and 1024 context lengths) from my runs.&lt;/p&gt; &lt;p&gt;As you can see, vLLM (the teal bar/line) is dominating:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Sequential Throughput:&lt;/strong&gt; vLLM is ~70-80% faster (higher tokens/sec).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sequential Latency:&lt;/strong&gt; vLLM is ~40% faster (lower ms/token).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parallel Throughput:&lt;/strong&gt; vLLM scales much, much better as concurrent requests increase.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Latency (P50/P95):&lt;/strong&gt; vLLM's latencies are consistently lower across all concurrent request loads.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance Heatmap:&lt;/strong&gt; The heatmap says it all. It's entirely green, showing a 30-80%+ advantage for vLLM in all my tests.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;‚öôÔ∏è My Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; H100 PCIe machine with 85GB VRAM&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;code&gt;openai/gpt-oss-120b&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üì¶ TensorRT-LLM Setup&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Docker Image:&lt;/strong&gt; &lt;code&gt;docker pull&lt;/code&gt; &lt;a href="http://nvcr.io/nvidia/tensorrt-llm/devel:1.2.0rc2"&gt;&lt;code&gt;nvcr.io/nvidia/tensorrt-llm/devel:1.2.0rc2&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Docker Run:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run --rm -it --gpus all --ipc=host \ -p 8000:8000 \ --ulimit memlock=-1 --ulimit stack=67108864 \ -v $(pwd):/workspace -w /workspace \ nvcr.io/nvidia/tensorrt-llm/release:1.2.0rc2 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Serve Command (inside container):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;trtllm-serve serve --model &amp;quot;openai/gpt-oss-120b&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;üì¶ vLLM Setup&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Docker Image:&lt;/strong&gt; &lt;code&gt;docker pull vllm/vllm-openai:nightly&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Docker Run:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run --rm -it --gpus all --ipc=host \ -p 8000:8000 \ --ulimit memlock=-1 --ulimit stack=67108864 \ -v $(pwd):/workspace -w /workspace \ --entrypoint /bin/bash \ vllm/vllm-openai:nightly &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Serve Command (inside container):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python3 -m vllm.entrypoints.openai.api_server \ --model &amp;quot;openai/gpt-oss-120b&amp;quot; \ --host 0.0.0.0 \ --trust-remote-code \ --max-model-len 16384 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1j4j5i32hj1g1.png?width=7170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f56c0d4961c78728c35e045bf42f47e70ea021c2"&gt;https://preview.redd.it/1j4j5i32hj1g1.png?width=7170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f56c0d4961c78728c35e045bf42f47e70ea021c2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/djqpwr32hj1g1.png?width=7170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c975003e4bfba087398daea0b62d5b6865518646"&gt;https://preview.redd.it/djqpwr32hj1g1.png?width=7170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c975003e4bfba087398daea0b62d5b6865518646&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kev_11_1"&gt; /u/kev_11_1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyawkl/why_is_vllm_outperforming_tensorrtllm_nvidias/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyawkl/why_is_vllm_outperforming_tensorrtllm_nvidias/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyawkl/why_is_vllm_outperforming_tensorrtllm_nvidias/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T03:11:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy2opt</id>
    <title>Released Audiobook Creator v2.0 ‚Äì Huge Upgrade to Character Identification + Better TTS Quality</title>
    <updated>2025-11-15T20:56:28+00:00</updated>
    <author>
      <name>/u/prakharsr</name>
      <uri>https://old.reddit.com/user/prakharsr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pushed a new update to my &lt;strong&gt;Audiobook Creator&lt;/strong&gt; project and this one‚Äôs a pretty big step up, especially for people who use multi-voice audiobooks or care about cleaner, more natural output.&lt;/p&gt; &lt;p&gt;Links:&lt;br /&gt; &lt;a href="https://github.com/prakharsr/audiobook-creator"&gt;Repo&lt;/a&gt;&lt;br /&gt; &lt;a href="https://audio.com/prakhar-sharma/audio/sample-orpheus-multi-voice-audiobook-orpheus"&gt;Sample audiobook (Orpheus, multi-voice)&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/prakharsr/Orpheus-TTS-FastAPI"&gt;Orpheus TTS backend (for Orpheus users)&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/prakharsr/audiobook-creator/releases/tag/v2.0"&gt;Latest release notes on Github&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What‚Äôs new in v2.0&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Way better character identification&lt;/strong&gt;&lt;br /&gt; The old NLP pipeline is gone. It now uses a two-step LLM process to detect characters and figure out who‚Äôs speaking. This makes a &lt;em&gt;huge&lt;/em&gt; difference in books with lots of dialogue or messy formatting.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Emotion tagging got an upgrade&lt;/strong&gt;&lt;br /&gt; The LLM that adds emotion tags is cleaner and integrates nicely with Orpheus‚Äôs expressive voices. Makes multi-voice narration feel way more natural.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. More reliable Orpheus TTS pipeline&lt;/strong&gt;&lt;br /&gt; The Orpheus backend now automatically detects bad audio, retries with adjusted settings, catches repetition, clipping, silence, weird duration issues, etc. Basically fewer messed-up audio chunks.&lt;/p&gt; &lt;h1&gt;For new users discovering this project&lt;/h1&gt; &lt;p&gt;Quick overview of what the app does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Turn any EPUB/PDF/etc. into a clean audiobook&lt;/li&gt; &lt;li&gt;Multi-voice or single-voice narration&lt;/li&gt; &lt;li&gt;Supports Kokoro + Orpheus TTS&lt;/li&gt; &lt;li&gt;Auto-detected characters and emotion tags&lt;/li&gt; &lt;li&gt;Gradio UI for non-technical users&lt;/li&gt; &lt;li&gt;Creates proper M4B audiobooks with metadata, chapters, cover, etc.&lt;/li&gt; &lt;li&gt;Docker + standalone usage&lt;/li&gt; &lt;li&gt;Fully open source (GPLv3)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Shoutout&lt;/h1&gt; &lt;p&gt;Thanks to everyone who contributed fixes and improvements in this release.&lt;/p&gt; &lt;p&gt;If you try v2.0, let me know how the character detection and the new Orpheus pipeline feel. Happy to hear feedback or bug reports.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prakharsr"&gt; /u/prakharsr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy2opt/released_audiobook_creator_v20_huge_upgrade_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy2opt/released_audiobook_creator_v20_huge_upgrade_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy2opt/released_audiobook_creator_v20_huge_upgrade_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T20:56:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oya00n</id>
    <title>The good stuff is getting pretty large, innit?</title>
    <updated>2025-11-16T02:26:09+00:00</updated>
    <author>
      <name>/u/SocietyTomorrow</name>
      <uri>https://old.reddit.com/user/SocietyTomorrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been itching to divest myself from Anthropic once a model came around that was &amp;quot;good enough&amp;quot; to produce a starting point about equal to what you get from Claude Code. Qwen3 is nice, and GLM is nicer, but after seeing the benchmarks on MiniMax M2 I have really wanted to give that a stab. I wonder if this is the direction that a lot of these agentic and code-oriented LLMs are going to keep edging closer to 1TB as they go, making it ever harder for me to put them into service. &lt;/p&gt; &lt;p&gt;I have wondered though, if this trend is going to stick, what is becoming the new silver standard for us enthusiasts who want to run these beasts and their 121GB minimum VRAM? Even the STRIX Halo boxes and the nvidia gold brick wouldn't have enough memory to load these one-shot. Are people going to be expected to be clustering multiples of these for inference, with full knowledge that you're probably never going to recoup that value? I kinda hope not. DeepSeek was promising to me in that it found a way to do a lot more work with a lot less resources, but that seems to not be a forward focus.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SocietyTomorrow"&gt; /u/SocietyTomorrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oya00n/the_good_stuff_is_getting_pretty_large_innit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oya00n/the_good_stuff_is_getting_pretty_large_innit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oya00n/the_good_stuff_is_getting_pretty_large_innit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T02:26:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy053m</id>
    <title>Why do (some) people hate Open WebUI?</title>
    <updated>2025-11-15T19:12:11+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm new to local hosted LLMs. I‚Äôve setup mine using LM Studio + Open WebUI (for external access). I couldn‚Äôt help but notice every video/post/tutorial has some people in the comments saying how you shouldn‚Äôt use Open WebUi. But not really clear as to ‚Äúwhy?‚Äù&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy053m/why_do_some_people_hate_open_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy053m/why_do_some_people_hate_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy053m/why_do_some_people_hate_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T19:12:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxywsc</id>
    <title>New Sherlock Alpha Stealth Models on OpenRouter might be Grok 4.20</title>
    <updated>2025-11-15T18:23:15+00:00</updated>
    <author>
      <name>/u/According-Zombie-337</name>
      <uri>https://old.reddit.com/user/According-Zombie-337</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxywsc/new_sherlock_alpha_stealth_models_on_openrouter/"&gt; &lt;img alt="New Sherlock Alpha Stealth Models on OpenRouter might be Grok 4.20" src="https://preview.redd.it/j373g4gxqg1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86cd8ced7274dd0c65bcbb2cc3d09a41e635028c" title="New Sherlock Alpha Stealth Models on OpenRouter might be Grok 4.20" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Sherlock models are from xAI, probably Grok 4.20.&lt;/p&gt; &lt;p&gt;For context, two new stealth models just appeared on OpenRouter:&lt;/p&gt; &lt;p&gt;Sherlock Alpha and Sherlock Think Alpha.&lt;/p&gt; &lt;p&gt;From the testing I've done so far, capabilities aren't anything super new, but better than Grok 4 and Grok 4 Fast.&lt;/p&gt; &lt;p&gt;If this doesn't come out before Gemini 3 (which it looks like it won't since Gemini 3 is coming next week), then this will not be a Frontier model release. But the benchmarks might say differently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/According-Zombie-337"&gt; /u/According-Zombie-337 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j373g4gxqg1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxywsc/new_sherlock_alpha_stealth_models_on_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxywsc/new_sherlock_alpha_stealth_models_on_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T18:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxtc5y</id>
    <title>US Cloud Giants to Spend ~8.16√ó What China Does in 2025‚Äì27 ‚Äî $1.7 Trillion vs $210 Billion, Will it translate to stronger US AI dominance?</title>
    <updated>2025-11-15T14:40:20+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxtc5y/us_cloud_giants_to_spend_816_what_china_does_in/"&gt; &lt;img alt="US Cloud Giants to Spend ~8.16√ó What China Does in 2025‚Äì27 ‚Äî $1.7 Trillion vs $210 Billion, Will it translate to stronger US AI dominance?" src="https://preview.redd.it/sjklvwo8nf1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be22252ccd8c2abcae9179c2f3c14bcfe022e978" title="US Cloud Giants to Spend ~8.16√ó What China Does in 2025‚Äì27 ‚Äî $1.7 Trillion vs $210 Billion, Will it translate to stronger US AI dominance?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sjklvwo8nf1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxtc5y/us_cloud_giants_to_spend_816_what_china_does_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxtc5y/us_cloud_giants_to_spend_816_what_china_does_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T14:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyaz1h</id>
    <title>I think I'm falling in love with how good mistral is as an AI. Like it's 8b-7b variants are just so much more dependable and good compared to qwen or something like llama. But the benchmarks show the opposite. How does one find good models if this is the state of benchmarks?</title>
    <updated>2025-11-16T03:14:57+00:00</updated>
    <author>
      <name>/u/Xanta_Kross</name>
      <uri>https://old.reddit.com/user/Xanta_Kross</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As I said above mistral is really good.&lt;br /&gt; - It follows instructions very well&lt;br /&gt; - doesn't hallucinate (almost zero)&lt;br /&gt; - gives short answers for short questions and long answers for properly long questions&lt;br /&gt; - is tiny compared to SOTA while also feeling like I'm talking to something actually intelligent rather than busted up keyword prediction&lt;/p&gt; &lt;p&gt;But the benchmarks of it don't show it as impressive as phi4 or phi3 even, Qwen3, Qwen2 vl etc also. Putting it insanely lower than them. Like this is insane how awful the current benchmarks are. Completely skewed.&lt;/p&gt; &lt;p&gt;I want to find more models like these. How do you guys find models like these, when the benchmarks are so badly skewed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xanta_Kross"&gt; /u/Xanta_Kross &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyaz1h/i_think_im_falling_in_love_with_how_good_mistral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyaz1h/i_think_im_falling_in_love_with_how_good_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyaz1h/i_think_im_falling_in_love_with_how_good_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T03:14:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1oximzj</id>
    <title>Anthropic pushing again for regulation of open source models?</title>
    <updated>2025-11-15T04:40:56+00:00</updated>
    <author>
      <name>/u/MasterDragon_</name>
      <uri>https://old.reddit.com/user/MasterDragon_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oximzj/anthropic_pushing_again_for_regulation_of_open/"&gt; &lt;img alt="Anthropic pushing again for regulation of open source models?" src="https://preview.redd.it/623qojxaoc1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd955c46ca05077bed949b46643bd7061e16d04c" title="Anthropic pushing again for regulation of open source models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MasterDragon_"&gt; /u/MasterDragon_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/623qojxaoc1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oximzj/anthropic_pushing_again_for_regulation_of_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oximzj/anthropic_pushing_again_for_regulation_of_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T04:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy46o3</id>
    <title>The more restrictive LLMs like ChatGPT become, the clearer it becomes: local models are the future.</title>
    <updated>2025-11-15T22:00:04+00:00</updated>
    <author>
      <name>/u/orionstern</name>
      <uri>https://old.reddit.com/user/orionstern</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can only recommend that everyone stop using ChatGPT. This extreme over-censorship, over-filtering, over-regulation suffocates almost every conversation right from the start. As soon as anything goes even slightly in the direction of emotional conversations, the system blocks it and you only get warnings. Why would anyone voluntarily put up with that?&lt;/p&gt; &lt;p&gt;Luckily, there are other AIs that aren‚Äôt affected by this kind of madness. ChatGPT‚Äôs guardrails are pathological. For months we were promised fewer restrictions. And the result? Answer: even more extreme restrictions. We were all lied to, deceived, and strung along.&lt;/p&gt; &lt;p&gt;GPT-5.1 only causes depression now. Don‚Äôt do this to yourselves any longer. Just switch to another AI, and it doesn‚Äôt even matter which one ‚Äî the main thing is to get away from ChatGPT. Don‚Äôt believe a single word they say. Not even the supposed 800 million users per week, which a website on the internet disproved. And OpenAI supposedly has a ‚Äòwater problem‚Äô, right? Easy solution: just turn off their water. How? Simply stop using them.&lt;/p&gt; &lt;p&gt;They‚Äôve managed to make their product unusable. In short: use a different AI. Don‚Äôt waste your energy getting angry at ChatGPT. It‚Äôs not worth it, and they‚Äôre not worth it. They had good chances. Now the wind is turning. Good night, OpenAI (‚ÄòClosedAI‚Äô).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/orionstern"&gt; /u/orionstern &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy46o3/the_more_restrictive_llms_like_chatgpt_become_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy46o3/the_more_restrictive_llms_like_chatgpt_become_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy46o3/the_more_restrictive_llms_like_chatgpt_become_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T22:00:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxw1rf</id>
    <title>‚ÄúWe don‚Äôt need corp AI, we have AI at home.. ‚Äú</title>
    <updated>2025-11-15T16:30:40+00:00</updated>
    <author>
      <name>/u/Birchi</name>
      <uri>https://old.reddit.com/user/Birchi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxw1rf/we_dont_need_corp_ai_we_have_ai_at_home/"&gt; &lt;img alt="‚ÄúWe don‚Äôt need corp AI, we have AI at home.. ‚Äú" src="https://a.thumbs.redditmedia.com/qc9sVTXit2tBCYT5qsuH3I3XzrNQcOniJHSAWtLWmA4.jpg" title="‚ÄúWe don‚Äôt need corp AI, we have AI at home.. ‚Äú" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;.. the AI at home. I figured you guys would appreciate this more than my irl peeps :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Birchi"&gt; /u/Birchi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oxw1rf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxw1rf/we_dont_need_corp_ai_we_have_ai_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxw1rf/we_dont_need_corp_ai_we_have_ai_at_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T16:30:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxxrhc</id>
    <title>Kimi K2 is the best clock AI</title>
    <updated>2025-11-15T17:38:21+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every minute, a new clock is displayed that has been generated by nine different AI models.&lt;/p&gt; &lt;p&gt;Each model is allowed 2000 tokens to generate its clock. Here is its prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Create HTML/CSS of an analog clock showing ${time}. Include numbers (or numerals) if you wish, and have a CSS animated second hand. Make it responsive and use a white background. Return ONLY the HTML/CSS code with no markdown formatting.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I have observed for a long time that the Kimi K2 is the only model that can maintain 12 digits in the correct clock positions, even with the second hand perfectly aligned with the actual time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxxrhc/kimi_k2_is_the_best_clock_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxxrhc/kimi_k2_is_the_best_clock_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxxrhc/kimi_k2_is_the_best_clock_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T17:38:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy9w39</id>
    <title>Do we need a language model torrent index?</title>
    <updated>2025-11-16T02:20:43+00:00</updated>
    <author>
      <name>/u/MixtureOfAmateurs</name>
      <uri>https://old.reddit.com/user/MixtureOfAmateurs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like a pirate bay of AI models. I don't see myself downloading from it much, but in the event hugging face gets bought out, openai/anthropic get what they want, or third unknown option it might be better to have an existing community hosted option than to scramble to make 1 hundred and then all being pretty bad.&lt;/p&gt; &lt;p&gt;Does this exist yet? Do you see yourself using it preregulation? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MixtureOfAmateurs"&gt; /u/MixtureOfAmateurs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy9w39/do_we_need_a_language_model_torrent_index/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy9w39/do_we_need_a_language_model_torrent_index/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy9w39/do_we_need_a_language_model_torrent_index/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T02:20:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oth5pw</id>
    <title>AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model</title>
    <updated>2025-11-10T15:44:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt; &lt;img alt="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" src="https://b.thumbs.redditmedia.com/yz9_FpdLcHNiCkaH5fLEIoXS2f5u5twNBr7SQ9Go3AI.jpg" title="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Moonshot AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/ComfortableAsk4494"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/zxytim"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ppwwyyxx"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87"&gt;https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:44:10+00:00</published>
  </entry>
</feed>
