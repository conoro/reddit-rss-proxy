<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-20T16:47:15+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r9w84r</id>
    <title>Persistent Memory Solutions</title>
    <updated>2026-02-20T14:04:58+00:00</updated>
    <author>
      <name>/u/Itchy_Supermarket_43</name>
      <uri>https://old.reddit.com/user/Itchy_Supermarket_43</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I am building a local first AI agent in my linux system (ubuntu). I am in the phase of implementing a persistent long term memory. I am currently thinking of starting off with creating a local JSON format. What do you suggest?&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Itchy_Supermarket_43"&gt; /u/Itchy_Supermarket_43 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9w84r/persistent_memory_solutions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9w84r/persistent_memory_solutions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9w84r/persistent_memory_solutions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T14:04:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9pxxc</id>
    <title>Context Size Frustration</title>
    <updated>2026-02-20T08:31:43+00:00</updated>
    <author>
      <name>/u/Aggressive-Spinach98</name>
      <uri>https://old.reddit.com/user/Aggressive-Spinach98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Guys&lt;/p&gt; &lt;p&gt;So this post might be a little bit longer as I got really frustrated with local AI and Context Size in particular. If you check my other posts you might notice that this topic has come up for me from time to time already and I`m once again seeking help. &lt;/p&gt; &lt;p&gt;Tl:dr What method do you use if you want to calculate how much context size you can have with your given hardware for Model X in a safe way?&lt;/p&gt; &lt;p&gt;So my use case is that I want to run an LLM Model locally and I want to get a feel for how much context size I can use on my hardware. &lt;/p&gt; &lt;p&gt;My setup is LM Studio, a RTX 6000 Pro Blackwell as well as 128GB DDR5 Ram. &lt;/p&gt; &lt;p&gt;I already know what tokens are, what context size in general is and where I can find in the model description or config file how much context size it should be able to run in theory. &lt;/p&gt; &lt;p&gt;Now if you search for information about context size you get either a lot of surface level knowledge or really in depth essays that are at the moment to complicated for me, if I`m a 100% honest. So what I did was trying to figure out, atleast roughly, how much context size I could plan with. So I took my Vram, subtracted the &amp;quot;size&amp;quot; of the modell in the chosen quantification level and then trying to calculate how much tokens I can squeeze into the remaining free space while leaving some buffer of an additional 10% for safety. The results of that was a formula like this: &lt;/p&gt; &lt;p&gt;&lt;em&gt;KV per token = 2 Ã— num_layers Ã— num_kv_heads Ã— head_dim Ã— bytes&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Were the necessary data comes from the config file of the model in question on huggingface. &lt;/p&gt; &lt;p&gt;The numbers behind the &amp;quot;=&amp;quot; are an example based on the Nevoria Modell:&lt;/p&gt; &lt;p&gt;&lt;em&gt;Number of layers (num_hidden_layers) = 80&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Number of KV heads (num_key_value_heads) = 8&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Head dimension (head_dim) = 128&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Data type for KV cache = Usually BF16 so 2 Bytes per Value&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Two tensors per token â†’ Key + Value (should be fixed, except for special structures)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;So to put these numbers into the formula it would look like this: &lt;/p&gt; &lt;p&gt;&lt;em&gt;KV per Token = 2 \&lt;/em&gt; 80 * 8 * 128 * 2*&lt;/p&gt; &lt;p&gt;&lt;em&gt;= 327.680 Bytes per Token&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;~320 KB per Token or 327.68 KB per Token&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Then I continued with: &lt;/p&gt; &lt;p&gt;&lt;em&gt;Available VRAM = Total GPU VRAM - Model Size - Safety Buffer&lt;/em&gt;&lt;/p&gt; &lt;p&gt;so in numbers: &lt;/p&gt; &lt;p&gt;&lt;em&gt;96 GB - 75 GB - 4 GB&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;= 17 GB&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Since I had the free space and the cost per token the last formula was: &lt;/p&gt; &lt;p&gt;&lt;em&gt;MAX Tokens = 17 GB in Bytes / 327.680 Bytes (Not KB)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Conversion = 17 GB \&lt;/em&gt; 1024 (MB) * 1024 (KB) * 1024 (Byte)*&lt;/p&gt; &lt;p&gt;&lt;em&gt;= ~55.706 Token&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Then usually I subtract an additional amount of tokens just to be more safe, so in this example I would go with 50k tokens context size. &lt;/p&gt; &lt;p&gt;This method worked for me and was most of the time save until two days ago when I hit a context problem that would literally crash my PC. While processing and generating an answer my PC would simply turn of, with the white Power LED still glowing. I had to completly restart everything. After some tests, and log files checking it seems that I have no hardware or heat problem but the context was simply to big so I ran out of memory or it caused another problem. &lt;/p&gt; &lt;p&gt;So while investigating I found an article that says, the more context you give the bigger the amount of (v)RAM you need as the requirements grow rapedly and are not linear, which I guess makes my formula redundant? The table goes like this: &lt;/p&gt; &lt;p&gt;4k context: Approximately 2-4 GB of (V)Ram&lt;/p&gt; &lt;p&gt;8k context: Approximately 4-8 GB of (V)Ram&lt;/p&gt; &lt;p&gt;32k context: Approximately 16-24 GB of (V)Ram&lt;/p&gt; &lt;p&gt;128k context: Approximately 64-96 GB of (V)Ram&lt;/p&gt; &lt;p&gt;The article I read also mentioned a lot of tricks or features that reduce these requirements like: Flash Attention, Sparse Attention, Sliding window Attention, Positional Embeddings and KV Cache Optimization. But not stating how much these methods would actually reduce the needed amount of RAM, if it is even possible to calculate that. &lt;/p&gt; &lt;p&gt;So, I once again feel like I`m standing in a forest unable to see the trees. Since I managed to kill my hardware atleast once, most likely because of context size, I`m really interested to get a better feeling for how many context size is safe to set, without just defaulting to 4k or something equally small. &lt;/p&gt; &lt;p&gt;Any help is greatly appreciated&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aggressive-Spinach98"&gt; /u/Aggressive-Spinach98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9pxxc/context_size_frustration/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9pxxc/context_size_frustration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9pxxc/context_size_frustration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T08:31:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9ze4n</id>
    <title>I spent 3 months interviewing AI engineers and got kind of depressed. Made this roadmap so you don't end up in the pile I kept rejecting.</title>
    <updated>2026-02-20T16:06:42+00:00</updated>
    <author>
      <name>/u/hemansnation</name>
      <uri>https://old.reddit.com/user/hemansnation</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9ze4n/i_spent_3_months_interviewing_ai_engineers_and/"&gt; &lt;img alt="I spent 3 months interviewing AI engineers and got kind of depressed. Made this roadmap so you don't end up in the pile I kept rejecting." src="https://preview.redd.it/sfhg559saokg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f67ba804f592110b66d2c09c2488cc2fe8db0436" title="I spent 3 months interviewing AI engineers and got kind of depressed. Made this roadmap so you don't end up in the pile I kept rejecting." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Okay so a bit of context before I dump this wall of text on you.&lt;/p&gt; &lt;p&gt;I have done somewhere around 30+ interviews over the past few months. I took notes on almost all of them because I started noticing the same patterns over and over and it was driving me insane.&lt;/p&gt; &lt;p&gt;I need to be honest with you &amp;gt; the market right now is brutal, but not for the reasons most people think. It's not that there aren't jobs. There's this massive gap between what people think is impressive and what actually gets you hired at the $150k+ level.&lt;/p&gt; &lt;p&gt;The thing that broke me was opening multiple resumes in a row and seeing [Built a Chatbot with OpenAI API] listed as the top project. 3 years ago I would have been genuinely excited. Now it reads the same way made a website using Dreamweaver did in 2012 (if you remember).&lt;/p&gt; &lt;p&gt;It just tells me you followed a YouTube tutorial and called it a day.&lt;/p&gt; &lt;p&gt;Here's what nobody says out loud = if your whole skillset lives or dies on an API key, you don't really have a skillset. You have a subscription.&lt;/p&gt; &lt;p&gt;So I put together the actual project types that have been making me stop and say okay, let's get this person in for a second round.&lt;/p&gt; &lt;p&gt;These are not easy and that's the whole point. Difficulty is what separates a portfolio from a tutorial graveyard.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Offline RAG System&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Self-Healing Agent&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Real-Time Voice Under 500ms Latency&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Fine-Tuning Pipeline&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Event-Driven Orchestration&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;6. Hybrid Memory System&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Stack summary if you want the TLDR:&lt;/p&gt; &lt;p&gt;Stop grinding LangChain syntax. Start learning architecture.&lt;/p&gt; &lt;p&gt;The tools that keep showing up in the builds I actually respect = Docker, LangGraph, FastAPI, Neo4j, Unsloth.&lt;/p&gt; &lt;p&gt;I turned it into a proper writeup on my Substack if you want the deep dive.&lt;/p&gt; &lt;p&gt;Happy to answer questions in the comments. If you are stuck on any of these or want to know what specifically I look for when someone walks me through their build, just ask.&lt;/p&gt; &lt;p&gt;&lt;a href="https://himanshuramchandani.substack.com/p/ai-engineer-roadmap-2026-ship-or"&gt;Check it out!&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hemansnation"&gt; /u/hemansnation &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sfhg559saokg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9ze4n/i_spent_3_months_interviewing_ai_engineers_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9ze4n/i_spent_3_months_interviewing_ai_engineers_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T16:06:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9rboa</id>
    <title>Buying cheap 'no display' gpus from ebay?</title>
    <updated>2026-02-20T09:57:35+00:00</updated>
    <author>
      <name>/u/getpodapp</name>
      <uri>https://old.reddit.com/user/getpodapp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm finding these RTX 4080/90's for like 200-300GBP on ebay marked as 'no display', clearly theres a risk that they're completely fucked. &lt;/p&gt; &lt;p&gt;If its literally just 'no display' but compute works it seems a stupid easy way of getting a bunch of vRAM on modern GPUs...?&lt;/p&gt; &lt;p&gt;Does anyone experience with this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getpodapp"&gt; /u/getpodapp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9rboa/buying_cheap_no_display_gpus_from_ebay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9rboa/buying_cheap_no_display_gpus_from_ebay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9rboa/buying_cheap_no_display_gpus_from_ebay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T09:57:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9vsye</id>
    <title>Nice interactive explanation of Speculative Decoding</title>
    <updated>2026-02-20T13:47:19+00:00</updated>
    <author>
      <name>/u/individual_kex</name>
      <uri>https://old.reddit.com/user/individual_kex</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vsye/nice_interactive_explanation_of_speculative/"&gt; &lt;img alt="Nice interactive explanation of Speculative Decoding" src="https://external-preview.redd.it/EhW4bQWT9WIeRw5amz2pS-lzd3lb6K6qLMCB-e4QXzU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb9aba232415d2dd8db7afab8bd1d38bfcb06a5d" title="Nice interactive explanation of Speculative Decoding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/individual_kex"&gt; /u/individual_kex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.adaptive-ml.com/post/speculative-decoding-visualized"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vsye/nice_interactive_explanation_of_speculative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vsye/nice_interactive_explanation_of_speculative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:47:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9mcjw</id>
    <title>GPT-OSS-120b on 2X RTX5090</title>
    <updated>2026-02-20T05:02:12+00:00</updated>
    <author>
      <name>/u/Interesting-Ad4922</name>
      <uri>https://old.reddit.com/user/Interesting-Ad4922</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mcjw/gptoss120b_on_2x_rtx5090/"&gt; &lt;img alt="GPT-OSS-120b on 2X RTX5090" src="https://preview.redd.it/atfvw7c10lkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c2e6695819ff38848b821da3efa31d5b86b8bb9" title="GPT-OSS-120b on 2X RTX5090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got GPT-OSS-120b deployed on dual RTX5090 rig. 128k context (Significant CPU offloading ~10t/s) I know it's nothing amazing I'm just a little proud of myself and needed to tell someone! Thanks for lookin!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Ad4922"&gt; /u/Interesting-Ad4922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/atfvw7c10lkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mcjw/gptoss120b_on_2x_rtx5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mcjw/gptoss120b_on_2x_rtx5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T05:02:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9mkgj</id>
    <title>PaddleOCR-VL now in llama.cpp</title>
    <updated>2026-02-20T05:13:34+00:00</updated>
    <author>
      <name>/u/PerfectLaw5776</name>
      <uri>https://old.reddit.com/user/PerfectLaw5776</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b8110"&gt;https://github.com/ggml-org/llama.cpp/releases/tag/b8110&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far this is the best performing open-source multilingual OCR model I've seen, would appreciate if other people can share their findings. It's 0.9b so it shouldn't brick our machines. &lt;a href="https://huggingface.co/octopusmegalopod/some-paddleocr1.5-vl-ggufs"&gt;Some GGUFs&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PerfectLaw5776"&gt; /u/PerfectLaw5776 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mkgj/paddleocrvl_now_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mkgj/paddleocrvl_now_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mkgj/paddleocrvl_now_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T05:13:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9x0l2</id>
    <title>We replaced the LLM in a voice assistant with a fine-tuned 0.6B model. 90.9% tool call accuracy vs. 87.5% for the 120B teacher. ~40ms inference.</title>
    <updated>2026-02-20T14:37:00+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9x0l2/we_replaced_the_llm_in_a_voice_assistant_with_a/"&gt; &lt;img alt="We replaced the LLM in a voice assistant with a fine-tuned 0.6B model. 90.9% tool call accuracy vs. 87.5% for the 120B teacher. ~40ms inference." src="https://preview.redd.it/lh8p2xv0vnkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbabc4097ccac2e5d448a628acbee7068bf698ee" title="We replaced the LLM in a voice assistant with a fine-tuned 0.6B model. 90.9% tool call accuracy vs. 87.5% for the 120B teacher. ~40ms inference." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Voice assistants almost always use a cloud LLM for the &amp;quot;brain&amp;quot; stage (intent routing, slot extraction, dialogue state). The LLM stage alone adds 375-750ms per turn, which pushes total pipeline latency past the 500-800ms threshold where conversations feel natural.&lt;/p&gt; &lt;p&gt;For bounded workflows like banking, insurance, or telecom, that's a lot of unnecessary overhead. The task is not open-ended generation -- it's classifying intent and extracting structured slots from what the user said. That's exactly where fine-tuned SLMs shine.&lt;/p&gt; &lt;p&gt;We built VoiceTeller, a banking voice assistant that swaps the LLM for a locally-running fine-tuned Qwen3-0.6B. Numbers:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Params&lt;/th&gt; &lt;th&gt;Single-Turn Tool Call Accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;GPT-oss-120B (teacher)&lt;/td&gt; &lt;td&gt;120B&lt;/td&gt; &lt;td&gt;87.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-0.6B (fine-tuned)&lt;/td&gt; &lt;td&gt;0.6B&lt;/td&gt; &lt;td&gt;&lt;strong&gt;90.9%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-0.6B (base)&lt;/td&gt; &lt;td&gt;0.6B&lt;/td&gt; &lt;td&gt;48.7%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;And the pipeline latency breakdown:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Stage&lt;/th&gt; &lt;th&gt;Cloud LLM&lt;/th&gt; &lt;th&gt;SLM&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;ASR&lt;/td&gt; &lt;td&gt;200-350ms&lt;/td&gt; &lt;td&gt;~200ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Brain&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;375-750ms&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;~40ms&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TTS&lt;/td&gt; &lt;td&gt;75-150ms&lt;/td&gt; &lt;td&gt;~75ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;680-1300ms&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;~315ms&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The fine-tuned model beats the 120B teacher by ~3 points while being 200x smaller. The base model at 48.7% is unusable -- over a 3-turn conversation that compounds to about 11.6% success rate.&lt;/p&gt; &lt;p&gt;Architecture note: the SLM never generates user-facing text. It only outputs structured JSON (function name + slots). A deterministic orchestrator handles slot elicitation and response templates. This keeps latency bounded and responses well-formed regardless of what the model outputs.&lt;/p&gt; &lt;p&gt;The whole thing runs locally: Qwen3-ASR-0.6B for speech-to-text, the fine-tuned Qwen3-0.6B via llama.cpp for intent routing, Qwen3-TTS for speech synthesis. Full pipeline on Apple Silicon with MPS.&lt;/p&gt; &lt;p&gt;GitHub (code + training data + pre-trained GGUF): &lt;a href="https://github.com/distil-labs/distil-voice-assistant-banking"&gt;https://github.com/distil-labs/distil-voice-assistant-banking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace model: &lt;a href="https://huggingface.co/distil-labs/distil-qwen3-0.6b-voice-assistant-banking"&gt;https://huggingface.co/distil-labs/distil-qwen3-0.6b-voice-assistant-banking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog post with the full write-up: &lt;a href="https://www.distillabs.ai/blog/the-llm-in-your-voice-assistant-is-the-bottleneck-replace-it-with-an-slm"&gt;https://www.distillabs.ai/blog/the-llm-in-your-voice-assistant-is-the-bottleneck-replace-it-with-an-slm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the training setup, the multi-turn tool calling format, or why the student beats the teacher.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lh8p2xv0vnkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9x0l2/we_replaced_the_llm_in_a_voice_assistant_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9x0l2/we_replaced_the_llm_in_a_voice_assistant_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T14:37:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9h3g8</id>
    <title>Qwen3 Coder Next 8FP in the process of converting the entire Flutter documentation for 12 hours now with just 3 sentence prompt with 64K max tokens at around 102GB memory (out of 128GB)...</title>
    <updated>2026-02-20T00:54:46+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9h3g8/qwen3_coder_next_8fp_in_the_process_of_converting/"&gt; &lt;img alt="Qwen3 Coder Next 8FP in the process of converting the entire Flutter documentation for 12 hours now with just 3 sentence prompt with 64K max tokens at around 102GB memory (out of 128GB)..." src="https://preview.redd.it/5ouemzagqjkg1.png?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=ae25e9c86a516f23c1f47828293a3fbe972468b8" title="Qwen3 Coder Next 8FP in the process of converting the entire Flutter documentation for 12 hours now with just 3 sentence prompt with 64K max tokens at around 102GB memory (out of 128GB)..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A remarkable LLM -- we really have a winner.&lt;/p&gt; &lt;p&gt;(Most of the models below were NVFP4)&lt;/p&gt; &lt;p&gt;GPT OSS 120B can't do this (though it's a bit outdated now)&lt;br /&gt; GLM 4.7 Flash can't do this&lt;br /&gt; SERA 32B tokens too slow&lt;br /&gt; Devstral 2 Small can't do this&lt;br /&gt; SEED OSS freezes while thinking&lt;br /&gt; Nemotron 3 Nano can't do this &lt;/p&gt; &lt;p&gt;(Unsure if it's Cline (when streaming &amp;lt;think&amp;gt;) or the LLM, but GPT OSS, GLM, Devstral, and Nemotron go on an insanity loop, for thinking, coding, or both)&lt;/p&gt; &lt;p&gt;Markdown isn't exactly coding, but for multi-iteration (because it runs out of context tokens) conversions, it's flawless.&lt;/p&gt; &lt;p&gt;Now I just wish VS Codium + Cline handles all these think boxes (on the right side of the UI) better. It's impossible to scroll even with 32GB RAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r9h3g8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9h3g8/qwen3_coder_next_8fp_in_the_process_of_converting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9h3g8/qwen3_coder_next_8fp_in_the_process_of_converting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T00:54:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9gve8</id>
    <title>I feel left behind. What is special about OpenClaw?</title>
    <updated>2026-02-20T00:44:48+00:00</updated>
    <author>
      <name>/u/Recent_Jellyfish2190</name>
      <uri>https://old.reddit.com/user/Recent_Jellyfish2190</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While there are tools like Manus ai, It seems like everyone is excited about OpenClaw lately, and I genuinely donâ€™t fully understand the differentiation. What exactly is the shift here? Is it UX, architecture, control layer, distribution? Not criticizing, just trying to understand what Iâ€™m missing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recent_Jellyfish2190"&gt; /u/Recent_Jellyfish2190 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9gve8/i_feel_left_behind_what_is_special_about_openclaw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9gve8/i_feel_left_behind_what_is_special_about_openclaw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9gve8/i_feel_left_behind_what_is_special_about_openclaw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T00:44:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9wbl3</id>
    <title>ggml / llama.cpp joining Hugging Face â€” implications for local inference?</title>
    <updated>2026-02-20T14:08:56+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ggml / llama.cpp joining HF feels like a significant moment for local inference.&lt;/p&gt; &lt;p&gt;On one hand, this could massively accelerate tooling, integration, and long-term support for local AI. On the other, it concentrates even more of the open model stack under one umbrella.&lt;/p&gt; &lt;p&gt;Is this a net win for the community?&lt;/p&gt; &lt;p&gt;What does this mean for alternative runtimes and independent inference stacks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wbl3/ggml_llamacpp_joining_hugging_face_implications/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wbl3/ggml_llamacpp_joining_hugging_face_implications/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wbl3/ggml_llamacpp_joining_hugging_face_implications/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T14:08:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9fkks</id>
    <title>We will have Gemini 3.1 before Gemma 4...</title>
    <updated>2026-02-19T23:49:53+00:00</updated>
    <author>
      <name>/u/xandep</name>
      <uri>https://old.reddit.com/user/xandep</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9fkks/we_will_have_gemini_31_before_gemma_4/"&gt; &lt;img alt="We will have Gemini 3.1 before Gemma 4..." src="https://preview.redd.it/hd5oal2ngjkg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c08bae0f338dab67384ce398502fe29f5b06645" title="We will have Gemini 3.1 before Gemma 4..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Appeared on Antigravity...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xandep"&gt; /u/xandep &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hd5oal2ngjkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9fkks/we_will_have_gemini_31_before_gemma_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9fkks/we_will_have_gemini_31_before_gemma_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T23:49:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9y6s8</id>
    <title>TranscriptionSuite - A fully local, private &amp; open source audio transcription for Linux, Windows &amp; macOS</title>
    <updated>2026-02-20T15:22:24+00:00</updated>
    <author>
      <name>/u/TwilightEncoder</name>
      <uri>https://old.reddit.com/user/TwilightEncoder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9y6s8/transcriptionsuite_a_fully_local_private_open/"&gt; &lt;img alt="TranscriptionSuite - A fully local, private &amp;amp; open source audio transcription for Linux, Windows &amp;amp; macOS" src="https://external-preview.redd.it/ZjVodnR2dGoyb2tnMfrHn1-Z1IlbM1M-CdvVLf1S0fx3BvVT39BjZwD6xxr6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c01a41fb0c91487d97d9e6bbd7ba58c3750d09f" title="TranscriptionSuite - A fully local, private &amp;amp; open source audio transcription for Linux, Windows &amp;amp; macOS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! This is a short presentation for my hobby project, &lt;a href="https://github.com/homelab-00/TranscriptionSuite"&gt;TranscriptionSuite&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; A fully local &amp;amp; private Speech-To-Text app for Linux, Windows &amp;amp; macOS. Python backend + Electron frontend, utilizing faster-whisper and CUDA acceleration.&lt;/p&gt; &lt;p&gt;If you're interested in the boring dev stuff, go to the bottom section.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;I'm releasing a major UI upgrade today. Enjoy!&lt;/p&gt; &lt;p&gt;Short sales pitch:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;100% Local&lt;/strong&gt;: &lt;em&gt;Everything&lt;/em&gt; runs on your own computer, the app doesn't need internet beyond the initial setup&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Truly Multilingual&lt;/strong&gt;: Supports &lt;a href="https://github.com/openai/whisper/blob/main/whisper/tokenizer.py"&gt;90+ languages&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully featured GUI&lt;/strong&gt;: Electron desktop app for Linux, Windows, and macOS (Apple Silicon)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU + CPU Mode&lt;/strong&gt;: NVIDIA CUDA acceleration (recommended), or CPU-only mode for any platform including macOS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Longform Transcription&lt;/strong&gt;: Record as long as you want and have it transcribed in seconds&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live Mode&lt;/strong&gt;: Real-time sentence-by-sentence transcription for continuous dictation workflows&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speaker Diarization&lt;/strong&gt;: PyAnnote-based speaker identification&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Static File Transcription&lt;/strong&gt;: Transcribe existing audio/video files with multi-file import queue, retry, and progress tracking&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Remote Access&lt;/strong&gt;: Securely access your desktop at home running the model from anywhere (utilizing Tailscale)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Audio Notebook&lt;/strong&gt;: An Audio Notebook mode, with a calendar-based view, full-text search, and LM Studio integration (chat about your notes with the AI)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;System Tray Control&lt;/strong&gt;: Quickly start/stop a recording, plus a lot of other controls, available via the system tray.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ðŸ“Œ&lt;em&gt;Half an hour of audio transcribed in under a minute (RTX 3060)!&lt;/em&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;The seed of the project was my desire to quickly and reliably interface with AI chatbots using my voice. That was about a year ago. Though less prevalent back then, still plenty of AI services like GhatGPT offered voice transcription. However the issue is that, like every other AI-infused company, they &lt;em&gt;always&lt;/em&gt; do it shittily. Yes is works fine for 30s recordings, but what if I want to ramble on for 10 minutes? The AI is smart enough to decipher what I mean and I can speak to it like a smarter rubber ducky, helping me work through the problem.&lt;/p&gt; &lt;p&gt;Well, from my testing back then speak more than 5 minutes and they all start to crap out. And you feel doubly stupid because not only did you get your transcription but you also wasted 10 minutes talking to the wall.&lt;/p&gt; &lt;p&gt;Moreover, there's the privacy issue. They already collect a ton of text data, giving them my voice feels like too much.&lt;/p&gt; &lt;p&gt;So I first looking at any existing solutions, but couldn't find any decent option that could run locally. Then I came across &lt;a href="https://github.com/KoljaB/RealtimeSTT"&gt;RealtimeSTT&lt;/a&gt;, an extremely impressive and efficient Python project that offered real-time transcription. It's more of a library or framework with only sample implementations.&lt;/p&gt; &lt;p&gt;So I started building around that package, stripping it down to its barest of bones in order to understand how it works so that I could modify it. This whole project grew out of that idea.&lt;/p&gt; &lt;p&gt;I built this project to satisfy my needs. I thought about releasing it only when it was decent enough where someone who doesn't know anything about it can just download a thing and run it. That's why I chose to Dockerize the server portion of the code.&lt;/p&gt; &lt;p&gt;The project was originally written in pure Python. Essentially it's a fancy wrapper around &lt;code&gt;faster-whisper&lt;/code&gt;. At some point I implemented a &lt;em&gt;server-client&lt;/em&gt; architecture and added a notebook mode (think of it like calendar for your audio notes).&lt;/p&gt; &lt;p&gt;And recently I decided to upgrade the frontend UI from Python to React + Typescript. Built all in Google AI Studio - App Builder mode for free believe it or not. No need to shell out the big bucks for Lovable, daddy Google's got you covered.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Don't hesitate to contact me here or open an issue on GitHub for any technical issues or other ideas!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TwilightEncoder"&gt; /u/TwilightEncoder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gxbrs1rj2okg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9y6s8/transcriptionsuite_a_fully_local_private_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9y6s8/transcriptionsuite_a_fully_local_private_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T15:22:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9ours</id>
    <title>Qwen3.5 Plus, GLM 5, Gemini 3.1 Pro, Sonnet 4.6, three new open source agents, and a lot more added to SanityBoard</title>
    <updated>2026-02-20T07:24:18+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link: &lt;a href="https://sanityboard.lr7.dev/"&gt;https://sanityboard.lr7.dev/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yeah I've been running evals and working on this for over 3 days straight all day to get this all finished. Too tired to do a proper writeup, so I will give some bullet points and a disclaimer.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;27 New eval results added in total&lt;/li&gt; &lt;li&gt;Got our first 4 community submissions, which brings us GPT 5.3 Codex Spark results, and a few Droid + Skills results to show us how big of a difference a suitable skills file can make.&lt;/li&gt; &lt;li&gt;3 New OSS coding agents; kilocode cli, cline cli, and pi*&lt;/li&gt; &lt;li&gt;Some site UI improvements, like date slider filter, being able to expand the filter options window, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Interesting pattern I realized. GPT-codex models do really well cause they like to iterate, a lot. These kinds of evals favor models with this kind of tendency. Claude models don't iterate as much, so they sometimes get edged out in these kinds of evals. In an actual interactive coding scenario, I do believe the claude models are still better. Now if you want to just assign a long running task and forget it, that's where the gpt-codex models shine. They just keep going and going until done, they're good at that.&lt;/p&gt; &lt;p&gt;A somewhat important note, the infra used makes a HUGE difference in scores. I noticed this very early on, back when I used to run a ton of terminal bench evals, and especially when I decided to run it against as many different providers as I could to see which one was the best for Kimi K2 thinking. Even the speed affected scores a lot. My bench is no different in this regard, although I tried my best to work around this by having generous retry limits, and manually vetting every run for infra issues (which probably takes up the majority of my time), and rerunning any evals that looked like they may have suffered infra issues. This however isn't perfect, I am human. The reason I mention this is cause &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt; infra is dying. It made it almost impossible to bench against the official api. It was actually more expensive to use than paying standard api rates to claude for opus lol. They ghosted after I asked if I could have credits back for the wasted tokens I never got.. but that's neither here nor there. And also you might see some of the same models but from different providers score differently for infra reasons. Even the date of eval might matter for this, since sometimes providers change, either improving and fixing things, or otherwise. Also worth noting since some runs are older than others, some things might not score as well, being on an older agent version. Hopefully the filter by date slider I added can help with this.&lt;/p&gt; &lt;p&gt;*Pi was a large part of why this took me so much time and reruns. The retry logic had to be changed cause it's the only agent that does not have streaming stdout for some reason, and buffers it all until it's done. It also has 0 iteration whatsoever, it just does everything on one shot and never iterates on it again, leading to very poor scores. No other agents behave like this. These changes introduced bugs, which meant a lot of time spent fixing things and having to rerun things for fair evals. Pi I think is really cool, but since it's headless mode or whatever you want to call it is only a half complete implementation at best, it's almost impossible to get a fair evaluation of it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9ours/qwen35_plus_glm_5_gemini_31_pro_sonnet_46_three/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9ours/qwen35_plus_glm_5_gemini_31_pro_sonnet_46_three/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9ours/qwen35_plus_glm_5_gemini_31_pro_sonnet_46_three/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T07:24:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9e27i</id>
    <title>Free ASIC Llama 3.1 8B inference at 16,000 tok/s - no, not a joke</title>
    <updated>2026-02-19T22:48:03+00:00</updated>
    <author>
      <name>/u/Easy_Calligrapher790</name>
      <uri>https://old.reddit.com/user/Easy_Calligrapher790</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;A fast inference hardware startup, Taalas, has released a free chatbot interface and API endpoint running on their chip. They chose a small model intentionally as proof of concept. Well, it worked out really well, it runs at 16k tps! I know this model is quite limited but there likely exists a group of users who find it sufficient and would benefit from hyper-speed on offer.&lt;/p&gt; &lt;p&gt;Anyways, they are of course moving on to bigger and better models, but are giving free access to their proof-of-concept to people who want it.&lt;/p&gt; &lt;p&gt;More info: &lt;a href="https://taalas.com/the-path-to-ubiquitous-ai/"&gt;https://taalas.com/the-path-to-ubiquitous-ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chatbot demo: &lt;a href="https://chatjimmy.ai/"&gt;https://chatjimmy.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Inference API service: &lt;a href="https://taalas.com/api-request-form"&gt;https://taalas.com/api-request-form&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's worth trying out the chatbot even just for a bit, the speed is really something to experience. Cheers!&lt;/p&gt; &lt;p&gt;EDIT: It's worth noting that the chatbot demo actually undersells the speed on display. Anything over a few hundred tps is perceived as instantaneous, so the experience of 1k tps vs 16k tps should be pretty similar. So you are only seeing the bottom few percent of the speed on offer. A proper demo would be using a token-intensive workload with their API. Now THAT would be something to see.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Easy_Calligrapher790"&gt; /u/Easy_Calligrapher790 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9e27i/free_asic_llama_31_8b_inference_at_16000_toks_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9e27i/free_asic_llama_31_8b_inference_at_16000_toks_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9e27i/free_asic_llama_31_8b_inference_at_16000_toks_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T22:48:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9zt8m</id>
    <title>The top 3 models on openrouter this week ( Chinese models are dominating!)</title>
    <updated>2026-02-20T16:21:50+00:00</updated>
    <author>
      <name>/u/keb_37</name>
      <uri>https://old.reddit.com/user/keb_37</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9zt8m/the_top_3_models_on_openrouter_this_week_chinese/"&gt; &lt;img alt="The top 3 models on openrouter this week ( Chinese models are dominating!)" src="https://preview.redd.it/h4l8zr4rdokg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1cb3201433eea5c7cd862fbc8c0f259e4e6b134" title="The top 3 models on openrouter this week ( Chinese models are dominating!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the first time i see a model exceed 3 trillion tokens per week on openrouter!&lt;/p&gt; &lt;p&gt;the first time i see more than one model exceed a trillion token per week ( it was only grok 4 fast month ago)&lt;/p&gt; &lt;p&gt;the first time i see chinese models destroying US ones like this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/keb_37"&gt; /u/keb_37 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h4l8zr4rdokg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9zt8m/the_top_3_models_on_openrouter_this_week_chinese/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9zt8m/the_top_3_models_on_openrouter_this_week_chinese/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T16:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9uu5h</id>
    <title>Qwen3 Coder Next on 8GB VRAM</title>
    <updated>2026-02-20T13:05:21+00:00</updated>
    <author>
      <name>/u/Juan_Valadez</name>
      <uri>https://old.reddit.com/user/Juan_Valadez</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I have a PC with 64 GB of RAM and an RTX 3060 12 GB, and I'm running Qwen3 Coder Next in MXFP4 with 131,072 context tokens.&lt;/p&gt; &lt;p&gt;I get a sustained speed of around 23 t/s throughout the entire conversation.&lt;/p&gt; &lt;p&gt;I mainly use it for front-end and back-end web development, and it works perfectly.&lt;/p&gt; &lt;p&gt;I've stopped paying for my Claude Max plan ($100 USD per month) to use only Claude Code with the following configuration:&lt;/p&gt; &lt;p&gt;&lt;code&gt;set GGML_CUDA_GRAPH_OPT=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server -m ../GGUF/qwen3-coder-next-mxfp4.gguf -ngl 999 -sm none -mg 0 -t 12 -fa on -cmoe -c 131072 -b 512 -ub 512 -np 1 --jinja --temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01 --repeat-penalty 1.0 --host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--port 8080&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I promise you it works fast enough and with incredible quality to work with complete SaaS applications (I know how to program, obviously, but I'm delegating practically everything to AI).&lt;/p&gt; &lt;p&gt;If you have at least 64 GB of RAM and 8 GB of VRAM, I recommend giving it a try; you won't regret it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juan_Valadez"&gt; /u/Juan_Valadez &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uu5h/qwen3_coder_next_on_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uu5h/qwen3_coder_next_on_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uu5h/qwen3_coder_next_on_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1r99yda</id>
    <title>Pack it up guys, open weight AI models running offline locally on PCs aren't real. ðŸ˜ž</title>
    <updated>2026-02-19T20:11:42+00:00</updated>
    <author>
      <name>/u/CesarOverlorde</name>
      <uri>https://old.reddit.com/user/CesarOverlorde</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r99yda/pack_it_up_guys_open_weight_ai_models_running/"&gt; &lt;img alt="Pack it up guys, open weight AI models running offline locally on PCs aren't real. ðŸ˜ž" src="https://preview.redd.it/ogkdei4udikg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8834b06cb1ae3aaa95c27230b622dd640e7d9634" title="Pack it up guys, open weight AI models running offline locally on PCs aren't real. ðŸ˜ž" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CesarOverlorde"&gt; /u/CesarOverlorde &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ogkdei4udikg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r99yda/pack_it_up_guys_open_weight_ai_models_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r99yda/pack_it_up_guys_open_weight_ai_models_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T20:11:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9tdvr</id>
    <title>Kimi K2.5 better than Opus 4.6 on hallucination benchmark in pharmaceutical domain</title>
    <updated>2026-02-20T11:54:25+00:00</updated>
    <author>
      <name>/u/aiprod</name>
      <uri>https://old.reddit.com/user/aiprod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9tdvr/kimi_k25_better_than_opus_46_on_hallucination/"&gt; &lt;img alt="Kimi K2.5 better than Opus 4.6 on hallucination benchmark in pharmaceutical domain" src="https://preview.redd.it/c1z228f22nkg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57a4ecba13b26df8634c1b123271ef9c3a609c4f" title="Kimi K2.5 better than Opus 4.6 on hallucination benchmark in pharmaceutical domain" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know the benchmark is mostly commercial models but Kimi K2.5 was part of it and I was actually surprised how well it did against its commercial counterparts.&lt;/p&gt; &lt;p&gt;The benchmark test 7 recent models for hallucinations on a realistic use case and data from the pharmaceutical domain.&lt;/p&gt; &lt;p&gt;Surprisingly, Opus 4.6 has the highest hallucination rate.&lt;/p&gt; &lt;p&gt;I labeled a good chunk of the data and from my impressions, it just invented clinical protocols or tests that werenâ€™t in the source data (probably trying to be helpful).&lt;/p&gt; &lt;p&gt;Kimi K2.5 did much better (albeit still not great).&lt;/p&gt; &lt;p&gt;You can read the full benchmark here: &lt;a href="https://www.blueguardrails.com/en/blog/placebo-bench-an-llm-hallucination-benchmark-for-pharma"&gt;https://www.blueguardrails.com/en/blog/placebo-bench-an-llm-hallucination-benchmark-for-pharma&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Dataset is also available on hugging face.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aiprod"&gt; /u/aiprod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c1z228f22nkg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9tdvr/kimi_k25_better_than_opus_46_on_hallucination/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9tdvr/kimi_k25_better_than_opus_46_on_hallucination/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T11:54:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9wvg4</id>
    <title>GGML and llama.cpp join HF to ensure the long-term progress of Local AI</title>
    <updated>2026-02-20T14:31:22+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wvg4/ggml_and_llamacpp_join_hf_to_ensure_the_longterm/"&gt; &lt;img alt="GGML and llama.cpp join HF to ensure the long-term progress of Local AI" src="https://external-preview.redd.it/tLGg2WMvFn2R5w7Nf2m6oJPphAYJILLSWaWPLPoW8i4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6bb0cd5000a00c0e28c8ae17203068e5acfb352" title="GGML and llama.cpp join HF to ensure the long-term progress of Local AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;article by Georgi Gerganov, Xuan-Son Nguyen, Aleksander Grygier, Lysandre, Victor Mustar, Julien Chaumond&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/ggml-joins-hf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wvg4/ggml_and_llamacpp_join_hf_to_ensure_the_longterm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wvg4/ggml_and_llamacpp_join_hf_to_ensure_the_longterm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T14:31:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9qa7l</id>
    <title>Kimi has context window expansion ambitions</title>
    <updated>2026-02-20T08:54:10+00:00</updated>
    <author>
      <name>/u/omarous</name>
      <uri>https://old.reddit.com/user/omarous</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9qa7l/kimi_has_context_window_expansion_ambitions/"&gt; &lt;img alt="Kimi has context window expansion ambitions" src="https://preview.redd.it/3cvl2bdh5mkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e22f6604997ccccf6f6215ae239ab8f8b1dd09c3" title="Kimi has context window expansion ambitions" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omarous"&gt; /u/omarous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3cvl2bdh5mkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9qa7l/kimi_has_context_window_expansion_ambitions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9qa7l/kimi_has_context_window_expansion_ambitions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T08:54:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9uuc6</id>
    <title>Deepseek and Gemma ??</title>
    <updated>2026-02-20T13:05:36+00:00</updated>
    <author>
      <name>/u/ZeusZCC</name>
      <uri>https://old.reddit.com/user/ZeusZCC</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uuc6/deepseek_and_gemma/"&gt; &lt;img alt="Deepseek and Gemma ??" src="https://preview.redd.it/84ph0pirenkg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d2b363b1900aae44bcfc12c0eeb9d8e2caa7d08" title="Deepseek and Gemma ??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZeusZCC"&gt; /u/ZeusZCC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/84ph0pirenkg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uuc6/deepseek_and_gemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uuc6/deepseek_and_gemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:05:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9vywq</id>
    <title>GGML.AI has got acquired by Huggingface</title>
    <updated>2026-02-20T13:54:26+00:00</updated>
    <author>
      <name>/u/Time_Reaper</name>
      <uri>https://old.reddit.com/user/Time_Reaper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vywq/ggmlai_has_got_acquired_by_huggingface/"&gt; &lt;img alt="GGML.AI has got acquired by Huggingface" src="https://external-preview.redd.it/l687iazpdDZhrDlIbQBxf8OTcfiJg6WGdsBpv03NqVo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9e45ab199a5cdbdf8c5eb1968743c094b946e98" title="GGML.AI has got acquired by Huggingface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Time_Reaper"&gt; /u/Time_Reaper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/19759"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vywq/ggmlai_has_got_acquired_by_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vywq/ggmlai_has_got_acquired_by_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:54:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60qu9</id>
    <title>AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)</title>
    <updated>2026-02-16T05:11:16+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" src="https://preview.redd.it/u11uh8jfisjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afc30b2b6ae673f2e940109e2001bb498bd818ad" title="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ðŸ‘‹&lt;/p&gt; &lt;p&gt;We're excited for Thursday's guests: &lt;strong&gt;The StepFun Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Thursday, Feb. 19th, 8 AMâ€“11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;âš ï¸ &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please donâ€™t post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u11uh8jfisjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T05:11:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
