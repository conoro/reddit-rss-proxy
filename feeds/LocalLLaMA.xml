<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-15T12:29:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o6yz59</id>
    <title>[WebGPU Demo] Granite Docling 258M — document parsing 100% in-browser (HF Space)</title>
    <updated>2025-10-15T02:18:14+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Run IBM’s &lt;strong&gt;Granite-Docling-258M&lt;/strong&gt; entirely in your browser via &lt;strong&gt;WebGPU + Transformers.js&lt;/strong&gt; to convert scanned pages/images into structured &lt;strong&gt;HTML&lt;/strong&gt;—no data leaves your machine. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Upload &lt;strong&gt;PNG/JPG/WEBP&lt;/strong&gt; → get clean HTML. &lt;/li&gt; &lt;li&gt;Local/WebGPU execution = privacy-friendly.&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://huggingface.co/spaces/ibm-granite/granite-docling-258M-WebGPU"&gt;&lt;code&gt;https://huggingface.co/spaces/ibm-granite/granite-docling-258M-WebGPU&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6yz59/webgpu_demo_granite_docling_258m_document_parsing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6yz59/webgpu_demo_granite_docling_258m_document_parsing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6yz59/webgpu_demo_granite_docling_258m_document_parsing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T02:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1o78i0i</id>
    <title>eGpu with two slots</title>
    <updated>2025-10-15T11:33:32+00:00</updated>
    <author>
      <name>/u/Medium_Question8837</name>
      <uri>https://old.reddit.com/user/Medium_Question8837</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys right now I am running a laptop with rtx4090 with 16gb vram.&lt;/p&gt; &lt;p&gt;Unfortunately I can not run middle size models efficiently.&lt;/p&gt; &lt;p&gt;I was wondering if there is any eGPU that supports two rtx4090 or two rtx 5090.&lt;/p&gt; &lt;p&gt;I checked the new razer eGPU but unfortunately it only supports one gpu.&lt;/p&gt; &lt;p&gt;I plan on using it with thunderbolt 4.&lt;/p&gt; &lt;p&gt;Any recommendations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Medium_Question8837"&gt; /u/Medium_Question8837 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o78i0i/egpu_with_two_slots/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o78i0i/egpu_with_two_slots/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o78i0i/egpu_with_two_slots/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T11:33:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1o74ufs</id>
    <title>Coding assistant with web search?</title>
    <updated>2025-10-15T07:49:40+00:00</updated>
    <author>
      <name>/u/ramendik</name>
      <uri>https://old.reddit.com/user/ramendik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was anyone successful at getting any open source coding assistant to offer web search tools and to get the model to actually use them when tricky library/framework/etc questions arise? If so I'd appreciate the configuration details.&lt;/p&gt; &lt;p&gt;Asking after chasing an Alpine.js UI glitch in endless circles until I went to Gemini web, which has built in search grounding. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ramendik"&gt; /u/ramendik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o74ufs/coding_assistant_with_web_search/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o74ufs/coding_assistant_with_web_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o74ufs/coding_assistant_with_web_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T07:49:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1o76ev6</id>
    <title>Reproducing Karpathy’s NanoChat on a Single GPU — Step by Step with AI Tools</title>
    <updated>2025-10-15T09:32:52+00:00</updated>
    <author>
      <name>/u/Fresh-Recover1552</name>
      <uri>https://old.reddit.com/user/Fresh-Recover1552</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o76ev6/reproducing_karpathys_nanochat_on_a_single_gpu/"&gt; &lt;img alt="Reproducing Karpathy’s NanoChat on a Single GPU — Step by Step with AI Tools" src="https://external-preview.redd.it/2nIcOLn_4UHZKPN7434gGUG3AWhQ6tAHEqr0xSwVurg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c96115f1ddf09012f32682be6f927394354d91df" title="Reproducing Karpathy’s NanoChat on a Single GPU — Step by Step with AI Tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI tools can now rebuild entire repos into runnable notebooks.&lt;br /&gt; I used DeepWiki + Gemini to reproduce Karpathy’s &lt;em&gt;NanoChat&lt;/em&gt; in a single Colab notebook running on one GPU. $0 spent.&lt;/p&gt; &lt;p&gt;Read the full story 👇&lt;br /&gt; &lt;a href="https://limcheekin.medium.com/reproducing-karpathys-nanochat-on-a-single-gpu-step-by-step-with-ai-tools-e9420aaee912"&gt;https://limcheekin.medium.com/reproducing-karpathys-nanochat-on-a-single-gpu-step-by-step-with-ai-tools-e9420aaee912&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Appreciate any feedback from you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fresh-Recover1552"&gt; /u/Fresh-Recover1552 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://limcheekin.medium.com/reproducing-karpathys-nanochat-on-a-single-gpu-step-by-step-with-ai-tools-e9420aaee912"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o76ev6/reproducing_karpathys_nanochat_on_a_single_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o76ev6/reproducing_karpathys_nanochat_on_a_single_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T09:32:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1o75s9p</id>
    <title>Anyone else having reasoning parser issue with Qwen-cli + GLM4.6 combo in vllm?</title>
    <updated>2025-10-15T08:52:11+00:00</updated>
    <author>
      <name>/u/kyazoglu</name>
      <uri>https://old.reddit.com/user/kyazoglu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75s9p/anyone_else_having_reasoning_parser_issue_with/"&gt; &lt;img alt="Anyone else having reasoning parser issue with Qwen-cli + GLM4.6 combo in vllm?" src="https://a.thumbs.redditmedia.com/xVHPmFcZJoIxprplw4g-acHlK81cRnFn5sqwKWagIM8.jpg" title="Anyone else having reasoning parser issue with Qwen-cli + GLM4.6 combo in vllm?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/exyq8j7eo8vf1.png?width=827&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eeae342db12cfa26110f947e60b1e92b7da99826"&gt;https://preview.redd.it/exyq8j7eo8vf1.png?width=827&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eeae342db12cfa26110f947e60b1e92b7da99826&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi. The issue is clear. I can't get rid of the think tokens. My serve command:&lt;/p&gt; &lt;p&gt;--host &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; --port 8000 --model zai-org/GLM-4.6-FP8 --dtype auto --gpu-memory-utilization 0.95 --api-key &amp;lt;some\_key\_here&amp;gt; --max-model-len 48000 --max-num-seqs 16 --enable-auto-tool-choice --tool-call-parser glm45 --reasoning-parser glm45 --enable-expert-parallel --tensor-parallel-size 4&lt;/p&gt; &lt;p&gt;I also tried qwen3 and deepseek_r1 for the reasoning-parser but that didn't work. I also added chat template argument and pointed to the jinja file in the model folder. That also didn't work. &lt;/p&gt; &lt;p&gt;Any ideas?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyazoglu"&gt; /u/kyazoglu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75s9p/anyone_else_having_reasoning_parser_issue_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75s9p/anyone_else_having_reasoning_parser_issue_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o75s9p/anyone_else_having_reasoning_parser_issue_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T08:52:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o77ag4</id>
    <title>A guide to the best agentic tools and the best way to use them on the cheap, locally or free</title>
    <updated>2025-10-15T10:26:15+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did you expect an AI generated post? Complete with annoying emojis and GPTisms? I don't blame you. These AI generated posts are getting out of hand, and hurt to read. Vibe-coders seem to be some of the worst offenders of this. Am I a vibe coder too? Don't know. I don't really rely on AI coding much, but thought it was pretty neat, so I spent some weeks checking out various tools and models to get a feel for them. How I use them might be very different from others, so going to give that warning in advance. I prefer to write my code, then see if I can use the agent to either improve it some way (help with refactoring, making some my monolithic scripts more modular, writing tests, this kind of stuff), and sometimes trying to add features to my existing tools. I have tried one shotting a few tools from scratch with AI, but it wasn't for me, especially the agents that like to overengineer things and get carried away with it. I like knowing what my code is doing. If you are just getting into coding, I don't suggest relying on these tools heavily. I've seen people be very productive with these kinds of tools and able to get a lot done with them, but almost all of those people were very experienced devs that know their way around code. I am not one of those people and am able to affirm that AI should not be heavily leaned upon without a solid foundation. Let's not forget the guy who vibe coded a script to &amp;quot;distill&amp;quot; much larger models into smaller ones, that ultimately did nothing, and ended up uploading &amp;quot;distills&amp;quot; that were identical weights to their original models (yeah, you might remember me from that post). Of course ppl still ate it up, cause confirmation bias, so I guess it's all about how you market the snake oil? Either way, if you're here interested in which agentic coding tools, and models work best, read on. I will share what I've learned, including some very cool free API options at the bottom of this post. We seem to be in the boom period of agentic coding, so a lot of providers and services are being very generous. And power users of agentic coding who probably know more than me, please do comment your thoughts and experiences.&lt;/p&gt; &lt;p&gt;Why does it matter? You can use the best model available, or even just a mediocre model, but the tool you use with it matters. A good tool will drastically give you better results. Not only that, some models work MUCH better with specific tools. Here are my recommendations, and non-recommendations, starting with a few non-recommendations:&lt;/p&gt; &lt;p&gt;- Warp: Looks like a great cli tool. Scores well in leaderboards/benchmarks, and is received well by users. BUT, no BYOK option. Makes them immediately dead on arrival as a serious option for me. You're completely at mercy to their service and any changes they make to it, randomly or not. I also don't really like the subscription model, makes little to no sense, because there's almost no transparency. You get credits to use monthly but NOWHERE do they tell you how many tokens, or requests those credits give you with any model. Their docs barely have anything on this, it's literally all vibes and doesn't tell you more than some models use more credits, and using more context, tool calls, tokens, etc use more credits.&lt;/p&gt; &lt;p&gt;- Cursor: Looks like a really nice ide, and seems to work pretty well. However, suffers all the same issues as above. A lot of agentic tools do. So I wont cover too many of these. These are more like platforms + service rather than tools to use with whatever service you want.&lt;/p&gt; &lt;p&gt;- Roocode: Want a quick answer? I'd probably recommend this. Very solid, all around choice. Very well recieved by the community. Has the highest rating out of all the AI extensions I saw on vscode, if that means anything. Scores very well in gosuevals (I highly suggest checking out his videos, search gosucoder on youtube, he goes very indepth in how well these agentic tools work, and in his comparisons) and is usually a top 1-3 in those monthly evals for most models. Supports code indexing for free with any provider, local api, or gemini embedding which is free via api it seems (and probably the very best embedding model available right now). Integrates well with vscode.&lt;/p&gt; &lt;p&gt;- Qwen Code CLI: I don't want to make ppl read a ton to get to the best choices, so going to go ahead and share this one next because it is by far, imo, the best free, no frills option. Signup for qwen account, login via browser for oath. Done, now you have 4k qwen-coder-plus requests daily, and it's fast too at 70t/s. Qwen3 coder is one of the best opensource models, and it works way better with qwen code cli, and imo, to the point of being better than most other OSS model + tool combinations. The recent updates are very nice, adding things like planning mode. This was also imo the easiest and simplest to use of the tools ive tried. Very underrated and slept on. Qwen coder plus was originally just Qwen3 Coder 480b, the open source model, and it might still be, but they have a newer updated version that's even better, not sure if this is the one we get access too now. If it is, this easily beats using anything outside of gpt5 or claude models. this tool is gemini cli based.&lt;/p&gt; &lt;p&gt;- Droid: Im still in the process of trying this one out (nothing bad yet though) so I'm going to withhold from saying too much subjective opinion and just share what I know. Scores the highest out of any agents in terminal bench so it seemed promising, but I've been looking around, and asking a lot of people about their experiences with it so far, and getting a lot of mixed feedback. I like it as a concept, will have to see if it's actually that good. Just a few anecdotal experiences are pretty unreliable after all and one big thing it has over others is that it supports BYOK at free tier without any extra caveats. The big complaint I've seen is that this tool absolutely chews through tokens (which makes their nice monthly plan less impressive), but this might not be a big deal if you use your own local model or a free api (more on this later). The most attractive thing about this tool to me is the very generous monthly plan. You get 20 million tokens for $20 monthly. Using claude sonnet uses those tokens at 1.2x, which is very nice pricing (essentially 16.7 million tokens, or around $400~ worth of tokens based off anthropic api pricing and how much artificial analysis cost to run) when compared to the claude monthly subs (I see ppl maxing out their $100 subs at around 70 million tokens), especially when you consider its not rate limited in 5 hour periods. They also have gpt 5 codex at 0.5x (so 40 million tokens monthly), and glm 4.6 at 0.25x (80 million monthly). This is a &lt;em&gt;very&lt;/em&gt; generous $20 sub imo, especially if their GLM model has thinking available (I dont think it does, which imo makes it not worth bothering to use, but the &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt; monthly sub also has thinking disabled). I wonder if theyre eating a loss or going at cost to try and build a userbase. Lastly, they have a very nice trial, giving you 20m tokens free for one month, or 40m for 2 months if you use a referral link. I will include mine here for convenience's sake, but I do not do nearly enough AI coding to benefit from any extra credits I get so you might do someone else the favor and use their referral link instead. &lt;a href="https://app.factory.ai/r/0ZC7E9H6"&gt;https://app.factory.ai/r/0ZC7E9H6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- zed: a rust based ide. feels somewhere between a text editor like notepad++ or kate (the kde default) and vscode. its incredibly fast, and works quite well. the UI will not feel too unfamiliar from vscode, but it doesnt have the huge extensions marketplace vscode does. on the other hand, its super performant and dead simple while still feeling very full-featured, with a lot more to be added in the future. I replaced my systems default editor (kate) with zed, and have been super happy with the decision. feels much better to use. I would use it in place of vscode, but some things have better integration with vscode so I only use zed sometimes. now lets talk about it agentic capabilities. its improved a lot, and is actually near the top of gosu's latest evals. the problem is, it absolutely &lt;em&gt;chews&lt;/em&gt; through tokens. same issue as droid, but even worse it seems like. They have a two week trial that gives you $20 credits. I used up $5 with sonnet 4.5 in less than a half hour. on the other hand, its byok, so I can see this being one of the best options for use with a local model, cheap api or even free api. the other thing is, I dont think there's a planning mode, or orchestrator mode, which has been the main reason I havent been using this agent. when I did test it, it absolutely overengineered everything and tried to do too much, so that might be something to watchout for as well.&lt;/p&gt; &lt;p&gt;- claude code: basically the benchmark cli tool, everyone compares other tools to this tool. Has a lot of features, and was the first to have a lot of the features other agentic tools have. It's reliable and works well. zed has native support for claude code now btw. this matters for things like access to lsp, following what the agent is doing, etc. you want to be using cli tools that are supported by your ide natively or have extensions for it (almost all cli tools have an extension for vscode, one of the reasons why I havent switched off of it completely).&lt;/p&gt; &lt;p&gt;- codex cli or vscode extension: mixed reception at first, but it's improved and ppl seem to really like it now. the gpt5 models (gpt-oss), especially codex don't really shine until used with this tool (similar to qwen coder with qwen code). The difference is very large, to the point I would say you are getting a hampered experience with those models until you use it with this tool.&lt;/p&gt; &lt;p&gt;- crush: made by main dev behind opencode and charm, who has made some of the best terminal ui libraries. sounds like the dream combination right? so far it's a pretty decent all around tool, that looks really nice, but isn't anything special yet. Not a bad choice by any means. open source too.&lt;/p&gt; &lt;p&gt;- gemini cli: well, the cli is nice. but gemini for whatever reason kind of sucks at agentic coding. would not bother with this until gemini 3.0 comes out. gemini 2.5 pro is however, still one of the best chat assistants, and an especially good for using with the research tool. if you have a student email of some sort, you can probably get a year free of gemini pro.&lt;/p&gt; &lt;p&gt;- trae + seed: no byok, but looks good on swebench? sorry, im a no byok hater.&lt;/p&gt; &lt;p&gt;- augment: no byok. crappy plan. doesnt even seem like its that great, better options out there.&lt;/p&gt; &lt;p&gt;- refact: looks good on swebench, havent actually tried it, and doesnt seem like anyone else has really. does seem like it supports byok atleast.&lt;/p&gt; &lt;p&gt;- kilocode: a novel idea, cline + roo was their main pitch, but roo has implemented most things that kilocode had, and just straight up performs better on most tasks these days. I get the feeling kilocode is just playing catchup, and only get's their once theyre upstream with roo's code since it's based off of it. some ppl still like kilocode and it can be worth using anyways if it fits your preference.&lt;/p&gt; &lt;p&gt;- cline: some ppl like cline more than roo, but most prefer roo. also lower rating than roo in vscode extension store.&lt;/p&gt; &lt;p&gt;There are a lot more agentic coding tools out there, but I'm running out of stamina to be going through them, so next I will cover the best model options, after mentioning one important thing. Use mcp servers. They will enhance your agentic coding by a lot. I highly suggest at least getting the likes of exa search, context7, etc. I haven't used very many of these yet and am in the process of experimenting with them, so I cant offer too much advice here (thankfully. Im writing way too much.)&lt;/p&gt; &lt;p&gt;The very best model right now, for agentic coding, is sonnet 4.5. This will probably change at some point so do some research if this post isnt recent anymore. Only gpt 5 codex comes close or is as good, and thats only if you use it with codex cli or the codex extension. These options can however be a little pricy, especially if you pay by the token in api cost. The monthly subs however, can be worth it to some. Afterall, sometimes it much better to get things done in one shot than spend hours reprompting, rolling back changes and trying again with a lesser model.&lt;/p&gt; &lt;p&gt;The next tier of models is pretty interesting. None of these come very close to the top two choices, but are all relatively close to each other in capability, regardless of cost. Gpt-5, the non codex model is one such model, and probably near the top of this tier, but it costs the same as gpt-5 codex so why would you use it? The best bang for buck model in this category is probably gpt 5 mini (medium reasoning, high reasoning isnt much better and takes up a lot more tokens), and deepseek v3.2-exp, if we go based purely of cost per token. gpt 5 mini is more capable, but a little more expensive. Deepseek v3.2 is by far the cheapest of this category, and surprisingly capable for how cheap it is, I would rate it just under kimi k2 0905 and qwen3 coder 480b. GLM 4.6 is only around those two mentioned models with reasoning disabled, but with reasoning enabled it becomes much better. Sadly, the glm sub that everyone has been so hyped about, has thinking disabled. So get the sub if you want.. it is cheap as heck, but.. know you are only getting around that level of capability. Here's where it gets interesting. Gpt 5 mini is completely free with copilot pro, which is also free if you have any old (or current) student email. This, with reasoning at medium is step above glm 4.6 without reasoning. Unfortunately you do get tied down to using it within copilot, or tools that have custom headers to spoof their agent built-in (I think opencode has this?). Now for the free models.. kimi k2 0905 is completely free, unlimited use at 40 rpm, via the nvidia nim api. just make an account and get an api key, use like any other openai compatible api. This is by far the best or one of the best non-thinking models. It's in the same realm as glm 4.6 without reasoning (above it slightly I'd say, but glm 4.6 with reasoning will blow it out), qwen coder 480b (above it slightly I'd say, unless used with qwen code, where I then give the edge to qwen coder). GLM 4.6, if reasoning is enabled is near the top of this pack, but this tier of models is still significantly below the best one or two models.&lt;/p&gt; &lt;p&gt;A note on roocode, and other tools that support code indexing via embedding models. roo specifically supports gemini embedding which is bar none the very best available, and is apparently completely free via api atm. but if your tool doesnt support it, nebiusai gives you $1 credit for free on signup, that never expires afaik, and their qwen3 embedding 8b model is the cheapest of any provider at 0.01 per million. That $1 will last you forever if you use it for embedding only, and it is the second best available embedding model behind gemini (and is the very best OSS embedding model atm). sadly they dont have any reranking models, but I think I only saw one tool that supported this? and cant remember which tool it is. if you do stumble across one, you can sign up with novita for a $1 voucher as well, and use qwen3 reranker 8b from their api. Pretty good combo on roo code, to use kimi k2 0905 from nvidia api, and either gemini embedding or nebius' qwen3 embedding.&lt;/p&gt; &lt;p&gt;As far as local models go for running on typical home computers, these unfortunately, have a very big gap between much larger OSS models, that youre better off using off a free api, or trial credits, but if you dont care enough to, or are just trying stuff for fun, privacy, etc, your best bets are qwen3 coder 30b a3b with qwen code cli, or gpt-oss 20b + codex cli/extension. next step up is gpt oss 120b with codex cli/extension if you have the ram and vram for it. Devstral small 2507 is okay too, but I dont think its quite as good for its size.&lt;/p&gt; &lt;p&gt;Lastly, speaking on free credits, I came across some reddit posts claiming free credits for some chinese openrouter clone looking website called agent router. Was extremely sussed out by it, and couldnt find much information on it other than few ppl saying they got it working after some hassle, and that the software stack is based off a real opensource stack with repos available on github (new api and one api). Decided to very reluctantly give it a shot, but the website was a buggy half implemented mess throwing backend errors galore, which sussed me out more. They only supported signup via oath from github and linux do. Me wondering what the catch was, checked my permissions after signing up with github, and saw they only got read access to what email my github was under. I saw I did get my credits from signing up via referral. The rates for sonnet looked typical, but the rates for the other models seemed too good to be true. So I get an api key, try it with my pageassist firefox extension (I highly recommend it, dev is great, has added a bunch of stuff after feedback on discord), and got 401 error. Tried with cherry studio (also very nice), same error. Website has me logged out now, and I cant log back in, I keep getting error too many requests in chinese. Gave up. Tried again daily for a few days and same issues. Finally, today the website is working perfectly, no lag either. Im amazed, was starting to think it was some sort of weird scam, which is why I hadnt told anyone about it yet. Says I have no api keys for some reason so I make a new one. doesnt work still. after some replies from other on reddit, and reading the docs, I realize, these models only work with specific tools, so that seems to be the main catch. after realizing this I reinstalled codex cli, followed the docs for using the api with codex cli (this is a must btw) after translating with deepseek v3.2 and it was working perfectly. Mind blown. So now I have $125 credits with temu openrouter, which serves gpt 5 at only 0.003 dollars per million tokens lol. Me and a few others have a sneaking suspicion the hidden catch is that they store, and use your data, probably for training, but personally I dont care. If this isnt an issue for you guys either, I highly suggest finding someone's referral link and using it to signup with github or linuxdo. You will get $100 from the referral, and $25 for logging in. Again, I still have my trial credits through from other tools, and dont use ai coding much so use someone elses referral if you wanna be nice, but I will throw mine in here anyways for convenience sake. &lt;a href="https://agentrouter.org/register?aff=ucNl"&gt;https://agentrouter.org/register?aff=ucNl&lt;/a&gt; PS I suggest using a translation tool as not all of it is in english, I used the first ai translation extension that works with openrouter I found from the firefox store lol.&lt;/p&gt; &lt;p&gt;On a second read, maybe I should have put this through some ai to make this more human readable. Ah well. I bet one of you will put this through claude sonnet anyways, and comment it below. wont be me though. Tl;dr if you skipped to the bottom though; nvidia nim api is free, use kimi k2 0905 from there with any tool that looks interesting, roo code is the all round solid choice. or just use qwen code cli with oath.&lt;/p&gt; &lt;p&gt;some links:&lt;/p&gt; &lt;p&gt;&lt;a href="https://build.nvidia.com/explore/discover"&gt;https://build.nvidia.com/explore/discover&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://gosuevals.com/"&gt;https://gosuevals.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/gosucoder"&gt;https://www.youtube.com/gosucoder&lt;/a&gt; (no im not affaliated with him, or anything/anyone mentioned in this post)&lt;/p&gt; &lt;p&gt;&lt;a href="https://discord.com/invite/YGS4AJ2MxA"&gt;https://discord.com/invite/YGS4AJ2MxA&lt;/a&gt; (his discord, I hang out here and the koboldai discord a lot if you wanna find me)&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/qwen-code"&gt;https://github.com/QwenLM/qwen-code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/upstash/context7"&gt;https://github.com/upstash/context7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://zed.dev/"&gt;https://zed.dev/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o77ag4/a_guide_to_the_best_agentic_tools_and_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o77ag4/a_guide_to_the_best_agentic_tools_and_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o77ag4/a_guide_to_the_best_agentic_tools_and_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T10:26:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6s89n</id>
    <title>Tested 9 RAG query transformation techniques – HydE is absurdly underrated</title>
    <updated>2025-10-14T21:22:30+00:00</updated>
    <author>
      <name>/u/Best-Information2493</name>
      <uri>https://old.reddit.com/user/Best-Information2493</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6s89n/tested_9_rag_query_transformation_techniques_hyde/"&gt; &lt;img alt="Tested 9 RAG query transformation techniques – HydE is absurdly underrated" src="https://preview.redd.it/fq5i6e8q95vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f8db07dad84a6951edf7b8129992c0a4a7da454f" title="Tested 9 RAG query transformation techniques – HydE is absurdly underrated" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Your RAG system isn't bad. Your queries are.&lt;/p&gt; &lt;p&gt;I just tested 9 query transformation techniques. Here's what actually moved the needle:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Top 3:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;HydE&lt;/strong&gt; – Generate a hypothetical answer, search for docs similar to &lt;em&gt;that&lt;/em&gt;. Sounds dumb, works incredibly well. Solves the semantic gap problem.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAG-Fusion&lt;/strong&gt; – Multi-query + reranking. Simple, effective, production-ready.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Step-Back&lt;/strong&gt; – Ask abstract questions first. &amp;quot;What is photosynthesis?&amp;quot; before &amp;quot;How do C4 plants fix carbon?&amp;quot;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Meh tier:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-Query: Good baseline, nothing special&lt;/li&gt; &lt;li&gt;Decomposition: Works but adds complexity&lt;/li&gt; &lt;li&gt;Recursive: Slow, minimal quality gain for simple queries&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key insight:&lt;/strong&gt; You're spending time optimizing embeddings when your query formulation is the actual bottleneck.&lt;/p&gt; &lt;p&gt;Notebook: &lt;a href="https://colab.research.google.com/drive/1HXhEudDjJsXCvP3tO4G7cAC15OyKW3nM?usp=sharing"&gt;https://colab.research.google.com/drive/1HXhEudDjJsXCvP3tO4G7cAC15OyKW3nM?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What techniques are you using? Anyone else seeing HydE results this good?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Best-Information2493"&gt; /u/Best-Information2493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fq5i6e8q95vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6s89n/tested_9_rag_query_transformation_techniques_hyde/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6s89n/tested_9_rag_query_transformation_techniques_hyde/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T21:22:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6h8jn</id>
    <title>We tested Claude Sonnet 4.5, GPT-5-codex, Qwen3-Coder, GLM and other 25+ models on fresh SWE-Bench like tasks from September 2025</title>
    <updated>2025-10-14T14:36:10+00:00</updated>
    <author>
      <name>/u/Fabulous_Pollution10</name>
      <uri>https://old.reddit.com/user/Fabulous_Pollution10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I’m Ibragim from Nebius.&lt;/p&gt; &lt;p&gt;We’ve updated the &lt;strong&gt;SWE-rebench&lt;/strong&gt; leaderboard with September runs on &lt;strong&gt;49 fresh GitHub PR bug-fix tasks&lt;/strong&gt; (last-month PR issues only). It’s a SWE-bench–style setup: models read real PR issues, run tests, edit code, and must make the suite pass.&lt;/p&gt; &lt;p&gt;Models: &lt;strong&gt;Sonnet-4.5, GPT-5-Codex, Grok Code Fast 1, GLM, Qwen, Kimi&lt;/strong&gt; and others&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Claude Sonnet 4.5 achieved the highest &lt;em&gt;pass@5&lt;/em&gt; (&lt;strong&gt;55.1%&lt;/strong&gt;) and uniquely solving several instances that &lt;strong&gt;no other model&lt;/strong&gt; on the leaderboard managed to resolve: &lt;a href="https://github.com/python-trio/trio/pull/3334"&gt;&lt;strong&gt;python-trio/trio-3334&lt;/strong&gt;&lt;/a&gt;, &lt;a href="https://github.com/cubed-dev/cubed/pull/799"&gt;&lt;strong&gt;cubed-dev/cubed-799&lt;/strong&gt;&lt;/a&gt;, &lt;a href="https://github.com/canopen-python/canopen/pull/613"&gt;&lt;strong&gt;canopen-python/canopen-613&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Coder&lt;/strong&gt; is the &lt;strong&gt;best open-source performer&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;All models on the leaderboard were evaluated using the ChatCompletions API, except for &lt;a href="https://platform.openai.com/docs/models/gpt-5-codex"&gt;&lt;strong&gt;gpt-5-codex&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://platform.openai.com/docs/models/gpt-oss-120b"&gt;&lt;strong&gt;gpt-oss-120b&lt;/strong&gt;&lt;/a&gt;, which are only accessible via the Responses API.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please check the leaderboard, the insights, and write if you want to request some models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Pollution10"&gt; /u/Fabulous_Pollution10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6h8jn/we_tested_claude_sonnet_45_gpt5codex_qwen3coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6h8jn/we_tested_claude_sonnet_45_gpt5codex_qwen3coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T14:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1o75bah</id>
    <title>best local model for article analysis and summarization</title>
    <updated>2025-10-15T08:20:53+00:00</updated>
    <author>
      <name>/u/Luke1144</name>
      <uri>https://old.reddit.com/user/Luke1144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i’m early in my testing journey of determining the best local model for my use case. &lt;/p&gt; &lt;p&gt;in this particular instance i’m trying to find a local model that can ingest article data and output structured responses around key points, impact analysis, and things of that nature.&lt;/p&gt; &lt;p&gt;is there a model that you think would best suit this kind of work?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Luke1144"&gt; /u/Luke1144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75bah/best_local_model_for_article_analysis_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75bah/best_local_model_for_article_analysis_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o75bah/best_local_model_for_article_analysis_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T08:20:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6u5o4</id>
    <title>gpt-oss20/120b AMD Strix Halo vs NVIDIA DGX Spark benchmark</title>
    <updated>2025-10-14T22:40:20+00:00</updated>
    <author>
      <name>/u/Educational_Sun_8813</name>
      <uri>https://old.reddit.com/user/Educational_Sun_8813</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;[EDIT] seems, that their results are way off, and for real performance values check: &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16578"&gt;https://github.com/ggml-org/llama.cpp/discussions/16578&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;NVIDIA DGX Spark (ollama)&lt;/th&gt; &lt;th align="left"&gt;Strix Halo (llama.cpp)&lt;/th&gt; &lt;th align="left"&gt;Winner&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss 20b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Prompt Processing (Prefill)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;2,053.98 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1,332.70 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;NVIDIA DGX Spark&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss 20b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Token Generation (Decode)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;49.69 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;72.87 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Strix Halo&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss 120b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Prompt Processing (Prefill)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;94.67 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;526.15 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Strix Halo&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss 120b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Token Generation (Decode)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;11.66 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;51.39 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Strix Halo&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Sun_8813"&gt; /u/Educational_Sun_8813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6u5o4/gptoss20120b_amd_strix_halo_vs_nvidia_dgx_spark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6u5o4/gptoss20120b_amd_strix_halo_vs_nvidia_dgx_spark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6u5o4/gptoss20120b_amd_strix_halo_vs_nvidia_dgx_spark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T22:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1o78vh7</id>
    <title>2x AMD GPUs: Is Llama.cpp still a good option?</title>
    <updated>2025-10-15T11:52:57+00:00</updated>
    <author>
      <name>/u/ParaboloidalCrest</name>
      <uri>https://old.reddit.com/user/ParaboloidalCrest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For years I've been happy with 1x 7900xtx + llama.cpp-vulkan. But then, I got a second 7900xtx to join the big(ger) boys club, and a B850 AI Top mobo with x8/x8 bifurcation, but now llama.cpp doesn't seem to be a good option anymore:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;According to &lt;a href="https://github.com/ggml-org/llama.cpp/wiki/Feature-matrix"&gt;llama.cpp feature matrix&lt;/a&gt;, tensor parallel (&lt;strong&gt;row split&lt;/strong&gt;) should be supported for ROCm (albeit poorly), but believe it or not, it has been &lt;em&gt;significantly&lt;/em&gt; &lt;em&gt;slower&lt;/em&gt; than &lt;strong&gt;layer&lt;/strong&gt; split from my experience.&lt;/li&gt; &lt;li&gt;ROCm offload-to-cpu behavior is different than Vulkan's. With Vulkan backend, you can stick -ngl 99 and it will shove as much layers into VRAM then the rest in RAM, automatically. With ROCm, -ngl N has to be carefully calculated or it will OOM.&lt;/li&gt; &lt;li&gt;Models that fits comfortably in 48GB VRAM under vulkan, will fail to load with ROCm, it's as though the later consumes more VRAM.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So, with ROCm tensor parallel out of the window and Vulkan continues to be the better backend over all, I can hardly justify using llama.cpp anymore. I think it's time to investigate vLLM after getting over the horrific experience I had with vllm-rocm 1+ year ago.&lt;/p&gt; &lt;p&gt;But I wonder, what inference engines are the the multi-amd-gpu owners use? Am I doing something wrong with llama.cpp-hip?&lt;/p&gt; &lt;p&gt;Edit: Using Arch Linux + ROCm 6.4.4.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParaboloidalCrest"&gt; /u/ParaboloidalCrest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o78vh7/2x_amd_gpus_is_llamacpp_still_a_good_option/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o78vh7/2x_amd_gpus_is_llamacpp_still_a_good_option/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o78vh7/2x_amd_gpus_is_llamacpp_still_a_good_option/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T11:52:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6vb48</id>
    <title>Quick Guide: Running Qwen3-Next-80B-A3B-Instruct-Q4_K_M Locally with FastLLM (Windows)</title>
    <updated>2025-10-14T23:29:26+00:00</updated>
    <author>
      <name>/u/ThetaCursed</name>
      <uri>https://old.reddit.com/user/ThetaCursed</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6vb48/quick_guide_running_qwen3next80ba3binstructq4_k_m/"&gt; &lt;img alt="Quick Guide: Running Qwen3-Next-80B-A3B-Instruct-Q4_K_M Locally with FastLLM (Windows)" src="https://b.thumbs.redditmedia.com/GVz5Ej45qwNz7Z-7_HggplD-Q37GuOg6QSgfhll7Bek.jpg" title="Quick Guide: Running Qwen3-Next-80B-A3B-Instruct-Q4_K_M Locally with FastLLM (Windows)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Nailed it first try with &lt;strong&gt;FastLLM&lt;/strong&gt;! No fuss.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup &amp;amp; Perf&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Required&lt;/strong&gt;: ~6 GB VRAM (for some reason it wasn't using my GPU to its maximum) + 48 GB RAM&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: ~8 t/s&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThetaCursed"&gt; /u/ThetaCursed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o6vb48"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6vb48/quick_guide_running_qwen3next80ba3binstructq4_k_m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6vb48/quick_guide_running_qwen3next80ba3binstructq4_k_m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T23:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1o77eox</id>
    <title>Is anyone else not getting any reasonable answers out of Qwen3-VL-4b MLX?</title>
    <updated>2025-10-15T10:33:11+00:00</updated>
    <author>
      <name>/u/Anuin</name>
      <uri>https://old.reddit.com/user/Anuin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using LM studio and the 4 bit MLX quant, Qwen3-VL-4b barely works at all. I gave it 3 test images of mine and asked it to describe them. Here are the results: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;An image with multiple graphs --&amp;gt; it did not see one of the graphs, mislabeled another, and gave a completely wrong description of what each of the graphs look like. At least it got the axis labels correctly, but everything else was almost random. &lt;/li&gt; &lt;li&gt;A diagram with lots of arrows showing different heat transfer mechanisms --&amp;gt; It got all of the colors correctly, but then completely misread an information bubble (instead of &amp;quot;Ignoring radiation inside&amp;quot; it read &amp;quot;igniter: Radiation/Conduction/Evaporation&amp;quot;) and argued for this being a typo in the original image&lt;/li&gt; &lt;li&gt;A scanned image of a brochure, asking for the highest-priced item on it --&amp;gt; it hallucinated prices, tables, and items before going into an infinite loop telling me the price of one (imaginary) item&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Is anyone else surprised by how unusable this is? I am using the default parameters.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anuin"&gt; /u/Anuin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o77eox/is_anyone_else_not_getting_any_reasonable_answers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o77eox/is_anyone_else_not_getting_any_reasonable_answers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o77eox/is_anyone_else_not_getting_any_reasonable_answers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T10:33:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6ofr9</id>
    <title>Intel Crescent Island GPU: 160GB of LPDDR5X memory</title>
    <updated>2025-10-14T19:01:14+00:00</updated>
    <author>
      <name>/u/On1ineAxeL</name>
      <uri>https://old.reddit.com/user/On1ineAxeL</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;About the GPU:&lt;/strong&gt; The new data center GPU code-named Crescent Island is being designed to be power and cost-optimized for air-cooled enterprise servers and to incorporate large amounts of memory capacity and bandwidth, optimized for inference workflows. &lt;/p&gt; &lt;p&gt;Key features include: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Xe3P microarchitecture with optimized performance-per-watt &lt;/li&gt; &lt;li&gt;160GB of LPDDR5X memory &lt;/li&gt; &lt;li&gt;Support for a broad range of data types, ideal for “tokens-as-a-service” providers and inference use cases &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://videocardz.com/newz/intel-confirms-xe3p-architecture-to-power-new-crescent-island-data-center-gpu-with-160gb-lpddr5x-memory"&gt;https://videocardz.com/newz/intel-confirms-xe3p-architecture-to-power-new-crescent-island-data-center-gpu-with-160gb-lpddr5x-memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://newsroom.intel.com/artificial-intelligence/intel-to-expand-ai-accelerator-portfolio-with-new-gpu"&gt;https://newsroom.intel.com/artificial-intelligence/intel-to-expand-ai-accelerator-portfolio-with-new-gpu&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/On1ineAxeL"&gt; /u/On1ineAxeL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ofr9/intel_crescent_island_gpu_160gb_of_lpddr5x_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ofr9/intel_crescent_island_gpu_160gb_of_lpddr5x_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ofr9/intel_crescent_island_gpu_160gb_of_lpddr5x_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T19:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1o734qe</id>
    <title>Running Qwen3-4B on a 6-Year-Old AMD APU? Yes, and It Works Surprisingly Well!</title>
    <updated>2025-10-15T06:00:03+00:00</updated>
    <author>
      <name>/u/rtsov</name>
      <uri>https://old.reddit.com/user/rtsov</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Running Qwen3-4B on a 6-Year-Old AMD APU? Yes, and It Works Surprisingly Well!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I just successfully ran &lt;strong&gt;unsloth/Qwen3-4B-Instruct-2507-UD-Q4_K_XL.gguf&lt;/strong&gt; on a modest home server with the following specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: AMD Ryzen 5 2400G (8) @ 3.600GHz&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: 16 GB (2 × 8 GiB DDR4-2133, unbuffered, unregistered)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;iGPU&lt;/strong&gt;: Radeon Vega 11 (with 2 GB of VRAM allocated in BIOS)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And the results?&lt;br /&gt; ✅ &lt;strong&gt;Prompt processing&lt;/strong&gt;: &lt;strong&gt;25.9 tokens/sec&lt;/strong&gt; (24 tokens)&lt;br /&gt; ✅ &lt;strong&gt;Text generation&lt;/strong&gt;: &lt;strong&gt;9.76 tokens/sec&lt;/strong&gt; (1,264 tokens)&lt;/p&gt; &lt;p&gt;This is honestly &lt;strong&gt;unexpected&lt;/strong&gt;—but it turns out that the Vega 11 iGPU, often overlooked for AI workloads, can actually handle &lt;strong&gt;lightweight LLM tasks&lt;/strong&gt; like news summarization or simple agent workflows quite effectively—even on hardware from 2018!&lt;/p&gt; &lt;h3&gt;Key Setup Details&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;BIOS&lt;/strong&gt;: 2 GB of system RAM allocated to integrated graphics&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Debian 12 with kernel (6.1.0-40-amd64) parameters&lt;/strong&gt;:&lt;br /&gt; &lt;code&gt;text GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;amdgpu.gttsize=8192&amp;quot; &lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Runtime&lt;/strong&gt;: &lt;code&gt;llama.cpp&lt;/code&gt; with &lt;strong&gt;Vulkan backend&lt;/strong&gt;, running inside a Docker container:&lt;br /&gt; &lt;a href="https://github.com/mostlygeek/llama-swap"&gt;&lt;code&gt;ghcr.io/mostlygeek/llama-swap:vulkan&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Docker Compose&lt;/h3&gt; &lt;p&gt;&lt;code&gt;yaml services: llama-swap: container_name: llama-swap image: ghcr.io/mostlygeek/llama-swap:vulkan devices: - /dev/kfd - /dev/dri group_add: - &amp;quot;video&amp;quot; security_opt: - seccomp=unconfined shm_size: 2g environment: - AMD_VISIBLE_DEVICES=all command: /app/llama-swap -config /app/config.yaml -watch-config &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;llama-swap Config (&lt;code&gt;config.yaml&lt;/code&gt;)&lt;/h3&gt; &lt;p&gt;```yaml macros: &amp;quot;llama-server-default&amp;quot;: | /app/llama-server --port ${PORT} --flash-attn on --no-webui&lt;/p&gt; &lt;p&gt;models: &amp;quot;qwen3-4b-instruct-2507&amp;quot;: name: &amp;quot;qwen3-4b-instruct-2507&amp;quot; cmd: | ${llama-server-default} --model /models/Qwen3-4B-Instruct-2507-UD-Q4_K_XL.gguf --ctx-size 4096 --temp 0.7 --top-k 20 --top-p 0.8 --min-p 0.0 --repeat-penalty 1.05 --cache-type-k q8_0 --cache-type-v q8_0 --jinja ttl: 60 ```&lt;/p&gt; &lt;h3&gt;Takeaway&lt;/h3&gt; &lt;p&gt;You &lt;strong&gt;don’t need a high-end GPU&lt;/strong&gt; to experiment with modern 4B-parameter models. With the right optimizations (Vulkan + llama.cpp + proper iGPU tuning), even aging AMD APUs can serve as capable local LLM endpoints for everyday tasks.&lt;/p&gt; &lt;p&gt;If you’ve got an old Ryzen desktop lying around—give it a try! 🚀&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rtsov"&gt; /u/rtsov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o734qe/running_qwen34b_on_a_6yearold_amd_apu_yes_and_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o734qe/running_qwen34b_on_a_6yearold_amd_apu_yes_and_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o734qe/running_qwen34b_on_a_6yearold_amd_apu_yes_and_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T06:00:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1o78zuc</id>
    <title>New models Qwen3-VL-4b/8b: hands-on notes</title>
    <updated>2025-10-15T11:59:11+00:00</updated>
    <author>
      <name>/u/chenqian615</name>
      <uri>https://old.reddit.com/user/chenqian615</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve got a pile of scanned PDFs, whiteboard photos, and phone receipts. The 4B Instruct fits well. For “read text fast and accurately,” the ramp-up is basically zero; most errors are formatting or extreme noise. Once it can read, I hand off to a text model for summarizing, comparison, and cleanup. This split beats forcing VQA reasoning on a small model.&lt;/p&gt; &lt;p&gt;For OCR + desktop/mobile GUI automation (“recognize → click → run flow”), the 8B Thinking is smooth. As a visual agent, it can spot UI elements and close the loop on tasks. The “visual coding enhancement” can turn screenshots into Draw.io/HTML/CSS/JS skeletons, which saves me scaffolding time.&lt;/p&gt; &lt;p&gt;Long videos: I search meeting recordings by keywords and the returned timestamps are reasonably accurate. The official notes mention structural upgrades for long-horizon/multi-scale (Interleaved‑MRoPE, DeepStack, Text–Timestamp Alignment). Net effect for me: retrieval feels more direct.&lt;/p&gt; &lt;p&gt;If I must nitpick: on complex logic or multi-step visual reasoning, the smaller models sometimes produce “looks right” answers. I don’t fight it, let them handle recognition; route reasoning to a bigger model. That’s more stable in production. I also care about spatial understanding, especially for UI/flowchart localization. From others’ tests, 2D/3D grounding looks solid this gen, finding buttons, arrows, and relative positions is reliable. For long/tall images, the 256K context (extendable to 1M) is friendly for multi-panel reading; cross-page references actually connect.&lt;/p&gt; &lt;p&gt;References: &lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe"&gt;https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chenqian615"&gt; /u/chenqian615 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o78zuc/new_models_qwen3vl4b8b_handson_notes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o78zuc/new_models_qwen3vl4b8b_handson_notes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o78zuc/new_models_qwen3vl4b8b_handson_notes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T11:59:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6pmxt</id>
    <title>Real-time study buddy that sees your screen and talks back</title>
    <updated>2025-10-14T19:46:04+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6pmxt/realtime_study_buddy_that_sees_your_screen_and/"&gt; &lt;img alt="Real-time study buddy that sees your screen and talks back" src="https://external-preview.redd.it/ZXlwZW5pYTNuNHZmMUsYDHUptOq0sYO1cNNkCl_tbC9KzkSKWyT6VTZxxWFL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff99a7daf7c346698fa23df7509fc456a4b3edc3" title="Real-time study buddy that sees your screen and talks back" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a real-time learning assistant that sees your screen, talks, and learns alongside you. All open models (Qwen3-VL, Parakeet, Orpheus) wired together. &lt;/p&gt; &lt;p&gt;I shared a biology site on cell structure to see if it could describe the page, identify the diagram, and answer targeted questions about the mitochondria. &lt;/p&gt; &lt;p&gt;These text and vision models are getting so good. Wiring them together levels them all up. Next step: going to try running it across multiple sites and have it auto-summarize my learnings into a study guide or PDF after.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ctp0k9a3n4vf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6pmxt/realtime_study_buddy_that_sees_your_screen_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6pmxt/realtime_study_buddy_that_sees_your_screen_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T19:46:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6wfpy</id>
    <title>GPT-OSS-20b TAKE THE WHEEL!</title>
    <updated>2025-10-15T00:20:31+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6wfpy/gptoss20b_take_the_wheel/"&gt; &lt;img alt="GPT-OSS-20b TAKE THE WHEEL!" src="https://external-preview.redd.it/EYfYrBdrbHJnRl1EvzhfVjhkBpr2GL8UU-8scxe6WCU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d3d6c023c0a8855a48f130b4207e4980d5b4f49" title="GPT-OSS-20b TAKE THE WHEEL!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In this experiment, I use a single 4090 hooked up to VLLM and a batching GPT-OSS-20b model set up with prefill prompts that explain the current game state (direction/velocity/location of asteroids and the direction/velocity/location of our ship in relation to them), and the LLM is forced to make a control decision to either turn left 25%, turn right 25%, thrust forward, reverse (turn 180 degrees and thrust), or fire. Since I'm only generating one token per generation, I am able to get latency down under 20ms, allowing the AI to make rapid fire decisions (multiple-per-second) and to apply them as control inputs to the spaceship.&lt;/p&gt; &lt;p&gt;As it runs, it's generating a high speed continuous stream of 20ms responses to input thanks to the continuous batching VLLM server (a largely prefix cached prompt with a bit of information updating the current game-state so it can make an input decision in near-realtime). It's able to successfully autopilot the ship around. I also gave it some instructions and a reward (higher points) for flying closer to asteroids and 'hot dogging' which made its chosen flightpath a bit more interesting.&lt;/p&gt; &lt;p&gt;I know it's just a silly experiment, and yes, it would be absolutely trivial to make a simple algorithm that could fly this ship around safely without needing hundreds of watts of screaming GPU, but I thought someone might appreciate making OSS 20b into a little autopilot that knows what's going on around it and controls the ship like it's using a game controller at latency that makes it a fairly competent pilot.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=NY6htCUWFqI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6wfpy/gptoss20b_take_the_wheel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6wfpy/gptoss20b_take_the_wheel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T00:20:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6zg97</id>
    <title>[Update] Qwen3-VL cookbooks coming — recognition, localization, doc parsing, video</title>
    <updated>2025-10-15T02:41:33+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6zg97/update_qwen3vl_cookbooks_coming_recognition/"&gt; &lt;img alt="[Update] Qwen3-VL cookbooks coming — recognition, localization, doc parsing, video" src="https://external-preview.redd.it/sMJBVR2ChB4qgLOpxT2QxURyjiN_Zh_hva5OCRGa7Ls.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=17e9d23803b4ee9beb0893f3fff3d9a55771c058" title="[Update] Qwen3-VL cookbooks coming — recognition, localization, doc parsing, video" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ny9qsbphu6vf1.png?width=955&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89a0a9878dd4c655c5d082543041d9d2c36b22fb"&gt;https://preview.redd.it/ny9qsbphu6vf1.png?width=955&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89a0a9878dd4c655c5d082543041d9d2c36b22fb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;cookbooks&lt;/strong&gt; for a bunch of real-world capabilities—&lt;strong&gt;recognition&lt;/strong&gt;, &lt;strong&gt;localization&lt;/strong&gt;, &lt;strong&gt;document parsing&lt;/strong&gt;, &lt;strong&gt;video understanding&lt;/strong&gt;, &lt;strong&gt;key information extraction&lt;/strong&gt;, and more&lt;/p&gt; &lt;h1&gt;Cookbooks&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL#cookbooks"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are preparing &lt;a href="https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks"&gt;cookbooks&lt;/a&gt; for many capabilities, including recognition, localization, document parsing, video understanding, key information extraction, and more. Welcome to learn more!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Cookbook&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Open&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/omni_recognition.ipynb"&gt;Omni Recognition&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Not only identify animals, plants, people, and scenic spots but also recognize various objects such as cars and merchandise.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/omni_recognition.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/document_parsing.ipynb"&gt;Powerful Document Parsing Capabilities&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;The parsing of documents has reached a higher level, including not only text but also layout position information and our Qwen HTML format.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/document_parsing.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/2d_grounding.ipynb"&gt;Precise Object Grounding Across Formats&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Using relative position coordinates, it supports both boxes and points, allowing for diverse combinations of positioning and labeling tasks.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/2d_grounding.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/ocr.ipynb"&gt;General OCR and Key Information Extraction&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Stronger text recognition capabilities in natural scenes and multiple languages, supporting diverse key information extraction needs.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/ocr.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/video_understanding.ipynb"&gt;Video Understanding&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Better video OCR, long video understanding, and video grounding.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/video_understanding.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/mobile_agent.ipynb"&gt;Mobile Agent&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Locate and think for mobile phone control.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mobile_agent.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/computer_use.ipynb"&gt;Computer-Use Agent&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Locate and think for controlling computers and Web.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/computer_use.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/3d_grounding.ipynb"&gt;3D Grounding&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Provide accurate 3D bounding boxes for both indoor and outdoor objects.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/3d_grounding.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/think_with_images.ipynb"&gt;Thinking with Images&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Utilize image_zoom_in_tool and search_tool to facilitate the model’s precise comprehension of fine-grained visual details within images.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/think_with_images.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/mmcode.ipynb"&gt;MultiModal Coding&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Generate accurate code based on rigorous comprehension of multimodal information.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mmcode.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/long_document_understanding.ipynb"&gt;Long Document Understanding&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Achieve rigorous semantic comprehension of ultra-long documents.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/long_document_understanding.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/spatial_understanding.ipynb"&gt;Spatial Understanding&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;See, understand and reason about the spatial information&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/spatial_understanding.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6zg97/update_qwen3vl_cookbooks_coming_recognition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6zg97/update_qwen3vl_cookbooks_coming_recognition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6zg97/update_qwen3vl_cookbooks_coming_recognition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T02:41:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6kchz</id>
    <title>Qwen3-VL-4B and 8B Instruct &amp; Thinking are here</title>
    <updated>2025-10-14T16:31:24+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can already run Qwen3-VL-4B &amp;amp; 8B locally Day-0 on NPU/GPU/CPU using MLX, GGUF, and NexaML with NexaSDK &lt;strong&gt;(&lt;/strong&gt;&lt;a href="https://github.com/NexaAI/nexa-sdk"&gt;&lt;strong&gt;GitHub&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Check out our GGUF, MLX, and NexaML collection on HuggingFace: &lt;a href="https://huggingface.co/collections/NexaAI/qwen3vl-68d46de18fdc753a7295190a"&gt;https://huggingface.co/collections/NexaAI/qwen3vl-68d46de18fdc753a7295190a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6kchz/qwen3vl4b_and_8b_instruct_thinking_are_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6kchz/qwen3vl4b_and_8b_instruct_thinking_are_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6kchz/qwen3vl4b_and_8b_instruct_thinking_are_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T16:31:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6uw71</id>
    <title>Qwen3-VL 4B vs 8B vs 235B</title>
    <updated>2025-10-14T23:11:20+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6uw71/qwen3vl_4b_vs_8b_vs_235b/"&gt; &lt;img alt="Qwen3-VL 4B vs 8B vs 235B" src="https://preview.redd.it/deo3nizps5vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58885a74f99e694dcdba21d3b954746fac3611ce" title="Qwen3-VL 4B vs 8B vs 235B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/deo3nizps5vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6uw71/qwen3vl_4b_vs_8b_vs_235b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6uw71/qwen3vl_4b_vs_8b_vs_235b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T23:11:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1o70fa7</id>
    <title>Sharing a few image transcriptions from Qwen3-VL-8B-Instruct</title>
    <updated>2025-10-15T03:28:58+00:00</updated>
    <author>
      <name>/u/Hoppss</name>
      <uri>https://old.reddit.com/user/Hoppss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o70fa7/sharing_a_few_image_transcriptions_from/"&gt; &lt;img alt="Sharing a few image transcriptions from Qwen3-VL-8B-Instruct" src="https://b.thumbs.redditmedia.com/Lka0yz7i3ahGr9QVYGE_4FpjZ35ZZZnacxxqUOVbwHI.jpg" title="Sharing a few image transcriptions from Qwen3-VL-8B-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hoppss"&gt; /u/Hoppss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o70fa7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o70fa7/sharing_a_few_image_transcriptions_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o70fa7/sharing_a_few_image_transcriptions_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T03:28:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o751o9</id>
    <title>My first 15 days with GLM-4.6 — honest thoughts after using Opus and Sonnet</title>
    <updated>2025-10-15T08:02:52+00:00</updated>
    <author>
      <name>/u/DecisionLow2640</name>
      <uri>https://old.reddit.com/user/DecisionLow2640</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When I first subscribed and started using &lt;strong&gt;GLM-4.6&lt;/strong&gt; with &lt;strong&gt;KiloCode&lt;/strong&gt;, I was honestly a bit disappointed. I had gotten used to the kind of UI/UX-focused results I was getting from &lt;strong&gt;Opus 4.1&lt;/strong&gt; and &lt;strong&gt;Sonnet&lt;/strong&gt;, and GLM felt different at first.&lt;/p&gt; &lt;p&gt;But after a couple of weeks of real use, I’ve started to really appreciate it. For &lt;strong&gt;pure programming tasks&lt;/strong&gt; — not design-related — GLM-4.6 is actually more &lt;strong&gt;precise, structured, and professional&lt;/strong&gt;. It doesn’t create as much random hard-coded mock data like Sonnet 4.5 often does. Every day it surprises me by solving problems more accurately and providing deeper diagnostics — even when I’m using it inside the &lt;strong&gt;VS Code KiloCode extension&lt;/strong&gt;, not ClaudeCode itself.&lt;/p&gt; &lt;p&gt;I had a case where Sonnet “solved” an issue but the bug was still there. I gave the exact same prompt to GLM-4.6, and it fixed it perfectly using proper &lt;strong&gt;software-engineering logic&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I also love that KiloCode can auto-generate &lt;strong&gt;UML diagrams&lt;/strong&gt;, which honestly reminded me of my early programming days in C and C++.&lt;/p&gt; &lt;p&gt;So yeah — I used to rely on Opus for its relaxed, intuitive style, but now I’m seeing the real &lt;strong&gt;power and precision of GLM-4.6&lt;/strong&gt;. If you have at least a basic understanding of programming, this model is a beast — more detailed, reliable, and consistent than Sonnet in many cases.&lt;/p&gt; &lt;p&gt;That’s my experience so far.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DecisionLow2640"&gt; /u/DecisionLow2640 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o751o9/my_first_15_days_with_glm46_honest_thoughts_after/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o751o9/my_first_15_days_with_glm46_honest_thoughts_after/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o751o9/my_first_15_days_with_glm46_honest_thoughts_after/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T08:02:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6ocfs</id>
    <title>If it's not local, it's not yours.</title>
    <updated>2025-10-14T18:57:54+00:00</updated>
    <author>
      <name>/u/inkberk</name>
      <uri>https://old.reddit.com/user/inkberk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ocfs/if_its_not_local_its_not_yours/"&gt; &lt;img alt="If it's not local, it's not yours." src="https://preview.redd.it/zzv4ey22j4vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebc1f207746b0fa04e90a129bafad3aef0ca9971" title="If it's not local, it's not yours." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkberk"&gt; /u/inkberk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zzv4ey22j4vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ocfs/if_its_not_local_its_not_yours/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ocfs/if_its_not_local_its_not_yours/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T18:57:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o75kkb</id>
    <title>AI has replaced programmers… totally.</title>
    <updated>2025-10-15T08:37:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"&gt; &lt;img alt="AI has replaced programmers… totally." src="https://preview.redd.it/bnnb2fb9m8vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1a55140b6915df726dfa4932943df64e43e7d94" title="AI has replaced programmers… totally." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bnnb2fb9m8vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T08:37:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
