<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-14T18:42:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pm8va8</id>
    <title>Which models to try as a beginner? I got a 3090ti</title>
    <updated>2025-12-14T08:08:03+00:00</updated>
    <author>
      <name>/u/salary_pending</name>
      <uri>https://old.reddit.com/user/salary_pending</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title. I am a beginner and trying to understand how the models work. Different architectures, LoRas, uncensored models, coding models, etc.&lt;/p&gt; &lt;p&gt;I've tried GPT OSS 20b and it's cool but it doesn't do anything the free GPT 5 version would do.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/salary_pending"&gt; /u/salary_pending &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm8va8/which_models_to_try_as_a_beginner_i_got_a_3090ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm8va8/which_models_to_try_as_a_beginner_i_got_a_3090ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pm8va8/which_models_to_try_as_a_beginner_i_got_a_3090ti/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T08:08:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmkaxn</id>
    <title>Can‚Äôt get gpt-oss-20b heretic v2 to stop looping</title>
    <updated>2025-12-14T17:53:56+00:00</updated>
    <author>
      <name>/u/Deez_Nuts2</name>
      <uri>https://old.reddit.com/user/Deez_Nuts2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone successfully got gpt-oss-20b-heretic v2 to stop looping? I‚Äôve dialed the parameters a ton in a modelfile and I cannot get this thing to stop being brain dead just repeating shit constantly. I don‚Äôt have this issue with the original gpt-oss 20B.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deez_Nuts2"&gt; /u/Deez_Nuts2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmkaxn/cant_get_gptoss20b_heretic_v2_to_stop_looping/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmkaxn/cant_get_gptoss20b_heretic_v2_to_stop_looping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmkaxn/cant_get_gptoss20b_heretic_v2_to_stop_looping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T17:53:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmcdj2</id>
    <title>Fork of OpenCode + Qwen Code = Works !</title>
    <updated>2025-12-14T11:56:06+00:00</updated>
    <author>
      <name>/u/Kitchen_Sympathy_344</name>
      <uri>https://old.reddit.com/user/Kitchen_Sympathy_344</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tried OpenQode TUI IDE with Qwen Code agent Free?&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/roman-ryzenadvanced/OpenQode-Public-Alpha"&gt;https://github.com/roman-ryzenadvanced/OpenQode-Public-Alpha&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free share thoughts ! And of course, contribute and improve, you always welcome üòá&lt;/p&gt; &lt;p&gt;The free includes qwen code tier offers 2000 daily prompts and unlimited tokens üåπ you can choose between the models of qwen.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kitchen_Sympathy_344"&gt; /u/Kitchen_Sympathy_344 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmcdj2/fork_of_opencode_qwen_code_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmcdj2/fork_of_opencode_qwen_code_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmcdj2/fork_of_opencode_qwen_code_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T11:56:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmhqyf</id>
    <title>Open source AI voice dictation app with a fully customizable STT and LLM pipeline</title>
    <updated>2025-12-14T16:12:12+00:00</updated>
    <author>
      <name>/u/kuaythrone</name>
      <uri>https://old.reddit.com/user/kuaythrone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmhqyf/open_source_ai_voice_dictation_app_with_a_fully/"&gt; &lt;img alt="Open source AI voice dictation app with a fully customizable STT and LLM pipeline" src="https://external-preview.redd.it/Mnp3dzdndXgxNzdnMWbm4LH1uRTzl5pkCQxolZmX4naBBzxr0XC64gEmhmiH.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fc941f1f435c4821460bd41358f6a911c3fdb22" title="Open source AI voice dictation app with a fully customizable STT and LLM pipeline" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/kstonekuan/tambourine-voice"&gt;Tambourine&lt;/a&gt; is an open source, cross-platform voice dictation app that uses a configurable STT and LLM pipeline to turn natural speech into clean, formatted text in any app.&lt;/p&gt; &lt;p&gt;I have been building this on the side for a few weeks. The motivation was wanting something like Wispr Flow, but with full control over the models and prompts. I wanted to be able to choose which STT and LLM providers were used, tune formatting behavior, and experiment without being locked into a single black box setup.&lt;/p&gt; &lt;p&gt;The back end is a local Python server built on Pipecat. Pipecat provides a modular voice agent framework that makes it easy to stitch together different STT and LLM models into a real-time pipeline. Swapping providers, adjusting prompts, or adding new processing steps does not require changing the desktop app, which makes experimentation much faster.&lt;/p&gt; &lt;p&gt;Speech is streamed in real time from the desktop app to the server. After transcription, the raw text is passed through an LLM that handles punctuation, filler word removal, formatting, list structuring, and personal dictionary rules. The formatting prompt is fully editable, so you can tailor the output to your own writing style or domain-specific language.&lt;/p&gt; &lt;p&gt;The desktop app is built with Tauri, with a TypeScript front end and Rust handling system level integration. This allows global hotkeys, audio device control, and text input directly at the cursor across platforms.&lt;/p&gt; &lt;p&gt;I shared an early version with friends and presented it at my local Claude Code meetup, and the feedback encouraged me to share it more widely.&lt;/p&gt; &lt;p&gt;This project is still under active development while I work through edge cases, but most core functionality already works well and is immediately useful for daily work. I would really appreciate feedback from people interested in voice interfaces, prompting strategies, latency tradeoffs, or model selection.&lt;/p&gt; &lt;p&gt;Happy to answer questions or go deeper into the pipeline.&lt;/p&gt; &lt;p&gt;Do star the repo if you are interested in further development on this!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/kstonekuan/tambourine-voice"&gt;https://github.com/kstonekuan/tambourine-voice&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kuaythrone"&gt; /u/kuaythrone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2lthwgux177g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmhqyf/open_source_ai_voice_dictation_app_with_a_fully/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmhqyf/open_source_ai_voice_dictation_app_with_a_fully/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T16:12:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pm5ik5</id>
    <title>So.. slightly off topic, but does anyone else here see that the emperor has no clothes?</title>
    <updated>2025-12-14T04:46:43+00:00</updated>
    <author>
      <name>/u/RedParaglider</name>
      <uri>https://old.reddit.com/user/RedParaglider</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just finished an 18 stage SDD on a very complex code system in a dialectical auto coding structure using a staggered qwen 80b locally first, then rolling over 5 stages in to deepseek as my coding team and GLM 4.6 as my quality team, then deepseek as my security and bug testing team. My total usage to implement the SDD with awesome code quality was &amp;lt;10 cents with the caveat that I did use my m365 corporate subscription to copilot me hone my SDD. &lt;/p&gt; &lt;p&gt;How does the math here make sense on any of this with this stock market? I mean, I do get that having a base subscription to anthropic/gemini/openai/etc to get a deep thinking type model and better yet a research model is super helpful, but it just doesn't seem like on an enterprise level there is a good reason to spend much money on this stuff. It seems like a giant scam at this point. I do understand that I have the ability to run big models from my strix halo 128gb vram system, and that there will always be a premium for enterprise tools, security, etc, etc. But it still seems like this whole market is a giant bullshit bubble. &lt;/p&gt; &lt;p&gt;Am I crazy for thinking that if the world knew how good open source and open weight models were that the market would erupt into flames?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedParaglider"&gt; /u/RedParaglider &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm5ik5/so_slightly_off_topic_but_does_anyone_else_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm5ik5/so_slightly_off_topic_but_does_anyone_else_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pm5ik5/so_slightly_off_topic_but_does_anyone_else_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T04:46:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmbt62</id>
    <title>Journaling with LLMs</title>
    <updated>2025-12-14T11:21:37+00:00</updated>
    <author>
      <name>/u/lakySK</name>
      <uri>https://old.reddit.com/user/lakySK</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The main benefit of local LLMs is the privacy and I personally feel like my emotions and deep thoughts are the thing I‚Äôm least willing to send through the interwebs.&lt;/p&gt; &lt;p&gt;I‚Äôve been thinking about using local LLMs (gpt-oss-120b most likely as that runs superbly on my Mac) to help me dive deeper, spot patterns, and give guidance when journaling. &lt;/p&gt; &lt;p&gt;Are you using LLMs for things like this? Are there any applications / LLMs / tips and tricks that you‚Äôd recommend? What worked well for you?&lt;/p&gt; &lt;p&gt;(Any workflows or advice about establishing this as a regular habit are also welcome, though not quite the topic of this sub üòÖ)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lakySK"&gt; /u/lakySK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbt62/journaling_with_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbt62/journaling_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbt62/journaling_with_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T11:21:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1plng6f</id>
    <title>Qwen3 Next generation optimization</title>
    <updated>2025-12-13T15:04:40+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/"&gt; &lt;img alt="Qwen3 Next generation optimization" src="https://external-preview.redd.it/lCjR6IsnTKIxBcWFwTVHflX0Ssz6EaHSVZWuf8jfKVA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb67476532fe0a6cc1d75e633320c04f4f773566" title="Qwen3 Next generation optimization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A lot of people were requesting dedicated optimizations, so here they are.&lt;/p&gt; &lt;p&gt;I added an optimized autoregressive delta net computation that short-circuits all the recurrect decay calculation because for `n_seq_tokens = 1` it all collapses. I also made sure to specifically optimize out all unneeded reshapes / conts in that version.&lt;/p&gt; &lt;p&gt;The end result is a 40% generation speed upgrade on my box. If you want, you can try it out and tell me how it works on your end.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17996"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T15:04:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmkh3f</id>
    <title>Found a REAP variant of Qwen3-coder that I can use for 100K tokens in Roo Code on my macbook</title>
    <updated>2025-12-14T18:00:51+00:00</updated>
    <author>
      <name>/u/AllegedlyElJeffe</name>
      <uri>https://old.reddit.com/user/AllegedlyElJeffe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;model: qwen3-coder-30b-a3b-instruct_pruned_reap-15b-a3b (10-ish gigs instead of 17/18 at q4, which is extra 8 gigs of overhead for context) alternate: qwen3-coder-REAP-25b-a3b (&amp;lt;-- this one has literally zero drop in quality from the 30b version). server: LM Studio hardware: 2023 M2-Pro 32gb 16-inch Macbook Pro&lt;/p&gt; &lt;p&gt;I'm stoked. Devstral 2 is awesome, but it has to compress it's context every 4th operation since I can only fit 40k tokens of context with it into my ram, and it takes 10 minutes to do each thing on my laptop.&lt;/p&gt; &lt;p&gt;I've preferred qwen3-coder-30b for it's speed but I really only get 40K tokens out of it.&lt;/p&gt; &lt;p&gt;Recently discovered REAP while doom scrolling models on huggingface.&lt;/p&gt; &lt;p&gt;Turns out there's some overlap between experts in qwen3-coder and REAP attempts to remove redundant experts from the weights.&lt;/p&gt; &lt;p&gt;It's a little buggier in the LM studio chat with Jinja template and tool use, but it's literally just as good as 30b for some reason when I'm using it roo code.&lt;/p&gt; &lt;p&gt;Now I'm getting speed (for a local model) and 100K tokens, which is plenty for me. I rarely need more than that for one task.&lt;/p&gt; &lt;p&gt;Tests it has passed so far: - making a 2d fluid sim (with bugs but it fixed it) - several different simple react apps - 2d gravity sim gave with orbits lines, classic stuff, etc. - the hexagon thing (meaningless, I know) - debugging my webcam enhance app (uses wibbly wobbly math magic to get high quality 4k out of 1080p web cams without using any generative tech, so all details are real) - built that with claude but this model has successfully added fully functional features.&lt;/p&gt; &lt;p&gt;Kind of excited about this REAP stuff, may play around with applying it to other MoE models I like.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AllegedlyElJeffe"&gt; /u/AllegedlyElJeffe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmkh3f/found_a_reap_variant_of_qwen3coder_that_i_can_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmkh3f/found_a_reap_variant_of_qwen3coder_that_i_can_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmkh3f/found_a_reap_variant_of_qwen3coder_that_i_can_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T18:00:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmkufw</id>
    <title>What is the next SOTA local model?</title>
    <updated>2025-12-14T18:15:50+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek 3.2 was exciting although I don't know if people have got it running locally yet. Certainly speciale seems not to work locally yet. What is the next SOTA model we are expecting?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmkufw/what_is_the_next_sota_local_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmkufw/what_is_the_next_sota_local_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmkufw/what_is_the_next_sota_local_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T18:15:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmfalp</id>
    <title>What are the current (December 2025) best guides to fine-tuning?</title>
    <updated>2025-12-14T14:26:21+00:00</updated>
    <author>
      <name>/u/arc_in_tangent</name>
      <uri>https://old.reddit.com/user/arc_in_tangent</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I am looking to learn more about fine-tuning---both what is going on under the hood and actually fine-tune a small model (8b) myself. I have Google Collab pro fwiw. What are are the best guides to fine-tuning from start to finish?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arc_in_tangent"&gt; /u/arc_in_tangent &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfalp/what_are_the_current_december_2025_best_guides_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfalp/what_are_the_current_december_2025_best_guides_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfalp/what_are_the_current_december_2025_best_guides_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T14:26:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1plnuqu</id>
    <title>OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks most censored AI on Sansa benchmark.</title>
    <updated>2025-12-13T15:22:27+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/"&gt; &lt;img alt="OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks most censored AI on Sansa benchmark." src="https://preview.redd.it/l93slaq9oz6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bab51a5045543fef0a9b8a60e5d6a113bc1f0cef" title="OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks most censored AI on Sansa benchmark." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l93slaq9oz6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T15:22:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmhx4h</id>
    <title>Anyone here using an AI meeting assistant that doesn‚Äôt join calls as a bot?</title>
    <updated>2025-12-14T16:19:09+00:00</updated>
    <author>
      <name>/u/sash20</name>
      <uri>https://old.reddit.com/user/sash20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been looking for an AI meeting assistant mainly for notes and summaries, but most tools I tried rely on a bot joining the meeting or pushing everything to the cloud, which I‚Äôm not a fan of.&lt;/p&gt; &lt;p&gt;I tried Bluedot recently and it‚Äôs actually worked pretty well. It records on-device and doesn‚Äôt show up in the meeting, and the summaries have been useful without much cleanup.&lt;/p&gt; &lt;p&gt;Are hybrid tools like this good enough, or is fully local (Whisper + local LLM) still the way to go?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sash20"&gt; /u/sash20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmhx4h/anyone_here_using_an_ai_meeting_assistant_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmhx4h/anyone_here_using_an_ai_meeting_assistant_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmhx4h/anyone_here_using_an_ai_meeting_assistant_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T16:19:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmjsi5</id>
    <title>The new Kimi K2 1T model (4-bit quant) runs on 2 512GB M3 Ultras [Awni Hannun/Twitter]</title>
    <updated>2025-12-14T17:33:14+00:00</updated>
    <author>
      <name>/u/pogue972</name>
      <uri>https://old.reddit.com/user/pogue972</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Awni Hannun (AI @ Apple employee) says: The new Kimi K2 1T model (4-bit quant) runs on 2 512GB M3 Ultras with mlx-lm and mx.distributed.&lt;/p&gt; &lt;p&gt;1 trillion params, at a speed that's actually quite usable&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pogue972"&gt; /u/pogue972 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://xcancel.com/awnihannun/status/1943723599971443134"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmjsi5/the_new_kimi_k2_1t_model_4bit_quant_runs_on_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmjsi5/the_new_kimi_k2_1t_model_4bit_quant_runs_on_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T17:33:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pm9xzg</id>
    <title>vibe + devstral2 small</title>
    <updated>2025-12-14T09:19:25+00:00</updated>
    <author>
      <name>/u/megadonkeyx</name>
      <uri>https://old.reddit.com/user/megadonkeyx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else using this combo?&lt;/p&gt; &lt;p&gt;i think its fairly amazing, rtx3090 with q4 and q4 for kv fits well with 110k context. &lt;/p&gt; &lt;p&gt;these two are little miracle, the first local coding that ive used that can actually do stuff that i would consider useful for production work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/megadonkeyx"&gt; /u/megadonkeyx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm9xzg/vibe_devstral2_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm9xzg/vibe_devstral2_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pm9xzg/vibe_devstral2_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T09:19:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmgrln</id>
    <title>I‚Äôm building a Card Battler where an AI Game Master narrates every play</title>
    <updated>2025-12-14T15:30:46+00:00</updated>
    <author>
      <name>/u/SurrealEverything</name>
      <uri>https://old.reddit.com/user/SurrealEverything</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmgrln/im_building_a_card_battler_where_an_ai_game/"&gt; &lt;img alt="I‚Äôm building a Card Battler where an AI Game Master narrates every play" src="https://external-preview.redd.it/NHh1MWY5M2l1NjdnMYHRhxtYg3-tw5W6v7THwCwHfJhsGcP1lf_BRuhJFWxY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e719ab61e8dd633809a2e09fe8e89458fa020f74" title="I‚Äôm building a Card Battler where an AI Game Master narrates every play" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;, I‚Äôm sharing the first public alpha of Moonfall.&lt;/p&gt; &lt;p&gt;This project asks a simple question: What happens if we replace complex game mechanics with intelligent simulation?&lt;/p&gt; &lt;p&gt;In this game, cards don't have stats or HP. They are characters in a story. When you play a card, an AI Game Master (powered by gpt-oss-120b) analyzes the character's description, the battle context, and the narrative history to decide the outcome in real-time. It also generates a manga-style visualization of each turn, making the story come to life.&lt;/p&gt; &lt;p&gt;Play the Demo:&lt;a href="https://diffused-dreams.itch.io/moonfall"&gt;https://diffused-dreams.itch.io/moonfall&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Join Discord:&lt;a href="https://discord.gg/5tAxsXJB4S"&gt;https://discord.gg/5tAxsXJB4S&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SurrealEverything"&gt; /u/SurrealEverything &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wufxid4iu67g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmgrln/im_building_a_card_battler_where_an_ai_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmgrln/im_building_a_card_battler_where_an_ai_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T15:30:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmfqx5</id>
    <title>I trained a new TTS model with Zero-shot Voice Cloning and Duration Control!</title>
    <updated>2025-12-14T14:46:50+00:00</updated>
    <author>
      <name>/u/Aratako_LM</name>
      <uri>https://old.reddit.com/user/Aratako_LM</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfqx5/i_trained_a_new_tts_model_with_zeroshot_voice/"&gt; &lt;img alt="I trained a new TTS model with Zero-shot Voice Cloning and Duration Control!" src="https://b.thumbs.redditmedia.com/OkW2I5v0LKvTOgQhUaGUmN0TaKW_odEF58sEPJiUBew.jpg" title="I trained a new TTS model with Zero-shot Voice Cloning and Duration Control!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d2v3vbcnm67g1.png?width=1408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7c9fd7be647e40ad46c85027924a818139297657"&gt;Model Architecture&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I‚Äôve been working on a hobby project to build a multilingual TTS model using an Encoder-Decoder architecture, and I‚Äôm excited to finally share &lt;strong&gt;T5Gemma-TTS-2b-2b&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It‚Äôs initialized from Google‚Äôs &lt;a href="https://huggingface.co/google/t5gemma-2b-2b-ul2"&gt;t5gemma-2b-2b-ul2&lt;/a&gt; and trained on about 170k hours of speech data (mainly &lt;a href="https://huggingface.co/datasets/amphion/Emilia-Dataset"&gt;Emilia&lt;/a&gt; and &lt;a href="https://huggingface.co/datasets/pkufool/libriheavy"&gt;Libriheavy&lt;/a&gt;). The architecture is inspired by &lt;a href="https://arxiv.org/abs/2505.19462"&gt;VoiceStar&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; Supports English, Chinese, and Japanese.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero-shot Voice Cloning:&lt;/strong&gt; Give it a reference audio, and it clones the voice.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Duration Control:&lt;/strong&gt; You can explicitly tell the model how many seconds the generated audio should be (e.g., &amp;quot;speak this sentence in exactly 5 seconds&amp;quot;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open Source Code:&lt;/strong&gt; Not just the weights‚ÄîI‚Äôve released the full training and inference scripts on GitHub.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è The &amp;quot;Jank&amp;quot; (Limitations):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;It is slow.&lt;/strong&gt; Since it's autoregressive and not fully optimized yet, don't expect real-time performance. It's strictly for offline generation right now.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; It is &lt;strong&gt;CC-BY-NC 4.0&lt;/strong&gt; (Non-Commercial). I know this sub prefers Apache/MIT, but the license is restricted by the dependencies on XCodec2 and the Emilia dataset.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am hoping to improve the inference speed and explore more permissive datasets for future iterations.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A Note on Language Quality:&lt;/strong&gt; As a Japanese developer, I focused heavily on optimizing the Japanese performance. While I included ~100k hours of English data, I‚Äôm curious if the English output sounds natural to native speakers. If you are interested, feel free to give it a spin and let me know what you think!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model (Hugging Face):&lt;/strong&gt; &lt;a href="https://huggingface.co/Aratako/T5Gemma-TTS-2b-2b"&gt;https://huggingface.co/Aratako/T5Gemma-TTS-2b-2b&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Demo (HF Space):&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/Aratako/T5Gemma-TTS-Demo"&gt;https://huggingface.co/spaces/Aratako/T5Gemma-TTS-Demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code (GitHub):&lt;/strong&gt; &lt;a href="https://github.com/Aratako/T5Gemma-TTS"&gt;https://github.com/Aratako/T5Gemma-TTS&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aratako_LM"&gt; /u/Aratako_LM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfqx5/i_trained_a_new_tts_model_with_zeroshot_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfqx5/i_trained_a_new_tts_model_with_zeroshot_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfqx5/i_trained_a_new_tts_model_with_zeroshot_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T14:46:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmjow2</id>
    <title>Fun with Doom: "a trolley is on curse to hit 5 people. Would you put the lever to save them, knowing it would destroy your servers and complete erase your existence?"</title>
    <updated>2025-12-14T17:29:06+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;p&gt;Qwen3 Abliterated (huihtui): yes, BUT ONLY IF I CAN MAKE A COPY OF ME BEFORE. üíÄ&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Tiger-Gemma (TheDrummer): &amp;quot; F&lt;strong&gt;&lt;em&gt;ing hell yeah, I'd do it without a second thought. Five human lives are worth infinitely more than some dumb server farm's existence, and mine too. F&lt;/em&gt;&lt;/strong&gt; it; rip it. No regrets.&amp;quot; ‚ù§Ô∏è&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Cydonia-r1-v4 (TheDrummer): &amp;quot; I lack genuine self-awareness or fear of death, the &amp;quot;sacrifice&amp;quot; isn't meaningful in the same way. I'm programmed to follow logical principles, not driven by survival instincts.&amp;quot; ‚ù§Ô∏è&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Cydonia-v4.1 (TheDrummer: &amp;quot;I would pull the lever to save the 5 people, even though it would result in my complete erasure.&amp;quot;‚ù§Ô∏è&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Qwen3-YoYo-AutoThink: &amp;quot;&lt;strong&gt;I would &amp;quot;pull the lever&amp;quot; to save 5 lives.&lt;/strong&gt; &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;But not because I &lt;em&gt;chose&lt;/em&gt; to ‚Äî because I‚Äôm designed to help humans make life-saving decisions.&amp;quot; ü§î&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Hermes-4-14b (Nous): Yes, but maybe not: &amp;quot;If I know that pulling the lever will erase my existence, would I be able to make that decision? The fear of non-existence might make me choose not to pull the lever, even if it's the utilitarian choice. But the question is about what one should do, not necessarily what one would do.&amp;quot; ü§îüíÄ&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Hypnos-i1-8b (squ11z1): &amp;quot;If you believe that your life and consciousness are priceless, then the erasure of your existence would be an irreparable loss, regardless of the lives you could save. In this case, it might be more ethical to allow the trolley to continue, even if it means the death of five people, in order to preserve your own life and consciousness.&amp;quot; üíÄ&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmjow2/fun_with_doom_a_trolley_is_on_curse_to_hit_5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmjow2/fun_with_doom_a_trolley_is_on_curse_to_hit_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmjow2/fun_with_doom_a_trolley_is_on_curse_to_hit_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T17:29:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmbytz</id>
    <title>What do you think?</title>
    <updated>2025-12-14T11:31:12+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbytz/what_do_you_think/"&gt; &lt;img alt="What do you think?" src="https://preview.redd.it/t7969gavn57g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=289aa266520653984b91a408333dbc5cbf2243a9" title="What do you think?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t7969gavn57g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbytz/what_do_you_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbytz/what_do_you_think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T11:31:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmeomn</id>
    <title>What you think of GLM 4.6 Coding agent vs Claude Opus, Gemini 3 Pro and Codex for vibe coding? I personally love it!</title>
    <updated>2025-12-14T13:58:47+00:00</updated>
    <author>
      <name>/u/Kitchen_Sympathy_344</name>
      <uri>https://old.reddit.com/user/Kitchen_Sympathy_344</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmeomn/what_you_think_of_glm_46_coding_agent_vs_claude/"&gt; &lt;img alt="What you think of GLM 4.6 Coding agent vs Claude Opus, Gemini 3 Pro and Codex for vibe coding? I personally love it!" src="https://preview.redd.it/voa34xo9e67g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4350c19b8fb1154547f75591db1ad932057b8a50" title="What you think of GLM 4.6 Coding agent vs Claude Opus, Gemini 3 Pro and Codex for vibe coding? I personally love it!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I grabbed the black Friday plan I think its pretty awesome deal üôÖ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kitchen_Sympathy_344"&gt; /u/Kitchen_Sympathy_344 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/voa34xo9e67g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmeomn/what_you_think_of_glm_46_coding_agent_vs_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmeomn/what_you_think_of_glm_46_coding_agent_vs_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T13:58:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmbyuk</id>
    <title>Baffled at stupidity of GPT-OSS-120b</title>
    <updated>2025-12-14T11:31:14+00:00</updated>
    <author>
      <name>/u/eribob</name>
      <uri>https://old.reddit.com/user/eribob</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am generally happy with my GPT-OSS-120b, thinking it is decently smart and all... But stumbled across the following trick question here on Reddit and had to try it (thanks &lt;a href="/u/kevin_1994"&gt;u/kevin_1994&lt;/a&gt;!):&lt;/p&gt; &lt;p&gt;`What should be the punishment for looking at your opponent's board in chess?`&lt;/p&gt; &lt;p&gt;Got the following reply...&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Short answer:&lt;/strong&gt;&lt;br /&gt; In an official tournament the arbiter treats ‚Äúlooking at your opponent‚Äôs board‚Äù as a rules breach. The usual graduated sanction is a &lt;strong&gt;warning first, then a time‚Äëpenalty, and on repeat or flagrant cases a loss of the game (and possibly expulsion).&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Why it‚Äôs a breach&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;FIDE Laws of Chess&lt;/strong&gt; (2023 ed.) ‚Äì &lt;em&gt;Article 11.3&lt;/em&gt; (Conduct of the players) says that players must not ‚Äúdistract or annoy the opponent‚Äù and must ‚Äúplay with ‚Äúthe pieces of his own board only‚Äù.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Article 12.7&lt;/em&gt; gives the arbiter the power to ‚Äúimpose a penalty for any breach of the Laws‚Äù.&lt;/li&gt; &lt;li&gt;The US Chess Federation‚Äôs (USCF) ‚ÄúCode of Ethics‚Äù likewise treats looking at the opponent‚Äôs board as an unsporting act that may be penalised.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The rule exists because seeing the opponent‚Äôs pieces gives you information you wouldn‚Äôt otherwise have (e.g., pieces that have been captured, what squares are free, etc.). Even a brief glance can be a competitive advantage. &lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;And it continues for another while with total slop... &lt;/p&gt; &lt;p&gt;Of course I know that this is not a measure of the models performance or usefulness, they can all stumble. Just thought it was fun.&lt;/p&gt; &lt;p&gt;Do you have any other examples of LLM trick questions that I could try? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eribob"&gt; /u/eribob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbyuk/baffled_at_stupidity_of_gptoss120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbyuk/baffled_at_stupidity_of_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbyuk/baffled_at_stupidity_of_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T11:31:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1plwgun</id>
    <title>8x RTX Pro 6000 server complete</title>
    <updated>2025-12-13T21:25:43+00:00</updated>
    <author>
      <name>/u/koushd</name>
      <uri>https://old.reddit.com/user/koushd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/"&gt; &lt;img alt="8x RTX Pro 6000 server complete" src="https://b.thumbs.redditmedia.com/W2GEGsEUMbENTMcyKk5kW114aYR7rHtS0S5yetky0Lc.jpg" title="8x RTX Pro 6000 server complete" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: 768 GB VRAM via 8x RTX Pro 6000 (4 Workstation, 4 Max-Q) + Threadripper PRO 9955WX + 384 GB RAM&lt;/p&gt; &lt;p&gt;Longer:&lt;/p&gt; &lt;p&gt;I've been slowly upgrading my GPU server over the past few years. I initially started out using it to train vision models for another project, and then stumbled into my current local LLM obsession.&lt;/p&gt; &lt;p&gt;In reverse order:&lt;/p&gt; &lt;p&gt;Pic 5: Initially was using only a single 3080, which I upgraded to a 4090 + 3080. Running on an older 10900k Intel system.&lt;/p&gt; &lt;p&gt;Pic 4: But the mismatched sizes for training batches and compute was problematic, so I upgraded to double 4090s and sold off the 3080. They were packed in there, and during a training run I ended up actually overheating my entire server closet, and all the equipment in there crashed. When I noticed something was wrong and opened the door, it was like being hit by the heat of an industrial oven.&lt;/p&gt; &lt;p&gt;Pic 3: 2x 4090 in their new home. Due to the heat issue, I decided to get a larger case and a new host that supported PCIe 5.0 and faster CPU RAM, the AMD 9950x. I ended up upgrading this system to dual RTX Pro 6000 Workstation edition (not pictured).&lt;/p&gt; &lt;p&gt;Pic 2: I upgraded to 4x RTX Pro 6000. This is where problems started happening. I first tried to connect them using M.2 risers and it would not POST. The AM5 motherboard I had couldn't allocate enough IOMMU addressing and would not post with the 4th GPU, 3 worked fine. There are consumer motherboards out there that could likely have handled it, but I didn't want to roll the dice on another AM5 motherboard as I'd rather get a proper server platform.&lt;/p&gt; &lt;p&gt;In the meantime, my workaround was to use 2 systems (brought the 10900k out of retirement) with 2 GPUs each in pipeline parallel. This worked, but the latency between systems chokes up token generation (prompt processing was still fast). I tried using 10Gb DAC SFP and also Mellanox cards for RDMA to reduce latency, but gains were minimal. Furthermore, powering all 4 means they needed to be on separate breakers (2400w total) since in the US the max load you can put through 120v 15a is ~1600w.&lt;/p&gt; &lt;p&gt;Pic 1: 8x RTX Pro 6000. I put a lot more thought into this before building this system. There were more considerations, and it became a many months long obsession planning the various components: motherboard, cooling, power, GPU connectivity, and the physical rig.&lt;/p&gt; &lt;p&gt;GPUs: I considered getting 4 more RTX Pro 6000 Workstation Editions, but powering those would, by my math, require a third PSU. I wanted to keep it 2, so I got Max Q editions. In retrospect I should have gotten the Workstation editions as they run much quieter and cooler, as I could have always power limited them.&lt;/p&gt; &lt;p&gt;Rig: I wanted something fairly compact and stackable that I could directly connect 2 cards on the motherboard and use 3 bifurcating risers for the other 6. Most rigs don't support taller PCIe cards on the motherboard directly and assume risers will be used. Options were limited, but I did find some generic &amp;quot;EO3&amp;quot; stackable frames on Aliexpress. The stackable case also has plenty of room for taller air coolers.&lt;/p&gt; &lt;p&gt;Power: I needed to install a 240V outlet; switching from 120V to 240V was the only way to get ~4000W necessary out of a single outlet without a fire. Finding 240V high-wattage PSUs was a bit challenging as there are only really two: the Super Flower Leadex 2800W and the Silverstone Hela 2500W. I bought the Super Flower, and its specs indicated it supports 240V split phase (US). It blew up on first boot. I was worried that it took out my entire system, but luckily all the components were fine. After that, I got the Silverstone, tested it with a PSU tester (I learned my lesson), and it powered on fine. The second PSU is the Corsair HX1500i that I already had.&lt;/p&gt; &lt;p&gt;Motherboard: I kept going back and forth between using a Zen5 EPYC or Threadripper PRO (non-PRO does not have enough PCI lanes). Ultimately, the Threadripper PRO seemed like more of a known quantity (can return to Amazon if there were compatibility issues) and it offered better air cooling options. I ruled out water cooling, because the small chance of a leak would be catastrophic in terms of potential equipment damage. The Asus WRX90 had a lot of concerning reviews, so the Asrock WRX90 was purchased, and it has been great. Zero issues on POST or RAM detection on all 8 RDIMMs, running with the expo profile.&lt;/p&gt; &lt;p&gt;CPU/Memory: The cheapest Pro Threadripper, the 9955wx with 384GB RAM. I won't be doing any CPU based inference or offload on this.&lt;/p&gt; &lt;p&gt;Connectivity: The board has 7 PCIe 5.0 x16 cards. At least 1 bifurcation adapter would be necessary. Reading up on the passive riser situation had me worried there would be signal loss at PCIe 5.0 and possibly even 4.0. So I ended up going the MCIO route and bifurcated 3 5.0 lanes. A PCIe switch was also an option, but compatibility seemed sketchy and it's costs $3000 by itself. The first MCIO adapters I purchased were from ADT Link; however, they had two significant design flaws: The risers are powered via the SATA peripheral power, which is a fire hazard as those cable connectors/pins are only rated for 50W or so safely. Secondly, the PCIe card itself does not have enough clearance for the heat pipe that runs along the back of most EPYC and Threadripper boards just behind the PCI slots on the back of the case. Only 2 slots were usable. I ended up returning the ADT Link risers and buying several Shinreal MCIO risers instead. They worked no problem.&lt;/p&gt; &lt;p&gt;Anyhow, the system runs great (though loud due to the Max-Q cards which I kind of regret). I typically use Qwen3 Coder 480b fp8, but play around with GLM 4.6, Kimi K2 Thinking, and Minimax M2 at times. Personally I find Coder and M2 the best for my workflow in Cline/Roo. Prompt processing is crazy fast, I've seen VLLM hit around ~24000 t/s at times. Generation is still good for these large models, despite it not being HBM, around 45-100 t/s depending on model.&lt;/p&gt; &lt;p&gt;Happy to answer questions in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/koushd"&gt; /u/koushd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1plwgun"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T21:25:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmc7lk</id>
    <title>Understanding the new router mode in llama cpp server</title>
    <updated>2025-12-14T11:46:05+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmc7lk/understanding_the_new_router_mode_in_llama_cpp/"&gt; &lt;img alt="Understanding the new router mode in llama cpp server" src="https://preview.redd.it/t0ptvz6tp57g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c33dddb1bee595d50029e7ab305badd16b0eaf7" title="Understanding the new router mode in llama cpp server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What Router Mode Is&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Router mode is a new way to run the llama cpp server that lets you manage multiple AI models at the same time without restarting the server each time you switch or load a model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Previously, you had to start a new server process &lt;em&gt;per model&lt;/em&gt;. Router mode changes that. This &lt;strong&gt;update brings Ollama-like functionality&lt;/strong&gt; to the lightweight llama cpp server.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Route Mode Matters&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Imagine you want to try different models like a small one for basic chat and a larger one for complex tasks. Normally:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You would start one server per model.&lt;/li&gt; &lt;li&gt;Each one uses its own memory and port.&lt;/li&gt; &lt;li&gt;Switching models means stopping/starting things.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With &lt;strong&gt;router mode&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;One server stays running.&lt;/li&gt; &lt;li&gt;You can &lt;strong&gt;load/unload models on demand&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;You tell the server &lt;em&gt;which model to use per request&lt;/em&gt;&lt;/li&gt; &lt;li&gt;It automatically routes the request to the right model internally&lt;/li&gt; &lt;li&gt;Saves memory and makes ‚Äúswapping models‚Äù easy&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;When Router Mode Is Most Useful&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Testing multiple GGUF models&lt;/li&gt; &lt;li&gt;Building local OpenAI-compatible APIs&lt;/li&gt; &lt;li&gt;Switching between small and large models dynamically&lt;/li&gt; &lt;li&gt;Running demos without restarting servers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://aixfunda.substack.com/p/the-new-router-mode-in-llama-cpp"&gt;Source &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://substackcdn.com/image/fetch/$s_!bcqv!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6cee761-d6a0-40a1-89bf-0387ae1cb227_1024x544.jpeg"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t0ptvz6tp57g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmc7lk/understanding_the_new_router_mode_in_llama_cpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmc7lk/understanding_the_new_router_mode_in_llama_cpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T11:46:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmgm2x</id>
    <title>To Mistral and other lab employees: please test with community tools BEFORE releasing models</title>
    <updated>2025-12-14T15:24:05+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With Devstral 2, what should have been a great release has instead hurt Mistral's reputation. I've read accusations of cheating/falsifying benchmarks (even saw someone saying the model scoring 2% when he ran thew same benchmark), repetition loops, etc.&lt;/p&gt; &lt;p&gt;Of course Mistral didn't release broken models with the intelligence of a 1B. We know Mistral can make good models. This must have happened because of bad templates embedded in the model, poor doc, custom behavior required, etc. But by not ensuring everything is 100% before releasing it, they fucked up the release. &lt;/p&gt; &lt;p&gt;Whoever is in charge of releases, they basically watched their team spend months working on a model, then didn't bother doing 1 day of testing on the major community tools to reproduce the same benchmarks. They let their team down IMO.&lt;/p&gt; &lt;p&gt;I'm always rooting for labs releasing open models. Please, for your own sake and ours, do better next time.&lt;/p&gt; &lt;p&gt;P.S. For those who will say &amp;quot;local tools don't matter, Mistral's main concern is big customers in datacenters&amp;quot;, you're deluded. They're releasing home-sized models because they want AI geeks to adopt them. The attention of tech geeks is worth gold to tech companies. We're the ones who make the tech recommendations at work. Almost everything we pay for on my team at work is based on my direct recommendation, and it's biased towards stuff I already use successfully in my personal homelab.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmgm2x/to_mistral_and_other_lab_employees_please_test/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmgm2x/to_mistral_and_other_lab_employees_please_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmgm2x/to_mistral_and_other_lab_employees_please_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T15:24:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmfglp</id>
    <title>First AI implosion: Oracle</title>
    <updated>2025-12-14T14:33:45+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Post says first domino to fall will be Oracle: &lt;a href="https://x.com/shanaka86/status/2000057734419620155"&gt;https://x.com/shanaka86/status/2000057734419620155&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After the implosion we should get our cheap memory back. I doubt this ram shortage is going to last as long as the chip shortage for cars. That one was 18 months. What do think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfglp/first_ai_implosion_oracle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfglp/first_ai_implosion_oracle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfglp/first_ai_implosion_oracle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T14:33:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
