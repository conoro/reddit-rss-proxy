<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-22T18:25:44+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nnefs0</id>
    <title>GLM-4.5V model for local computer use</title>
    <updated>2025-09-22T05:49:19+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnefs0/glm45v_model_for_local_computer_use/"&gt; &lt;img alt="GLM-4.5V model for local computer use" src="https://external-preview.redd.it/MTdjemJhbjlubnFmMbWADQNBSkImjVESNjfi_q43l9ostHKNGAFX_QJdfnS0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e71ff3c4f0395f4af1a821212361417ff7c59ec" title="GLM-4.5V model for local computer use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On OSWorld-V, it scores 35.8% - beating UI-TARS-1.5, matching Claude-3.7-Sonnet-20250219, and setting SOTA for fully open-source computer-use models.&lt;/p&gt; &lt;p&gt;Run it with Cua either: Locally via Hugging Face Remotely via OpenRouter&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs + examples: &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/s7ecb9y9nnqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnefs0/glm45v_model_for_local_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnefs0/glm45v_model_for_local_computer_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T05:49:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnfwmo</id>
    <title>Moving from Cursor to Qwen-code</title>
    <updated>2025-09-22T07:20:39+00:00</updated>
    <author>
      <name>/u/Honest-Debate-6863</name>
      <uri>https://old.reddit.com/user/Honest-Debate-6863</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Never been faster &amp;amp; happier, I basically live on terminal. tmux 8 panes +qwen on each with llamacpp qwen3 30b server. Definitely recommend.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Honest-Debate-6863"&gt; /u/Honest-Debate-6863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnfwmo/moving_from_cursor_to_qwencode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnfwmo/moving_from_cursor_to_qwencode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnfwmo/moving_from_cursor_to_qwencode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T07:20:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nntlsz</id>
    <title>MediaTek Dimensity 9500 almost twice as fast on transformer inference</title>
    <updated>2025-09-22T17:57:23+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nntlsz/mediatek_dimensity_9500_almost_twice_as_fast_on/"&gt; &lt;img alt="MediaTek Dimensity 9500 almost twice as fast on transformer inference" src="https://b.thumbs.redditmedia.com/Lu3m4w5q00QXkBipEQtobnn9gYFs_E7Q9gTJwj-xnkI.jpg" title="MediaTek Dimensity 9500 almost twice as fast on transformer inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ai-benchmark.com/ranking_processors.html"&gt;https://ai-benchmark.com/ranking_processors.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nntlsz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nntlsz/mediatek_dimensity_9500_almost_twice_as_fast_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nntlsz/mediatek_dimensity_9500_almost_twice_as_fast_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T17:57:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnmfne</id>
    <title>SWE-Bench Pro released, targeting dataset contamination</title>
    <updated>2025-09-22T13:25:49+00:00</updated>
    <author>
      <name>/u/Pristine-Woodpecker</name>
      <uri>https://old.reddit.com/user/Pristine-Woodpecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmfne/swebench_pro_released_targeting_dataset/"&gt; &lt;img alt="SWE-Bench Pro released, targeting dataset contamination" src="https://external-preview.redd.it/Qw0D15PigrzJYps6l8gVjMdI1NYqWX8uTNTfIGJFrdQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c93337ea63bf45258fb1db695bf6f13eef2f9d43" title="SWE-Bench Pro released, targeting dataset contamination" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pristine-Woodpecker"&gt; /u/Pristine-Woodpecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://scale.com/research/swe_bench_pro"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmfne/swebench_pro_released_targeting_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmfne/swebench_pro_released_targeting_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T13:25:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnu9b2</id>
    <title>üî• Qwen-Image-Edit-2509 IS LIVE ‚Äî and it‚Äôs a GAME CHANGER. üî•</title>
    <updated>2025-09-22T18:20:53+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnu9b2/qwenimageedit2509_is_live_and_its_a_game_changer/"&gt; &lt;img alt="üî• Qwen-Image-Edit-2509 IS LIVE ‚Äî and it‚Äôs a GAME CHANGER. üî•" src="https://preview.redd.it/taitk409drqf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=216fec124343c71e7a56513855309026dafdb0d2" title="üî• Qwen-Image-Edit-2509 IS LIVE ‚Äî and it‚Äôs a GAME CHANGER. üî•" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üî• Qwen-Image-Edit-2509 IS LIVE ‚Äî and it‚Äôs a GAME CHANGER. üî•&lt;/p&gt; &lt;p&gt;We didn‚Äôt just upgrade it. We rebuilt it for creators, designers, and AI tinkerers who demand pixel-perfect control.&lt;/p&gt; &lt;p&gt;‚úÖ Multi-Image Editing? YES.&lt;/p&gt; &lt;p&gt;Drag in ‚Äúperson + product‚Äù or ‚Äúperson + scene‚Äù ‚Äî it blends them like magic. No more Franken-images.&lt;/p&gt; &lt;p&gt;‚úÖ Single-Image? Rock-Solid Consistency.&lt;/p&gt; &lt;p&gt;‚Ä¢ üë§ Faces stay you ‚Äî through poses, filters, and wild styles.&lt;/p&gt; &lt;p&gt;‚Ä¢ üõçÔ∏è Products keep their identity ‚Äî ideal for ads &amp;amp; posters.&lt;/p&gt; &lt;p&gt;‚Ä¢ ‚úçÔ∏è Text? Edit everything: content, font, color, even material texture.&lt;/p&gt; &lt;p&gt;‚úÖ ControlNet Built-In.&lt;/p&gt; &lt;p&gt;Depth. Edges. Keypoints. Plug &amp;amp; play precision.&lt;/p&gt; &lt;p&gt;‚ú® Blog: &lt;a href="https://qwen.ai/blog?id=7a90090115ee193ce6a7f619522771dd9696dd93&amp;amp;from=research.latest-advancements-list"&gt;https://qwen.ai/blog?id=7a90090115ee193ce6a7f619522771dd9696dd93&amp;amp;from=research.latest-advancements-list&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üí¨ QwenChat: &lt;a href="https://chat.qwen.ai/?inputFeature=image_edit"&gt;https://chat.qwen.ai/?inputFeature=image_edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üêô GitHub: &lt;a href="https://github.com/QwenLM/Qwen-Image"&gt;https://github.com/QwenLM/Qwen-Image&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ó HuggingFace: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2509"&gt;https://huggingface.co/Qwen/Qwen-Image-Edit-2509&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üß© ModelScope: &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2509"&gt;https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2509&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/taitk409drqf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnu9b2/qwenimageedit2509_is_live_and_its_a_game_changer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnu9b2/qwenimageedit2509_is_live_and_its_a_game_changer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T18:20:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nncyvv</id>
    <title>baidu releases Qianfan-VL 70B/8B/3B</title>
    <updated>2025-09-22T04:23:58+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/baidu/Qianfan-VL-8B"&gt;https://huggingface.co/baidu/Qianfan-VL-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/baidu/Qianfan-VL-70B"&gt;https://huggingface.co/baidu/Qianfan-VL-70B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/baidu/Qianfan-VL-3B"&gt;https://huggingface.co/baidu/Qianfan-VL-3B&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Model Description&lt;/h1&gt; &lt;p&gt;Qianfan-VL is a series of general-purpose multimodal large language models enhanced for enterprise-level multimodal applications. The models offer deep optimization for high-frequency scenarios in industrial deployment while maintaining strong general capabilities.&lt;/p&gt; &lt;h1&gt;Model Variants&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Parameters&lt;/th&gt; &lt;th align="left"&gt;Context Length&lt;/th&gt; &lt;th align="left"&gt;CoT Support&lt;/th&gt; &lt;th align="left"&gt;Best For&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qianfan-VL-3B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;32k&lt;/td&gt; &lt;td align="left"&gt;‚ùå&lt;/td&gt; &lt;td align="left"&gt;Edge deployment, real-time OCR&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qianfan-VL-8B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;32k&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;td align="left"&gt;Server-side general scenarios, fine-tuning&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qianfan-VL-70B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;70B&lt;/td&gt; &lt;td align="left"&gt;32k&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;td align="left"&gt;Complex reasoning, data synthesis&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Architecture&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Language Model&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Qianfan-VL-3B: Based on Qwen2.5-3B&lt;/li&gt; &lt;li&gt;Qianfan-VL-8B/70B: Based on Llama 3.1 architecture&lt;/li&gt; &lt;li&gt;Enhanced with 3T multilingual corpus&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vision Encoder&lt;/strong&gt;: InternViT-based, supports dynamic patching up to 4K resolution&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cross-modal Fusion&lt;/strong&gt;: MLP adapter for efficient vision-language bridging&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Capabilities&lt;/h1&gt; &lt;h1&gt;üîç OCR &amp;amp; Document Understanding&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Full-Scenario OCR&lt;/strong&gt;: Handwriting, formulas, natural scenes, cards/documents&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Document Intelligence&lt;/strong&gt;: Layout analysis, table parsing, chart understanding, document Q&amp;amp;A&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High Precision&lt;/strong&gt;: Industry-leading performance on OCR benchmarks&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üßÆ Chain-of-Thought Reasoning (8B &amp;amp; 70B)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Complex chart analysis and reasoning&lt;/li&gt; &lt;li&gt;Mathematical problem-solving with step-by-step derivation&lt;/li&gt; &lt;li&gt;Visual reasoning and logical inference&lt;/li&gt; &lt;li&gt;Statistical computation and trend prediction&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nncyvv/baidu_releases_qianfanvl_70b8b3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nncyvv/baidu_releases_qianfanvl_70b8b3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nncyvv/baidu_releases_qianfanvl_70b8b3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T04:23:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnlylf</id>
    <title>Benchmarked 2x 5090 with vLLM and Gemma-3-12b unquantized</title>
    <updated>2025-09-22T13:06:15+00:00</updated>
    <author>
      <name>/u/somealusta</name>
      <uri>https://old.reddit.com/user/somealusta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tested a dual 5090 setup with vLLM and Gemma-3-12b unquantized inference performance.&lt;br /&gt; Goal was to see how much more performance and tokens/s a second GPU gives when the inference engine is better than Ollama or LM-studio.&lt;/p&gt; &lt;p&gt;Test setup&lt;/p&gt; &lt;p&gt;Epyc siena 24core 64GB RAM, 1500W NZXT PSU&lt;/p&gt; &lt;p&gt;2x5090 in pcie 5.0 16X slots Both power limited to 400W&lt;/p&gt; &lt;p&gt;Benchmark command:&lt;/p&gt; &lt;p&gt;&lt;code&gt;python3 benchmark_serving.py --backend vllm --base-url &amp;quot;http://127.0.0.1:8000&amp;quot; --endpoint='/v1/completions' --model google/gemma-3-12b-it --served-model-name vllm/gemma-3 --dataset-name random --num-prompts 200 --max-concurrency 64 --request-rate inf --random-input-len 64 --random-output-len 128&lt;/code&gt;&lt;/p&gt; &lt;p&gt;(I changed the max concurrency and num-prompts values in the below tests.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;requests&lt;/th&gt; &lt;th align="left"&gt;2x 5090 (total tokens/s)&lt;/th&gt; &lt;th align="left"&gt;1x 5090&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1 requests concurrency&lt;/td&gt; &lt;td align="left"&gt;117.82&lt;/td&gt; &lt;td align="left"&gt;84.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;64 requests concurrency&lt;/td&gt; &lt;td align="left"&gt;3749.04&lt;/td&gt; &lt;td align="left"&gt;2331.57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;124 requests concurrency&lt;/td&gt; &lt;td align="left"&gt;4428.10&lt;/td&gt; &lt;td align="left"&gt;2542.67&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;---- tensor-parallel = 2&lt;/strong&gt; (2 cards)&lt;/p&gt; &lt;p&gt;--num-prompts 10 --max-concurrency 1&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 10 Maximum request concurrency: 1 Benchmark duration (s): 13.89 Total input tokens: 630 Total generated tokens: 1006 Request throughput (req/s): 0.72 Output token throughput (tok/s): 72.45 Total Token throughput (tok/s): 117.82 ---------------Time to First Token---------------- Mean TTFT (ms): 20.89 Median TTFT (ms): 20.85 P99 TTFT (ms): 21.31 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 13.77 Median TPOT (ms): 13.72 P99 TPOT (ms): 14.12 ---------------Inter-token Latency---------------- Mean ITL (ms): 13.73 Median ITL (ms): 13.67 P99 ITL (ms): 14.55 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;--num-prompts 200 --max-concurrency 64&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 200 Maximum request concurrency: 64 Benchmark duration (s): 9.32 Total input tokens: 12600 Total generated tokens: 22340 Request throughput (req/s): 21.46 Output token throughput (tok/s): 2397.07 Total Token throughput (tok/s): 3749.04 ---------------Time to First Token---------------- Mean TTFT (ms): 191.26 Median TTFT (ms): 212.97 P99 TTFT (ms): 341.05 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 24.86 Median TPOT (ms): 22.93 P99 TPOT (ms): 53.04 ---------------Inter-token Latency---------------- Mean ITL (ms): 23.04 Median ITL (ms): 22.09 P99 ITL (ms): 47.91 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;--num-prompts 300 --max-concurrency 124&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 300 Maximum request concurrency: 124 Benchmark duration (s): 11.89 Total input tokens: 18898 Total generated tokens: 33750 Request throughput (req/s): 25.23 Output token throughput (tok/s): 2838.63 Total Token throughput (tok/s): 4428.10 ---------------Time to First Token---------------- Mean TTFT (ms): 263.10 Median TTFT (ms): 228.77 P99 TTFT (ms): 554.57 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 37.19 Median TPOT (ms): 34.55 P99 TPOT (ms): 158.76 ---------------Inter-token Latency---------------- Mean ITL (ms): 34.44 Median ITL (ms): 33.23 P99 ITL (ms): 51.66 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;---- tensor-parallel = 1&lt;/strong&gt; (1 card)&lt;/p&gt; &lt;p&gt;--num-prompts 10 --max-concurrency 1&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 10 Maximum request concurrency: 1 Benchmark duration (s): 19.45 Total input tokens: 630 Total generated tokens: 1006 Request throughput (req/s): 0.51 Output token throughput (tok/s): 51.71 Total Token throughput (tok/s): 84.10 ---------------Time to First Token---------------- Mean TTFT (ms): 35.58 Median TTFT (ms): 36.64 P99 TTFT (ms): 37.14 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 19.14 Median TPOT (ms): 19.16 P99 TPOT (ms): 19.23 ---------------Inter-token Latency---------------- Mean ITL (ms): 19.17 Median ITL (ms): 19.17 P99 ITL (ms): 19.46 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;--num-prompts 200 --max-concurrency 64&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 200 Maximum request concurrency: 64 Benchmark duration (s): 15.00 Total input tokens: 12600 Total generated tokens: 22366 Request throughput (req/s): 13.34 Output token throughput (tok/s): 1491.39 Total Token throughput (tok/s): 2331.57 ---------------Time to First Token---------------- Mean TTFT (ms): 332.08 Median TTFT (ms): 330.50 P99 TTFT (ms): 549.43 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 40.50 Median TPOT (ms): 36.66 P99 TPOT (ms): 139.68 ---------------Inter-token Latency---------------- Mean ITL (ms): 36.96 Median ITL (ms): 35.48 P99 ITL (ms): 64.42 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;--num-prompts 300 --max-concurrency 124&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 300 Maximum request concurrency: 124 Benchmark duration (s): 20.74 Total input tokens: 18898 Total generated tokens: 33842 Request throughput (req/s): 14.46 Output token throughput (tok/s): 1631.57 Total Token throughput (tok/s): 2542.67 ---------------Time to First Token---------------- Mean TTFT (ms): 1398.51 Median TTFT (ms): 1012.84 P99 TTFT (ms): 4301.30 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 57.72 Median TPOT (ms): 49.13 P99 TPOT (ms): 251.44 ---------------Inter-token Latency---------------- Mean ITL (ms): 52.97 Median ITL (ms): 35.83 P99 ITL (ms): 256.72 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Why unquantized model:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In a parallel requests environment, unquantized models can often be faster than quantized models, even though quantization reduces the model size. This counter-intuitive behavior is due to several key factors that affect how GPUs process these requests. 1. Dequantization Overhead, 2.Memory Access Patterns, 3. The Shift from Memory-Bound to Compute-Bound&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Why &amp;quot;only&amp;quot; 12B model. Its unquantized and takes 24GB of VRAM. So it fits into 1GPU also and the benchmark was possible to take. 27B unquantized Gemma3 takes about 50GB of VRAM.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/somealusta"&gt; /u/somealusta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnlylf/benchmarked_2x_5090_with_vllm_and_gemma312b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnlylf/benchmarked_2x_5090_with_vllm_and_gemma312b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnlylf/benchmarked_2x_5090_with_vllm_and_gemma312b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T13:06:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnrftm</id>
    <title>Qwen releases API (only) of Qwen3-TTS-Flash</title>
    <updated>2025-09-22T16:36:41+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnrftm/qwen_releases_api_only_of_qwen3ttsflash/"&gt; &lt;img alt="Qwen releases API (only) of Qwen3-TTS-Flash" src="https://preview.redd.it/ojxbaozruqqf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d47853f3c43e75164c79a3f5228821619abcd5c2" title="Qwen releases API (only) of Qwen3-TTS-Flash" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üéôÔ∏è Meet Qwen3-TTS-Flash ‚Äî the new text-to-speech model that‚Äôs redefining voice AI!&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-TTS-Demo"&gt;https://huggingface.co/spaces/Qwen/Qwen3-TTS-Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://qwen.ai/blog?id=b4264e11fb80b5e37350790121baf0a0f10daf82&amp;amp;from=research.latest-advancements-list"&gt;https://qwen.ai/blog?id=b4264e11fb80b5e37350790121baf0a0f10daf82&amp;amp;from=research.latest-advancements-list&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video: &lt;a href="https://youtu.be/MC6s4TLwX0A"&gt;https://youtu.be/MC6s4TLwX0A&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚úÖ Best-in-class Chinese &amp;amp; English stability&lt;/p&gt; &lt;p&gt;üåç SOTA multilingual WER for CN, EN, IT, FR&lt;/p&gt; &lt;p&gt;üé≠ 17 expressive voices √ó 10 languages&lt;/p&gt; &lt;p&gt;üó£Ô∏è Supports 9+ Chinese dialects: Cantonese, Hokkien, Sichuanese &amp;amp; more&lt;/p&gt; &lt;p&gt;‚ö° Ultra-fast: First packet in just 97ms&lt;/p&gt; &lt;p&gt;ü§ñ Auto tone adaptation + robust text handling&lt;/p&gt; &lt;p&gt;Perfect for apps, games, IVR, content ‚Äî anywhere you need natural, human-like speech.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ojxbaozruqqf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnrftm/qwen_releases_api_only_of_qwen3ttsflash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnrftm/qwen_releases_api_only_of_qwen3ttsflash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T16:36:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnj83s</id>
    <title>Magistral Small 2509 - Jinja Template Modification (Based on Unsloth's) - No thinking by default - straight quick answers in Mistral Small 3.2 style and quality~, need thinking? simple activation with "/think" command anywhere in the system prompt.</title>
    <updated>2025-09-22T10:52:12+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnj83s/magistral_small_2509_jinja_template_modification/"&gt; &lt;img alt="Magistral Small 2509 - Jinja Template Modification (Based on Unsloth's) - No thinking by default - straight quick answers in Mistral Small 3.2 style and quality~, need thinking? simple activation with &amp;quot;/think&amp;quot; command anywhere in the system prompt." src="https://b.thumbs.redditmedia.com/reer8ADbaCCmPSvEFLw22uSl2QwyNkai_YlopFuEYLc.jpg" title="Magistral Small 2509 - Jinja Template Modification (Based on Unsloth's) - No thinking by default - straight quick answers in Mistral Small 3.2 style and quality~, need thinking? simple activation with &amp;quot;/think&amp;quot; command anywhere in the system prompt." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nnj83s"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnj83s/magistral_small_2509_jinja_template_modification/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnj83s/magistral_small_2509_jinja_template_modification/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T10:52:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nncssq</id>
    <title>Qwen3-Omni Promotional Video</title>
    <updated>2025-09-22T04:14:46+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=RRlAen2kIUU"&gt;https://www.youtube.com/watch?v=RRlAen2kIUU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen dropped a promotional video for Qwen3-Omni, looks like the weights are just around the corner!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nncssq/qwen3omni_promotional_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nncssq/qwen3omni_promotional_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nncssq/qwen3omni_promotional_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T04:14:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnmjt0</id>
    <title>DeepSeek-V3.1-Terminus</title>
    <updated>2025-09-22T13:30:32+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmjt0/deepseekv31terminus/"&gt; &lt;img alt="DeepSeek-V3.1-Terminus" src="https://preview.redd.it/ih6z5vljxpqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15bc33beee1daca428fa6b9ed4d7c64f51d360b6" title="DeepSeek-V3.1-Terminus" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ih6z5vljxpqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmjt0/deepseekv31terminus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmjt0/deepseekv31terminus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T13:30:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnb8sq</id>
    <title>I'll show you mine, if you show me yours: Local AI tech stack September 2025</title>
    <updated>2025-09-22T02:53:16+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnb8sq/ill_show_you_mine_if_you_show_me_yours_local_ai/"&gt; &lt;img alt="I'll show you mine, if you show me yours: Local AI tech stack September 2025" src="https://preview.redd.it/rq2ple7trmqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ba6a2e65b89f81d77c39c353119c9e596157a9b" title="I'll show you mine, if you show me yours: Local AI tech stack September 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rq2ple7trmqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnb8sq/ill_show_you_mine_if_you_show_me_yours_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnb8sq/ill_show_you_mine_if_you_show_me_yours_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T02:53:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnmbkz</id>
    <title>deepseek-ai/DeepSeek-V3.1-Terminus ¬∑ Hugging Face</title>
    <updated>2025-09-22T13:21:04+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmbkz/deepseekaideepseekv31terminus_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.1-Terminus ¬∑ Hugging Face" src="https://external-preview.redd.it/lElvQgBFLJ25eAKrUxM2O_G26a4lbsVKHgY4b4Y5JIc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1eefab6fb85acd29ad52251c7b36d0ae23c1ac6a" title="deepseek-ai/DeepSeek-V3.1-Terminus ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmbkz/deepseekaideepseekv31terminus_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmbkz/deepseekaideepseekv31terminus_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T13:21:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnhlx5</id>
    <title>Official FP8-quantizion of Qwen3-Next-80B-A3B</title>
    <updated>2025-09-22T09:14:06+00:00</updated>
    <author>
      <name>/u/touhidul002</name>
      <uri>https://old.reddit.com/user/touhidul002</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking-FP8"&gt;https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking-FP8&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/touhidul002"&gt; /u/touhidul002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnhlx5/official_fp8quantizion_of_qwen3next80ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnhlx5/official_fp8quantizion_of_qwen3next80ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnhlx5/official_fp8quantizion_of_qwen3next80ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T09:14:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nntdok</id>
    <title>Qwen3-Omni looks insane</title>
    <updated>2025-09-22T17:48:53+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nntdok/qwen3omni_looks_insane/"&gt; &lt;img alt="Qwen3-Omni looks insane" src="https://external-preview.redd.it/B4ZZCzuzrlsMnQHNhsoc21qTthMSpFr8qrrtucUS_RU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7250574f82c5b2852f214b634dbe23e3e38e029b" title="Qwen3-Omni looks insane" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Truly a multimodal model that can handle inputs in audio, video, text, and images. Outputs include text and audio with near real-time responses. &lt;/p&gt; &lt;p&gt;# of use cases this can support is wild: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time conversational agents: low-latency speech-to-speech assistants for customer support, tutoring, or accessibility.&lt;/li&gt; &lt;li&gt;Multilingual: cross-language text chat and voice translation across 100+ languages.&lt;/li&gt; &lt;li&gt;Audio and video understanding: transcription, summarization, and captioning of meetings, lectures, or media (up to 30 mins of audio, short video clips).&lt;/li&gt; &lt;li&gt;Content accessibility: generating captions and descriptions for audio and video content.&lt;/li&gt; &lt;li&gt;Interactive multimodal apps: applications that need to handle text, images, audio, and video seamlessly.&lt;/li&gt; &lt;li&gt;Tool-integrated agents: assistants that can call APIs or external services (e.g., booking systems, productivity apps).&lt;/li&gt; &lt;li&gt;Personalized AI experiences: customizable personas or characters for therapy, entertainment, education, or branded interactions.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Wonder how OpenAI and other closed models are feeling right about now .... &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=_zdOrPju4_g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nntdok/qwen3omni_looks_insane/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nntdok/qwen3omni_looks_insane/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T17:48:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnsz3z</id>
    <title>Qwen3-Omni</title>
    <updated>2025-09-22T17:33:58+00:00</updated>
    <author>
      <name>/u/JawGBoi</name>
      <uri>https://old.reddit.com/user/JawGBoi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnsz3z/qwen3omni/"&gt; &lt;img alt="Qwen3-Omni" src="https://external-preview.redd.it/ktbPqK316US_xKrngLchajXyzydUl2qGgd_RzyQVGrw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0142c63f372aedb3019c9bac509f1f0cc9e58e3e" title="Qwen3-Omni" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Captioner"&gt;https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Captioner&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Instruct&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JawGBoi"&gt; /u/JawGBoi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnsz3z/qwen3omni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnsz3z/qwen3omni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T17:33:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nntr5a</id>
    <title>üöÄ Qwen released Qwen3-Omni!</title>
    <updated>2025-09-22T18:02:33+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nntr5a/qwen_released_qwen3omni/"&gt; &lt;img alt="üöÄ Qwen released Qwen3-Omni!" src="https://b.thumbs.redditmedia.com/G9yWrU9TAoh7X4gkG7uSbhBJ3c763zZhRkFx6pNI0wo.jpg" title="üöÄ Qwen released Qwen3-Omni!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ Introducing Qwen3-Omni ‚Äî the first natively end-to-end omni-modal AI unifying text, image, audio &amp;amp; video in one model ‚Äî no modality trade-offs!&lt;/p&gt; &lt;p&gt;üèÜ SOTA on 22/36 audio &amp;amp; AV benchmarks&lt;/p&gt; &lt;p&gt;üåç 119L text / 19L speech in / 10L speech out&lt;/p&gt; &lt;p&gt;‚ö° 211ms latency | üéß 30-min audio understanding&lt;/p&gt; &lt;p&gt;üé® Fully customizable via system prompts&lt;/p&gt; &lt;p&gt;üîó Built-in tool calling&lt;/p&gt; &lt;p&gt;üé§ Open-source Captioner model (low-hallucination!)&lt;/p&gt; &lt;p&gt;üåü What‚Äôs Open-Sourced? &lt;/p&gt; &lt;p&gt;We‚Äôve open-sourced Qwen3-Omni-30B-A3B-Instruct, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner, to empower developers to explore a variety of applications from instruction-following to creative tasks.&lt;/p&gt; &lt;p&gt;Try it now üëá&lt;/p&gt; &lt;p&gt;üí¨ Qwen Chat: &lt;a href="https://chat.qwen.ai/?models=qwen3-omni-flash"&gt;https://chat.qwen.ai/?models=qwen3-omni-flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üíª GitHub: &lt;a href="https://github.com/QwenLM/Qwen3-Omni"&gt;https://github.com/QwenLM/Qwen3-Omni&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ó HF Models: &lt;a href="https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe"&gt;https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ñ MS Models: &lt;a href="https://modelscope.cn/collections/Qwen3-Omni-867aef131e7d4f"&gt;https://modelscope.cn/collections/Qwen3-Omni-867aef131e7d4f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üé¨ Demo: &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-Omni-Demo"&gt;https://huggingface.co/spaces/Qwen/Qwen3-Omni-Demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nntr5a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nntr5a/qwen_released_qwen3omni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nntr5a/qwen_released_qwen3omni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T18:02:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnki20</id>
    <title>The DeepSeek online model has been upgraded</title>
    <updated>2025-09-22T11:59:55+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnki20/the_deepseek_online_model_has_been_upgraded/"&gt; &lt;img alt="The DeepSeek online model has been upgraded" src="https://external-preview.redd.it/fj84M-Z0L_zDI8VjgLPR-vGFwXVTTqVZFoa_h5offPs.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf8b3f8dce31098b2bdb03126d4f6c603326511a" title="The DeepSeek online model has been upgraded" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/wk15my1bhpqf1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0e007554233eee03a311b2657d714af9da13f353"&gt;https://preview.redd.it/wk15my1bhpqf1.jpg?width=1440&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0e007554233eee03a311b2657d714af9da13f353&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The DeepSeek online model has been upgraded. The current version number is DeepSeek-V3.1-Terminus. Everyone is welcome to test it and report any issues~&lt;/p&gt; &lt;p&gt;edit:&lt;/p&gt; &lt;p&gt;&lt;a href="https://api-docs.deepseek.com/updates#deepseek-v31-terminus"&gt;https://api-docs.deepseek.com/updates#deepseek-v31-terminus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This update maintains the model's original capabilities while addressing issues reported by users, including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Language consistency: Reduced occurrences of Chinese-English mixing and occasional abnormal characters;&lt;/li&gt; &lt;li&gt;Agent capabilities: Further optimized the performance of the Code Agent and Search Agent.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnki20/the_deepseek_online_model_has_been_upgraded/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnki20/the_deepseek_online_model_has_been_upgraded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnki20/the_deepseek_online_model_has_been_upgraded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T11:59:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnsx1a</id>
    <title>Qwen3-Omni has been released</title>
    <updated>2025-09-22T17:31:45+00:00</updated>
    <author>
      <name>/u/eu-thanos</name>
      <uri>https://old.reddit.com/user/eu-thanos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnsx1a/qwen3omni_has_been_released/"&gt; &lt;img alt="Qwen3-Omni has been released" src="https://external-preview.redd.it/ktbPqK316US_xKrngLchajXyzydUl2qGgd_RzyQVGrw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0142c63f372aedb3019c9bac509f1f0cc9e58e3e" title="Qwen3-Omni has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eu-thanos"&gt; /u/eu-thanos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnsx1a/qwen3omni_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnsx1a/qwen3omni_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T17:31:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnj67v</id>
    <title>too many qwens</title>
    <updated>2025-09-22T10:49:14+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnj67v/too_many_qwens/"&gt; &lt;img alt="too many qwens" src="https://preview.redd.it/z6ehmb4r4pqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac3c6bfbc9c3ca1495fb8ed54680114fd1e007ff" title="too many qwens" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z6ehmb4r4pqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnj67v/too_many_qwens/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnj67v/too_many_qwens/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T10:49:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnt539</id>
    <title>Qwen-Image-Edit-2509 has been released</title>
    <updated>2025-09-22T17:40:02+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2509"&gt;https://huggingface.co/Qwen/Qwen-Image-Edit-2509&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This September, we are pleased to introduce Qwen-Image-Edit-2509, the monthly iteration of Qwen-Image-Edit. To experience the latest model, please visit &lt;a href="https://qwen.ai"&gt;Qwen Chat&lt;/a&gt; and select the &amp;quot;Image Editing&amp;quot; feature. Compared with Qwen-Image-Edit released in August, the main improvements of Qwen-Image-Edit-2509 include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-image Editing Support&lt;/strong&gt;: For multi-image inputs, Qwen-Image-Edit-2509 builds upon the Qwen-Image-Edit architecture and is further trained via image concatenation to enable multi-image editing. It supports various combinations such as &amp;quot;person + person,&amp;quot; &amp;quot;person + product,&amp;quot; and &amp;quot;person + scene.&amp;quot; Optimal performance is currently achieved with 1 to 3 input images.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced Single-image Consistency&lt;/strong&gt;: For single-image inputs, Qwen-Image-Edit-2509 significantly improves editing consistency, specifically in the following areas: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Improved Person Editing Consistency&lt;/strong&gt;: Better preservation of facial identity, supporting various portrait styles and pose transformations;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Improved Product Editing Consistency&lt;/strong&gt;: Better preservation of product identity, supporting product poster editingÔºõ&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Improved Text Editing Consistency&lt;/strong&gt;: In addition to modifying text content, it also supports editing text fonts, colors, and materialsÔºõ&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Native Support for ControlNet&lt;/strong&gt;: Including depth maps, edge maps, keypoint maps, and more.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnt539/qwenimageedit2509_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnt539/qwenimageedit2509_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnt539/qwenimageedit2509_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T17:40:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnr7ys</id>
    <title>The Qwen3-TTS demo is now out!</title>
    <updated>2025-09-22T16:28:50+00:00</updated>
    <author>
      <name>/u/nonredditaccount</name>
      <uri>https://old.reddit.com/user/nonredditaccount</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing Qwen3-TTS! Our new text-to-speech model is designed to be multi-timbre, multi-lingual, and multi-dialect for natural, expressive audio. It delivers strong performance in English &amp;amp; Chinese, and we're excited for you to hear it for yourself!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nonredditaccount"&gt; /u/nonredditaccount &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Ali_TongyiLab/status/1970160304748437933"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnr7ys/the_qwen3tts_demo_is_now_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnr7ys/the_qwen3tts_demo_is_now_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T16:28:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnt1bw</id>
    <title>3 Qwen3-Omni models have been released</title>
    <updated>2025-09-22T17:36:10+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Captioner"&gt;https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Captioner&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen3-Omni is the natively end-to-end multilingual omni-modal foundation models. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several architectural upgrades to improve performance and efficiency. Key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;State-of-the-art across modalities&lt;/strong&gt;: Early text-first pretraining and mixed multimodal training provide native multimodal support. While achieving strong audio and audio-video results, unimodal text and image performance does not regress. Reaches SOTA on 22 of 36 audio/video benchmarks and open-source SOTA on 32 of 36; ASR, audio understanding, and voice conversation performance is comparable to Gemini 2.5 Pro.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual&lt;/strong&gt;: Supports 119 text languages, 19 speech input languages, and 10 speech output languages. &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Speech Input&lt;/strong&gt;: English, Chinese, Korean, Japanese, German, Russian, Italian, French, Spanish, Portuguese, Malay, Dutch, Indonesian, Turkish, Vietnamese, Cantonese, Arabic, Urdu.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speech Output&lt;/strong&gt;: English, Chinese, French, German, Russian, Italian, Spanish, Portuguese, Japanese, Korean.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Novel Architecture&lt;/strong&gt;: MoE-based Thinker‚ÄìTalker design with AuT pretraining for strong general representations, plus a multi-codebook design that drives latency to a minimum.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time Audio/Video Interaction&lt;/strong&gt;: Low-latency streaming with natural turn-taking and immediate text or speech responses.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible Control&lt;/strong&gt;: Customize behavior via system prompts for fine-grained control and easy adaptation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Detailed Audio Captioner&lt;/strong&gt;: Qwen3-Omni-30B-A3B-Captioner is now open source: a general-purpose, highly detailed, low-hallucination audio captioning model that fills a critical gap in the open-source community.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Below is the description of all Qwen3-Omni models. Please select and download the model that fits your needs.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/td&gt; &lt;td align="left"&gt;The Instruct model of Qwen3-Omni-30B-A3B, containing both thinker and talker, supporting audio, video, and text input, with audio and text output. For more information, please read the &lt;a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/assets/Qwen3_Omni.pdf"&gt;Qwen3-Omni Technical Report&lt;/a&gt;.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/td&gt; &lt;td align="left"&gt;The Thinking model of Qwen3-Omni-30B-A3B, containing the thinker component, equipped with chain-of-thought reasoning, supporting audio, video, and text input, with text output. For more information, please read the &lt;a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/assets/Qwen3_Omni.pdf"&gt;Qwen3-Omni Technical Report&lt;/a&gt;.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Omni-30B-A3B-Captioner&lt;/td&gt; &lt;td align="left"&gt;A downstream audio fine-grained caption model fine-tuned from Qwen3-Omni-30B-A3B-Instruct, which produces detailed, low-hallucination captions for arbitrary audio inputs. It contains the thinker, supporting audio input and text output. For more information, you can refer to the model's &lt;a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb"&gt;cookbook&lt;/a&gt;.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnt1bw/3_qwen3omni_models_have_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnt1bw/3_qwen3omni_models_have_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnt1bw/3_qwen3omni_models_have_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T17:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnmhai</id>
    <title>üöÄ DeepSeek released DeepSeek-V3.1-Terminus</title>
    <updated>2025-09-22T13:27:35+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmhai/deepseek_released_deepseekv31terminus/"&gt; &lt;img alt="üöÄ DeepSeek released DeepSeek-V3.1-Terminus" src="https://preview.redd.it/729mf2l1xpqf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e9ce1e335c2a491a3d422d6f4d70b33dbf6a25f" title="üöÄ DeepSeek released DeepSeek-V3.1-Terminus" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ DeepSeek-V3.1 ‚Üí DeepSeek-V3.1-Terminus The latest update builds on V3.1‚Äôs strengths while addressing key user feedback.&lt;/p&gt; &lt;p&gt;‚ú® What‚Äôs improved?&lt;/p&gt; &lt;p&gt;üåê Language consistency: fewer CN/EN mix-ups &amp;amp; no more random chars.&lt;/p&gt; &lt;p&gt;ü§ñ Agent upgrades: stronger Code Agent &amp;amp; Search Agent performance.&lt;/p&gt; &lt;p&gt;üìä DeepSeek-V3.1-Terminus delivers more stable &amp;amp; reliable outputs across benchmarks compared to the previous version.&lt;/p&gt; &lt;p&gt;üëâ Available now on: App / Web / API üîó Open-source weights here: &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks to everyone for your feedback. It drives us to keep improving and refining the experience! üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/729mf2l1xpqf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmhai/deepseek_released_deepseekv31terminus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmhai/deepseek_released_deepseekv31terminus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T13:27:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnnws0</id>
    <title>Qwen üòÅ</title>
    <updated>2025-09-22T14:25:11+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnnws0/qwen/"&gt; &lt;img alt="Qwen üòÅ" src="https://preview.redd.it/milakcbb7qqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7af57edddeef91bdc75a874fb95e8ac60d2746ae" title="Qwen üòÅ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/milakcbb7qqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnnws0/qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnnws0/qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T14:25:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1njjz7j</id>
    <title>Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)</title>
    <updated>2025-09-17T17:44:17+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt; &lt;img alt="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" src="https://preview.redd.it/4xt9enbairpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cc1ac16a9a2b96134934fcb2f81a9f3d4916b31" title="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4xt9enbairpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T17:44:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkft9l</id>
    <title>AMA with the LM Studio team</title>
    <updated>2025-09-18T18:12:24+00:00</updated>
    <author>
      <name>/u/yags-lms</name>
      <uri>https://old.reddit.com/user/yags-lms</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We're excited for this AMA. Thank you for having us here today. We got a full house from the LM Studio team: &lt;/p&gt; &lt;p&gt;- Yags &lt;a href="https://t.co/ERfA4NrR96"&gt;https://reddit.com/user/yags-lms/&lt;/a&gt; (founder)&lt;br /&gt; - Neil &lt;a href="https://t.co/KyiHVfv0QG"&gt;https://reddit.com/user/neilmehta24/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Will &lt;a href="https://t.co/IjAZJL2JMK"&gt;https://reddit.com/user/will-lms/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Matt &lt;a href="https://t.co/6MNkItPYnI"&gt;https://reddit.com/user/matt-lms/&lt;/a&gt; (LLM engines, runtime, and APIs)&lt;br /&gt; - Ryan &lt;a href="https://t.co/0snuNUPizo"&gt;https://reddit.com/user/ryan-lms/&lt;/a&gt; (Core system and APIs)&lt;br /&gt; - Rugved &lt;a href="https://t.co/xGtYHsJZI3"&gt;https://reddit.com/user/rugved_lms/&lt;/a&gt; (CLI and SDKs)&lt;br /&gt; - Alex &lt;a href="https://t.co/wtT2IFf0z6"&gt;https://reddit.com/user/alex-lms/&lt;/a&gt; (App)&lt;br /&gt; - Julian &lt;a href="https://www.reddit.com/user/julian-lms/"&gt;https://www.reddit.com/user/julian-lms/&lt;/a&gt; (Ops) &lt;/p&gt; &lt;p&gt;Excited to chat about: the latest local models, UX for local models, steering local models effectively, LM Studio SDK and APIs, how we support multiple LLM engines (llama.cpp, MLX, and more), privacy philosophy, why local AI matters, our open source projects (mlx-engine, lms, lmstudio-js, lmstudio-python, venvstacks), why ggerganov and Awni are the GOATs, where is TheBloke, and more. &lt;/p&gt; &lt;p&gt;Would love to hear about people's setup, which models you use, use cases that really work, how you got into local AI, what needs to improve in LM Studio and the ecosystem as a whole, how you use LM Studio, and anything in between!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Everyone: it was awesome to see your questions here today and share replies! Thanks a lot for the welcoming AMA. We will continue to monitor this post for more questions over the next couple of days, but for now we're signing off to continue building üî®&lt;/p&gt; &lt;p&gt;We have several marquee features we've been working on for a loong time coming out later this month that we hope you'll love and find lots of value in. And don't worry, UI for n cpu moe is on the way too :)&lt;/p&gt; &lt;p&gt;Special shoutout and thanks to ggerganov, Awni Hannun, TheBloke, Hugging Face, and all the rest of the open source AI community!&lt;/p&gt; &lt;p&gt;Thank you and see you around! - Team LM Studio üëæ&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yags-lms"&gt; /u/yags-lms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T18:12:24+00:00</published>
  </entry>
</feed>
