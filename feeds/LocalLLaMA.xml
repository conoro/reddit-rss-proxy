<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-05T07:07:01+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n8hq8i</id>
    <title>Flavors of Moonshine: Tiny Monolingual ASR Models for Edge Devices (Preprint + Open Weights)</title>
    <updated>2025-09-04T18:13:50+00:00</updated>
    <author>
      <name>/u/keveman</name>
      <uri>https://old.reddit.com/user/keveman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We open-sourced &lt;strong&gt;6 monolingual ASR models (27M params)&lt;/strong&gt; for Arabic, Ukrainian, Japanese, Korean, Chinese &amp;amp; Vietnamese.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;As small as Whisper Tiny, but rivals Whisper Medium (28√ó larger)&lt;/li&gt; &lt;li&gt;48% lower error than Whisper Tiny&lt;/li&gt; &lt;li&gt;5‚Äì15√ó faster, CPU/edge-device friendly&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Preprint: &lt;a href="http://arxiv.org/abs/2509.02523"&gt;http://arxiv.org/abs/2509.02523&lt;/a&gt;&lt;br /&gt; Models on HuggingFace üëá&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ar: &lt;a href="https://huggingface.co/UsefulSensors/moonshine-tiny-ar"&gt;https://huggingface.co/UsefulSensors/moonshine-tiny-ar&lt;/a&gt;&lt;/li&gt; &lt;li&gt;uk: &lt;a href="https://huggingface.co/UsefulSensors/moonshine-tiny-uk"&gt;https://huggingface.co/UsefulSensors/moonshine-tiny-uk&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ja: &lt;a href="https://huggingface.co/UsefulSensors/moonshine-tiny-ja"&gt;https://huggingface.co/UsefulSensors/moonshine-tiny-ja&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ko: &lt;a href="https://huggingface.co/UsefulSensors/moonshine-tiny-ko"&gt;https://huggingface.co/UsefulSensors/moonshine-tiny-ko&lt;/a&gt;&lt;/li&gt; &lt;li&gt;zh: &lt;a href="https://huggingface.co/UsefulSensors/moonshine-tiny-zh"&gt;https://huggingface.co/UsefulSensors/moonshine-tiny-zh&lt;/a&gt;&lt;/li&gt; &lt;li&gt;vi: &lt;a href="https://huggingface.co/UsefulSensors/moonshine-tiny-vi"&gt;https://huggingface.co/UsefulSensors/moonshine-tiny-vi&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/keveman"&gt; /u/keveman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8hq8i/flavors_of_moonshine_tiny_monolingual_asr_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8hq8i/flavors_of_moonshine_tiny_monolingual_asr_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8hq8i/flavors_of_moonshine_tiny_monolingual_asr_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T18:13:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7zk45</id>
    <title>VibeVoice RIP? What do you think?</title>
    <updated>2025-09-04T03:28:29+00:00</updated>
    <author>
      <name>/u/Fabix84</name>
      <uri>https://old.reddit.com/user/Fabix84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zk45/vibevoice_rip_what_do_you_think/"&gt; &lt;img alt="VibeVoice RIP? What do you think?" src="https://preview.redd.it/un6uilkoh2nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39144e5e650c4ae66ef8205b6d09c62f6427edad" title="VibeVoice RIP? What do you think?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the past two weeks, I had been working hard to try and contribute to OpenSource AI by creating the VibeVoice nodes for ComfyUI. I‚Äôm glad to see that my contribution has helped quite a few people:&lt;br /&gt; &lt;a href="https://github.com/Enemyx-net/VibeVoice-ComfyUI"&gt;https://github.com/Enemyx-net/VibeVoice-ComfyUI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A short while ago, Microsoft suddenly deleted its official VibeVoice repository on GitHub. As of the time I‚Äôm writing this, the reason is still unknown (or at least I don‚Äôt know it).&lt;/p&gt; &lt;p&gt;At the same time, Microsoft also removed the VibeVoice-Large and VibeVoice-Large-Preview models from HF. For now, they are still available here: &lt;a href="https://modelscope.cn/models/microsoft/VibeVoice-Large/files"&gt;https://modelscope.cn/models/microsoft/VibeVoice-Large/files&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Of course, for those who have already downloaded and installed my nodes and the models, they will continue to work. Technically, I could decide to embed a copy of VibeVoice directly into my repo, but first I need to understand why Microsoft chose to remove its official repository. My hope is that they are just fixing a few things and that it will be back online soon. I also hope there won‚Äôt be any changes to the usage license...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;UPDATE: I have released a new 1.0.9 version that embed VibeVoice. No longer requires external VibeVoice installation.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabix84"&gt; /u/Fabix84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/un6uilkoh2nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zk45/vibevoice_rip_what_do_you_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7zk45/vibevoice_rip_what_do_you_think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T03:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8tym3</id>
    <title>Introducing EmbeddingGemma: The Best-in-Class Open Model for On-Device Embeddings- Google Developers Blog</title>
    <updated>2025-09-05T02:53:03+00:00</updated>
    <author>
      <name>/u/richardanaya</name>
      <uri>https://old.reddit.com/user/richardanaya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8tym3/introducing_embeddinggemma_the_bestinclass_open/"&gt; &lt;img alt="Introducing EmbeddingGemma: The Best-in-Class Open Model for On-Device Embeddings- Google Developers Blog" src="https://external-preview.redd.it/YVJV2K5BxWqUcGq19dxvYXP4zu_HpvAC56XyJ41zblc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25f438fe039f96afc22af657623fb3d5cc7ef067" title="Introducing EmbeddingGemma: The Best-in-Class Open Model for On-Device Embeddings- Google Developers Blog" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/richardanaya"&gt; /u/richardanaya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/introducing-embeddinggemma/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8tym3/introducing_embeddinggemma_the_bestinclass_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8tym3/introducing_embeddinggemma_the_bestinclass_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T02:53:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8er8l</id>
    <title>Multi-participant local AI convo (role playing both people lol)</title>
    <updated>2025-09-04T16:21:56+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8er8l/multiparticipant_local_ai_convo_role_playing_both/"&gt; &lt;img alt="Multi-participant local AI convo (role playing both people lol)" src="https://external-preview.redd.it/YW5wNnRhMXFhNm5mMfQ42angarv6HXrCpThvDtXvGVcDni6Zg7S-_yFlN5xt.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40bb3ccaa58a1b858112dd222506dd014a184b43" title="Multi-participant local AI convo (role playing both people lol)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So most AI convos seem limited to 1-on-1 (1 human, 1 AI). I wanted to see if I could get multiple humans talking to the AI locally.&lt;/p&gt; &lt;p&gt;The setup: two audio streams, a speech-to-text pipeline, and a templating system, all on a 3090. It &lt;em&gt;should&lt;/em&gt; scale assuming the underlying LLM is smart enough. &lt;/p&gt; &lt;p&gt;I didn‚Äôt actually have two mics sooooo I played both people LOL. Bob is me. Alice is me in a wig (didn't look too bad :P). I just muted one mic, swapped over, and went back and forth with myself.&lt;/p&gt; &lt;p&gt;It‚Äôs still early, but fully modular so you can use whatever models you want. Looks like multi-party convos with locally running AI is possible!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/p5e7bb1qa6nf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8er8l/multiparticipant_local_ai_convo_role_playing_both/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8er8l/multiparticipant_local_ai_convo_role_playing_both/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T16:21:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8abe6</id>
    <title>Eigent ‚Äì Open Source, Local-First Multi-Agent Workforce</title>
    <updated>2025-09-04T13:36:11+00:00</updated>
    <author>
      <name>/u/FitHeron1933</name>
      <uri>https://old.reddit.com/user/FitHeron1933</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8abe6/eigent_open_source_localfirst_multiagent_workforce/"&gt; &lt;img alt="Eigent ‚Äì Open Source, Local-First Multi-Agent Workforce" src="https://b.thumbs.redditmedia.com/o7mm3i-ghjjH2wm6PPrdvzWbqLN-x3TC3x0fwhx5pMw.jpg" title="Eigent ‚Äì Open Source, Local-First Multi-Agent Workforce" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A month ago we shared &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mdbm5t/eigent_open_source_localfirst_multiagent_workforce/?utm_source=chatgpt.com&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;Eigent&lt;/a&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mdbm5t/eigent_open_source_localfirst_multiagent_workforce/?utm_source=chatgpt.com&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;here&lt;/a&gt;, our attempt at building a fully open-source, local-first multi-agent workforce you can run on your own machine.&lt;/p&gt; &lt;p&gt;The response was amazing, and so was the feedback. Two things came up the most:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Needing to sign up before trying it&lt;/li&gt; &lt;li&gt;Concerns about the license not feeling ‚Äútruly open‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So we focused on those. Now Eigent is fully local, you‚Äôll still see a signup pipeline in the UI, but everything is stored only on your own device in a private Postgres database. Nothing leaves your machine. On the licensing side, we‚Äôve also made updates. Eigent is now free for individuals and small teams of up to 10 users, including commercial use.&lt;/p&gt; &lt;p&gt;We‚Äôd love for you to give Eigent another try and let us know what you think. Your input is what helps us shape it into something that‚Äôs genuinely useful for developers and teams who want privacy, flexibility, and full ownership of their AI workflows, while unlocking exceptional productivity.&lt;/p&gt; &lt;p&gt;Follow the guide for setting it up locally: &lt;a href="https://github.com/eigent-ai/eigent/blob/main/server/README_EN.md"&gt;https://github.com/eigent-ai/eigent/blob/main/server/README_EN.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚Üí GitHub: &lt;a href="https://github.com/eigent-ai/eigent"&gt;https://github.com/eigent-ai/eigent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚Üí Download: &lt;a href="https://eigent.ai"&gt;https://eigent.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And if you find it useful, please give the repo a ‚≠ê and spread the word!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FitHeron1933"&gt; /u/FitHeron1933 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n8abe6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8abe6/eigent_open_source_localfirst_multiagent_workforce/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8abe6/eigent_open_source_localfirst_multiagent_workforce/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T13:36:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1n86rl2</id>
    <title>Which is the Best LLM you can run on your hardware? Discover it with llm-eval simple</title>
    <updated>2025-09-04T10:45:34+00:00</updated>
    <author>
      <name>/u/gnorrisan</name>
      <uri>https://old.reddit.com/user/gnorrisan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n86rl2/which_is_the_best_llm_you_can_run_on_your/"&gt; &lt;img alt="Which is the Best LLM you can run on your hardware? Discover it with llm-eval simple" src="https://preview.redd.it/nsuc0la2n4nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6a9d7274af0bf58b60061454cb27096d8dcb54d" title="Which is the Best LLM you can run on your hardware? Discover it with llm-eval simple" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can check your prompts and get an heatmap of the most correct and fast LLMs you can run on your computer for the use-cases you care. The most intense colors means a faster reply.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/grigio/llm-eval-simple"&gt;https://github.com/grigio/llm-eval-simple&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gnorrisan"&gt; /u/gnorrisan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nsuc0la2n4nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n86rl2/which_is_the_best_llm_you_can_run_on_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n86rl2/which_is_the_best_llm_you_can_run_on_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T10:45:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8h3oj</id>
    <title>chatterbox multilingual</title>
    <updated>2025-09-04T17:50:08+00:00</updated>
    <author>
      <name>/u/manmaynakhashi</name>
      <uri>https://old.reddit.com/user/manmaynakhashi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing Chatterbox Multilingual!&lt;br /&gt; &lt;a href="https://github.com/resemble-ai/chatterbox"&gt;https://github.com/resemble-ai/chatterbox&lt;/a&gt;&lt;br /&gt; production-grade open-source text-to-speech (TTS) model that speaks 23 languages out of the box. From Arabic and Hindi to French, Japanese, and Swahili.&lt;br /&gt; With emotion and intensity control, zero-shot voice cloning, and PerTh watermarking enabled by default, Chatterbox Multilingual is built for developers, creators, and teams designing the next generation of agents, games, videos, and interactive apps. MIT licensed and ready to use today.&lt;br /&gt; Note: en es it pt fr de hi - are more stable now&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/manmaynakhashi"&gt; /u/manmaynakhashi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8h3oj/chatterbox_multilingual/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8h3oj/chatterbox_multilingual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8h3oj/chatterbox_multilingual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T17:50:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1n84rp5</id>
    <title>Mistral Set for $14 Billion Valuation With New Funding Round</title>
    <updated>2025-09-04T08:43:07+00:00</updated>
    <author>
      <name>/u/robberviet</name>
      <uri>https://old.reddit.com/user/robberviet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n84rp5/mistral_set_for_14_billion_valuation_with_new/"&gt; &lt;img alt="Mistral Set for $14 Billion Valuation With New Funding Round" src="https://external-preview.redd.it/M4AwBB5q0ft-ep7S9kw_Y8TYtAOJMnISlkcxXVEEP40.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=741648af7070d19298c0fb93541f0b6cf31c20b1" title="Mistral Set for $14 Billion Valuation With New Funding Round" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mistral has secured new funding, ensuring continued independence. No more rumors.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robberviet"&gt; /u/robberviet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.bloomberg.com/news/articles/2025-09-03/mistral-set-for-14-billion-valuation-with-new-funding-round"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n84rp5/mistral_set_for_14_billion_valuation_with_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n84rp5/mistral_set_for_14_billion_valuation_with_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T08:43:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1n82ndz</id>
    <title>Finally: 3090 Successor: 5070 Ti super 24Gb 800$</title>
    <updated>2025-09-04T06:24:02+00:00</updated>
    <author>
      <name>/u/On1ineAxeL</name>
      <uri>https://old.reddit.com/user/On1ineAxeL</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n82ndz/finally_3090_successor_5070_ti_super_24gb_800/"&gt; &lt;img alt="Finally: 3090 Successor: 5070 Ti super 24Gb 800$" src="https://external-preview.redd.it/kT4ohg_saogl0QowFisFMgdjPOl3cV1Xjwbw3qji8TU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01c103360d1456c04311f988c3089b01de5157d0" title="Finally: 3090 Successor: 5070 Ti super 24Gb 800$" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/j9riehskc3nf1.jpg?width=1341&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fd5386a95c701b1a750a20a2b4116c93df426306"&gt;https://preview.redd.it/j9riehskc3nf1.jpg?width=1341&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fd5386a95c701b1a750a20a2b4116c93df426306&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=9ii4qrzfV5w"&gt;https://www.youtube.com/watch?v=9ii4qrzfV5w&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If they are well compressed in terms of energy consumption, then now it will be possible to assemble a rig with 100 gigabytes of VRAM without kilowatts of energy consumption, and we shouldn‚Äôt forget about the new FP4 formats&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/On1ineAxeL"&gt; /u/On1ineAxeL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n82ndz/finally_3090_successor_5070_ti_super_24gb_800/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n82ndz/finally_3090_successor_5070_ti_super_24gb_800/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n82ndz/finally_3090_successor_5070_ti_super_24gb_800/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T06:24:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8flm8</id>
    <title>Welcome EmbeddingGemma, Google's new efficient embedding model</title>
    <updated>2025-09-04T16:53:38+00:00</updated>
    <author>
      <name>/u/-Cubie-</name>
      <uri>https://old.reddit.com/user/-Cubie-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8flm8/welcome_embeddinggemma_googles_new_efficient/"&gt; &lt;img alt="Welcome EmbeddingGemma, Google's new efficient embedding model" src="https://external-preview.redd.it/wK_NlXq1ONqyIYDxucL4h__hCZ_W82Nv0bvUoRBbUiw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56ded22139e25dfa406e5e0466e1889db55d384e" title="Welcome EmbeddingGemma, Google's new efficient embedding model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cubie-"&gt; /u/-Cubie- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/embeddinggemma"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8flm8/welcome_embeddinggemma_googles_new_efficient/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8flm8/welcome_embeddinggemma_googles_new_efficient/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T16:53:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8eyrj</id>
    <title>The Semantic Galaxy: An interactive 3D embedding visualization demo, built with Google's new EmbeddingGemma model</title>
    <updated>2025-09-04T16:29:41+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8eyrj/the_semantic_galaxy_an_interactive_3d_embedding/"&gt; &lt;img alt="The Semantic Galaxy: An interactive 3D embedding visualization demo, built with Google's new EmbeddingGemma model" src="https://external-preview.redd.it/bmM1aGc5Mmc4Nm5mMXNiReDuZYB6lpqEJX0zHZTcugGbb2eldaGNuOlpAnsU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97e03ec7d227bde27ddc660aa04045e9288b6374" title="The Semantic Galaxy: An interactive 3D embedding visualization demo, built with Google's new EmbeddingGemma model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Semantic Galaxy lets you explore your documents as an interactive 3D universe. Each document becomes a star, clustered together with other documents of similar meaning. Simply type a query, and fly through the galaxy to find the most relevant result. The web app runs EmbeddingGemma 100% locally in your browser using Transformers.js, computing rich 768-dimensional vectors for each of your documents. We then perform dimensionality reduction with UMAP to map these vectors into 3D coordinates for visualization. Because this entire process happens on your device, your data remains completely private and the app even works offline. &lt;/p&gt; &lt;p&gt;Link to demo: &lt;a href="https://huggingface.co/spaces/webml-community/semantic-galaxy"&gt;https://huggingface.co/spaces/webml-community/semantic-galaxy&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fp0mle2g86nf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8eyrj/the_semantic_galaxy_an_interactive_3d_embedding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8eyrj/the_semantic_galaxy_an_interactive_3d_embedding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T16:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1n89wi8</id>
    <title>power limit your GPU(s) to reduce electricity costs</title>
    <updated>2025-09-04T13:18:43+00:00</updated>
    <author>
      <name>/u/MelodicRecognition7</name>
      <uri>https://old.reddit.com/user/MelodicRecognition7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89wi8/power_limit_your_gpus_to_reduce_electricity_costs/"&gt; &lt;img alt="power limit your GPU(s) to reduce electricity costs" src="https://a.thumbs.redditmedia.com/uyJHr0LNNqlJuU5QeZvn8UmkeX2y2aeJN7dPnMhD4t0.jpg" title="power limit your GPU(s) to reduce electricity costs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;many people worry about high electricity costs, the solution is simply power limit the GPU to about 50% of its TDP (&lt;code&gt;nvidia-smi -i $GPU_ID --power-limit=$LIMIT_IN_WATTS&lt;/code&gt;) because token generation speed does not increase past some power limit amount so you just waste electricity with the full power. As an example here is a result of &lt;code&gt;llama-bench&lt;/code&gt; (pp1024, tg1024, model Qwen3-32B Q8_0 33 GB) running on RTX Pro 6000 Workstation (600W TDP) power limited from 150W to 600W in 30W increments. 350W is the best spot for that card which is obvious on the token generation speed chart, however the prompt processing speed rise is also not linear and starts to slow down at about 350W. And another example: the best power limit for 4090 (450W TDP) is 270W, tested with Qwen3 8B.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MelodicRecognition7"&gt; /u/MelodicRecognition7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n89wi8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89wi8/power_limit_your_gpus_to_reduce_electricity_costs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n89wi8/power_limit_your_gpus_to_reduce_electricity_costs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T13:18:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8eo92</id>
    <title>Introducing EmbeddingGemma: The highest ranking open text embedding model under 500M on MTEB</title>
    <updated>2025-09-04T16:18:56+00:00</updated>
    <author>
      <name>/u/codemaker1</name>
      <uri>https://old.reddit.com/user/codemaker1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8eo92/introducing_embeddinggemma_the_highest_ranking/"&gt; &lt;img alt="Introducing EmbeddingGemma: The highest ranking open text embedding model under 500M on MTEB" src="https://external-preview.redd.it/YVJV2K5BxWqUcGq19dxvYXP4zu_HpvAC56XyJ41zblc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25f438fe039f96afc22af657623fb3d5cc7ef067" title="Introducing EmbeddingGemma: The highest ranking open text embedding model under 500M on MTEB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codemaker1"&gt; /u/codemaker1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/introducing-embeddinggemma"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8eo92/introducing_embeddinggemma_the_highest_ranking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8eo92/introducing_embeddinggemma_the_highest_ranking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T16:18:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1n89ryn</id>
    <title>Most affordable AI computer with GPU (‚ÄúGPUter‚Äù) you can build in 2025?</title>
    <updated>2025-09-04T13:13:12+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89ryn/most_affordable_ai_computer_with_gpu_gputer_you/"&gt; &lt;img alt="Most affordable AI computer with GPU (‚ÄúGPUter‚Äù) you can build in 2025?" src="https://preview.redd.it/bk6tf5l2e5nf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8da7afc16f4d8ff260c98ad24de5cc8adc50a222" title="Most affordable AI computer with GPU (‚ÄúGPUter‚Äù) you can build in 2025?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After a bunch of testing and experiments, we landed on what looks like the best price-to-performance build you can do right now (using all new parts in the US, 2025). Total spend: $1,040.&lt;/p&gt; &lt;p&gt;That‚Äôs the actual GPUter in the photo ‚Äî whisper-quiet but surprisingly powerful.&lt;/p&gt; &lt;p&gt;Parts list:&lt;/p&gt; &lt;p&gt;GPU: NVIDIA RTX 5060 Ti 16GB Blackwell (759 AI TOPS) ‚Äì $429 &lt;a href="https://newegg.com/p/N82E16814932791"&gt;https://newegg.com/p/N82E16814932791&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Motherboard: B550M ‚Äì $99 &lt;a href="https://amazon.com/dp/B0BDCZRBD6"&gt;https://amazon.com/dp/B0BDCZRBD6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CPU: AMD Ryzen 5 5500 ‚Äì $60 &lt;a href="https://amazon.com/dp/B09VCJ171S"&gt;https://amazon.com/dp/B09VCJ171S&lt;/a&gt;&lt;/p&gt; &lt;p&gt;RAM: 32GB DDR4 (2√ó16GB) ‚Äì $52 &lt;a href="https://amazon.com/dp/B07RW6Z692"&gt;https://amazon.com/dp/B07RW6Z692&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Storage: M.2 SSD 4TB ‚Äì $249 &lt;a href="https://amazon.com/dp/B0DHLBDSP7"&gt;https://amazon.com/dp/B0DHLBDSP7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Case: JONSBO/JONSPLUS Z20 mATX ‚Äì $109 &lt;a href="https://amazon.com/dp/B0D1YKXXJD"&gt;https://amazon.com/dp/B0D1YKXXJD&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PSU: 600W ‚Äì $42 &lt;a href="https://amazon.com/dp/B014W3EMAO"&gt;https://amazon.com/dp/B014W3EMAO&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Grand total: $1,040&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Note: configs can vary, and you can go wild if you want (e.g. check out used AMD EPYC CPUs on eBay - 128 vCPUs for cheap üòâ)&lt;/p&gt; &lt;p&gt;In terms of memory, here‚Äôs what this build gives you:&lt;/p&gt; &lt;p&gt;‚ö° 16 GB of GDDR7 VRAM on the GPU with 448 GB/s bandwidth&lt;/p&gt; &lt;p&gt;üñ•Ô∏è 32 GB of DDR4 RAM on the CPU side (dual channel) with ~51 GB/s bandwidth&lt;/p&gt; &lt;p&gt;On our workloads, GPU VRAM runs at about 86% utilization, while CPU RAM sits around 50% usage.&lt;/p&gt; &lt;p&gt;This machine also boots straight into AI workloads using the AI-optimized Linux distro Sbnb Linux: &lt;a href="https://github.com/sbnb-io/sbnb"&gt;https://github.com/sbnb-io/sbnb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üí° &lt;strong&gt;What can this thing actually do?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We used this exact setup in our Google Gemma3n Hackathon submission ‚Äî it was able to process 16 live security camera feeds with real-time video understanding: &lt;a href="https://kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sixth-sense-for-security-guards-powered-by-googles"&gt;https://kaggle.com/competitions/google-gemma-3n-hackathon/writeups/sixth-sense-for-security-guards-powered-by-googles&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy building if anyone wants to replicate! Feel free to share your configs and findings üöÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bk6tf5l2e5nf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89ryn/most_affordable_ai_computer_with_gpu_gputer_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n89ryn/most_affordable_ai_computer_with_gpu_gputer_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T13:13:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8nr7y</id>
    <title>Summary of August big events</title>
    <updated>2025-09-04T22:08:35+00:00</updated>
    <author>
      <name>/u/nh_local</name>
      <uri>https://old.reddit.com/user/nh_local</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Google introduced &lt;strong&gt;Gemini 2.5 Deep Think&lt;/strong&gt;, a special &amp;quot;extended thinking&amp;quot; mode for solving complex problems and exploring alternatives. (&lt;em&gt;special&lt;/em&gt;)&lt;/li&gt; &lt;li&gt;Anthropic released &lt;strong&gt;Claude Opus 4.1&lt;/strong&gt;, an upgrade focused on improving agentic capabilities and real-world coding.&lt;/li&gt; &lt;li&gt;Google DeepMind announced &lt;strong&gt;Genie 3.0&lt;/strong&gt;, a &amp;quot;world model&amp;quot; for creating interactive 3D environments from text, maintaining consistency for several minutes. (&lt;em&gt;special&lt;/em&gt;)&lt;/li&gt; &lt;li&gt;OpenAI released &lt;strong&gt;gpt-oss-120b&lt;/strong&gt; and &lt;strong&gt;gpt-oss-20b&lt;/strong&gt;, a family of open-source models with high reasoning capabilities, optimized to run on accessible hardware.&lt;/li&gt; &lt;li&gt;OpenAI launched &lt;strong&gt;GPT-5&lt;/strong&gt;, the company's next-generation model, with significant improvements in coding and a dynamic &amp;quot;thinking&amp;quot; mode to reduce hallucinations.&lt;/li&gt; &lt;li&gt;DeepSeek released &lt;strong&gt;DeepSeek V3.1&lt;/strong&gt;, a hybrid model combining fast and slow &amp;quot;thinking&amp;quot; modes to improve performance in agentic tasks and tool use.&lt;/li&gt; &lt;li&gt;Google launched a preview of &lt;strong&gt;Gemini 2.5 Flash Image&lt;/strong&gt; (showcased as &lt;em&gt;nano-banana&lt;/em&gt;), an advanced model for precise image editing, merging, and maintaining character consistency. (&lt;em&gt;special&lt;/em&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nh_local"&gt; /u/nh_local &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8nr7y/summary_of_august_big_events/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8nr7y/summary_of_august_big_events/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8nr7y/summary_of_august_big_events/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T22:08:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8c56m</id>
    <title>Hugging Face open-sources FineVision</title>
    <updated>2025-09-04T14:44:45+00:00</updated>
    <author>
      <name>/u/futterneid</name>
      <uri>https://old.reddit.com/user/futterneid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm Andi, the multimodal research lead at Hugging Face. We just open-sourced FineVision, the largest curation of datasets for VLMs, with over 200 sources! &lt;/p&gt; &lt;p&gt;With Finevision we have:&lt;/p&gt; &lt;p&gt;&amp;gt; 20% improvement across 10 benchmarks&lt;br /&gt; &amp;gt; 17M unique images&lt;br /&gt; &amp;gt; 10B answer tokens&lt;br /&gt; &amp;gt; New capabilities: GUI navigation, pointing, counting&lt;/p&gt; &lt;p&gt;We wrote a blog full of interesting details for the dataset, go check it out and let me know what you think :)&lt;br /&gt; &lt;a href="https://huggingface.co/spaces/HuggingFaceM4/FineVision"&gt;https://huggingface.co/spaces/HuggingFaceM4/FineVision&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/futterneid"&gt; /u/futterneid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c56m/hugging_face_opensources_finevision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c56m/hugging_face_opensources_finevision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c56m/hugging_face_opensources_finevision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T14:44:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8kk48</id>
    <title>New AI Dungeon Models: Wayfarer 2 12B &amp; Nova 70B</title>
    <updated>2025-09-04T20:01:58+00:00</updated>
    <author>
      <name>/u/NottKolby</name>
      <uri>https://old.reddit.com/user/NottKolby</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8kk48/new_ai_dungeon_models_wayfarer_2_12b_nova_70b/"&gt; &lt;img alt="New AI Dungeon Models: Wayfarer 2 12B &amp;amp; Nova 70B" src="https://external-preview.redd.it/YTw5l9Vh8yq-jd3sLHEgGG1W0jXu67lGtXgcd-NGSsQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3798d8af683fa3a9a0d957b91f08d8b015b64c2f" title="New AI Dungeon Models: Wayfarer 2 12B &amp;amp; Nova 70B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today AI Dungeon open sourced two new SOTA narrative roleplay models!&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/LatitudeGames/Wayfarer-2-12B"&gt;Wayfarer 2 12B&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;Wayfarer 2 further refines the formula that made the original Wayfarer so popular, slowing the pacing, increasing the length and detail of responses and making death a distinct possibility for all characters‚Äînot just the user.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/LatitudeGames/Nova-70B-Llama-3.3"&gt;Nova 70B&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;Built on Llama 70B and trained with the same techniques that made Muse good at stories about relationships and character development, Nova brings the greater reasoning abilities of a larger model to understanding the nuance that makes characters feel real and stories come to life. Whether you're roleplaying cloak-and-dagger intrigue, personal drama or an epic quest, Nova is designed to keep characters consistent across extended contexts while delivering the nuanced character work that defines compelling stories.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NottKolby"&gt; /u/NottKolby &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8kk48/new_ai_dungeon_models_wayfarer_2_12b_nova_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8kk48/new_ai_dungeon_models_wayfarer_2_12b_nova_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8kk48/new_ai_dungeon_models_wayfarer_2_12b_nova_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T20:01:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8r5fj</id>
    <title>new stealth model carrot ü•ï, works well for coding</title>
    <updated>2025-09-05T00:38:24+00:00</updated>
    <author>
      <name>/u/Illustrious_Row_9971</name>
      <uri>https://old.reddit.com/user/Illustrious_Row_9971</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8r5fj/new_stealth_model_carrot_works_well_for_coding/"&gt; &lt;img alt="new stealth model carrot ü•ï, works well for coding" src="https://preview.redd.it/v1itltb5s8nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f91cc956af85e049c81bcbf8619265b81e1b1d7d" title="new stealth model carrot ü•ï, works well for coding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious_Row_9971"&gt; /u/Illustrious_Row_9971 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v1itltb5s8nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8r5fj/new_stealth_model_carrot_works_well_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8r5fj/new_stealth_model_carrot_works_well_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T00:38:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8n9u6</id>
    <title>Converted my unused laptop into a family server for gpt-oss 20B</title>
    <updated>2025-09-04T21:48:48+00:00</updated>
    <author>
      <name>/u/Vaddieg</name>
      <uri>https://old.reddit.com/user/Vaddieg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent few hours on setting everything up and asked my wife (frequent chatGPT user) to help with testing. We're very satisfied so far.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Keys specs:&lt;/strong&gt;&lt;br /&gt; Generation: 46-40 t/s&lt;br /&gt; Context: 20K&lt;br /&gt; Idle power: 2W (around 5 EUR annually)&lt;br /&gt; Generation power: 38W&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt;&lt;br /&gt; 2021 m1 pro macbook pro 16GB&lt;br /&gt; 45W GaN charger&lt;br /&gt; Power meter&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Challenges faced:&lt;/strong&gt;&lt;br /&gt; Extremely tight model+context fit into 16GB RAM&lt;br /&gt; Avoiding laptop battery degradation in 24/7 plugged mode&lt;br /&gt; Preventing sleep with lid closed and OS autoupdates&lt;br /&gt; Accessing the service from everywhere&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tools used:&lt;/strong&gt;&lt;br /&gt; Battery Toolkit&lt;br /&gt; llama.cpp server&lt;br /&gt; DynDNS&lt;br /&gt; Terminal+SSH (logging into GUI isn't an option due to RAM shortage)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Thoughts on gpt-oss:&lt;/strong&gt;&lt;br /&gt; Very fast and laconic thinking, good instruction following, precise answers in most cases. But sometimes it spits out very strange factual errors never seen even in old 8B models, it might be a sign of intentional weights corruption or &amp;quot;fine-tuning&amp;quot; of their commercial o3 with some garbage data&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vaddieg"&gt; /u/Vaddieg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8n9u6/converted_my_unused_laptop_into_a_family_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8n9u6/converted_my_unused_laptop_into_a_family_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8n9u6/converted_my_unused_laptop_into_a_family_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T21:48:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8kdxi</id>
    <title>[SWE-rebench] GLM-4.5 &amp; Qwen3-Coder right behind Sonnet/GPT-5 on fresh GitHub tasks</title>
    <updated>2025-09-04T19:55:29+00:00</updated>
    <author>
      <name>/u/Fabulous_Pollution10</name>
      <uri>https://old.reddit.com/user/Fabulous_Pollution10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8kdxi/swerebench_glm45_qwen3coder_right_behind/"&gt; &lt;img alt="[SWE-rebench] GLM-4.5 &amp;amp; Qwen3-Coder right behind Sonnet/GPT-5 on fresh GitHub tasks" src="https://preview.redd.it/7xidzcpxc7nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6b3daa90d295013ce73775054ed5a8a3110c2eb" title="[SWE-rebench] GLM-4.5 &amp;amp; Qwen3-Coder right behind Sonnet/GPT-5 on fresh GitHub tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I‚Äôm Ibragim from Nebius.&lt;/p&gt; &lt;p&gt;We benchmarked &lt;strong&gt;52 fresh GitHub PR tasks from August 2025&lt;/strong&gt; on the&lt;a href="http://swe-rebench.com"&gt; SWE-rebench&lt;/a&gt; leaderboard. These are real, recent problems (no train leakage). We ran both proprietary and open-source models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick takeaways:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Top = &lt;strong&gt;Sonnet 4 and GPT-5:&lt;/strong&gt; on the August slice there is no statistically significant gap between them.&lt;/li&gt; &lt;li&gt;Very close: &lt;strong&gt;GLM-4.5 and Qwen3-Coder-480B.&lt;/strong&gt; Results are strong ‚Äî &lt;strong&gt;open source looks great&lt;/strong&gt; here!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grok Code Fast 1&lt;/strong&gt; is ~similar to &lt;strong&gt;o3&lt;/strong&gt; in quality, but about &lt;strong&gt;20√ó&lt;/strong&gt; cheaper (~$0.05 per task).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Please check the&lt;a href="https://swe-rebench.com/"&gt; leaderboard &lt;/a&gt;itself ‚Äî 30+ models there, including &lt;strong&gt;gpt-oss-20b&lt;/strong&gt;, &lt;strong&gt;Qwen3-Coder-30B-A3B-Instruct&lt;/strong&gt;, &lt;strong&gt;GLM-4.5-Air&lt;/strong&gt;, etc. Also you can click Inspect to see each of the &lt;strong&gt;52 tasks from 51 repos&lt;/strong&gt;. And we added price per instance! &lt;/p&gt; &lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; If you would like us to add more models, or if you notice any questionable tasks, please write in the comments. After our previous post, we received a lot of feedback and updated the leaderboard based on that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Pollution10"&gt; /u/Fabulous_Pollution10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7xidzcpxc7nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8kdxi/swerebench_glm45_qwen3coder_right_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8kdxi/swerebench_glm45_qwen3coder_right_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T19:55:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8egxb</id>
    <title>EmbeddingGemma - 300M parameter, state-of-the-art for its size, open embedding model from Google</title>
    <updated>2025-09-04T16:11:17+00:00</updated>
    <author>
      <name>/u/curiousily_</name>
      <uri>https://old.reddit.com/user/curiousily_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EmbeddingGemma (300M) embedding model by Google&lt;/p&gt; &lt;ul&gt; &lt;li&gt;300M parameters&lt;/li&gt; &lt;li&gt;text only&lt;/li&gt; &lt;li&gt;Trained with data in 100+ languages&lt;/li&gt; &lt;li&gt;768 output embedding size (smaller too with MRL)&lt;/li&gt; &lt;li&gt;License &amp;quot;Gemma&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Weights on HuggingFace: &lt;a href="https://huggingface.co/google/embeddinggemma-300m"&gt;https://huggingface.co/google/embeddinggemma-300m&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Available on Ollama: &lt;a href="https://ollama.com/library/embeddinggemma"&gt;https://ollama.com/library/embeddinggemma&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog post with evaluations (credit goes to &lt;a href="https://old.reddit.com/user/-Cubie-"&gt;-Cubie-&lt;/a&gt;): &lt;a href="https://huggingface.co/blog/embeddinggemma"&gt;https://huggingface.co/blog/embeddinggemma&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/curiousily_"&gt; /u/curiousily_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8egxb/embeddinggemma_300m_parameter_stateoftheart_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8egxb/embeddinggemma_300m_parameter_stateoftheart_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8egxb/embeddinggemma_300m_parameter_stateoftheart_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T16:11:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8rytq</id>
    <title>Th AI/LLM race is absolutely insane</title>
    <updated>2025-09-05T01:16:59+00:00</updated>
    <author>
      <name>/u/No-Underscore_s</name>
      <uri>https://old.reddit.com/user/No-Underscore_s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just look at the past 3 months. We‚Äôve had so many ups and downs in various areas of the field. The research, the business side, consumer side etc. &lt;/p&gt; &lt;p&gt;Now 6 months: Qwen coder, GLM models, new grok models, then recently nanobanana, with gpt 5 before it, then they dropped an improved codex, meanwhile across the board independent services are providing api access to some models too heavy to be hosted locally. Every day a new deal about ai is being made. Where is this all even heading to? Are we just waiting to watch the bubble blow up? Or are LLMs just going to be another thing before the next thing ?&lt;/p&gt; &lt;p&gt;Companies pooring billions up billions into this whole race, &lt;/p&gt; &lt;p&gt;Every other day something new drop, new model, new techniques, new way lf increasing tps, etc.&lt;/p&gt; &lt;p&gt;We‚Äôre really witnessing something crazy.&lt;/p&gt; &lt;p&gt;What part of this whole picture are you? Trying to make a business out of it ? Personal usage ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Underscore_s"&gt; /u/No-Underscore_s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8rytq/th_aillm_race_is_absolutely_insane/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8rytq/th_aillm_race_is_absolutely_insane/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8rytq/th_aillm_race_is_absolutely_insane/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T01:16:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8wyla</id>
    <title>I've made some fun demos using the new kimi-k2-0905</title>
    <updated>2025-09-05T05:35:25+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8wyla/ive_made_some_fun_demos_using_the_new_kimik20905/"&gt; &lt;img alt="I've made some fun demos using the new kimi-k2-0905" src="https://external-preview.redd.it/aGZ4NjJ2a3o3YW5mMQ6wDUEz-v_Nzg5h_KpwfXxI3dQiiTxqUDt15pQk26OB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53a1c44fdc22349784a666c01c3f87736334da77" title="I've made some fun demos using the new kimi-k2-0905" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They were all created with a single-pass, AI-generated prompt using both claude-code and kimi-k2-0905.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wavkswkz7anf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8wyla/ive_made_some_fun_demos_using_the_new_kimik20905/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8wyla/ive_made_some_fun_demos_using_the_new_kimik20905/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T05:35:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n89dy9</id>
    <title>ü§∑‚Äç‚ôÇÔ∏è</title>
    <updated>2025-09-04T12:56:20+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89dy9/_/"&gt; &lt;img alt="ü§∑‚Äç‚ôÇÔ∏è" src="https://preview.redd.it/21ivxa12b5nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e7a2744c78f03b518a206253cd3c9e861ea71c9" title="ü§∑‚Äç‚ôÇÔ∏è" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/21ivxa12b5nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89dy9/_/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n89dy9/_/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T12:56:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8ues8</id>
    <title>Kimi-K2-Instruct-0905 Released!</title>
    <updated>2025-09-05T03:15:27+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8ues8/kimik2instruct0905_released/"&gt; &lt;img alt="Kimi-K2-Instruct-0905 Released!" src="https://preview.redd.it/6jq7r55ak9nf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a5eec08b8c7bedbb50e39a668de98e599c3a0b6" title="Kimi-K2-Instruct-0905 Released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6jq7r55ak9nf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8ues8/kimik2instruct0905_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8ues8/kimik2instruct0905_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-05T03:15:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n7j5z2</id>
    <title>Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)</title>
    <updated>2025-09-03T16:14:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt; &lt;img alt="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" src="https://preview.redd.it/wdx4ivdw3zmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=876855c03867ead70389d15b60f24b91d478f835" title="Our 2nd AMA: Hugging Face Science Team, Creators of SmolLM, SmolVLM, and more! (Tomorrow, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wdx4ivdw3zmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n7j5z2/our_2nd_ama_hugging_face_science_team_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-03T16:14:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n8c3l2</id>
    <title>AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more.</title>
    <updated>2025-09-04T14:43:01+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt; &lt;img alt="AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more." src="https://external-preview.redd.it/y8IJElEOEd_2568MHNUZQsP7_aRTCAzyzXUKpDJwl1Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e377887ea8d7eae841499cc497b90b82aa97816" title="AMA with Hugging Face Science, the team behind SmolLM, SmolVLM, Fineweb and more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're super excited to do this AMA. Come ask your questions to the researchers behind &lt;strong&gt;SmolLM, SmolVLM, FineWeb&lt;/strong&gt;, and more. You can learn more about our work at &lt;a href="http://hf.co/science"&gt;hf.co/science&lt;/a&gt; ü§ó&lt;/p&gt; &lt;p&gt;If you want to get started in ML, a good place is &lt;a href="https://hf.co/learn"&gt;https://hf.co/learn&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To celebrate the AMA, we release a new &lt;strong&gt;FineVision&lt;/strong&gt; dataset, check it out! &lt;a href="https://huggingface.co/datasets/HuggingFaceM4/FineVision"&gt;https://huggingface.co/datasets/HuggingFaceM4/FineVision&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our participants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/eliebak"&gt;Elie Bakouch&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/eliebakk"&gt;u/eliebakk&lt;/a&gt; (SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/loubnabnl"&gt;Loubna Ben Allal&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/loubnabnl"&gt;u/loubnabnl&lt;/a&gt; (SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/nouamanetazi"&gt;Nouamane Tazi&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/Norlax"&gt;u/Norlax&lt;/a&gt;_42 (Nanotron/SmolLM)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lvwerra"&gt;Leandro von Werra&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/lvwerra"&gt;u/lvwerra&lt;/a&gt; (Head of Research)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/edbeeching"&gt;Edward Beeching&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/edbeeching"&gt;u/edbeeching&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/cmpatino"&gt;Carlos Miguel Pati√±o&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/cmpatino"&gt;u/cmpatino&lt;/a&gt;_ (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kashif"&gt;Kashif Rasul&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/krasul"&gt;u/krasul&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lewtun"&gt;Lewis Tunstall&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/lewtun"&gt;u/lewtun&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/qgallouedec"&gt;Quentin Gallou√©dec&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/qgallouedec"&gt;u/qgallouedec&lt;/a&gt; (Post Training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/clefourrier"&gt;Cl√©mentine Fourrier&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/clefourrier"&gt;u/clefourrier&lt;/a&gt; (Eval)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/SaylorTwift"&gt;Nathan Habib&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/HauntingMoment"&gt;u/HauntingMoment&lt;/a&gt; (Eval)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/lusxvr"&gt;Luis Wiedmann&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/luswd"&gt;u/luswd&lt;/a&gt; (Multimodal)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/andito"&gt;Andres Marafioti&lt;/a&gt;, &lt;a href="/u/futterneid"&gt;u/futterneid&lt;/a&gt; (Multimodal)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/guipenedo"&gt;Guilherme Penedo&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/PhilipsNostrum"&gt;u/PhilipsNostrum&lt;/a&gt; (Data)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/hynky"&gt;Hynek Kydl√≠ƒçek&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/Other"&gt;u/Other&lt;/a&gt;_Housing8453 (Data)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/reach-vb"&gt;Vaibhav Srivastav,&lt;/a&gt; &lt;a href="/u/vaibhavs10"&gt;u/vaibhavs10&lt;/a&gt; (Head of Developer Experience and Community)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/BrigitteTousi"&gt;Brigitte Tousignant&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/BriggieSmalls1992"&gt;u/BriggieSmalls1992&lt;/a&gt; (Comms)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Xenova"&gt;Xenova&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/xenovatech"&gt;u/xenovatech&lt;/a&gt; (Transformers.js)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/craffel"&gt;Colin Raffel&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/craffel"&gt;u/craffel&lt;/a&gt; (Research)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ngxson"&gt;Xuan Son Nguyen&lt;/a&gt;&lt;strong&gt;,&lt;/strong&gt; &lt;a href="/u/MediocreProgrammer99"&gt;u/MediocreProgrammer99&lt;/a&gt; (llama.cpp)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you are passionate about open source and open science like us, apply at &lt;a href="https://hf.co/jobs"&gt;https://hf.co/jobs&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Hugging Face team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o6moshv0u5nf1.png?width=2013&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee6a9392c3da8651e8a1425264ed855a51b69135"&gt;https://preview.redd.it/o6moshv0u5nf1.png?width=2013&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee6a9392c3da8651e8a1425264ed855a51b69135&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended but we will still answer question async for the next 24h. Follow our &lt;a href="https://hf.co/science"&gt;Hugging Face Science Org&lt;/a&gt; to be aware of our latest release! ü§ó&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n8c3l2/ama_with_hugging_face_science_the_team_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-04T14:43:01+00:00</published>
  </entry>
</feed>
