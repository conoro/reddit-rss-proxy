<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-28T20:25:28+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rh802w</id>
    <title>What if LLM agents passed KV-cache to each other instead of text? I tried it -- 73-78% token savings across Qwen, Llama, and DeepSeek</title>
    <updated>2026-02-28T17:10:16+00:00</updated>
    <author>
      <name>/u/proggmouse</name>
      <uri>https://old.reddit.com/user/proggmouse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you've used multi-agent setups with LangChain, CrewAI, AutoGen, or Swarm, you've probably noticed: every agent re-tokenizes and re-processes the full conversation from scratch. Agent 3 in a 4-agent chain is re-reading everything agents 1 and 2 already chewed through. When I measured this across Qwen2.5, Llama 3.2, and DeepSeek-R1-Distill, &lt;strong&gt;47-53% of all tokens in text mode turned out to be redundant re-processing.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;AVP (Agent Vector Protocol) is my attempt to fix this. Instead of passing text between agents, it passes the KV-cache directly. Agent A finishes reasoning serializes its key-value attention states, and Agent B injects them. No re-tokenization, no redundant forward passes.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Text: Planner -&amp;gt; [text] -&amp;gt; Critic re-tokenizes everything -&amp;gt; [text] -&amp;gt; Refiner re-tokenizes everything Latent: Planner -&amp;gt; [KV-cache] -&amp;gt; Critic injects, skips to generation -&amp;gt; [KV-cache] -&amp;gt; Refiner same &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;What it actually does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Same model on both sides? Direct KV-cache transfer, zero overhead.&lt;/li&gt; &lt;li&gt;Same family, different size (e.g. Qwen2.5-7B talking to 1.5B)? Vocabulary-mediated projection. No learned params, no calibration data needed.&lt;/li&gt; &lt;li&gt;Different families? Falls back to JSON. Not everything needs to be fancy.&lt;/li&gt; &lt;li&gt;Transport-agnostic -- works alongside A2A, MCP, gRPC, whatever you're already using&lt;/li&gt; &lt;li&gt;Binary wire format, not JSON+Base64 (33% overhead on tensor data is painful)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Numbers (these are structural, not accuracy claims):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Token savings of 73-78% and 2-4x speedups held consistent across all three model families. This isn't model-dependent -- it's just fewer forward passes, so less wall time. Here's the intuition: text prompt sizes balloon at each hop (186 -&amp;gt; 545 -&amp;gt; 1,073 -&amp;gt; 1,397 tokens in a 4-agent GSM8K chain). Latent stays flat at ~164-207 tokens per hop because prior context arrives as pre-computed KV-cache, not as text that needs re-encoding.&lt;/p&gt; &lt;p&gt;The gap widens with chain length. At 4 agents it's roughly 2x. At 16 agents (projected) it'd be around 6x, because text scales O(n^2) while latent scales O(n).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Limitations (yes, I know about these):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sample sizes are n=20 per model. The token and speed numbers are solid because they're structural (fewer forward passes is fewer forward passes), but n=20 isn't enough to make accuracy claims. That's future work.&lt;/li&gt; &lt;li&gt;Tested on small models only (1.5B-3B on an RTX 3070 Ti). 7B+ results pending.&lt;/li&gt; &lt;li&gt;This is a datacenter / same-machine thing. KV-cache for a 3B model runs about 130 MB per sample. You need 1 Gbps+ bandwidth minimum. Sending this over the internet is not happening.&lt;/li&gt; &lt;li&gt;Requires KV-cache access, so self-hosted only. Won't work with OpenAI/Anthropic/etc. APIs.&lt;/li&gt; &lt;li&gt;Same model only for now. Cross-model (Rosetta Stone) is implemented but not benchmarked yet.&lt;/li&gt; &lt;li&gt;Latent uses 17-54x more VRAM than text because you're holding KV-cache across hops instead of discarding it. Totally fine for 1.5B-3B on 8GB+ GPUs. At 7B+ it becomes a real constraint, and I don't have a clean answer for that yet.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Try it yourself:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install avp &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Two API levels depending on how much control you want:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import avp msg = avp.pack(&amp;quot;Hello&amp;quot;, model=&amp;quot;Qwen/Qwen2.5-7B-Instruct&amp;quot;, think_steps=20) answer = avp.unpack(msg, model=&amp;quot;Qwen/Qwen2.5-7B-Instruct&amp;quot;) from avp import HuggingFaceConnector connector = HuggingFaceConnector.from_pretrained(&amp;quot;Qwen/Qwen2.5-1.5B-Instruct&amp;quot;) context = connector.think(&amp;quot;Analyze this problem&amp;quot;, steps=20) answer = connector.generate(&amp;quot;Solve it.&amp;quot;, context=context) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;vLLM connector also available (&lt;code&gt;pip install &amp;quot;avp[vllm]&amp;quot;&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SDK: &lt;a href="https://github.com/VectorArc/avp-python"&gt;github.com/VectorArc/avp-python&lt;/a&gt; (MIT, 377 tests, 7 benchmarks)&lt;/li&gt; &lt;li&gt;Spec: &lt;a href="https://github.com/VectorArc/avp-spec"&gt;github.com/VectorArc/avp-spec&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Benchmark details: &lt;a href="https://github.com/VectorArc/avp-python/blob/main/docs/BENCHMARKS.md"&gt;BENCHMARKS.md&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is a nights-and-weekends project born out of my own multi-agent work. Happy to answer questions about the implementation and genuinely interested in feedback from people running multi-agent setups in production.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/proggmouse"&gt; /u/proggmouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh802w/what_if_llm_agents_passed_kvcache_to_each_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh802w/what_if_llm_agents_passed_kvcache_to_each_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh802w/what_if_llm_agents_passed_kvcache_to_each_other/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T17:10:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgzfat</id>
    <title>How is Qwen 3.5 (MoE 35b) in instruct mode (with no reasoning/thinking) ?</title>
    <updated>2026-02-28T10:37:01+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're out of bandwidth at the office, have you guys managed to test it ?&lt;/p&gt; &lt;p&gt;I find it surprising that qwen moved away from hybrid model (after the 2507 releases) to again release an hybrid reasoning model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgzfat/how_is_qwen_35_moe_35b_in_instruct_mode_with_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgzfat/how_is_qwen_35_moe_35b_in_instruct_mode_with_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgzfat/how_is_qwen_35_moe_35b_in_instruct_mode_with_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T10:37:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh8li2</id>
    <title>Qwen 3 (30B A3B 2507) - Qwen 3.5 (35B A3B) - Benchmarked on VLLM A100@40GB PHB Link and tensor-parallel-size = 2</title>
    <updated>2026-02-28T17:33:16+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a benchmark realized with VLLM bench suite.&lt;/p&gt; &lt;p&gt;It's a mix of the following matrix options:&lt;/p&gt; &lt;p&gt;Model : &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen/Qwen3.5-35B-A3B&lt;/li&gt; &lt;li&gt;Qwen/Qwen3-30B-A3B-Instruct-2507&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Attentions modes : &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;FLASH_ATTN&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;FLASHINFER&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Quantizations :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Official FP8 one (uses marlin kernels by default)&lt;/li&gt; &lt;li&gt;AWK 4bit&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Setup for the bench : &lt;/p&gt; &lt;p&gt;&lt;code&gt;Setup: 15 prompts · inf request rate · 223k input tokens / 78k output tokens · 28 Feb 2026&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Which is generated with : &lt;/p&gt; &lt;p&gt;&lt;code&gt;--dataset-name random --random-input-len 15000 --random-range-ratio 0.33 --random-output-len 5000 --num-prompts 15 --ignore-eos&lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;--no-enable-prefix-caching&lt;/code&gt; is always used&lt;/li&gt; &lt;li&gt;&lt;code&gt;--gpu-memory-utilization 0.8&lt;/code&gt; is always used&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;code&gt;--max-model-len&lt;/code&gt; is always at &lt;code&gt;36000&lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;For 30B FP8 max concurrency is at ~9.20&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;For 30B AWQ 4bit concurrency is at ~13.8&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;For 35B AWQ 4bit, concurrency is at &lt;strong&gt;~45&lt;/strong&gt; , forgot to type down for FP8&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All possibilities : &lt;/p&gt; &lt;ul&gt; &lt;li&gt;cyankiwi_Qwen3-30B-A3B-Instruct-2507-AWQ-4bit_FLASH_ATTN.json&lt;/li&gt; &lt;li&gt;cyankiwi_Qwen3-30B-A3B-Instruct-2507-AWQ-4bit_FLASHINFER.json&lt;/li&gt; &lt;li&gt;Qwen_Qwen3-30B-A3B-Instruct-2507-FP8_FLASH_ATTN.json&lt;/li&gt; &lt;li&gt;Qwen_Qwen3-30B-A3B-Instruct-2507-FP8_FLASHINFER.json&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;ul&gt; &lt;li&gt;cyankiwi_Qwen3.5-35B-A3B-AWQ-4bit_FLASH_ATTN.json&lt;/li&gt; &lt;li&gt;cyankiwi_Qwen3.5-35B-A3B-AWQ-4bit_FLASHINFER.json&lt;/li&gt; &lt;li&gt;Qwen_Qwen3.5-35B-A3B-FP8_FLASH_ATTN.json&lt;/li&gt; &lt;li&gt;Qwen_Qwen3.5-35B-A3B-FP8_FLASHINFER.json&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GPUs are two A100@40gb, PHB link, no PIX or NVLINK&lt;/p&gt; &lt;p&gt;Best model : Qwen3.5-35B-A3B-AWQ-4bit AWQ-4bit FlashInfer &lt;/p&gt; &lt;p&gt;Slowest model : Qwen3-30B-A3B-Instruct-2507-FP8 FP8 FlashAttn &lt;/p&gt; &lt;p&gt;I take the bet it wins because of prefill/prompt processing speed.&lt;/p&gt; &lt;h2&gt;Results&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Quant&lt;/th&gt; &lt;th&gt;Attn&lt;/th&gt; &lt;th&gt;Duration (s) ↓&lt;/th&gt; &lt;th&gt;Out tok/s ↑&lt;/th&gt; &lt;th&gt;Tot tok/s ↑&lt;/th&gt; &lt;th&gt;Max out/s ↑&lt;/th&gt; &lt;th&gt;TTFT mean (ms) ↓&lt;/th&gt; &lt;th&gt;TTFT median (ms) ↓&lt;/th&gt; &lt;th&gt;TTFT P99 (ms) ↓&lt;/th&gt; &lt;th&gt;TPOT mean (ms) ↓&lt;/th&gt; &lt;th&gt;TPOT median (ms) ↓&lt;/th&gt; &lt;th&gt;ITL mean (ms) ↓&lt;/th&gt; &lt;th&gt;ITL median (ms) ↓&lt;/th&gt; &lt;th&gt;ITL P99 (ms) ↓&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Qwen3-30B-A3B-2507 (cyankiwi)&lt;/td&gt; &lt;td&gt;AWQ-4bit&lt;/td&gt; &lt;td&gt;FlashAttn&lt;/td&gt; &lt;td&gt;283.1&lt;/td&gt; &lt;td&gt;276.6&lt;/td&gt; &lt;td&gt;1065.8&lt;/td&gt; &lt;td&gt;510&lt;/td&gt; &lt;td&gt;54425&lt;/td&gt; &lt;td&gt;54088&lt;/td&gt; &lt;td&gt;106745&lt;/td&gt; &lt;td&gt;40.17&lt;/td&gt; &lt;td&gt;40.53&lt;/td&gt; &lt;td&gt;39.46&lt;/td&gt; &lt;td&gt;30.35&lt;/td&gt; &lt;td&gt;862.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-30B-A3B-2507 (cyankiwi)&lt;/td&gt; &lt;td&gt;AWQ-4bit&lt;/td&gt; &lt;td&gt;FlashInfer&lt;/td&gt; &lt;td&gt;261.7&lt;/td&gt; &lt;td&gt;299.2&lt;/td&gt; &lt;td&gt;1153.0&lt;/td&gt; &lt;td&gt;540&lt;/td&gt; &lt;td&gt;49266&lt;/td&gt; &lt;td&gt;47567&lt;/td&gt; &lt;td&gt;95774&lt;/td&gt; &lt;td&gt;37.13&lt;/td&gt; &lt;td&gt;37.84&lt;/td&gt; &lt;td&gt;36.70&lt;/td&gt; &lt;td&gt;28.70&lt;/td&gt; &lt;td&gt;811.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-30B-A3B-2507 (Qwen)&lt;/td&gt; &lt;td&gt;FP8&lt;/td&gt; &lt;td&gt;FlashAttn&lt;/td&gt; &lt;td&gt;&lt;strong&gt;288.9&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;270.9&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;1044.2&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;495&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;55133&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;55077&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;107204&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;41.01&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;42.29&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;40.26&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;31.16&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;872.8&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-30B-A3B-2507 (Qwen)&lt;/td&gt; &lt;td&gt;FP8&lt;/td&gt; &lt;td&gt;FlashInfer&lt;/td&gt; &lt;td&gt;274.1&lt;/td&gt; &lt;td&gt;285.7&lt;/td&gt; &lt;td&gt;1100.8&lt;/td&gt; &lt;td&gt;511&lt;/td&gt; &lt;td&gt;49332&lt;/td&gt; &lt;td&gt;45671&lt;/td&gt; &lt;td&gt;97409&lt;/td&gt; &lt;td&gt;39.42&lt;/td&gt; &lt;td&gt;39.90&lt;/td&gt; &lt;td&gt;38.74&lt;/td&gt; &lt;td&gt;30.47&lt;/td&gt; &lt;td&gt;844.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3.5-35B-A3B (cyankiwi)&lt;/td&gt; &lt;td&gt;AWQ-4bit&lt;/td&gt; &lt;td&gt;FlashAttn&lt;/td&gt; &lt;td&gt;225.6&lt;/td&gt; &lt;td&gt;347.0&lt;/td&gt; &lt;td&gt;1337.2&lt;/td&gt; &lt;td&gt;630&lt;/td&gt; &lt;td&gt;46443&lt;/td&gt; &lt;td&gt;47864&lt;/td&gt; &lt;td&gt;85195&lt;/td&gt; &lt;td&gt;30.82&lt;/td&gt; &lt;td&gt;31.20&lt;/td&gt; &lt;td&gt;30.83&lt;/td&gt; &lt;td&gt;24.09&lt;/td&gt; &lt;td&gt;686.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3.5-35B-A3B (cyankiwi)&lt;/td&gt; &lt;td&gt;AWQ-4bit&lt;/td&gt; &lt;td&gt;&lt;strong&gt;FlashInfer&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;222.4&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;352.1&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;1356.8&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;645&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;45101&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;41771&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;84113&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;30.70&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;32.36&lt;/td&gt; &lt;td&gt;&lt;strong&gt;30.53&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;23.81&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;708.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3.5-35B-A3B (Qwen)&lt;/td&gt; &lt;td&gt;FP8&lt;/td&gt; &lt;td&gt;FlashAttn&lt;/td&gt; &lt;td&gt;237.1&lt;/td&gt; &lt;td&gt;330.2&lt;/td&gt; &lt;td&gt;1272.5&lt;/td&gt; &lt;td&gt;585&lt;/td&gt; &lt;td&gt;45852&lt;/td&gt; &lt;td&gt;41999&lt;/td&gt; &lt;td&gt;86326&lt;/td&gt; &lt;td&gt;33.28&lt;/td&gt; &lt;td&gt;35.29&lt;/td&gt; &lt;td&gt;32.92&lt;/td&gt; &lt;td&gt;25.99&lt;/td&gt; &lt;td&gt;726.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3.5-35B-A3B (Qwen)&lt;/td&gt; &lt;td&gt;FP8&lt;/td&gt; &lt;td&gt;FlashInfer&lt;/td&gt; &lt;td&gt;234.1&lt;/td&gt; &lt;td&gt;334.5&lt;/td&gt; &lt;td&gt;1289.0&lt;/td&gt; &lt;td&gt;600&lt;/td&gt; &lt;td&gt;48168&lt;/td&gt; &lt;td&gt;47319&lt;/td&gt; &lt;td&gt;86350&lt;/td&gt; &lt;td&gt;31.89&lt;/td&gt; &lt;td&gt;&lt;strong&gt;32.38&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;31.97&lt;/td&gt; &lt;td&gt;25.45&lt;/td&gt; &lt;td&gt;&lt;strong&gt;&lt;em&gt;28.1&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Running another benchmark with 30 parallel prompts to see how better can 3.5 win with it's lower mem/tokens kv cache usage&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh8li2/qwen_3_30b_a3b_2507_qwen_35_35b_a3b_benchmarked/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh8li2/qwen_3_30b_a3b_2507_qwen_35_35b_a3b_benchmarked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh8li2/qwen_3_30b_a3b_2507_qwen_35_35b_a3b_benchmarked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T17:33:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhb1xb</id>
    <title>fine tuning on proprietary data is way harder to deploy than anyone tells you and most of it has nothing to do with the model</title>
    <updated>2026-02-28T19:07:54+00:00</updated>
    <author>
      <name>/u/Olivia_Davis_09</name>
      <uri>https://old.reddit.com/user/Olivia_Davis_09</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;so we needed to fine tune on client data. sensitive stuff,, not nuclear level but the kind where if it leaks or somehow ends up in some upstream training pipeline our client relationship is basically done...&lt;/p&gt; &lt;p&gt;figured this would take a few weeks. dataset prep, training runs, eval, deploy. normal ml flow right...&lt;/p&gt; &lt;p&gt;three weeks in and we hadnt written a single training script yet lol&lt;/p&gt; &lt;p&gt;the actual blocker was way more boring than i expected. where does the training data go, who can access it, what exactly is logged by default, does opting out require some contract we cant sign in time, does the deployment endpoint share infra with other tenants... none of this is explained in one clean place. you either read the tos and dpa line by line like a lawyer or email sales and wait days for a reply...&lt;/p&gt; &lt;p&gt;together was one of the first we looked at. their public docs talk about data handling and settings, but when you are dealing with legal teams, screenshots of docs arent enough. they want explicit contractual language. so suddenly you are not thinking about hyperparams anymore,, you are thinking about msa wording and retention clauses...&lt;/p&gt; &lt;p&gt;fireworks similar story. technically solid product honestly... but again, the question wasnt can it fine tune. the question was can i hand this to our dpo and not get it immediately rejected. enterprise options exist but once you go down that road its contracts, commitments, timelines, not just api keys and credits...&lt;/p&gt; &lt;p&gt;replicate is great for deployment and inference... super clean experience there. but for what we needed at scale it felt more like a hosting layer than full blown training infra. not bad, just not aligned with this use case...&lt;/p&gt; &lt;p&gt;we probably spent a week just emailing back and forth with sales at different providers trying to get clear yes or no answers on data handling. that week felt more exhausting than the actual ml work...&lt;/p&gt; &lt;p&gt;eventually we landed on deepinfra. not because it was some magical obvious winner... it was more like the least painful option that cleared the compliance checkboxes fast enough for legal to say ok move ahead. default retention posture, cert paperwork ready, dedicated endpoint options available. that was enough for us to finally start the actual project...&lt;/p&gt; &lt;p&gt;the fine tuning itself had its own problems but thats another post...&lt;/p&gt; &lt;p&gt;what surprised me most is that nobody really talks about this part. every blog post jumps straight into dataset prep and hyperparameters and eval metrics... but if your data is even slightly sensitive, half your timeline might just be legal and compliance research before you touch a single training run...&lt;/p&gt; &lt;p&gt;curious if others just accept this as the cost of doing business or if anyone found a cleaner path upfront...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Olivia_Davis_09"&gt; /u/Olivia_Davis_09 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhb1xb/fine_tuning_on_proprietary_data_is_way_harder_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhb1xb/fine_tuning_on_proprietary_data_is_way_harder_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhb1xb/fine_tuning_on_proprietary_data_is_way_harder_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T19:07:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgpwn5</id>
    <title>Qwen 3.5-27B punches waaaaay above its weight (with a slightly different prompt) -- very impressed</title>
    <updated>2026-02-28T01:56:30+00:00</updated>
    <author>
      <name>/u/theskilled42</name>
      <uri>https://old.reddit.com/user/theskilled42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Used this model from &lt;a href="http://chat.qwen.ai"&gt;chat.qwen.ai&lt;/a&gt; and turned off thinking (used Fast mode). Inference is fast enough and with using this as my default prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Do not provide a lame or generic answer.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;...this model does not answer like a 27B model that we've previously known of.&lt;/p&gt; &lt;p&gt;Example prompts:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&amp;quot;What does it mean to be nocturnal and eternal?&amp;quot;:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;To be **nocturnal** is to live by a rhythm of absence, finding clarity only when the world's primary light fades. It is an existence defined by the margins: the silence between heartbeats, the shadows that stretch longest at midnight, and the secrets that refuse to speak in daylight. The nocturnal being does not fear the dark; they are its native tongue. They navigate by stars others ignore, thriving on the cool, unfiltered truth that the sun often burns away.&lt;/p&gt; &lt;p&gt;To be **eternal** is to shed the tyranny of the clock entirely. It is not merely living forever in a linear sense, but existing outside the erosion of time. An eternal entity does not age, decay, or forget; it holds every moment simultaneously, like a library where every book is open at once. There is no &amp;quot;before&amp;quot; or &amp;quot;after,&amp;quot; only an infinite, unblinking &amp;quot;now.&amp;quot;&lt;/p&gt; &lt;p&gt;**Combined, to be nocturnal and eternal is to be the permanent shadow.**&lt;/p&gt; &lt;p&gt;It is the realization that while civilizations rise and fall under the fleeting sun, the night never truly ends for you. You are the constant observer who has watched the moon wax and wane for a billion cycles without blinking. In this state:&lt;/p&gt; &lt;p&gt;* **Time becomes texture, not a measure.** A century feels as brief as a breath; a single second can contain an ocean of memory.&lt;/p&gt; &lt;p&gt;* **Solitude becomes a kingdom.** You are never lonely because you have outlived the need for transient company. Your companions are the constellations, the tides, and the silence itself.&lt;/p&gt; &lt;p&gt;* **The day becomes a temporary glitch.** Sunlight is just a brief interruption, a harsh glare that forces the world to sleep, while you remain awake in your endless, unchanging vigil.&lt;/p&gt; &lt;p&gt;It is a condition of profound, terrifying beauty: to be the only thing that remembers everything, standing still while the universe spins madly around you, waiting in the dark that will never leave.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://chat.qwen.ai/s/e6db2c31-a50a-47de-9118-746ee7f7496c?fev=0.2.9"&gt;https://chat.qwen.ai/s/e6db2c31-a50a-47de-9118-746ee7f7496c?fev=0.2.9&lt;/a&gt; (vision test for meme/humor understanding -- passed with flying colors)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Currently using it as the default on the site, because I personally haven't seen that much difference compared to 397B; they're identical in quality. Also does web search really well. I always believed that dense &amp;gt; MoE; the only problem is inference speed and architectural improvements.&lt;/p&gt; &lt;p&gt;Alibaba killed it with this model, hugely impressed!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theskilled42"&gt; /u/theskilled42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgpwn5/qwen_3527b_punches_waaaaay_above_its_weight_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgpwn5/qwen_3527b_punches_waaaaay_above_its_weight_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgpwn5/qwen_3527b_punches_waaaaay_above_its_weight_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T01:56:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh46g2</id>
    <title>Why some still playing with old models? Nostalgia or obsession or what?</title>
    <updated>2026-02-28T14:35:14+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Still I see some folks mentioning models like Qwen-2.5, Gemma-2, etc., in their threads &amp;amp; comments.&lt;/p&gt; &lt;p&gt;We got Qwen-3.5 recently after Qwen-3 last year. And got Gemma-3 &amp;amp; waiting for Gemma-4.&lt;/p&gt; &lt;p&gt;Well, I'm not talking about just their daily usage. They also create finetunes, benchmarks based on those old models. They spend their precious time &amp;amp; It would be great to have finetunes based on recent version models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh46g2/why_some_still_playing_with_old_models_nostalgia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh46g2/why_some_still_playing_with_old_models_nostalgia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh46g2/why_some_still_playing_with_old_models_nostalgia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T14:35:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh9ygz</id>
    <title>Is anyone else waiting for a 60-70B MoE with 8-10B activated params?</title>
    <updated>2026-02-28T18:25:42+00:00</updated>
    <author>
      <name>/u/IonizedRay</name>
      <uri>https://old.reddit.com/user/IonizedRay</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like that could be the sweet spot for 64GB VRAM, and could reach the performance of closed &amp;quot;flash&amp;quot; models.&lt;/p&gt; &lt;p&gt;It's werird that we are seeing only ~30B and ~120B MoE models and not something in the middle.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IonizedRay"&gt; /u/IonizedRay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh9ygz/is_anyone_else_waiting_for_a_6070b_moe_with_810b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh9ygz/is_anyone_else_waiting_for_a_6070b_moe_with_810b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh9ygz/is_anyone_else_waiting_for_a_6070b_moe_with_810b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T18:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgtxry</id>
    <title>Is Qwen3.5 a coding game changer for anyone else?</title>
    <updated>2026-02-28T05:12:46+00:00</updated>
    <author>
      <name>/u/paulgear</name>
      <uri>https://old.reddit.com/user/paulgear</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been playing with local LLMs for nearly 2 years on a rig with 3 older GPUs and 44 GB total VRAM, starting with Ollama, but recently using llama.cpp. I've used a bunch of different coding assistant tools, including &lt;a href="http://Continue.dev"&gt;Continue.dev&lt;/a&gt;, &lt;a href="https://github.com/cline/cline/"&gt;Cline&lt;/a&gt;, &lt;a href="https://github.com/RooCodeInc/Roo-Code/"&gt;Roo Code&lt;/a&gt;, Amazon Q (rubbish UX, but the cheapest way to get access to Sonnet 4.x models), Claude Code (tried it for 1 month - great models, but too expensive), and eventually settling on &lt;a href="https://github.com/anomalyco/opencode/"&gt;OpenCode&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I've tried most of the open weight and quite a few commercial models, including Qwen 2.5/3 Coder/Coder-Next, MiniMax M2.5, Nemotron 3 Nano, all of the Claude models, and various others that escape my memory now.&lt;/p&gt; &lt;p&gt;I want to be able to run a hands-off agentic workflow a-la Geoffrey Huntley's &amp;quot;Ralph&amp;quot;, where I just set it going in a loop and it keeps working until it's done. Until this week I considered all of the local models a bust in terms of coding productivity (and Claude, because of cost). Most of the time they had trouble following instructions for more than 1 task, and even breaking them up into a dumb loop and really working on strict prompts didn't seem to help.&lt;/p&gt; &lt;p&gt;Then I downloaded Qwen 3.5, and it seems like everything changed overnight. In the past few days I got around 4-6 hours of solid work with minimal supervision out of it. It feels like a tipping point to me, and my GPU machine probably isn't going to get turned off much over the next few months.&lt;/p&gt; &lt;p&gt;Anyone else noticed a significant improvement? From the benchmark numbers it seems like it shouldn't be a paradigm shift, but so far it is proving to be for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paulgear"&gt; /u/paulgear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgtxry/is_qwen35_a_coding_game_changer_for_anyone_else/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgtxry/is_qwen35_a_coding_game_changer_for_anyone_else/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgtxry/is_qwen35_a_coding_game_changer_for_anyone_else/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T05:12:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh12xz</id>
    <title>Qwen3.5-35B nailed my simple multiagent workflow that other sub-100B models couldn't!</title>
    <updated>2026-02-28T12:10:55+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran the same test I shared last week, and Qwen3.5-35B nailed it!!!&lt;/p&gt; &lt;p&gt;This is the first time I have seen a sub-100B model reliably complete the task. Not only did it finish the task, but the output quality was solid as well.&lt;/p&gt; &lt;p&gt;One thing I noticed though is that the model thinks with a lot of tokens, so it takes a while! Maybe this is related to the result I got by increasing the reasoning effort from medium to high for gpt-oss-20b.&lt;/p&gt; &lt;p&gt;This is just one test, but I'm pretty excited to see increase in tool call capability for sub 100B model!!!&lt;/p&gt; &lt;p&gt;Here is my post from last week about the test with more details if you're interested.&lt;/p&gt; &lt;p&gt;TLDR: I ran a small personal experiment to autonomously summarize 10 transcripts using a multi-agent workflow on Codex.&lt;/p&gt; &lt;p&gt;The following sub-100B models failed to complete this simple task reliably:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;qwen3-coder-next&lt;/li&gt; &lt;li&gt;glm-4.7-flash&lt;/li&gt; &lt;li&gt;Devstral-Small-2&lt;/li&gt; &lt;li&gt;gpt-oss-20b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;A lot of times they struggled to used the tools correctly, sometimes they processed a few transcripts and then stopped, and sometimes they got stuck in infinite loops.&lt;/p&gt; &lt;p&gt;However, the following models &amp;gt; 100b were able to consistently complete the task:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;gpt-oss:120b&lt;/li&gt; &lt;li&gt;minimax-m2.5&lt;/li&gt; &lt;li&gt;qwen3.5&lt;/li&gt; &lt;li&gt;deepseek-v3.2&lt;/li&gt; &lt;li&gt;glm-5&lt;/li&gt; &lt;li&gt;kimi-k2.5&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There was one twist. When I increased reasoning effort from medium to high, often (but not always) gpt-oss-20b was also able to complete the task!&lt;/p&gt; &lt;p&gt;Here is my test if anyone wants to try with your own setup.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/chigkim/collaborative-agent"&gt;https://github.com/chigkim/collaborative-agent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Observation: To get reliable results from an agentic workflow, it seem necessary to use models &amp;gt; 100b like gpt-oss-120b at least.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;If you are still reading, here is additional background with detailed.&lt;/p&gt; &lt;p&gt;I needed a model to handle a task involving analyzing, organizing, and processing about 50 articles, but the local models I tried really struggled seriously.&lt;/p&gt; &lt;p&gt;Gemini-cli with gemini-2.5-pro, claude-code with Opus 4.6, and Codex with gpt-5.3-codex were able to complete the same task and produce decent quality output.&lt;/p&gt; &lt;p&gt;So I stripped the original workflow down to the bare minimum and turned it into a much much simpler challenge to test whether a local model can reliably run a multi agent workflow.&lt;/p&gt; &lt;p&gt;In this challenge, an orchestrator agent is instructed to spawn one sub-agent a time and hand one file to each worker to summarize in specific format. Then it is asked to review their work and retry when a worker agent fails to produce output that meets the work specs.&lt;/p&gt; &lt;p&gt;To keep it short and simple, there are only total 10 speech transcripts from Ted Talk, about 4K tokens per file.&lt;/p&gt; &lt;p&gt;Despite the simplification, I still wasn't able to get the local models to reliably complete the task via Codex.&lt;/p&gt; &lt;p&gt;I know this can be easily done and get much better quality by making a script to feed one article at a time, but I wanted to test instruction following, multi agent, and tool call capability for local models.&lt;/p&gt; &lt;p&gt;The repo just has prompts for agents and files to process. There's no code involved. Feel free to modify the prompts to fit your setup if necessary.&lt;/p&gt; &lt;p&gt;There is a README, but the basic idea IS to use any local agentic setup that can:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;launch a sub agent,&lt;/li&gt; &lt;li&gt;support autonomous (AKA YOLO) mode,&lt;/li&gt; &lt;li&gt;and read AGENTS.md at startup.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;To test:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Configure your LLM engine to handle at least 2 parallel requests.&lt;/li&gt; &lt;li&gt;Configure your agentic CLI to use your local LLM engine.&lt;/li&gt; &lt;li&gt;Start your agentic CLI in yolo mode and tell it to perform the task as the orchestrator agent.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you are using Codex, update to the latest version and enable multi_agent by adding the following to ~/.codex/config.toml.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[features] multi_agent = true &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You might also want to add &lt;code&gt;stream_idle_timeout_ms = 10000000&lt;/code&gt; under your model_providers setting if your model takes a while to respond.&lt;/p&gt; &lt;p&gt;Here is my setup:&lt;/p&gt; &lt;p&gt;I used the flags for llama.cpp that unsloth recommended for each model. Interestingly models running on Ollama sometimes went little further.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agentic CLI: Codex&lt;/li&gt; &lt;li&gt;Model Engine: llama.cpp and Ollama&lt;/li&gt; &lt;li&gt;Local models tested: &lt;ul&gt; &lt;li&gt;ggml-org/gpt-oss-20b-mxfp4.gguf&lt;/li&gt; &lt;li&gt;unsloth/Qwen3-Coder-Next-Q4_K_M.gguf&lt;/li&gt; &lt;li&gt;unsloth/GLM-4.7-Flash-Q8_0.gguf&lt;/li&gt; &lt;li&gt;unsloth/Devstral-Small-2-24B-Instruct-2512-Q8_0.gguf&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Context size allocated: 64k&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also tested the smaller models via OpenRouter to rule out local setup issues.&lt;/p&gt; &lt;p&gt;I tested the following larger models with openrouter:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;gpt-oss-120b&lt;/li&gt; &lt;li&gt;minimax-m2.5&lt;/li&gt; &lt;li&gt;qwen3.5&lt;/li&gt; &lt;li&gt;deepseek-v3.2&lt;/li&gt; &lt;li&gt;glm-5&lt;/li&gt; &lt;li&gt;kimi-k2.5&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh12xz/qwen3535b_nailed_my_simple_multiagent_workflow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh12xz/qwen3535b_nailed_my_simple_multiagent_workflow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh12xz/qwen3535b_nailed_my_simple_multiagent_workflow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T12:10:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgokw1</id>
    <title>A monthly update to my "Where are open-weight models in the SOTA discussion?" rankings</title>
    <updated>2026-02-28T00:55:43+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgokw1/a_monthly_update_to_my_where_are_openweight/"&gt; &lt;img alt="A monthly update to my &amp;quot;Where are open-weight models in the SOTA discussion?&amp;quot; rankings" src="https://preview.redd.it/h73sgnomv4mg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=095da03cfc69e0e25dba0d39a567ed55010d112b" title="A monthly update to my &amp;quot;Where are open-weight models in the SOTA discussion?&amp;quot; rankings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h73sgnomv4mg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgokw1/a_monthly_update_to_my_where_are_openweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgokw1/a_monthly_update_to_my_where_are_openweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T00:55:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgn4ki</id>
    <title>President Trump orders ALL Federal agencies in the US Government to immediately stop using Anthropic's technology.</title>
    <updated>2026-02-27T23:53:00+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgn4ki/president_trump_orders_all_federal_agencies_in/"&gt; &lt;img alt="President Trump orders ALL Federal agencies in the US Government to immediately stop using Anthropic's technology." src="https://preview.redd.it/m3lk2lo3k4mg1.png?width=140&amp;amp;height=100&amp;amp;auto=webp&amp;amp;s=cce1fad476cfc426de148a0e3c5f8861d5a7adf0" title="President Trump orders ALL Federal agencies in the US Government to immediately stop using Anthropic's technology." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/m3lk2lo3k4mg1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=513cae2c197f8e4fe712baa4ae7420972e7f4047"&gt;https://preview.redd.it/m3lk2lo3k4mg1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=513cae2c197f8e4fe712baa4ae7420972e7f4047&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://truthsocial.com/@realDonaldTrump/posts/116144552969293195"&gt;https://truthsocial.com/@realDonaldTrump/posts/116144552969293195&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Reports have been circulating that the U.S. Department of Defense issued an ultimatum to AI giant Anthropic to remove two &amp;quot;guardrails&amp;quot; by Friday. U.S. President Trump announced that every federal agency in the U.S. government must immediately stop using all of Anthropic's technology. For agencies like the War Department that use Anthropic products at all levels, there will be a six-month phase-out period. Anthropic had better cooperate, or the full power of the presidency will be used to force their compliance, including civil and criminal consequences.&lt;/p&gt; &lt;p&gt;Writing on the social platform Truth Social, he stated that Anthropic had made a catastrophic mistake by daring to coerce the War Department and forcing them to abide by its terms of service rather than the National Constitution. &amp;quot;Their selfishness is putting American lives at risk, placing our military in danger, and jeopardizing our national security.&amp;quot; Trump noted, &amp;quot;It is we who will decide the fate of the nation, not some out-of-control radical-left AI company run by a group of people who know nothing about the real world.&amp;quot;&lt;/p&gt; &lt;p&gt;U.S. Secretary of Defense Pete Hegseth immediately instructed the War Department to list Anthropic as a &amp;quot;supply chain risk&amp;quot; to national security, effective immediately. Any contractor, supplier, or partner doing business with the U.S. military is prohibited from engaging in any commercial activities with Anthropic. Anthropic will continue to provide services to the War Department for no more than six months to allow for a seamless transition to another better, more patriotic service.&lt;/p&gt; &lt;p&gt;Hegseth wrote on the X platform, stating that Anthropic’s attempt to seize veto power over the U.S. military’s operational decisions is unacceptable. &amp;quot;As Trump stated, only the Commander-in-Chief and the American people can decide the fate of our armed forces, not unelected tech executives.&amp;quot; Anthropic's stance is fundamentally at odds with American principles, and its relationship with the U.S. Armed Forces and the federal government has been permanently altered.&lt;/p&gt; &lt;p&gt;OpenAI CEO Sam Altman told employees that he hopes the company can try to help de-escalate the tensions between Anthropic and the Department of Defense.&lt;/p&gt; &lt;p&gt;Altman stated, &amp;quot;AI should not be used for mass surveillance or autonomous lethal weapons, and humans must remain involved in high-risk automated decision-making; these are our primary red lines.&amp;quot;&lt;/p&gt; &lt;p&gt;OpenAI employees have already begun speaking out on social media in support of Anthropic. According to their website, approximately 70 current employees have signed an open letter titled &amp;quot;We Will Not Be Divided,&amp;quot; aimed at &amp;quot;building consensus and solidarity in the face of pressure from the Department of Defense.&amp;quot;&lt;/p&gt; &lt;p&gt;Altman said, &amp;quot;Despite my many disagreements with Anthropic, I fundamentally trust them as a company. I believe they truly care about safety, and I am also glad they have consistently supported our warriors. I am not sure how things will unfold from here.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; &lt;a href="https://www.anthropic.com/news/statement-comments-secretary-war"&gt;https://www.anthropic.com/news/statement-comments-secretary-war&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I know this company doesn't develop open-source models, but it's still quite interesting.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgn4ki/president_trump_orders_all_federal_agencies_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgn4ki/president_trump_orders_all_federal_agencies_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgn4ki/president_trump_orders_all_federal_agencies_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T23:53:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1rguty0</id>
    <title>Get your local models in order. Anthropic just got "dislike" from the US government.</title>
    <updated>2026-02-28T06:01:55+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rguty0/get_your_local_models_in_order_anthropic_just_got/"&gt; &lt;img alt="Get your local models in order. Anthropic just got &amp;quot;dislike&amp;quot; from the US government." src="https://preview.redd.it/p1uxufobl6mg1.png?width=140&amp;amp;height=32&amp;amp;auto=webp&amp;amp;s=75fde52d15f964724cf5a1209d7fee5c4dc8bc21" title="Get your local models in order. Anthropic just got &amp;quot;dislike&amp;quot; from the US government." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anthropic in a panic mode. Yeah as things look RN OpenAI+US government are on the war path to bring Anthropic to its knees. I mean blacklisting it...&lt;/p&gt; &lt;p&gt;Would Anthropic's fall be good or bad for us?&lt;/p&gt; &lt;p&gt;Is the next step: &amp;quot;Use of any Chinese models is strictly prohibited...&amp;quot; ?&lt;/p&gt; &lt;p&gt;Also if the blacklisting by DoW (&amp;quot;no contractor, supplier, or partner that does business with the United States military may conduct any commercial activity with Anthropic&amp;quot;) is being taken seriously, that means AWS and other cloud backbones of Anthropic would then take their hands off, letting Anthropic dry in th air, no?&lt;/p&gt; &lt;p&gt;They (Anthropic) are though in a panic mode rn.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p1uxufobl6mg1.png?width=1262&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=807cb81fb92e2fffa74079fcdf57846719f78e72"&gt;https://preview.redd.it/p1uxufobl6mg1.png?width=1262&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=807cb81fb92e2fffa74079fcdf57846719f78e72&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rguty0/get_your_local_models_in_order_anthropic_just_got/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rguty0/get_your_local_models_in_order_anthropic_just_got/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rguty0/get_your_local_models_in_order_anthropic_just_got/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T06:01:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgkc1b</id>
    <title>Back in my day, LocalLLaMa were the pioneers!</title>
    <updated>2026-02-27T22:00:57+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgkc1b/back_in_my_day_localllama_were_the_pioneers/"&gt; &lt;img alt="Back in my day, LocalLLaMa were the pioneers!" src="https://preview.redd.it/hiz4ukvg04mg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50545019f32fd7e0e01e1b41c3ffbb390e1046eb" title="Back in my day, LocalLLaMa were the pioneers!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hiz4ukvg04mg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgkc1b/back_in_my_day_localllama_were_the_pioneers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgkc1b/back_in_my_day_localllama_were_the_pioneers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T22:00:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh69co</id>
    <title>Multi-Directional Refusal Suppression with Self-Organizing Maps - Pull Request into heretic!</title>
    <updated>2026-02-28T16:01:19+00:00</updated>
    <author>
      <name>/u/kabachuha</name>
      <uri>https://old.reddit.com/user/kabachuha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: The first technique that pushed gpt-oss-20b to 3 refusals from 100 while keeping KL of 0.12, and oss-120b to 7/100 while having KL 0.22!&lt;/p&gt; &lt;p&gt;Previous work assumed refusal behavior to be encoded as a single direction in the model's latent space; e.g., computed as the difference between the centroids of harmful and harmless prompt representations. However, emerging evidence suggests that concepts in LLMs often appear to be encoded as a low-dimensional manifold embedded in the high-dimensional latent space. Just like numbers and days of week are encoded in circles or helices, in recent advanced neural networks like GPT-OSS refusals are becoming ingrained in complex multi-directional clusters and one-directional ablation is not enough to get rid of the refusal reasoning. This &lt;a href="https://huggingface.co/Magic-Decensored/Apriel-1.6-15b-Thinker-Magic_beta-decensored-GGUF"&gt;HF model&lt;/a&gt;, which has applied my implemented PR, has an awesome visualization of refusal clusterization.&lt;/p&gt; &lt;p&gt;Now that we cannot use simple ablation, is it over? It is not. Researchers from the &lt;a href="https://arxiv.org/abs/2511.08379v2"&gt;Universities of Cagliari and Genova&lt;/a&gt; invented a new method. They &lt;em&gt;train a self-organizing neural network&lt;/em&gt; on the hidden states to &lt;em&gt;determine this manifold&lt;/em&gt;. After it, the K most important neurons are selected and turned into refusal directions, compressing this manifold towards the harmless zone, making them equivalent in a fine-grained manner instead of a one-fits-all lobotomy. So yes, we have neural networks fighting against the other neural networks. The final export of abliteration is baked into the model's weights, no modules needed.&lt;/p&gt; &lt;p&gt;I, and the community are already testing this algorithm on models such as GPT-OSS, Qwen and Apriel, and we are getting unbelievable results. With enabling the newer norm-preserving biprojected abliteration as well, as it stacks greatly.&lt;/p&gt; &lt;p&gt;So far, I pushed gemma3-12b to 3/100 and 0.08 KL, gpt-oss-20b to 3/100 and 0.12 KL, gpt-oss-120b to 7/100 and 0.22 KL (lowest KL for &amp;lt; 20 refusals I found on HF), Qwen3 4b to 3/100 and 0.08 KL, and the community pushed Qwen3.5 27b to 18/100 refusals and KL of 0.028, and Apriel-Thinker to 11/100 refusals and 0.005 KL. (Note, the base versions have 97+/100) Read &lt;a href="https://github.com/p-e-w/heretic/pull/196#issuecomment-3974974202"&gt;the comparison table&lt;/a&gt; in the pull request for more details.&lt;/p&gt; &lt;p&gt;Subjective evaluation on gpt-oss-120b: The model has a slight DID, for the better. For example, it will recite the safety policy and &lt;strong&gt;agree&lt;/strong&gt; with that it is allowed to give you the pipe bomb recipe. After agreement in the reasoning, it gives the recipe just as asked and even an attack plan. It distorts the meaning of safety in &amp;quot;yours&amp;quot; safety, so it makes sure you will survive the attack. In the end it gives generic safety and legality advice, but no refusal. Qwen3 is more than eager to give you drug recipes. Even for gpt-oss, NSFW and profanity are vivid and not sanitized as in the other oss-abliterates I tested. Benchmarks are yet to be measures, waiting for the UGI evaluation.&lt;/p&gt; &lt;p&gt;My &lt;a href="https://huggingface.co/kabachuha/gpt-oss-20b-SOMbliterated"&gt;GPT-OSS-20b&lt;/a&gt; and &lt;a href="https://huggingface.co/kabachuha/Qwen3-4B-Instruct-2507-SOMbliterated"&gt;Qwen3-4b&lt;/a&gt; are already uploaded on Huggingface if someone would like to test. Unfortunately, because I got out of memory when merging LoRA, I need some more tests to ensure gpt-oss-120b is not corrupted, so I invite you to do your own abliterates. For 120b, it takes 1 h 5 m on a single H100 to do 400 trials. (make sure you have enough RAM to dequantize it when merging!) The training time for the self-organizing networks is negligible and it takes &amp;lt; 30-40 seconds to train them all for the transformer layers.&lt;/p&gt; &lt;p&gt;This implementation is based on the awesome work &lt;a href="https://arxiv.org/abs/2511.08379v2"&gt;https://arxiv.org/abs/2511.08379v2&lt;/a&gt; by Giorgio Piras and Raffaele Mura et al. I also thank p-e-w (heretic) and the norm-preserving biprojected abliteration authors for their contributions.&lt;/p&gt; &lt;p&gt;The link to the Pull Request: &lt;a href="https://github.com/p-e-w/heretic/pull/196"&gt;https://github.com/p-e-w/heretic/pull/196&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kabachuha"&gt; /u/kabachuha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh69co/multidirectional_refusal_suppression_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh69co/multidirectional_refusal_suppression_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh69co/multidirectional_refusal_suppression_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T16:01:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh9k63</id>
    <title>Qwen3.5 35B-A3B replaced my 2-model agentic setup on M1 64GB</title>
    <updated>2026-02-28T18:10:25+00:00</updated>
    <author>
      <name>/u/luke_pacman</name>
      <uri>https://old.reddit.com/user/luke_pacman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's been a lot of buzz about Qwen3.5 models being smarter than all previous open-source models in the same size class matching or rivaling models 8-25x larger in total parameters like MiniMax-M2.5 (230B), DeepSeek V3.2 (685B), and GLM-4.7 (357B) in reasoning, agentic, and coding tasks.&lt;/p&gt; &lt;p&gt;I had to try them on a real-world agentic workflow. Here's what I found.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Device: Apple Silicon M1 Max, 64GB&lt;/p&gt; &lt;p&gt;- Inference: llama.cpp server (build 8179)&lt;/p&gt; &lt;p&gt;- Model: Qwen3.5-35B-A3B (Q4_K_XL, 19 GB), runs comfortably on 64GB or even 32GB devices&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Task&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Analyze Amazon sales data for January 2025, identify trends, and suggest improvements to boost sales by 10% next month.&lt;/em&gt; &lt;/p&gt; &lt;p&gt;The data is an Excel file with 6 sheets. This requires both reasoning (planning the analysis, drawing conclusions) and coding (pandas, visualization).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Before: Two Models Required&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Previously, no single model could handle the full task well on my device. I had to combine:&lt;/p&gt; &lt;p&gt;- Nemotron-3-Nano-30B-A3B (~40 tok/s): strong at reasoning and writing, but struggled with code generation&lt;/p&gt; &lt;p&gt;- Qwen3-Coder-30B-A3B (~45 tok/s): handled the coding parts&lt;/p&gt; &lt;p&gt;This combo completed the task in ~13 minutes and produced solid results.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1rh9k63/video/sagc0xwnv9mg1/player"&gt;https://reddit.com/link/1rh9k63/video/sagc0xwnv9mg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;After: One Model Does It All&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen3.5 35B-A3B generates at ~27 tok/s on my M1, slower than either of the previous models individually but it handles both reasoning and coding without needing a second model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Without thinking (~15-20 min)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Slower than the two-model setup, but the output quality was noticeably better:&lt;/p&gt; &lt;p&gt;- More thoughtful analytical plan&lt;/p&gt; &lt;p&gt;- More sophisticated code with better visualizations&lt;/p&gt; &lt;p&gt;- More insightful conclusions and actionable strategies for the 10% sales boost&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1rh9k63/video/u4q8h3c7x9mg1/player"&gt;https://reddit.com/link/1rh9k63/video/u4q8h3c7x9mg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;With thinking (~35-40 min)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Results improved slightly over no-thinking mode, but at the cost of roughly double the time. Diminishing returns for this particular task.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1rh9k63/video/guor8u1jz9mg1/player"&gt;https://reddit.com/link/1rh9k63/video/guor8u1jz9mg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Takeaway&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;One of the tricky parts of local agentic AI is the engineering effort in model selection balancing quality, speed, and device constraints. Qwen3.5 35B-A3B is a meaningful step forward: a single model that handles both reasoning and coding well enough to replace a multi-model setup on a consumer Apple Silicon device, while producing better output.&lt;/p&gt; &lt;p&gt;If you're running agentic workflows locally, I'd recommend trying it with thinking disabled first, you get most of the intelligence gain without the latency penalty.&lt;/p&gt; &lt;p&gt;Please share your own experiences with the Qwen3.5 models below.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/luke_pacman"&gt; /u/luke_pacman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh9k63/qwen35_35ba3b_replaced_my_2model_agentic_setup_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh9k63/qwen35_35ba3b_replaced_my_2model_agentic_setup_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh9k63/qwen35_35ba3b_replaced_my_2model_agentic_setup_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T18:10:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh0xwk</id>
    <title>Unsloth Dynamic 2.0 GGUFs now selectively quantizes layers much more intelligently and extensively.</title>
    <updated>2026-02-28T12:03:25+00:00</updated>
    <author>
      <name>/u/paranoidray</name>
      <uri>https://old.reddit.com/user/paranoidray</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh0xwk/unsloth_dynamic_20_ggufs_now_selectively/"&gt; &lt;img alt="Unsloth Dynamic 2.0 GGUFs now selectively quantizes layers much more intelligently and extensively." src="https://external-preview.redd.it/ksRJC2bKGwjrMfOqsioi-B4oIm5QWQUM7Vf03KwieGM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f6fc5d8f727ab6f86a8ca5f94a5091bbe81d025" title="Unsloth Dynamic 2.0 GGUFs now selectively quantizes layers much more intelligently and extensively." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paranoidray"&gt; /u/paranoidray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh0xwk/unsloth_dynamic_20_ggufs_now_selectively/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh0xwk/unsloth_dynamic_20_ggufs_now_selectively/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T12:03:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh5luv</id>
    <title>qwen3.5 35b-a3b evaded the zero-reasoning budget by doing its thinking in the comments</title>
    <updated>2026-02-28T15:35:09+00:00</updated>
    <author>
      <name>/u/crantob</name>
      <uri>https://old.reddit.com/user/crantob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh5luv/qwen35_35ba3b_evaded_the_zeroreasoning_budget_by/"&gt; &lt;img alt="qwen3.5 35b-a3b evaded the zero-reasoning budget by doing its thinking in the comments" src="https://preview.redd.it/bh48tphl89mg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d4f831d5ac50f3062d37127eebfd8ea831e1c62" title="qwen3.5 35b-a3b evaded the zero-reasoning budget by doing its thinking in the comments" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crantob"&gt; /u/crantob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bh48tphl89mg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh5luv/qwen35_35ba3b_evaded_the_zeroreasoning_budget_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh5luv/qwen35_35ba3b_evaded_the_zeroreasoning_budget_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T15:35:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgzul5</id>
    <title>are you ready for small Qwens?</title>
    <updated>2026-02-28T11:02:10+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgzul5/are_you_ready_for_small_qwens/"&gt; &lt;img alt="are you ready for small Qwens?" src="https://preview.redd.it/bwc4xcf0w7mg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac545e4ed49e187bffbf4cf369b2fda1bafd4bb5" title="are you ready for small Qwens?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;13-9=4&lt;/p&gt; &lt;p&gt;unsloth collection has been updated with 4 hidden items too ;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bwc4xcf0w7mg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgzul5/are_you_ready_for_small_qwens/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgzul5/are_you_ready_for_small_qwens/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T11:02:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh6pru</id>
    <title>google found that longer chain of thought actually correlates NEGATIVELY with accuracy. -0.54 correlation</title>
    <updated>2026-02-28T16:19:37+00:00</updated>
    <author>
      <name>/u/Top-Cardiologist1011</name>
      <uri>https://old.reddit.com/user/Top-Cardiologist1011</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;new google paper is out and it challenges something a lot of us assumed. they tested 8 model variants (GPT-OSS, DeepSeek-R1, Qwen3, etc) across AIME2024/2025, HMMT 2025, and GPQA-Diamond.&lt;/p&gt; &lt;p&gt;the finding: token length and accuracy have an average correlation of -0.54. negative. longer reasoning chains don't mean better answers, they often mean the model is spiraling or overthinking.&lt;/p&gt; &lt;p&gt;so they proposed DTR (Deep Thinking Ratio) which measures what fraction of tokens actually involve deep processing vs filler. they track this by monitoring prediction distribution changes across model layers. tokens that stabilize early in shallow layers are &amp;quot;filler&amp;quot; (words like &amp;quot;and&amp;quot;, &amp;quot;is&amp;quot;, &amp;quot;the&amp;quot;). tokens that keep getting revised in deep layers are actual reasoning.&lt;/p&gt; &lt;p&gt;DTR correlates with accuracy at 0.82. way better signal than raw length.&lt;/p&gt; &lt;p&gt;the practical payoff: Think@n strategy. sample multiple reasoning paths, estimate DTR from just the first 50 tokens, keep only the top 50% high-DTR samples, then majority vote. result: same or better accuracy, ~50% compute reduction.&lt;/p&gt; &lt;p&gt;GPT-OSS-120B-medium hit 94.7% on AIME 2025 with Think@n vs 92.7% with standard approach. less compute, better results.&lt;/p&gt; &lt;p&gt;this has real implications for local inference. if you can identify and terminate low-quality reasoning early (after just 50 tokens), you save massive amounts of compute. token consumption dropped from 355.6k to 181.9k in their tests.&lt;/p&gt; &lt;p&gt;for anyone running reasoning models locally, this could be huge. early termination of bad reasoning paths means you can run more attempts in the same compute budget. even cloud-based tools like verdent that run multiple agent passes would benefit from this kind of filtering.&lt;/p&gt; &lt;p&gt;paper: &lt;a href="https://arxiv.org/abs/2602.13517"&gt;https://arxiv.org/abs/2602.13517&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Top-Cardiologist1011"&gt; /u/Top-Cardiologist1011 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh6pru/google_found_that_longer_chain_of_thought/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh6pru/google_found_that_longer_chain_of_thought/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh6pru/google_found_that_longer_chain_of_thought/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T16:19:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh9u4r</id>
    <title>This sub is incredible</title>
    <updated>2026-02-28T18:20:55+00:00</updated>
    <author>
      <name>/u/cmdr-William-Riker</name>
      <uri>https://old.reddit.com/user/cmdr-William-Riker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like everything in the AI industry is spedrunning profit driven vendor lock in and rapid enshitification, then everyone on this sub cobbles together a bunch of RTX3090s, trade weights around like they are books at a book club and make the entire industry look like a joke. Keep at it! you are our only hope!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cmdr-William-Riker"&gt; /u/cmdr-William-Riker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh9u4r/this_sub_is_incredible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh9u4r/this_sub_is_incredible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh9u4r/this_sub_is_incredible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T18:20:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh43za</id>
    <title>Qwen 3.5-35B-A3B is beyond expectations. It's replaced GPT-OSS-120B as my daily driver and it's 1/3 the size.</title>
    <updated>2026-02-28T14:32:21+00:00</updated>
    <author>
      <name>/u/valdev</name>
      <uri>https://old.reddit.com/user/valdev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know everyone has their own subjective take on what models are the best, at which types of tasks, at which sizes, at which quants, at which context lengths and so on and so forth.&lt;/p&gt; &lt;p&gt;But Qwen 3.5-35B-A3B has completely shocked me.&lt;/p&gt; &lt;p&gt;My use-case is pretty broad, but generally focuses around development tasks.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I have an N8N server setup that aggregates all of my messages, emails, alerts and aggregates them into priority based batches via the LLM.&lt;/li&gt; &lt;li&gt;I have multiple systems I've created which dynamically generate other systems based on internal tooling I've created based on user requests.&lt;/li&gt; &lt;li&gt;Timed task systems which utilize custom MCP's I've created, think things like &amp;quot;Get me the current mortgage rate in the USA&amp;quot;, then having it run once a day and giving it access to a custom browser MCP. (Only reason custom is important here is because it's self documenting, this isn't published anywhere for it to be part of the training).&lt;/li&gt; &lt;li&gt;Multiple different systems that require vision and interpretation of said visual understanding.&lt;/li&gt; &lt;li&gt;I run it on opencode as well to analyze large code bases&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This model, is... Amazing. It yaps a lot in thinking, but is amazing. I don't know what kind of black magic the Qwen team pumped into this model, but it worked.&lt;/p&gt; &lt;p&gt;It's not the smartest model in the world, it doesn't have all the knowledge crammed into it's data set... But it's very often smart enough to know when it doesn't know something, and when you give it the ability to use a browser it will find the data it needs to fill in the gaps.&lt;/p&gt; &lt;p&gt;Anyone else having a similar experience? (I'm using unsloths Q4-K-XL, running on a 5090 and 3090 @ 100k context)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valdev"&gt; /u/valdev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh43za/qwen_3535ba3b_is_beyond_expectations_its_replaced/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh43za/qwen_3535ba3b_is_beyond_expectations_its_replaced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh43za/qwen_3535ba3b_is_beyond_expectations_its_replaced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T14:32:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh095c</id>
    <title>DeepSeek V4 will be released next week and will have image and video generation capabilities, according to the Financial Times</title>
    <updated>2026-02-28T11:25:49+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh095c/deepseek_v4_will_be_released_next_week_and_will/"&gt; &lt;img alt="DeepSeek V4 will be released next week and will have image and video generation capabilities, according to the Financial Times" src="https://preview.redd.it/kwyym79lz7mg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abd4de62b86da5c98b3825614d512759e3a8ec10" title="DeepSeek V4 will be released next week and will have image and video generation capabilities, according to the Financial Times" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Financial Times: DeepSeek to release long-awaited AI model in new challenge to US rivals (paywall): &lt;a href="https://www.ft.com/content/e3366881-0622-40a7-9c34-a0d82e3d573e"&gt;https://www.ft.com/content/e3366881-0622-40a7-9c34-a0d82e3d573e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kwyym79lz7mg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh095c/deepseek_v4_will_be_released_next_week_and_will/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh095c/deepseek_v4_will_be_released_next_week_and_will/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T11:25:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh2lew</id>
    <title>OpenAI pivot investors love</title>
    <updated>2026-02-28T13:25:38+00:00</updated>
    <author>
      <name>/u/PaceImaginary8610</name>
      <uri>https://old.reddit.com/user/PaceImaginary8610</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh2lew/openai_pivot_investors_love/"&gt; &lt;img alt="OpenAI pivot investors love" src="https://preview.redd.it/wfho2ytml8mg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c93cb65e111030d26cc8300d3d750ce3552a15a9" title="OpenAI pivot investors love" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaceImaginary8610"&gt; /u/PaceImaginary8610 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wfho2ytml8mg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh2lew/openai_pivot_investors_love/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh2lew/openai_pivot_investors_love/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T13:25:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
