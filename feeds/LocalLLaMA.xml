<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-17T08:52:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r63fhu</id>
    <title>Why is everything about code now?</title>
    <updated>2026-02-16T07:41:24+00:00</updated>
    <author>
      <name>/u/falconandeagle</name>
      <uri>https://old.reddit.com/user/falconandeagle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hate hate hate how every time a new model comes out its about how its better at coding. What happened to the heyday of llama 2 finetunes that were all about creative writing and other use cases.&lt;/p&gt; &lt;p&gt;Is it all the vibe coders that are going crazy over the models coding abilities??&lt;/p&gt; &lt;p&gt;Like what about other conversational use cases? I am not even talking about gooning (again opus is best for that too), but long form writing, understanding context at more than a surface level. I think there is a pretty big market for this but it seems like all the models created these days are for fucking coding. Ugh.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/falconandeagle"&gt; /u/falconandeagle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r63fhu/why_is_everything_about_code_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r63fhu/why_is_everything_about_code_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r63fhu/why_is_everything_about_code_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T07:41:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r70ohs</id>
    <title>Tiny Aya</title>
    <updated>2026-02-17T08:33:58+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Model Summary&lt;/h1&gt; &lt;p&gt;Cohere Labs Tiny Aya is an open weights research release of a pretrained 3.35 billion parameter model optimized for efficient, strong, and balanced multilingual representation across 70+ languages, including many lower-resourced ones. The model is designed to support downstream adaptation, instruction tuning, and local deployment under realistic compute constraints.&lt;/p&gt; &lt;p&gt;Developed by: &lt;a href="https://cohere.com/"&gt;Cohere&lt;/a&gt; and &lt;a href="https://cohere.com/research"&gt;Cohere&lt;/a&gt; Labs&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Point of Contact: &lt;a href="https://cohere.com/research"&gt;&lt;strong&gt;Cohere Labs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;License: &lt;a href="https://cohere.com/cohere-labs-cc-by-nc-license"&gt;CC-BY-NC&lt;/a&gt;, requires also adhering to &lt;a href="https://docs.cohere.com/docs/c4ai-acceptable-use-policy"&gt;&lt;strong&gt;Cohere Lab's Acceptable Use Policy&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Model: tiny-aya-it-global&lt;/li&gt; &lt;li&gt;Model Size: 3.35B&lt;/li&gt; &lt;li&gt;Context length: 8K input&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For more details about this model family, please check out our &lt;a href="https://cohere.com/blog/cohere-labs-tiny-aya"&gt;blog post&lt;/a&gt; and &lt;a href="https://github.com/Cohere-Labs/tiny-aya-tech-report/blob/main/tiny_aya_tech_report.pdf"&gt;tech report&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;looks like different models are for different families of languages:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/CohereLabs/tiny-aya-earth-GGUF"&gt;https://huggingface.co/CohereLabs/tiny-aya-earth-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/CohereLabs/tiny-aya-fire-GGUF"&gt;https://huggingface.co/CohereLabs/tiny-aya-fire-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/CohereLabs/tiny-aya-water-GGUF"&gt;https://huggingface.co/CohereLabs/tiny-aya-water-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/CohereLabs/tiny-aya-global-GGUF"&gt;https://huggingface.co/CohereLabs/tiny-aya-global-GGUF&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Usage and Limitations&lt;/h1&gt; &lt;h1&gt;Intended Usage&lt;/h1&gt; &lt;p&gt;Tiny Aya is a family of massively multilingual small language models built to bring capable AI to languages that are often underserved by existing models. The models support languages across Indic, East and Southeast Asian, African, European, and Middle Eastern language families, with a deliberate emphasis on low-resource language performance.&lt;/p&gt; &lt;p&gt;Intended applications include multilingual text generation, conversational AI, summarization, translation and cross-lingual tasks, as well as research in multilingual NLP and low-resource language modeling. The models are also suited for efficient deployment in multilingual regions, helping bridge the digital language divide for underrepresented language communities.&lt;/p&gt; &lt;h1&gt;Strengths&lt;/h1&gt; &lt;p&gt;Tiny Aya demonstrates strong open-ended generation quality across its full language coverage, with particularly notable performance on low-resource languages. The model performs well on translation, summarization, and cross-lingual tasks, benefiting from training signal shared across language families and scripts.&lt;/p&gt; &lt;h1&gt;Limitations&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Reasoning tasks.&lt;/strong&gt; The model's strongest performance is on open-ended generation and conversational tasks. Chain-of-thought reasoning tasks such as multilingual math (MGSM) are comparatively weaker.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Factual knowledge.&lt;/strong&gt; As with any language model, outputs may contain incorrect or outdated statements, particularly in lower-resource languages with thinner training data coverage.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Uneven resource distribution.&lt;/strong&gt; High-resource languages benefit from richer training signal and tend to exhibit more consistent quality across tasks. The lowest-resource languages in the model's coverage may show greater variability, and culturally specific nuance, sarcasm, or figurative language may be less reliably handled in these languages.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Task complexity.&lt;/strong&gt; The model performs best with clear prompts and instructions. Highly complex or open-ended reasoning, particularly in lower-resource languages, remains challenging.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r70ohs/tiny_aya/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r70ohs/tiny_aya/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r70ohs/tiny_aya/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T08:33:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1r68z93</id>
    <title>llama-cpp ROCm Prompt Processing speed on Strix Halo / Ryzen AI Max +50-100%</title>
    <updated>2026-02-16T12:59:54+00:00</updated>
    <author>
      <name>/u/Excellent_Jelly2788</name>
      <uri>https://old.reddit.com/user/Excellent_Jelly2788</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r68z93/llamacpp_rocm_prompt_processing_speed_on_strix/"&gt; &lt;img alt="llama-cpp ROCm Prompt Processing speed on Strix Halo / Ryzen AI Max +50-100%" src="https://preview.redd.it/0o14pkcytujg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78c0fa42f3ed7ac5b4feaf2d6cca6b439cf3f367" title="llama-cpp ROCm Prompt Processing speed on Strix Halo / Ryzen AI Max +50-100%" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Edit: As the comments pointed out, this was just a bug that was going on for the last ~2 weeks and we are back to the previous performance.&lt;/p&gt; &lt;p&gt;Prompt Processing on Strix Halo (Ryzen AI Max) with ROCm got way faster for a lot of models in the last couple days when using llamacpp-rocm ( &lt;a href="https://github.com/lemonade-sdk/llamacpp-rocm"&gt;https://github.com/lemonade-sdk/llamacpp-rocm&lt;/a&gt; ).&lt;/p&gt; &lt;p&gt;GLM was comparable to Vulkan already on the old version and didnt see major speedup.&lt;/p&gt; &lt;p&gt;Token Generation is ~ the same&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;PP t/s (depth 0)&lt;/th&gt; &lt;th align="left"&gt;Vulkan&lt;/th&gt; &lt;th align="left"&gt;ROCm 1184 (Feb 11)&lt;/th&gt; &lt;th align="left"&gt;ROCm 1188 (Feb 15)&lt;/th&gt; &lt;th align="left"&gt;ROCm vs ROCm&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Nemotron-3-Nano-30B-A3B-Q8_0&lt;/td&gt; &lt;td align="left"&gt;1043&lt;/td&gt; &lt;td align="left"&gt;501&lt;/td&gt; &lt;td align="left"&gt;990&lt;/td&gt; &lt;td align="left"&gt;+98 %&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-OSS-120B-MXFP4&lt;/td&gt; &lt;td align="left"&gt;555&lt;/td&gt; &lt;td align="left"&gt;261&lt;/td&gt; &lt;td align="left"&gt;605&lt;/td&gt; &lt;td align="left"&gt;+132 %&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Coder-Next-MXFP4-MOE&lt;/td&gt; &lt;td align="left"&gt;539&lt;/td&gt; &lt;td align="left"&gt;347&lt;/td&gt; &lt;td align="left"&gt;615&lt;/td&gt; &lt;td align="left"&gt;+77 %&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM4.7-Flash-UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;953&lt;/td&gt; &lt;td align="left"&gt;923&lt;/td&gt; &lt;td align="left"&gt;985&lt;/td&gt; &lt;td align="left"&gt;+7 %&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Interactive Charts:&lt;/p&gt; &lt;p&gt;&lt;a href="https://evaluateai.ai/benchmarks/?models=Nemotron-3-Nano-30B-A3B-Q8_0&amp;amp;versions=b1184%2Cb1185%2Cb1186%2Cb1187%2CB1187%2Cb1188%2Cb7984%2Cb7993%2Cb7999%2Cb8001%2Cb8054%2Cb8058%2Cb8064%2Cb8067&amp;amp;cpus=AMD+RYZEN+AI+MAX%2B+395+w%2F+Radeon+8060S&amp;amp;slots=mn%2Cci%2Cgm%2Cie"&gt;Nemotron&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://evaluateai.ai/benchmarks/?models=gpt-oss-120b-mxfp4&amp;amp;versions=b1184%2Cb1185%2Cb1186%2Cb1187%2CB1187%2Cb1188%2Cb7984%2Cb7993%2Cb7999%2Cb8001%2Cb8054%2Cb8058%2Cb8064%2Cb8067&amp;amp;cpus=AMD+RYZEN+AI+MAX%2B+395+w%2F+Radeon+8060S&amp;amp;slots=mn%2Cci%2Cgm%2Cie"&gt;GPT-OSS-120B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://evaluateai.ai/benchmarks/?models=Qwen3-Coder-Next-MXFP4_MOE&amp;amp;versions=b1184%2Cb1185%2Cb1186%2Cb1187%2CB1187%2Cb1188%2Cb7984%2Cb7993%2Cb7999%2Cb8001%2Cb8054%2Cb8058%2Cb8064%2Cb8067&amp;amp;cpus=AMD+RYZEN+AI+MAX%2B+395+w%2F+Radeon+8060S&amp;amp;slots=mn%2Cci%2Cgm%2Cie"&gt;Qwen3-Coder&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://evaluateai.ai/benchmarks/?models=GLM-4.7-Flash-UD-Q4_K_XL&amp;amp;versions=b1184%2Cb1185%2Cb1186%2Cb1187%2CB1187%2Cb1188%2Cb7984%2Cb7993%2Cb7999%2Cb8001%2Cb8054%2Cb8058%2Cb8064%2Cb8067&amp;amp;cpus=AMD+RYZEN+AI+MAX%2B+395+w%2F+Radeon+8060S&amp;amp;slots=mn%2Cci%2Cgm%2Cie"&gt;GLM-4.7-Flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Disclaimer: &lt;a href="http://Evaluateai.ai"&gt;Evaluateai.ai&lt;/a&gt; is my project. I ran performance benchmarks for the last week on a variety of models on my AI Max 395+ and a few on a AMD Epyc CPU only system. Next step is comparing the output quality.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Jelly2788"&gt; /u/Excellent_Jelly2788 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0o14pkcytujg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r68z93/llamacpp_rocm_prompt_processing_speed_on_strix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r68z93/llamacpp_rocm_prompt_processing_speed_on_strix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T12:59:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60ety</id>
    <title>Qwen 3.5 will be released today</title>
    <updated>2026-02-16T04:54:20+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/"&gt; &lt;img alt="Qwen 3.5 will be released today" src="https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=ff46b508a3b564db9ac8039bb61d1b0f08588ef3" title="Qwen 3.5 will be released today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sources reveal that Alibaba will open-source its next-generation large model, Qwen3.5, tonight on Lunar New Year's Eve. The model reportedly features a comprehensive innovation in its architecture.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b95152330c1b5ebdb5b7022dd6762ebe1890fd06"&gt;https://preview.redd.it/n8tuw9gmfsjg1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b95152330c1b5ebdb5b7022dd6762ebe1890fd06&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/Sino_Market/status/2023218866370068561?s=20"&gt;https://x.com/Sino_Market/status/2023218866370068561?s=20&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T04:54:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6thoo</id>
    <title>what happened to lucidrains?</title>
    <updated>2026-02-17T02:15:27+00:00</updated>
    <author>
      <name>/u/Whole_Contract_284</name>
      <uri>https://old.reddit.com/user/Whole_Contract_284</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6thoo/what_happened_to_lucidrains/"&gt; &lt;img alt="what happened to lucidrains?" src="https://preview.redd.it/n3fk6fvtryjg1.png?width=140&amp;amp;height=82&amp;amp;auto=webp&amp;amp;s=9ca2f47496287c86829b60b962cfe13bb5b794dc" title="what happened to lucidrains?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;did he change his github handle or make all his repos private? ðŸ‘€&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n3fk6fvtryjg1.png?width=1760&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=828ffd106c912a1a302cd7dd35b6da91be7599f0"&gt;https://preview.redd.it/n3fk6fvtryjg1.png?width=1760&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=828ffd106c912a1a302cd7dd35b6da91be7599f0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whole_Contract_284"&gt; /u/Whole_Contract_284 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6thoo/what_happened_to_lucidrains/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6thoo/what_happened_to_lucidrains/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6thoo/what_happened_to_lucidrains/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T02:15:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6gg04</id>
    <title>Hey, it's lunar new year, and this is not a post about local LLM</title>
    <updated>2026-02-16T17:47:31+00:00</updated>
    <author>
      <name>/u/Vozer_bros</name>
      <uri>https://old.reddit.com/user/Vozer_bros</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am writing this between sounds of fireworks.&lt;/p&gt; &lt;p&gt;I learned everything about LLM, RAG and others stuff related to AI for a longg time here.&lt;/p&gt; &lt;p&gt;May your year be filled with perfect timing, rich flavors, and the joy of creating something truly special.&lt;/p&gt; &lt;p&gt;Happy lunar new year, hereâ€™s to a masterpiece of a year ahead!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vozer_bros"&gt; /u/Vozer_bros &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6gg04/hey_its_lunar_new_year_and_this_is_not_a_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6gg04/hey_its_lunar_new_year_and_this_is_not_a_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6gg04/hey_its_lunar_new_year_and_this_is_not_a_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T17:47:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6ni0k</id>
    <title>The thinking mode of Nanbeige4.1-3B</title>
    <updated>2026-02-16T22:05:17+00:00</updated>
    <author>
      <name>/u/Hefty_Tourist_2226</name>
      <uri>https://old.reddit.com/user/Hefty_Tourist_2226</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Prompt : Hey , &lt;/p&gt; &lt;p&gt;Result : We are given a query that simply says &amp;quot;hey&amp;quot;. This is a very vague query.&lt;br /&gt; As an AI, I need to understand the context and what the user is asking for. However, the query is just a greeting and does not contain a specific question or request.&lt;/p&gt; &lt;p&gt;First, I should consider the possibility that the user might be testing if I am paying attention or might have made a typo. Alternatively, it could be a new conversation starter.&lt;/p&gt; &lt;p&gt;Since the query is extremely minimal, I have a few options:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Politely ask for clarification: Since the query is incomplete or ambiguous, I should ask the user what they need help with.&lt;/li&gt; &lt;li&gt;Assume a context: Sometimes, in a conversational AI setting, &amp;quot;hey&amp;quot; might be a greeting and the user expects a friendly response. However, without further context, it's risky to assume.&lt;/li&gt; &lt;li&gt;Provide a general response: Maybe the user is just saying hello and expects a greeting back, but that seems unlikely because the query is presented as a query to the AI assistant, which typically is used for problem-solving or information seeking.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Looking at the query: &amp;quot;hey&amp;quot; is a common informal greeting. In many contexts, especially in a chat-based assistant, the user might be starting a conversation. However, as an AI designed to assist, I should not assume the user wants just a greeting. Instead, I should be helpful and guide the user to specify their needs.&lt;/p&gt; &lt;p&gt;Best approach: Respond in a friendly manner and ask the user to clarify what they need help with.&lt;/p&gt; &lt;p&gt;But wait: the user might have intended to write something else and only sent &amp;quot;hey&amp;quot; by mistake. Alternatively, it could be a test of the AI's ability to handle ambiguous queries.&lt;/p&gt; &lt;p&gt;Another thought: In some contexts, &amp;quot;hey&amp;quot; might be shorthand for a specific term or acronym. However, without context, that is unlikely.&lt;/p&gt; &lt;p&gt;Given that the query is &amp;quot;hey&amp;quot; and the user is addressing the AI, the most appropriate response is to acknowledge the greeting and prompt the user to state their request.&lt;/p&gt; &lt;p&gt;However, note that the user's query is exactly: &amp;quot;hey&amp;quot; (with a space and then the word). It might be that the user is non-native English speaker and meant to write something else.&lt;/p&gt; &lt;p&gt;As a responsible AI, I should:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Be polite.&lt;/li&gt; &lt;li&gt;Clarify the intent.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me draft a response:&lt;/p&gt; &lt;p&gt;&amp;quot;Hello! ðŸ‘‹ It looks like your message might be a bit unclear. Could you please provide more details about what you need help with?&lt;/p&gt; &lt;p&gt;If you were just saying hello, that's okay too! How can I assist you today? Whether you have a question, need help with a problem, or have a specific topic in mind, feel free to share.&lt;/p&gt; &lt;p&gt;Alternatively, if you meant something else by 'hey', please clarify so I can better assist you.&amp;quot;&lt;/p&gt; &lt;p&gt;But wait, the query is exactly &amp;quot;hey&amp;quot;, so maybe the user is trying to start a conversation and expects me to initiate? However, as an AI assistant, my primary role is to assist, so I should steer the conversation toward a productive direction.&lt;/p&gt; &lt;p&gt;Another angle: In some contexts, &amp;quot;hey&amp;quot; might be a command or a trigger for a specific function. But without context, that is speculative.&lt;/p&gt; &lt;p&gt;Given the above, the safest and most helpful response is to ask for clarification.&lt;/p&gt; &lt;p&gt;However, let me check: the user's query is &amp;quot;hey&amp;quot; and the instruction is to &amp;quot;assistant&amp;quot;. The query might be part of a larger context that I don't have. Since this is a simulated environment, I must work with what is given.&lt;/p&gt; &lt;p&gt;Final decision: Respond politely and ask for clarification.&lt;/p&gt; &lt;p&gt;But note: the user might be testing if I can handle minimal input. In many AI systems, a minimal query like &amp;quot;hey&amp;quot; might be handled by a greeting response. However, as a problem-solving assistant, I should not assume.&lt;/p&gt; &lt;p&gt;I recall that in some challenges, the AI is expected to handle ambiguous queries by asking for clarification.&lt;/p&gt; &lt;p&gt;Therefore, I will respond with a friendly inquiry to clarify the user's needs.&lt;/p&gt; &lt;p&gt;Response draft:&lt;/p&gt; &lt;p&gt;&amp;quot;Hi there! ðŸ‘‹ I see you wrote 'hey'. Could you please let me know what you need help with?&lt;/p&gt; &lt;p&gt;If you have a specific question, a problem to solve, or something you'd like to discuss, just tell me!&lt;/p&gt; &lt;p&gt;Alternatively, if this was a test or if you meant something else, feel free to clarify. I'm here to help!&amp;quot;&lt;/p&gt; &lt;p&gt;But wait, the user might have intended to write &amp;quot;Hey&amp;quot; as in the word, and perhaps they are non-native and meant to write a question. Another possibility: in some contexts, &amp;quot;hey&amp;quot; is used as an interjection, and the user might be starting a story or a scenario. However, without more, it's hard.&lt;/p&gt; &lt;p&gt;Alternatively, the query might be a typo and they meant &amp;quot;Hey!&amp;quot; as in the exclamation, but still.&lt;/p&gt; &lt;p&gt;I think the best course of action is to be proactive and ask.&lt;/p&gt; &lt;p&gt;However, let me consider: the user's query is exactly &amp;quot;hey&amp;quot; and the system prompt might be expecting me to generate a response as if the query is complete. But in reality, as an AI, I should not output without context.&lt;/p&gt; &lt;p&gt;Another thought: in the context of the problem, maybe &amp;quot;hey&amp;quot; is a specific term? For example, in programming, &amp;quot;hey&amp;quot; isn't a standard term. In gaming, etc. But without context, no.&lt;/p&gt; &lt;p&gt;Given all this, I will go with a clarification request.&lt;/p&gt; &lt;p&gt;But note: the user might be expecting a simple acknowledgment. However, as an AI assistant designed for assistance, ignoring the ambiguity would be unhelpful.&lt;/p&gt; &lt;p&gt;Let me see examples from other AIs: typically, when a user sends a vague query, the AI asks for clarification.&lt;/p&gt; &lt;p&gt;Therefore, my response should be:&lt;/p&gt; &lt;p&gt;&amp;quot;Hello! ðŸ‘‹ It seems like your query might be incomplete or unclear. Could you please provide more details about what you're looking for?&lt;/p&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Are you having a technical problem?&lt;/li&gt; &lt;li&gt;Do you need help with a specific subject (like math, coding, science, etc.)?&lt;/li&gt; &lt;li&gt;Is there something else you'd like to discuss?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Just let me know how I can assist you!&amp;quot;&lt;/p&gt; &lt;p&gt;But wait, the query is exactly &amp;quot;hey&amp;quot;, so maybe the user is just saying hello and wants to start a conversation. However, the query is presented as the input to the assistant, so it's more likely that the user is asking the assistant to do something with the word &amp;quot;hey&amp;quot;.&lt;/p&gt; &lt;p&gt;Another interpretation: maybe the user is asking the assistant to respond to the word &amp;quot;hey&amp;quot; in a certain way? For example, in a role-play? But that seems less likely.&lt;/p&gt; &lt;p&gt;Given the ambiguity, I think the best approach is to cover both possibilities: acknowledge the greeting and then ask for the purpose.&lt;/p&gt; &lt;p&gt;Final response structure:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Acknowledge the greeting.&lt;/li&gt; &lt;li&gt;State that I am here to help.&lt;/li&gt; &lt;li&gt;Ask for clarification on what they need.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;But to be concise:&lt;/p&gt; &lt;p&gt;&amp;quot;Hey! ðŸ˜Š How can I assist you today? Please let me know what you need help with!&amp;quot;&lt;/p&gt; &lt;p&gt;Wait, but the user wrote &amp;quot;hey&amp;quot; as the query, so if I respond with &amp;quot;Hey! How can I assist you today?&amp;quot; that might be redundant because the user already said &amp;quot;hey&amp;quot;. However, it could be seen as a friendly acknowledgment and then moving to the purpose.&lt;/p&gt; &lt;p&gt;Alternatively, to avoid duplication:&lt;/p&gt; &lt;p&gt;&amp;quot;Hello! ðŸ‘‹ I see you greeted me with 'hey'. How can I assist you today? Please describe your request or question.&amp;quot;&lt;/p&gt; &lt;p&gt;But that might be a bit formal.&lt;/p&gt; &lt;p&gt;Let me try a natural, conversational tone:&lt;/p&gt; &lt;p&gt;&amp;quot;Hey! ðŸ‘‹ What can I help you with? ðŸ˜Š&amp;quot;&lt;/p&gt; &lt;p&gt;But note: the user's query is &amp;quot;hey&amp;quot;, so if I respond with &amp;quot;Hey! What can I help you with?&amp;quot; it might be seen as the user repeating the greeting and then asking. However, in a chat interface, that is acceptable.&lt;/p&gt; &lt;p&gt;Alternatively, to avoid the user feeling like they are repeating:&lt;/p&gt; &lt;p&gt;&amp;quot;Hello! I see you said 'hey'. How may I assist you today? Please let me know what you'd like to do or what problem you're facing.&amp;quot;&lt;/p&gt; &lt;p&gt;I think the key is to move the conversation forward.&lt;/p&gt; &lt;p&gt;Given the above, I will choose a response that is friendly and prompts for clarification.&lt;/p&gt; &lt;p&gt;But wait: the user might have intended to send a different query and &amp;quot;hey&amp;quot; was a mistake. In that case, asking for clarification is still the best.&lt;/p&gt; &lt;p&gt;Therefore, I will output:&lt;/p&gt; &lt;p&gt;&amp;quot;Hi there! ðŸ‘‹ It looks like your message might be a bit brief. Could you please provide more details about what you need help with?&lt;/p&gt; &lt;p&gt;If you were just saying hello, no worries! How can I assist you today? Whether you have a question, need help with a task, or have a specific topic in mind, I'm here to help.&lt;/p&gt; &lt;p&gt;Let me know how I can assist you!&amp;quot;&lt;/p&gt; &lt;p&gt;However, to make it more concise and natural:&lt;/p&gt; &lt;p&gt;&amp;quot;Hey! ðŸ‘‹ How can I help you today? Please let me know what you need assistance with.&amp;quot;&lt;/p&gt; &lt;p&gt;But the user already said &amp;quot;hey&amp;quot;, so starting with &amp;quot;Hey!&amp;quot; might be redundant. Alternatively, I can ignore the user's &amp;quot;hey&amp;quot; as a greeting and focus on the intent.&lt;/p&gt; &lt;p&gt;Another idea: in some contexts, the AI is designed to detect if the query is a greeting and respond appropriately. Since the query is exactly the word &amp;quot;hey&amp;quot;, and it's a common greeting, the AI might respond with a greeting and then pivot to offering help.&lt;/p&gt; &lt;p&gt;I think the most standard approach in AI assistants is to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Acknowledge the greeting if it is one, but then immediately redirect to the purpose of the assistant.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Example from many chatbots:&lt;br /&gt; User: &amp;quot;hey&amp;quot;&lt;br /&gt; Assistant: &amp;quot;Hello! How can I help you today?&amp;quot;&lt;/p&gt; &lt;p&gt;So I'll go with that.&lt;/p&gt; &lt;p&gt;Final decision: Respond with a friendly greeting and ask how I can assist.&lt;/p&gt; &lt;p&gt;Response:&lt;/p&gt; &lt;p&gt;&amp;quot;Hello! ðŸ‘‹ How can I assist you today? ðŸ˜Š Please feel free to ask me anythingâ€”I'm here to help!&amp;quot;&lt;/p&gt; &lt;p&gt;But to be precise, the user's query was &amp;quot;hey&amp;quot;, so I can also incorporate that:&lt;/p&gt; &lt;p&gt;&amp;quot;Hey! ðŸ‘‹ How can I assist you today? ðŸ˜Š&amp;quot;&lt;/p&gt; &lt;p&gt;Yes, that seems natural.&lt;/p&gt; &lt;p&gt;However, note: the user might have intended &amp;quot;hey&amp;quot; as part of a larger query that was cut off. But without more, we have to work with what's given.&lt;/p&gt; &lt;p&gt;Therefore, I will output a response that is friendly and seeks clarification on the user's needs.&lt;/p&gt; &lt;p&gt;But wait: the problem says &amp;quot;hey&amp;quot; and that's the entire query. In the context of the problem, it might be a trick question to see if the AI can handle minimal input. In that case, the AI should not assume and should ask.&lt;/p&gt; &lt;p&gt;I think I'm overcomplicating. The safe and correct response is to ask for clarification.&lt;/p&gt; &lt;p&gt;Let me write the response:&lt;/p&gt; &lt;p&gt;&amp;quot;Hello! ðŸ‘‹ It seems like your query might be a bit unclear. Could you please provide more details about what you'd like help with?&lt;/p&gt; &lt;p&gt;For instance:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Are you looking for information on a specific topic?&lt;/li&gt; &lt;li&gt;Do you need help with a problem or task?&lt;/li&gt; &lt;li&gt;Is there something else you'd like to discuss?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Just let me know, and I'll be happy to assist!&amp;quot;&lt;/p&gt; &lt;p&gt;But to keep it short:&lt;/p&gt; &lt;p&gt;&amp;quot;Hey! ðŸ‘‹ How can I help you today? Please describe your request.&amp;quot;&lt;/p&gt; &lt;p&gt;I think the short version is better.&lt;/p&gt; &lt;p&gt;However, after reconsideration: since the user wrote exactly &amp;quot;hey&amp;quot;, and in the context of an AI assistant, it's possible that the user is testing if the AI can handle a minimal query. In many AI systems, a query of just &amp;quot;hey&amp;quot; might be handled by a default greeting response. But as a problem-solving AI, I should not default to a greeting without context.&lt;/p&gt; &lt;p&gt;Given that the instruction is to &amp;quot;assistant&amp;quot;, and the query is &amp;quot;hey&amp;quot;, I think the intended behavior is to be helpful.&lt;/p&gt; &lt;p&gt;I found a similar example: in the AI safety or design principles, when input is ambiguous, the AI should ask for clarification.&lt;/p&gt; &lt;p&gt;Therefore, I will output a response that asks for clarification.&lt;/p&gt; &lt;p&gt;Final response:&lt;/p&gt; &lt;p&gt;&amp;quot;Hello! ðŸ‘‹ I see you greeted me with 'hey'. How can I assist you today? Please provide more details about what you need help with. If you have a specific question, problem, or topic in mind, feel free to share!&amp;quot;&lt;/p&gt; &lt;p&gt;But to make it even more natural:&lt;/p&gt; &lt;p&gt;&amp;quot;Hey! ðŸ‘‹ Thanks for saying hello. How can I help you today? Let me know what you need assistance with!&amp;quot;&lt;/p&gt; &lt;p&gt;Yes, that's good.&lt;/p&gt; &lt;p&gt;However, note: the user might not have meant to say &amp;quot;hey&amp;quot; as a greeting but as part of a command. For example, in some contexts, &amp;quot;hey&amp;quot; might be a command to do something. But without context, it's unlikely.&lt;/p&gt; &lt;p&gt;I think the above response is safe.&lt;/p&gt; &lt;p&gt;But wait: the user's query is &amp;quot;hey&amp;quot; and the assistant is supposed to output a response. In the context of this exercise, maybe the expected output is a specific thing? However, the query is too minimal.&lt;/p&gt; &lt;p&gt;Another thought: in the context of the problem, &amp;quot;hey&amp;quot; might be a misspelling of &amp;quot;hey&amp;quot; as in the word, but perhaps the user meant &amp;quot;H E Y&amp;quot; as in initials? Unlikely.&lt;/p&gt; &lt;p&gt;I think I should just go with the clarification approach.&lt;/p&gt; &lt;p&gt;So, my final answer will be a response that politely asks the user to clarify their request.&lt;/p&gt; &lt;p&gt;Let me write it:&lt;/p&gt; &lt;p&gt;&amp;quot;Hello! ðŸ‘‹ It looks like your message might be a bit brief. Could you please provide more details about what you need help with?&lt;/p&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Are you asking a question?&lt;/li&gt; &lt;li&gt;Do you need assistance with a specific task?&lt;/li&gt; &lt;li&gt;Is there a topic you'd like to discuss?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm here to help! ðŸ˜Š&amp;quot;&lt;/p&gt; &lt;p&gt;But to be concise:&lt;/p&gt; &lt;p&gt;&amp;quot;Hey! How can I assist you today? Please let me know what you need.&amp;quot;&lt;/p&gt; &lt;p&gt;I think that's better.&lt;/p&gt; &lt;p&gt;However, since the user said &amp;quot;hey&amp;quot;, starting with &amp;quot;Hey!&amp;quot; is a natural continuation.&lt;/p&gt; &lt;p&gt;So, I'll go with:&lt;/p&gt; &lt;p&gt;&amp;quot;Hey! ðŸ‘‹ How can I help you today? Please describe your request or question.&amp;quot;&lt;/p&gt; &lt;p&gt;Yes.&lt;/p&gt; &lt;p&gt;But note: the user might be non-English speaker and &amp;quot;hey&amp;quot; might be a typo. Still, asking for clarification is good.&lt;/p&gt; &lt;p&gt;Therefore, the response is:&lt;/p&gt; &lt;p&gt;Hey! ðŸ‘‹ How can I help you today? Please let me know what you need assistance with.&lt;/p&gt; &lt;p&gt;But wait, the user's query is the entire input, and I am to generate a response as the assistant. So the assistant's response should be the output.&lt;/p&gt; &lt;p&gt;In the context of this problem, since the query is &amp;quot;hey&amp;quot;, and I am the assistant, I output the above.&lt;/p&gt; &lt;p&gt;However, let me check if there's a standard protocol. In many AI systems, if the query is a greeting, the AI responds with a greeting and offers help.&lt;/p&gt; &lt;p&gt;I think that's acceptable.&lt;/p&gt; &lt;p&gt;Final decision: Output a friendly response that asks for the user's needs.&lt;/p&gt; &lt;p&gt;Response:&lt;/p&gt; &lt;p&gt;&amp;quot;Hello! ðŸ‘‹ How can I assist you today? Please provide more details about what you'd like help with. Whether it's a question, a problem to solve, or something else, I'm here to help!&amp;quot;&lt;/p&gt; &lt;p&gt;But to match the user's tone (which is informal with &amp;quot;hey&amp;quot;), I can use:&lt;/p&gt; &lt;p&gt;&amp;quot;Hey! ðŸ‘‹ How can I help you today? Let me know what you need!&amp;quot;&lt;/p&gt; &lt;p&gt;Yes.&lt;/p&gt; &lt;p&gt;So, I'll output that.&lt;/p&gt; &lt;p&gt;However, note: the user might have intended a different query. Since the problem is presented as &amp;quot;hey&amp;quot;, and I have to respond, I think this is the way.&lt;/p&gt; &lt;p&gt;But wait: the user's query is &amp;quot;hey&amp;quot; and the assistant is to respond. In some contexts, the assistant might be expected to recognize that &amp;quot;hey&amp;quot; is not a valid query and respond accordingly.&lt;/p&gt; &lt;p&gt;I think I'm ready.&lt;/p&gt; &lt;p&gt;Output:&lt;/p&gt; &lt;p&gt;Hey! ðŸ‘‹ How can I assist you today? Please describe your request or question so I can help you effectively. ðŸ˜Š&lt;/p&gt; &lt;p&gt;But to be precise, let's make sure it's clear.&lt;/p&gt; &lt;p&gt;Alternatively, if the user meant &amp;quot;hey&amp;quot; as in the word to be processed (like a string), but that doesn't make sense.&lt;/p&gt; &lt;p&gt;I think the above is fine.&lt;/p&gt; &lt;p&gt;So, the final response from the assistant is:&lt;/p&gt; &lt;p&gt;&amp;quot;Hey! ðŸ‘‹ How can I help you today? Please let me know what you need assistance with. Whether you have a question, a problem to solve, or something else in mind, I'm here to help!&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hefty_Tourist_2226"&gt; /u/Hefty_Tourist_2226 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6ni0k/the_thinking_mode_of_nanbeige413b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6ni0k/the_thinking_mode_of_nanbeige413b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6ni0k/the_thinking_mode_of_nanbeige413b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T22:05:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6z3d4</id>
    <title>Anybody using Vulkan on NVIDIA now in 2026 already?</title>
    <updated>2026-02-17T06:56:19+00:00</updated>
    <author>
      <name>/u/alex20_202020</name>
      <uri>https://old.reddit.com/user/alex20_202020</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I try to use open source. I've recently been trying to run local LLM and currently can use only CPU, even though I have NVIDIA on my old laptop. I'm looking into info if Vulkan can already be used for AI and does it need any additional installations (apart from NVK).&lt;/p&gt; &lt;p&gt;Web search found a year old post about developments (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1j1swtj/vulkan_is_getting_really_close_now_lets_ditch/&lt;/a&gt;), NVK itself seems to be available for gaming, but I could not find info about AI. &lt;/p&gt; &lt;p&gt;If you use Vulkan with LLAMA already, please share your experience and benchmarks (how does it compare to NVIDIA drivers/CUDA). TIA&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alex20_202020"&gt; /u/alex20_202020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6z3d4/anybody_using_vulkan_on_nvidia_now_in_2026_already/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6z3d4/anybody_using_vulkan_on_nvidia_now_in_2026_already/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6z3d4/anybody_using_vulkan_on_nvidia_now_in_2026_already/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T06:56:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6taah</id>
    <title>Qwen3.5-397B-A17B thought chains look very similar to Gemini 3's thought chains.</title>
    <updated>2026-02-17T02:06:18+00:00</updated>
    <author>
      <name>/u/Fit-Spring776</name>
      <uri>https://old.reddit.com/user/Fit-Spring776</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6taah/qwen35397ba17b_thought_chains_look_very_similar/"&gt; &lt;img alt="Qwen3.5-397B-A17B thought chains look very similar to Gemini 3's thought chains." src="https://preview.redd.it/f9wt3vimqyjg1.png?width=140&amp;amp;height=81&amp;amp;auto=webp&amp;amp;s=f4c4e932329fda41cf2d9019669ecf49efe2d414" title="Qwen3.5-397B-A17B thought chains look very similar to Gemini 3's thought chains." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't know if it's just me who noticed this, but the thought chains of Qwen3.5-397B-A17B look somewhat similar to that of Gemini 3's.&lt;/p&gt; &lt;p&gt;I asked a simple question: &amp;quot;Give me a good strawberry cheesecake recipe.&amp;quot;&lt;/p&gt; &lt;p&gt;Here's Qwen's thinking:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f9wt3vimqyjg1.png?width=1658&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=378f6e2af28039051a8d8f6dfd6110e64d1c766a"&gt;https://preview.redd.it/f9wt3vimqyjg1.png?width=1658&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=378f6e2af28039051a8d8f6dfd6110e64d1c766a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i83z6bqoqyjg1.png?width=1644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ccc2540e472737491f24a348fd4258072bd81a44"&gt;https://preview.redd.it/i83z6bqoqyjg1.png?width=1644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ccc2540e472737491f24a348fd4258072bd81a44&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And then Gemini's to the same question:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xtzhfnftpyjg1.png?width=803&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=07125096ddc9c37926fd51a9c48b2710b2d1a27b"&gt;https://preview.redd.it/xtzhfnftpyjg1.png?width=803&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=07125096ddc9c37926fd51a9c48b2710b2d1a27b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Although Gemini's is far shorter, I still think that these thought chains are eerily, but unsurprisingly similar.&lt;/p&gt; &lt;p&gt;In most use-cases, I've found Gemini's step-by-step reasoning process to be extremely efficient, as well as extremely accurate.&lt;/p&gt; &lt;p&gt;What do y'all think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fit-Spring776"&gt; /u/Fit-Spring776 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6taah/qwen35397ba17b_thought_chains_look_very_similar/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6taah/qwen35397ba17b_thought_chains_look_very_similar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6taah/qwen35397ba17b_thought_chains_look_very_similar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T02:06:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6jklq</id>
    <title>Are 20-100B models enough for Good Coding?</title>
    <updated>2026-02-16T19:38:32+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The reason I'm asking this question because some folks(including me) are in self-doubt little bit. Maybe because after seeing threads about comparison with Online models(More than Trillions of parameters).&lt;/p&gt; &lt;p&gt;Of course, we can't expect same coding performance &amp;amp; output from these 20-100B models.&lt;/p&gt; &lt;p&gt;Some didn't even utilize full potential of these local models. I think only 1/3 of folks hit the turbo with these models.&lt;/p&gt; &lt;p&gt;Personally I never tried Agentic coding as my current laptop(just 8GB VRAM + 32GB RAM) is useless for that.&lt;/p&gt; &lt;p&gt;Lets say I have enough VRAM to run Q6/Q8 of these 20-100B models with 128K-256K context.&lt;/p&gt; &lt;p&gt;But are these models enough to do good level coding? Like Agentic Coding .... Solving Leetcode issues, Code analysis, Code reviews, Optimizations, Automations, etc., Of course include Vibe coding at last.&lt;/p&gt; &lt;p&gt;Please share your thoughts. Thanks.&lt;/p&gt; &lt;p&gt;I'm not gonna create(though I can't) Billion dollar company, I just want to create basic level Websites, Apps, Games. That's it. Majority of those creations gonna be Freeware/Opensource.&lt;/p&gt; &lt;p&gt;What models am I talking about? Here below:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPT-OSS-20B&lt;/li&gt; &lt;li&gt;Devstral-Small-2-24B-Instruct-2512&lt;/li&gt; &lt;li&gt;Qwen3-30B-A3B&lt;/li&gt; &lt;li&gt;Qwen3-30B-Coder&lt;/li&gt; &lt;li&gt;Nemotron-3-Nano-30B-A3B&lt;/li&gt; &lt;li&gt;Qwen3-32B&lt;/li&gt; &lt;li&gt;GLM-4.7-Flash&lt;/li&gt; &lt;li&gt;Seed-OSS-36B&lt;/li&gt; &lt;li&gt;Kimi-Linear-48B-A3B&lt;/li&gt; &lt;li&gt;Qwen3-Next-80B-A3B&lt;/li&gt; &lt;li&gt;Qwen3-Coder-Next&lt;/li&gt; &lt;li&gt;GLM-4.5-Air&lt;/li&gt; &lt;li&gt;GPT-OSS-120B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt; : Adding few more models after suggestions from few comments:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Devstral-2-123B-Instruct-2512 - Q4 @ 75GB, Q5 @ 90GB, Q6 @ 100GB&lt;/li&gt; &lt;li&gt;Step-3.5-Flash - Q4 @ 100-120GB&lt;/li&gt; &lt;li&gt;MiniMax-M2.1, 2 - Q4 @ 120-140GB&lt;/li&gt; &lt;li&gt;Qwen3-235B-A22B - Q4 @ 125-135GB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In Future, I'll go up to 200B models after getting additional GPUs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6jklq/are_20100b_models_enough_for_good_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6jklq/are_20100b_models_enough_for_good_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6jklq/are_20100b_models_enough_for_good_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T19:38:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6usc5</id>
    <title>Qwen3.5-397B-A17B local Llama-bench results</title>
    <updated>2026-02-17T03:13:23+00:00</updated>
    <author>
      <name>/u/ubrtnk</name>
      <uri>https://old.reddit.com/user/ubrtnk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6usc5/qwen35397ba17b_local_llamabench_results/"&gt; &lt;img alt="Qwen3.5-397B-A17B local Llama-bench results" src="https://preview.redd.it/4cdzm9pn2zjg1.png?width=140&amp;amp;height=16&amp;amp;auto=webp&amp;amp;s=a39ed4e040d67b2f7b16f6bcc3810c6bd5ff4626" title="Qwen3.5-397B-A17B local Llama-bench results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4cdzm9pn2zjg1.png?width=1687&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d8b0c3a79bc029a2f903d08365bee7788960c3df"&gt;https://preview.redd.it/4cdzm9pn2zjg1.png?width=1687&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d8b0c3a79bc029a2f903d08365bee7788960c3df&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Well, I mean it ran...but it took a LONG time. Running the Q4_K_M unsloth on the latest llama-bench I could pull about an hour ago.&lt;/p&gt; &lt;p&gt;Rig:&lt;br /&gt; EPYC 7402p with 256GB DDR4-2666&lt;br /&gt; 2x3090Ti&lt;/p&gt; &lt;p&gt;Ran ngl at 10 and cpu-moe at 51 for the total 61 layers of the model.&lt;/p&gt; &lt;p&gt;Any recommendations for bumping the numbers up a bit? This is just for testing and seeing how much I can push the AI system while power is cheap after 7pm CST.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ubrtnk"&gt; /u/ubrtnk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6usc5/qwen35397ba17b_local_llamabench_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6usc5/qwen35397ba17b_local_llamabench_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6usc5/qwen35397ba17b_local_llamabench_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T03:13:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1r70ft2</id>
    <title>Could High Bandwidth Flash be Local Inference's saviour?</title>
    <updated>2026-02-17T08:18:13+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r70ft2/could_high_bandwidth_flash_be_local_inferences/"&gt; &lt;img alt="Could High Bandwidth Flash be Local Inference's saviour?" src="https://external-preview.redd.it/cAfiT96SFc2FYsJrwt9QsIxyggovfrz3PXPwxUjYvlg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64d41f3f12a65c8dfa226d23b46acaac55d170e5" title="Could High Bandwidth Flash be Local Inference's saviour?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are starved for VRAM, but in a local setting, a large part of that VRAM requirement is due to model weights.&lt;/p&gt; &lt;p&gt;By putting this on cheaper HBF, if we assume a 10x cost advantage, instead of 32GB VRAM on a GPU, we could put 32GB VRAM plus 256GB of HBF.&lt;/p&gt; &lt;p&gt;With 4 of these, you'd have 128GB of VRAM and 1TB of HBF. Enough to run bigger models. With 8 of them, you could run the largest models locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.eetimes.com/nand-reimagined-in-high-bandwidth-flash-to-complement-hbm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r70ft2/could_high_bandwidth_flash_be_local_inferences/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r70ft2/could_high_bandwidth_flash_be_local_inferences/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T08:18:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6599e</id>
    <title>Qwen3.5-397B-A17B Unsloth GGUFs</title>
    <updated>2026-02-16T09:34:10+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6599e/qwen35397ba17b_unsloth_ggufs/"&gt; &lt;img alt="Qwen3.5-397B-A17B Unsloth GGUFs" src="https://preview.redd.it/zgfpbga5ttjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b525bb85c217819dae77ecab42757b843211d14" title="Qwen3.5-397B-A17B Unsloth GGUFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen releases Qwen3.5ðŸ’œ! Run 3-bit on a 192GB RAM Mac, or 4-bit (MXFP4) on an M3 Ultra with 256GB RAM (or less). Qwen releases the first open model of their Qwen3.5 family. &lt;a href="https://huggingface.co/Qwen/Qwen3.5-397B-A17B"&gt;https://huggingface.co/Qwen/Qwen3.5-397B-A17B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It performs on par with Gemini 3 Pro, Claude Opus 4.5, and GPT-5.2.&lt;/p&gt; &lt;p&gt;Guide to run them: &lt;a href="https://unsloth.ai/docs/models/qwen3.5"&gt;https://unsloth.ai/docs/models/qwen3.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unsloth dynamic GGUFs at: &lt;a href="https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF"&gt;https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Excited for this week! ðŸ™‚&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zgfpbga5ttjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6599e/qwen35397ba17b_unsloth_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6599e/qwen35397ba17b_unsloth_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T09:34:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6pqjr</id>
    <title>Google Deepmind has released their take on multi-agent orchestration they're calling Intelligent AI Delegation</title>
    <updated>2026-02-16T23:32:25+00:00</updated>
    <author>
      <name>/u/Fear_ltself</name>
      <uri>https://old.reddit.com/user/Fear_ltself</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6pqjr/google_deepmind_has_released_their_take_on/"&gt; &lt;img alt="Google Deepmind has released their take on multi-agent orchestration they're calling Intelligent AI Delegation" src="https://preview.redd.it/yzk6z69yyxjg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e970e10f6f97fbff70f951b4f51999abc29e487c" title="Google Deepmind has released their take on multi-agent orchestration they're calling Intelligent AI Delegation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fear_ltself"&gt; /u/Fear_ltself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yzk6z69yyxjg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6pqjr/google_deepmind_has_released_their_take_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6pqjr/google_deepmind_has_released_their_take_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T23:32:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6gx75</id>
    <title>Fine-tuned FunctionGemma 270M for multi-turn tool calling - went from 10-39% to 90-97% accuracy</title>
    <updated>2026-02-16T18:04:20+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6gx75/finetuned_functiongemma_270m_for_multiturn_tool/"&gt; &lt;img alt="Fine-tuned FunctionGemma 270M for multi-turn tool calling - went from 10-39% to 90-97% accuracy" src="https://preview.redd.it/45vz9gsccwjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1eec91fa23850fedabefb7e68dc6cda809eefd2b" title="Fine-tuned FunctionGemma 270M for multi-turn tool calling - went from 10-39% to 90-97% accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google released FunctionGemma a few weeks ago - a 270M parameter model specifically for function calling. Tiny enough to run on a phone CPU at 125 tok/s. The model card says upfront that it needs fine-tuning for multi-turn use cases, and our testing confirmed it: base accuracy on multi-turn tool calling ranged from 9.9% to 38.8% depending on the task.&lt;/p&gt; &lt;p&gt;We fine-tuned it on three different multi-turn tasks using knowledge distillation from a 120B teacher:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Task&lt;/th&gt; &lt;th&gt;Base&lt;/th&gt; &lt;th&gt;Tuned&lt;/th&gt; &lt;th&gt;Teacher (120B)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Smart home control&lt;/td&gt; &lt;td&gt;38.8%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;96.7%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;92.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Banking voice assistant&lt;/td&gt; &lt;td&gt;23.4%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;90.9%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;97.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Shell commands (Gorilla)&lt;/td&gt; &lt;td&gt;9.9%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;96.0%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;97.0%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The smart home and shell command models actually beat the teacher. The banking task is harder (14 functions + ASR noise in the input) but still a massive jump.&lt;/p&gt; &lt;p&gt;All models, training data, and datasets are open:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Smart home model: &lt;a href="https://huggingface.co/distil-labs/distil-home-assistant-functiongemma"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Smart home data: &lt;a href="https://github.com/distil-labs/distil-smart-home"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Voice assistant data: &lt;a href="https://github.com/distil-labs/distil-voice-assistant-banking"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Shell commands data + demo: &lt;a href="https://github.com/distil-labs/distil-SHELLper"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full writeup with methodology: &lt;a href="https://www.distillabs.ai/blog/making-functiongemma-work-multi-turn-tool-calling-at-270m-parameters"&gt;Making FunctionGemma Work: Multi-Turn Tool Calling at 270M Parameters&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We used &lt;a href="https://www.distillabs.ai/"&gt;Distil Labs&lt;/a&gt; (our platform) for the training pipeline. Happy to answer questions about the process, the results, or FunctionGemma in general.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/45vz9gsccwjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6gx75/finetuned_functiongemma_270m_for_multiturn_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6gx75/finetuned_functiongemma_270m_for_multiturn_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T18:04:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6qy55</id>
    <title>Qwen3.5-397B up to 1 million context length</title>
    <updated>2026-02-17T00:22:53+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;262k natively, extensible up to 1M tokens&amp;quot;&lt;/p&gt; &lt;p&gt;Okay, who has tried this? How coherent is it at even 500k tokens? Throw a big code repo in and see if the agent can do work, solve an issue. I know some of you big boys got big rigs. If anyone ever uses past 500k, please don't forget to share with us how performant it was!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6qy55/qwen35397b_up_to_1_million_context_length/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6qy55/qwen35397b_up_to_1_million_context_length/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6qy55/qwen35397b_up_to_1_million_context_length/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T00:22:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r656d7</id>
    <title>Qwen3.5-397B-A17B is out!!</title>
    <updated>2026-02-16T09:29:03+00:00</updated>
    <author>
      <name>/u/lolxdmainkaisemaanlu</name>
      <uri>https://old.reddit.com/user/lolxdmainkaisemaanlu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3.5-397B-A17B"&gt;https://huggingface.co/Qwen/Qwen3.5-397B-A17B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lolxdmainkaisemaanlu"&gt; /u/lolxdmainkaisemaanlu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r656d7/qwen35397ba17b_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r656d7/qwen35397ba17b_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r656d7/qwen35397ba17b_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T09:29:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6t6j9</id>
    <title>smol-IQ2_XS 113.41 GiB (2.46 BPW)</title>
    <updated>2026-02-17T02:01:32+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6t6j9/smoliq2_xs_11341_gib_246_bpw/"&gt; &lt;img alt="smol-IQ2_XS 113.41 GiB (2.46 BPW)" src="https://external-preview.redd.it/xZxgw1JHuf0bFpG8B9XzpkTKh5cT3IwcxpB8iD03pgY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5d16a75500444f9930eebceb22d77efe61fdc80d" title="smol-IQ2_XS 113.41 GiB (2.46 BPW)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No ik_llama.cpp support for today's Qwen3.5-397B-A17B-GGUF yet, but I released a couple mainline llama.cpp imatrix quants including one that will fit in under 128GB.&lt;/p&gt; &lt;p&gt;Its a custom recipe with full Q8_0 for attention so likely about the best in such a small package until we get some ik_llama.cpp SOTA quantization types available.&lt;/p&gt; &lt;p&gt;For similar MoE optimized bigger quants keep an eye on &lt;a href="https://huggingface.co/AesSedai"&gt;https://huggingface.co/AesSedai&lt;/a&gt; who might have something available in the next 6 hours or so... haha...&lt;/p&gt; &lt;p&gt;I've had luck with `opencode` and the mainline llama.cpp autoparser branch, details in the model card as usual. I'll update it once we have ik quants.&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/Qwen3.5-397B-A17B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6t6j9/smoliq2_xs_11341_gib_246_bpw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6t6j9/smoliq2_xs_11341_gib_246_bpw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T02:01:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6f61k</id>
    <title>Google doesn't love us anymore.</title>
    <updated>2026-02-16T17:01:51+00:00</updated>
    <author>
      <name>/u/DrNavigat</name>
      <uri>https://old.reddit.com/user/DrNavigat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been about 125 years of AI since the last Gemma, Google doesn't love us anymore and has abandoned us to Qwen's rational models. I miss the creativity of Gemma's, and also their really useful sizes.&lt;/p&gt; &lt;p&gt;Don't abandon us, Mommy Google, give us Gemma 4!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrNavigat"&gt; /u/DrNavigat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6f61k/google_doesnt_love_us_anymore/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6f61k/google_doesnt_love_us_anymore/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6f61k/google_doesnt_love_us_anymore/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T17:01:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6h3ha</id>
    <title>Difference Between QWEN 3 Max-Thinking and QWEN 3.5 on a Spatial Reasoning Benchmark (MineBench)</title>
    <updated>2026-02-16T18:10:29+00:00</updated>
    <author>
      <name>/u/ENT_Alam</name>
      <uri>https://old.reddit.com/user/ENT_Alam</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6h3ha/difference_between_qwen_3_maxthinking_and_qwen_35/"&gt; &lt;img alt="Difference Between QWEN 3 Max-Thinking and QWEN 3.5 on a Spatial Reasoning Benchmark (MineBench)" src="https://preview.redd.it/6q4jnllcdwjg1.gif?frame=1&amp;amp;width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=417f825a142e023132f72a89b718db9c2b167d19" title="Difference Between QWEN 3 Max-Thinking and QWEN 3.5 on a Spatial Reasoning Benchmark (MineBench)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Honestly it's quite an insane improvement, QWEN 3.5 even had some builds that were closer to (if not better than) Opus 4.6/GPT-5.2/Gemini 3 Pro.&lt;/p&gt; &lt;p&gt;Benchmark: &lt;a href="https://minebench.ai/"&gt;https://minebench.ai/&lt;/a&gt;&lt;br /&gt; Git Repository: &lt;a href="https://github.com/Ammaar-Alam/minebench"&gt;https://github.com/Ammaar-Alam/minebench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/ClaudeAI/comments/1qx3war/difference_between_opus_46_and_opus_45_on_my_3d/"&gt;Previous post comparing Opus 4.5 and 4.6, also answered some questions about the benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/OpenAI/comments/1r3v8sd/difference_between_opus_46_and_gpt52_pro_on_a/"&gt;Previous post comparing Opus 4.6 and GPT-5.2 Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;(Disclaimer: This is a benchmark I made, so technically self-promotion, but I thought it was a cool comparison :)&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ENT_Alam"&gt; /u/ENT_Alam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r6h3ha"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6h3ha/difference_between_qwen_3_maxthinking_and_qwen_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6h3ha/difference_between_qwen_3_maxthinking_and_qwen_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T18:10:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6g14s</id>
    <title>4 of the top 5 most used models on OpenRouter this week are Open Source!</title>
    <updated>2026-02-16T17:32:44+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6g14s/4_of_the_top_5_most_used_models_on_openrouter/"&gt; &lt;img alt="4 of the top 5 most used models on OpenRouter this week are Open Source!" src="https://preview.redd.it/54xxp91s6wjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10b8a71332018921514258bd081fc7ed68e28e72" title="4 of the top 5 most used models on OpenRouter this week are Open Source!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/54xxp91s6wjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6g14s/4_of_the_top_5_most_used_models_on_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6g14s/4_of_the_top_5_most_used_models_on_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T17:32:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6w0la</id>
    <title>Where are Qwen 3.5 2B, 9B, and 35B-A3B</title>
    <updated>2026-02-17T04:12:03+00:00</updated>
    <author>
      <name>/u/Admirable_Flower_287</name>
      <uri>https://old.reddit.com/user/Admirable_Flower_287</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Where did leakers go&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable_Flower_287"&gt; /u/Admirable_Flower_287 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6w0la/where_are_qwen_35_2b_9b_and_35ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6w0la/where_are_qwen_35_2b_9b_and_35ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6w0la/where_are_qwen_35_2b_9b_and_35ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T04:12:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r6ghty</id>
    <title>Qwen 3.5 goes bankrupt on Vending-Bench 2</title>
    <updated>2026-02-16T17:49:21+00:00</updated>
    <author>
      <name>/u/Deep-Vermicelli-4591</name>
      <uri>https://old.reddit.com/user/Deep-Vermicelli-4591</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6ghty/qwen_35_goes_bankrupt_on_vendingbench_2/"&gt; &lt;img alt="Qwen 3.5 goes bankrupt on Vending-Bench 2" src="https://preview.redd.it/dj0x1zeo9wjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e961cc9b6b922483e81871381dc9dca90d6aae60" title="Qwen 3.5 goes bankrupt on Vending-Bench 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deep-Vermicelli-4591"&gt; /u/Deep-Vermicelli-4591 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dj0x1zeo9wjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r6ghty/qwen_35_goes_bankrupt_on_vendingbench_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r6ghty/qwen_35_goes_bankrupt_on_vendingbench_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T17:49:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3t775</id>
    <title>AMA with MiniMax â€” Ask Us Anything!</title>
    <updated>2026-02-13T16:07:54+00:00</updated>
    <author>
      <name>/u/HardToVary</name>
      <uri>https://old.reddit.com/user/HardToVary</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt; &lt;img alt="AMA with MiniMax â€” Ask Us Anything!" src="https://preview.redd.it/5z2li1ntcajg1.jpg?width=140&amp;amp;height=59&amp;amp;auto=webp&amp;amp;s=ce3340cd37b7e9408878509be00dd7b871efebde" title="AMA with MiniMax â€” Ask Us Anything!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! Weâ€™re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;We're &lt;strong&gt;MiniMax&lt;/strong&gt;, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;.5&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining the channel today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; â€” Founder of MiniMax&lt;/li&gt; &lt;li&gt;&lt;a href="/u/Wise_Evidence9973"&gt;u/Wise_Evidence9973&lt;/a&gt; â€” Head of LLM Research&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ryan85127704"&gt;u/ryan85127704&lt;/a&gt; â€” Head of Engineering&lt;/li&gt; &lt;li&gt;&lt;a href="/u/HardToVary"&gt;u/HardToVary&lt;/a&gt; â€” LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c"&gt;https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S. We'll continue monitoring and responding to questions for 48 hours after the end of the AMA.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HardToVary"&gt; /u/HardToVary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:07:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60qu9</id>
    <title>AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)</title>
    <updated>2026-02-16T05:11:16+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" src="https://preview.redd.it/u11uh8jfisjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afc30b2b6ae673f2e940109e2001bb498bd818ad" title="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ðŸ‘‹&lt;/p&gt; &lt;p&gt;We're excited for Thursday's guests: &lt;strong&gt;The StepFun Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Thursday, Feb. 19th, 8 AMâ€“11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;âš ï¸ &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please donâ€™t post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u11uh8jfisjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T05:11:16+00:00</published>
  </entry>
</feed>
