<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-28T16:06:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nsjwv1</id>
    <title>What is your primary reason to run LLM’s locally</title>
    <updated>2025-09-28T09:22:15+00:00</updated>
    <author>
      <name>/u/okaris</name>
      <uri>https://old.reddit.com/user/okaris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/poll/1nsjwv1"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/okaris"&gt; /u/okaris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsjwv1/what_is_your_primary_reason_to_run_llms_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsjwv1/what_is_your_primary_reason_to_run_llms_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsjwv1/what_is_your_primary_reason_to_run_llms_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T09:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns50u5</id>
    <title>More money than brains... building a workstation for local LLM.</title>
    <updated>2025-09-27T20:10:50+00:00</updated>
    <author>
      <name>/u/chisleu</name>
      <uri>https://old.reddit.com/user/chisleu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.asus.com/us/motherboards-components/motherboards/workstation/pro-ws-wrx90e-sage-se/"&gt;https://www.asus.com/us/motherboards-components/motherboards/workstation/pro-ws-wrx90e-sage-se/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I ordered this motherboard because it has 7 slots of PCIE 5.0x16 lanes.&lt;/p&gt; &lt;p&gt;Then I ordered this GPU: &lt;a href="https://www.amazon.com/dp/B0F7Y644FQ?th=1"&gt;https://www.amazon.com/dp/B0F7Y644FQ?th=1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The plan is to have 4 of them so I'm going to change my order to the max Q version&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.amazon.com/AMD-RyzenTM-ThreadripperTM-PRO-7995WX/dp/B0CK2ZQJZ6/"&gt;https://www.amazon.com/AMD-RyzenTM-ThreadripperTM-PRO-7995WX/dp/B0CK2ZQJZ6/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ordered this CPU. I think I got the right one.&lt;/p&gt; &lt;p&gt;I really need help understanding which RAM to buy... &lt;/p&gt; &lt;p&gt;I'm aware that selecting the right CPU and memory are critical steps and I want to be sure I get this right. I need to be sure I have at least support for 4x GPUs and 4x PCIE 5.0x4 SSDs for model storage. Raid 0 :D&lt;/p&gt; &lt;p&gt;Anyone got any tips for an old head? I haven't built a PC is so long the technology all went and changed on me.&lt;/p&gt; &lt;p&gt;EDIT: Added this case because of a user suggestion. Keep them coming!! &amp;lt;3 this community &lt;a href="https://www.silverstonetek.com/fr/product/info/computer-chassis/alta_d1/"&gt;https://www.silverstonetek.com/fr/product/info/computer-chassis/alta_d1/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Got two of these power supplies: &lt;a href="https://www.newegg.com/asrock-atx3-1-pcie5-1-1650-w-cybenetics-titanium-power-supply-black-tc-1650t/p/N82E16817955001?cm_mmc=snc-reddit-_-sr-_-17-955-001-_-09282025&amp;amp;utm_campaign=snc-reddit-_-sr-_-17-955-001-_-09282025&amp;amp;utm_medium=social&amp;amp;utm_source=reddit"&gt;ASRock TC-1650T 1650 W Power Supply&lt;/a&gt;| $479.99&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chisleu"&gt; /u/chisleu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns50u5/more_money_than_brains_building_a_workstation_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns50u5/more_money_than_brains_building_a_workstation_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns50u5/more_money_than_brains_building_a_workstation_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T20:10:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsoxww</id>
    <title>Are these VibeVoice models SAME?</title>
    <updated>2025-09-28T13:55:00+00:00</updated>
    <author>
      <name>/u/Dragonacious</name>
      <uri>https://old.reddit.com/user/Dragonacious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;VibeVoice Large: &lt;a href="https://www.modelscope.cn/models/microsoft/VibeVoice-Large/files"&gt;https://www.modelscope.cn/models/microsoft/VibeVoice-Large/files&lt;/a&gt;&lt;/p&gt; &lt;p&gt;VibeVoice 7B: &lt;a href="https://www.modelscope.cn/models/microsoft/VibeVoice-7B/files"&gt;https://www.modelscope.cn/models/microsoft/VibeVoice-7B/files&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Are these same or?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dragonacious"&gt; /u/Dragonacious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsoxww/are_these_vibevoice_models_same/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsoxww/are_these_vibevoice_models_same/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsoxww/are_these_vibevoice_models_same/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T13:55:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsey15</id>
    <title>Tried Meituan's new LongCat Flash Thinking model.</title>
    <updated>2025-09-28T04:12:52+00:00</updated>
    <author>
      <name>/u/xieyutong</name>
      <uri>https://old.reddit.com/user/xieyutong</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, I got some hands-on time with Meituan's newly dropped LongCat-Flash-Thinking model and checked out some other outputs floating around. Here are my quick thoughts to save you some evaluation time.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Speed: Crazy fast. Like, you-gotta-try-it-to-believe-it fast.&lt;/li&gt; &lt;li&gt;Performance: Overall, a solid step up from standard chat models for reasoning tasks.&lt;/li&gt; &lt;li&gt;Instruction Following: Really good. It picks up on subtle hints in prompts.&lt;/li&gt; &lt;li&gt;Answer Length: Weirdly, its final answers are often shorter than you'd get from a chat model. Even with the &amp;quot;thinking&amp;quot; chain included, the total output feels more concise (except for code/math).&lt;/li&gt; &lt;li&gt;Benchmarks: Seems to line up with the claimed leaderboard performance.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Nitty-Gritty:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Watch out for code generation: Sometimes the complete code ends up in the &amp;quot;thinking&amp;quot; part, and the final answer might have chunks missing. Needs a careful look.&lt;/li&gt; &lt;li&gt;Agent stuff: I tested it with some dummy tools and it understood the concepts well.&lt;/li&gt; &lt;li&gt;Built-in Code Interpreter: Has that functionality, which is nice.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xieyutong"&gt; /u/xieyutong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsey15/tried_meituans_new_longcat_flash_thinking_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsey15/tried_meituans_new_longcat_flash_thinking_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsey15/tried_meituans_new_longcat_flash_thinking_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T04:12:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsmp8i</id>
    <title>What's the simplest gpu provider?</title>
    <updated>2025-09-28T12:08:23+00:00</updated>
    <author>
      <name>/u/test12319</name>
      <uri>https://old.reddit.com/user/test12319</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey,&lt;br /&gt; looking for the easiest way to run gpu jobs. Ideally it’s couple of clicks from cli/vs code. Not chasing the absolute cheapest, just simple + predictable pricing. eu data residency/sovereignty would be great.&lt;/p&gt; &lt;p&gt;I use modal today, just found lyceum, pretty new, but so far looks promising (auto hardware pick, runtime estimate). Also eyeing runpod, lambda, and ovhcloud. maybe vast or paperspace?&lt;/p&gt; &lt;p&gt;what’s been the least painful for you?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/test12319"&gt; /u/test12319 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsmp8i/whats_the_simplest_gpu_provider/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsmp8i/whats_the_simplest_gpu_provider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsmp8i/whats_the_simplest_gpu_provider/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T12:08:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrx3jr</id>
    <title>When are GPU prices going to get cheaper?</title>
    <updated>2025-09-27T14:48:09+00:00</updated>
    <author>
      <name>/u/KardelenAyshe</name>
      <uri>https://old.reddit.com/user/KardelenAyshe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm starting to lose hope. I really can't afford these current GPU prices. Does anyone have any insight on when we might see a significant price drop?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KardelenAyshe"&gt; /u/KardelenAyshe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrx3jr/when_are_gpu_prices_going_to_get_cheaper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrx3jr/when_are_gpu_prices_going_to_get_cheaper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrx3jr/when_are_gpu_prices_going_to_get_cheaper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T14:48:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nslgig</id>
    <title>About Kokoro TTS Voice Finetuning</title>
    <updated>2025-09-28T11:00:20+00:00</updated>
    <author>
      <name>/u/Mysterious-Comment94</name>
      <uri>https://old.reddit.com/user/Mysterious-Comment94</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to create a voice similar to a character from an anime I liked, so I used &lt;a href="https://github.com/RobViren/kvoicewalk"&gt;https://github.com/RobViren/kvoicewalk&lt;/a&gt;&lt;br /&gt; this repo and the output voice I got was very satisfactory. There was a .wav file where u could hear how it would sound like. I was then supposed to put the pytorch .pt file with the corresponding name into Kokoro tts and use the newly created voice there. &lt;/p&gt; &lt;p&gt;However the voice I heard in Kokoro after plugging it in is nowhere close to the voice I heard. The process of creating this voice took 21 hours. I left my system untouched for lots of hours and I genuinely think there were no mistakes in my setup process, cuz the output sound in the wav file sounded like what I was going for. &lt;/p&gt; &lt;p&gt;Is there another way for me to get my desired voice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious-Comment94"&gt; /u/Mysterious-Comment94 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nslgig/about_kokoro_tts_voice_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nslgig/about_kokoro_tts_voice_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nslgig/about_kokoro_tts_voice_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T11:00:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nryoa5</id>
    <title>Megrez2: 21B latent, 7.5B on VRAM, 3B active—MoE on single 8GB card</title>
    <updated>2025-09-27T15:53:01+00:00</updated>
    <author>
      <name>/u/Normal_Onion_512</name>
      <uri>https://old.reddit.com/user/Normal_Onion_512</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nryoa5/megrez2_21b_latent_75b_on_vram_3b_activemoe_on/"&gt; &lt;img alt="Megrez2: 21B latent, 7.5B on VRAM, 3B active—MoE on single 8GB card" src="https://external-preview.redd.it/glz22pd-75yG_ynznmuaF8hifkLCtseU0s4FKfNwWlI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01875fcb778a5024d673b34876da00b5dcb1b48e" title="Megrez2: 21B latent, 7.5B on VRAM, 3B active—MoE on single 8GB card" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I came across Megrez2-3x7B-A3B on Hugging Face and thought it worth sharing. &lt;/p&gt; &lt;p&gt;I read through their tech report, and it says that the model has a unique MoE architecture with a layer-sharing expert design, so the &lt;strong&gt;checkpoint stores 7.5B params&lt;/strong&gt; yet can compose with the &lt;strong&gt;equivalent of 21B latent weights&lt;/strong&gt; at run-time while only 3B are active per token.&lt;/p&gt; &lt;p&gt;I was intrigued by the published Open-Compass figures, since it places the model &lt;strong&gt;on par with or slightly above Qwen-30B-A3B&lt;/strong&gt; in MMLU / GPQA / MATH-500 with roughly &lt;strong&gt;1/4 the VRAM requirements&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;There is already a &lt;strong&gt;GGUF file&lt;/strong&gt; and the matching &lt;strong&gt;llama.cpp branch&lt;/strong&gt; which I posted below (though it can also be found in the gguf page). The supplied &lt;strong&gt;Q4 quant occupies about 4 GB; FP8 needs approximately 8 GB&lt;/strong&gt;. The developer notes that FP16 currently has a couple of issues with coding tasks though, which they are working on solving. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;License is Apache 2.0, and it is currently running a Huggingface Space as well.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Model: [Infinigence/Megrez2-3x7B-A3B] &lt;a href="https://huggingface.co/Infinigence/Megrez2-3x7B-A3B"&gt;https://huggingface.co/Infinigence/Megrez2-3x7B-A3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF"&gt;https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Live Demo: &lt;a href="https://huggingface.co/spaces/Infinigence/Megrez2-3x7B-A3B"&gt;https://huggingface.co/spaces/Infinigence/Megrez2-3x7B-A3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github Repo: &lt;a href="https://github.com/Infinigence/Megrez2"&gt;https://github.com/Infinigence/Megrez2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;llama.cpp branch: &lt;a href="https://github.com/infinigence/llama.cpp/tree/support-megrez"&gt;https://github.com/infinigence/llama.cpp/tree/support-megrez&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If anyone tries it, I would be interested to hear your throughput and quality numbers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Normal_Onion_512"&gt; /u/Normal_Onion_512 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nryoa5/megrez2_21b_latent_75b_on_vram_3b_activemoe_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nryoa5/megrez2_21b_latent_75b_on_vram_3b_activemoe_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T15:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsm53q</id>
    <title>Initial results with gpt120 after rehousing 2 x 3090 into 7532</title>
    <updated>2025-09-28T11:38:46+00:00</updated>
    <author>
      <name>/u/Secure_Reflection409</name>
      <uri>https://old.reddit.com/user/Secure_Reflection409</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using old DDR4 2400 I had sitting in a server I hadn't turned on for 2 years:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PP: 356 ---&amp;gt; 522 t/s&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;TG: 37 ---&amp;gt; 60 t/s&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Still so much to get to grips with to get maximum performance out of this. So little visibility in Linux compared to what I take for granted in Windows.&lt;br /&gt; HTF do you view memory timings in Linux, for example?&lt;br /&gt; What clock speeds are my 3090s ramping up to and how quickly?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;gpt-oss-120b-MXFP4 @ 7800X3D @ 67GB/s (mlc)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;C:\LCP&amp;gt;llama-bench.exe -m openai_gpt-oss-120b-MXFP4-00001-of-00002.gguf -ot &amp;quot;.ffn_gate_exps.=CPU&amp;quot; --flash-attn 1 --threads 12 ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 2 CUDA devices: Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes Device 1: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes load_backend: loaded CUDA backend from C:\LCP\ggml-cuda.dll load_backend: loaded RPC backend from C:\LCP\ggml-rpc.dll load_backend: loaded CPU backend from C:\LCP\ggml-cpu-icelake.dll | model | size | params | backend | ngl | threads | fa | ot | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -: | --------------------- | --------------: | -------------------: | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | CUDA,RPC | 99 | 12 | 1 | .ffn_gate_exps.=CPU | pp512 | 356.99 ± 26.04 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | CUDA,RPC | 99 | 12 | 1 | .ffn_gate_exps.=CPU | tg128 | 37.95 ± 0.18 | build: b9382c38 (6340) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;gpt-oss-120b-MXFP4 @ 7532 @ 138GB/s (mlc)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ llama-bench -m openai_gpt-oss-120b-MXFP4-00001-of-00002.gguf --flash-attn 1 --threads 32 -ot &amp;quot;.ffn_gate_exps.=CPU&amp;quot; ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 2 CUDA devices: Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes Device 1: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes | model | size | params | backend | ngl | fa | ot | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------------- | --------------: | -------------------: | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | CUDA | 99 | 1 | .ffn_gate_exps.=CPU | pp512 | 522.05 ± 2.87 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | CUDA | 99 | 1 | .ffn_gate_exps.=CPU | tg128 | 60.61 ± 0.29 | build: e6d65fb0 (6611) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Reflection409"&gt; /u/Secure_Reflection409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsm53q/initial_results_with_gpt120_after_rehousing_2_x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsm53q/initial_results_with_gpt120_after_rehousing_2_x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsm53q/initial_results_with_gpt120_after_rehousing_2_x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T11:38:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsg3o9</id>
    <title>Built an MCP server for Claude Desktop to browse Reddit in real-time</title>
    <updated>2025-09-28T05:19:35+00:00</updated>
    <author>
      <name>/u/karanb192</name>
      <uri>https://old.reddit.com/user/karanb192</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsg3o9/built_an_mcp_server_for_claude_desktop_to_browse/"&gt; &lt;img alt="Built an MCP server for Claude Desktop to browse Reddit in real-time" src="https://preview.redd.it/ognd8gkeburf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=21f07e83b0ff9a3f2a857392a8f320f0f686f3c3" title="Built an MCP server for Claude Desktop to browse Reddit in real-time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just released this - Claude can now browse Reddit natively through MCP!&lt;/p&gt; &lt;p&gt;I got tired of copy-pasting Reddit threads to get insights, so I built reddit-mcp-buddy.&lt;/p&gt; &lt;p&gt;Setup (2 minutes):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open your Claude Desktop config&lt;/li&gt; &lt;li&gt;Add this JSON snippet&lt;/li&gt; &lt;li&gt;Restart Claude&lt;/li&gt; &lt;li&gt;Start browsing Reddit!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Config to add:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{ &amp;quot;mcpServers&amp;quot;: { &amp;quot;reddit&amp;quot;: { &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;, &amp;quot;args&amp;quot;: [&amp;quot;reddit-mcp-buddy&amp;quot;] } } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What you can ask: - &amp;quot;What's trending in &lt;a href="/r/technology"&gt;r/technology&lt;/a&gt;?&amp;quot; - &amp;quot;Summarize the drama in &lt;a href="/r/programming"&gt;r/programming&lt;/a&gt; this week&amp;quot; - &amp;quot;Find startup ideas in &lt;a href="/r/entrepreneur"&gt;r/entrepreneur&lt;/a&gt;&amp;quot; - &amp;quot;What do people think about the new iPhone in &lt;a href="/r/apple"&gt;r/apple&lt;/a&gt;?&amp;quot;&lt;/p&gt; &lt;p&gt;Free tier: 10 requests/min&lt;/p&gt; &lt;p&gt;With Reddit login: 100 requests/min (that's 10,000 posts per minute!)&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/karanb192/reddit-mcp-buddy"&gt;https://github.com/karanb192/reddit-mcp-buddy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone built other cool MCP servers? Looking for inspiration!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/karanb192"&gt; /u/karanb192 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ognd8gkeburf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsg3o9/built_an_mcp_server_for_claude_desktop_to_browse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsg3o9/built_an_mcp_server_for_claude_desktop_to_browse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T05:19:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nskxmf</id>
    <title>Lessons from building an intelligent LLM router</title>
    <updated>2025-09-28T10:28:17+00:00</updated>
    <author>
      <name>/u/botirkhaltaev</name>
      <uri>https://old.reddit.com/user/botirkhaltaev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’ve been experimenting with routing inference across LLMs, and the path has been full of wrong turns.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Attempt 1:&lt;/strong&gt; Just use a large LLM to decide routing.&lt;br /&gt; → Too costly, and the decisions were wildly unreliable.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Attempt 2:&lt;/strong&gt; Train a small fine-tuned LLM as a router.&lt;br /&gt; → Cheaper, but outputs were poor and not trustworthy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Attempt 3:&lt;/strong&gt; Write heuristics that map prompt types to model IDs.&lt;br /&gt; → Worked for a while, but brittle. Every time APIs changed or workloads shifted, it broke.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Shift in approach:&lt;/strong&gt; Instead of routing to specific model IDs, we switched to &lt;em&gt;model criteria&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;That means benchmarking models across task types, domains, and complexity levels, and making routing decisions based on those profiles.&lt;/p&gt; &lt;p&gt;To estimate task type and complexity, we started using NVIDIA’s &lt;a href="https://huggingface.co/nvidia/prompt-task-and-complexity-classifier"&gt;Prompt Task and Complexity Classifier&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It’s a multi-headed DeBERTa model that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Classifies prompts into 11 categories (QA, summarization, code gen, classification, etc.)&lt;/li&gt; &lt;li&gt;Scores prompts across six dimensions (creativity, reasoning, domain knowledge, contextual knowledge, constraints, few-shots)&lt;/li&gt; &lt;li&gt;Produces a weighted overall complexity score&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This gave us a structured way to decide when a prompt justified a premium model like Claude Opus 4.1, and when a smaller model like GPT-5-mini would perform just as well.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Now:&lt;/strong&gt; We’re working on integrating this with Google’s UniRoute.&lt;/p&gt; &lt;p&gt;UniRoute represents models as error vectors over representative prompts, allowing routing to generalize to unseen models. Our next step is to expand this idea by incorporating &lt;strong&gt;task complexity and domain-awareness&lt;/strong&gt; into the same framework, so routing isn’t just performance-driven but context-aware.&lt;/p&gt; &lt;p&gt;UniRoute Paper: &lt;a href="https://arxiv.org/abs/2502.08773"&gt;https://arxiv.org/abs/2502.08773&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Takeaway&lt;/strong&gt;: routing isn’t just “pick the cheapest vs biggest model.” It’s about matching workload complexity and domain needs to models with proven benchmark performance, and adapting as new models appear.&lt;/p&gt; &lt;p&gt;Repo (open source): &lt;a href="https://github.com/Egham-7/adaptive"&gt;https://github.com/Egham-7/adaptive&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’d love to hear from anyone else who has worked on inference routing or explored UniRoute-style approaches.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/botirkhaltaev"&gt; /u/botirkhaltaev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nskxmf/lessons_from_building_an_intelligent_llm_router/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nskxmf/lessons_from_building_an_intelligent_llm_router/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nskxmf/lessons_from_building_an_intelligent_llm_router/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T10:28:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsfurg</id>
    <title>Supermicro GPU Server</title>
    <updated>2025-09-28T05:04:49+00:00</updated>
    <author>
      <name>/u/desexmachina</name>
      <uri>https://old.reddit.com/user/desexmachina</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsfurg/supermicro_gpu_server/"&gt; &lt;img alt="Supermicro GPU Server" src="https://preview.redd.it/33oz8zct8urf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f84e171fb4e7d2c4bd80622d00fad63d0d901b7" title="Supermicro GPU Server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I recently picked up a couple of servers from a company for a project I’m doing, I totally forgot that they’ve got a bunch of Supermicro GPU servers they’re getting rid of. Conditions unknown, they’d have to be QC’d and tested each. Educate me on what we’re looking at here and if these have value to guys like us. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/desexmachina"&gt; /u/desexmachina &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/33oz8zct8urf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsfurg/supermicro_gpu_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsfurg/supermicro_gpu_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T05:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsrrod</id>
    <title>Can crowd shape the open future, or is everything up to huge investors?</title>
    <updated>2025-09-28T15:51:24+00:00</updated>
    <author>
      <name>/u/Guardian-Spirit</name>
      <uri>https://old.reddit.com/user/Guardian-Spirit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am quite a bit concerned about the future of open-weight AI.&lt;/p&gt; &lt;p&gt;Right now, we're &lt;em&gt;mostly&lt;/em&gt; good: there is a lot of competition, a lot of open companies, but the gap between closed and open-weight is way larger than I'd like to have it. And capitalism usually means that the gap will only get larger, as commercialy successful labs will gain more power to produce their closed models, eventually leaving the competition far behind.&lt;/p&gt; &lt;p&gt;What can really be done by mortal crowd to ensure &amp;quot;utopia&amp;quot;, and not some megacorp-controlled &amp;quot;dystopia&amp;quot;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Guardian-Spirit"&gt; /u/Guardian-Spirit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsrrod/can_crowd_shape_the_open_future_or_is_everything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsrrod/can_crowd_shape_the_open_future_or_is_everything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsrrod/can_crowd_shape_the_open_future_or_is_everything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T15:51:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsk4ff</id>
    <title>I wonder if anyone else noticed drop of quality between magistral small 2506 and later revisions.</title>
    <updated>2025-09-28T09:36:10+00:00</updated>
    <author>
      <name>/u/zekses</name>
      <uri>https://old.reddit.com/user/zekses</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;it's entirely subjective, but I am using it for c++ code reviews and 2506 was startlingly adequate for the task. Somehow 2507 and later started hallucinating much more. I am not sure whether I myself am not hallucinating that difference. Did anyone else notice it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zekses"&gt; /u/zekses &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsk4ff/i_wonder_if_anyone_else_noticed_drop_of_quality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsk4ff/i_wonder_if_anyone_else_noticed_drop_of_quality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsk4ff/i_wonder_if_anyone_else_noticed_drop_of_quality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T09:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsr0vf</id>
    <title>Bring Your Own Data (BYOD)</title>
    <updated>2025-09-28T15:21:12+00:00</updated>
    <author>
      <name>/u/Long_Complex_4395</name>
      <uri>https://old.reddit.com/user/Long_Complex_4395</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The knowledge of Large Language Models sky rocketed after ChatGPT was born, everyone jumped into the trend of building and using LLMs whether its to sell to companies or companies integrating it into their system. Frequently, many models get released with new benchmarks, targeting specific tasks such as sales, code generation and reviews and the likes.&lt;/p&gt; &lt;p&gt;Last month, Harvard Business Review wrote an article on MIT Media Lab’s research which highlighted the study that 95% of investments in gen AI have produced zero returns. This is not a technical issue, but more of a business one where everybody wants to create or integrate their own AI due to the hype and FOMO. This research may or may not have put a wedge in the adoption of AI into existing systems.&lt;/p&gt; &lt;p&gt;To combat the lack of returns, Small Language Models seems to do pretty well as they are more specialized to achieve a given task. This led me to working on Otto - an end-to-end small language model builder where you build your model with your own data, its open source, still rough around the edges.&lt;/p&gt; &lt;p&gt;To demonstrate this pipeline, I got data from Huggingface - a 142MB data containing automotive customer service transcript with the following parameters&lt;/p&gt; &lt;ul&gt; &lt;li&gt;6 layers, 6 heads, 384 embedding dimensions&lt;/li&gt; &lt;li&gt;50,257 vocabulary tokens&lt;/li&gt; &lt;li&gt;128 tokens for block size.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;which gave 16.04M parameters. Its training loss improved from 9.2 to 2.2 with domain specialization where it learned automotive service conversation structure.&lt;/p&gt; &lt;p&gt;This model learned the specific patterns of automotive customer service calls, including technical vocabulary, conversation flow, and domain-specific terminology that a general-purpose model might miss or handle inefficiently.&lt;/p&gt; &lt;p&gt;There are still improvements needed for the pipeline which I am working on, you can try it out here: &lt;a href="https://github.com/Nwosu-Ihueze/otto"&gt;https://github.com/Nwosu-Ihueze/otto&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Long_Complex_4395"&gt; /u/Long_Complex_4395 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsr0vf/bring_your_own_data_byod/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsr0vf/bring_your_own_data_byod/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsr0vf/bring_your_own_data_byod/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T15:21:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns9jj1</id>
    <title>ChatGPT won't let you build an LLM server that passes through reasoning content</title>
    <updated>2025-09-27T23:30:33+00:00</updated>
    <author>
      <name>/u/Acceptable_Adagio_91</name>
      <uri>https://old.reddit.com/user/Acceptable_Adagio_91</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI are trying so hard to protect their special sauce now that they have added a rule in ChatGPT which disallows it from building code that will facilitate reasoning content being passed through an LLM server to a client. It doesn't care that it's an open source model, or not an OpenAI model, it will add in reasoning content filters (without being asked to) and it definitely will not remove them if asked.&lt;/p&gt; &lt;p&gt;Pretty annoying when you're just trying to work with open source models where I can see all the reasoning content anyway and for my use case, I specifically want the reasoning content to be presented to the client...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable_Adagio_91"&gt; /u/Acceptable_Adagio_91 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns9jj1/chatgpt_wont_let_you_build_an_llm_server_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns9jj1/chatgpt_wont_let_you_build_an_llm_server_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns9jj1/chatgpt_wont_let_you_build_an_llm_server_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T23:30:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns7f86</id>
    <title>Native MCP now in Open WebUI!</title>
    <updated>2025-09-27T21:52:59+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns7f86/native_mcp_now_in_open_webui/"&gt; &lt;img alt="Native MCP now in Open WebUI!" src="https://external-preview.redd.it/M25kcGJzOW4zc3JmMUhHt6uNZXDs9ywsBLgDtMNnOeRDGUuA-xcxHHChg7dp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=893d840d90d19c5f13b37eb84534bdf21af148f9" title="Native MCP now in Open WebUI!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4qv7zp9n3srf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns7f86/native_mcp_now_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns7f86/native_mcp_now_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T21:52:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns2fbl</id>
    <title>For llama.cpp/ggml AMD MI50s are now universally faster than NVIDIA P40s</title>
    <updated>2025-09-27T18:24:00+00:00</updated>
    <author>
      <name>/u/Remove_Ayys</name>
      <uri>https://old.reddit.com/user/Remove_Ayys</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In 2023 I implemented llama.cpp/ggml CUDA support specifically for NVIDIA P40s since they were one of the cheapest options for GPUs with 24 GB VRAM. Recently AMD MI50s became very cheap options for GPUs with 32 GB VRAM, selling for well below $150 if you order multiple of them off of Alibaba. However, the llama.cpp ROCm performance was very bad because the code was originally written for NVIDIA GPUs and simply translated to AMD via HIP. I have now optimized the CUDA FlashAttention code in particular for AMD and as a result MI50s now actually have better performance than P40s:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Test&lt;/th&gt; &lt;th&gt;Depth&lt;/th&gt; &lt;th&gt;t/s P40 (CUDA)&lt;/th&gt; &lt;th&gt;t/s P40 (Vulkan)&lt;/th&gt; &lt;th&gt;t/s MI50 (ROCm)&lt;/th&gt; &lt;th&gt;t/s MI50 (Vulkan)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Gemma 3 Instruct 27b q4_K_M&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;266.63&lt;/td&gt; &lt;td&gt;32.02&lt;/td&gt; &lt;td&gt;272.95&lt;/td&gt; &lt;td&gt;85.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma 3 Instruct 27b q4_K_M&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;16384&lt;/td&gt; &lt;td&gt;210.77&lt;/td&gt; &lt;td&gt;30.51&lt;/td&gt; &lt;td&gt;230.32&lt;/td&gt; &lt;td&gt;51.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma 3 Instruct 27b q4_K_M&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;13.50&lt;/td&gt; &lt;td&gt;14.74&lt;/td&gt; &lt;td&gt;22.29&lt;/td&gt; &lt;td&gt;20.91&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma 3 Instruct 27b q4_K_M&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;16384&lt;/td&gt; &lt;td&gt;12.09&lt;/td&gt; &lt;td&gt;12.76&lt;/td&gt; &lt;td&gt;19.12&lt;/td&gt; &lt;td&gt;16.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 30b a3b q4_K_M&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1095.11&lt;/td&gt; &lt;td&gt;114.08&lt;/td&gt; &lt;td&gt;1140.27&lt;/td&gt; &lt;td&gt;372.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 30b a3b q4_K_M&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;16384&lt;/td&gt; &lt;td&gt;249.98&lt;/td&gt; &lt;td&gt;73.54&lt;/td&gt; &lt;td&gt;420.88&lt;/td&gt; &lt;td&gt;92.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 30b a3b q4_K_M&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;67.30&lt;/td&gt; &lt;td&gt;63.54&lt;/td&gt; &lt;td&gt;77.15&lt;/td&gt; &lt;td&gt;81.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 30b a3b q4_K_M&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;16384&lt;/td&gt; &lt;td&gt;36.15&lt;/td&gt; &lt;td&gt;42.66&lt;/td&gt; &lt;td&gt;39.91&lt;/td&gt; &lt;td&gt;40.69&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I did not yet touch regular matrix multiplications so the speed on an empty context is probably still suboptimal. The Vulkan performance is in some instances better than the ROCm performance. Since I've already gone to the effort to read the AMD ISA documentation I've also purchased an MI100 and RX 9060 XT and I will optimize the ROCm performance for that hardware as well. An AMD person said they would sponsor me a Ryzen AI MAX system, I'll get my RDNA3 coverage from that.&lt;/p&gt; &lt;p&gt;Edit: looking at the numbers again there is an instance where the optimal performance of the P40 is still better than the optimal performance of the MI50 so the &amp;quot;universally&amp;quot; qualifier is not quite correct. But Reddit doesn't let me edit the post title so we'll just have to live with it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remove_Ayys"&gt; /u/Remove_Ayys &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns2fbl/for_llamacppggml_amd_mi50s_are_now_universally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns2fbl/for_llamacppggml_amd_mi50s_are_now_universally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns2fbl/for_llamacppggml_amd_mi50s_are_now_universally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T18:24:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsqe5i</id>
    <title>I created a simple tool to manage your llama.cpp settings &amp; installation</title>
    <updated>2025-09-28T14:55:42+00:00</updated>
    <author>
      <name>/u/igorwarzocha</name>
      <uri>https://old.reddit.com/user/igorwarzocha</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsqe5i/i_created_a_simple_tool_to_manage_your_llamacpp/"&gt; &lt;img alt="I created a simple tool to manage your llama.cpp settings &amp;amp; installation" src="https://preview.redd.it/z2jl2s624xrf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c627cf5e755bc96918498e6d100146f512f95e46" title="I created a simple tool to manage your llama.cpp settings &amp;amp; installation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yo! I was messing around with my configs etc and noticed it was a massive pain to keep it all in one place... So I vibecoded this thing. &lt;a href="https://github.com/IgorWarzocha/llama_cpp_manager"&gt;https://github.com/IgorWarzocha/llama_cpp_manager&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A zero-bs configuration tool for llama.cpp that runs in your terminal and keeps it all organised in one folder.&lt;/p&gt; &lt;p&gt;It starts with a wizard to configure your basic defaults, it sorts out your llama.cpp download/update - it checks the appropriate compiled binary file from the github repo, downloads it, unzips, cleans up the temp file, etc etc.&lt;/p&gt; &lt;p&gt;There's a model config management module that guides you through editing basic config, but you can also add your own parameters... All saved in json files in plain sight.&lt;/p&gt; &lt;p&gt;I also included a basic benchmarking utility that will run your saved model configs (in batch if you want) against your current server config with a pre-selected prompt and give you stats.&lt;/p&gt; &lt;p&gt;Anyway, I tested it thoroughly enough on Ubuntu/Vulkan. Can't vouch for any other situations. If you have your own compiled llama.cpp you can drop it into llama-cpp folder.&lt;/p&gt; &lt;p&gt;Let me know if it works for you (works on my machine, hah), if you would like to see any features added etc. It's hard to keep a &amp;quot;good enough&amp;quot; mindset and avoid being overwhelming or annoying lolz.&lt;/p&gt; &lt;p&gt;Cheerios.&lt;/p&gt; &lt;p&gt;edit, before you start roasting, I have now fixed hardcoded paths, hopefully all of them this time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/igorwarzocha"&gt; /u/igorwarzocha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z2jl2s624xrf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsqe5i/i_created_a_simple_tool_to_manage_your_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsqe5i/i_created_a_simple_tool_to_manage_your_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T14:55:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsnahe</id>
    <title>September 2025 benchmarks - 3x3090</title>
    <updated>2025-09-28T12:37:58+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsnahe/september_2025_benchmarks_3x3090/"&gt; &lt;img alt="September 2025 benchmarks - 3x3090" src="https://a.thumbs.redditmedia.com/oowsP26bvqdNGyybdB_e9u8e_0jjWcpcYEbbz5x_2V0.jpg" title="September 2025 benchmarks - 3x3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please enjoy the benchmarks on 3×3090 GPUs. &lt;/p&gt; &lt;p&gt;(If you want to reproduce my steps on your setup, you may need a fresh &lt;strong&gt;llama.cpp&lt;/strong&gt; build)&lt;/p&gt; &lt;p&gt;To run the benchmark, simply execute:&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-bench -m &amp;lt;path-to-the-model&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Sometimes you may need to add &lt;code&gt;--n-cpu-moe&lt;/code&gt; or &lt;code&gt;-ts&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;We’ll be testing a faster “dry run” and a run with a prefilled context (10000 tokens). So for each model, you’ll see boundaries between the initial speed and later, slower speed.&lt;/p&gt; &lt;p&gt;results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;gemma3 27B Q8 - 23t/s, 26t/s&lt;/li&gt; &lt;li&gt;Llama4 Scout Q5 - 23t/s, 30t/s&lt;/li&gt; &lt;li&gt;gpt oss 120B - 95t/s, 125t/s&lt;/li&gt; &lt;li&gt;dots Q3 - 15t/s, 20t/s&lt;/li&gt; &lt;li&gt;Qwen3 30B A3B - 78t/s, 130t/s&lt;/li&gt; &lt;li&gt;Qwen3 32B - 17t/s, 23t/s&lt;/li&gt; &lt;li&gt;Magistral Q8 - 28t/s, 33t/s&lt;/li&gt; &lt;li&gt;GLM 4.5 Air Q4 - 22t/s, 36t/s&lt;/li&gt; &lt;li&gt;Nemotron 49B Q8 - 13t/s, 16t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;please share your results on your setup&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nsnahe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsnahe/september_2025_benchmarks_3x3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsnahe/september_2025_benchmarks_3x3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T12:37:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsetwi</id>
    <title>LMStudio + MCP is so far the best experience I've had with models in a while.</title>
    <updated>2025-09-28T04:06:29+00:00</updated>
    <author>
      <name>/u/Komarov_d</name>
      <uri>https://old.reddit.com/user/Komarov_d</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;M4 Max 128gb&lt;br /&gt; Mostly use latest gpt-oss 20b or latest mistral with thinking/vision/tools in MLX format, since a bit faster (that's the whole point of MLX I guess, since we still don't have any proper LLMs in CoreML for apple neural engine...).&lt;/p&gt; &lt;p&gt;Connected around 10 MCPs for different purposes, works just purely amazing.&lt;br /&gt; Haven't been opening chat com or claude for a couple of days. &lt;/p&gt; &lt;p&gt;Pretty happy.&lt;/p&gt; &lt;p&gt;the next step is having a proper agentic conversation/flow under the hood, being able to leave it for autonomous working sessions, like cleaning up and connecting things in my Obsidian Vault during the night while I sleep, right...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Komarov_d"&gt; /u/Komarov_d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsetwi/lmstudio_mcp_is_so_far_the_best_experience_ive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsetwi/lmstudio_mcp_is_so_far_the_best_experience_ive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsetwi/lmstudio_mcp_is_so_far_the_best_experience_ive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T04:06:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nslth7</id>
    <title>Holy moly what did those madlads at llama cpp do?!!</title>
    <updated>2025-09-28T11:20:42+00:00</updated>
    <author>
      <name>/u/Similar-Republic149</name>
      <uri>https://old.reddit.com/user/Similar-Republic149</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just ran gpt oss 20b on my mi50 32gb and im getting 90tkps !?!?!? before it was around 40 . &lt;/p&gt; &lt;p&gt;./llama-bench -m /home/server/.lmstudio/models/lmstudio-community/gpt-oss-20b-GGUF/gpt-oss-20b-MXFP4.gguf -ngl 999 -fa on -mg 1 -dev Vulkan1 &lt;/p&gt; &lt;p&gt;load_backend: loaded RPC backend from /home/server/Desktop/Llama/llama-b6615-bin-ubuntu-vulkan-x64/build/bin/libggml-rpc.so&lt;/p&gt; &lt;p&gt;ggml_vulkan: Found 2 Vulkan devices:&lt;/p&gt; &lt;p&gt;ggml_vulkan: 0 = NVIDIA GeForce RTX 2060 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: KHR_coopmat&lt;/p&gt; &lt;p&gt;ggml_vulkan: 1 = AMD Instinct MI50/MI60 (RADV VEGA20) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: none&lt;/p&gt; &lt;p&gt;load_backend: loaded Vulkan backend from /home/server/Desktop/Llama/llama-b6615-bin-ubuntu-vulkan-x64/build/bin/libggml-vulkan.so&lt;/p&gt; &lt;p&gt;load_backend: loaded CPU backend from /home/server/Desktop/Llama/llama-b6615-bin-ubuntu-vulkan-x64/build/bin/libggml-cpu-haswell.so&lt;/p&gt; &lt;p&gt;| model | size | params | backend | ngl | main_gpu | dev | test | t/s |&lt;/p&gt; &lt;p&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ------------ | --------------: | -------------------: |&lt;/p&gt; &lt;p&gt;| gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | RPC,Vulkan | 999 | 1 | Vulkan1 | pp512 | 620.68 ± 6.62 |&lt;/p&gt; &lt;p&gt;| gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | RPC,Vulkan | 999 | 1 | Vulkan1 | tg128 | 91.42 ± 1.51 |&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Similar-Republic149"&gt; /u/Similar-Republic149 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nslth7/holy_moly_what_did_those_madlads_at_llama_cpp_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nslth7/holy_moly_what_did_those_madlads_at_llama_cpp_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nslth7/holy_moly_what_did_those_madlads_at_llama_cpp_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T11:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsghai</id>
    <title>Hunyan Image 3 Llm with image output</title>
    <updated>2025-09-28T05:42:53+00:00</updated>
    <author>
      <name>/u/ArtichokeNo2029</name>
      <uri>https://old.reddit.com/user/ArtichokeNo2029</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsghai/hunyan_image_3_llm_with_image_output/"&gt; &lt;img alt="Hunyan Image 3 Llm with image output" src="https://external-preview.redd.it/o3nW-C4go8YOZmYJKNZ4Y6tpE64YDvgP6ucRppFfdVQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0726b4b60205c7c2cac24ba84a82a9bbfa3680c3" title="Hunyan Image 3 Llm with image output" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty sure this a first of kind open sourced. They also plan a Thinking model too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArtichokeNo2029"&gt; /u/ArtichokeNo2029 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/HunyuanImage-3.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsghai/hunyan_image_3_llm_with_image_output/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsghai/hunyan_image_3_llm_with_image_output/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T05:42:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsfkqd</id>
    <title>dont buy the api from the website like openrouther or groq or anyother provider they reduce the qulaity of the model to make a profit . buy the api only from official website or run the model in locally</title>
    <updated>2025-09-28T04:48:30+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsfkqd/dont_buy_the_api_from_the_website_like/"&gt; &lt;img alt="dont buy the api from the website like openrouther or groq or anyother provider they reduce the qulaity of the model to make a profit . buy the api only from official website or run the model in locally" src="https://b.thumbs.redditmedia.com/a-fo9Zu2i0HXnjVJmdjuGYWbgWVo6fs53xAXUEtZjsw.jpg" title="dont buy the api from the website like openrouther or groq or anyother provider they reduce the qulaity of the model to make a profit . buy the api only from official website or run the model in locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;even there is no guarantee that official will be same good as the benchmark shown us .&lt;/p&gt; &lt;p&gt;so running the model locally is the best way to use the full power of the model .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nsfkqd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsfkqd/dont_buy_the_api_from_the_website_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsfkqd/dont_buy_the_api_from_the_website_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T04:48:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsmksq</id>
    <title>What are Kimi devs smoking</title>
    <updated>2025-09-28T12:02:02+00:00</updated>
    <author>
      <name>/u/Thechae9</name>
      <uri>https://old.reddit.com/user/Thechae9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsmksq/what_are_kimi_devs_smoking/"&gt; &lt;img alt="What are Kimi devs smoking" src="https://preview.redd.it/t8wfkk09bwrf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ec5f0e9de05cf0aafc8bc507d4950ca47e8ef09" title="What are Kimi devs smoking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Strangee&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thechae9"&gt; /u/Thechae9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t8wfkk09bwrf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsmksq/what_are_kimi_devs_smoking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsmksq/what_are_kimi_devs_smoking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T12:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
