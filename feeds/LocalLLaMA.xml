<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-21T18:42:31+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ocjgbf</id>
    <title>Embeddings</title>
    <updated>2025-10-21T17:37:52+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What‚Äôs good in embedding models these days?&lt;/p&gt; &lt;p&gt;Firstly for text and secondly for multimodal image and text&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocjgbf/embeddings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocjgbf/embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocjgbf/embeddings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T17:37:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ockagv</id>
    <title>FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems</title>
    <updated>2025-10-21T18:08:32+00:00</updated>
    <author>
      <name>/u/YiyanZ</name>
      <uri>https://old.reddit.com/user/YiyanZ</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ockagv/flashinferbench_building_the_virtuous_cycle_for/"&gt; &lt;img alt="FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems" src="https://b.thumbs.redditmedia.com/FOo5hRCPO5Mpj116Q3BLbFjKUWelZzgfLkDrFbe9bGg.jpg" title="FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;ü§î Can AI optimize the systems it runs on?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;üöÄ &lt;strong&gt;Introducing FlashInfer-Bench&lt;/strong&gt; ‚Äî a workflow that makes AI systems &lt;em&gt;self-improving&lt;/em&gt; through agents.&lt;/p&gt; &lt;p&gt;It‚Äôs designed to push the boundaries of LLM serving efficiency:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Standardized signature for LLM serving kernels&lt;/li&gt; &lt;li&gt;Implement kernels in any language you like&lt;/li&gt; &lt;li&gt;Benchmark them against real-world serving workloads&lt;/li&gt; &lt;li&gt;Fastest kernels get &lt;strong&gt;day-0 integrated&lt;/strong&gt; into production&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;FlashInfer-Bench launches with first-class integration into &lt;strong&gt;FlashInfer&lt;/strong&gt;, &lt;strong&gt;SGLang&lt;/strong&gt;, and &lt;strong&gt;vLLM&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qc6kumc58iwf1.png?width=2178&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e2f1a9bb2e0b338577bdbda3925c965a9876dda"&gt;Systematically Approaching AI for AI systems with FlashInfer-Bench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîó &lt;strong&gt;Blog post:&lt;/strong&gt; &lt;a href="https://flashinfer.ai/2025/10/21/flashinfer-bench.html"&gt;flashinfer.ai/2025/10/21/flashinfer-bench.html&lt;/a&gt;&lt;br /&gt; üìä &lt;strong&gt;Leaderboard:&lt;/strong&gt; &lt;a href="https://bench.flashinfer.ai/"&gt;bench.flashinfer.ai&lt;/a&gt;&lt;br /&gt; üíª &lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/flashinfer-ai/flashinfer-bench"&gt;github.com/flashinfer-ai/flashinfer-bench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YiyanZ"&gt; /u/YiyanZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ockagv/flashinferbench_building_the_virtuous_cycle_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ockagv/flashinferbench_building_the_virtuous_cycle_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ockagv/flashinferbench_building_the_virtuous_cycle_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T18:08:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1obrvab</id>
    <title>Support for Ling and Ring models (1000B/103B/16B) has finally been merged into llama.cpp</title>
    <updated>2025-10-20T19:54:08+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obrvab/support_for_ling_and_ring_models_1000b103b16b_has/"&gt; &lt;img alt="Support for Ling and Ring models (1000B/103B/16B) has finally been merged into llama.cpp" src="https://external-preview.redd.it/n2_CIH2NdPrPVJO7RzSAqCKKA-IjXoFSmGm_ZeORNmA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d670894b141c1fd2de6b21248ecab346ff0c897" title="Support for Ling and Ring models (1000B/103B/16B) has finally been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been following this PR for over a month because it adds support for some interesting MoE, the 103B size sounds cool&lt;/p&gt; &lt;p&gt;1T models:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T"&gt;https://huggingface.co/inclusionAI/Ring-1T&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-1T"&gt;https://huggingface.co/inclusionAI/Ling-1T&lt;/a&gt;&lt;/p&gt; &lt;p&gt;103B models&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-flash-2.0"&gt;https://huggingface.co/inclusionAI/Ling-flash-2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-flash-2.0"&gt;https://huggingface.co/inclusionAI/Ring-flash-2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;16B models&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-mini-2.0"&gt;https://huggingface.co/inclusionAI/Ring-mini-2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-mini-2.0"&gt;https://huggingface.co/inclusionAI/Ling-mini-2.0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16063"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obrvab/support_for_ling_and_ring_models_1000b103b16b_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obrvab/support_for_ling_and_ring_models_1000b103b16b_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:54:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1och72d</id>
    <title>Are there LLMs I can run via LM Studio that have voice input and output?</title>
    <updated>2025-10-21T16:12:46+00:00</updated>
    <author>
      <name>/u/123android</name>
      <uri>https://old.reddit.com/user/123android</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I guess I don't need to specifically run it in LM Studio if there's a better option but I'm wondering if what I want to do is possible. Basically I want to set up a local language assistant I can chat with in Portuguese to help me learn the language. Is this possible with local LLMs yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/123android"&gt; /u/123android &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1och72d/are_there_llms_i_can_run_via_lm_studio_that_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1och72d/are_there_llms_i_can_run_via_lm_studio_that_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1och72d/are_there_llms_i_can_run_via_lm_studio_that_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T16:12:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc9vvl</id>
    <title>AMD iGPU + dGPU : llama.cpp tensor-split not working with Vulkan backend</title>
    <updated>2025-10-21T11:01:02+00:00</updated>
    <author>
      <name>/u/Sixbroam</name>
      <uri>https://old.reddit.com/user/Sixbroam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Edit : Picard12832 gave me the solution, using --device Vulkan0,Vulkan1 instead of passing GGML_VK_VISIBLE_DEVICES=0,1 did the trick.&lt;/p&gt; &lt;p&gt;Trying to run gpt-oss-120b with llama.cpp with Vulkan backend using my 780M iGPU (64GB shared) and Vega 64 (8GB VRAM) but tensor-split just doesn't work. Everything dumps onto the Vega and uses GTT while the iGPU does nothing.&lt;/p&gt; &lt;p&gt;Output says &amp;quot;using device Vulkan1&amp;quot; and all 59GB goes there.&lt;/p&gt; &lt;p&gt;Tried flipping device order, different ts values, --main-gpu 0, split-mode layer, bunch of env vars... always picks Vulkan1.&lt;/p&gt; &lt;p&gt;Does tensor-split even work with Vulkan? Works fine for CUDA apparently but can't find anyone doing multi-GPU with Vulkan.&lt;/p&gt; &lt;p&gt;The model barely overflows my RAM so I just need the Vega to handle that bit, not for compute. If the split worked it'd be perfect.&lt;/p&gt; &lt;p&gt;Any help would be greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sixbroam"&gt; /u/Sixbroam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc9vvl/amd_igpu_dgpu_llamacpp_tensorsplit_not_working/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc9vvl/amd_igpu_dgpu_llamacpp_tensorsplit_not_working/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc9vvl/amd_igpu_dgpu_llamacpp_tensorsplit_not_working/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T11:01:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1obrde8</id>
    <title>Cerebras REAP update: pruned checkpoints for GLM4.5-Air &amp; Qwen3-Coder-30B now of HF!</title>
    <updated>2025-10-20T19:35:48+00:00</updated>
    <author>
      <name>/u/ilzrvch</name>
      <uri>https://old.reddit.com/user/ilzrvch</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/"&gt; &lt;img alt="Cerebras REAP update: pruned checkpoints for GLM4.5-Air &amp;amp; Qwen3-Coder-30B now of HF!" src="https://b.thumbs.redditmedia.com/c9KTXS-jH2CuE3vbdqAs-d7zeKzxIjLJapF1oi1eETU.jpg" title="Cerebras REAP update: pruned checkpoints for GLM4.5-Air &amp;amp; Qwen3-Coder-30B now of HF!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have heard your feedback on our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"&gt;initial REAP post&lt;/a&gt; and are excited to released REAP-pruned checkpoints for more lightweight models, GLM4.5-Air and Qwen3-Coder-30B:&lt;/p&gt; &lt;p&gt;25% pruned GLM4.5-Air: &lt;a href="https://hf.co/cerebras/GLM-4.5-Air-REAP-82B-A12B"&gt;https://hf.co/cerebras/GLM-4.5-Air-REAP-82B-A12B&lt;/a&gt;&lt;br /&gt; 20% pruned Qwen3-Coder-30B: &lt;a href="https://huggingface.co/cerebras/Qwen3-Coder-REAP-25B-A3B"&gt;https://huggingface.co/cerebras/Qwen3-Coder-REAP-25B-A3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are releasing those in BF16 so more accurate low-bit quantized GGUFs can be created for streamlined local deployment.&lt;/p&gt; &lt;p&gt;TLDR on REAP: &lt;/p&gt; &lt;p&gt;We show that one-shot pruning of experts in large MoEs is better than expert merging when looking at realistic benchmarks, not just perplexity measures.&lt;/p&gt; &lt;p&gt;Using a saliency criterion that measures expected routed contribution of each expert (REAP), we pruned Qwen3-Coder-480B to 363B (25% pruning) and 246B (50% pruning), all in FP8. At 25%, accuracy degradation is minimal across a suite of benchmarks. More on arXiv: &lt;a href="https://arxiv.org/abs/2510.13999"&gt;https://arxiv.org/abs/2510.13999&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let us know which models we should prune next in the comments!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vuu82b8sjbwf1.png?width=6539&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc8a064e15281f6e830e724e70d86a1b46721dc3"&gt;https://preview.redd.it/vuu82b8sjbwf1.png?width=6539&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc8a064e15281f6e830e724e70d86a1b46721dc3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilzrvch"&gt; /u/ilzrvch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:35:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ock0lc</id>
    <title>Qwen3-VL kinda sucks in LM Studio</title>
    <updated>2025-10-21T17:58:45+00:00</updated>
    <author>
      <name>/u/waescher</name>
      <uri>https://old.reddit.com/user/waescher</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ock0lc/qwen3vl_kinda_sucks_in_lm_studio/"&gt; &lt;img alt="Qwen3-VL kinda sucks in LM Studio" src="https://b.thumbs.redditmedia.com/FzqRtu_bjl1T0o6OQ-s1rtah6kqxUffbI4j5XO-W5vI.jpg" title="Qwen3-VL kinda sucks in LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else finding qwen3 VL absolutely terrible in LM Studio? I am using the 6bix MLX variant and even the VL 30b-a3b is really bad. Online demos like &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-VL-30B-A3B-Demo"&gt;this&lt;/a&gt; here work perfectly well.&lt;/p&gt; &lt;p&gt;Using the staff pick 30b model at up to 120k context.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waescher"&gt; /u/waescher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ock0lc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ock0lc/qwen3vl_kinda_sucks_in_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ock0lc/qwen3vl_kinda_sucks_in_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T17:58:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocgao6</id>
    <title>Noob starting advice please: I'm building a community-based RP model for a video-game character</title>
    <updated>2025-10-21T15:38:55+00:00</updated>
    <author>
      <name>/u/Pangolin_Beatdown</name>
      <uri>https://old.reddit.com/user/Pangolin_Beatdown</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think this project is pretty simple. I want to build a chatbot that speaks and behaves like a specific character (Alistair) from a specific game (Dragon Age Origins). I think the community can generate several thousand high-quality training examples to capture his specific personality, but i understand fine tuning an RP chatbot takes 50k-100k examples. &lt;/p&gt; &lt;p&gt;The model will be entirely locally-hosted, no API calls to the web, no cutting edge LLMs.&lt;/p&gt; &lt;p&gt;I want to fine-tune this model on my 3090, which runs Qwen2.5:32B very well (for example). I want the fully trained model to be able to run on gaming laptops with 8GB vram, so 7B or smaller would be best for the final deployed model (or have a small version, and then another version for people with more VRAM).&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;I assume I can come up with 2000 very high quality training examples hand-written by community members from the game dialog.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Can I find a general-purpose (personality-agnostic) training set for the initial fine--tune, then do a second round of fine-tuning, weighted, with our personality examples? Can anyone suggest some appropriate sets and where to find them? Most RP chatbots seem to be women and flirty in a way that doesn't suit our character.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What are the best pre-tuned models for an RP chatbot?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Has anyone done a similar project that you can point me to? &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I plan to provide Knowledge base files that describe the environments in the game (Denerim city etc for you DAO nerds) so our NPC behaves appropriately in-context. Different system prompts will allow the user to start their chat at specific points in the game with a known world state, and play forward from there with original model-generated conversations and choices.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It would be cool to add conversation summary save to give continuity between sessions. Maybe update specific game plot parameters.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It would be cool to build in some radiant-quest givers that generate plot-appropriate quests.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I know and ennvision running this in openwebui but I know other UIs maybe better suited to this task, can you recommend?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pangolin_Beatdown"&gt; /u/Pangolin_Beatdown &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocgao6/noob_starting_advice_please_im_building_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocgao6/noob_starting_advice_please_im_building_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocgao6/noob_starting_advice_please_im_building_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T15:38:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocj4w8</id>
    <title>Llama-Embed-Nemotron-8B Takes the Top Spot on MMTEB Multilingual Retrieval Leaderboard</title>
    <updated>2025-10-21T17:26:05+00:00</updated>
    <author>
      <name>/u/PDXcoder2000</name>
      <uri>https://old.reddit.com/user/PDXcoder2000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocj4w8/llamaembednemotron8b_takes_the_top_spot_on_mmteb/"&gt; &lt;img alt="Llama-Embed-Nemotron-8B Takes the Top Spot on MMTEB Multilingual Retrieval Leaderboard" src="https://external-preview.redd.it/dMdQFElElXfyOj3dCITHMNWHT928JuIqOE8NxO-zqNU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ff4a54d8ef7e031aa7399b4c4e5d6a20fd14935" title="Llama-Embed-Nemotron-8B Takes the Top Spot on MMTEB Multilingual Retrieval Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For developers working on multilingual search or similarity tasks, Llama‚ÄëEmbed‚ÄëNemotron‚Äë8B might be worth checking out. It‚Äôs designed to generate 4,096‚Äëdimensional embeddings that work well across languages ‚Äî especially useful for retrieval, re‚Äëranking, classification, and bi‚Äëtext mining projects.&lt;/p&gt; &lt;p&gt;What makes it stand out is how effectively it handles cross‚Äëlingual and low‚Äëresource queries, areas where many models still struggle. It was trained on a mix of 16 million query‚Äëdocument pairs (half public and half synthetic), combining model merging and careful hard‚Äënegative mining to boost accuracy.&lt;/p&gt; &lt;p&gt;Key details:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Strong performance for retrieval, re‚Äëranking, classification, and bi‚Äëtext mining&lt;/li&gt; &lt;li&gt;Handles low‚Äëresource and cross‚Äëlingual queries effectively&lt;/li&gt; &lt;li&gt;Trained on 16M query‚Äëdocument pairs (8M public + 8M synthetic)&lt;/li&gt; &lt;li&gt;Combines model merging and refined hard‚Äënegative mining for better accuracy&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model is built on meta-llama/Llama‚Äë3.1‚Äë8B and uses the &lt;a href="https://huggingface.co/datasets/nvidia/Nemotron-CC-v2."&gt;Nemotron‚ÄëCC‚Äëv2 dataset&lt;/a&gt; and it‚Äôs now ranked first on the &lt;a href="https://huggingface.co/spaces/mteb/leaderboard"&gt;MMTEB multilingual retrieval leaderboard&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;üìñ Read our &lt;a href="https://huggingface.co/blog/nvidia/llama-embed-nemotron-8b"&gt;blog &lt;/a&gt;on Hugging Face to learn more about the model, architectural highlights, training methodology, performance evaluation and more.&lt;/p&gt; &lt;p&gt;üí°If you‚Äôve got suggestions or ideas, we are inviting feedback at &lt;a href="http://nemotron.ideas.nvidia.com"&gt;http://nemotron.ideas.nvidia.com&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/oqhem2nz1iwf1.gif"&gt;https://i.redd.it/oqhem2nz1iwf1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PDXcoder2000"&gt; /u/PDXcoder2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocj4w8/llamaembednemotron8b_takes_the_top_spot_on_mmteb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocj4w8/llamaembednemotron8b_takes_the_top_spot_on_mmteb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocj4w8/llamaembednemotron8b_takes_the_top_spot_on_mmteb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T17:26:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc3f0i</id>
    <title>Qwen3 Omni interactive speech</title>
    <updated>2025-10-21T04:21:40+00:00</updated>
    <author>
      <name>/u/Powerful-Angel-301</name>
      <uri>https://old.reddit.com/user/Powerful-Angel-301</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 Omni is very interesting. They claim it supports real-time voice, but I couldn't find out how and there was no tutorial for this on their github. &lt;/p&gt; &lt;p&gt;Anyone having any experience with that? Basically continuously talk to the model and get voice responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Angel-301"&gt; /u/Powerful-Angel-301 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc3f0i/qwen3_omni_interactive_speech/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc3f0i/qwen3_omni_interactive_speech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc3f0i/qwen3_omni_interactive_speech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T04:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1obn0q7</id>
    <title>The Innovations in DeepSeek OCR</title>
    <updated>2025-10-20T16:29:30+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek just released a pretty shocking new paper. They really buried the lede here by referring to it simply as DeepSeek OCR. &lt;/p&gt; &lt;p&gt;While it‚Äôs a very strong OCR model, the purpose of it and the implications of their approach go far beyond what you‚Äôd expect of ‚Äúyet another OCR model.‚Äù&lt;/p&gt; &lt;p&gt;Traditionally, vision LLM tokens almost seemed like an afterthought or ‚Äúbolt on‚Äù to the LLM paradigm. And 10k words of English would take up far more space in a multimodal LLM when expressed as intelligible pixels than when expressed as tokens.&lt;/p&gt; &lt;p&gt;So those 10k words may have turned into 15k tokens, or 30k to 60k ‚Äúvisual tokens.‚Äù So vision tokens were way less efficient and really only made sense to use for data that couldn‚Äôt be effectively conveyed with words. &lt;/p&gt; &lt;p&gt;But that gets inverted now from the ideas in this paper. DeepSeek figured out how to get 10x better compression using vision tokens than with text tokens! So you could theoretically store those 10k words in just 1,500 of their special compressed visual tokens.&lt;/p&gt; &lt;p&gt;This might not be as unexpected as it sounds if you think of how your own mind works. After all, I know that when I‚Äôm looking for a part of a book that I‚Äôve already read, I imagine it visually and always remember which side of the book it was on and approximately where on the page it was, which suggests some kind of visual memory representation at work.&lt;/p&gt; &lt;p&gt;Now, it‚Äôs not clear how exactly this interacts with the other downstream cognitive functioning of an LLM; can the model reason as intelligently over those compressed visual tokens as it can using regular text tokens? Does it make the model less articulate by forcing it into a more vision-oriented modality? &lt;/p&gt; &lt;p&gt;But you can imagine that, depending on the exact tradeoffs, it could be a very exciting new axis to greatly expand effective context sizes. Especially when combined with DeepSeek‚Äôs other recent paper from a couple weeks ago about sparse attention.&lt;/p&gt; &lt;p&gt;For all we know, Google could have already figured out something like this, which could explain why Gemini has such a huge context size and is so good and fast at OCR tasks. If they did, they probably wouldn‚Äôt say because it would be viewed as an important trade secret.&lt;/p&gt; &lt;p&gt;But the nice thing about DeepSeek is that they‚Äôve made the entire thing open source and open weights and explained how they did it, so now everyone can try it out and explore.&lt;/p&gt; &lt;p&gt;Even if these tricks make attention more lossy, the potential of getting a frontier LLM with a 10 or 20 million token context window is pretty exciting. &lt;/p&gt; &lt;p&gt;You could basically cram all of a company‚Äôs key internal documents into a prompt preamble and cache this with OpenAI and then just add your specific query or prompt on top of that and not have to deal with search tools and still have it be fast and cost-effective. &lt;/p&gt; &lt;p&gt;Or put an entire code base into the context and cache it, and then just keep appending the equivalent of the git diffs as you make changes to the code. &lt;/p&gt; &lt;p&gt;If you‚Äôve ever read stories about the great physicist Hans Bethe, he was known for having vast amounts of random physical facts memorized (like the entire periodic table; boiling points of various substances, etc.) so that he could seamlessly think and compute without ever having to interrupt his flow to look something up in a reference table. &lt;/p&gt; &lt;p&gt;Having vast amounts of task-specific knowledge in your working memory is extremely useful. This seems like a very clever and additive approach to potentially expanding that memory bank by 10x or more.&lt;/p&gt; &lt;p&gt;source: &lt;a href="https://x.com/doodlestein/status/1980282222893535376"&gt;https://x.com/doodlestein/status/1980282222893535376&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obn0q7/the_innovations_in_deepseek_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obn0q7/the_innovations_in_deepseek_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obn0q7/the_innovations_in_deepseek_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T16:29:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1occcel</id>
    <title>SmolVLM AWQ Text Quantization (4 GB ‚Üí 2GB with minimal quality loss on DocVQA)</title>
    <updated>2025-10-21T13:02:06+00:00</updated>
    <author>
      <name>/u/Ok_Employee_6418</name>
      <uri>https://old.reddit.com/user/Ok_Employee_6418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occcel/smolvlm_awq_text_quantization_4_gb_2gb_with/"&gt; &lt;img alt="SmolVLM AWQ Text Quantization (4 GB ‚Üí 2GB with minimal quality loss on DocVQA)" src="https://external-preview.redd.it/Kjegehdr73l6a0EStswJLB7yLrGnAt87gT0UjQYkxvk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e9752bb52ed840b99326e2e047a618fbe01dff3c" title="SmolVLM AWQ Text Quantization (4 GB ‚Üí 2GB with minimal quality loss on DocVQA)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing AWQ and GPTQ quantized versions of SmolVLM from Hugging Face. &lt;/p&gt; &lt;p&gt;These models only had their text models quantized, and had a 50% model size reduction (4GB~2GB) while keeping model degradation under 1% on the DocVQA benchmark. &lt;/p&gt; &lt;p&gt;#huggingface #smolvlm #smollm&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Employee_6418"&gt; /u/Ok_Employee_6418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ronantakizawa/SmolVLM-Instruct-awq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occcel/smolvlm_awq_text_quantization_4_gb_2gb_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1occcel/smolvlm_awq_text_quantization_4_gb_2gb_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T13:02:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocjhug</id>
    <title>I built an offline-first voice AI with &lt;1 s latency on my Mac M3</title>
    <updated>2025-10-21T17:39:28+00:00</updated>
    <author>
      <name>/u/mshubham</name>
      <uri>https://old.reddit.com/user/mshubham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocjhug/i_built_an_offlinefirst_voice_ai_with_1_s_latency/"&gt; &lt;img alt="I built an offline-first voice AI with &amp;lt;1 s latency on my Mac M3" src="https://external-preview.redd.it/xB_CA3iDlXtwzT5bC0DnSUQZ7myr5MTWUPuiuHH7JC8.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=41d158b450f13d467631338511219a4c0be4dd41" title="I built an offline-first voice AI with &amp;lt;1 s latency on my Mac M3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So... I built an offline-first voice AI from scratch ‚Äî no LiveKit, Pipecat, or any framework.&lt;/p&gt; &lt;p&gt;A perfectly blended pipeline of VAD + Turn Detection + STT + LLM + TTS.&lt;/p&gt; &lt;p&gt;Runs locally on my M3 Pro, replies in &amp;lt; 1 s, and stays under 1 K lines of code ‚Äî with a minimal UI.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xmmn9fsg8iwf1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=606ce0e136a49952c0878b4313e1175609657bbf"&gt;https://preview.redd.it/xmmn9fsg8iwf1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=606ce0e136a49952c0878b4313e1175609657bbf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/6IEK2fXB_ok"&gt;Youtube Demo&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/shubhdotai/offline-voice-ai"&gt;Gtihub Repo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mshubham"&gt; /u/mshubham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocjhug/i_built_an_offlinefirst_voice_ai_with_1_s_latency/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocjhug/i_built_an_offlinefirst_voice_ai_with_1_s_latency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocjhug/i_built_an_offlinefirst_voice_ai_with_1_s_latency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T17:39:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc8dqx</id>
    <title>Vascura FRONT - Open Source (Apache 2.0), Bloat Free, Portable and Lightweight (288 kb) LLM Frontend.</title>
    <updated>2025-10-21T09:31:29+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc8dqx/vascura_front_open_source_apache_20_bloat_free/"&gt; &lt;img alt="Vascura FRONT - Open Source (Apache 2.0), Bloat Free, Portable and Lightweight (288 kb) LLM Frontend." src="https://external-preview.redd.it/Z24wYzZvdXdvZndmMTaXKbAxEnkCSlwbwmJDXf_lyDQzd483n4JJoFhjK3xD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae56ff1f5b7f8b918196f743f18a36cbbf044718" title="Vascura FRONT - Open Source (Apache 2.0), Bloat Free, Portable and Lightweight (288 kb) LLM Frontend." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4oaz6nuwofwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc8dqx/vascura_front_open_source_apache_20_bloat_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc8dqx/vascura_front_open_source_apache_20_bloat_free/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T09:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc8vb4</id>
    <title>Do you have any ideas for OCR on pages of documents with very very low contrast?</title>
    <updated>2025-10-21T10:01:29+00:00</updated>
    <author>
      <name>/u/suelzsuelz</name>
      <uri>https://old.reddit.com/user/suelzsuelz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc8vb4/do_you_have_any_ideas_for_ocr_on_pages_of/"&gt; &lt;img alt="Do you have any ideas for OCR on pages of documents with very very low contrast?" src="https://preview.redd.it/yhbgv2pztfwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ac61a3033e6e868cec48291c7feaa1ca30e5a51" title="Do you have any ideas for OCR on pages of documents with very very low contrast?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My use case is to locally extract pdf content into Markdown or JSON-structured data. The problem, as demonstrated by the example, is that the contrast between the text and background is very poor.&lt;/p&gt; &lt;p&gt;Has anyone ever processed similar documents?&lt;br /&gt; Which local models with how many parameters can do this reliably? &lt;/p&gt; &lt;p&gt;Newer cloud models don't seem to have any problems. We have already tested these:&lt;/p&gt; &lt;p&gt;- granite3.2-vision&lt;br /&gt; - minicpm-v2.6:8b&lt;br /&gt; - llama3.2-vision:11b&lt;br /&gt; - DeepSeek-OCR&lt;/p&gt; &lt;p&gt;Maybe they are just too small?&lt;/p&gt; &lt;p&gt;We are able to use a 4 x RTX 3090 Workstation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/suelzsuelz"&gt; /u/suelzsuelz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yhbgv2pztfwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc8vb4/do_you_have_any_ideas_for_ocr_on_pages_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc8vb4/do_you_have_any_ideas_for_ocr_on_pages_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T10:01:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocko1m</id>
    <title>Comparison new qwen 32b-vl vs qwen 30a3-vl</title>
    <updated>2025-10-21T18:22:10+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocko1m/comparison_new_qwen_32bvl_vs_qwen_30a3vl/"&gt; &lt;img alt="Comparison new qwen 32b-vl vs qwen 30a3-vl" src="https://b.thumbs.redditmedia.com/IQJxhOFgNXeP5HldjZTtWHPpItZ5QEcogwrGdma7mqE.jpg" title="Comparison new qwen 32b-vl vs qwen 30a3-vl" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ocko1m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocko1m/comparison_new_qwen_32bvl_vs_qwen_30a3vl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocko1m/comparison_new_qwen_32bvl_vs_qwen_30a3vl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T18:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc7uio</id>
    <title>DeepSeek-OCR Playground ‚Äî Dockerized FastAPI + React workbench (5090-ready), image ‚Üí text/description, more to come</title>
    <updated>2025-10-21T08:56:32+00:00</updated>
    <author>
      <name>/u/Putrid_Passion_6916</name>
      <uri>https://old.reddit.com/user/Putrid_Passion_6916</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Repo: &lt;a href="https://github.com/rdumasia303/deepseek_ocr_app"&gt;https://github.com/rdumasia303/deepseek_ocr_app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TL;DR: A tiny web app to mess with the new DeepSeek-OCR locally. Upload an image, pick a mode (Plain OCR, Describe, Find/grounding, Freeform), and get results instantly. &lt;/p&gt; &lt;p&gt;It runs in Docker with GPU (tested on 5090/Blackwell), has a slick UI, and is ‚Äúgood enough‚Äù to ship &amp;amp; let the community break/fix/improve it. PRs welcome.&lt;/p&gt; &lt;p&gt;What‚Äôs inside&lt;/p&gt; &lt;p&gt;Frontend: React/Vite + glassy Tailwind UI (drag-drop, live preview, copy/download). Backend: FastAPI + Transformers, calls DeepSeek-OCR with eval_mode=True. GPU: Blackwell-friendly (bfloat16), designed to run on RTX 5090 (or any CUDA GPU).&lt;/p&gt; &lt;p&gt;Modes shipped now: Plain OCR (super strong) Describe (short freeform caption) Find (grounding) ‚Äî returns boxes for a term (e.g., ‚ÄúTotal Due‚Äù, ‚ÄúSignature‚Äù) Freeform ‚Äî your own instruction&lt;/p&gt; &lt;p&gt;There‚Äôs groundwork laid for more modes (Markdown, Tables‚ÜíCSV/MD, KV‚ÜíJSON, PII, Layout map). If you add one, make a PR!&lt;/p&gt; &lt;p&gt;Quick start&lt;/p&gt; &lt;h1&gt;clone&lt;/h1&gt; &lt;p&gt;git clone &lt;a href="https://github.com/rdumasia303/deepseek_ocr_app"&gt;https://github.com/rdumasia303/deepseek_ocr_app&lt;/a&gt; cd deepseek_ocr_app&lt;/p&gt; &lt;h1&gt;run&lt;/h1&gt; &lt;p&gt;docker compose up -d --build&lt;/p&gt; &lt;h1&gt;open&lt;/h1&gt; &lt;h1&gt;frontend: http://localhost:3000 (or whatever the repo says)&lt;/h1&gt; &lt;h1&gt;backend: http://localhost:8000/docs&lt;/h1&gt; &lt;p&gt;Heads-up: First model load downloads weights + custom code (trust_remote_code). If you want reproducibility, pin a specific HF revision in the backend.&lt;/p&gt; &lt;p&gt;Sample prompts (try these) Plain OCR: (no need to type anything ‚Äî just run the mode) Describe: ‚ÄúDescribe this image concisely in 2‚Äì3 sentences.‚Äù Find: set term to Total Due, Signature, Logo, etc. Freeform: ‚ÄúConvert the document to markdown.‚Äù ‚ÄúExtract every table and output CSV only.‚Äù ‚ÄúReturn strict JSON with fields {invoice_no, date, vendor, total:{amount,currency}}.‚Äù Known rough edges (be gentle, or better, fix them üòÖ)&lt;/p&gt; &lt;p&gt;Grounding (boxes) can be flaky; plain OCR and describe are rock-solid. Structured outputs (CSV/MD/JSON) need post-processing to be 100% reliable.&lt;/p&gt; &lt;p&gt;Roadmap / ideas (grab an issue &amp;amp; go wild)&lt;/p&gt; &lt;p&gt;Add Markdown / Tables / JSON / PII / Layout modes (OCR-first with deterministic fallbacks).&lt;/p&gt; &lt;p&gt;Proper box overlay scaling (processed size vs CSS pixels) ‚Äî coords should snap exactly.&lt;/p&gt; &lt;p&gt;PDF ingestion (pdf2image ‚Üí per-page OCR + merge).&lt;/p&gt; &lt;p&gt;Simple telemetry (mode counts, latency, GPU mem) for perf tuning.&lt;/p&gt; &lt;p&gt;One-click HuggingFace revision pin to avoid surprise code updates. If you try it, please drop feedback ) ‚Äî I‚Äôll iterate. If you make it better, I‚Äôll take your PRs ASAP. üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Putrid_Passion_6916"&gt; /u/Putrid_Passion_6916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc7uio/deepseekocr_playground_dockerized_fastapi_react/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc7uio/deepseekocr_playground_dockerized_fastapi_react/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc7uio/deepseekocr_playground_dockerized_fastapi_react/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T08:56:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocbggm</id>
    <title>Poll on thinking/no thinking for the next open-weights Google model</title>
    <updated>2025-10-21T12:22:21+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/osanseviero/status/1980553451261292628"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocbggm/poll_on_thinkingno_thinking_for_the_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocbggm/poll_on_thinkingno_thinking_for_the_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T12:22:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocbkry</id>
    <title>[By GLM Team] Glyph: Scaling Context Windows via Visual-Text Compression</title>
    <updated>2025-10-21T12:27:54+00:00</updated>
    <author>
      <name>/u/NeterOster</name>
      <uri>https://old.reddit.com/user/NeterOster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2510.17800"&gt;https://arxiv.org/abs/2510.17800&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling context windows to the million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. In this work, we take a different perspective-visual context scaling-to tackle this challenge. Instead of extending token-based sequences, we propose Glyph, a framework that renders long texts into images and processes them with vision-language models (VLMs). This approach substantially compresses textual input while preserving semantic information, and we further design an LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression. Through extensive experiments, we demonstrate that our method achieves 3-4x token compression while maintaining accuracy comparable to leading LLMs such as Qwen3-8B on various long-context benchmarks. This compression also leads to around 4x faster prefilling and decoding, and approximately 2x faster SFT training. Furthermore, under extreme compression, a 128K-context VLM could scale to handle 1M-token-level text tasks. In addition, the rendered text data benefits real-world multimodal tasks, such as document understanding. Our code and model are released at &lt;a href="https://github.com/thu-coai/Glyph"&gt;this https URL&lt;/a&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The model is not yet available at the moment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeterOster"&gt; /u/NeterOster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocbkry/by_glm_team_glyph_scaling_context_windows_via/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocbkry/by_glm_team_glyph_scaling_context_windows_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocbkry/by_glm_team_glyph_scaling_context_windows_via/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T12:27:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1occ8uv</id>
    <title>Confirmed: Junk social media data makes LLMs dumber</title>
    <updated>2025-10-21T12:58:04+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occ8uv/confirmed_junk_social_media_data_makes_llms_dumber/"&gt; &lt;img alt="Confirmed: Junk social media data makes LLMs dumber" src="https://a.thumbs.redditmedia.com/It6udXwo12VzdtUYx3sIoCiTZnQLhz-QLVIedLuaJN8.jpg" title="Confirmed: Junk social media data makes LLMs dumber" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new study from Texas A&amp;amp;M University and Purdue University proposes the &lt;em&gt;LLM Brain Rot Hypothesis&lt;/em&gt;: continual pretraining on ‚Äújunk‚Äù social-media text (short, viral, sensational content) causes lasting declines in reasoning, long-context and safety.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wq569rzfpgwf1.png?width=2772&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7a14a98cc9682cd209918c93fa23222d2df7b23"&gt;https://preview.redd.it/wq569rzfpgwf1.png?width=2772&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7a14a98cc9682cd209918c93fa23222d2df7b23&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ARC-Challenge with Chain Of Thoughts drops 74.9 ‚Üí 57.2 and RULER-CWE 84.4 ‚Üí 52.3 as junk ratio rises from 0% to 100%.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occ8uv/confirmed_junk_social_media_data_makes_llms_dumber/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occ8uv/confirmed_junk_social_media_data_makes_llms_dumber/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1occ8uv/confirmed_junk_social_media_data_makes_llms_dumber/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T12:58:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1och0jn</id>
    <title>Nvidia quietly released RTX Pro 5000 Blackwell 72Gb</title>
    <updated>2025-10-21T16:05:55+00:00</updated>
    <author>
      <name>/u/AleksHop</name>
      <uri>https://old.reddit.com/user/AleksHop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/nvidia/comments/1oc76i7/nvidia_quietly_launches_rtx_pro_5000_blackwell/"&gt;https://www.reddit.com/r/nvidia/comments/1oc76i7/nvidia_quietly_launches_rtx_pro_5000_blackwell/&lt;/a&gt;&lt;br /&gt; Price will be about 5000$&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AleksHop"&gt; /u/AleksHop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1och0jn/nvidia_quietly_released_rtx_pro_5000_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1och0jn/nvidia_quietly_released_rtx_pro_5000_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1och0jn/nvidia_quietly_released_rtx_pro_5000_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T16:05:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1occan8</id>
    <title>vLLM + OpenWebUI + Tailscale = private, portable AI</title>
    <updated>2025-10-21T13:00:13+00:00</updated>
    <author>
      <name>/u/zhambe</name>
      <uri>https://old.reddit.com/user/zhambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occan8/vllm_openwebui_tailscale_private_portable_ai/"&gt; &lt;img alt="vLLM + OpenWebUI + Tailscale = private, portable AI" src="https://b.thumbs.redditmedia.com/cRxS8E_CLjIUT07vC7k-8ob1jCAW2BZimcQfFKJicNg.jpg" title="vLLM + OpenWebUI + Tailscale = private, portable AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My mind is positively blown... My own AI?!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zhambe"&gt; /u/zhambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1occan8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occan8/vllm_openwebui_tailscale_private_portable_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1occan8/vllm_openwebui_tailscale_private_portable_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T13:00:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocgun0</id>
    <title>DeepSeek-OCR AI can scan an entire microfiche sheet and not just cells and retain 100% of the data in seconds...</title>
    <updated>2025-10-21T16:00:06+00:00</updated>
    <author>
      <name>/u/Xtianus21</name>
      <uri>https://old.reddit.com/user/Xtianus21</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocgun0/deepseekocr_ai_can_scan_an_entire_microfiche/"&gt; &lt;img alt="DeepSeek-OCR AI can scan an entire microfiche sheet and not just cells and retain 100% of the data in seconds..." src="https://b.thumbs.redditmedia.com/Dsj93JBsqXXjXoOsuIY5K8HbnnE2QjHtG5dVlESoKDU.jpg" title="DeepSeek-OCR AI can scan an entire microfiche sheet and not just cells and retain 100% of the data in seconds..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/BrianRoemmele/status/1980634806145957992"&gt;https://x.com/BrianRoemmele/status/1980634806145957992&lt;/a&gt;&lt;/p&gt; &lt;p&gt;AND &lt;/p&gt; &lt;p&gt;Have a full understanding of the text/complex drawings and their context. &lt;/p&gt; &lt;p&gt;I just changed offline data curation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xtianus21"&gt; /u/Xtianus21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ocgun0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocgun0/deepseekocr_ai_can_scan_an_entire_microfiche/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocgun0/deepseekocr_ai_can_scan_an_entire_microfiche/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T16:00:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1occyly</id>
    <title>Qwen3-Next 80B-A3B llama.cpp implementation with CUDA support half-working already (up to 40k context only), also Instruct GGUFs</title>
    <updated>2025-10-21T13:28:06+00:00</updated>
    <author>
      <name>/u/Ok_Top9254</name>
      <uri>https://old.reddit.com/user/Ok_Top9254</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occyly/qwen3next_80ba3b_llamacpp_implementation_with/"&gt; &lt;img alt="Qwen3-Next 80B-A3B llama.cpp implementation with CUDA support half-working already (up to 40k context only), also Instruct GGUFs" src="https://preview.redd.it/a21ouwhkvgwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=242dfbde4c1caaaa4f35057e48df50de5e9cc8f6" title="Qwen3-Next 80B-A3B llama.cpp implementation with CUDA support half-working already (up to 40k context only), also Instruct GGUFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095#issuecomment-3424224842"&gt;Llama.cpp pull request&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF"&gt;GGUFs for Instruct model&lt;/a&gt; (old news but info for the uninitiated)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Top9254"&gt; /u/Ok_Top9254 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a21ouwhkvgwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occyly/qwen3next_80ba3b_llamacpp_implementation_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1occyly/qwen3next_80ba3b_llamacpp_implementation_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T13:28:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1och7m9</id>
    <title>Qwen3-VL-2B and Qwen3-VL-32B Released</title>
    <updated>2025-10-21T16:13:23+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1och7m9/qwen3vl2b_and_qwen3vl32b_released/"&gt; &lt;img alt="Qwen3-VL-2B and Qwen3-VL-32B Released" src="https://preview.redd.it/n4rx9o72phwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31c5eea069f1786b249324b0d23eca9977c6918b" title="Qwen3-VL-2B and Qwen3-VL-32B Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n4rx9o72phwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1och7m9/qwen3vl2b_and_qwen3vl32b_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1och7m9/qwen3vl2b_and_qwen3vl32b_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T16:13:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1obqkpe</id>
    <title>Best Local LLMs - October 2025</title>
    <updated>2025-10-20T19:06:06+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Welcome to the first monthly &amp;quot;Best Local LLMs&amp;quot; post!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;General&lt;/li&gt; &lt;li&gt;Agentic/Tool Use&lt;/li&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;Creative Writing/RP&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(look for the top level comments for each Application and please thread your responses under that)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:06:06+00:00</published>
  </entry>
</feed>
