<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-23T05:14:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qk9wef</id>
    <title>Have byte latent transformers seen adoption?</title>
    <updated>2026-01-22T23:09:56+00:00</updated>
    <author>
      <name>/u/EmbarrassedBiscotti9</name>
      <uri>https://old.reddit.com/user/EmbarrassedBiscotti9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I remember it seemed promising when the paper came out, offering a few tangible advantages, but I haven't seen any meaningful movement in that direction since then.&lt;/p&gt; &lt;p&gt;Have any noteworthy models adopted the BLT architecture that I may have missed?&lt;/p&gt; &lt;p&gt;I tried searching the sub but &amp;quot;byte latent transformer&amp;quot; shows mostly ByteDance results, and &amp;quot;BLT&amp;quot; only has results from shortly after the paper was published.&lt;/p&gt; &lt;p&gt;If not, are there any specific issues with the architecture to explain the lack of adoption? Or is it a matter of the benefits not being worth the logistical headaches/complexity/cost of speculative training runs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmbarrassedBiscotti9"&gt; /u/EmbarrassedBiscotti9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk9wef/have_byte_latent_transformers_seen_adoption/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk9wef/have_byte_latent_transformers_seen_adoption/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qk9wef/have_byte_latent_transformers_seen_adoption/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T23:09:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkcypo</id>
    <title>possibly stupid question, but is there a model I can run locally on a 1080Ti</title>
    <updated>2026-01-23T01:20:04+00:00</updated>
    <author>
      <name>/u/Flaky_Bullfrog_4905</name>
      <uri>https://old.reddit.com/user/Flaky_Bullfrog_4905</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR, I'm setting up a scaled content generation product. I need to generate large amounts of text (for now), and I don't really care about quality (for now) as I will probably go through many variants of prompts and processing workflows while I make something sensible.&lt;/p&gt; &lt;p&gt;I also want people to be able to test the product which will potentially also consume large amounts of tokens (e.g. processing 40 page transcripts type of thing). &lt;/p&gt; &lt;p&gt;People have spoken highly to me of Llama.&lt;/p&gt; &lt;p&gt;Speaking from complete ignorance, I have an old PC (i7-7700, 1080Ti 11GBvram, 16gb RAM) that i was debating using as a &amp;quot;server&amp;quot; solely to run a small model that can process inputs and spit out results. I don't want to spend $$$ on tokens throughout this process until I'm a fair bit closer to having the &amp;quot;final&amp;quot; state. &lt;/p&gt; &lt;p&gt;Is this even possible? Or would it be way too slow / clunky i.e. just a huge time sink / distraction vs switching to a cheaper model like haiku or whatever and spending $100 on tokens.&lt;/p&gt; &lt;p&gt;I know absolutely nothing about using models locally fwiw.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flaky_Bullfrog_4905"&gt; /u/Flaky_Bullfrog_4905 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkcypo/possibly_stupid_question_but_is_there_a_model_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkcypo/possibly_stupid_question_but_is_there_a_model_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkcypo/possibly_stupid_question_but_is_there_a_model_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T01:20:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjjrmq</id>
    <title>Fei Fei Li dropped a non-JEPA world model, and the spatial intelligence is insane</title>
    <updated>2026-01-22T03:39:33+00:00</updated>
    <author>
      <name>/u/coloradical5280</name>
      <uri>https://old.reddit.com/user/coloradical5280</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjjrmq/fei_fei_li_dropped_a_nonjepa_world_model_and_the/"&gt; &lt;img alt="Fei Fei Li dropped a non-JEPA world model, and the spatial intelligence is insane" src="https://external-preview.redd.it/dHl1a3MydnZsdGVnMeL77RZtngf3FeBaBzx1OTOSVuPnAYqXhpVZKKbs_rnV.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce79108f93f379daa30c29208725387fb2320ee4" title="Fei Fei Li dropped a non-JEPA world model, and the spatial intelligence is insane" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fei-Fei Li, the &amp;quot;godmother of modern AI&amp;quot; and a pioneer in computer vision, founded World Labs a few years ago with a small team and $230 million in funding. Last month, they launched &lt;a href="https://marble.worldlabs.ai/"&gt;https://marble.worldlabs.ai/&lt;/a&gt;, a generative world model that‚Äôs not JEPA, but instead built on Neural Radiance Fields (NeRF) and Gaussian splatting. &lt;/p&gt; &lt;p&gt;It‚Äôs &lt;em&gt;insanely fast&lt;/em&gt; for what it does, generating explorable 3D worlds in minutes. For example: &lt;a href="https://marble.worldlabs.ai/world/5b850e80-a587-48d7-9340-186e0bcbf46b"&gt;this scene&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;Crucially, it‚Äôs not video. The frames aren‚Äôt rendered on-the-fly as you move. Instead, it‚Äôs a fully stateful 3D environment represented as a dense cloud of Gaussian splats‚Äîeach with position, scale, rotation, color, and opacity. This means the world is persistent, editable, and supports non-destructive iteration. You can expand regions, modify materials, and even merge multiple worlds together. &lt;/p&gt; &lt;p&gt;You can share your world, others can build on it, and you can build on theirs. It natively supports VR (Vision Pro, Quest 3), and you can export splats or meshes for use in Unreal, Unity, or Blender via USDZ or GLB. &lt;/p&gt; &lt;p&gt;It's early, there are (very literally) rough edges, but it's crazy to think about this in 5 years. For free, you get a few generations to experiment; $20/month unlocks a lot, I just did one month so I could actually play, and definitely didn't max out credits. &lt;/p&gt; &lt;p&gt;Fei-Fei Li is an OG AI visionary, but zero hype. She‚Äôs been quiet, especially about this. So Marble hasn‚Äôt gotten the attention it deserves. &lt;/p&gt; &lt;p&gt;At first glance, visually, you might think, ‚Äúmeh‚Äù... but there‚Äôs &lt;strong&gt;no triangle-based geometry here, no real-time rendering pipeline, no frame-by-frame generation.&lt;/strong&gt; Just a solid, exportable, editable, stateful pile of splats. &lt;/p&gt; &lt;p&gt;The breakthrough isn't the image though, it‚Äôs the spatial intelligence. Y'all should play around, it's wild. &lt;/p&gt; &lt;p&gt;&lt;em&gt;I know this is a violation of Rule #2 but honestly there just aren't that many subs with people smart enough to appreciate this; no hard feelings if it needs be removed though.&lt;/em&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coloradical5280"&gt; /u/coloradical5280 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/udsg2ztvlteg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjjrmq/fei_fei_li_dropped_a_nonjepa_world_model_and_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjjrmq/fei_fei_li_dropped_a_nonjepa_world_model_and_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T03:39:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qke63u</id>
    <title>75 ‚ÄúMost Popular‚Äù agent skills nobody‚Äôs willing to share</title>
    <updated>2026-01-23T02:14:24+00:00</updated>
    <author>
      <name>/u/LandscapeAway8896</name>
      <uri>https://old.reddit.com/user/LandscapeAway8896</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qke63u/75_most_popular_agent_skills_nobodys_willing_to/"&gt; &lt;img alt="75 ‚ÄúMost Popular‚Äù agent skills nobody‚Äôs willing to share" src="https://external-preview.redd.it/MGh5bTJtbDFkMGZnMR56Et7yMLtH83SLJLlhPtZkZV0iKpi0NCNBYj4kjqib.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=149566fc26bc69d5f51e483111d6c99de84c2a3b" title="75 ‚ÄúMost Popular‚Äù agent skills nobody‚Äôs willing to share" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A Novel approach to Codebase Intelligence&lt;/p&gt; &lt;p&gt;Everyone working with AI has experienced it ‚Ä¶The code is good but it doesn‚Äôt fit your codebase. It fails to match how you handle auth calls or that weird fix you guys added to make web sockets work.&lt;/p&gt; &lt;p&gt;So I built Drift &lt;/p&gt; &lt;p&gt;Drift fixes this. It scans your codebase, learns YOUR patterns and then feed compressed, weighted, json formatted data to your agent in the MCP, CLI or VS code extensions.&lt;/p&gt; &lt;p&gt;Best part is? The source code is completely open source! Check it out here: &lt;a href="https://github.com/dadbodgeoff/drift"&gt;https://github.com/dadbodgeoff/drift&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What makes drift special?&lt;/p&gt; &lt;p&gt;Here‚Äôs the flow:&lt;/p&gt; &lt;p&gt;Your Code ‚Üí Drift Scan ‚Üí Pattern Detection ‚Üí MCP Server ‚Üí AI understands your codebase&lt;/p&gt; &lt;p&gt;The Stack&lt;/p&gt; &lt;p&gt;@drift/core ‚Üí Parsing, detection, storage&lt;/p&gt; &lt;p&gt;@drift/detectors ‚Üí 150+ pattern detectors &lt;/p&gt; &lt;p&gt;@drift/mcp ‚Üí Model Context Protocol server&lt;/p&gt; &lt;p&gt;@drift/cli ‚Üí Command line interface&lt;/p&gt; &lt;p&gt;@drift/vscode ‚Üí VS Code extension&lt;/p&gt; &lt;p&gt;Example: Ask AI about your code&lt;/p&gt; &lt;p&gt;You: &amp;quot;How does auth work in this codebase?&amp;quot;&lt;/p&gt; &lt;p&gt;AI (via MCP): &amp;quot;Based on 47 pattern matches:&lt;/p&gt; &lt;p&gt;- JWT middleware in src/middleware/auth.ts&lt;/p&gt; &lt;p&gt;- Role checks use @RequireRole decorator&lt;/p&gt; &lt;p&gt;- 3 unprotected routes flagged as outliers&amp;quot;&lt;/p&gt; &lt;p&gt;Install drift today with: &lt;a href="https://www.npmjs.com/package/driftdetect"&gt;https://www.npmjs.com/package/driftdetect&lt;/a&gt;&lt;/p&gt; &lt;p&gt;npm install -g driftdetect&lt;/p&gt; &lt;p&gt;Ive also decided to release the biggest skill set ive seen with the secrets that no other person has been willing to share because it makes them an outlier. See the full list below..&lt;/p&gt; &lt;p&gt;üîê AUTH &amp;amp; SECURITY (9) ‚ö° RESILIENCE (10) üîß WORKERS (5)&lt;/p&gt; &lt;p&gt;‚îú‚îÄ jwt-auth ‚îú‚îÄ circuit-breaker ‚îú‚îÄ background-jobs&lt;/p&gt; &lt;p&gt;‚îú‚îÄ row-level-security ‚îú‚îÄ distributed-lock ‚îú‚îÄ dead-letter-queue&lt;/p&gt; &lt;p&gt;‚îú‚îÄ oauth-social-login ‚îú‚îÄ leader-election ‚îú‚îÄ job-state-machine&lt;/p&gt; &lt;p&gt;‚îú‚îÄ webhook-security ‚îú‚îÄ graceful-shutdown ‚îî‚îÄ worker-orchestration&lt;/p&gt; &lt;p&gt;‚îî‚îÄ audit-logging ‚îî‚îÄ checkpoint-resume&lt;/p&gt; &lt;p&gt;üìä DATA PIPELINE (10) üåê API (7) üì° REALTIME (5)&lt;/p&gt; &lt;p&gt;‚îú‚îÄ batch-processing ‚îú‚îÄ rate-limiting ‚îú‚îÄ websocket-management&lt;/p&gt; &lt;p&gt;‚îú‚îÄ fuzzy-matching ‚îú‚îÄ idempotency ‚îú‚îÄ sse-resilience&lt;/p&gt; &lt;p&gt;‚îú‚îÄ analytics-pipeline ‚îú‚îÄ api-versioning ‚îú‚îÄ atomic-matchmaking&lt;/p&gt; &lt;p&gt;‚îî‚îÄ scoring-engine ‚îî‚îÄ pagination ‚îî‚îÄ server-tick&lt;/p&gt; &lt;p&gt;ü§ñ AI (4) üí≥ INTEGRATIONS (4) üé® FRONTEND (4)&lt;/p&gt; &lt;p&gt;‚îú‚îÄ prompt-engine ‚îú‚îÄ stripe-integration ‚îú‚îÄ design-tokens&lt;/p&gt; &lt;p&gt;‚îú‚îÄ ai-coaching ‚îú‚îÄ email-service ‚îú‚îÄ mobile-components&lt;/p&gt; &lt;p&gt;‚îú‚îÄ ai-generation-client ‚îî‚îÄ oauth-integration ‚îî‚îÄ game-loop&lt;/p&gt; &lt;p&gt;‚îî‚îÄ provenance-audit&lt;/p&gt; &lt;p&gt;Ive built in silence to start my new passion and career. That ends now, from here on out its me and the community trying to find the way out of the permanent underclass before its to late‚Ä¶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LandscapeAway8896"&gt; /u/LandscapeAway8896 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h0g7o7o1d0fg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qke63u/75_most_popular_agent_skills_nobodys_willing_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qke63u/75_most_popular_agent_skills_nobodys_willing_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T02:14:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qke9d2</id>
    <title>PCIe bandwidth and LLM inference speed</title>
    <updated>2026-01-23T02:18:27+00:00</updated>
    <author>
      <name>/u/hainesk</name>
      <uri>https://old.reddit.com/user/hainesk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR: Does PCIe 4.0 x16 offer any performance uplift vs PCIe 3.0 x4 when it comes to inference?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My current setup involves connecting my video cards over oculink cables with bifurcated PCIe slots (X470 motherboard). The oculink signal doesn't work well at PCIe 4 speeds, so each card is connected at PCIe 3.0 x4.&lt;/p&gt; &lt;p&gt;What I've noticed is that actual generation speed doesn't seem to be hurt too much at this speed, but I'm wondering if prompt processing is delayed at that reduced speed. However with vLLM I am still able to get &amp;gt; 10k tps PP when doing something like 4x tensor parallel with GLM 4.5 Air.&lt;/p&gt; &lt;p&gt;I've considered upgrading to a Threadripper Pro or Epyc platform in order to get full x16 PCIe speeds, but I'm just wondering if there is any real benefit for that when it comes to LLM inferencing? Does anyone have any experience going from low bandwidth to high bandwidth PCIe and seen any significant difference or advantage?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hainesk"&gt; /u/hainesk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qke9d2/pcie_bandwidth_and_llm_inference_speed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qke9d2/pcie_bandwidth_and_llm_inference_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qke9d2/pcie_bandwidth_and_llm_inference_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T02:18:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qk7o8z</id>
    <title>PromptBridge-0.6b-Alpha</title>
    <updated>2026-01-22T21:39:59+00:00</updated>
    <author>
      <name>/u/reto-wyss</name>
      <uri>https://old.reddit.com/user/reto-wyss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a thingy.&lt;/p&gt; &lt;p&gt;I wanted something that can generate diverse and detailed text-to-image prompts for single subjects quickly.&lt;/p&gt; &lt;p&gt;So, after I had spend many-a-token on Qwen3-VL-32b and gpt-oss-120b, and time generating the &amp;quot;inspiration images&amp;quot; based on randomized keywords. I turn the prompts into a dataset, reversed them into keywords and one-sentence descriptions and then trained Qwen3-0.6b to expand (or compress, I trained both ways).&lt;/p&gt; &lt;p&gt;It's a shortcut from keywords to t2i prompts and it can also be used in round-trip mode: keywords -&amp;gt; prompt -&amp;gt; keywords -&amp;gt; prompt&lt;/p&gt; &lt;p&gt;Therefore, naturally generate diversity for similar themes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Original Short Prompt: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;woman, flowing red dress, standing, sunset beach&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Step 1: Expand: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A woman stands on a sandy beach at sunset, facing forward with a composed, neutral expression. Her fair skin shows a subtle, natural sheen from the warm light. She has a slender, athletic build and long, straight dark‚Äëbrown hair that falls to the middle of her back, the strands catching faint highlights that suggest a soft, slightly wavy texture. Her face is symmetrical, [...]&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Step 2: Compress to Keywords: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;fair-skinned athletic woman, dark brown wavy hair, red sleeveless dress, standing on sand beach, sunset golden glow, left arm bent hand on hip, right arm relaxed palm inward, dark leather sandals, silver hoop earring necklace, thin matte black chain necklace, silver bracelet on wrist, large orange teal sky, gentle ripples and dunes, warm diffused lighting&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Step 3: Expand Again: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;A woman stands on a sandy beach at sunset, her body angled slightly to the right while she faces the camera. Her dark brown hair is long, wavy, and falls past her shoulders, with a few strands framing her face and catching the warm light. She has fair skin, high cheekbones, full lips, and dark brown eyes that look directly at the viewer [...]&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It works with vllm (single 5090 will do about 12k tg/s with 100 concurrent requests).&lt;/li&gt; &lt;li&gt;It's on Huggingface: &lt;a href="https://huggingface.co/retowyss/PromptBridge-0.6b-Alpha"&gt;https://huggingface.co/retowyss/PromptBridge-0.6b-Alpha&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Space (ZERO) for testing: &lt;a href="https://huggingface.co/spaces/retowyss/PromptBridge-Demo"&gt;https://huggingface.co/spaces/retowyss/PromptBridge-Demo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I have no experience converting to gguf, 4bit may be interesting for a standalone webapp. I might try that. Feedback is very welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reto-wyss"&gt; /u/reto-wyss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk7o8z/promptbridge06balpha/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk7o8z/promptbridge06balpha/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qk7o8z/promptbridge06balpha/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T21:39:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjxp4p</id>
    <title>VibeVoice LoRAs are a thing</title>
    <updated>2026-01-22T15:36:02+00:00</updated>
    <author>
      <name>/u/llamabott</name>
      <uri>https://old.reddit.com/user/llamabott</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wasn't aware of this until recently, but started experimenting with them for the last couple days. Some learnings below, plus some sample output.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Trainer:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This trainer has worked very well so far: &lt;a href="https://github.com/voicepowered-ai/VibeVoice-finetuning"&gt;https://github.com/voicepowered-ai/VibeVoice-finetuning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The sample arguments in the README for using a local dataset are fine, but &lt;code&gt;--voice_prompt_drop_rate&lt;/code&gt;should be set to 1 for single-speaker training. Also, lowering gradient accumulation steps to like 4 helps. Training against the 1.5B model fills up the full 24GB of my 4090. I've found all intermediate checkpoints starting from 15 minutes on ('wall clock time') to be very usable. Further training yields incremental improvements, though sometimes hard to tell one way or the other. And it seems pretty difficult to fry the lora, at least with datasets I've been using, which have ranged from 45 minutes to 2 hours' worth of audio.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pros/cons;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Using loras instead of voice clone samples resolves the most important weaknesses of the 1.5B model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No more random music (yes really)&lt;/li&gt; &lt;li&gt;No more chronic truncation of the last word of a prompt&lt;/li&gt; &lt;li&gt;No more occurrences of a reference voice prompt &lt;em&gt;leaking&lt;/em&gt; into the audio output (that's the one that really kills me)&lt;/li&gt; &lt;li&gt;Dramatically lower word error rate all the way around, equaling the 7B model + zero shot voice clone or basically any other open weight TTS model I've tried for that matter.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In terms of raw voice likeness, my loras thus far have ranged from just okay to very good, but can't quite match the results of simple zero shot voice cloning. But the more unique the qualities of the source vocal material are, the better (though I guess that's always the case, regardless). &lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to run:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The gradio demo in the &lt;a href="https://github.com/vibevoice-community/VibeVoice"&gt;VibeVoice Community repo&lt;/a&gt; accepts loras by adding a command line argument `--checkpoint_path path/to/checkpoint`.&lt;/p&gt; &lt;p&gt;And I just added vibevoice lora support to my audiobook creator app &lt;a href="https://github.com/zeropointnine/tts-audiobook-tool"&gt;tts-audiobook-tool&lt;/a&gt; (&lt;code&gt;Voice clone and model settings&lt;/code&gt; &amp;gt; &lt;code&gt;Lora&lt;/code&gt;, and enter either a local path or a huggingface dataset repo id). &lt;/p&gt; &lt;p&gt;CFG matters a lot and should be experimented with whenever testing a new checkpoint. A very low CFG (approaching 1.0) tends to be more raw, more sibilant (which can be good or bad, depending), and sometimes gives a greater likeness but also less stable. ~3.0 is usually my preference: More stable, often yields a fuller sound, and should still maintain good likeness without starting to sound generic if you've cherrypicked the right checkpoint.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Examples:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://zeropointnine.github.io/tts-audiobook-tool/browser_player/?url=https://zeropointnine.github.io/tts-audiobook-tool/browser_player/waves-vibevoice-1.5b-lora-hsrjl.abr.m4a"&gt;Here's some sample output&lt;/a&gt; using a lora I made using the settings described above and generated through tts-audiobook-tool (The web player is a feature of the project).&lt;/p&gt; &lt;p&gt;Not sure I should share the lora itself, but bonus points if you recognize the vocal source material and in which case, you'll be able to form opinions about likeness.&lt;/p&gt; &lt;p&gt;I did, however, create a lora using public domain source material for the purpose of sharing: &lt;a href="https://huggingface.co/vibevoice-community/klett"&gt;vibevoice-community/klett&lt;/a&gt;. Sound quality is somewhat compromised by the source audio and I'm not that crazy about the degree of likeness, but it can still be useful as a point of reference. (&lt;a href="https://zeropointnine.github.io/tts-audiobook-tool/browser_player/?url=https://zeropointnine.github.io/tts-audiobook-tool/browser_player/waves-vibevoice-1.5b-lora-klett.abr.m4a"&gt;sample output&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/llamabott"&gt; /u/llamabott &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjxp4p/vibevoice_loras_are_a_thing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjxp4p/vibevoice_loras_are_a_thing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjxp4p/vibevoice_loras_are_a_thing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T15:36:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkdeke</id>
    <title>Repurposed an old rig into a 64gb vram build. What local models would you recommend?</title>
    <updated>2026-01-23T01:40:26+00:00</updated>
    <author>
      <name>/u/grunt_monkey_</name>
      <uri>https://old.reddit.com/user/grunt_monkey_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkdeke/repurposed_an_old_rig_into_a_64gb_vram_build_what/"&gt; &lt;img alt="Repurposed an old rig into a 64gb vram build. What local models would you recommend?" src="https://external-preview.redd.it/x4pjZxmWhhHP4fIa5li1ma0pqFHNA_jzuimlp5pk4GA.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35c4d876cb971d73e128218a2561ade113c45bde" title="Repurposed an old rig into a 64gb vram build. What local models would you recommend?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grunt_monkey_"&gt; /u/grunt_monkey_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/w5XWduo.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkdeke/repurposed_an_old_rig_into_a_64gb_vram_build_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkdeke/repurposed_an_old_rig_into_a_64gb_vram_build_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T01:40:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjt08m</id>
    <title>Sleeping on Engram</title>
    <updated>2026-01-22T12:16:42+00:00</updated>
    <author>
      <name>/u/cravic</name>
      <uri>https://old.reddit.com/user/cravic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The more I look at it the more I am convinced that the Engram model developed by Deepseek will have a similar impact on AI development as RL and the Transformer.&lt;/p&gt; &lt;p&gt;To expand on why.&lt;/p&gt; &lt;p&gt;1) Grounded fact checking fixing most hallucinations.&lt;/p&gt; &lt;p&gt;2) Vast model knowledge being available for very small models... think 3 billion parameter models that do better on knowledge task than 1 trillion parameter models because they have 1 trillion parameter Engram tables to pull grounded facts from. &lt;/p&gt; &lt;p&gt;3) the biggest reason is the impact it has on RL scaling for small models. We know reasoning benefits from RL more than model size and RL is much cheaper on smaller models... a 3 billion parameter doing the same RL training as a 3 trillion parameter model will cost literally 1000X less compute. &lt;/p&gt; &lt;p&gt;This allows for previously unthinkable RL scaling for small models without risking losing its factual knowledge because the factual knowledge is stored in the Engram table. &lt;/p&gt; &lt;p&gt;We have seen small models match larger models in limited use cases when RL is applied... but this was not scalable before because the small models lose their factual knowledge to make room for reasoning capability because of limited parameter space... Engram fixes that. &lt;/p&gt; &lt;p&gt;Over time this leads to very capable small models that border on AGI capabilities.&lt;/p&gt; &lt;p&gt;Yet the community seems almost silent on Engram.. can anyone say why the odd silence?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cravic"&gt; /u/cravic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjt08m/sleeping_on_engram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjt08m/sleeping_on_engram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjt08m/sleeping_on_engram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T12:16:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjqgnr</id>
    <title>This Week's Hottest Hugging Face Releases: Top Picks by Category!</title>
    <updated>2026-01-22T09:51:15+00:00</updated>
    <author>
      <name>/u/techlatest_net</name>
      <uri>https://old.reddit.com/user/techlatest_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging Face trending is on fire this week with fresh drops in text generation, image, audio, and more.&lt;/p&gt; &lt;p&gt;Check 'em out and drop your thoughts‚Äîwhich one's getting deployed first?&lt;/p&gt; &lt;h1&gt;Text Generation&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.7-Flash"&gt;&lt;strong&gt;zai-org/GLM-4.7-Flash&lt;/strong&gt;&lt;/a&gt;: 31B param model for fast, efficient text gen‚Äîupdated 2 days ago with 124k downloads and 932 likes. Ideal for real-time apps and agents.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"&gt;&lt;strong&gt;unsloth/GLM-4.7-Flash-GGUF&lt;/strong&gt;&lt;/a&gt;: Quantized 30B version for easy local inference‚Äîhot with 112k downloads in hours. Great for low-resource setups.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Image / Multimodal&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/zai-org/GLM-Image"&gt;&lt;strong&gt;zai-org/GLM-Image&lt;/strong&gt;&lt;/a&gt;: Image-text-to-image powerhouse‚Äî10.8k downloads, 938 likes. Excels in creative edits and generation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/google/translategemma-4b-it"&gt;&lt;strong&gt;google/translategemma-4b-it&lt;/strong&gt;&lt;/a&gt;: 5B vision-language model for multilingual image-text tasks‚Äî45.4k downloads, supports translation + vision.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Audio / Speech&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/kyutai/pocket-tts"&gt;&lt;strong&gt;kyutai/pocket-tts&lt;/strong&gt;&lt;/a&gt;: Compact TTS for natural voices‚Äî38.8k downloads, 397 likes. Pocket-sized for mobile/edge deployment.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;&lt;strong&gt;microsoft/VibeVoice-ASR&lt;/strong&gt;&lt;/a&gt;: 9B ASR for multilingual speech recognition‚Äîultra-low latency, 816 downloads already spiking.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Other Hot Categories (Video/Agentic)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2"&gt;&lt;strong&gt;Lightricks/LTX-2&lt;/strong&gt;&lt;/a&gt; (Image-to-Video): 1.96M downloads, 1.25k likes‚Äîpro-level video from images.&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/stepfun-ai/Step3-VL-10B"&gt;&lt;strong&gt;stepfun-ai/Step3-VL-10B&lt;/strong&gt;&lt;/a&gt; (Image-Text-to-Text): 10B VL model for advanced reasoning‚Äî28.6k downloads in hours.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These are dominating trends with massive community traction.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techlatest_net"&gt; /u/techlatest_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjqgnr/this_weeks_hottest_hugging_face_releases_top/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjqgnr/this_weeks_hottest_hugging_face_releases_top/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjqgnr/this_weeks_hottest_hugging_face_releases_top/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T09:51:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkgxzk</id>
    <title>For coding, is it worth spinning to bigger models using heavy RAM, or staying small for speed? 48GB VRAM/120GB RAM</title>
    <updated>2026-01-23T04:21:21+00:00</updated>
    <author>
      <name>/u/CharlesStross</name>
      <uri>https://old.reddit.com/user/CharlesStross</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this is sort of a &amp;quot;how long is a length of string&amp;quot; question because ultimately it comes down to speed vs. quality, but wondered if anyone felt like there was a sufficient-enough-win using something like qwen 3 235b a22b that will just barely fit a quant in VRAM+RAM vs devstral that's going to fit entirely in VRAM. I'm kinda leading towards &amp;quot;code async and use the quality,&amp;quot; but maybe there's a better solution. I'm coming from Claude Code (can't keep spending $200/mo lol) so know there's gonna be a downgrade, but care a lot about code quality I'm working on primarily backend python as well as a smattering of very boring frontend, and occasionally systems work (ansible, terraform, etc.).&lt;/p&gt; &lt;p&gt;Any obvious thoughts or is it just a reality of &amp;quot;well, it's a trade off&amp;quot;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CharlesStross"&gt; /u/CharlesStross &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkgxzk/for_coding_is_it_worth_spinning_to_bigger_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkgxzk/for_coding_is_it_worth_spinning_to_bigger_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkgxzk/for_coding_is_it_worth_spinning_to_bigger_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T04:21:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qk93ol</id>
    <title>Built a mobile app (KernelAI) that runs 43+ models 100% on-device, 100 offline &amp; very well optimized AND it includes Gemma 3, llama 3, and other sick models like Phi and uncensored models like Dolphin. For fun I have included GPT-2 if you were ever wondering what AI looked like couple of years ago</title>
    <updated>2026-01-22T22:37:06+00:00</updated>
    <author>
      <name>/u/Better_Comment_7749</name>
      <uri>https://old.reddit.com/user/Better_Comment_7749</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To begin with, I hope you are having a wonderful day.&lt;/p&gt; &lt;p&gt;I got nerd snipped into build this app, I'm well aware that there is at least 2 other local ai apps in mobile. The goal of the current app is to offer a much higher model selection with a better UI experience (hopefully), and include as many IOS versions/phone models as possible. The app also include vision models (Qwen) that can read images, and TTS. I have put a LOT of efforts in trying to optimize the RAM consumption as much as possible, and the battery as well. So far, the recommended models (Llama 3.2, Gemma 3, IBM granite 4.0 micro etc..) are only consuming around 400 to 600 MB RAM.&lt;/p&gt; &lt;p&gt;If there is anything missing, or if you notice a bug, please do not hesitate to reach out. My current objective is to release the android version in the next days (It's a bit more challenging given that android have a ton of phone models).&lt;/p&gt; &lt;p&gt;kernelai in the appstore, link : &lt;a href="https://apps.apple.com/ca/app/kernelai/id6757350731"&gt;https://apps.apple.com/ca/app/kernelai/id6757350731&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd appreciate a lot a positive review in the app store!&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;p&gt;edit : 100% free &amp;amp; no friction&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Better_Comment_7749"&gt; /u/Better_Comment_7749 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk93ol/built_a_mobile_app_kernelai_that_runs_43_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk93ol/built_a_mobile_app_kernelai_that_runs_43_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qk93ol/built_a_mobile_app_kernelai_that_runs_43_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T22:37:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjrsur</id>
    <title>GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp</title>
    <updated>2026-01-22T11:10:42+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjrsur/glm_47_flash_fa_fix_for_cuda_has_been_merged_into/"&gt; &lt;img alt="GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp" src="https://external-preview.redd.it/TcGseIeP3Z00NB4otbKR8-_fs_ssjxg6HC4Fv_lVbUU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d37072f08acdffcd4c3617847f00d2a9b9bafcf4" title="GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18953"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjrsur/glm_47_flash_fa_fix_for_cuda_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjrsur/glm_47_flash_fa_fix_for_cuda_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T11:10:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qk18y6</id>
    <title>Unsloth announces support for finetuning embedding models</title>
    <updated>2026-01-22T17:44:56+00:00</updated>
    <author>
      <name>/u/-Cubie-</name>
      <uri>https://old.reddit.com/user/-Cubie-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk18y6/unsloth_announces_support_for_finetuning/"&gt; &lt;img alt="Unsloth announces support for finetuning embedding models" src="https://external-preview.redd.it/ksRJC2bKGwjrMfOqsioi-B4oIm5QWQUM7Vf03KwieGM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f6fc5d8f727ab6f86a8ca5f94a5091bbe81d025" title="Unsloth announces support for finetuning embedding models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Daniel Han from Unsloth just announced finetuning embedding models with Unsloth and Sentence Transformers together:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Unsloth now has 1.8x-3.3x faster 20% less VRAM embedding finetuning! EmbeddingGemma, Qwen3 Embedding &amp;amp; all others work!&lt;br /&gt; We made 6 notebooks showing how you can customize for RAG, semantic similarity tasks &amp;amp; more. Transformers v5 works as well. Thanks huggingface for the collab!&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I've heard really good things about Unsloth for finetuning LLMs, so I have high hopes for this as well. Very promising for retrieval models for RAG etc, I think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cubie-"&gt; /u/-Cubie- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://unsloth.ai/docs/new/embedding-finetuning"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk18y6/unsloth_announces_support_for_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qk18y6/unsloth_announces_support_for_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T17:44:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkbv12</id>
    <title>Finnaly I am in the club, rate my set up üòú</title>
    <updated>2026-01-23T00:31:35+00:00</updated>
    <author>
      <name>/u/black7stone</name>
      <uri>https://old.reddit.com/user/black7stone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkbv12/finnaly_i_am_in_the_club_rate_my_set_up/"&gt; &lt;img alt="Finnaly I am in the club, rate my set up üòú" src="https://preview.redd.it/dda95brpuzeg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0691e250029de84aec9a48bcb7ab1cd35596c718" title="Finnaly I am in the club, rate my set up üòú" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys finnaly I managed to get my own server PC, here a screenshot of the specifics.&lt;/p&gt; &lt;p&gt;At the moment I have an 3060 of 12 gb VRAM but I have ordered the 5060 ti 16gb Vram (ordered on the 3rd of January and will arrive on the 20th of Feb XD) then later I will keep both in my set up.&lt;/p&gt; &lt;p&gt;So what do you think about? I have 36 cores and 72 threads, 128 gb ram DDR 4 all on a nvme V4 of 1tb and running Ubuntu 24.&lt;/p&gt; &lt;p&gt;Any suggestions? Now I would like to profit from this set up some how, any tip? So I can make more more money and upgrade slowly.&lt;/p&gt; &lt;p&gt;I am installing llama 70b any other LLM worth it?&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/black7stone"&gt; /u/black7stone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dda95brpuzeg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkbv12/finnaly_i_am_in_the_club_rate_my_set_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkbv12/finnaly_i_am_in_the_club_rate_my_set_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T00:31:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkghpk</id>
    <title>Quiet Threadripper AI Workstation - 768GB DDR5 and 160GB VRAM (RTX 5090 + 4x R9700)</title>
    <updated>2026-01-23T04:00:22+00:00</updated>
    <author>
      <name>/u/sloptimizer</name>
      <uri>https://old.reddit.com/user/sloptimizer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkghpk/quiet_threadripper_ai_workstation_768gb_ddr5_and/"&gt; &lt;img alt="Quiet Threadripper AI Workstation - 768GB DDR5 and 160GB VRAM (RTX 5090 + 4x R9700)" src="https://b.thumbs.redditmedia.com/OvcCfJivz4D4HOjUCQinn-sUra3cRaaS32dKiyRmuRM.jpg" title="Quiet Threadripper AI Workstation - 768GB DDR5 and 160GB VRAM (RTX 5090 + 4x R9700)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seeing all the quad R9700 builds inspired me to post mine!&lt;/p&gt; &lt;p&gt;I managed to squeeze in RTX 5090 and four R9700 into a workstation build by fitting some GPUs vertically in the front section. Two power supplies: 1600W for the main system and most of the components, and a smaller 850W power supply for 3 of the Radeons (the power cable is threaded through the system popping out through a small gap left by RTX 5090).&lt;/p&gt; &lt;p&gt;DeepSeek-V3.1-Terminus with context = 37279 tokens: PP = 151.76 tps, TG = 10.85 tps&lt;/p&gt; &lt;p&gt;Some things I discovered running local LLMs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For water-cooled CPU systems, there is not enough air circulation to cool the RAM! &lt;ul&gt; &lt;li&gt;Adding RAM fans got me a 30% performance boost with DeepSeek&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Turning off remote management on WRX90E-SAGE makes it boot much faster&lt;/li&gt; &lt;li&gt;You can combine Nvidia and AMD cards in llama.cpp by compiling with &lt;code&gt;-DGGML_BACKEND_DL=ON&lt;/code&gt;&lt;/li&gt; &lt;li&gt;No significant performance penalty running RTX 5090 at 400W, but much cooler and quieter &lt;ul&gt; &lt;li&gt;To fix, run: &lt;code&gt;sudo nvidia-smi -pl 400&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;R9700 has crazy auto-overclocking by default, draining power and making a lot of noise for little gain &lt;ul&gt; &lt;li&gt;To fix, run: &lt;code&gt;sudo amd-smi set --perf-level=HIGH&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Despite aggressive auto-overclocking, R9700's default mode is sub-optimal for MoE offloading (perf-level=HIGH fixes that as well)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Component List:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Motherboard - Pro WS WRX90E-SAGE SE&lt;/li&gt; &lt;li&gt;CPU - AMD Ryzen Threadripper PRO 7975WX&lt;/li&gt; &lt;li&gt;RAM - 8x KINGSTON 96GB DDR5 5600MHz CL46&lt;/li&gt; &lt;li&gt;GPU1 - ASUS TUF GeForce RTX 5090&lt;/li&gt; &lt;li&gt;GPU2 - 4x ASRock Creator Radeon AI Pro R9700&lt;/li&gt; &lt;li&gt;NVMe - 4x Samsung 9100 PRO 2TB&lt;/li&gt; &lt;li&gt;HDD - 2x Seagate Exos 16TB Enterprise&lt;/li&gt; &lt;li&gt;Power1 - Dark Power Pro 13 1600W 80+ Titanium&lt;/li&gt; &lt;li&gt;Power2 - Seasonic FOCUS V3 GX-850, 850W 80+ Gold&lt;/li&gt; &lt;li&gt;Case - Fractal Design Define 7 XL&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sloptimizer"&gt; /u/sloptimizer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qkghpk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkghpk/quiet_threadripper_ai_workstation_768gb_ddr5_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkghpk/quiet_threadripper_ai_workstation_768gb_ddr5_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T04:00:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkef77</id>
    <title>I pre-trained and instruction tuned a 394M parameter LM from scratch :)</title>
    <updated>2026-01-23T02:25:42+00:00</updated>
    <author>
      <name>/u/SadEqual5367</name>
      <uri>https://old.reddit.com/user/SadEqual5367</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is the link to my repo: &lt;a href="https://github.com/pradyGn/zoof"&gt;https://github.com/pradyGn/zoof&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am reading about reasoning in SLMs and planning to add those capabilities into zoof. Any suggestions on interesting papers / repositories that I can read?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SadEqual5367"&gt; /u/SadEqual5367 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkef77/i_pretrained_and_instruction_tuned_a_394m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkef77/i_pretrained_and_instruction_tuned_a_394m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkef77/i_pretrained_and_instruction_tuned_a_394m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T02:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjul2g</id>
    <title>Qwen3 TTS just dropped üó£Ô∏èüîà</title>
    <updated>2026-01-22T13:31:10+00:00</updated>
    <author>
      <name>/u/Reasonable-Fun-7078</name>
      <uri>https://old.reddit.com/user/Reasonable-Fun-7078</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/QwenLM/Qwen3-TTS"&gt;https://github.com/QwenLM/Qwen3-TTS&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/collections/Qwen/qwen3-tts"&gt;https://huggingface.co/collections/Qwen/qwen3-tts&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable-Fun-7078"&gt; /u/Reasonable-Fun-7078 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul2g/qwen3_tts_just_dropped/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul2g/qwen3_tts_just_dropped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul2g/qwen3_tts_just_dropped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T13:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkckmc</id>
    <title>Mistral Small Creative just beat Claude Opus 4.5, Sonnet 4.5, and GPT-OSS-120B on practical communication tasks</title>
    <updated>2026-01-23T01:02:44+00:00</updated>
    <author>
      <name>/u/Silver_Raspberry_811</name>
      <uri>https://old.reddit.com/user/Silver_Raspberry_811</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkckmc/mistral_small_creative_just_beat_claude_opus_45/"&gt; &lt;img alt="Mistral Small Creative just beat Claude Opus 4.5, Sonnet 4.5, and GPT-OSS-120B on practical communication tasks" src="https://a.thumbs.redditmedia.com/MkBdgYy1VwuwU-cAJXDpHPy8VKhmYf9muQ0eqpiQkV0.jpg" title="Mistral Small Creative just beat Claude Opus 4.5, Sonnet 4.5, and GPT-OSS-120B on practical communication tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run daily peer evaluations called The Multivac ‚Äî frontier models judging each other blind. Today's test: write 3 versions of an API outage message (internal Slack, enterprise email, public status page).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mistral Small Creative‚Äîa model that gets a fraction of the attention of frontier giants‚Äîtook first place on a practical business task.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pre2wmf600fg1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d61bcbd4f368918233a544dfd5311bf596431c6d"&gt;https://preview.redd.it/pre2wmf600fg1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d61bcbd4f368918233a544dfd5311bf596431c6d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What made it win:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Its internal Slack message felt like an actual engineering lead wrote it. Specific, blameless, with concrete action items:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;That's the kind of language that actually helps teams improve.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The meta observation:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For practical communication tasks, raw parameter count isn't everything. Mistral seems to have strong instincts for tone and audience calibration‚Äîskills that don't necessarily scale linearly with model size.&lt;/p&gt; &lt;p&gt;Full methodology + all responses: &lt;a href="http://themultivac.com"&gt;themultivac.com&lt;/a&gt;&lt;br /&gt; LINK: &lt;a href="https://open.substack.com/pub/themultivac/p/a-small-model-just-beat-claude-opus?r=72olj0&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=true"&gt;https://open.substack.com/pub/themultivac/p/a-small-model-just-beat-claude-opus?r=72olj0&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=true&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Phase 3 coming soon:&lt;/strong&gt; We're working on the next evolution of evals. Datasets and outputs will be available for everyone to test and play with directly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silver_Raspberry_811"&gt; /u/Silver_Raspberry_811 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkckmc/mistral_small_creative_just_beat_claude_opus_45/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkckmc/mistral_small_creative_just_beat_claude_opus_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkckmc/mistral_small_creative_just_beat_claude_opus_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T01:02:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qk9vmv</id>
    <title>1.8-3.3x faster Embedding finetuning now in Unsloth (~3GB VRAM)</title>
    <updated>2026-01-22T23:09:04+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk9vmv/1833x_faster_embedding_finetuning_now_in_unsloth/"&gt; &lt;img alt="1.8-3.3x faster Embedding finetuning now in Unsloth (~3GB VRAM)" src="https://preview.redd.it/wwwlbq9ffzeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08d147840833d98030a378036e90b68b7a8dd2ff" title="1.8-3.3x faster Embedding finetuning now in Unsloth (~3GB VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLLaMA! We added embedding fine-tuning support in Unsloth! &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; trains embedding models &lt;strong&gt;1.8-3.3x faster with 20% less VRAM&lt;/strong&gt;, 2x longer context &amp;amp; no accuracy loss vs. FA2 setups. Most need only 3GB of VRAM for 4bit QLoRA. 6GB for 16bit LoRA.&lt;/p&gt; &lt;p&gt;Full finetuning, LoRA (16bit) and QLoRA (4bit) are all faster by default!&lt;/p&gt; &lt;p&gt;Fine-tuning embedding models can improve retrieval &amp;amp; RAG by aligning vectors to your domain-specific notion of similarity, improving search, clustering, and recommendations on your data.&lt;/p&gt; &lt;p&gt;Blog + Guide: &lt;a href="https://unsloth.ai/docs/new/embedding-finetuning"&gt;https://unsloth.ai/docs/new/embedding-finetuning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After finetuning, you can deploy your fine-tuned model anywhere: transformers, LangChain, Ollama, vLLM, llama.cpp&lt;/p&gt; &lt;p&gt;We'd like to thank Hugging Face and Unsloth contributor: electroglyph for making this possible!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Try the &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/EmbeddingGemma_(300M"&gt;EmbeddingGemma notebook&lt;/a&gt;.ipynb) in a free Colab T4 instance&lt;/li&gt; &lt;li&gt;We support ModernBERT, Qwen Embedding, Embedding Gemma, MiniLM-L6-v2, mpnet, BGE and all other models are supported automatically!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And code for doing EmbeddingGemma:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from unsloth import FastSentenceTransformer model = FastSentenceTransformer.from_pretrained( model_name = &amp;quot;unsloth/embeddinggemma-300m&amp;quot;, max_seq_length = 1024, # Choose any for long context! full_finetuning = False, # [NEW!] We have full finetuning now! ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Update Unsloth via &lt;code&gt;pip install --upgrade unsloth unsloth_zoo&lt;/code&gt; to get the latest updates. Thanks everyone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wwwlbq9ffzeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk9vmv/1833x_faster_embedding_finetuning_now_in_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qk9vmv/1833x_faster_embedding_finetuning_now_in_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T23:09:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qk68n8</id>
    <title>vLLM raising $150M confirms it: We have moved from the "Throughput Era" to the "Latency(Cold Starts)."</title>
    <updated>2026-01-22T20:45:42+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The news today that the team behind vLLM (Inferact) raised a $150M Seed Round at an $800M valuation is a massive signal for everyone in this space.&lt;/p&gt; &lt;p&gt;For the last two years, all the capital flowed into &lt;strong&gt;Training&lt;/strong&gt; (Foundation Models, massive clusters). This raise signals that the bottleneck has officially shifted to &lt;strong&gt;Serving&lt;/strong&gt; (Efficiency, Latency, Throughput).&lt;/p&gt; &lt;p&gt;It validates a few things we've been seeing in the open-source community:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Software &amp;gt; Hardware:&lt;/strong&gt; buying more H100s isn't enough anymore. You need the software stack (PagedAttention, specialized kernels) to actually utilize them. The &amp;quot;Software Tax&amp;quot; on inference is real.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &amp;quot;Standardization&amp;quot; Race:&lt;/strong&gt; vLLM is clearly aiming to be the &amp;quot;Linux of Inference&amp;quot;‚Äîthe default engine that runs on NVIDIA, AMD, and Intel. I wonder though, With this kind of war chest, do we think they go for &lt;strong&gt;Horizontal Compatibility&lt;/strong&gt; (making AMD/Intel usable) or &lt;strong&gt;Vertical Optimization&lt;/strong&gt; (squeezing more latency out of CUDA)?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Personally, I think &amp;quot;Throughput&amp;quot; (Batched tokens) is largely solved. The next massive hurdle is &lt;strong&gt;Latency&lt;/strong&gt; (Cold starts and Time-to-First-Token).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk68n8/vllm_raising_150m_confirms_it_we_have_moved_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk68n8/vllm_raising_150m_confirms_it_we_have_moved_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qk68n8/vllm_raising_150m_confirms_it_we_have_moved_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T20:45:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjtyw8</id>
    <title>Qwen dev on Twitter!!</title>
    <updated>2026-01-22T13:03:26+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/"&gt; &lt;img alt="Qwen dev on Twitter!!" src="https://preview.redd.it/avu4mhyvfweg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60312577cba6dc65c74da0313ab4d31252bd6be2" title="Qwen dev on Twitter!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/avu4mhyvfweg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T13:03:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjul5t</id>
    <title>Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages</title>
    <updated>2026-01-22T13:31:16+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/"&gt; &lt;img alt="Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp;amp; 1.8B), Support for 10 languages" src="https://preview.redd.it/wo9tqflvkweg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75bf194547e68a1bb648f530175a2ec826899fd0" title="Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp;amp; 1.8B), Support for 10 languages" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Github: &lt;a href="https://github.com/QwenLM/Qwen3-TTS"&gt;https://github.com/QwenLM/Qwen3-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/collections/Qwen/qwen3-tts"&gt;https://huggingface.co/collections/Qwen/qwen3-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://qwen.ai/blog?id=qwen3tts-0115"&gt;https://qwen.ai/blog?id=qwen3tts-0115&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://github.com/QwenLM/Qwen3-TTS/blob/main/assets/Qwen3_TTS.pdf"&gt;https://github.com/QwenLM/Qwen3-TTS/blob/main/assets/Qwen3_TTS.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face Demo: &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-TTS"&gt;https://huggingface.co/spaces/Qwen/Qwen3-TTS&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wo9tqflvkweg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T13:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qk8zj1</id>
    <title>Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing?</title>
    <updated>2026-01-22T22:31:28+00:00</updated>
    <author>
      <name>/u/Empty_Enthusiasm_167</name>
      <uri>https://old.reddit.com/user/Empty_Enthusiasm_167</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lately I go on Reddit and I keep seeing the same idea repeated over and over again. Another chat app, another assistant, another ‚ÄúAI tool‚Äù that, in reality, already exists ‚Äî or worse, already exists in a better and more polished form.&lt;/p&gt; &lt;p&gt;Many of these are applications that could be solved perfectly with an extension, a plugin, or a simple feature inside an app we already use. I‚Äôm not saying AI is bad ‚Äî quite the opposite, it‚Äôs incredible. But there are people pouring all their money into Anthropic subscriptions or increasing their electricity bill just to build a less polished version of things like OpenWebUI, Open Code, Cline, etc&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Empty_Enthusiasm_167"&gt; /u/Empty_Enthusiasm_167 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T22:31:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
