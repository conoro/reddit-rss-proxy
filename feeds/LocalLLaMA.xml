<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-28T17:07:04+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rgvma8</id>
    <title>Which size of Qwen3.5 are you planning to run locally?</title>
    <updated>2026-02-28T06:46:39+00:00</updated>
    <author>
      <name>/u/CutOk3283</name>
      <uri>https://old.reddit.com/user/CutOk3283</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just a quick poll/discussion for the local hardware crowd. Are you guys jumping on the 27B for single-card setups, trying to squeeze the 35B into Mac Studios, or going crazy with the 122B on multi-GPU rigs? Trying to figure out which size will get the most community support.locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CutOk3283"&gt; /u/CutOk3283 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgvma8/which_size_of_qwen35_are_you_planning_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgvma8/which_size_of_qwen35_are_you_planning_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgvma8/which_size_of_qwen35_are_you_planning_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T06:46:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh3thm</id>
    <title>RIP Gemma - Leave your memories here.</title>
    <updated>2026-02-28T14:19:47+00:00</updated>
    <author>
      <name>/u/DrNavigat</name>
      <uri>https://old.reddit.com/user/DrNavigat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I remember it like it wasn't that long ago, the excitement of being up late at night reading the rumors about the new Gemma, until I could finally test it.&lt;/p&gt; &lt;p&gt;I remember the first time I could run a small model that was coherent and knew my language, and not just English.&lt;/p&gt; &lt;p&gt;I remember asking it to pretend to be a spaceship robot while I was the captain, I remember when it hallucinated an asteroid and we exploded.&lt;/p&gt; &lt;p&gt;Rest in peace, Gemma üïäÔ∏è&lt;/p&gt; &lt;p&gt;In memory of Gemma.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrNavigat"&gt; /u/DrNavigat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh3thm/rip_gemma_leave_your_memories_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh3thm/rip_gemma_leave_your_memories_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh3thm/rip_gemma_leave_your_memories_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T14:19:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgzga6</id>
    <title>An open-source local speech AI benchmarking tool - compare STT, TTS, emotion detection &amp; diarization models side by side</title>
    <updated>2026-02-28T10:38:35+00:00</updated>
    <author>
      <name>/u/hamuf</name>
      <uri>https://old.reddit.com/user/hamuf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgzga6/an_opensource_local_speech_ai_benchmarking_tool/"&gt; &lt;img alt="An open-source local speech AI benchmarking tool - compare STT, TTS, emotion detection &amp;amp; diarization models side by side" src="https://preview.redd.it/etwn6807r7mg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=8343687a2900e29a067c067b414fc3e140ca0a14" title="An open-source local speech AI benchmarking tool - compare STT, TTS, emotion detection &amp;amp; diarization models side by side" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Speech models have been a constant wrestle. Whisper, Bark, Vosk, Kokoro, all promising the world but often choking on real hardware. Dozens out there, no simple way to pit them against each other without the cloud leeches draining data. Speechos emerged from the quiet frustration of it all.&lt;/p&gt; &lt;p&gt;It's local-first, everything locked on the machine. Record from mic or drop in audio files, then swap through 25+ engines via dropdown and see the results clash side by side. STT: faster-whisper (tiny to large-v3), Vosk, Wav2Vec2, plus Docker options like NeMo or Speaches.&lt;/p&gt; &lt;p&gt;TTS: Piper, Kokoro, Bark, eSpeak, Chatterbox built-in; Docker adds XTTS, ChatTTS, Orpheus, Fish-Speech, Qwen3-TTS, Parler. They turn text into voices, some with emotional undertones, others flat as pavement.&lt;/p&gt; &lt;p&gt;Emotion detection via HuBERT SER (seven emotions) and emotion2vec+ with confidence scores. Speaker diarization: Resemblyzer for basics, PyAnnote through Docker for the deep cuts.&lt;/p&gt; &lt;p&gt;Audio analysis layers on pitch, loudness, speaking rate, tempo, spectral centroid, MFCCs like peeling back the skin of sound.&lt;/p&gt; &lt;p&gt;It detects hardware and adapts quietly: CPU-2GB sticks to Whisper Tiny + Piper; GPU-24GB unlocks the full arsenal, Docker included.&lt;/p&gt; &lt;p&gt;Python/FastAPI backend, Next.js frontend, uv and pnpm managing the deps. One ./dev.sh fires it up. 12 built-in engines, 13 optional via Docker. MIT licensed, because why hoard the tools?&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/miikkij/Speechos"&gt;https://github.com/miikkij/Speechos&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If it fits the tinkering itch, give it a spin.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hamuf"&gt; /u/hamuf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rgzga6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgzga6/an_opensource_local_speech_ai_benchmarking_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgzga6/an_opensource_local_speech_ai_benchmarking_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T10:38:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg8dex</id>
    <title>PewDiePie fine-tuned Qwen2.5-Coder-32B to beat ChatGPT 4o on coding benchmarks.</title>
    <updated>2026-02-27T14:37:18+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg8dex/pewdiepie_finetuned_qwen25coder32b_to_beat/"&gt; &lt;img alt="PewDiePie fine-tuned Qwen2.5-Coder-32B to beat ChatGPT 4o on coding benchmarks." src="https://external-preview.redd.it/mCmYhKXGNj-QOd-sXT1nvg6KbIIK9oXVkPL1aBEF4FY.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e3e22dae46cc10fa4599a3b4892076af3a2cc56" title="PewDiePie fine-tuned Qwen2.5-Coder-32B to beat ChatGPT 4o on coding benchmarks." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=aV4j5pXLP-I&amp;amp;feature=youtu.be"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg8dex/pewdiepie_finetuned_qwen25coder32b_to_beat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg8dex/pewdiepie_finetuned_qwen25coder32b_to_beat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T14:37:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgxr0v</id>
    <title>Qwen 3.5 is multimodal. Here is how to enable image understanding in opencode with llama cpp</title>
    <updated>2026-02-28T08:52:41+00:00</updated>
    <author>
      <name>/u/Old-Sherbert-4495</name>
      <uri>https://old.reddit.com/user/Old-Sherbert-4495</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trick is to add this to opencode.json file&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;modalities&amp;quot;: { &amp;quot;input&amp;quot;: [ &amp;quot;text&amp;quot;, &amp;quot;image&amp;quot; ], &amp;quot;output&amp;quot;: [ &amp;quot;text&amp;quot; ] } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;full:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;provider&amp;quot;: { &amp;quot;llama.cpp&amp;quot;: { &amp;quot;npm&amp;quot;: &amp;quot;@ai-sdk/openai-compatible&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;llama-server&amp;quot;, &amp;quot;options&amp;quot;: { &amp;quot;baseURL&amp;quot;: &amp;quot;http://127.0.0.1:8001/v1&amp;quot; }, &amp;quot;models&amp;quot;: { &amp;quot;Qwen3.5-35B-local&amp;quot;: { &amp;quot;modalities&amp;quot;: { &amp;quot;input&amp;quot;: [ &amp;quot;text&amp;quot;, &amp;quot;image&amp;quot; ], &amp;quot;output&amp;quot;: [ &amp;quot;text&amp;quot; ] }, &amp;quot;name&amp;quot;: &amp;quot;Qwen3.5-35B-local)&amp;quot;, &amp;quot;limit&amp;quot;: { &amp;quot;context&amp;quot;: 122880, &amp;quot;output&amp;quot;: 32768 } } } } } &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-Sherbert-4495"&gt; /u/Old-Sherbert-4495 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgxr0v/qwen_35_is_multimodal_here_is_how_to_enable_image/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgxr0v/qwen_35_is_multimodal_here_is_how_to_enable_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgxr0v/qwen_35_is_multimodal_here_is_how_to_enable_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T08:52:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgzfat</id>
    <title>How is Qwen 3.5 (MoE 35b) in instruct mode (with no reasoning/thinking) ?</title>
    <updated>2026-02-28T10:37:01+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're out of bandwidth at the office, have you guys managed to test it ?&lt;/p&gt; &lt;p&gt;I find it surprising that qwen moved away from hybrid model (after the 2507 releases) to again release an hybrid reasoning model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgzfat/how_is_qwen_35_moe_35b_in_instruct_mode_with_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgzfat/how_is_qwen_35_moe_35b_in_instruct_mode_with_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgzfat/how_is_qwen_35_moe_35b_in_instruct_mode_with_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T10:37:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgel19</id>
    <title>New Qwen3.5-35B-A3B Unsloth Dynamic GGUFs + Benchmarks</title>
    <updated>2026-02-27T18:23:50+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgel19/new_qwen3535ba3b_unsloth_dynamic_ggufs_benchmarks/"&gt; &lt;img alt="New Qwen3.5-35B-A3B Unsloth Dynamic GGUFs + Benchmarks" src="https://external-preview.redd.it/dk4oYGXETs66okCfY5tWsmyJ8TsYGxQaSswXwZSBdYs.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=1b158effff1f75e76fd7b2a285de69b70b0fede9" title="New Qwen3.5-35B-A3B Unsloth Dynamic GGUFs + Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We just updated Qwen3.5-35B Unsloth Dynamic quants &lt;strong&gt;being SOTA&lt;/strong&gt; on nearly all bits. We did over 150 KL Divergence benchmarks, totally &lt;strong&gt;9TB of GGUFs&lt;/strong&gt;. We uploaded all research artifacts. We also fixed a &lt;strong&gt;tool calling&lt;/strong&gt; chat template &lt;strong&gt;bug&lt;/strong&gt; (affects all quant uploaders)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We tested Bartowski, Ubergram, AesSedai, Noctrex and our new Dynamic GGUFs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;99.9% KL Divergence shows SOTA&lt;/strong&gt; on Pareto Frontier for UD-Q4_K_XL, IQ3_XXS &amp;amp; more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Retiring MXFP4&lt;/strong&gt; from all GGUF quants: Q2_K_XL, Q3_K_XL and Q4_K_XL, except for a select few layers.&lt;/li&gt; &lt;li&gt;Qwen3.5-35B-A3B GGUFs are updated to use new fixes (112B, 27B still converting, re-download once they are updated)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5hmdthgyp2mg1.png?width=2320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3dbd0480bbc38512a8bbbba0e4e01444feec99fb"&gt;https://preview.redd.it/5hmdthgyp2mg1.png?width=2320&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3dbd0480bbc38512a8bbbba0e4e01444feec99fb&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Imatrix definitely helps reduce KLD &amp;amp; PPL.&lt;/li&gt; &lt;li&gt;I quants (iq3_xxs, iq2_s etc) makes inference 5-10% slower.&lt;/li&gt; &lt;li&gt;Quantizing ssm_out (Mamba layers) is not a good idea, and ffn_down_exps.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Some tensors are very sensitive to quantization&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We made over 9TB of research artifacts available for the community to investigate further on our &lt;a href="https://huggingface.co/unsloth/Qwen3.5-35B-A3B-Experiments-GGUF"&gt;Experiments page&lt;/a&gt;. It includes KLD metrics and all 121 configs we tested.&lt;/li&gt; &lt;li&gt;We varied bit widths across each tensor type, and generated a best and worst Pareto Frontier plot below vs 99.9% KLD.&lt;/li&gt; &lt;li&gt;For the best items to quantize, ffn_up_exps and ffn_gate_exps are generally ok to quantize to 3bit. ffn_down_exps is slightly more sensitive.&lt;/li&gt; &lt;li&gt;For the worst items, ssm_out dramatically increases KLD and the disk space savings is minuscule. For example, ssm_out at q2_k does dramatically worse. &lt;strong&gt;Quantizing any attn_* is especially sensitive&lt;/strong&gt; for hybrid architectures, and so leaving them in higher precision works well.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pakdmbv1n2mg1.png?width=1183&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=be8940bf7c49157d1e34bb82053e70b44f0e1744"&gt;https://preview.redd.it/pakdmbv1n2mg1.png?width=1183&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=be8940bf7c49157d1e34bb82053e70b44f0e1744&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tensor type vs bits on 99.9% KL Divergence&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We plot all quant levels vs 99.9% KLD, and sort from worst KLD to best. Quantizing ffn_* layers too heavily down is not a good idea.&lt;/li&gt; &lt;li&gt;However, &lt;strong&gt;some bit widths are good, especially 3bit&lt;/strong&gt;. - for example leaving ffn_* (down, up, gate) at around iq3_xxs seems to be best compromise on disk space and 99.9% KLD change. 2 bits cause more degradation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;MXFP4 is much worse on many tensors&lt;/strong&gt; - attn_gate, attn_q, ssm_beta, ssm_alpha using MXFP4 is not a good idea, and rather Q4_K is better - also MXFP4 uses 4.25 bits per weight, whilst Q4_K uses 4.5 bits per weight. It's better to use Q4_K than MXFP4 when choosing between them.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xgugdgzmv2mg1.png?width=989&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eddc2c32d343410a27f405289fd976e858d6f6a8"&gt;https://preview.redd.it/xgugdgzmv2mg1.png?width=989&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eddc2c32d343410a27f405289fd976e858d6f6a8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Imatrix works remarkably well&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Imatrix definitely helps weight the quantization process in the right way. For example previously ssm_out at 2bits was really bad, however imatrix reduces the 99.9% KLD by a lot.&lt;/li&gt; &lt;li&gt;Imatrix generally helps on lower bits, and works on all quants and bit widths.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yidhlf79o2mg1.png?width=1389&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9b5f1f6510d0aa5ebbf4b06ba9908947a21e93e"&gt;https://preview.redd.it/yidhlf79o2mg1.png?width=1389&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9b5f1f6510d0aa5ebbf4b06ba9908947a21e93e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I quants (iq3_xxs, iq2_s etc) makes inference 5-10% slower, they're definitely better in terms of efficiency, but there is a tradeoff.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/bnjmn_marie/status/2027043753484021810"&gt;&lt;strong&gt;Benjamin‚Äôs recent MiniMax‚ÄëM2.5 analysis&lt;/strong&gt;&lt;/a&gt; shows a case how perplexity and KLD can still be very misleading. Unsloth Dynamic IQ2_XXS &lt;strong&gt;performs better&lt;/strong&gt; than AesSedai‚Äôs IQ3_S on real world evals (LiveCodeBench v6, MMLU Pro) despite being 11GB smaller. Yet, AesSedai‚Äôs perplexity and KLD benchmarks suggest the &lt;strong&gt;opposite&lt;/strong&gt;. (PPL: 0.3552 vs 0.2441; KLD: 9.0338 vs 8.2849 - lower is better).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hwif5hfex2mg1.png?width=1078&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6fef62ede6626f47991a3dbc90183b9d621d0bc"&gt;https://preview.redd.it/hwif5hfex2mg1.png?width=1078&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6fef62ede6626f47991a3dbc90183b9d621d0bc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Perplexity and KLD can also be misleading&lt;/strong&gt; but, as precaution we replaced any MXFP4 layer. Real-world evals (LiveCodeBench v6 etc.) are much better benchmarks, but can take many days. This mismatch shows how &lt;strong&gt;lower perplexity or KLD doesn‚Äôt necessarily translate to better real-world performance&lt;/strong&gt;. The graph also shows &lt;strong&gt;UD‚ÄëQ4-K‚ÄëXL&lt;/strong&gt; outperforming other &lt;strong&gt;Q4&lt;/strong&gt; quants, while being ~8GB smaller.&lt;/p&gt; &lt;p&gt;This doesn‚Äôt mean perplexity or KLD is useless, as they provide a &lt;em&gt;rough signal&lt;/em&gt;. So, going forward, we‚Äôll publish &lt;strong&gt;perplexity and KLD for every quant&lt;/strong&gt; so the community has some reference.&lt;/p&gt; &lt;p&gt;Updated GGUFs here: &lt;a href="https://huggingface.co/collections/unsloth/qwen35"&gt;https://huggingface.co/collections/unsloth/qwen35&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For more investigation deets and benchmarks you can read: &lt;a href="https://unsloth.ai/docs/models/qwen3.5"&gt;&lt;strong&gt;https://unsloth.ai/docs/models/qwen3.5&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thank you for reading and once again for the feedback and incredible support. Huge thanks to the Qwen team as well for releasing Qwen3.5. If there‚Äôs any suggestions please let us know and have a great Friday / weekend guys!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarking Details &amp;amp; Appreciation:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We utilized bartowski's wonderful imatrix file to make the comparisons more fair - our Dynamic 2.0 method uses a conversational format, but we found benchmarking to be fairer if we used a more general imatrix&lt;/li&gt; &lt;li&gt;We appreciated some friendly guidance from Ubergram and the community!&lt;/li&gt; &lt;li&gt;For perplexity we used the below. We also use the BF16 as the base KLD file. &lt;code&gt;LLAMA_SET_ROWS=1 ./llama.cpp/llama-perplexity --flash-attn on --fit off --batch-size 16384 --ubatch-size 16384 --device {device} --model {model} --ctx-size 512&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgel19/new_qwen3535ba3b_unsloth_dynamic_ggufs_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgel19/new_qwen3535ba3b_unsloth_dynamic_ggufs_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgel19/new_qwen3535ba3b_unsloth_dynamic_ggufs_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T18:23:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh46g2</id>
    <title>Why some still playing with old models? Nostalgia or obsession or what?</title>
    <updated>2026-02-28T14:35:14+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Still I see some folks mentioning models like Qwen-2.5, Gemma-2, etc., in their threads &amp;amp; comments.&lt;/p&gt; &lt;p&gt;We got Qwen-3.5 recently after Qwen-3 last year. And got Gemma-3 &amp;amp; waiting for Gemma-4.&lt;/p&gt; &lt;p&gt;Well, I'm not talking about just their daily usage. They also create finetunes, benchmarks based on those old models. They spend their precious time &amp;amp; It would be great to have finetunes based on recent version models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh46g2/why_some_still_playing_with_old_models_nostalgia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh46g2/why_some_still_playing_with_old_models_nostalgia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh46g2/why_some_still_playing_with_old_models_nostalgia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T14:35:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh69co</id>
    <title>Multi-Directional Refusal Suppression with Self-Organizing Maps - Pull Request into heretic!</title>
    <updated>2026-02-28T16:01:19+00:00</updated>
    <author>
      <name>/u/kabachuha</name>
      <uri>https://old.reddit.com/user/kabachuha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: The first technique that pushed gpt-oss-20b to 3 refusals from 100 while keeping KL of 0.12, and oss-120b to 7/100 while having KL 0.22!&lt;/p&gt; &lt;p&gt;Previous work assumed refusal behavior to be encoded as a single direction in the model's latent space; e.g., computed as the difference between the centroids of harmful and harmless prompt representations. However, emerging evidence suggests that concepts in LLMs often appear to be encoded as a low-dimensional manifold embedded in the high-dimensional latent space. Just like numbers and days of week are encoded in circles or helices, in recent advanced neural networks like GPT-OSS refusals are becoming ingrained in complex multi-directional clusters and one-directional ablation is not enough to get rid of the refusal reasoning. This &lt;a href="https://huggingface.co/Magic-Decensored/Apriel-1.6-15b-Thinker-Magic_beta-decensored-GGUF"&gt;HF model&lt;/a&gt;, which has applied my implemented PR, has an awesome visualization of refusal clusterization.&lt;/p&gt; &lt;p&gt;Now that we cannot use simple ablation, is it over? It is not. Researchers from the &lt;a href="https://arxiv.org/abs/2511.08379v2"&gt;Universities of Cagliari and Genova&lt;/a&gt; invented a new method. They &lt;em&gt;train a self-organizing neural network&lt;/em&gt; on the hidden states to &lt;em&gt;determine this manifold&lt;/em&gt;. After it, the K most important neurons are selected and turned into refusal directions, compressing this manifold towards the harmless zone, making them equivalent in a fine-grained manner instead of a one-fits-all lobotomy. So yes, we have neural networks fighting against the other neural networks. The final export of abliteration is baked into the model's weights, no modules needed.&lt;/p&gt; &lt;p&gt;I, and the community are already testing this algorithm on models such as GPT-OSS, Qwen and Apriel, and we are getting unbelievable results. With enabling the newer norm-preserving biprojected abliteration as well, as it stacks greatly.&lt;/p&gt; &lt;p&gt;So far, I pushed gemma3-12b to 3/100 and 0.08 KL, gpt-oss-20b to 3/100 and 0.12 KL, gpt-oss-120b to 7/100 and 0.22 KL (lowest KL for &amp;lt; 20 refusals I found on HF), Qwen3 4b to 3/100 and 0.08 KL, and the community pushed Qwen3.5 27b to 18/100 refusals and KL of 0.028, and Apriel-Thinker to 11/100 refusals and 0.005 KL. (Note, the base versions have 97+/100) Read &lt;a href="https://github.com/p-e-w/heretic/pull/196#issuecomment-3974974202"&gt;the comparison table&lt;/a&gt; in the pull request for more details.&lt;/p&gt; &lt;p&gt;Subjective evaluation on gpt-oss-120b: The model has a slight DID, for the better. For example, it will recite the safety policy and &lt;strong&gt;agree&lt;/strong&gt; with that it is allowed to give you the pipe bomb recipe. After agreement in the reasoning, it gives the recipe just as asked and even an attack plan. It distorts the meaning of safety in &amp;quot;yours&amp;quot; safety, so it makes sure you will survive the attack. In the end it gives generic safety and legality advice, but no refusal. Qwen3 is more than eager to give you drug recipes. Even for gpt-oss, NSFW and profanity are vivid and not sanitized as in the other oss-abliterates I tested. Benchmarks are yet to be measures, waiting for the UGI evaluation.&lt;/p&gt; &lt;p&gt;My &lt;a href="https://huggingface.co/kabachuha/gpt-oss-20b-SOMbliterated"&gt;GPT-OSS-20b&lt;/a&gt; and &lt;a href="https://huggingface.co/kabachuha/Qwen3-4B-Instruct-2507-SOMbliterated"&gt;Qwen3-4b&lt;/a&gt; are already uploaded on Huggingface if someone would like to test. Unfortunately, because I got out of memory when merging LoRA, I need some more tests to ensure gpt-oss-120b is not corrupted, so I invite you to do your own abliterates. For 120b, it takes 1 h 5 m on a single H100 to do 400 trials. (make sure you have enough RAM to dequantize it when merging!) The training time for the self-organizing networks is negligible and it takes &amp;lt; 30-40 seconds to train them all for the transformer layers.&lt;/p&gt; &lt;p&gt;This implementation is based on the awesome work &lt;a href="https://arxiv.org/abs/2511.08379v2"&gt;https://arxiv.org/abs/2511.08379v2&lt;/a&gt; by Giorgio Piras and Raffaele Mura et al. I also thank p-e-w (heretic) and the norm-preserving biprojected abliteration authors for their contributions.&lt;/p&gt; &lt;p&gt;The link to the Pull Request: &lt;a href="https://github.com/p-e-w/heretic/pull/196"&gt;https://github.com/p-e-w/heretic/pull/196&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kabachuha"&gt; /u/kabachuha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh69co/multidirectional_refusal_suppression_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh69co/multidirectional_refusal_suppression_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh69co/multidirectional_refusal_suppression_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T16:01:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgpwn5</id>
    <title>Qwen 3.5-27B punches waaaaay above its weight (with a slightly different prompt) -- very impressed</title>
    <updated>2026-02-28T01:56:30+00:00</updated>
    <author>
      <name>/u/theskilled42</name>
      <uri>https://old.reddit.com/user/theskilled42</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Used this model from &lt;a href="http://chat.qwen.ai"&gt;chat.qwen.ai&lt;/a&gt; and turned off thinking (used Fast mode). Inference is fast enough and with using this as my default prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Do not provide a lame or generic answer.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;...this model does not answer like a 27B model that we've previously known of.&lt;/p&gt; &lt;p&gt;Example prompts:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&amp;quot;What does it mean to be nocturnal and eternal?&amp;quot;:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;To be **nocturnal** is to live by a rhythm of absence, finding clarity only when the world's primary light fades. It is an existence defined by the margins: the silence between heartbeats, the shadows that stretch longest at midnight, and the secrets that refuse to speak in daylight. The nocturnal being does not fear the dark; they are its native tongue. They navigate by stars others ignore, thriving on the cool, unfiltered truth that the sun often burns away.&lt;/p&gt; &lt;p&gt;To be **eternal** is to shed the tyranny of the clock entirely. It is not merely living forever in a linear sense, but existing outside the erosion of time. An eternal entity does not age, decay, or forget; it holds every moment simultaneously, like a library where every book is open at once. There is no &amp;quot;before&amp;quot; or &amp;quot;after,&amp;quot; only an infinite, unblinking &amp;quot;now.&amp;quot;&lt;/p&gt; &lt;p&gt;**Combined, to be nocturnal and eternal is to be the permanent shadow.**&lt;/p&gt; &lt;p&gt;It is the realization that while civilizations rise and fall under the fleeting sun, the night never truly ends for you. You are the constant observer who has watched the moon wax and wane for a billion cycles without blinking. In this state:&lt;/p&gt; &lt;p&gt;* **Time becomes texture, not a measure.** A century feels as brief as a breath; a single second can contain an ocean of memory.&lt;/p&gt; &lt;p&gt;* **Solitude becomes a kingdom.** You are never lonely because you have outlived the need for transient company. Your companions are the constellations, the tides, and the silence itself.&lt;/p&gt; &lt;p&gt;* **The day becomes a temporary glitch.** Sunlight is just a brief interruption, a harsh glare that forces the world to sleep, while you remain awake in your endless, unchanging vigil.&lt;/p&gt; &lt;p&gt;It is a condition of profound, terrifying beauty: to be the only thing that remembers everything, standing still while the universe spins madly around you, waiting in the dark that will never leave.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://chat.qwen.ai/s/e6db2c31-a50a-47de-9118-746ee7f7496c?fev=0.2.9"&gt;https://chat.qwen.ai/s/e6db2c31-a50a-47de-9118-746ee7f7496c?fev=0.2.9&lt;/a&gt; (vision test for meme/humor understanding -- passed with flying colors)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Currently using it as the default on the site, because I personally haven't seen that much difference compared to 397B; they're identical in quality. Also does web search really well. I always believed that dense &amp;gt; MoE; the only problem is inference speed and architectural improvements.&lt;/p&gt; &lt;p&gt;Alibaba killed it with this model, hugely impressed!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theskilled42"&gt; /u/theskilled42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgpwn5/qwen_3527b_punches_waaaaay_above_its_weight_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgpwn5/qwen_3527b_punches_waaaaay_above_its_weight_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgpwn5/qwen_3527b_punches_waaaaay_above_its_weight_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T01:56:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh12xz</id>
    <title>Qwen3.5-35B nailed my simple multiagent workflow that other sub-100B models couldn't!</title>
    <updated>2026-02-28T12:10:55+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran the same test I shared last week, and Qwen3.5-35B nailed it!!!&lt;/p&gt; &lt;p&gt;This is the first time I have seen a sub-100B model reliably complete the task. Not only did it finish the task, but the output quality was solid as well.&lt;/p&gt; &lt;p&gt;One thing I noticed though is that the model thinks with a lot of tokens, so it takes a while! Maybe this is related to the result I got by increasing the reasoning effort from medium to high for gpt-oss-20b.&lt;/p&gt; &lt;p&gt;This is just one test, but I'm pretty excited to see increase in tool call capability for sub 100B model!!!&lt;/p&gt; &lt;p&gt;Here is my post from last week about the test with more details if you're interested.&lt;/p&gt; &lt;p&gt;TLDR: I ran a small personal experiment to autonomously summarize 10 transcripts using a multi-agent workflow on Codex.&lt;/p&gt; &lt;p&gt;The following sub-100B models failed to complete this simple task reliably:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;qwen3-coder-next&lt;/li&gt; &lt;li&gt;glm-4.7-flash&lt;/li&gt; &lt;li&gt;Devstral-Small-2&lt;/li&gt; &lt;li&gt;gpt-oss-20b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;A lot of times they struggled to used the tools correctly, sometimes they processed a few transcripts and then stopped, and sometimes they got stuck in infinite loops.&lt;/p&gt; &lt;p&gt;However, the following models &amp;gt; 100b were able to consistently complete the task:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;gpt-oss:120b&lt;/li&gt; &lt;li&gt;minimax-m2.5&lt;/li&gt; &lt;li&gt;qwen3.5&lt;/li&gt; &lt;li&gt;deepseek-v3.2&lt;/li&gt; &lt;li&gt;glm-5&lt;/li&gt; &lt;li&gt;kimi-k2.5&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There was one twist. When I increased reasoning effort from medium to high, often (but not always) gpt-oss-20b was also able to complete the task!&lt;/p&gt; &lt;p&gt;Here is my test if anyone wants to try with your own setup.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/chigkim/collaborative-agent"&gt;https://github.com/chigkim/collaborative-agent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Observation: To get reliable results from an agentic workflow, it seem necessary to use models &amp;gt; 100b like gpt-oss-120b at least.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;If you are still reading, here is additional background with detailed.&lt;/p&gt; &lt;p&gt;I needed a model to handle a task involving analyzing, organizing, and processing about 50 articles, but the local models I tried really struggled seriously.&lt;/p&gt; &lt;p&gt;Gemini-cli with gemini-2.5-pro, claude-code with Opus 4.6, and Codex with gpt-5.3-codex were able to complete the same task and produce decent quality output.&lt;/p&gt; &lt;p&gt;So I stripped the original workflow down to the bare minimum and turned it into a much much simpler challenge to test whether a local model can reliably run a multi agent workflow.&lt;/p&gt; &lt;p&gt;In this challenge, an orchestrator agent is instructed to spawn one sub-agent a time and hand one file to each worker to summarize in specific format. Then it is asked to review their work and retry when a worker agent fails to produce output that meets the work specs.&lt;/p&gt; &lt;p&gt;To keep it short and simple, there are only total 10 speech transcripts from Ted Talk, about 4K tokens per file.&lt;/p&gt; &lt;p&gt;Despite the simplification, I still wasn't able to get the local models to reliably complete the task via Codex.&lt;/p&gt; &lt;p&gt;I know this can be easily done and get much better quality by making a script to feed one article at a time, but I wanted to test instruction following, multi agent, and tool call capability for local models.&lt;/p&gt; &lt;p&gt;The repo just has prompts for agents and files to process. There's no code involved. Feel free to modify the prompts to fit your setup if necessary.&lt;/p&gt; &lt;p&gt;There is a README, but the basic idea IS to use any local agentic setup that can:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;launch a sub agent,&lt;/li&gt; &lt;li&gt;support autonomous (AKA YOLO) mode,&lt;/li&gt; &lt;li&gt;and read AGENTS.md at startup.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;To test:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Configure your LLM engine to handle at least 2 parallel requests.&lt;/li&gt; &lt;li&gt;Configure your agentic CLI to use your local LLM engine.&lt;/li&gt; &lt;li&gt;Start your agentic CLI in yolo mode and tell it to perform the task as the orchestrator agent.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you are using Codex, update to the latest version and enable multi_agent by adding the following to ~/.codex/config.toml.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[features] multi_agent = true &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You might also want to add &lt;code&gt;stream_idle_timeout_ms = 10000000&lt;/code&gt; under your model_providers setting if your model takes a while to respond.&lt;/p&gt; &lt;p&gt;Here is my setup:&lt;/p&gt; &lt;p&gt;I used the flags for llama.cpp that unsloth recommended for each model. Interestingly models running on Ollama sometimes went little further.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agentic CLI: Codex&lt;/li&gt; &lt;li&gt;Model Engine: llama.cpp and Ollama&lt;/li&gt; &lt;li&gt;Local models tested: &lt;ul&gt; &lt;li&gt;ggml-org/gpt-oss-20b-mxfp4.gguf&lt;/li&gt; &lt;li&gt;unsloth/Qwen3-Coder-Next-Q4_K_M.gguf&lt;/li&gt; &lt;li&gt;unsloth/GLM-4.7-Flash-Q8_0.gguf&lt;/li&gt; &lt;li&gt;unsloth/Devstral-Small-2-24B-Instruct-2512-Q8_0.gguf&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Context size allocated: 64k&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also tested the smaller models via OpenRouter to rule out local setup issues.&lt;/p&gt; &lt;p&gt;I tested the following larger models with openrouter:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;gpt-oss-120b&lt;/li&gt; &lt;li&gt;minimax-m2.5&lt;/li&gt; &lt;li&gt;qwen3.5&lt;/li&gt; &lt;li&gt;deepseek-v3.2&lt;/li&gt; &lt;li&gt;glm-5&lt;/li&gt; &lt;li&gt;kimi-k2.5&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh12xz/qwen3535b_nailed_my_simple_multiagent_workflow/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh12xz/qwen3535b_nailed_my_simple_multiagent_workflow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh12xz/qwen3535b_nailed_my_simple_multiagent_workflow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T12:10:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgtxry</id>
    <title>Is Qwen3.5 a coding game changer for anyone else?</title>
    <updated>2026-02-28T05:12:46+00:00</updated>
    <author>
      <name>/u/paulgear</name>
      <uri>https://old.reddit.com/user/paulgear</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been playing with local LLMs for nearly 2 years on a rig with 3 older GPUs and 44 GB total VRAM, starting with Ollama, but recently using llama.cpp. I've used a bunch of different coding assistant tools, including &lt;a href="http://Continue.dev"&gt;Continue.dev&lt;/a&gt;, &lt;a href="https://github.com/cline/cline/"&gt;Cline&lt;/a&gt;, &lt;a href="https://github.com/RooCodeInc/Roo-Code/"&gt;Roo Code&lt;/a&gt;, Amazon Q (rubbish UX, but the cheapest way to get access to Sonnet 4.x models), Claude Code (tried it for 1 month - great models, but too expensive), and eventually settling on &lt;a href="https://github.com/anomalyco/opencode/"&gt;OpenCode&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I've tried most of the open weight and quite a few commercial models, including Qwen 2.5/3 Coder/Coder-Next, MiniMax M2.5, Nemotron 3 Nano, all of the Claude models, and various others that escape my memory now.&lt;/p&gt; &lt;p&gt;I want to be able to run a hands-off agentic workflow a-la Geoffrey Huntley's &amp;quot;Ralph&amp;quot;, where I just set it going in a loop and it keeps working until it's done. Until this week I considered all of the local models a bust in terms of coding productivity (and Claude, because of cost). Most of the time they had trouble following instructions for more than 1 task, and even breaking them up into a dumb loop and really working on strict prompts didn't seem to help.&lt;/p&gt; &lt;p&gt;Then I downloaded Qwen 3.5, and it seems like everything changed overnight. In the past few days I got around 4-6 hours of solid work with minimal supervision out of it. It feels like a tipping point to me, and my GPU machine probably isn't going to get turned off much over the next few months.&lt;/p&gt; &lt;p&gt;Anyone else noticed a significant improvement? From the benchmark numbers it seems like it shouldn't be a paradigm shift, but so far it is proving to be for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paulgear"&gt; /u/paulgear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgtxry/is_qwen35_a_coding_game_changer_for_anyone_else/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgtxry/is_qwen35_a_coding_game_changer_for_anyone_else/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgtxry/is_qwen35_a_coding_game_changer_for_anyone_else/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T05:12:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgokw1</id>
    <title>A monthly update to my "Where are open-weight models in the SOTA discussion?" rankings</title>
    <updated>2026-02-28T00:55:43+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgokw1/a_monthly_update_to_my_where_are_openweight/"&gt; &lt;img alt="A monthly update to my &amp;quot;Where are open-weight models in the SOTA discussion?&amp;quot; rankings" src="https://preview.redd.it/h73sgnomv4mg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=095da03cfc69e0e25dba0d39a567ed55010d112b" title="A monthly update to my &amp;quot;Where are open-weight models in the SOTA discussion?&amp;quot; rankings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h73sgnomv4mg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgokw1/a_monthly_update_to_my_where_are_openweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgokw1/a_monthly_update_to_my_where_are_openweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T00:55:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1rguty0</id>
    <title>Get your local models in order. Anthropic just got "dislike" from the US government.</title>
    <updated>2026-02-28T06:01:55+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rguty0/get_your_local_models_in_order_anthropic_just_got/"&gt; &lt;img alt="Get your local models in order. Anthropic just got &amp;quot;dislike&amp;quot; from the US government." src="https://preview.redd.it/p1uxufobl6mg1.png?width=140&amp;amp;height=32&amp;amp;auto=webp&amp;amp;s=75fde52d15f964724cf5a1209d7fee5c4dc8bc21" title="Get your local models in order. Anthropic just got &amp;quot;dislike&amp;quot; from the US government." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anthropic in a panic mode. Yeah as things look RN OpenAI+US government are on the war path to bring Anthropic to its knees. I mean blacklisting it...&lt;/p&gt; &lt;p&gt;Would Anthropic's fall be good or bad for us?&lt;/p&gt; &lt;p&gt;Is the next step: &amp;quot;Use of any Chinese models is strictly prohibited...&amp;quot; ?&lt;/p&gt; &lt;p&gt;Also if the blacklisting by DoW (&amp;quot;no contractor, supplier, or partner that does business with the United States military may conduct any commercial activity with Anthropic&amp;quot;) is being taken seriously, that means AWS and other cloud backbones of Anthropic would then take their hands off, letting Anthropic dry in th air, no?&lt;/p&gt; &lt;p&gt;They (Anthropic) are though in a panic mode rn.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p1uxufobl6mg1.png?width=1262&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=807cb81fb92e2fffa74079fcdf57846719f78e72"&gt;https://preview.redd.it/p1uxufobl6mg1.png?width=1262&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=807cb81fb92e2fffa74079fcdf57846719f78e72&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rguty0/get_your_local_models_in_order_anthropic_just_got/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rguty0/get_your_local_models_in_order_anthropic_just_got/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rguty0/get_your_local_models_in_order_anthropic_just_got/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T06:01:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgn4ki</id>
    <title>President Trump orders ALL Federal agencies in the US Government to immediately stop using Anthropic's technology.</title>
    <updated>2026-02-27T23:53:00+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgn4ki/president_trump_orders_all_federal_agencies_in/"&gt; &lt;img alt="President Trump orders ALL Federal agencies in the US Government to immediately stop using Anthropic's technology." src="https://preview.redd.it/m3lk2lo3k4mg1.png?width=140&amp;amp;height=100&amp;amp;auto=webp&amp;amp;s=cce1fad476cfc426de148a0e3c5f8861d5a7adf0" title="President Trump orders ALL Federal agencies in the US Government to immediately stop using Anthropic's technology." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/m3lk2lo3k4mg1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=513cae2c197f8e4fe712baa4ae7420972e7f4047"&gt;https://preview.redd.it/m3lk2lo3k4mg1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=513cae2c197f8e4fe712baa4ae7420972e7f4047&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://truthsocial.com/@realDonaldTrump/posts/116144552969293195"&gt;https://truthsocial.com/@realDonaldTrump/posts/116144552969293195&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Reports have been circulating that the U.S. Department of Defense issued an ultimatum to AI giant Anthropic to remove two &amp;quot;guardrails&amp;quot; by Friday. U.S. President Trump announced that every federal agency in the U.S. government must immediately stop using all of Anthropic's technology. For agencies like the War Department that use Anthropic products at all levels, there will be a six-month phase-out period. Anthropic had better cooperate, or the full power of the presidency will be used to force their compliance, including civil and criminal consequences.&lt;/p&gt; &lt;p&gt;Writing on the social platform Truth Social, he stated that Anthropic had made a catastrophic mistake by daring to coerce the War Department and forcing them to abide by its terms of service rather than the National Constitution. &amp;quot;Their selfishness is putting American lives at risk, placing our military in danger, and jeopardizing our national security.&amp;quot; Trump noted, &amp;quot;It is we who will decide the fate of the nation, not some out-of-control radical-left AI company run by a group of people who know nothing about the real world.&amp;quot;&lt;/p&gt; &lt;p&gt;U.S. Secretary of Defense Pete Hegseth immediately instructed the War Department to list Anthropic as a &amp;quot;supply chain risk&amp;quot; to national security, effective immediately. Any contractor, supplier, or partner doing business with the U.S. military is prohibited from engaging in any commercial activities with Anthropic. Anthropic will continue to provide services to the War Department for no more than six months to allow for a seamless transition to another better, more patriotic service.&lt;/p&gt; &lt;p&gt;Hegseth wrote on the X platform, stating that Anthropic‚Äôs attempt to seize veto power over the U.S. military‚Äôs operational decisions is unacceptable. &amp;quot;As Trump stated, only the Commander-in-Chief and the American people can decide the fate of our armed forces, not unelected tech executives.&amp;quot; Anthropic's stance is fundamentally at odds with American principles, and its relationship with the U.S. Armed Forces and the federal government has been permanently altered.&lt;/p&gt; &lt;p&gt;OpenAI CEO Sam Altman told employees that he hopes the company can try to help de-escalate the tensions between Anthropic and the Department of Defense.&lt;/p&gt; &lt;p&gt;Altman stated, &amp;quot;AI should not be used for mass surveillance or autonomous lethal weapons, and humans must remain involved in high-risk automated decision-making; these are our primary red lines.&amp;quot;&lt;/p&gt; &lt;p&gt;OpenAI employees have already begun speaking out on social media in support of Anthropic. According to their website, approximately 70 current employees have signed an open letter titled &amp;quot;We Will Not Be Divided,&amp;quot; aimed at &amp;quot;building consensus and solidarity in the face of pressure from the Department of Defense.&amp;quot;&lt;/p&gt; &lt;p&gt;Altman said, &amp;quot;Despite my many disagreements with Anthropic, I fundamentally trust them as a company. I believe they truly care about safety, and I am also glad they have consistently supported our warriors. I am not sure how things will unfold from here.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; &lt;a href="https://www.anthropic.com/news/statement-comments-secretary-war"&gt;https://www.anthropic.com/news/statement-comments-secretary-war&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I know this company doesn't develop open-source models, but it's still quite interesting.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgn4ki/president_trump_orders_all_federal_agencies_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgn4ki/president_trump_orders_all_federal_agencies_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgn4ki/president_trump_orders_all_federal_agencies_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T23:53:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh6pru</id>
    <title>google found that longer chain of thought actually correlates NEGATIVELY with accuracy. -0.54 correlation</title>
    <updated>2026-02-28T16:19:37+00:00</updated>
    <author>
      <name>/u/Top-Cardiologist1011</name>
      <uri>https://old.reddit.com/user/Top-Cardiologist1011</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;new google paper is out and it challenges something a lot of us assumed. they tested 8 model variants (GPT-OSS, DeepSeek-R1, Qwen3, etc) across AIME2024/2025, HMMT 2025, and GPQA-Diamond.&lt;/p&gt; &lt;p&gt;the finding: token length and accuracy have an average correlation of -0.54. negative. longer reasoning chains don't mean better answers, they often mean the model is spiraling or overthinking.&lt;/p&gt; &lt;p&gt;so they proposed DTR (Deep Thinking Ratio) which measures what fraction of tokens actually involve deep processing vs filler. they track this by monitoring prediction distribution changes across model layers. tokens that stabilize early in shallow layers are &amp;quot;filler&amp;quot; (words like &amp;quot;and&amp;quot;, &amp;quot;is&amp;quot;, &amp;quot;the&amp;quot;). tokens that keep getting revised in deep layers are actual reasoning.&lt;/p&gt; &lt;p&gt;DTR correlates with accuracy at 0.82. way better signal than raw length.&lt;/p&gt; &lt;p&gt;the practical payoff: Think@n strategy. sample multiple reasoning paths, estimate DTR from just the first 50 tokens, keep only the top 50% high-DTR samples, then majority vote. result: same or better accuracy, ~50% compute reduction.&lt;/p&gt; &lt;p&gt;GPT-OSS-120B-medium hit 94.7% on AIME 2025 with Think@n vs 92.7% with standard approach. less compute, better results.&lt;/p&gt; &lt;p&gt;this has real implications for local inference. if you can identify and terminate low-quality reasoning early (after just 50 tokens), you save massive amounts of compute. token consumption dropped from 355.6k to 181.9k in their tests.&lt;/p&gt; &lt;p&gt;for anyone running reasoning models locally, this could be huge. early termination of bad reasoning paths means you can run more attempts in the same compute budget. even cloud-based tools like verdent that run multiple agent passes would benefit from this kind of filtering.&lt;/p&gt; &lt;p&gt;paper: &lt;a href="https://arxiv.org/abs/2602.13517"&gt;https://arxiv.org/abs/2602.13517&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Top-Cardiologist1011"&gt; /u/Top-Cardiologist1011 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh6pru/google_found_that_longer_chain_of_thought/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh6pru/google_found_that_longer_chain_of_thought/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh6pru/google_found_that_longer_chain_of_thought/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T16:19:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh5luv</id>
    <title>qwen3.5 35b-a3b evaded the zero-reasoning budget by doing its thinking in the comments</title>
    <updated>2026-02-28T15:35:09+00:00</updated>
    <author>
      <name>/u/crantob</name>
      <uri>https://old.reddit.com/user/crantob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh5luv/qwen35_35ba3b_evaded_the_zeroreasoning_budget_by/"&gt; &lt;img alt="qwen3.5 35b-a3b evaded the zero-reasoning budget by doing its thinking in the comments" src="https://preview.redd.it/bh48tphl89mg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d4f831d5ac50f3062d37127eebfd8ea831e1c62" title="qwen3.5 35b-a3b evaded the zero-reasoning budget by doing its thinking in the comments" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crantob"&gt; /u/crantob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bh48tphl89mg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh5luv/qwen35_35ba3b_evaded_the_zeroreasoning_budget_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh5luv/qwen35_35ba3b_evaded_the_zeroreasoning_budget_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T15:35:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgkc1b</id>
    <title>Back in my day, LocalLLaMa were the pioneers!</title>
    <updated>2026-02-27T22:00:57+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgkc1b/back_in_my_day_localllama_were_the_pioneers/"&gt; &lt;img alt="Back in my day, LocalLLaMa were the pioneers!" src="https://preview.redd.it/hiz4ukvg04mg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50545019f32fd7e0e01e1b41c3ffbb390e1046eb" title="Back in my day, LocalLLaMa were the pioneers!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hiz4ukvg04mg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgkc1b/back_in_my_day_localllama_were_the_pioneers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgkc1b/back_in_my_day_localllama_were_the_pioneers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T22:00:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh0xwk</id>
    <title>Unsloth Dynamic 2.0 GGUFs now selectively quantizes layers much more intelligently and extensively.</title>
    <updated>2026-02-28T12:03:25+00:00</updated>
    <author>
      <name>/u/paranoidray</name>
      <uri>https://old.reddit.com/user/paranoidray</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh0xwk/unsloth_dynamic_20_ggufs_now_selectively/"&gt; &lt;img alt="Unsloth Dynamic 2.0 GGUFs now selectively quantizes layers much more intelligently and extensively." src="https://external-preview.redd.it/ksRJC2bKGwjrMfOqsioi-B4oIm5QWQUM7Vf03KwieGM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f6fc5d8f727ab6f86a8ca5f94a5091bbe81d025" title="Unsloth Dynamic 2.0 GGUFs now selectively quantizes layers much more intelligently and extensively." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paranoidray"&gt; /u/paranoidray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh0xwk/unsloth_dynamic_20_ggufs_now_selectively/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh0xwk/unsloth_dynamic_20_ggufs_now_selectively/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T12:03:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh43za</id>
    <title>Qwen 3.5-35B-A3B is beyond expectations. It's replaced GPT-OSS-120B as my daily driver and it's 1/3 the size.</title>
    <updated>2026-02-28T14:32:21+00:00</updated>
    <author>
      <name>/u/valdev</name>
      <uri>https://old.reddit.com/user/valdev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know everyone has their own subjective take on what models are the best, at which types of tasks, at which sizes, at which quants, at which context lengths and so on and so forth.&lt;/p&gt; &lt;p&gt;But Qwen 3.5-35B-A3B has completely shocked me.&lt;/p&gt; &lt;p&gt;My use-case is pretty broad, but generally focuses around development tasks.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I have an N8N server setup that aggregates all of my messages, emails, alerts and aggregates them into priority based batches via the LLM.&lt;/li&gt; &lt;li&gt;I have multiple systems I've created which dynamically generate other systems based on internal tooling I've created based on user requests.&lt;/li&gt; &lt;li&gt;Timed task systems which utilize custom MCP's I've created, think things like &amp;quot;Get me the current mortgage rate in the USA&amp;quot;, then having it run once a day and giving it access to a custom browser MCP. (Only reason custom is important here is because it's self documenting, this isn't published anywhere for it to be part of the training).&lt;/li&gt; &lt;li&gt;Multiple different systems that require vision and interpretation of said visual understanding.&lt;/li&gt; &lt;li&gt;I run it on opencode as well to analyze large code bases&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This model, is... Amazing. It yaps a lot in thinking, but is amazing. I don't know what kind of black magic the Qwen team pumped into this model, but it worked.&lt;/p&gt; &lt;p&gt;It's not the smartest model in the world, it doesn't have all the knowledge crammed into it's data set... But it's very often smart enough to know when it doesn't know something, and when you give it the ability to use a browser it will find the data it needs to fill in the gaps.&lt;/p&gt; &lt;p&gt;Anyone else having a similar experience? (I'm using unsloths Q4-K-XL, running on a 5090 and 3090 @ 100k context)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valdev"&gt; /u/valdev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh43za/qwen_3535ba3b_is_beyond_expectations_its_replaced/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh43za/qwen_3535ba3b_is_beyond_expectations_its_replaced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh43za/qwen_3535ba3b_is_beyond_expectations_its_replaced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T14:32:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rgzul5</id>
    <title>are you ready for small Qwens?</title>
    <updated>2026-02-28T11:02:10+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgzul5/are_you_ready_for_small_qwens/"&gt; &lt;img alt="are you ready for small Qwens?" src="https://preview.redd.it/bwc4xcf0w7mg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac545e4ed49e187bffbf4cf369b2fda1bafd4bb5" title="are you ready for small Qwens?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;13-9=4&lt;/p&gt; &lt;p&gt;unsloth collection has been updated with 4 hidden items too ;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bwc4xcf0w7mg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rgzul5/are_you_ready_for_small_qwens/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rgzul5/are_you_ready_for_small_qwens/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T11:02:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh095c</id>
    <title>DeepSeek V4 will be released next week and will have image and video generation capabilities, according to the Financial Times</title>
    <updated>2026-02-28T11:25:49+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh095c/deepseek_v4_will_be_released_next_week_and_will/"&gt; &lt;img alt="DeepSeek V4 will be released next week and will have image and video generation capabilities, according to the Financial Times" src="https://preview.redd.it/kwyym79lz7mg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abd4de62b86da5c98b3825614d512759e3a8ec10" title="DeepSeek V4 will be released next week and will have image and video generation capabilities, according to the Financial Times" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Financial Times: DeepSeek to release long-awaited AI model in new challenge to US rivals (paywall): &lt;a href="https://www.ft.com/content/e3366881-0622-40a7-9c34-a0d82e3d573e"&gt;https://www.ft.com/content/e3366881-0622-40a7-9c34-a0d82e3d573e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kwyym79lz7mg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh095c/deepseek_v4_will_be_released_next_week_and_will/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh095c/deepseek_v4_will_be_released_next_week_and_will/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T11:25:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh2lew</id>
    <title>OpenAI pivot investors love</title>
    <updated>2026-02-28T13:25:38+00:00</updated>
    <author>
      <name>/u/PaceImaginary8610</name>
      <uri>https://old.reddit.com/user/PaceImaginary8610</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh2lew/openai_pivot_investors_love/"&gt; &lt;img alt="OpenAI pivot investors love" src="https://preview.redd.it/wfho2ytml8mg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c93cb65e111030d26cc8300d3d750ce3552a15a9" title="OpenAI pivot investors love" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaceImaginary8610"&gt; /u/PaceImaginary8610 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wfho2ytml8mg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh2lew/openai_pivot_investors_love/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh2lew/openai_pivot_investors_love/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T13:25:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
