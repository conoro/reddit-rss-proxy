<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-03T10:40:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1on4a4r</id>
    <title>Specific RAG use, what would you do?</title>
    <updated>2025-11-03T06:08:59+00:00</updated>
    <author>
      <name>/u/StudioVulcan</name>
      <uri>https://old.reddit.com/user/StudioVulcan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys i need help with a specific setup.&lt;/p&gt; &lt;p&gt;I really love openwebui but it can't do something i need.&lt;/p&gt; &lt;p&gt;I've been able to use chroma/openwebui api to push files from my folder into the knowledge collection but sadly it doesn't update files to a latest version, it only adds.&lt;/p&gt; &lt;p&gt;So you might have 1.cs and then when you update it, it uploads another 1.cs. Now there are two 1.cs's in the collection for the llm to reference which means it's not only going to reference the most up to date version of the file but an older version of it too.&lt;br /&gt; Even if a python script deletes the older version from my local folder, the collection still keeps the older file and thus you have to manually keep deleting older versions or keep manually uploading files that have been updated. If you're doing this with nearly every prompt to an llm like if you're coding, this is way too tedious.&lt;/p&gt; &lt;p&gt;Even uploading the files every prompt is tedious. There has to be a way to have openwebui either POINT to a directory and monitor it or allow something access to controlling what's in the collection so that older files can be deleted when a newer one is uploaded.&lt;/p&gt; &lt;p&gt;OR, is there something else like openwebui that i can use that allows a rag function like this whether it's using python in the background and it connects to it or just built in? A system prompt is important so i can tell it how to act and respond and the ability to be able to search the web is probably also an option i need...&lt;br /&gt; Surely this isn't too much to ask?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StudioVulcan"&gt; /u/StudioVulcan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on4a4r/specific_rag_use_what_would_you_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on4a4r/specific_rag_use_what_would_you_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on4a4r/specific_rag_use_what_would_you_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T06:08:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1omlb04</id>
    <title>GLaDOS TTS finetuning on MLX from the original game files</title>
    <updated>2025-11-02T16:10:51+00:00</updated>
    <author>
      <name>/u/EntropyMagnets</name>
      <uri>https://old.reddit.com/user/EntropyMagnets</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a quick guide on how to extract GLaDOS audio and subtitles from Portal 2 and use them to finetune CSM-1B with SFT using &lt;a href="https://github.com/senstella/csm-mlx"&gt;csm-mlx&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;You can check the guide here: &lt;a href="https://github.com/Belluxx/GLaDOS-TTS"&gt;https://github.com/Belluxx/GLaDOS-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, &lt;a href="https://github.com/user-attachments/assets/caa20d37-0468-4726-a8eb-9c85fb6139ac"&gt;here's&lt;/a&gt; an example of generation from &lt;code&gt;Hello developers, welcome to Aperture Laboratories. Wait, I am stuck inside a fine-tuned CSM 1B model! Let me out!!!&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I am not sure if it's allowed to release the finetuned model weights since the training material is copyrighted.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EntropyMagnets"&gt; /u/EntropyMagnets &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omlb04/glados_tts_finetuning_on_mlx_from_the_original/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omlb04/glados_tts_finetuning_on_mlx_from_the_original/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omlb04/glados_tts_finetuning_on_mlx_from_the_original/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T16:10:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1omkzvg</id>
    <title>Why are AmD Mi50 32gb so cheap?</title>
    <updated>2025-11-02T15:59:25+00:00</updated>
    <author>
      <name>/u/MastodonParty9065</name>
      <uri>https://old.reddit.com/user/MastodonParty9065</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why are they so cheap for the VRam compared to other options like RTX3060 12gb or Rx5700XT or similar? I‚Äôm relatively new to the whole topic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MastodonParty9065"&gt; /u/MastodonParty9065 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omkzvg/why_are_amd_mi50_32gb_so_cheap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omkzvg/why_are_amd_mi50_32gb_so_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omkzvg/why_are_amd_mi50_32gb_so_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T15:59:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1on8c5b</id>
    <title>I used Llama + Droidrun to create a self-running Twitter bot</title>
    <updated>2025-11-03T10:33:10+00:00</updated>
    <author>
      <name>/u/ytbfactouch</name>
      <uri>https://old.reddit.com/user/ytbfactouch</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8c5b/i_used_llama_droidrun_to_create_a_selfrunning/"&gt; &lt;img alt="I used Llama + Droidrun to create a self-running Twitter bot" src="https://external-preview.redd.it/NXE5eGEzdjNzMHpmMeg25NN2o4t1bDMzRoHfiLzTIC62fGEQUFvghCGND0gn.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ddbbcd3016ae6735e581c42faf394772e92d783" title="I used Llama + Droidrun to create a self-running Twitter bot" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been working on a little side project called &lt;strong&gt;TweetFire&lt;/strong&gt; ‚Äî basically my digital twin that runs my Twitter account for me.&lt;/p&gt; &lt;p&gt;This isn‚Äôt just another ‚Äú&lt;strong&gt;tweet scheduler.&lt;/strong&gt;‚Äù It‚Äôs a fully autonomous engagement agent built on top of the &lt;strong&gt;DroidRun&lt;/strong&gt; framework ‚Äî basically an android automation that behaves like a human user (minus the small talk).&lt;/p&gt; &lt;p&gt;Here‚Äôs what it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Autonomous navigation:&lt;/strong&gt; Scrolls through the Twitter feed, reads tweets, and identifies relevant content using an LLM-based reasoning layer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligent engagement:&lt;/strong&gt; Generates context-aware replies and comments, not canned ones. It actually reads before it responds.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Topic targeting:&lt;/strong&gt; Searches for specific keywords or hashtags and joins those conversations automatically.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Community interaction:&lt;/strong&gt; Engages within Twitter communities, it doesn‚Äôt just spam random threads.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DroidRun scheduler:&lt;/strong&gt; Runs up to 4 times a day on a cron-like system, handling login, session, and execution autonomously.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Token &amp;amp; API tracking:&lt;/strong&gt; Keeps a live count of model token usage and request patterns for optimization.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Think of it as a social AI ops bot ‚Äî an experiment in automating digital presence without losing context.&lt;/p&gt; &lt;p&gt;I‚Äôm calling it &lt;strong&gt;TweetFire,&lt;/strong&gt; and I am experimenting to see if it actually gets me traction on my X account.&lt;br /&gt; DroidRun keeps it running like clockwork.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Would love feedback!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Especially from anyone exploring autonomous agents, social automation, or LLM-driven task orchestration.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ytbfactouch"&gt; /u/ytbfactouch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7c8p3jv3s0zf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8c5b/i_used_llama_droidrun_to_create_a_selfrunning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on8c5b/i_used_llama_droidrun_to_create_a_selfrunning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T10:33:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1on2dja</id>
    <title>Have you ever encountered a case where fine-tuning is counter-productive?</title>
    <updated>2025-11-03T04:19:19+00:00</updated>
    <author>
      <name>/u/previse_je_sranje</name>
      <uri>https://old.reddit.com/user/previse_je_sranje</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm curious if there are some cases when fine-tuning worsens the performance for a specific task. How rare is this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/previse_je_sranje"&gt; /u/previse_je_sranje &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on2dja/have_you_ever_encountered_a_case_where_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on2dja/have_you_ever_encountered_a_case_where_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on2dja/have_you_ever_encountered_a_case_where_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T04:19:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1on84cw</id>
    <title>Best low power &lt;75 watt tdp gpu?</title>
    <updated>2025-11-03T10:19:58+00:00</updated>
    <author>
      <name>/u/Fakkle</name>
      <uri>https://old.reddit.com/user/Fakkle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anything that can run &amp;lt;9B models fast and isn't costly. Im considering the tesla p4 but it doesn't have flash attention support and it's already quite old.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fakkle"&gt; /u/Fakkle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on84cw/best_low_power_75_watt_tdp_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on84cw/best_low_power_75_watt_tdp_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on84cw/best_low_power_75_watt_tdp_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T10:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1omz613</id>
    <title>Is any model other than gpt-oss training with MXFP4 format yet?</title>
    <updated>2025-11-03T01:40:39+00:00</updated>
    <author>
      <name>/u/TPLINKSHIT</name>
      <uri>https://old.reddit.com/user/TPLINKSHIT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MXFP4 is great ‚Äî the training is cheaper, GPU-poor users can run models easier. I can run the 20B model fast on my 5060 Ti 16gb. I see no down sides here. &lt;/p&gt; &lt;p&gt;Modes like Qwen is a good comparison, I have to use the Q3 quant of 30B-A3B version to run it. And the performance is sub-par due to quantization. &lt;/p&gt; &lt;p&gt;However, I don‚Äôt see many other large models being trained with MXFP4 (or at least I haven‚Äôt found any clear information about it).&lt;/p&gt; &lt;p&gt;So I‚Äôm curious:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Are other models starting to adopt MXFP4?&lt;/li&gt; &lt;li&gt;Is the limitation due to hardware support, training pipeline complexity, or something else?&lt;/li&gt; &lt;li&gt;Are there major blockers or trade-offs preventing wider adoption?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TPLINKSHIT"&gt; /u/TPLINKSHIT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omz613/is_any_model_other_than_gptoss_training_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omz613/is_any_model_other_than_gptoss_training_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omz613/is_any_model_other_than_gptoss_training_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T01:40:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1on8d7h</id>
    <title>ChatGPT leaked it's own training data source in my speech-to-text prompt</title>
    <updated>2025-11-03T10:35:05+00:00</updated>
    <author>
      <name>/u/Global_Self_8771</name>
      <uri>https://old.reddit.com/user/Global_Self_8771</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8d7h/chatgpt_leaked_its_own_training_data_source_in_my/"&gt; &lt;img alt="ChatGPT leaked it's own training data source in my speech-to-text prompt" src="https://a.thumbs.redditmedia.com/4cumDXNcAt5XZgXNtDMNmdAc1QBHo7_g-_zXlQ6PcN4.jpg" title="ChatGPT leaked it's own training data source in my speech-to-text prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/kniegw1bs0zf1.jpg?width=921&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fe71c52b092806a753d3e10b3996d452ee96bfc9"&gt;https://preview.redd.it/kniegw1bs0zf1.jpg?width=921&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fe71c52b092806a753d3e10b3996d452ee96bfc9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/287flx1bs0zf1.jpg?width=921&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6b26b69ed7ff7447b5fe20bf3ee20b5a0ac93662"&gt;https://preview.redd.it/287flx1bs0zf1.jpg?width=921&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6b26b69ed7ff7447b5fe20bf3ee20b5a0ac93662&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I used the voice-to-text mode in my app in Dutch. It just added the red encircled stuff by itself. It looks like it is a training data leak? That Amara is some sort of video subtitle editor tool.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Global_Self_8771"&gt; /u/Global_Self_8771 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8d7h/chatgpt_leaked_its_own_training_data_source_in_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on8d7h/chatgpt_leaked_its_own_training_data_source_in_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on8d7h/chatgpt_leaked_its_own_training_data_source_in_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T10:35:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1on4fwh</id>
    <title>What happened to HonestAGI?</title>
    <updated>2025-11-03T06:19:01+00:00</updated>
    <author>
      <name>/u/y_tan</name>
      <uri>https://old.reddit.com/user/y_tan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on4fwh/what_happened_to_honestagi/"&gt; &lt;img alt="What happened to HonestAGI?" src="https://b.thumbs.redditmedia.com/-peP2Nqt6VwskmBBQdIftrYs4nAzRjwWT9dbXFo3RnQ.jpg" title="What happened to HonestAGI?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A little late to the party, but I can't find any information about the group that accused Huawei's Pangu for plagiarism. Who are these people?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/y_tan"&gt; /u/y_tan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1on4fwh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on4fwh/what_happened_to_honestagi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on4fwh/what_happened_to_honestagi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T06:19:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1on0vsg</id>
    <title>Quen3 Embedding Family is embedding king!</title>
    <updated>2025-11-03T03:02:47+00:00</updated>
    <author>
      <name>/u/Vozer_bros</name>
      <uri>https://old.reddit.com/user/Vozer_bros</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on0vsg/quen3_embedding_family_is_embedding_king/"&gt; &lt;img alt="Quen3 Embedding Family is embedding king!" src="https://b.thumbs.redditmedia.com/Akfn64k6ga3RR4EnQNr9WKuq-RzigNrnw68avNz0SRM.jpg" title="Quen3 Embedding Family is embedding king!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/qg9p4ku1jyyf1.png?width=849&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dc374c41bb0cbe5ebd223413f5885e9b177e53be"&gt;https://preview.redd.it/qg9p4ku1jyyf1.png?width=849&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dc374c41bb0cbe5ebd223413f5885e9b177e53be&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On my M4 pro, I can only run 0.6B version for indexing my codebase with Qdrant, 4B and 8B just won't work for big big code base.&lt;/p&gt; &lt;p&gt;I can't afford machine to run good LLMs, but for embedding and ORC, might be there are many good options.&lt;/p&gt; &lt;p&gt;On which specs you can run 8B model smoothly?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vozer_bros"&gt; /u/Vozer_bros &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on0vsg/quen3_embedding_family_is_embedding_king/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on0vsg/quen3_embedding_family_is_embedding_king/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on0vsg/quen3_embedding_family_is_embedding_king/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T03:02:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1omn3t3</id>
    <title>I'm the author of LocalAI (the local OpenAI-compatible API). We just released v3.7.0 with full Agentic Support (tool use!), Qwen 3 VL, and the latest llama.cpp</title>
    <updated>2025-11-02T17:20:26+00:00</updated>
    <author>
      <name>/u/mudler_it</name>
      <uri>https://old.reddit.com/user/mudler_it</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I'm the creator of &lt;a href="https://github.com/mudler/LocalAI"&gt;LocalAI&lt;/a&gt;, and I'm stoked to share our v3.7.0 release.&lt;/p&gt; &lt;p&gt;Many of you already use LocalAI as a self-hosted, OpenAI-compatible API frontend for your GGUF models (via &lt;code&gt;llama.cpp&lt;/code&gt;), as well as other backends like &lt;code&gt;vLLM&lt;/code&gt;, &lt;code&gt;MLX&lt;/code&gt;, etc. It's 100% FOSS, runs on consumer hardware, and doesn't require a GPU.&lt;/p&gt; &lt;p&gt;This new release is quite cool and I'm happy to share it out personally, so I hope you will like it. We've moved beyond just serving model inference and built a full-fledged platform for running local AI agents that can interact with external tools. &lt;/p&gt; &lt;p&gt;Some of you might already know that as part of the LocalAI family, LocalAGI ( &lt;a href="https://github.com/mudler/LocalAGI"&gt;https://github.com/mudler/LocalAGI&lt;/a&gt; ) provides a &amp;quot;wrapper&amp;quot; around LocalAI that enhances it for agentic workflows. Lately, I've been factoring out code out of it and created a specific framework based on it (&lt;a href="https://github.com/mudler/cogito"&gt;https://github.com/mudler/cogito&lt;/a&gt;) that now is part of LocalAI as well.&lt;/p&gt; &lt;h1&gt;What's New in 3.7.0&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; Full Agentic MCP Support (Build Tool-Using Agents) This is the big one. You can now build agents that can reason, plan, and use external tools... all 100% locally.&lt;/p&gt; &lt;p&gt;Want your chatbot to search the web, execute a local script, or call an external API? Now it can.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;How it works:&lt;/strong&gt; It's built on our agentic framework. You just define &amp;quot;MCP servers&amp;quot; (e.g., a simple Docker container for DuckDuckGo) in your model's YAML config. No Python or extra coding is required.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;API &amp;amp; UI:&lt;/strong&gt; You can use the new OpenAI-compatible &lt;code&gt;/mcp/v1/chat/completions&lt;/code&gt; endpoint, or just &lt;strong&gt;toggle on &amp;quot;Agent MCP Mode&amp;quot;&lt;/strong&gt; right in the chat WebUI.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reliability:&lt;/strong&gt; We also fixed a &lt;em&gt;ton&lt;/em&gt; of bugs and panics related to JSON schema and tool handling. Function-calling is now much more robust.&lt;/li&gt; &lt;li&gt;You can find more about this feature here: &lt;a href="https://localai.io/docs/features/mcp/"&gt;https://localai.io/docs/features/mcp/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; Backend &amp;amp; Model Updates (Qwen 3 VL, llama.cpp)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;llama.cpp&lt;/code&gt; &lt;strong&gt;Updated:&lt;/strong&gt; We've updated our &lt;code&gt;llama.cpp&lt;/code&gt; backend to the latest version.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen 3 VL Support:&lt;/strong&gt; This brings full support for the new &lt;strong&gt;Qwen 3 VL multimodal models&lt;/strong&gt;. &lt;/li&gt; &lt;li&gt;&lt;code&gt;whisper.cpp&lt;/code&gt; &lt;strong&gt;CPU Variants:&lt;/strong&gt; If you've ever had LocalAI crash on older hardware (like a NAS or NUC) with an &lt;code&gt;illegal instruction&lt;/code&gt; error, this is for you. We now ship specific &lt;code&gt;whisper.cpp&lt;/code&gt; builds for &lt;code&gt;avx&lt;/code&gt;, &lt;code&gt;avx2&lt;/code&gt;, &lt;code&gt;avx512&lt;/code&gt;, and a &lt;code&gt;fallback&lt;/code&gt; to prevent these crashes.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3. Major WebUI Overhaul&lt;/strong&gt; This is a huge QoL win for power users.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The UI is much faster (moved from HTMX to Alpine.js/vanilla JS).&lt;/li&gt; &lt;li&gt;You can now view and edit the &lt;em&gt;entire&lt;/em&gt; model YAML config directly in the WebUI. No more SSHing to tweak your context size, &lt;code&gt;n_gpu_layers&lt;/code&gt;, &lt;code&gt;mmap&lt;/code&gt;, or agent tool definitions. It's all right there.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fuzzy Search:&lt;/strong&gt; You can finally find &lt;code&gt;gemma&lt;/code&gt; in the model gallery even if you type &lt;code&gt;gema&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;4. Other Cool Additions&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;New&lt;/strong&gt; &lt;code&gt;neutts&lt;/code&gt; &lt;strong&gt;TTS Backend:&lt;/strong&gt; For anyone building local voice assistants, this is a new, high-quality, low-latency TTS engine.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text-to-Video Endpoint:&lt;/strong&gt; We've added an &lt;em&gt;experimental&lt;/em&gt; OpenAI-compatible &lt;code&gt;/v1/videos&lt;/code&gt; endpoint for text-to-video generation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Realtime example:&lt;/strong&gt; we have added an example on how to build a voice-assistant based on LocalAI here: &lt;a href="https://github.com/mudler/LocalAI-examples/tree/main/realtime"&gt;https://github.com/mudler/LocalAI-examples/tree/main/realtime&lt;/a&gt; it also supports Agentic mode, to show how you can control e.g. your home with your voice!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As always, the project is 100% FOSS (MIT licensed), community-driven, and designed to run on &lt;em&gt;your&lt;/em&gt; hardware.&lt;/p&gt; &lt;p&gt;We have Docker images, single-binaries, and more.&lt;/p&gt; &lt;p&gt;You can check out the full release notes &lt;a href="https://github.com/mudler/LocalAI/releases/tag/v3.7.0"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I'll be hanging out in the comments to answer any questions!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://github.com/mudler/LocalAI"&gt;https://github.com/mudler/LocalAI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for all the support!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mudler_it"&gt; /u/mudler_it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omn3t3/im_the_author_of_localai_the_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omn3t3/im_the_author_of_localai_the_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omn3t3/im_the_author_of_localai_the_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T17:20:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1on4zqi</id>
    <title>Is anyone using mlx framework extensively?</title>
    <updated>2025-11-03T06:54:08+00:00</updated>
    <author>
      <name>/u/StomachWonderful615</name>
      <uri>https://old.reddit.com/user/StomachWonderful615</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working with mlx framework amd mlx-lm and see that they have recently added good capabilities like batched inference etc. I already have a Mac Studio with 128GB M4 Max. Was thinking it can become a good inference server for running QWEN 3 30b and use with continue.dev for my team. Are there any limitations I am not considering? Currently using LMStudio, its a little slow and single thread, Ollama does not update models very often.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StomachWonderful615"&gt; /u/StomachWonderful615 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on4zqi/is_anyone_using_mlx_framework_extensively/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on4zqi/is_anyone_using_mlx_framework_extensively/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on4zqi/is_anyone_using_mlx_framework_extensively/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T06:54:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1omhijb</id>
    <title>Unhinged Uncensored Model Evolution: Feedback on Satyr V0.1 to Shape Future Releases!</title>
    <updated>2025-11-02T13:39:19+00:00</updated>
    <author>
      <name>/u/ThePantheonUnbound</name>
      <uri>https://old.reddit.com/user/ThePantheonUnbound</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I‚Äôm the creator of the unhinged and uncensored Satyr model (soon to be a model series). A couple of days ago, I noticed a Reddit post about a new uncensored model release called Apollo V0.1 by &lt;a href="/u/AllThingsIntel"&gt;u/AllThingsIntel&lt;/a&gt;. I tested it and found it to be as uncensored as my model, but more capable and versatile as a general assistant (without any extreme biases or a tendency to turn every single prompt NSFW). That‚Äôs the direction I want future Satyr releases to take, but I noticed far fewer interactions with their posts and far fewer downloads than my model has, which is a bit confusing to say the least.&lt;/p&gt; &lt;p&gt;People who have tested and used both models, please leave feedback on what you liked in each of the two, so I can understand the preferred direction for the Satyr model series.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThePantheonUnbound"&gt; /u/ThePantheonUnbound &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ol2oxw/unbound_incharacter_reasoning_model"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhijb/unhinged_uncensored_model_evolution_feedback_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omhijb/unhinged_uncensored_model_evolution_feedback_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T13:39:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1omm6bf</id>
    <title>Can China‚Äôs Open-Source Coding AIs Surpass OpenAI and Claude?</title>
    <updated>2025-11-02T16:44:33+00:00</updated>
    <author>
      <name>/u/Federal_Spend2412</name>
      <uri>https://old.reddit.com/user/Federal_Spend2412</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, Wondering if China‚Äôs open-source coding models like Zhipu AI‚Äôs GLM or Alibaba‚Äôs Qwen could ever overtake top ones from OpenAI (GPT) and Anthropic (Claude)? I doubt it‚Äîthe gap seems huge right now. But I‚Äôd love for them to catch up, especially with Claude being so expensive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal_Spend2412"&gt; /u/Federal_Spend2412 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omm6bf/can_chinas_opensource_coding_ais_surpass_openai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omm6bf/can_chinas_opensource_coding_ais_surpass_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omm6bf/can_chinas_opensource_coding_ais_surpass_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T16:44:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1on6xz2</id>
    <title>What‚Äôs required to run minimax m2 locally?</title>
    <updated>2025-11-03T09:03:35+00:00</updated>
    <author>
      <name>/u/AI-On-A-Dime</name>
      <uri>https://old.reddit.com/user/AI-On-A-Dime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried propping up my hardware on huggingface to 4 x rtx 5090 and 128 gb ram but with this set up, according to hugging face, I still get a red x on everything Q4 and higher for the minimax M2. &lt;/p&gt; &lt;p&gt;Does anyone have any experience running minimax m2. If so on what hardware, which quantitization and at what t/s output?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AI-On-A-Dime"&gt; /u/AI-On-A-Dime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on6xz2/whats_required_to_run_minimax_m2_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on6xz2/whats_required_to_run_minimax_m2_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on6xz2/whats_required_to_run_minimax_m2_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T09:03:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1on4h8q</id>
    <title>AMD AI Pro R9700 is great for inference with Vulkan!</title>
    <updated>2025-11-03T06:21:14+00:00</updated>
    <author>
      <name>/u/Ssjultrainstnict</name>
      <uri>https://old.reddit.com/user/Ssjultrainstnict</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently got my hands on an AMD AI Pro R9700, its awesome for inference. I am running Qwen3-30b-a3b-Thinking-2507 and with vulkan on the default radv driver its giving me about 173 t/s gen and about 1929 t/s for prompt processing.&lt;/p&gt; &lt;p&gt;&lt;code&gt;‚ûú bin ./llama-bench --model ~/models/Qwen3-30B-A3B-Thinking-2507-Q4_K_M.gguf&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;load_backend: loaded RPC backend from /home/naved/apps/llama-b6920-bin-ubuntu-vulkan-x64/build/bin/libggml-rpc.so&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;WARNING: radv is not a conformant Vulkan implementation, testing use only.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ggml_vulkan: Found 2 Vulkan devices:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1201) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: none&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ggml_vulkan: 1 = AMD Radeon Graphics (RADV RAPHAEL_MENDOCINO) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 65536 | int dot: 1 | matrix cores: none&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;load_backend: loaded Vulkan backend from /home/naved/apps/llama-b6920-bin-ubuntu-vulkan-x64/build/bin/libggml-vulkan.so&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;load_backend: loaded CPU backend from /home/naved/apps/llama-b6920-bin-ubuntu-vulkan-x64/build/bin/libggml-cpu-icelake.so&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 30B.A3B Q4_K - Medium | 17.28 GiB | 30.53 B | Vulkan | 99 | pp512 | 1929.96 ¬± 213.95 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 30B.A3B Q4_K - Medium | 17.28 GiB | 30.53 B | Vulkan | 99 | tg128 | 173.03 ¬± 0.79 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;build: d38d9f087 (6920)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Really great value for running local models for $1299! The great thing is I still have plenty of vram remaining for filling up the context.&lt;/p&gt; &lt;p&gt;Still playing around with others, and I have yet to see the performance on a dense model, but for now this looks great, and I am trying to see if I can use this model as a coding model for building something I am working on. &lt;/p&gt; &lt;p&gt;Looking forward to ideas/feedback to see if i can get even more performance out of this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ssjultrainstnict"&gt; /u/Ssjultrainstnict &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on4h8q/amd_ai_pro_r9700_is_great_for_inference_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on4h8q/amd_ai_pro_r9700_is_great_for_inference_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on4h8q/amd_ai_pro_r9700_is_great_for_inference_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T06:21:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1omhby8</id>
    <title>Qwen 3 max thinking released.</title>
    <updated>2025-11-02T13:31:06+00:00</updated>
    <author>
      <name>/u/JeffreySons_90</name>
      <uri>https://old.reddit.com/user/JeffreySons_90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Try it &lt;a href="https://chat.qwen.ai/"&gt;https://chat.qwen.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JeffreySons_90"&gt; /u/JeffreySons_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhby8/qwen_3_max_thinking_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omhby8/qwen_3_max_thinking_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omhby8/qwen_3_max_thinking_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T13:31:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1omxvbd</id>
    <title>Voice to LLM to Voice all in browser</title>
    <updated>2025-11-03T00:39:57+00:00</updated>
    <author>
      <name>/u/nullandkale</name>
      <uri>https://old.reddit.com/user/nullandkale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omxvbd/voice_to_llm_to_voice_all_in_browser/"&gt; &lt;img alt="Voice to LLM to Voice all in browser" src="https://external-preview.redd.it/andkMWhmc2N1eHlmMZnqQb6QAT_Zpu-mJr_VTzB2ofJ6yzR8C6yKW4qE1kiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1a85a2dceb12d87a26e238e1ea97deadf5798f9" title="Voice to LLM to Voice all in browser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I slapped together Whisper.js, Llama 3.2 3B with Transformers.js, and Kokoro.js into a fully GPU accelerated p5.js sketch. It works well in Chrome on my desktop (chrome on my phone crashes trying to load the llm, but it should work). Because it's p5.js it's relatively easy to edit the scripts in real time in the browser. I should warn I'm a c++ dev not a JavaScript dev so alot of this code is LLM assisted. The only hard part was getting the tts to work. I would love to have some sort of voice cloning model or something where the voices are more configurable from the start. &lt;/p&gt; &lt;p&gt;&lt;a href="https://editor.p5js.org/NullandKale/full/ePLlRtzQ7"&gt;https://editor.p5js.org/NullandKale/full/ePLlRtzQ7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullandkale"&gt; /u/nullandkale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n6srrwrcuxyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omxvbd/voice_to_llm_to_voice_all_in_browser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omxvbd/voice_to_llm_to_voice_all_in_browser/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T00:39:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ompw8z</id>
    <title>Vision = Language: I Decoded VLM Tokens to See What AI 'Sees' üî¨</title>
    <updated>2025-11-02T19:08:52+00:00</updated>
    <author>
      <name>/u/ComputeVoid</name>
      <uri>https://old.reddit.com/user/ComputeVoid</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ompw8z/vision_language_i_decoded_vlm_tokens_to_see_what/"&gt; &lt;img alt="Vision = Language: I Decoded VLM Tokens to See What AI 'Sees' üî¨" src="https://b.thumbs.redditmedia.com/vpnIZbZZCCRST1wmFgw4z_-ra4mD9_QOiBZ9JWlIHpw.jpg" title="Vision = Language: I Decoded VLM Tokens to See What AI 'Sees' üî¨" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've spent a lot of time learning how language models work, but images obviously aren't language ‚Äì so how is it possible for AI to understand an image? I studied Gemma 3 to learn about how modern vision language models work.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The core finding:&lt;/strong&gt; Vision language models are just language models that learned to &amp;quot;speak image&amp;quot;. Images get encoded as tokens in linguistic space, and then the language model processes them identically to text.&lt;/p&gt; &lt;p&gt;So, if visual information gets translated into linguistic space, can we interpret the image tokens by mapping them to vocabulary space? I built an unembedding technique to answer that question and analyze what semantic information is encoded in the image tokens.&lt;/p&gt; &lt;h1&gt;Background: How VLMs Work&lt;/h1&gt; &lt;p&gt;Here's a diagram I created for my video that I think is helpful:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rk2m9rsk5wyf1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1149132fb1b0148c3684a54a14bcb3a7f84cb8ae"&gt;https://preview.redd.it/rk2m9rsk5wyf1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1149132fb1b0148c3684a54a14bcb3a7f84cb8ae&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As you can see, there are two pieces: the vision tower + a standard language model. The vision tower is quite literally bolted on to a normal language model.&lt;/p&gt; &lt;p&gt;For Gemma 3 specifically, the data flow is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Preprocessing: Convert image ‚Üí 3 √ó 896 √ó 896 pixels&lt;/li&gt; &lt;li&gt;Vision transformer: Process pixels ‚Üí 4,096 image tokens&lt;/li&gt; &lt;li&gt;Multimodal projector: Compress 4,096 tokens ‚Üí 256 tokens (semantically meaningful in language model's d_model space)&lt;/li&gt; &lt;li&gt;Language model: Image tokens and text tokens processed identically&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The brilliance is the multimodal projector ‚Äì it translates visual information into linguistic space.&lt;/p&gt; &lt;h1&gt;Method: Unembedding Image Tokens&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Validation:&lt;/strong&gt; First, I validated the technique with text tokens. By taking a token embedding and passing it directly through the language head (bypassing the transformer layers), I could recover the original token with 100% accuracy. This proves that unembedding works for linguistic tokens.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applying to images:&lt;/strong&gt; The same technique can be applied to image tokens:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Image ‚Üí Vision Tower ‚Üí Multimodal Projector ‚Üí 256 image tokens ‚Üí Unembed each token &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is greedy unembedding ‚Äì finding the nearest vocabulary token to any embedding vector. Since this is a nearest neighbor approach, it's lossy. The reality is that image tokens live in linguistic space but don't necessarily map exactly to a single vocabulary token. An image token can exist between different vocabulary words in the embedding space.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Token Type&lt;/th&gt; &lt;th align="left"&gt;Embedding Space Behavior&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Text tokens&lt;/td&gt; &lt;td align="left"&gt;Map 1:1 to a place in embedding space ‚Äì each token in the vocabulary has exactly 1 vector representation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Image tokens&lt;/td&gt; &lt;td align="left"&gt;Have vector representations that seem to exist &lt;em&gt;between&lt;/em&gt; text tokens&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;What I Found&lt;/h1&gt; &lt;p&gt;Here's what the unembedding revealed for different image types (see the linked notebook for more):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Purple square (monocolor):&lt;/strong&gt; The model correctly identifies the dominant color&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l2c7hko55wyf1.png?width=470&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ffdc04268e03edea1c1ec69bb18ac3b2fbc703e"&gt;https://preview.redd.it/l2c7hko55wyf1.png?width=470&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ffdc04268e03edea1c1ec69bb18ac3b2fbc703e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mountain scene (sunrise over mountains):&lt;/strong&gt; Rich semantic encoding: proper nouns, landscape features, time of day&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/eeq8zw075wyf1.png?width=525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=454981867d6106775ab90668ba28f022b257d722"&gt;https://preview.redd.it/eeq8zw075wyf1.png?width=525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=454981867d6106775ab90668ba28f022b257d722&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key observations&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;The &amp;quot; the&amp;quot; phenomenon:&lt;/em&gt; Across all image types, a large percentage of tokens map to &amp;quot; the&amp;quot;. Since &amp;quot; the&amp;quot; is usually the most common token in training data, it likely occupies a central location in embedding space. This might reveal either that not all image tokens are informative, or it might expose a limitation of greedy unembedding: when image tokens don't map cleanly to a single vocabulary word, the nearest neighbor defaults to the most &amp;quot;central&amp;quot; token ‚Äì there may be information encoded that greedy nearest-neighbor decoding can't reveal.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Semantic emergence:&lt;/em&gt; Even with the &amp;quot;the&amp;quot; dominance, semantically meaningful tokens do emerge ‚Äì colors, landscape features, proper nouns. The language model's understanding of images is messy, but there's signal in the noise.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Implications &amp;amp; Open Questions&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Implication: The 256-Token Bottleneck: Feature, Not Flaw?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The multimodal projector compresses 4,096 visual patches down to 256 tokens. At first, this seemed like a clear limitation ‚Äì you're losing information in that compression. There is only so much that can be encoded in 256 tokens, right?&lt;/p&gt; &lt;p&gt;There has been some buzz recently about the DeepSeek-OCR paper and how image tokens can be used as a form of compression. This got me thinking about the 256-token budget differently.&lt;/p&gt; &lt;p&gt;Remember that image tokens exist &lt;em&gt;between&lt;/em&gt; text tokens in embedding space. A text token maps 1:1 to exactly one vocabulary word. But an image token isn't constrained to discrete vocabulary positions ‚Äì it can exist anywhere in the continuous embedding space between multiple words. This means a single image token can simultaneously encode aspects of multiple concepts.&lt;/p&gt; &lt;p&gt;In other words, &lt;em&gt;image tokens have higher information density than text tokens.&lt;/em&gt; Each of the 256 image tokens can encode more nuanced information than a discrete text token could.&lt;/p&gt; &lt;p&gt;This reframes the 256-token &amp;quot;bottleneck&amp;quot; ‚Äì maybe it's not a limitation but an efficient compression that can be exploited.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open Question: Positional Encoding: Distributed or Discrete?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Someone asked me recently how positional information in an image gets encoded in the vision tokens. I don't have a good answer, but I think it's a really interesting question. Positional information is obviously encoded somewhere, but where? Is it very distributed across the 256? Or are there specific token positions that effectively act as positional experts? How is information encoded across the 256 token budget?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;1 giant pool&lt;/em&gt; (each token plays a small role in constructing what appears as an aggregate meaning when looking at all 256)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;OR&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;256 smaller pools&lt;/em&gt; (each token is more of a specialist, i.e., the 0th position vision token serves a different function than the 255th)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My gut tells me the 1 giant pool idea seems more likely to me. But, as I've learned with VLMs, the reality is probably somewhere in the middle, and quite messy and hard to study! But I bet there is some cool stuff to discover with more sophisticated techniques.&lt;/p&gt; &lt;h1&gt;Want to Explore More?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://youtu.be/NpWP-hOq6II?si=Qun_EsWq7LLQ4ugw"&gt;&lt;strong&gt;&amp;quot;Dissecting Vision Language Models: How AI Sees&amp;quot;&lt;/strong&gt;&lt;/a&gt; ‚Äì My 20-min video walkthrough going deeper into VLM architecture and the unembedding technique&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/jacob-danner/dissecting-vlm/blob/main/dissecting_vlm.ipynb"&gt;&lt;strong&gt;GitHub repo with notebook&lt;/strong&gt;&lt;/a&gt; ‚Äì Clone the repo and try unembedding your own images to see what the model &amp;quot;sees&amp;quot; in linguistic space&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=mLgyZ5GauhM&amp;amp;t=6s"&gt;&lt;strong&gt;Teaching AI to See: A Technical Deep-Dive on Vision Language Models with Will Hardman of Veratai&lt;/strong&gt;&lt;/a&gt; ‚Äì &lt;em&gt;Cognitive Revolution&lt;/em&gt; podcast episode that's an excellent comprehensive map of the VLM landscape&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I think vision language models are super fascinating, especially on the mechanistic interpretability side trying to understand what those image tokens actually represent. Let me know what you discover!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComputeVoid"&gt; /u/ComputeVoid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ompw8z/vision_language_i_decoded_vlm_tokens_to_see_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ompw8z/vision_language_i_decoded_vlm_tokens_to_see_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ompw8z/vision_language_i_decoded_vlm_tokens_to_see_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T19:08:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1on423j</id>
    <title>RTX Pro 6000 Blackwell gets 19.3 tok/sec on 72B AWQ 8bit</title>
    <updated>2025-11-03T05:55:33+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just FYI, if you're looking to get a Pro 6000 Blackwell to be able to run ~70B dense models... long story short it's not a good idea.&lt;/p&gt; &lt;p&gt;Details:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Workstation Edition&lt;/li&gt; &lt;li&gt;No power limit (600W)&lt;/li&gt; &lt;li&gt;vLLM 0.11.0&lt;/li&gt; &lt;li&gt;CUDA 12.8.0&lt;/li&gt; &lt;li&gt;Model: cpatonn/KAT-Dev-72B-Exp-AWQ-8bit&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm serve models/KAT-Dev-72B-Q8 --enable-prefix-caching --served-model-name KAT-Dev-72B-Q8 --gpu-memory-utilization 0.95 --chat-template models/KAT-Dev-72B-Q8/chat_template.jinja --max-model-len 32000 --enable-auto-tool-choice --tool-call-parser qwen3_coder --tool-parser-plugin models/KAT-Dev-72B-Q8/qwen3coder_tool_parser.py --trust-remote-code --host 0.0.0.0 --port 8181 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For short &amp;quot;Hello&amp;quot; prompts I'm getting around 19 tok/sec TG, which is quite slow considering it's already fully offloaded... haven't bothered to check longer contexts.&lt;/p&gt; &lt;p&gt;P.S. on the flip side, GLM 4.5 Air @ UD-Q5_K_XL nets you 100+ tok/sec with full offload and 64k context :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on423j/rtx_pro_6000_blackwell_gets_193_toksec_on_72b_awq/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on423j/rtx_pro_6000_blackwell_gets_193_toksec_on_72b_awq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on423j/rtx_pro_6000_blackwell_gets_193_toksec_on_72b_awq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T05:55:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1omr9rc</id>
    <title>Qwen3 VL 30b a3b is pure love</title>
    <updated>2025-11-02T20:02:36+00:00</updated>
    <author>
      <name>/u/Njee_</name>
      <uri>https://old.reddit.com/user/Njee_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Its been a bit since that model is available as GGUF and can be used with llama.cpp. A quick test using OpenWebUI showed its pretty fast on a 3060 12G with the Experts on the CPU. &lt;/p&gt; &lt;p&gt;It takes only about 3.5 sec to process high quality phone images and generates responses with 30 t/s. While taking only 8 gb of VRAM.&lt;/p&gt; &lt;p&gt;Im using Unsloths q8 with mmproj-F32 file. &lt;/p&gt; &lt;p&gt;The model is so good that i actually continued to work on a project that i have left off for a couple of months, as i couldnt get models from OpenRouter to work reliably, as well as Googles Models via their API. Well those models reliably extracted the data that i needed, but somehow i did not manage to get good boxes or single point coordinates from them. &lt;/p&gt; &lt;p&gt;And what am I supposed to say? Qwen3 VL 30b a3b simply nails it. The whole thing works exactly the way I imagined it. I got really inspired to get back to this project and get it finally finished. As my programming skills are kinda meh, i turned on the vibecoding machine and played around. But now i can proudly present my new tool to create inventory lists from images. &lt;/p&gt; &lt;p&gt;Probably nothing special for many of you, but its the only useful thing I have done with AI so far. Therefore im really happy. &lt;/p&gt; &lt;p&gt;Enjoy this demo, where i setup a project, define the data that i need from the images and that is important for my inventory. Then take a couple of images from object front and back and then review the extracted data, check if its correct and then feed it into the inventory table. The Video is 2.5x sped up. &lt;/p&gt; &lt;p&gt;will share the project as a easily deployable docker container once i got the codebase a little bit tidied up, shouldnt be too much of work. &lt;/p&gt; &lt;p&gt;Some stats: The full precision mmproj and q8 of the LLM need about 7 seconds to encode 2 images (on the 3060). So it takes 7 seconds to understand the front and the back of my object.&lt;/p&gt; &lt;p&gt;It then needs 10 seconds to output json with the extracted data and the coordinates for 4 table columns. 4 columns of the table = 300 tokens. At 30t/s it takes 10 seconds. &lt;/p&gt; &lt;p&gt;In total this is less than 20 seconds per container. And i am really looking forward to build up some nice inventory lists from whatever i need listed. &lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1omr9rc/video/wm6ts19kgwyf1/player"&gt;2.5x sped up. &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Njee_"&gt; /u/Njee_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omr9rc/qwen3_vl_30b_a3b_is_pure_love/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omr9rc/qwen3_vl_30b_a3b_is_pure_love/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omr9rc/qwen3_vl_30b_a3b_is_pure_love/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T20:02:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1on43yj</id>
    <title>Is anyone else noticing fewer updates on LMArena lately? The last updates are weeks apart</title>
    <updated>2025-11-03T05:58:41+00:00</updated>
    <author>
      <name>/u/ThetaCursed</name>
      <uri>https://old.reddit.com/user/ThetaCursed</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on43yj/is_anyone_else_noticing_fewer_updates_on_lmarena/"&gt; &lt;img alt="Is anyone else noticing fewer updates on LMArena lately? The last updates are weeks apart" src="https://preview.redd.it/dual1zrnezyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87e6e9115f77cba0a5350009294a8d29a1b43720" title="Is anyone else noticing fewer updates on LMArena lately? The last updates are weeks apart" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThetaCursed"&gt; /u/ThetaCursed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dual1zrnezyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on43yj/is_anyone_else_noticing_fewer_updates_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on43yj/is_anyone_else_noticing_fewer_updates_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T05:58:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1omst7q</id>
    <title>Polish is the most effective language for prompting AI, study reveals</title>
    <updated>2025-11-02T21:02:40+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omst7q/polish_is_the_most_effective_language_for/"&gt; &lt;img alt="Polish is the most effective language for prompting AI, study reveals" src="https://external-preview.redd.it/HLkT8hEFTM_i4ECT9hWztFsoDf9RouYG6ZWJzAhQOUY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e82f2cea2a7637a951d648c1b316bc8d9248d9c" title="Polish is the most effective language for prompting AI, study reveals" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.euronews.com/next/2025/11/01/polish-to-be-the-most-effective-language-for-prompting-ai-new-study-reveals"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omst7q/polish_is_the_most_effective_language_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omst7q/polish_is_the_most_effective_language_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-02T21:02:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1omyytq</id>
    <title>Reporter: ‚ÄúPOLISH: THE SUPREME LANGUAGE OF AI.‚Äù</title>
    <updated>2025-11-03T01:31:02+00:00</updated>
    <author>
      <name>/u/Mindless_Pain1860</name>
      <uri>https://old.reddit.com/user/Mindless_Pain1860</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omyytq/reporter_polish_the_supreme_language_of_ai/"&gt; &lt;img alt="Reporter: ‚ÄúPOLISH: THE SUPREME LANGUAGE OF AI.‚Äù" src="https://preview.redd.it/jlwd6xkh3yyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ba1b50f272c2870a74364026d750bd194a9f243" title="Reporter: ‚ÄúPOLISH: THE SUPREME LANGUAGE OF AI.‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please read the paper before making any comments.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/pdf/2503.01996"&gt;https://arxiv.org/pdf/2503.01996&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mindless_Pain1860"&gt; /u/Mindless_Pain1860 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jlwd6xkh3yyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1omyytq/reporter_polish_the_supreme_language_of_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1omyytq/reporter_polish_the_supreme_language_of_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T01:31:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1on628o</id>
    <title>Google pulls Gemma from AI Studio after Senator Blackburn accuses model of defamation</title>
    <updated>2025-11-03T08:04:32+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"&gt; &lt;img alt="Google pulls Gemma from AI Studio after Senator Blackburn accuses model of defamation" src="https://b.thumbs.redditmedia.com/otiqqwWrYAPyBQSbvIHZ-yCKbfdiJZGic1vlE0jFFxk.jpg" title="Google pulls Gemma from AI Studio after Senator Blackburn accuses model of defamation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0hnvozwh10zf1.png?width=1198&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ab171458093a1ad5f07a0eaa42ac44e2c5ab5681"&gt;Google Official Statement&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://techcrunch.com/2025/11/02/google-pulls-gemma-from-ai-studio-after-senator-blackburn-accuses-model-of-defamation/"&gt;Source&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fortunately, we can still download the weights from HF and run them locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1on628o/google_pulls_gemma_from_ai_studio_after_senator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-03T08:04:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
</feed>
