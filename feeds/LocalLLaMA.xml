<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-17T10:06:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nirkn1</id>
    <title>Roo Code and Qwen3 Next is Not Impressive</title>
    <updated>2025-09-16T19:42:06+00:00</updated>
    <author>
      <name>/u/gamblingapocalypse</name>
      <uri>https://old.reddit.com/user/gamblingapocalypse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;I wanted to share my experience with the thinking and instruct versions of the new Qwen3 Next model. Both run impressively well on my computer, delivering fast and reasonably accurate responses outside the Roo code development environment.&lt;/p&gt; &lt;p&gt;However, their performance in the Roo code environment is less consistent. While both models handle tool calling effectively, the instruct model struggles with fixing issues, and the thinking model takes excessively long to process solutions, making other models like GLM Air more reliable in these cases.&lt;/p&gt; &lt;p&gt;Despite these challenges, I‚Äôm optimistic about the model‚Äôs potential, especially given its longer context window. I‚Äôm eager for the GGUF releases and believe increasing the active parameters could enhance accuracy.&lt;/p&gt; &lt;p&gt;Thanks for reading! I‚Äôd love to hear your thoughts. And if if you recommend another set of tools to use with Qwen3 Next other than roo, please do share.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gamblingapocalypse"&gt; /u/gamblingapocalypse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nirkn1/roo_code_and_qwen3_next_is_not_impressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nirkn1/roo_code_and_qwen3_next_is_not_impressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nirkn1/roo_code_and_qwen3_next_is_not_impressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T19:42:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj0p7r</id>
    <title>ArchGW 0.3.12 üöÄ Model aliases: allow clients to use friendly, semantic names and swap out underlying models without changing application code.</title>
    <updated>2025-09-17T02:04:53+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj0p7r/archgw_0312_model_aliases_allow_clients_to_use/"&gt; &lt;img alt="ArchGW 0.3.12 üöÄ Model aliases: allow clients to use friendly, semantic names and swap out underlying models without changing application code." src="https://preview.redd.it/y12ej5klrmpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49d0c97a765a892fb138b713ad332655f5cddfab" title="ArchGW 0.3.12 üöÄ Model aliases: allow clients to use friendly, semantic names and swap out underlying models without changing application code." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I added this lightweight abstraction to &lt;a href="https://github.com/katanemo/archgw"&gt;archgw&lt;/a&gt; to decouple app code from specific model names. Instead of sprinkling hardcoded model names like&lt;code&gt;gpt-4o-mini&lt;/code&gt; or &lt;code&gt;llama3.2&lt;/code&gt; everywhere, you point to an &lt;em&gt;alias&lt;/em&gt; that encodes intent, and allows you to test new models, swap out the config safely without having to do codewide search/replace every time you want to experiment with a new model or version.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;arch.summarize.v1 ‚Üí cheap/fast summarization arch.v1 ‚Üí default ‚Äúlatest‚Äù general-purpose model arch.reasoning.v1 ‚Üí heavier reasoning &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The app calls the alias, not the vendor. Swap the model in config, and the entire system updates without touching code. Of course, you would want to use models compatible. Meaning if you map an embedding model to an alias, when the application expects a chat model, it won't be a good day. &lt;/p&gt; &lt;p&gt;Where are we headed with this...&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Guardrails -&amp;gt; Apply safety, cost, or latency rules at the alias level: arch.reasoning.v1:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;arch.reasoning.v1: target: gpt-oss-120b guardrails: max_latency: 5s block_categories: [‚Äújailbreak‚Äù, ‚ÄúPII‚Äù] &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Fallbacks -&amp;gt; Provide a chain if a model fails or hits quota:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;arch.summarize.v1: target: gpt-4o-mini fallback: llama3.2 &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt; Traffic splitting &amp;amp; canaries -&amp;gt; Let an alias fan out traffic across multiple targets:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;arch.v1: targets: - model: llama3.2 weight: 80 - model: gpt-4o-mini weight: 20 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y12ej5klrmpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj0p7r/archgw_0312_model_aliases_allow_clients_to_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj0p7r/archgw_0312_model_aliases_allow_clients_to_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T02:04:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1niktfz</id>
    <title>VoxCPM-0.5B</title>
    <updated>2025-09-16T15:34:15+00:00</updated>
    <author>
      <name>/u/k-en</name>
      <uri>https://old.reddit.com/user/k-en</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niktfz/voxcpm05b/"&gt; &lt;img alt="VoxCPM-0.5B" src="https://external-preview.redd.it/r3qnehuYhIo41bAc9p8n4efqIezTbTJqzszutOT9598.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c21b8a7fb420d7443519db438480fdc9bd7c71a4" title="VoxCPM-0.5B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;VoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Supports both Regular text and Phoneme input. Seems promising!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k-en"&gt; /u/k-en &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/openbmb/VoxCPM-0.5B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niktfz/voxcpm05b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1niktfz/voxcpm05b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T15:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj626b</id>
    <title>M1 Ultra Mac Studio vs AMD Ryzen AI Max 395+ for local AI?</title>
    <updated>2025-09-17T06:54:18+00:00</updated>
    <author>
      <name>/u/doweig</name>
      <uri>https://old.reddit.com/user/doweig</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Looking at two options for a local AI sandbox:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Mac Studio M1 Ultra - 128GB RAM, 2TB SSD - $2500 (second hand, barely used)&lt;/li&gt; &lt;li&gt;AMD Ryzen AI Max 395+ (GMKtec mini pc) - 128GB RAM, 2TB SSD - $2000 (new)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Main use will be playing around with LLMs, image gen, maybe some video/audio stuff.&lt;/p&gt; &lt;p&gt;The M1 Ultra has way better memory bandwidth (800GB/s) which should help with LLMs, but I'm wondering if the AMD's RDNA 3.5 GPU might be better for other AI workloads? Also not sure about software support differences.&lt;/p&gt; &lt;p&gt;Anyone have experience with either for local AI? What would you pick?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doweig"&gt; /u/doweig &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj626b/m1_ultra_mac_studio_vs_amd_ryzen_ai_max_395_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj626b/m1_ultra_mac_studio_vs_amd_ryzen_ai_max_395_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj626b/m1_ultra_mac_studio_vs_amd_ryzen_ai_max_395_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T06:54:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1niysfm</id>
    <title>The best fine-tunable real time TTS</title>
    <updated>2025-09-17T00:36:14+00:00</updated>
    <author>
      <name>/u/AwkwardBoysenberry26</name>
      <uri>https://old.reddit.com/user/AwkwardBoysenberry26</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am searching a good open source TTS model to fine tune it on a specific voice dataset of 1 hour.I find that kokoro is good but I couldn‚Äôt find a documentation about it‚Äôs fine-tuning,also if the model supports non verbal expressions such as [laugh],[sigh],ect‚Ä¶ would be better (not a requirement).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AwkwardBoysenberry26"&gt; /u/AwkwardBoysenberry26 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niysfm/the_best_finetunable_real_time_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niysfm/the_best_finetunable_real_time_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1niysfm/the_best_finetunable_real_time_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T00:36:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nipox6</id>
    <title>Fine-tuning Small Language models/ qwen2.5 0.5 B</title>
    <updated>2025-09-16T18:33:01+00:00</updated>
    <author>
      <name>/u/Mysterious_Ad_3788</name>
      <uri>https://old.reddit.com/user/Mysterious_Ad_3788</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nipox6/finetuning_small_language_models_qwen25_05_b/"&gt; &lt;img alt="Fine-tuning Small Language models/ qwen2.5 0.5 B" src="https://preview.redd.it/hoplx2colkpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35a01423a78f194d98f7f162fddd9b55eec0fee6" title="Fine-tuning Small Language models/ qwen2.5 0.5 B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been up all week trying to fine-tune a small language model using Unsloth, and I've experimented with RAG. I generated around 1,500 domain-specific questions, but my LLM is still hallucinating. Below is a summary of my training setup and data distribution:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Epochs&lt;/strong&gt;: 20 (training stops around epoch 11)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Batch size&lt;/strong&gt;: 8&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Learning rate&lt;/strong&gt;: 1e-4&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Warmup ratio&lt;/strong&gt;: 0.5&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Max sequence length&lt;/strong&gt;: 4096&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoRA rank&lt;/strong&gt;: 32&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoRA alpha&lt;/strong&gt;: 16&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: Includes both positive and negative QA-style examples&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Despite this setup, hallucinations persist the model dont even know what it was finetuned on. Can anyone help me understand what I might be doing wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Ad_3788"&gt; /u/Mysterious_Ad_3788 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hoplx2colkpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nipox6/finetuning_small_language_models_qwen25_05_b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nipox6/finetuning_small_language_models_qwen25_05_b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T18:33:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj5n67</id>
    <title>Is anyone able to successfully run Qwen 30B Coder BF16?</title>
    <updated>2025-09-17T06:27:47+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With Llama.cpp and the Unsloth GGUFs for Qwen 3 30B Coder BF16, I am getting frequent crashes on two entirely different systems, a Ryzen AI Max, and a another sustem with an RTX 6000 Blackwell.&lt;/p&gt; &lt;p&gt;Llama.cpp just exits with no error message after a few messages. &lt;/p&gt; &lt;p&gt;VLLM works perfectly on the Blackwell with the official model from Qwen, except tool calling is currently broken, even with the new qwen 3 tool call parser which VLLM added. So the tool call instructions just end up in the chat stream, which makes the model unusable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj5n67/is_anyone_able_to_successfully_run_qwen_30b_coder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj5n67/is_anyone_able_to_successfully_run_qwen_30b_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj5n67/is_anyone_able_to_successfully_run_qwen_30b_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T06:27:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj758c</id>
    <title>Can I use Cursor Agent (or similar) with a local LLM setup (8B / 13B)?</title>
    <updated>2025-09-17T08:04:26+00:00</updated>
    <author>
      <name>/u/BudgetPurple3002</name>
      <uri>https://old.reddit.com/user/BudgetPurple3002</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I want to set up a local LLM (running 8B and possibly 13B parameter models). I was wondering if tools like Cursor Agent (or other AI coding agents) can work directly with my local setup, or if they require cloud-based APIs only.&lt;/p&gt; &lt;p&gt;Basically:&lt;/p&gt; &lt;p&gt;Is it possible to connect Cursor (or any similar coding agent) to a local model?&lt;/p&gt; &lt;p&gt;If not Cursor specifically, are there any good agent frameworks that can plug into local models for tasks like code generation and project automation?&lt;/p&gt; &lt;p&gt;Would appreciate any guidance from folks who‚Äôve tried this. üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BudgetPurple3002"&gt; /u/BudgetPurple3002 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj758c/can_i_use_cursor_agent_or_similar_with_a_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj758c/can_i_use_cursor_agent_or_similar_with_a_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj758c/can_i_use_cursor_agent_or_similar_with_a_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T08:04:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nijikb</id>
    <title>Inference will win ultimately</title>
    <updated>2025-09-16T14:46:07+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nijikb/inference_will_win_ultimately/"&gt; &lt;img alt="Inference will win ultimately" src="https://preview.redd.it/jp7ada3lhjpf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2651ab9359b4d75a0e7c1c55003fec8ea92f4fdb" title="Inference will win ultimately" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;inference is where the real value shows up. it‚Äôs where models are actually used at scale.&lt;/p&gt; &lt;p&gt;A few reasons why I think this is where the winners will be: ‚Ä¢Hardware is shifting. Morgan Stanley recently noted that more chips will be dedicated to inference than training in the years ahead. The market is already preparing for this transition. ‚Ä¢Open-source is exploding. Meta‚Äôs Llama models alone have crossed over a billion downloads. That‚Äôs a massive long tail of developers and companies who need efficient ways to serve all kinds of models. ‚Ä¢Agents mean real usage. Training is abstract , inference is what everyday people experience when they use agents, apps, and platforms. That‚Äôs where latency, cost, and availability matter. ‚Ä¢Inefficiency is the opportunity. Right now GPUs are underutilized, cold starts are painful, and costs are high. Whoever cracks this at scale , making inference efficient, reliable, and accessible , will capture enormous value.&lt;/p&gt; &lt;p&gt;In short, inference isn‚Äôt just a technical detail. It‚Äôs where AI meets reality. And that‚Äôs why inference will win.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jp7ada3lhjpf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nijikb/inference_will_win_ultimately/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nijikb/inference_will_win_ultimately/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T14:46:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1nipldx</id>
    <title>Ktransformers now supports qwen3-next</title>
    <updated>2025-09-16T18:29:30+00:00</updated>
    <author>
      <name>/u/Betadoggo_</name>
      <uri>https://old.reddit.com/user/Betadoggo_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nipldx/ktransformers_now_supports_qwen3next/"&gt; &lt;img alt="Ktransformers now supports qwen3-next" src="https://external-preview.redd.it/GCXHZq6UgvHr-07Ef7MzKApM7hyb5aZQRF1Wd5lmCZ0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45539a92e2924b07b2035937feb0f51a09d5cc5e" title="Ktransformers now supports qwen3-next" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This was a few days ago but I haven't seen it mentioned here so I figured I'd post it. They claim 6GB of vram usage with 320GB of system memory. Hopefully in the future the system memory requirements can be brought down if they support quantized variants.&lt;/p&gt; &lt;p&gt;I think this could be the ideal way to run it on low vram systems in the short term before llamacpp gets support.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Betadoggo_"&gt; /u/Betadoggo_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/Qwen3-Next.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nipldx/ktransformers_now_supports_qwen3next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nipldx/ktransformers_now_supports_qwen3next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T18:29:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj48gh</id>
    <title>embeddinggemma with Qdrant compatible uint8 tensors output</title>
    <updated>2025-09-17T05:04:49+00:00</updated>
    <author>
      <name>/u/terminoid_</name>
      <uri>https://old.reddit.com/user/terminoid_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hacked on the int8-sized community ONNX model of emnbeddinggemma to get it to output uint8 tensors which are compatible with Qdrant. For some reason it benchmarks higher than the base model on most of the NanoBEIR benchmarks.&lt;/p&gt; &lt;p&gt;benchmarks and info here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/electroglyph/embeddinggemma-300m-ONNX-uint8"&gt;https://huggingface.co/electroglyph/embeddinggemma-300m-ONNX-uint8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/terminoid_"&gt; /u/terminoid_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj48gh/embeddinggemma_with_qdrant_compatible_uint8/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj48gh/embeddinggemma_with_qdrant_compatible_uint8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj48gh/embeddinggemma_with_qdrant_compatible_uint8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T05:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj80eu</id>
    <title>Has anyone been able to use GLM 4.5 with the Github copilot extension in VSCode?</title>
    <updated>2025-09-17T09:02:13+00:00</updated>
    <author>
      <name>/u/Intelligent-Top3333</name>
      <uri>https://old.reddit.com/user/Intelligent-Top3333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I couldn't make it work, tried insiders too, I get this error:&lt;br /&gt; ```&lt;/p&gt; &lt;p&gt;Sorry, your request failed. Please try again. Request id: add5bf64-832a-4bd5-afd2-6ba10be9a734&lt;/p&gt; &lt;p&gt;Reason: Rate limit exceeded&lt;/p&gt; &lt;p&gt;{&amp;quot;code&amp;quot;:&amp;quot;1113&amp;quot;,&amp;quot;message&amp;quot;:&amp;quot;Insufficient balance or no resource package. Please recharge.&amp;quot;}&lt;br /&gt; ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Intelligent-Top3333"&gt; /u/Intelligent-Top3333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj80eu/has_anyone_been_able_to_use_glm_45_with_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj80eu/has_anyone_been_able_to_use_glm_45_with_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj80eu/has_anyone_been_able_to_use_glm_45_with_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T09:02:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj5wk4</id>
    <title>OpenAI usage breakdown released</title>
    <updated>2025-09-17T06:44:15+00:00</updated>
    <author>
      <name>/u/LeatherRub7248</name>
      <uri>https://old.reddit.com/user/LeatherRub7248</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj5wk4/openai_usage_breakdown_released/"&gt; &lt;img alt="OpenAI usage breakdown released" src="https://preview.redd.it/njcotg7i7opf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e00350509ba0edd32cc7dfb7341451356402cd8" title="OpenAI usage breakdown released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would have thought image generation would be higher... but this might be skewed by the fact that the 4o image (the whole ghibli craze) only came out in march 2025&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.nber.org/system/files/working_papers/w34255/w34255.pdf"&gt;https://www.nber.org/system/files/working_papers/w34255/w34255.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.nber.org/papers/w34255"&gt;https://www.nber.org/papers/w34255&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeatherRub7248"&gt; /u/LeatherRub7248 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/njcotg7i7opf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj5wk4/openai_usage_breakdown_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj5wk4/openai_usage_breakdown_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T06:44:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj8hee</id>
    <title>[Release] DASLab GGUF Non-Uniform Quantization Toolkit</title>
    <updated>2025-09-17T09:32:39+00:00</updated>
    <author>
      <name>/u/Loginhe</name>
      <uri>https://old.reddit.com/user/Loginhe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj8hee/release_daslab_gguf_nonuniform_quantization/"&gt; &lt;img alt="[Release] DASLab GGUF Non-Uniform Quantization Toolkit" src="https://a.thumbs.redditmedia.com/z81IhWllCbQrFrgfgKe5CFHfXTjYN85Bz1DiPDLtGE0.jpg" title="[Release] DASLab GGUF Non-Uniform Quantization Toolkit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're excited to release the &lt;strong&gt;first open-source toolkit&lt;/strong&gt; that brings &lt;strong&gt;GPTQ + EvoPress&lt;/strong&gt; to the &lt;strong&gt;GGUF format&lt;/strong&gt;, enabling &lt;em&gt;heterogeneous quantization&lt;/em&gt; based on importance.&lt;br /&gt; &lt;strong&gt;Delivering Higher-quality models, same file size.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;What's inside&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2210.17323"&gt;&lt;strong&gt;GPTQ (ICLR '23)&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;quantization with GGUF export:&lt;/strong&gt; delivers error-correcting calibration for improved performance&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2410.14649"&gt;&lt;strong&gt;EvoPress (ICML '25)&lt;/strong&gt;&lt;/a&gt;: runs evolutionary search to automatically discover optimal per-layer quantization configs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model assembly tools:&lt;/strong&gt; package models to be fully functional with llama.cpp&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why it matters&lt;/h1&gt; &lt;p&gt;Unlike standard uniform quantization, our toolkit &lt;strong&gt;optimizes precision where it matters most&lt;/strong&gt;.&lt;br /&gt; Critical layers (e.g. attention) can use higher precision, while others (e.g. FFN) compress more aggressively.&lt;br /&gt; With &lt;strong&gt;EvoPress search + GPTQ quantization&lt;/strong&gt;, these trade-offs are discovered automatically.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;p&gt;Below are zero-shot evaluations. Full benchmark results are available in the repo.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3eg7rp0vyopf1.png?width=3569&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6590f70e8abf59f3442df57321eaa55ea85ba9c"&gt;https://preview.redd.it/3eg7rp0vyopf1.png?width=3569&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6590f70e8abf59f3442df57321eaa55ea85ba9c&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Resources&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/IST-DASLab/gptq-gguf-toolkit"&gt;DASLab GGUF Quantization Toolkit (GitHub Repo Link)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are happy to get feedback, contributions, and experiments!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loginhe"&gt; /u/Loginhe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj8hee/release_daslab_gguf_nonuniform_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj8hee/release_daslab_gguf_nonuniform_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj8hee/release_daslab_gguf_nonuniform_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T09:32:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj673e</id>
    <title>STT ‚Äì&gt; LLM ‚Äì&gt; TTS pipeline in C</title>
    <updated>2025-09-17T07:02:35+00:00</updated>
    <author>
      <name>/u/rhinodevil</name>
      <uri>https://old.reddit.com/user/rhinodevil</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For &lt;strong&gt;S&lt;/strong&gt;peech-&lt;strong&gt;T&lt;/strong&gt;o-&lt;strong&gt;T&lt;/strong&gt;ext, &lt;strong&gt;L&lt;/strong&gt;arge-&lt;strong&gt;L&lt;/strong&gt;anguage-&lt;strong&gt;M&lt;/strong&gt;odel inference and &lt;strong&gt;T&lt;/strong&gt;ext-&lt;strong&gt;T&lt;/strong&gt;o-&lt;strong&gt;S&lt;/strong&gt;peech I created three wrapper libraries in C/C++ (using &lt;a href="https://github.com/ggml-org/whisper.cpp"&gt;Whisper.cpp&lt;/a&gt;, &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;Llama.cpp&lt;/a&gt; and &lt;a href="https://github.com/rhasspy/piper"&gt;Piper&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;They offer pure C interfaces, Windows and Linux are supported, meant to be used on standard consumer hardware.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/RhinoDevel/mt_stt"&gt;mt_stt&lt;/a&gt; for &lt;strong&gt;S&lt;/strong&gt;peech-&lt;strong&gt;T&lt;/strong&gt;o-&lt;strong&gt;T&lt;/strong&gt;ext.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/RhinoDevel/mt_llm"&gt;mt_llm&lt;/a&gt; for &lt;strong&gt;L&lt;/strong&gt;arge-&lt;strong&gt;L&lt;/strong&gt;anguage-&lt;strong&gt;M&lt;/strong&gt;odel inference.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/RhinoDevel/mt_tts"&gt;mt_tts&lt;/a&gt; for &lt;strong&gt;T&lt;/strong&gt;ext-&lt;strong&gt;T&lt;/strong&gt;o-&lt;strong&gt;S&lt;/strong&gt;peech.&lt;/p&gt; &lt;p&gt;An example implementation of an &lt;strong&gt;STT -&amp;gt; LLM -&amp;gt; TTS pipeline&lt;/strong&gt; in C can be found &lt;a href="https://github.com/RhinoDevel/mt_llm/tree/main/stt_llm_tts-pipeline-example"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rhinodevil"&gt; /u/rhinodevil &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj673e/stt_llm_tts_pipeline_in_c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj673e/stt_llm_tts_pipeline_in_c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj673e/stt_llm_tts_pipeline_in_c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T07:02:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nis417</id>
    <title>Alibaba Tongyi released open-source (Deep Research) Web Agent</title>
    <updated>2025-09-16T20:02:18+00:00</updated>
    <author>
      <name>/u/kahlil29</name>
      <uri>https://old.reddit.com/user/kahlil29</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging Face link to weights : &lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kahlil29"&gt; /u/kahlil29 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Ali_TongyiLab/status/1967988004179546451?s=19"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nis417/alibaba_tongyi_released_opensource_deep_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nis417/alibaba_tongyi_released_opensource_deep_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T20:02:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nis0za</id>
    <title>Alibaba-NLP/Tongyi-DeepResearch-30B-A3B ¬∑ Hugging Face</title>
    <updated>2025-09-16T19:59:13+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nis0za/alibabanlptongyideepresearch30ba3b_hugging_face/"&gt; &lt;img alt="Alibaba-NLP/Tongyi-DeepResearch-30B-A3B ¬∑ Hugging Face" src="https://external-preview.redd.it/Br8d0DO81Y2NXG6ObCzOPqMnemqzEFVfpKOIf-1Xb3Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f864231608ef7f1e9dcabddf98002a4ed64cb7df" title="Alibaba-NLP/Tongyi-DeepResearch-30B-A3B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nis0za/alibabanlptongyideepresearch30ba3b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nis0za/alibabanlptongyideepresearch30ba3b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T19:59:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj4axf</id>
    <title>Thread for CPU-only LLM performance comparison</title>
    <updated>2025-09-17T05:08:52+00:00</updated>
    <author>
      <name>/u/MLDataScientist</name>
      <uri>https://old.reddit.com/user/MLDataScientist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I could not find any recent posts about CPU only performance comparison of different CPUs. With recent advancements in CPUs, we are seeing incredible memory bandwidth speeds with DDR5 6400 12 channel EPYC 9005 (614.4 GB/s theoretical bw). AMD also announced that Zen 6 CPUs will have 1.6TB/s memory bw. The future of CPUs looks exciting. But for now, I wanted to test what we already have. I need your help to see where we stand with CPUs currently.&lt;/p&gt; &lt;p&gt;For this CPU only comparison, I want to use ik_llama - &lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt; . I compiled and tested both ik_llama and llama.cpp with MoE models like Qwen3 30B3A Q4_1, gpt-oss 120B Q8 and qwen3 235B Q4_1. ik_llama is at least 2x faster prompt processing (PP) and 50% faster in text generation (TG).&lt;/p&gt; &lt;p&gt;For this benchmark, I used Qwen3 30B3A Q4_1 (19.2GB) and ran ik_llama in Ubuntu 24.04.3.&lt;/p&gt; &lt;p&gt;ik_llama installation:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/ikawrakow/ik_llama.cpp.git cd ik_llama.cpp cmake -B build cmake --build build --config Release -j $(nproc) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;llama-bench benchmark (make sure GPUs are disabled with CUDA_VISIBLE_DEVICES=&amp;quot;&amp;quot; just in case if you compiled for GPUs):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=&amp;quot;&amp;quot; ./build/bin/llama-bench -m /media/ai-llm/wd_2t/models/Qwen3-30B-A3B-Q4_1.gguf -mmp 0 --threads 32 | model | size | params | backend | threads | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | ------: | ---: | ------------: | ---------------: | | qwen3moe ?B Q4_1 | 17.87 GiB | 30.53 B | CPU | 32 | 0 | pp512 | 263.02 ¬± 2.53 | | qwen3moe ?B Q4_1 | 17.87 GiB | 30.53 B | CPU | 32 | 0 | tg128 | 38.98 ¬± 0.16 | build: 6d2e7ca4 (3884) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GPT-OSS 120B:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=&amp;quot;&amp;quot; ./build/bin/llama-bench -m /media/ai-llm/wd_2t/models/GPT_OSS_120B_UD-Q8_K_XL/gpt-oss-120b-UD-Q8_K_XL-00001-of-00002.gguf -mmp 0 --threads 32 | model | size | params | backend | threads | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | ------: | ---: | ------------: | ---------------: | | gpt-oss ?B Q8_0 | 60.03 GiB | 116.83 B | CPU | 32 | 0 | pp512 | 163.24 ¬± 4.46 | | gpt-oss ?B Q8_0 | 60.03 GiB | 116.83 B | CPU | 32 | 0 | tg128 | 24.77 ¬± 0.42 | build: 6d2e7ca4 (3884) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So, the requirement for this benchmark is simple:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Required: list your MB, CPU, RAM size, type and channels.&lt;/li&gt; &lt;li&gt;Required: use CPU only inference (No APUs, NPUs, or build-in GPUs allowed)&lt;/li&gt; &lt;li&gt;use ik-llama (any recent version) if possible since llama.cpp will be slower for your CPU performance&lt;/li&gt; &lt;li&gt;Required model: ( &lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/blob/main/Qwen3-30B-A3B-Q4_1.gguf"&gt;https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/blob/main/Qwen3-30B-A3B-Q4_1.gguf&lt;/a&gt; ) Run the standard llama-bench benchmark with Qwen3-30B-A3B-Q4_1.gguf (2703 version should also be fine as long as it is Q4_1) and share the command with output in the comments as I shared above.&lt;/li&gt; &lt;li&gt;Optional (not required but good to have): run CPU only benchmark with GPT-OSS 120B (file here: &lt;a href="https://huggingface.co/unsloth/gpt-oss-120b-GGUF/tree/main/UD-Q8%5C_K%5C_XL"&gt;https://huggingface.co/unsloth/gpt-oss-120b-GGUF/tree/main/UD-Q8\_K\_XL&lt;/a&gt;) and share the command with output in the comments.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I will start by adding my CPU performance in this table below.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Motherboard&lt;/th&gt; &lt;th align="left"&gt;CPU (physical cores)&lt;/th&gt; &lt;th align="left"&gt;RAM size and type&lt;/th&gt; &lt;th align="left"&gt;channels&lt;/th&gt; &lt;th align="left"&gt;Qwen3 30B3A Q4_1 TG&lt;/th&gt; &lt;th align="left"&gt;Qwen3 30B3A Q4_1 PP&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;AsRock ROMED8-2T&lt;/td&gt; &lt;td align="left"&gt;AMD EPYC 7532 (32 cores)&lt;/td&gt; &lt;td align="left"&gt;8x32GB DDR4 3200Mhz&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;39.98&lt;/td&gt; &lt;td align="left"&gt;263.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I will check comments daily and keep updating the table.&lt;/p&gt; &lt;p&gt;This awesome community is the best place to collect such performance metrics.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLDataScientist"&gt; /u/MLDataScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj4axf/thread_for_cpuonly_llm_performance_comparison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj4axf/thread_for_cpuonly_llm_performance_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj4axf/thread_for_cpuonly_llm_performance_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T05:08:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj7pik</id>
    <title>support for the upcoming Olmo3 model has been merged into llama.cpp</title>
    <updated>2025-09-17T08:42:16+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7pik/support_for_the_upcoming_olmo3_model_has_been/"&gt; &lt;img alt="support for the upcoming Olmo3 model has been merged into llama.cpp" src="https://external-preview.redd.it/11Q8uZ2-M8bnIL12n-O39P2wooNtmpfM5ORG4VqYvik.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fab39b6ac711c07bf4835557388faf67e2bdb807" title="support for the upcoming Olmo3 model has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16015"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7pik/support_for_the_upcoming_olmo3_model_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7pik/support_for_the_upcoming_olmo3_model_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T08:42:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nit4v6</id>
    <title>Granite 4 release today? Collection updated with 8 private repos.</title>
    <updated>2025-09-16T20:40:36+00:00</updated>
    <author>
      <name>/u/ironwroth</name>
      <uri>https://old.reddit.com/user/ironwroth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nit4v6/granite_4_release_today_collection_updated_with_8/"&gt; &lt;img alt="Granite 4 release today? Collection updated with 8 private repos." src="https://preview.redd.it/ihwp4dy78lpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=310d508b27499694f225a40decad5893a979dfda" title="Granite 4 release today? Collection updated with 8 private repos." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ironwroth"&gt; /u/ironwroth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ihwp4dy78lpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nit4v6/granite_4_release_today_collection_updated_with_8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nit4v6/granite_4_release_today_collection_updated_with_8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T20:40:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1nivz2n</id>
    <title>We got a 2B param model running on iPhone at ~500MB RAM ‚Äî fully offline demo</title>
    <updated>2025-09-16T22:32:32+00:00</updated>
    <author>
      <name>/u/Josiahhenryus</name>
      <uri>https://old.reddit.com/user/Josiahhenryus</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nivz2n/we_got_a_2b_param_model_running_on_iphone_at/"&gt; &lt;img alt="We got a 2B param model running on iPhone at ~500MB RAM ‚Äî fully offline demo" src="https://external-preview.redd.it/ZDZxemk3OWFzbHBmMVMFq2pfv69EmnrpZl789HXOOBvSofKD3EML3NWxX5eD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=517811a91d95e57b2a6e0b44c23976628615830f" title="We got a 2B param model running on iPhone at ~500MB RAM ‚Äî fully offline demo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ongoing research out of Derive DX Labs in Lafayette, Louisiana. We‚Äôve been experimenting with efficiency optimizations and managed to get a 2B parameter chain-of-thought model running on iPhone with ~400‚Äì500MB RAM, fully offline.&lt;/p&gt; &lt;p&gt;I‚Äôm not super active on Reddit, so please don‚Äôt kill me if I‚Äôm slow to respond to comments ‚Äî but I‚Äôll do my best to answer questions.&lt;/p&gt; &lt;p&gt;[Correction: Meant Gemma-3N not Gemini-3B]&lt;/p&gt; &lt;p&gt;[Update on memory measurement: After running with Instruments, the total unified memory footprint is closer to ~2 GB (CPU + GPU) during inference, not just the 400‚Äì500 MB reported earlier. The earlier number reflected only CPU-side allocations. Still a big step down compared to the usual multi-GB requirements for 2B+ models.]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Josiahhenryus"&gt; /u/Josiahhenryus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6rczu79aslpf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nivz2n/we_got_a_2b_param_model_running_on_iphone_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nivz2n/we_got_a_2b_param_model_running_on_iphone_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T22:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1niwb8l</id>
    <title>500,000 public datasets on Hugging Face</title>
    <updated>2025-09-16T22:46:45+00:00</updated>
    <author>
      <name>/u/clem59480</name>
      <uri>https://old.reddit.com/user/clem59480</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niwb8l/500000_public_datasets_on_hugging_face/"&gt; &lt;img alt="500,000 public datasets on Hugging Face" src="https://preview.redd.it/rokftav6vlpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=26f96c62b0cfcf4ab8d9a212645ed0b0f54e16e2" title="500,000 public datasets on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clem59480"&gt; /u/clem59480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rokftav6vlpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niwb8l/500000_public_datasets_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1niwb8l/500000_public_datasets_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T22:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nifajh</id>
    <title>I bought a modded 4090 48GB in Shenzhen. This is my story.</title>
    <updated>2025-09-16T11:52:20+00:00</updated>
    <author>
      <name>/u/king_priam_of_Troy</name>
      <uri>https://old.reddit.com/user/king_priam_of_Troy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt; &lt;img alt="I bought a modded 4090 48GB in Shenzhen. This is my story." src="https://external-preview.redd.it/1vD_R63iqu4vnM_qQf7pZNwXb9dy_UDc_Gl2j3LnTpU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5102c5612db16c04c26877a1e72e86700648e25" title="I bought a modded 4090 48GB in Shenzhen. This is my story." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ume4fe3jmipf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9aa908d45211be937b291377b1c495c9917834fe"&gt;https://preview.redd.it/ume4fe3jmipf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9aa908d45211be937b291377b1c495c9917834fe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few years ago, before ChatGPT became popular, I managed to score a Tesla P40 on eBay for around $150 shipped. With a few tweaks, I installed it in a Supermicro chassis. At the time, I was mostly working on video compression and simulation. It worked, but the card consistently climbed to 85¬∞C.&lt;/p&gt; &lt;p&gt;When DeepSeek was released, I was impressed and installed Ollama in a container. With 24GB of VRAM, it worked‚Äîbut slowly. After trying Stable Diffusion, it became clear that an upgrade was necessary.&lt;/p&gt; &lt;p&gt;The main issue was finding a modern GPU that could actually &lt;strong&gt;fit&lt;/strong&gt; in the server chassis. Standard 4090/5090 cards are designed for desktops: they're too large, and the power plug is inconveniently placed on top. After watching the LTT video featuring a modded 4090 with 48GB (and a follow-up from Gamers Nexus), I started searching the only place I knew might have one: Alibaba.com.&lt;/p&gt; &lt;p&gt;I contacted a seller and got a quote: &lt;strong&gt;CNY 22,900&lt;/strong&gt;. Pricey, but cheaper than expected. However, Alibaba enforces VAT collection, and I‚Äôve had bad experiences with DHL‚Äîthere was a non-zero chance I‚Äôd be charged twice for taxes. I was already over ‚Ç¨700 in taxes and fees.&lt;/p&gt; &lt;p&gt;Just for fun, I checked &lt;a href="http://Trip.com"&gt;Trip.com&lt;/a&gt; and realized that for the same amount of money, I could fly to Hong Kong and back, with a few days to explore. After confirming with the seller that they‚Äôd meet me at their business location, I booked a flight and an Airbnb in Hong Kong.&lt;/p&gt; &lt;p&gt;For context, I don‚Äôt speak Chinese at all. Finding the place using a Chinese address was tricky. Google Maps is useless in China, Apple Maps gave some clues, and Baidu Maps was beyond my skill level. With a little help from DeepSeek, I decoded the address and located the place in an industrial estate outside the city center. Thanks to Shenzhen‚Äôs extensive metro network, I didn‚Äôt need a taxi.&lt;/p&gt; &lt;p&gt;After arriving, the manager congratulated me for being the first foreigner to find them unassisted. I was given the card from a large batch‚Äîthey‚Äôre clearly producing these in volume at a factory elsewhere in town (I was proudly shown videos of the assembly line). I asked them to retest the card so I could verify its authenticity.&lt;/p&gt; &lt;p&gt;During the office tour, it was clear that their next frontier is repurposing old mining cards. I saw a large collection of NVIDIA Ampere mining GPUs. I was also told that modded 5090s with over 96GB of VRAM are in development.&lt;/p&gt; &lt;p&gt;After the test was completed, I paid in cash (a &lt;em&gt;lot&lt;/em&gt; of banknotes!) and returned to Hong Kong with my new purchase.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/king_priam_of_Troy"&gt; /u/king_priam_of_Troy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T11:52:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj7mbu</id>
    <title>Big AI pushes the "we need to beat China" narrative cuz they want fat government contracts and zero democratic oversight. It's an old trick. Fear sells.</title>
    <updated>2025-09-17T08:36:32+00:00</updated>
    <author>
      <name>/u/katxwoods</name>
      <uri>https://old.reddit.com/user/katxwoods</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Throughout the Cold War, the military-industrial complex spent a fortune pushing the false narrative that the Soviet military was far more advanced than they actually were.&lt;/p&gt; &lt;p&gt;Why? To ensure the money from Congress kept flowing.&lt;/p&gt; &lt;p&gt;They lied‚Ä¶ and lied‚Ä¶ and lied again to get bigger and bigger defense contracts.&lt;/p&gt; &lt;p&gt;Now, obviously, there is &lt;em&gt;some&lt;/em&gt; amount of competition between the US and China, but &lt;strong&gt;Big Tech is stoking the flames beyond what is reasonable to terrify Congress into giving them whatever they want.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;What they want is fat government contracts and zero democratic oversight. Day after day we hear about another big AI company announcing a giant contract with the Department of Defense.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/katxwoods"&gt; /u/katxwoods &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7mbu/big_ai_pushes_the_we_need_to_beat_china_narrative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7mbu/big_ai_pushes_the_we_need_to_beat_china_narrative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7mbu/big_ai_pushes_the_we_need_to_beat_china_narrative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T08:36:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nixynv</id>
    <title>The Qwen of Pain.</title>
    <updated>2025-09-16T23:58:16+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nixynv/the_qwen_of_pain/"&gt; &lt;img alt="The Qwen of Pain." src="https://preview.redd.it/0px1banw6mpf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8edc833e57220e0c00a8b11ba32c881974742ef1" title="The Qwen of Pain." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0px1banw6mpf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nixynv/the_qwen_of_pain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nixynv/the_qwen_of_pain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T23:58:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
