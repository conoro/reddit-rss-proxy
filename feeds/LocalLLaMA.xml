<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-14T07:18:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r4e3w3</id>
    <title>KaniTTS2, our text-to-speech model with frame-level position encodings, optimized for real-time conversational AI.</title>
    <updated>2026-02-14T07:10:30+00:00</updated>
    <author>
      <name>/u/KokaOP</name>
      <uri>https://old.reddit.com/user/KokaOP</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're excited to release KaniTTS2, our text-to-speech model with frame-level position encodings, optimized for real-time conversational AI.&lt;/p&gt; &lt;p&gt;What's in the release:&lt;/p&gt; &lt;p&gt;Pretrained Model (multilingual â€” English, Spanish, Kyrgyz)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-2-pt"&gt;https://huggingface.co/nineninesix/kani-tts-2-pt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ğŸ“Œ Currently supports 3 languages, with more being added over time. Stay tuned for updates as we expand language coverage.&lt;/p&gt; &lt;p&gt;ğŸ‡¬ğŸ‡§ English-specific Model&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-2-en"&gt;https://huggingface.co/nineninesix/kani-tts-2-en&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ğŸ› ï¸ Full Pretraining Code â€” train your own TTS model from scratch&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/nineninesix-ai/kani-tts-2-pretrain"&gt;https://github.com/nineninesix-ai/kani-tts-2-pretrain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;p&gt;400M parameter model built on LiquidAI's LFM2 backbone + Nvidia NanoCodec&lt;/p&gt; &lt;p&gt;~0.2 RTF on an RTX 5080, 3GB VRAM â€” fast enough for real-time use&lt;/p&gt; &lt;p&gt;Voice cloning with speaker embeddings&lt;/p&gt; &lt;p&gt;Pretrained on ~10k hours of speech data (8x H100s, just 6 hours of training!)&lt;/p&gt; &lt;p&gt;Why we're releasing the pretrain code: We want anyone to be able to train a TTS model for their own language, accent, or domain from scratch. The framework includes FSDP multi-GPU training, Flash Attention 2, YAML-driven configs, and built-in attention analysis metrics to validate layer isolation. Everything you need to go from dataset to deployed model.&lt;/p&gt; &lt;p&gt;Licensed Apache 2.0. Try the demos on our HF Spaces, and come chat with us on Discord if you have questions or want to contribute.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KokaOP"&gt; /u/KokaOP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4e3w3/kanitts2_our_texttospeech_model_with_framelevel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4e3w3/kanitts2_our_texttospeech_model_with_framelevel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4e3w3/kanitts2_our_texttospeech_model_with_framelevel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T07:10:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r45700</id>
    <title>I've been building a P2P protocol for distributed 1-bit inference on CPU. Here are the real benchmarks across AMD and Intel.</title>
    <updated>2026-02-13T23:52:30+00:00</updated>
    <author>
      <name>/u/EiwazDeath</name>
      <uri>https://old.reddit.com/user/EiwazDeath</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been working on this for a while now and figured it was time to share it properly. ARIA Protocol is a peer-to-peer protocol for distributed AI inference using 1-bit quantized models (BitNet b1.58 from Microsoft Research). The whole idea is that ternary weights (-1, 0, +1) turn matrix multiplications into additions and subtractions, so you can run inference on any CPU without a GPU.&lt;/p&gt; &lt;p&gt;I've benchmarked it on two machines and the results were interesting, especially the cross-architecture comparison.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;AMD Ryzen 9 7845HX&lt;/strong&gt; (DDR5-5600, 8 threads):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;0.7B: 89.65 tok/s&lt;/li&gt; &lt;li&gt;2.4B: 36.94 tok/s&lt;/li&gt; &lt;li&gt;8.0B: 15.03 tok/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Intel i7-11370H Tiger Lake&lt;/strong&gt; (DDR4, 8 threads):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;0.7B: 62 tok/s&lt;/li&gt; &lt;li&gt;2.4B: 77 tok/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Yeah, the Intel is faster on the 2.4B model. Tiger Lake has native 512-bit AVX-512 which gives it a massive advantage for ternary operations. The AMD has more memory bandwidth (DDR5) which helps on the smaller model, but the AVX-512 wins on the larger one. This confirms that 1-bit inference is compute-bound on the lookup table operations, not just memory-bound like standard quantization.&lt;/p&gt; &lt;p&gt;Energy wise, we're looking at roughly 11 to 66 mJ per token depending on the model. That's estimated via CPU-time x TDP, not direct RAPL measurement, so take it as an upper bound. But it's still a 70-82% reduction compared to GPU inference for equivalent models.&lt;/p&gt; &lt;p&gt;The protocol side has P2P networking with pipeline parallelism (shard model layers across nodes), a provenance ledger that logs every inference, consent contracts so nodes control exactly what resources they share, and an OpenAI-compatible API so you can use it as a drop-in replacement.&lt;/p&gt; &lt;p&gt;One thing I've been spending a lot of time on is the memory architecture. Most AI agent memory is just vector search for past conversations. I went deeper and implemented a full cognitive memory model based on the psychology literature: episodic, semantic, procedural, working, and prospective memory. The prospective part is the one nobody else does. It's the &amp;quot;remember to bring this up when we talk about topic X next time&amp;quot; type of memory, with a dual-pathway trigger system inspired by Einstein and McDaniel's Multiprocess Framework. If anyone here is building agent systems, I'd love to hear how you're handling deferred intentions.&lt;/p&gt; &lt;p&gt;MIT licensed, Python 3.10+, 196 tests passing. Desktop app with Tauri/Electron for Windows/macOS/Linux.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/spmfrance-cloud/aria-protocol"&gt;https://github.com/spmfrance-cloud/aria-protocol&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Website: &lt;a href="https://spmfrance-cloud.github.io/aria-protocol"&gt;https://spmfrance-cloud.github.io/aria-protocol&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions. If you've compiled bitnet.cpp you can run the benchmarks yourself in a couple of minutes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EiwazDeath"&gt; /u/EiwazDeath &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r45700/ive_been_building_a_p2p_protocol_for_distributed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r45700/ive_been_building_a_p2p_protocol_for_distributed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r45700/ive_been_building_a_p2p_protocol_for_distributed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T23:52:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3r3bt</id>
    <title>Make a SVG of a Pelican riding a bicycle - Small MoE edition.</title>
    <updated>2026-02-13T14:48:21+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3r3bt/make_a_svg_of_a_pelican_riding_a_bicycle_small/"&gt; &lt;img alt="Make a SVG of a Pelican riding a bicycle - Small MoE edition." src="https://preview.redd.it/hrah141zm9jg1.png?width=140&amp;amp;height=65&amp;amp;auto=webp&amp;amp;s=1be312b3eb2454d314cc7c1a60371b3afb3f0f8d" title="Make a SVG of a Pelican riding a bicycle - Small MoE edition." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r3r3bt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3r3bt/make_a_svg_of_a_pelican_riding_a_bicycle_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3r3bt/make_a_svg_of_a_pelican_riding_a_bicycle_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T14:48:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3toe5</id>
    <title>MiniMax 2.5 full precision FP8 running LOCALLY on vLLM x 8x Pro 6000</title>
    <updated>2026-02-13T16:25:08+00:00</updated>
    <author>
      <name>/u/cyysky</name>
      <uri>https://old.reddit.com/user/cyysky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe5/minimax_25_full_precision_fp8_running_locally_on/"&gt; &lt;img alt="MiniMax 2.5 full precision FP8 running LOCALLY on vLLM x 8x Pro 6000" src="https://preview.redd.it/o66j8wb57ajg1.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=470df23fa8a32ba9ccb885ddda7d09d8607191ca" title="MiniMax 2.5 full precision FP8 running LOCALLY on vLLM x 8x Pro 6000" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MiniMax 2.5 full precision FP8 running LOCALLY on vLLM x 8x Pro 6000 &lt;/p&gt; &lt;p&gt;Hosting it is easier then I thought, it just reuse the same script for M2.1.&lt;br /&gt; Time to do the vibe coding test! &lt;/p&gt; &lt;p&gt;Generation: 70 tokens-per-sec and 122 tokens-per-sec for two conneciton&lt;br /&gt; Peak Memory: 728GB&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o66j8wb57ajg1.png?width=1916&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddf90e73b3792510afd31f58604a8ccd0ab18246"&gt;https://preview.redd.it/o66j8wb57ajg1.png?width=1916&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddf90e73b3792510afd31f58604a8ccd0ab18246&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/99vp2ub57ajg1.png?width=845&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=40fe8e0b643735c6fc10b5d6e47bb5fa279b45f2"&gt;https://preview.redd.it/99vp2ub57ajg1.png?width=845&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=40fe8e0b643735c6fc10b5d6e47bb5fa279b45f2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cyysky"&gt; /u/cyysky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe5/minimax_25_full_precision_fp8_running_locally_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe5/minimax_25_full_precision_fp8_running_locally_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe5/minimax_25_full_precision_fp8_running_locally_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:25:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3hlfq</id>
    <title>UG student launches Dhi-5B (Trained from Scratch)</title>
    <updated>2026-02-13T06:13:29+00:00</updated>
    <author>
      <name>/u/gradNorm</name>
      <uri>https://old.reddit.com/user/gradNorm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3hlfq/ug_student_launches_dhi5b_trained_from_scratch/"&gt; &lt;img alt="UG student launches Dhi-5B (Trained from Scratch)" src="https://preview.redd.it/5tsgquvue7jg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fac59fbf4b00df28aabae2f993f4d65bb88169c" title="UG student launches Dhi-5B (Trained from Scratch)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hii everyone,&lt;/p&gt; &lt;p&gt;I present Dhi-5B: A 5 billion parameter Multimodal Language Model trained compute optimally with just â‚¹1.1 lakh ($1200).&lt;/p&gt; &lt;p&gt;I incorporate the latest architecture design and training methodologies in this. And I also use a custom built codebase for training these models.&lt;/p&gt; &lt;p&gt;I train the Dhi-5B in 5 stages:-&lt;/p&gt; &lt;p&gt;ğŸ“š Pre-Training: The most compute heavy phase, where the core is built. (Gives the Base varient.)&lt;/p&gt; &lt;p&gt;ğŸ“œ Context-Length-Extension: The model learns to handle 16k context from the 4k learned during PT.&lt;/p&gt; &lt;p&gt;ğŸ“– Mid-Training: Annealing on very high quality datasets.&lt;/p&gt; &lt;p&gt;ğŸ’¬ Supervised-Fine-Tuning: Model learns to handle conversations. (Gives the Instruct model.)&lt;/p&gt; &lt;p&gt;ğŸ‘€ Vision-Extension: The model learns to see. (Results in The Dhi-5B.)&lt;/p&gt; &lt;p&gt;I'll be dropping it in 3 phases:-&lt;/p&gt; &lt;p&gt;i. Dhi-5B-Base (available now)&lt;/p&gt; &lt;p&gt;ii. Dhi-5B-Instruct (coming soon)&lt;/p&gt; &lt;p&gt;iii. The Dhi-5B (coming soon)&lt;/p&gt; &lt;p&gt;Some details about the Dhi-5B-Base model:-&lt;/p&gt; &lt;p&gt;The base varient is of 4 billion parameters. It is trained on 40 billion natural language tokens mostly in english from FineWeb-Edu dataset.&lt;/p&gt; &lt;p&gt;I use the new Muon optimizer for optimising the Matrix Layers, and rest are optimized by AdamW.&lt;/p&gt; &lt;p&gt;The model has 32 layers, with 3072 width, SwiGLU MLPs, the full MHA attention with FlashAttention-3, 4096 context length, 64k vocab and 2 million batch size during training.&lt;/p&gt; &lt;p&gt;Attached are some evaluations of the base model, the compared models are about 10x more expensive than ours.&lt;/p&gt; &lt;p&gt;Thank you, everyone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gradNorm"&gt; /u/gradNorm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5tsgquvue7jg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3hlfq/ug_student_launches_dhi5b_trained_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3hlfq/ug_student_launches_dhi5b_trained_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T06:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3toe1</id>
    <title>Minimax-M2.5 at same level of GLM-4.7 and DeepSeek-3.2</title>
    <updated>2026-02-13T16:25:08+00:00</updated>
    <author>
      <name>/u/Rascazzione</name>
      <uri>https://old.reddit.com/user/Rascazzione</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe1/minimaxm25_at_same_level_of_glm47_and_deepseek32/"&gt; &lt;img alt="Minimax-M2.5 at same level of GLM-4.7 and DeepSeek-3.2" src="https://preview.redd.it/ps0fnwi7fajg1.png?width=140&amp;amp;height=38&amp;amp;auto=webp&amp;amp;s=e14c464f298d56218853942eb3cb6fdf095ae035" title="Minimax-M2.5 at same level of GLM-4.7 and DeepSeek-3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ps0fnwi7fajg1.png?width=1462&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1209b5ed071f67d465b5ab243fcbc309a676c17"&gt;Coding Index 13/02/2026 Artificial Analisys&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fepkt4hffajg1.png?width=1468&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c457992a63fd80a590b2c3296b1ce95843c7f8f8"&gt;General Index Intelligence 13/02/2026 Artificial Analisys&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Seems Minimax-M2.5 is on par with GLM-4.7 and DeepSeek-3.2, let's see if the Agent capabilities makes differences. &lt;/p&gt; &lt;p&gt;Stats from &lt;a href="https://artificialanalysis.ai/"&gt;https://artificialanalysis.ai/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rascazzione"&gt; /u/Rascazzione &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe1/minimaxm25_at_same_level_of_glm47_and_deepseek32/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe1/minimaxm25_at_same_level_of_glm47_and_deepseek32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3toe1/minimaxm25_at_same_level_of_glm47_and_deepseek32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:25:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3kzce</id>
    <title>MiniMax-M2.5 Checkpoints on huggingface will be in 8 hours</title>
    <updated>2026-02-13T09:41:01+00:00</updated>
    <author>
      <name>/u/Own_Forever_5997</name>
      <uri>https://old.reddit.com/user/Own_Forever_5997</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzce/minimaxm25_checkpoints_on_huggingface_will_be_in/"&gt; &lt;img alt="MiniMax-M2.5 Checkpoints on huggingface will be in 8 hours" src="https://preview.redd.it/p94fz9gsf8jg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=920f76b1a80dd8b1b58e34745f143966274a40a4" title="MiniMax-M2.5 Checkpoints on huggingface will be in 8 hours" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own_Forever_5997"&gt; /u/Own_Forever_5997 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p94fz9gsf8jg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzce/minimaxm25_checkpoints_on_huggingface_will_be_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3kzce/minimaxm25_checkpoints_on_huggingface_will_be_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T09:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1r42r9b</id>
    <title>I gave my on-device LLM 3% English data. It decided to be better at English than main language.</title>
    <updated>2026-02-13T22:09:37+00:00</updated>
    <author>
      <name>/u/shoonee_balavolka</name>
      <uri>https://old.reddit.com/user/shoonee_balavolka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r42r9b/i_gave_my_ondevice_llm_3_english_data_it_decided/"&gt; &lt;img alt="I gave my on-device LLM 3% English data. It decided to be better at English than main language." src="https://preview.redd.it/wo8sb8vi5cjg1.jpg?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=a694a92b630b753078592a9bc17afb1b5591f052" title="I gave my on-device LLM 3% English data. It decided to be better at English than main language." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/wo8sb8vi5cjg1.jpg?width=1856&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ffb852d59eec38cf022616fe150f55ca43f91c88"&gt;https://preview.redd.it/wo8sb8vi5cjg1.jpg?width=1856&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ffb852d59eec38cf022616fe150f55ca43f91c88&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Iâ€™ve been messing around with Gemma 3 270M lately, and Iâ€™ve run into the most hilarious reality check.&lt;/p&gt; &lt;p&gt;Since Iâ€™m based in Korea, I spent weeks obsessing over a fine-tuning dataset that was 97% Korean. I really tried to bake in every possible nuance and emotional expression. I threw in a tiny 3% of English data just so it wouldnâ€™t be totally lost in translationâ€”I honestly didn't expect much at all.&lt;/p&gt; &lt;p&gt;But hereâ€™s the twist: The Korean sideâ€”the part I actually put my blood, sweat, and tears intoâ€”is still a bit of a wild card and gives random or off-topic responses sometimes. Meanwhile, the 3% English data is pumping out relatively clean and coherent replies!&lt;/p&gt; &lt;p&gt;Itâ€™s pretty humbling (and a bit frustrating!) to see my &amp;quot;low-effort&amp;quot; English support behaving better than the language I actually focused on. I guess the base modelâ€™s pre-training is doing some heavy lifting here, but it definitely means Iâ€™ve still got some work to do on the Korean side!&lt;/p&gt; &lt;p&gt;Just for some context on the screenshot, Iâ€™m actually building an on-device diary app called Offgram. The idea is to have a locally running LLM act as a companion that leaves thoughtful (and hopefully not too random) comments on your daily entries so you don't feel like you're just writing into a void.&lt;/p&gt; &lt;p&gt;Since it's a diary, I'm a firm believer that privacy is non-negotiable, so everything runs 100% on-deviceâ€”zero data ever leaves your phone. Using the tiny 270M model keeps things super snappy with basically no latency. Itâ€™s still under heavy development, but Iâ€™m planning to launch it soon!&lt;/p&gt; &lt;p&gt;Has anyone else working with these ultra-small models seen this kind of &amp;quot;language flip&amp;quot;? Iâ€™d love to hear your theories or any tips on how to keep these tiny models on track!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shoonee_balavolka"&gt; /u/shoonee_balavolka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r42r9b/i_gave_my_ondevice_llm_3_english_data_it_decided/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r42r9b/i_gave_my_ondevice_llm_3_english_data_it_decided/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r42r9b/i_gave_my_ondevice_llm_3_english_data_it_decided/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T22:09:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3o6je</id>
    <title>New DeepSeek update: "DeepSeek Web / APP is currently testing a new long-context model architecture, supporting a 1M context window."</title>
    <updated>2026-02-13T12:43:50+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3o6je/new_deepseek_update_deepseek_web_app_is_currently/"&gt; &lt;img alt="New DeepSeek update: &amp;quot;DeepSeek Web / APP is currently testing a new long-context model architecture, supporting a 1M context window.&amp;quot;" src="https://preview.redd.it/dg94ujw1c9jg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55de58cf8a3e4a397d81184a2473b94f7a31aa33" title="New DeepSeek update: &amp;quot;DeepSeek Web / APP is currently testing a new long-context model architecture, supporting a 1M context window.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From AiBattle on ğ•: &lt;a href="https://x.com/AiBattle_/status/2022280288643039235"&gt;https://x.com/AiBattle_/status/2022280288643039235&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dg94ujw1c9jg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3o6je/new_deepseek_update_deepseek_web_app_is_currently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3o6je/new_deepseek_update_deepseek_web_app_is_currently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T12:43:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r45hkh</id>
    <title>Running GLM-4.7 on an old AMD GPU</title>
    <updated>2026-02-14T00:05:04+00:00</updated>
    <author>
      <name>/u/Begetan</name>
      <uri>https://old.reddit.com/user/Begetan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know I am a bit late to GLM-4.7 party, but as a poor unlucky AMD GPU owner I was late to buy a good Nvidia videocard, so I got AMD RX6900XT with 16GB RAM because miners did not want it for their rigs.&lt;/p&gt; &lt;p&gt;I was inspired by other post about running GLM-4.7 model on a baseline hardware and I believe we need to share a successful working configuration to help other people and new models to make decisions.&lt;/p&gt; &lt;h1&gt;My config&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;GPU: AMD RX6900XT 16GB&lt;/li&gt; &lt;li&gt;CPU: Intel i9-10900k&lt;/li&gt; &lt;li&gt;RAM: DDR4 3200 32GB&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;My llama.cpp build&lt;/h1&gt; &lt;p&gt;```bash rm -rf build HIPCXX=&amp;quot;$(hipconfig -l)/clang&amp;quot; \ HIP_PATH=&amp;quot;$(hipconfig -R)&amp;quot; \ cmake -S . -B build \ -DGGML_HIP=ON \ -DGPU_TARGETS=gfx1030 \ -DCMAKE_BUILD_TYPE=Release \ -DCMAKE_BUILD_RPATH='$ORIGIN/../lib'&lt;/p&gt; &lt;p&gt;cmake --build build -j 16&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;It is important to provide you target architecture.&lt;/p&gt; &lt;h1&gt;My llama.cpp run&lt;/h1&gt; &lt;p&gt;&lt;code&gt;bash ./build/bin/llama-server \ --model unsloth/GLM-4.7-Flash-UD-Q4_K_XL.gguf \ --alias &amp;quot;glm-4.7-flash&amp;quot; \ --jinja \ --repeat-penalty 1.0 \ --seed 1234 \ --temp 0.7 \ --top-p 1 \ --min-p 0.01 \ --threads 12 \ --n-cpu-moe 32 \ --fit on \ --kv-unified \ --flash-attn off \ --batch-size 256 \ --ubatch-size 256 \ --ctx-size 65535 \ --host 0.0.0.0 &lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;The most important setting was &lt;code&gt;--flash-attn off&lt;/code&gt; ! Since old AMD RDNA2 cards doesn't support flash attention, llama switches to fallback CPU and makes work unusable.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The second important parameter is &lt;code&gt;--n-cpu-moe xx&lt;/code&gt; which allows your to balance RAM between CPU and GPU. Here is my result:&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;code&gt;bash load_tensors: CPU_Mapped model buffer size = 11114.88 MiB load_tensors: ROCm0 model buffer size = 6341.37 MiB &lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;the rest thing is about fighting for the model brains (size) and allocation. You can run a bigger model if you decrease a context size and batches and vice versa.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Experiments&lt;/h3&gt; &lt;p&gt;During my experiments I switched between several models. I also generated test promt and passed output to Cloud to make raiting.&lt;/p&gt; &lt;p&gt;Here is tested models: 1. GLM-4.7-Flash-REAP-23B-A3B-UD-Q3_K_XL.gguf 2. GLM-4.7-Flash-UD-Q3_K_XL.gguf (no reasoning) 3. GLM-4.7-Flash-UD-Q3_K_XL.gguf 4. GLM-4.7-Flash-UD-Q4_K_XL.gguf&lt;/p&gt; &lt;p&gt;I run once a model without reasoning occasionally, but it was very useful for raiting evaluation&lt;/p&gt; &lt;p&gt;Here is a test prompt:&lt;/p&gt; &lt;p&gt;```bash time curl http://myserver:8080/v1/chat/completions \ -H &amp;quot;Content-Type: application/json&amp;quot; \ -d '{ &amp;quot;model&amp;quot;: &amp;quot;glm-4.7-flash&amp;quot;, &amp;quot;messages&amp;quot;: [ { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Write a JavaScript function to sort an array.&amp;quot; } ], &amp;quot;temperature&amp;quot;: 0.7, &amp;quot;max_tokens&amp;quot;: 2048, &amp;quot;stream&amp;quot;: false, &amp;quot;stop&amp;quot;: [&amp;quot;&amp;lt;|user|&amp;gt;&amp;quot;, &amp;quot;&amp;lt;|endoftext|&amp;gt;&amp;quot;] }'&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;This prompt was processed in 1:08 minute in average&lt;/p&gt; &lt;h3&gt;Benchmark&lt;/h3&gt; &lt;p&gt;The biggest model which fits into GPU memory is &lt;code&gt;GLM-4.7-Flash-UD-Q3_K_XL.gguf&lt;/code&gt; Here is a benchmark of this model with all defaults &lt;/p&gt; &lt;p&gt;&lt;code&gt; /build/bin/llama-bench --model unsloth/GLM-4.7-Flash-UD-Q3_K_XL.gguf -ngl 99 ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon RX 6900 XT, gfx1030 (0x1030), VMM: no, Wave Size: 32 | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | deepseek2 ?B Q3_K - Medium | 12.85 GiB | 29.94 B | ROCm | 99 | pp512 | 1410.65 Â± 3.52 | | deepseek2 ?B Q3_K - Medium | 12.85 GiB | 29.94 B | ROCm | 99 | tg128 | 66.19 Â± 0.03 |&lt;/code&gt;&lt;/p&gt; &lt;h3&gt;Claude raiting&lt;/h3&gt; &lt;p&gt;I need to say here that I really love Claude, but it is very chatty. I put the main takeaways from it's report &lt;/p&gt; &lt;h4&gt;&lt;strong&gt;B. Feature Completeness&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;code&gt;text â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Feature â”‚ Model 1 â”‚ Model 2 â”‚ Model 3 â”‚ Model 4 â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ Ascending sort â”‚ âœ… â”‚ âœ… â”‚ âœ… â”‚ âœ… â”‚ â”‚ Descending sort â”‚ âœ… â”‚ âœ… â”‚ âœ… â”‚ âœ… â”‚ â”‚ String sorting â”‚ âŒ â”‚ âŒ â”‚ âœ… â”‚ âœ… â”‚ â”‚ Object sorting â”‚ âœ… â”‚ âœ… â”‚ âŒ â”‚ âŒ â”‚ â”‚ Bubble Sort â”‚ âŒ â”‚ âŒ â”‚ âœ… â”‚ âœ… â”‚ â”‚ Immutability (spread) â”‚ âŒ â”‚ âŒ â”‚ âœ… â”‚ âŒ â”‚ â”‚ Mutation warning â”‚ âŒ â”‚ âœ… â”‚ âœ… â”‚ âœ… â”‚ â”‚ Comparator explanation â”‚ âœ… â”‚ âœ… â”‚ âœ… â”‚ âœ… â”‚ â”‚ Copy technique â”‚ âŒ â”‚ âŒ â”‚ âŒ â”‚ âœ… â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ TOTAL FEATURES â”‚ 4/9 â”‚ 5/9 â”‚ 7/9 â”‚ 7/9 â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;&lt;strong&gt;Updated Final Rankings&lt;/strong&gt;&lt;/h3&gt; &lt;h4&gt;&lt;strong&gt;ğŸ¥‡ GOLD: Model 4 (Q4_K_XL)&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Score: 94/100&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Strengths:&lt;/strong&gt; - âœ… &lt;strong&gt;Best-organized reasoning&lt;/strong&gt; (9-step structured process) - âœ… &lt;strong&gt;Clearest section headers&lt;/strong&gt; with use-case labels - âœ… &lt;strong&gt;Explicit copy technique warning&lt;/strong&gt; (immutability guidance) - âœ… &lt;strong&gt;Good array example&lt;/strong&gt; (shows string sort bug) - âœ… &lt;strong&gt;String + Bubble Sort&lt;/strong&gt; included - âœ… &lt;strong&gt;Fast generation&lt;/strong&gt; (23.62 tok/sec, 2nd place) - âœ… &lt;strong&gt;Higher quality quantization&lt;/strong&gt; (Q4 vs Q3)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Weaknesses:&lt;/strong&gt; - âŒ Doesn't use spread operator in examples (tells user to do it) - âŒ No object sorting - âŒ 15 fewer tokens of content than Model 3&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Professional development, code reviews, production guidance&lt;/p&gt; &lt;h4&gt;&lt;strong&gt;4th Place: Model 1 (Q3_K_XL REAP-23B-A3B)&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Score: 78/100&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Strengths:&lt;/strong&gt; - âœ… Has reasoning - âœ… Object sorting included - âœ… Functional code&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Weaknesses:&lt;/strong&gt; - âŒ &lt;strong&gt;Weakest array example&lt;/strong&gt; - âŒ &lt;strong&gt;Slowest generation&lt;/strong&gt; (12.53 tok/sec = &lt;strong&gt;50% slower&lt;/strong&gt; than Model 3) - âŒ &lt;strong&gt;Fewest features&lt;/strong&gt; (4/9) - âŒ No Bubble Sort - âŒ No string sorting - âŒ No immutability patterns - âŒ Special REAP quantization doesn't show advantages here&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Resource-constrained environments, basic use cases&lt;/p&gt; &lt;h3&gt;My conclusions&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;We can still use old AMD GPUs for local inference&lt;/li&gt; &lt;li&gt;Model size still does matter, even with quantisation!&lt;/li&gt; &lt;li&gt;But we can run models bigger than GPU VRAM size!&lt;/li&gt; &lt;li&gt;Recent llama flags give you a large space for experiments&lt;/li&gt; &lt;li&gt;&lt;code&gt;--n-cpu-moe&lt;/code&gt; is very useful for GPU/CPU balance&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And the most important conclusion that this is not the final result!&lt;/p&gt; &lt;p&gt;Please feel free to share you findings and improvements with humans and robots! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Begetan"&gt; /u/Begetan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r45hkh/running_glm47_on_an_old_amd_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r45hkh/running_glm47_on_an_old_amd_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r45hkh/running_glm47_on_an_old_amd_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T00:05:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3uj0h</id>
    <title>MiniMax-M2.5 (230B MoE) GGUF is here - First impressions on M3 Max 128GB</title>
    <updated>2026-02-13T16:56:58+00:00</updated>
    <author>
      <name>/u/Remarkable_Jicama775</name>
      <uri>https://old.reddit.com/user/Remarkable_Jicama775</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ğŸ”¥ UPDATE 2: Strict Perplexity Benchmark &amp;amp; Trade-off Analysis&lt;/p&gt; &lt;p&gt;Thanks to &lt;a href="/u/ubergarm"&gt;u/ubergarm&lt;/a&gt; and the community for pointing out the context discrepancy in my initial PPL run (I used -c 4096, which inflated the score).&lt;/p&gt; &lt;p&gt;I just re-ran the benchmark on the M3 Max using standard comparison parameters (-c 512, -b 2048, --seed 1337) to get an apples-to-apples comparison with SOTA custom mixes (like IQ4_XS).&lt;/p&gt; &lt;p&gt;The Real Numbers:&lt;/p&gt; &lt;p&gt;My Q3_K_L (Standard): 8.7948 PPL (+/- 0.07)&lt;/p&gt; &lt;p&gt;Custom IQ4_XS Mix (ubergarm): ~8.57 PPL&lt;/p&gt; &lt;p&gt;The Verdict / Why use this Q3_K_L? While the custom mix wins on pure reasoning density (~0.22 PPL delta), this Q3_K_L remains the &amp;quot;bandwidth king&amp;quot; for Mac users.&lt;/p&gt; &lt;p&gt;RAM Headroom: It fits comfortably in 128GB with room for context (unlike Q4 which hits swap).&lt;/p&gt; &lt;p&gt;Speed: Because the attn.* tensors are smaller (Q3 vs Q8 in custom mixes), we are seeing 28.7 t/s generation speed due to lower memory bandwidth pressure.&lt;/p&gt; &lt;p&gt;TL;DR: Use this Q3_K_L if you are strictly limited to 128GB RAM and prioritize speed/compatibility. Use an IQ4_XS mix if you have 192GB+ or prioritize absolute maximum reasoning over speed. &lt;strong&gt;Update: Q3_K_L is officially LIVE on Hugging Face! Link. Tested and verified at 28.7 t/s on M3 Max. Enjoy the native RAM performance!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;ğŸ”¬ &lt;strong&gt;Perplexity Validation (WikiText-2)&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Final PPL: 8.2213 +/- 0.09&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Context: 4096 / 32 chunks&lt;/p&gt; &lt;p&gt;Outcome: The Q3_K_L quantization maintains high logical coherence while boosting speed to 28.7 t/s. Minimal degradation for a ~20GB size reduction vs Q4. Just ran PPL on my Q3_K_L (110.22 GiB). Got a Final PPL of 8.2213 (+/- 0.09) on WikiText-2. It seems that going the FP8 -&amp;gt; F16 Master -&amp;gt; Q3_K_L route really paid off compared to standard quants. It beats the IQ4_XS efficiency curve while fitting perfectly in 128GB RAM at 28.7 t/s&lt;/p&gt; &lt;p&gt;The new MiniMax-M2.5 is a beast, but running a 230B MoE locally isn't easy. Iâ€™ve finished the quantization process using llama.cpp (b8022) and optimized it specifically for high-RAM Apple Silicon.&lt;/p&gt; &lt;p&gt;ğŸš€ The &amp;quot;Sweet Spot&amp;quot; for 128GB RAM: Q3_K_L After initial testing with Q4_K_M (132GB), it was clear that hitting the swap was killing performance. I went back to the F16 Master (457GB) to cook a high-quality Q3_K_L (~110GB).&lt;/p&gt; &lt;p&gt;Benchmarks (M3 Max 128GB):&lt;/p&gt; &lt;p&gt;Prompt Processing: &lt;strong&gt;99.2 t/s&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Generation: &lt;strong&gt;28.7 t/s ğŸš€&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;RAM Behavior: 100% native RAM usage. Zero swap lag.&lt;/p&gt; &lt;p&gt;ğŸ›  Technical Details To ensure maximum reasoning fidelity, I avoided direct FP8-to-Quant conversion. The workflow was: Original FP8 -&amp;gt; F16 GGUF Master -&amp;gt; K-Quants (Q4_K_M &amp;amp; Q3_K_L).&lt;/p&gt; &lt;p&gt;Architecture: 230B Mixture of Experts (MiniMax-M2).&lt;/p&gt; &lt;p&gt;Logic: The Jinja chat template is working perfectly; &amp;lt;think&amp;gt; tags are isolated as intended.&lt;/p&gt; &lt;p&gt;Context: Native 196k support.&lt;/p&gt; &lt;p&gt;ğŸ“¥ Links &amp;amp; Resources GGUF Repo: &lt;a href="https://huggingface.co/ox-ox/MiniMax-M2.5-GGUF"&gt;https://huggingface.co/ox-ox/MiniMax-M2.5-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Usage: ./llama-cli -m minimax-m2.5-Q3_K_L.gguf -n -1 \ -c 262000 \ -ngl 99 -fa on -ctk q4_0 -ctv q4_0 -b 2048 -ub 1024 --port 8080 --jinja --verbose -sm none --draft 16 -ncmoe 0 --cache-reuse 1024 --draft-p-min 0.5&lt;/p&gt; &lt;p&gt;For those with 64GB or 96GB setups, let me know if there's interest in IQ2_XXS or IQ3_XS versions. I'm happy to cook more if the demand is there!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remarkable_Jicama775"&gt; /u/Remarkable_Jicama775 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3uj0h/minimaxm25_230b_moe_gguf_is_here_first/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3uj0h/minimaxm25_230b_moe_gguf_is_here_first/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3uj0h/minimaxm25_230b_moe_gguf_is_here_first/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:56:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1r49hob</id>
    <title>Minimax 2.5 is out, considering local deployment</title>
    <updated>2026-02-14T03:09:14+00:00</updated>
    <author>
      <name>/u/Dramatic_Spirit_8436</name>
      <uri>https://old.reddit.com/user/Dramatic_Spirit_8436</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently tried out Minimax 2.5, which just dropped, and from what Iâ€™ve heard, the results are pretty impressive. I gave it a go on zenmux, and I have to say, it really covers a lot of ground. The flexibility, speed, and accuracy are definitely noticeable improvements.&lt;/p&gt; &lt;p&gt;Now, Iâ€™m thinking about deploying it locally. Iâ€™ve used Ollama for deployments before, but I noticed that for Minimax 2.5, Ollama only offers a cloud version. Iâ€™m curious about other deployment options and wondering what the difficulty level and hardware costs would be for a local setup.&lt;/p&gt; &lt;p&gt;Has anyone tried deploying Minimax 2.5 locally, or can share any insights into the hardware requirements? Any advice would be greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dramatic_Spirit_8436"&gt; /u/Dramatic_Spirit_8436 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r49hob/minimax_25_is_out_considering_local_deployment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r49hob/minimax_25_is_out_considering_local_deployment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r49hob/minimax_25_is_out_considering_local_deployment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T03:09:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r47fz0</id>
    <title>Claude Code with Local Models: Full Prompt Reprocessing with Every Request</title>
    <updated>2026-02-14T01:33:26+00:00</updated>
    <author>
      <name>/u/postitnote</name>
      <uri>https://old.reddit.com/user/postitnote</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very recently, I found that Claude Code was triggering full prompt processing for every request. I looked into the logs and found CC is adding this to the list of system messages: &lt;code&gt; text:&amp;quot;x-anthropic-billing-header: cc_version=2.1.39.c39; cc_entrypoint=cli; cch=56445;&amp;quot;, type:&amp;quot;text&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;The values in the header changed with every request, and the template rendered it as text in the system prompt which caused a full reprocessing. With a little google search, I found &lt;a href="https://github.com/musistudio/claude-code-router/issues/1161"&gt;this&lt;/a&gt;, which recommended doing this to remove the header:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;set env &amp;quot;CLAUDE_CODE_ATTRIBUTION_HEADER&amp;quot;: &amp;quot;0&amp;quot; in claude settings.json&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And placing that in my ~/.claude/settings.json in the &amp;quot;env&amp;quot; section was enough to remove that from the system prompt and get my KV cache back to being effective again.&lt;/p&gt; &lt;p&gt;Hope that helps anyone running into the same issue.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/postitnote"&gt; /u/postitnote &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r47fz0/claude_code_with_local_models_full_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r47fz0/claude_code_with_local_models_full_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r47fz0/claude_code_with_local_models_full_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T01:33:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3yahe</id>
    <title>LLaDA2.1 (100B/16B) released â€” now with token editing for massive speed gains</title>
    <updated>2026-02-13T19:16:39+00:00</updated>
    <author>
      <name>/u/FeelingWatercress871</name>
      <uri>https://old.reddit.com/user/FeelingWatercress871</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LLaDA2.1 builds on LLaDA2.0 by introducing Token-to-Token (T2T) editing alongside the standard Mask-to-Token decoding. Instead of locking in tokens once generated, the model can now retroactively correct errors during inference â€” enabling much more aggressive parallel drafting.&lt;/p&gt; &lt;p&gt;Two decoding modes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;S Mode (Speedy): Aggressively low masking threshold + T2T correction. On coding tasks, LLaDA2.1-flash (100B) hits 892 TPS on HumanEval+, 801 TPS on BigCodeBench, 663 TPS on LiveCodeBench.&lt;/li&gt; &lt;li&gt;Q Mode (Quality): Conservative thresholds for best benchmark scores â€” surpasses LLaDA2.0 on both Mini and Flash.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;First large-scale RL framework for diffusion LLMs (EBPO), improving reasoning and instruction following&lt;/li&gt; &lt;li&gt;Multi-Block Editing (MBE): revisit and revise previously generated blocks, consistent gains on reasoning/coding at modest speed cost&lt;/li&gt; &lt;li&gt;LLaDA2.1-mini (16B) peaks at ~1587 TPS on HumanEval+&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/collections/inclusionAI/llada21"&gt;https://huggingface.co/collections/inclusionAI/llada21&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/inclusionAI/LLaDA2.X"&gt;https://github.com/inclusionAI/LLaDA2.X&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tech Report: &lt;a href="https://huggingface.co/papers/2602.08676"&gt;https://huggingface.co/papers/2602.08676&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeelingWatercress871"&gt; /u/FeelingWatercress871 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yahe/llada21_100b16b_released_now_with_token_editing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yahe/llada21_100b16b_released_now_with_token_editing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yahe/llada21_100b16b_released_now_with_token_editing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T19:16:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3uixu</id>
    <title>GPT-OSS (20B) running 100% locally in your browser on WebGPU</title>
    <updated>2026-02-13T16:56:53+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3uixu/gptoss_20b_running_100_locally_in_your_browser_on/"&gt; &lt;img alt="GPT-OSS (20B) running 100% locally in your browser on WebGPU" src="https://external-preview.redd.it/azltbmk2OWprYWpnMcJUN0NJi-FsRvjcOQ-2jdC_J8rSz1PUOqY6x-ztdpX7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c42411a8a77fe60dfe81ecb5c06b854e8c0ac88" title="GPT-OSS (20B) running 100% locally in your browser on WebGPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, I released a demo showcasing GPT-OSS (20B) running 100% locally in-browser on WebGPU, powered by Transformers.js v4 (preview) and ONNX Runtime Web. Hope you like it! &lt;/p&gt; &lt;p&gt;Links:&lt;br /&gt; - Demo (+ source code): &lt;a href="https://huggingface.co/spaces/webml-community/GPT-OSS-WebGPU"&gt;https://huggingface.co/spaces/webml-community/GPT-OSS-WebGPU&lt;/a&gt;&lt;br /&gt; - Optimized ONNX model: &lt;a href="https://huggingface.co/onnx-community/gpt-oss-20b-ONNX"&gt;https://huggingface.co/onnx-community/gpt-oss-20b-ONNX&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ioqb4q8jkajg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3uixu/gptoss_20b_running_100_locally_in_your_browser_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3uixu/gptoss_20b_running_100_locally_in_your_browser_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r40o83</id>
    <title>ubergarm/MiniMax-2.5-GGUF</title>
    <updated>2026-02-13T20:47:04+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r40o83/ubergarmminimax25gguf/"&gt; &lt;img alt="ubergarm/MiniMax-2.5-GGUF" src="https://preview.redd.it/e7zeec20qbjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dec023595454edde747bd1bebdaab70e22a17fe5" title="ubergarm/MiniMax-2.5-GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just cooked and benchmarked (perplexity) of some MiniMax-M2.5 GGUF quants over at: &lt;a href="https://huggingface.co/ubergarm/MiniMax-M2.5-GGUF"&gt;https://huggingface.co/ubergarm/MiniMax-M2.5-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The IQ4_XS works on mainline llama.cpp, LMStudio, Kobold CPP etc. The other quants require ik_llama.cpp (which supports all of the quant types of mainline as well).&lt;/p&gt; &lt;p&gt;Gonna get some llama-sweep-bench tests for PP/TG drop off across context depth next. The smol-IQ3_KS was working in my `opencode` local testing and seems promising but probably a bit too large for enough context on 96GB VRAM hence the smaller IQ2_KS is also available at a cost to quality.&lt;/p&gt; &lt;p&gt;Fun stuff!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e7zeec20qbjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r40o83/ubergarmminimax25gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r40o83/ubergarmminimax25gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T20:47:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3t8ro</id>
    <title>Nvidiaâ€™s new technique cuts LLM reasoning costs by 8x without losing accuracy</title>
    <updated>2026-02-13T16:09:31+00:00</updated>
    <author>
      <name>/u/Mission-Street4214</name>
      <uri>https://old.reddit.com/user/Mission-Street4214</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nvidia developed a new technique called Dynamic Memory Sparsification (DMS) that vastly improves how LLMs manage their KV cache during inference. It accomplishes this by retrofitting existing models so that the attention layers output a &lt;strong&gt;learned keep or evict&lt;/strong&gt; signal for each token in the KV cache. &lt;/p&gt; &lt;p&gt;In addition, they've added a &amp;quot;delayed eviction&amp;quot; that marks a token as low-importance, but doesn't delete it immediately. Instead, it remains accessible for a short time and allows the model to extract any useful information into newer tokens before it's discarded.&lt;/p&gt; &lt;p&gt;These advancements reduce KV memory usage by up to &lt;strong&gt;8x&lt;/strong&gt;, allowing the model to think longer, run faster and handle more concurrent requests.&lt;/p&gt; &lt;p&gt;Definitely recommend reading the full article. Looking forward to seeing this on self hosted hardware.&lt;/p&gt; &lt;p&gt;&lt;a href="https://venturebeat.com/orchestration/nvidias-new-technique-cuts-llm-reasoning-costs-by-8x-without-losing-accuracy"&gt;VentureBeat Article&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mission-Street4214"&gt; /u/Mission-Street4214 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t8ro/nvidias_new_technique_cuts_llm_reasoning_costs_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t8ro/nvidias_new_technique_cuts_llm_reasoning_costs_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t8ro/nvidias_new_technique_cuts_llm_reasoning_costs_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:09:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r41013</id>
    <title>GLM-5 Is a local GOAT</title>
    <updated>2026-02-13T21:00:08+00:00</updated>
    <author>
      <name>/u/FineClassroom2085</name>
      <uri>https://old.reddit.com/user/FineClassroom2085</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r41013/glm5_is_a_local_goat/"&gt; &lt;img alt="GLM-5 Is a local GOAT" src="https://external-preview.redd.it/MTlvZ25qOTVyYmpnMet_8L-GzQ_poWye6LYGoFL5kcPokh15ZfJ1OHhOgrf9.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa3d1833719afef6a4e55f5f11807d2e7ef7d341" title="GLM-5 Is a local GOAT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;: I am a developer with over two decades of experience. I use LLMs heavily day to day from all of the major providers. Since the first Llama models came out I've been toying with local models, benchmarking them on real-world heavy use cases.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Long story short:&lt;/strong&gt; GLM-5 is the first model I've been able to run locally that's actually impressed me. In 3 'shots' I was able to make a retro styled flappy clone AND deploy it to AWS with a cost assessment if it went viral.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My prompt&lt;/strong&gt;: Please generate a GPU accelerated clone of the game â€˜Flappy Birdâ€™ where using the spacebar causes the bird to â€˜flapâ€™, give it a 'retro inspired' design.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Setup&lt;/strong&gt;:&lt;br /&gt; - Dual RTX 6000 PRO MaxQ GPUs&lt;br /&gt; - 128gb of DDR5&lt;br /&gt; - AMD Ryzen Threadripper PRO 7975WX&lt;br /&gt; - GLM-5-744B served over vLLM with 128k context at IQ2_M&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Caveats&lt;/strong&gt;: Even with my decently powerful hardware, the token output was painfully slow at 16.5t/s. IMO, completely worth the wait though. The same test with Qwen3-Next-80b, GPT-OSS-120b and a few other leaders was unimpressive.&lt;/p&gt; &lt;p&gt;&lt;a href="https://flappy.tjameswilliams.com/"&gt;https://flappy.tjameswilliams.com/&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FineClassroom2085"&gt; /u/FineClassroom2085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7l7iri95rbjg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r41013/glm5_is_a_local_goat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r41013/glm5_is_a_local_goat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T21:00:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3yuyd</id>
    <title>has it begun?</title>
    <updated>2026-02-13T19:38:01+00:00</updated>
    <author>
      <name>/u/Acceptable_Home_</name>
      <uri>https://old.reddit.com/user/Acceptable_Home_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yuyd/has_it_begun/"&gt; &lt;img alt="has it begun?" src="https://preview.redd.it/ei9lt0u4ebjg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=36034757efbb832ba75f43ed04c4dc8c7bb34675" title="has it begun?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.bloomberg.com/news/articles/2026-02-13/us-to-put-alibaba-on-list-for-aiding-china-s-military-reuters"&gt;https://www.bloomberg.com/news/articles/2026-02-13/us-to-put-alibaba-on-list-for-aiding-china-s-military-reuters&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They were about to present the name of alibaba and Baidu as a potential threat or issue for helping chinese military in the Pentagon, but ultimately took their names off the list&lt;/p&gt; &lt;p&gt;Would love to hear what y'all think about this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable_Home_"&gt; /u/Acceptable_Home_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ei9lt0u4ebjg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yuyd/has_it_begun/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yuyd/has_it_begun/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T19:38:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3pxy7</id>
    <title>MiniMaxAI/MiniMax-M2.5 Â· Hugging Face</title>
    <updated>2026-02-13T14:01:52+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/"&gt; &lt;img alt="MiniMaxAI/MiniMax-M2.5 Â· Hugging Face" src="https://external-preview.redd.it/U31ABjps-QUJnQnBzpD57R8aGaCakUKOyzXjWrvNYSg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de0bab4be78008336f973196f0ed98e2bbe49764" title="MiniMaxAI/MiniMax-M2.5 Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can monitor quants begin to appear with this search: &lt;a href="https://huggingface.co/models?sort=modified&amp;amp;search=minimax+m2.5"&gt;https://huggingface.co/models?sort=modified&amp;amp;search=minimax+m2.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T14:01:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3weq3</id>
    <title>SWE-rebench Jan 2026: GLM-5, MiniMax M2.5, Qwen3-Coder-Next, Opus 4.6, Codex Performance</title>
    <updated>2026-02-13T18:06:40+00:00</updated>
    <author>
      <name>/u/CuriousPlatypus1881</name>
      <uri>https://old.reddit.com/user/CuriousPlatypus1881</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, Iâ€™m Anton from Nebius.&lt;/p&gt; &lt;p&gt;Weâ€™ve updated the &lt;strong&gt;SWE-rebench leaderboard&lt;/strong&gt; with our &lt;strong&gt;January runs&lt;/strong&gt; on &lt;strong&gt;48 fresh GitHub PR tasks&lt;/strong&gt; (PRs created in the previous month only). The setup is standard SWE-bench: models read real PR issues, edit code, run tests, and must make the full suite pass.&lt;/p&gt; &lt;p&gt;Key observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Claude Code (Opus 4.6)&lt;/strong&gt; leads this snapshot at &lt;strong&gt;52.9% resolved rate&lt;/strong&gt; and also achieves the highest &lt;strong&gt;pass@5 (70.8%)&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Claude Opus 4.6&lt;/strong&gt; and &lt;strong&gt;gpt-5.2-xhigh&lt;/strong&gt; follow very closely (51.7%), making the top tier extremely tight.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;gpt-5.2-medium (51.0%)&lt;/strong&gt; performs surprisingly close to the frontier configuration.&lt;/li&gt; &lt;li&gt;Among open models, &lt;strong&gt;Kimi K2 Thinking (43.8%)&lt;/strong&gt;, &lt;strong&gt;GLM-5 (42.1%)&lt;/strong&gt;, and &lt;strong&gt;Qwen3-Coder-Next (40.0%)&lt;/strong&gt; lead the pack.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MiniMax M2.5 (39.6%)&lt;/strong&gt; continues to show strong performance while remaining one of the cheapest options.&lt;/li&gt; &lt;li&gt;Clear gap between Kimi variants: &lt;strong&gt;K2 Thinking (43.8%)&lt;/strong&gt; vs &lt;strong&gt;K2.5 (37.9%)&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Newer smaller/flash variants (e.g., GLM-4.7 Flash, gpt-5-mini-medium) trade performance for efficiency, landing in the 25â€“31% range.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to your thoughts and feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CuriousPlatypus1881"&gt; /u/CuriousPlatypus1881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/?insight=jan_2026"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3weq3/swerebench_jan_2026_glm5_minimax_m25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3weq3/swerebench_jan_2026_glm5_minimax_m25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T18:06:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3zuuf</id>
    <title>GPT-OSS 120b Uncensored Aggressive Release (MXFP4 GGUF)</title>
    <updated>2026-02-13T20:15:33+00:00</updated>
    <author>
      <name>/u/hauhau901</name>
      <uri>https://old.reddit.com/user/hauhau901</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, made an uncensored version of GPT-OSS 120B.&lt;/p&gt; &lt;p&gt;Quick specs: 117B total params, ~5.1B active (MoE with 128 experts, top-4 routing), 128K context. MXFP4 is the model's native precision - this isn't a quantization, it's how it was trained. No overall quality loss, though you can see CoT behave differently at times.&lt;/p&gt; &lt;p&gt;This is the aggressive variant - &lt;strong&gt;observed 0 refusals to any query during testing.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Completely uncensored while keeping full model capabilities intact.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://huggingface.co/HauhauCS/GPTOSS-120B-Uncensored-HauhauCS-Aggressive"&gt;https://huggingface.co/HauhauCS/GPTOSS-120B-Uncensored-HauhauCS-Aggressive&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sampling settings:&lt;/p&gt; &lt;p&gt;- --temp 1.0 --top-k 40&lt;/p&gt; &lt;p&gt;- Disable everything else (top_p, min_p, repeat penalty, etc.) - some clients turn&lt;/p&gt; &lt;p&gt;these on by default&lt;/p&gt; &lt;p&gt;- llama.cpp users: --jinja is required for the Harmony response format or the model won't work right&lt;/p&gt; &lt;p&gt;- Example: llama-server -m model.gguf --jinja -fa -b 2048 -ub 2048&lt;/p&gt; &lt;p&gt;Single 61GB file. Fits on one H100. For lower VRAM, use --n-cpu-moe N in llama.cpp to offload MoE layers to CPU.&lt;/p&gt; &lt;p&gt;Works with llama.cpp, LM Studio, Ollama, etc.&lt;/p&gt; &lt;p&gt;If you want smaller models, I also have GPT-OSS 20B, GLM 4.7 Flash and Qwen3 8b VL uncensored:&lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/HauhauCS/models/"&gt;https://huggingface.co/HauhauCS/models/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As with all my releases, the goal is effectively lossless uncensoring - no dataset changes and no capability loss.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hauhau901"&gt; /u/hauhau901 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3zuuf/gptoss_120b_uncensored_aggressive_release_mxfp4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3zuuf/gptoss_120b_uncensored_aggressive_release_mxfp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3zuuf/gptoss_120b_uncensored_aggressive_release_mxfp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T20:15:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r44fzk</id>
    <title>The gap between open-weight and proprietary model intelligence is as small as it has ever been, with Claude Opus 4.6 and GLM-5'</title>
    <updated>2026-02-13T23:20:10+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r44fzk/the_gap_between_openweight_and_proprietary_model/"&gt; &lt;img alt="The gap between open-weight and proprietary model intelligence is as small as it has ever been, with Claude Opus 4.6 and GLM-5'" src="https://preview.redd.it/4rozb901icjg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0af0fe460ed577cfa1d0490e0386a39aa78b986f" title="The gap between open-weight and proprietary model intelligence is as small as it has ever been, with Claude Opus 4.6 and GLM-5'" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4rozb901icjg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r44fzk/the_gap_between_openweight_and_proprietary_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r44fzk/the_gap_between_openweight_and_proprietary_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T23:20:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3t775</id>
    <title>AMA with MiniMax â€” Ask Us Anything!</title>
    <updated>2026-02-13T16:07:54+00:00</updated>
    <author>
      <name>/u/HardToVary</name>
      <uri>https://old.reddit.com/user/HardToVary</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt; &lt;img alt="AMA with MiniMax â€” Ask Us Anything!" src="https://preview.redd.it/5z2li1ntcajg1.jpg?width=140&amp;amp;height=59&amp;amp;auto=webp&amp;amp;s=ce3340cd37b7e9408878509be00dd7b871efebde" title="AMA with MiniMax â€” Ask Us Anything!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! Weâ€™re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;We're &lt;strong&gt;MiniMax&lt;/strong&gt;, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;.5&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining the channel today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; â€” Founder of MiniMax&lt;/li&gt; &lt;li&gt;&lt;a href="/u/Wise_Evidence9973"&gt;u/Wise_Evidence9973&lt;/a&gt; â€” Head of LLM Research&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ryan85127704"&gt;u/ryan85127704&lt;/a&gt; â€” Head of Engineering&lt;/li&gt; &lt;li&gt;&lt;a href="/u/HardToVary"&gt;u/HardToVary&lt;/a&gt; â€” LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c"&gt;https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S. We'll continue monitoring and responding to questions for 48 hours after the end of the AMA.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HardToVary"&gt; /u/HardToVary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:07:54+00:00</published>
  </entry>
</feed>
