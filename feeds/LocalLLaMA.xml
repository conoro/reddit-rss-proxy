<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-15T22:23:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nhcf8t</id>
    <title>Successfully tuning 5090's for low heat, high speed in Linux with LACT</title>
    <updated>2025-09-15T04:30:04+00:00</updated>
    <author>
      <name>/u/mr_zerolith</name>
      <uri>https://old.reddit.com/user/mr_zerolith</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhcf8t/successfully_tuning_5090s_for_low_heat_high_speed/"&gt; &lt;img alt="Successfully tuning 5090's for low heat, high speed in Linux with LACT" src="https://preview.redd.it/hqvthud379pf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb2f46293e4ca93fd0ece2afb27cd142ce846dba" title="Successfully tuning 5090's for low heat, high speed in Linux with LACT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share a pro-tip.&lt;/p&gt; &lt;p&gt;The classic trick for making 5090's more efficient in Windows is to undervolt them, but to my knowledge, no linux utility allows you to do this directly.&lt;/p&gt; &lt;p&gt;Moving the power limit to 400w shaves a substantial amount of heat during inference, only incurring a few % loss in speed. This is a good start to lowering the insane amount of heat these can produce, but it's not good enough.&lt;/p&gt; &lt;p&gt;I found out that all you have to do to get this few % of speed loss back is to jack up the GPU memory speed. Yeah, memory bandwidth really does matter.&lt;/p&gt; &lt;p&gt;But this wasn't enough, this thing still generated too much heat. So i tried a massive downclock of the GPU, and i found out that i don't lose any speed, but i lose a ton of heat, and the voltage under full load dropped quite a bit.&lt;/p&gt; &lt;p&gt;It feels like half the heat and my tokens/sec is only down 1-2 versus stock. Not bad!!!&lt;/p&gt; &lt;p&gt;In the picture, we're running SEED OSS 36B in the post-thinking stage, where the load is highest.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_zerolith"&gt; /u/mr_zerolith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hqvthud379pf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhcf8t/successfully_tuning_5090s_for_low_heat_high_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhcf8t/successfully_tuning_5090s_for_low_heat_high_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T04:30:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhvnw0</id>
    <title>I need help choosing between 2 GPUs for AI</title>
    <updated>2025-09-15T19:33:08+00:00</updated>
    <author>
      <name>/u/ee_di_tor</name>
      <uri>https://old.reddit.com/user/ee_di_tor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good time.&lt;/p&gt; &lt;p&gt;My PC configuration:&lt;br /&gt; &lt;strong&gt;CPU&lt;/strong&gt; - i3 10100f&lt;br /&gt; &lt;strong&gt;GPU&lt;/strong&gt; - GTX 1650&lt;br /&gt; &lt;strong&gt;RAM&lt;/strong&gt; - 32 GB&lt;br /&gt; &lt;strong&gt;Motherboard&lt;/strong&gt; - Asus Prime B560MK&lt;/p&gt; &lt;p&gt;I am considering to buy a new GPU. Right now I have two options:&lt;br /&gt; 1. &lt;strong&gt;RTX 3060 12GB&lt;/strong&gt;&lt;br /&gt; 2. &lt;strong&gt;Intel Arc B580 12GB&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The main concerns I have - &lt;strong&gt;stability&lt;/strong&gt; and &lt;strong&gt;software support&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;I lean more to bying &lt;strong&gt;B580&lt;/strong&gt; - AI and game benchmarks look good.&lt;br /&gt; Also - around my place &lt;strong&gt;B580&lt;/strong&gt; is a bit lower in price than &lt;strong&gt;3060.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;What am I doing - &lt;strong&gt;video editing (Premiere Pro, Davinci Resolve), AI (ComfyUI, koboldcpp), gaming (Mordhau, Paradox Games, Cyberpunk 2077, etc..), video recording (OBS)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Will B580 be a &lt;strong&gt;plug-and-use/play experience&lt;/strong&gt; or should I just pick up 3060?&lt;/p&gt; &lt;p&gt;Also, if you know - does B560MK support ReBAR or not?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ee_di_tor"&gt; /u/ee_di_tor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhvnw0/i_need_help_choosing_between_2_gpus_for_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhvnw0/i_need_help_choosing_between_2_gpus_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhvnw0/i_need_help_choosing_between_2_gpus_for_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T19:33:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhxp1a</id>
    <title>for hybrid setups (some layers in ram, some on ssd) - how do you decide which layers to keep in memory? is there a pattern to which layers benefit most from fast access?</title>
    <updated>2025-09-15T20:47:59+00:00</updated>
    <author>
      <name>/u/EmbarrassedAsk2887</name>
      <uri>https://old.reddit.com/user/EmbarrassedAsk2887</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;been experimenting with offloading and noticed some layers seem way more sensitive to access speed than others. like attention layers vs feed-forward - wondering if there's actual research on this or if it's mostly trial and error.&lt;/p&gt; &lt;p&gt;also curious about the autoregressive nature - since each token generation needs to access the kv cache, are you prioritizing keeping certain attention heads in fast memory? or is it more about the embedding layers that get hit constantly?&lt;/p&gt; &lt;p&gt;seen some mention that early layers (closer to input) might be more critical for speed since they process every token, while deeper layers might be okay on slower storage. but then again, the later layers are doing the heavy reasoning work.&lt;/p&gt; &lt;p&gt;anyone have concrete numbers on latency differences? like if attention layers are on ssd vs ram, how much does that actually impact tokens/sec compared to having the ffn layers there instead?&lt;/p&gt; &lt;p&gt;thinking about building a smarter layer allocation system but want to understand the actual bottlenecks first rather than just guessing based on layer size.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmbarrassedAsk2887"&gt; /u/EmbarrassedAsk2887 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhxp1a/for_hybrid_setups_some_layers_in_ram_some_on_ssd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhxp1a/for_hybrid_setups_some_layers_in_ram_some_on_ssd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhxp1a/for_hybrid_setups_some_layers_in_ram_some_on_ssd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T20:47:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhucwr</id>
    <title>Best approach for generating test cases from a 25-page BRD - chunk for prompts or implement RAG?</title>
    <updated>2025-09-15T18:45:28+00:00</updated>
    <author>
      <name>/u/KarimAbdelQader</name>
      <uri>https://old.reddit.com/user/KarimAbdelQader</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm working with a 25-page Business Requirements Document (BRD) for a banking system (Limits &amp;amp; Collateral module) and need to generate comprehensive test cases from it. &lt;/p&gt; &lt;p&gt;The document has detailed functional requirements, integration points, validation rules, and field specifications.I'm torn between two approaches:&lt;/p&gt; &lt;p&gt;Option 1: Chunk + Prompt Break the BRD into logical sections (country allocations, limit utilization, collateral management, etc.) Feed each chunk to an LLM with specific prompts for test case generation&lt;/p&gt; &lt;p&gt;Option 2: RAG Implementation Store the entire document in a vector database Query specific requirements as needed&lt;/p&gt; &lt;p&gt;What approach would you recommend?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KarimAbdelQader"&gt; /u/KarimAbdelQader &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhucwr/best_approach_for_generating_test_cases_from_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhucwr/best_approach_for_generating_test_cases_from_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhucwr/best_approach_for_generating_test_cases_from_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T18:45:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nh9pc9</id>
    <title>Qwen3‑Next‑80B‑A3B‑Instruct (FP8) on Windows 11 WSL2 + vLLM + Docker (Blackwell)</title>
    <updated>2025-09-15T02:11:54+00:00</updated>
    <author>
      <name>/u/IngeniousIdiocy</name>
      <uri>https://old.reddit.com/user/IngeniousIdiocy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EDIT: SEE COMMENTS BELOW. NEW DOCKER IMAGE FROM vLLM MAKES THIS MOOT&lt;/p&gt; &lt;p&gt;I used a LLM to summarize a lot of what I dealt with below. I wrote this because it doesn't exist anywhere on the internet as far as I can tell and you need to scour the internet to find the pieces to pull it together.&lt;/p&gt; &lt;p&gt;Generated content with my editing below:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;br /&gt; If you’re trying to serve &lt;strong&gt;Qwen3‑Next‑80B‑A3B‑Instruct FP8&lt;/strong&gt; on a &lt;strong&gt;Blackwell&lt;/strong&gt; card in &lt;strong&gt;WSL2&lt;/strong&gt;, pin: &lt;strong&gt;PyTorch 2.8.0 (cu128)&lt;/strong&gt;, &lt;strong&gt;vLLM 0.10.2&lt;/strong&gt;, &lt;strong&gt;FlashInfer ≥ 0.3.0 (0.3.1 preferred)&lt;/strong&gt;, and &lt;strong&gt;Transformers (main)&lt;/strong&gt;. Make sure you use the nightly cu128 container from vLLM and it can see &lt;code&gt;/dev/dxg&lt;/code&gt; and &lt;code&gt;/usr/lib/wsl/lib&lt;/code&gt; (so &lt;code&gt;libcuda.so.1&lt;/code&gt; resolves). I used a &lt;strong&gt;CUDA‑12.8 vLLM image&lt;/strong&gt; and mounted a small &lt;code&gt;run.sh&lt;/code&gt;to install the exact userspace combo and start the server. Without upgrading FlashInfer I got the infamous &lt;strong&gt;“FlashInfer requires sm75+”&lt;/strong&gt; crash on Blackwell. After bumping to &lt;strong&gt;0.3.1&lt;/strong&gt;, everything lit up, CUDA graphs enabled, and the OpenAI endpoints served normally. Running at 80 TPS output now single stream and 185 TPS over three streams. If you are leaning on Claude or Chatgpt to guide you through this then they will encourage you to to not use flashinfer or the cuda graphs but you can take advantage of both of these with the right versions of the stack, as shown below.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My setup&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OS:&lt;/strong&gt; Windows 11 + &lt;strong&gt;WSL2&lt;/strong&gt; (Ubuntu)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; &lt;strong&gt;RTX PRO 6000 Blackwell&lt;/strong&gt; (96 GB)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Serving:&lt;/strong&gt; &lt;strong&gt;vLLM&lt;/strong&gt; OpenAI‑compatible server&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;code&gt;TheClusterDev/Qwen3-Next-80B-A3B-Instruct-FP8-Dynamic&lt;/code&gt; (80B total, ~3B activated per token) Heads‑up: despite the 3B activated MoE, &lt;strong&gt;you still need VRAM for the full 80B weights&lt;/strong&gt;. FP8 helped, but it still occupied ~75 GiB on my box. You cannot do this with a quantization flag on the released model unless you have the memory for the 16bit weights. Also, you need the -dynamic version of this model from TheClusterDev to work with vLLM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The docker command I ended up with after much trial and error:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run --rm --name vllm-qwen \ --gpus all \ --ipc=host \ -p 8000:8000 \ --entrypoint bash \ --device /dev/dxg \ -v /usr/lib/wsl/lib:/usr/lib/wsl/lib:ro \ -e LD_LIBRARY_PATH=&amp;quot;/usr/lib/wsl/lib:$LD_LIBRARY_PATH&amp;quot; \ -e HUGGING_FACE_HUB_TOKEN=&amp;quot;$HF_TOKEN&amp;quot; \ -e HF_TOKEN=&amp;quot;$HF_TOKEN&amp;quot; \ -e VLLM_ATTENTION_BACKEND=FLASHINFER \ -v &amp;quot;$HOME/.cache/huggingface:/root/.cache/huggingface&amp;quot; \ -v &amp;quot;$HOME/.cache/torch:/root/.cache/torch&amp;quot; \ -v &amp;quot;$HOME/.triton:/root/.triton&amp;quot; \ -v /data/models/qwen3_next_fp8:/models \ -v &amp;quot;$PWD/run-vllm-qwen.sh:/run.sh:ro&amp;quot; \ lmcache/vllm-openai:latest-nightly-cu128 \ -lc '/run.sh' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Why these flags matter:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;--device /dev/dxg&lt;/code&gt; + &lt;code&gt;-v /usr/lib/wsl/lib:...&lt;/code&gt; exposes the WSL GPU and &lt;strong&gt;WSL CUDA stubs&lt;/strong&gt; (e.g., &lt;code&gt;libcuda.so.1&lt;/code&gt;) to the container. Microsoft/NVIDIA docs confirm the WSL CUDA driver lives here. If you don’t mount this, PyTorch can’t dlopen &lt;code&gt;libcuda.so.1&lt;/code&gt; inside the container.&lt;/li&gt; &lt;li&gt;&lt;code&gt;-p 8000:8000&lt;/code&gt; + &lt;code&gt;--entrypoint bash -lc '/run.sh'&lt;/code&gt; runs my script (below) and binds vLLM on &lt;code&gt;0.0.0.0:8000&lt;/code&gt;(OpenAI‑compatible server). Official vLLM docs describe the OpenAI endpoints (&lt;code&gt;/v1/chat/completions&lt;/code&gt;, etc.).&lt;/li&gt; &lt;li&gt;The CUDA &lt;strong&gt;12.8&lt;/strong&gt; image matches &lt;strong&gt;PyTorch 2.8&lt;/strong&gt; and &lt;strong&gt;vLLM 0.10.2&lt;/strong&gt; expectations (vLLM 0.10.2 upgraded to &lt;strong&gt;PT 2.8&lt;/strong&gt; and &lt;strong&gt;FlashInfer 0.3.0&lt;/strong&gt;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why I bothered with a shell script:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The stock image didn’t have the exact combo I needed for &lt;strong&gt;Blackwell + Qwen3‑Next&lt;/strong&gt; (and I wanted CUDA graphs + FlashInfer active). The script:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Verifies &lt;code&gt;libcuda.so.1&lt;/code&gt; is loadable (from &lt;code&gt;/usr/lib/wsl/lib&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Pins &lt;strong&gt;Torch 2.8.0 cu128&lt;/strong&gt;, &lt;strong&gt;vLLM 0.10.2&lt;/strong&gt;, &lt;strong&gt;Transformers main&lt;/strong&gt;, &lt;strong&gt;FlashInfer 0.3.1&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Prints a small sanity block (Torch CUDA on, vLLM native import OK, FI version)&lt;/li&gt; &lt;li&gt;Serves the model with OpenAI‑compatible endpoints&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It’s short, reproducible, and keeps the Docker command clean.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;References that helped me pin the stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;FlashInfer ≥ 0.3.0&lt;/strong&gt;: &lt;strong&gt;SM120/121&lt;/strong&gt; bring‑up + FP8 GEMM for Blackwell (fixes the “requires sm75+” path). &lt;a href="https://github.com/flashinfer-ai/flashinfer/releases"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;vLLM 0.10.2 release&lt;/strong&gt;: upgrades to &lt;strong&gt;PyTorch 2.8.0&lt;/strong&gt;, &lt;strong&gt;FlashInfer 0.3.0&lt;/strong&gt;, adds &lt;strong&gt;Qwen3‑Next&lt;/strong&gt; hybrid attention, enables &lt;strong&gt;full CUDA graphs&lt;/strong&gt; by default for hybrid, disables prefix cache for hybrid/Mamba. &lt;a href="https://github.com/vllm-project/vllm/releases"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI‑compatible server docs&lt;/strong&gt; (endpoints, clients): &lt;a href="https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html?utm_source=chatgpt.com"&gt;VLLM Documentation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;WSL CUDA&lt;/strong&gt; (why &lt;code&gt;/usr/lib/wsl/lib&lt;/code&gt; and &lt;code&gt;/dev/dxg&lt;/code&gt; matter): &lt;a href="https://learn.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl?utm_source=chatgpt.com"&gt;Microsoft Learn+1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;cu128 wheel index&lt;/strong&gt; (for PT 2.8 stack alignment): &lt;a href="https://download.pytorch.org/whl/cu128?utm_source=chatgpt.com"&gt;PyTorch Download&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3‑Next 80B model card/discussion&lt;/strong&gt; (80B total, ~3B activated per token; still need full weights in VRAM): &lt;a href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct?utm_source=chatgpt.com"&gt;Hugging Face+1&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The tiny shell script that made it work:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The base image didn’t have the right userspace stack for Blackwell + Qwen3‑Next, so I install/verify &lt;strong&gt;exact versions&lt;/strong&gt; and then &lt;code&gt;vllm serve&lt;/code&gt;. Key bits:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pin &lt;strong&gt;Torch 2.8.0 + cu128&lt;/strong&gt; from the PyTorch cu128 wheel index&lt;/li&gt; &lt;li&gt;Install &lt;strong&gt;vLLM 0.10.2&lt;/strong&gt; (aligned to PT 2.8)&lt;/li&gt; &lt;li&gt;Install &lt;strong&gt;Transformers (main)&lt;/strong&gt; (for &lt;strong&gt;Qwen3‑Next&lt;/strong&gt; hybrid arch)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Crucial:&lt;/strong&gt; &lt;strong&gt;FlashInfer 0.3.1&lt;/strong&gt; (0.3.0+ adds &lt;strong&gt;SM120/SM121&lt;/strong&gt; bring‑up + FP8 GEMM; fixed the “requires sm75+” crash I saw)&lt;/li&gt; &lt;li&gt;Sanity‑check &lt;code&gt;libcuda.so.1&lt;/code&gt;, torch CUDA, and vLLM native import before serving&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’ve inlined the updated script here as a reference (trimmed to relevant bits);&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# ... preflight: detect /dev/dxg and export LD_LIBRARY_PATH=/usr/lib/wsl/lib ... # Torch 2.8.0 (CUDA 12.8 wheels) pip install -U --index-url https://download.pytorch.org/whl/cu128 \ &amp;quot;torch==2.8.0+cu128&amp;quot; &amp;quot;torchvision==0.23.0+cu128&amp;quot; &amp;quot;torchaudio==2.8.0+cu128&amp;quot; # vLLM 0.10.2 pip install -U &amp;quot;vllm==0.10.2&amp;quot; --extra-index-url &amp;quot;https://wheels.vllm.ai/0.10.2/&amp;quot; # Transformers main (Qwen3NextForCausalLM) pip install -U https://github.com/huggingface/transformers/archive/refs/heads/main.zip # FlashInfer (Blackwell-ready) pip install -U --no-deps &amp;quot;flashinfer-python==0.3.1&amp;quot; # (0.3.0 also OK) # Serve (OpenAI-compatible) vllm serve TheClusterDev/Qwen3-Next-80B-A3B-Instruct-FP8-Dynamic \ --download-dir /models --host 0.0.0.0 --port 8000 \ --served-model-name qwen3-next-fp8 \ --max-model-len 32768 --gpu-memory-utilization 0.92 \ --max-num-batched-tokens 8192 --max-num-seqs 128 --trust-remote-code &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IngeniousIdiocy"&gt; /u/IngeniousIdiocy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh9pc9/qwen3next80ba3binstruct_fp8_on_windows_11_wsl2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh9pc9/qwen3next80ba3binstruct_fp8_on_windows_11_wsl2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nh9pc9/qwen3next80ba3binstruct_fp8_on_windows_11_wsl2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T02:11:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhayg1</id>
    <title>Blackwell 6000 RTX Pro is still too new.. (Training/Fine-tuning/Unsloth)</title>
    <updated>2025-09-15T03:13:20+00:00</updated>
    <author>
      <name>/u/Aroochacha</name>
      <uri>https://old.reddit.com/user/Aroochacha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Had a nightmare of a weekend trying to train/fine-tune GPT-OSS-120B/20B. I was able to get this working on my 5090 but not the RTX 6000 PRO Workstation edition. I kid you not, the script kept erroring out. Tried everything, doing it normally how I do it, building stuff from source, etc.. I tried Unsloth's instructions for Blackwell along with the latest drivers and Cuda tool kit.&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.unsloth.ai/basics/training-llms-with-blackwell-rtx-50-series-and-unsloth"&gt;https://docs.unsloth.ai/basics/training-llms-with-blackwell-rtx-50-series-and-unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For those of you who want to train Unsloth's fixed GPT-OSS-120B or GPT-OSS-20B, they have a docker image available that should be ready to go.&lt;/p&gt; &lt;p&gt;&lt;a href="https://hub.docker.com/r/unsloth/unsloth-blackwell"&gt;https://hub.docker.com/r/unsloth/unsloth-blackwell&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I just saved you a day and of a half of misery.&lt;br /&gt; You're welcome.&lt;br /&gt; Aroochacha.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aroochacha"&gt; /u/Aroochacha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhayg1/blackwell_6000_rtx_pro_is_still_too_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhayg1/blackwell_6000_rtx_pro_is_still_too_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhayg1/blackwell_6000_rtx_pro_is_still_too_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T03:13:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nh86i7</id>
    <title>vLLM is kinda awesome</title>
    <updated>2025-09-15T00:58:41+00:00</updated>
    <author>
      <name>/u/Secure_Reflection409</name>
      <uri>https://old.reddit.com/user/Secure_Reflection409</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh86i7/vllm_is_kinda_awesome/"&gt; &lt;img alt="vLLM is kinda awesome" src="https://b.thumbs.redditmedia.com/sCRN2mdOtD5l5xVe1j7kNCfWMccS6tEJIyjvpJoPh9I.jpg" title="vLLM is kinda awesome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vs0d2b3098pf1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1071464f396e186209597ad86acf5ac891b6bf8"&gt;https://preview.redd.it/vs0d2b3098pf1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c1071464f396e186209597ad86acf5ac891b6bf8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The last time I ran this test on this card via &lt;strong&gt;LCP&lt;/strong&gt; it took &lt;strong&gt;2 hours 46 minutes 17 seconds&lt;/strong&gt;:&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mjceor/qwen3_30b_2507_thinking_benchmarks/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mjceor/qwen3_30b_2507_thinking_benchmarks/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This time via &lt;strong&gt;vLLM&lt;/strong&gt;? &lt;strong&gt;14 minutes 1 second&lt;/strong&gt; :D&lt;br /&gt; vLLM is a game changer for benchmarking and it just so happens on this run I slightly beat my score from last time too (83.90% vs 83.41%): &lt;/p&gt; &lt;pre&gt;&lt;code&gt;(vllm_env) tests@3090Ti:~/Ollama-MMLU-Pro$ python run_openai.py 2025-09-15 01:09:13.078761 { &amp;quot;comment&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;server&amp;quot;: { &amp;quot;url&amp;quot;: &amp;quot;http://localhost:8000/v1&amp;quot;, &amp;quot;model&amp;quot;: &amp;quot;Qwen3-30B-A3B-Thinking-2507-AWQ-4bit&amp;quot;, &amp;quot;timeout&amp;quot;: 600.0 }, &amp;quot;inference&amp;quot;: { &amp;quot;temperature&amp;quot;: 0.6, &amp;quot;top_p&amp;quot;: 0.95, &amp;quot;max_tokens&amp;quot;: 16384, &amp;quot;system_prompt&amp;quot;: &amp;quot;The following are multiple choice questions (with answers) about {subject}. Think step by step and then finish your answer with \&amp;quot;the answer is (X)\&amp;quot; where X is the correct letter choice.&amp;quot;, &amp;quot;style&amp;quot;: &amp;quot;multi_chat&amp;quot; }, &amp;quot;test&amp;quot;: { &amp;quot;subset&amp;quot;: 1.0, &amp;quot;parallel&amp;quot;: 16 }, &amp;quot;log&amp;quot;: { &amp;quot;verbosity&amp;quot;: 0, &amp;quot;log_prompt&amp;quot;: true } } assigned subjects ['computer science'] computer science: 100%|######################################################################################################| 410/410 [14:01&amp;lt;00:00, 2.05s/it, Correct=344, Wrong=66, Accuracy=83.90] Finished testing computer science in 14 minutes 1 seconds. Total, 344/410, 83.90% Random Guess Attempts, 0/410, 0.00% Correct Random Guesses, division by zero error Adjusted Score Without Random Guesses, 344/410, 83.90% Finished the benchmark in 14 minutes 3 seconds. Total, 344/410, 83.90% Token Usage: Prompt tokens: min 1448, average 1601, max 2897, total 656306, tk/s 778.12 Completion tokens: min 61, average 1194, max 16384, total 489650, tk/s 580.53 Markdown Table: | overall | computer science | | ------- | ---------------- | | 83.90 | 83.90 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is super basic out of the box stuff really. I see loads of warnings in the vLLM startup for things that need to be optimised.&lt;/p&gt; &lt;p&gt;vLLM runtime args (Primary 3090Ti only):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm serve cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit --tensor-parallel-size 1 --max-model-len 40960 --max-num-seqs 16 --served-model-name Qwen3-30B-A3B-Thinking-2507-AWQ-4bit &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;During the run, the vLLM console would show things like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;(APIServer pid=23678) INFO 09-15 01:20:40 [loggers.py:123] Engine 000: Avg prompt throughput: 1117.7 tokens/s, Avg generation throughput: 695.3 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 79.9%, Prefix cache hit rate: 79.5% (APIServer pid=23678) INFO: 127.0.0.1:52368 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52370 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52368 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52322 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52368 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52268 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO 09-15 01:20:50 [loggers.py:123] Engine 000: Avg prompt throughput: 919.6 tokens/s, Avg generation throughput: 687.4 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 88.9%, Prefix cache hit rate: 79.2% (APIServer pid=23678) INFO: 127.0.0.1:52278 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52370 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52268 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52322 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52278 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52268 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO: 127.0.0.1:52370 - &amp;quot;POST /v1/chat/completions HTTP/1.1&amp;quot; 200 OK (APIServer pid=23678) INFO 09-15 01:21:00 [loggers.py:123] Engine 000: Avg prompt throughput: 1072.6 tokens/s, Avg generation throughput: 674.5 tokens/s, Running: 16 reqs, Waiting: 0 reqs, GPU KV cache usage: 90.3%, Prefix cache hit rate: 79.1% &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I did do a small bit of benchmarking before this run as I have 2 x 3090Ti but one sits in a crippled x1 slot. 16 threads seems like the sweet spot. At 32 threads MMLU-Pro correct answer rate nose dived.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Single request&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# 1 parallel request - primary card - 512 prompt Throughput: 1.14 requests/s, 724.81 total tokens/s, 145.42 output tokens/s Total num prompt tokens: 50997 Total num output tokens: 12800 (vllm_env) tests@3090Ti:~$ vllm bench throughput --model cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit --tensor-parallel-size 1 --max-model-len 32768 --max-num-seqs 1 --input-len 512 --num-prompts 100 # 1 parallel request - both cards - 512 prompt Throughput: 0.71 requests/s, 453.38 total tokens/s, 90.96 output tokens/s Total num prompt tokens: 50997 Total num output tokens: 12800 (vllm_env) tests@3090Ti:~$ vllm bench throughput --model cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit --tensor-parallel-size 2 --max-model-len 32768 --max-num-seqs 1 --input-len 512 --num-prompts 100 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;8 requests&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# 8 parallel requests - primary card Throughput: 4.17 requests/s, 2660.79 total tokens/s, 533.85 output tokens/s Total num prompt tokens: 50997 Total num output tokens: 12800 (vllm_env) tests@3090Ti:~$ vllm bench throughput --model cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit --tensor-parallel-size 1 --max-model-len 32768 --max-num-seqs 8 --input-len 512 --num-prompts 100 # 8 parallel requests - both cards Throughput: 2.02 requests/s, 1289.21 total tokens/s, 258.66 output tokens/s Total num prompt tokens: 50997 Total num output tokens: 12800 (vllm_env) tests@3090Ti:~$ vllm bench throughput --model cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit --tensor-parallel-size 2 --max-model-len 32768 --max-num-seqs 8 --input-len 512 --num-prompts 100 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;16, 32, 64 requests - primary only&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# 16 parallel requests - primary card - 100 prompts Throughput: 5.69 requests/s, 3631.00 total tokens/s, 728.51 output tokens/s Total num prompt tokens: 50997 Total num output tokens: 12800 (vllm_env) tests@3090Ti:~$ vllm bench throughput --model cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit --tensor-parallel-size 1 --max-model-len 32768 --max-num-seqs 16 --input-len 512 --num-prompts 100 # 32 parallel requests - primary card - 200 prompts (100 was completing too fast it seemed) Throughput: 7.27 requests/s, 4643.05 total tokens/s, 930.81 output tokens/s Total num prompt tokens: 102097 Total num output tokens: 25600 (vllm_env) tests@3090Ti:~$ vllm bench throughput --model cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit --tensor-parallel-size 1 --max-model-len 32768 --max-num-seqs 32 --input-len 512 --num-prompts 200 # 64 parallel requests - primary card - 200 prompts Throughput: 8.54 requests/s, 5454.48 total tokens/s, 1093.48 output tokens/s Total num prompt tokens: 102097 Total num output tokens: 25600 (vllm_env) tests@3090Ti:~$ vllm bench throughput --model cpatonn/Qwen3-30B-A3B-Thinking-2507-AWQ-4bit --tensor-parallel-size 1 --max-model-len 32768 --max-num-seqs 64 --input-len 512 --num-prompts 200 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Reflection409"&gt; /u/Secure_Reflection409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh86i7/vllm_is_kinda_awesome/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh86i7/vllm_is_kinda_awesome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nh86i7/vllm_is_kinda_awesome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T00:58:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhd7x9</id>
    <title>I made a game using LLMs (gpt-oss:20b) -- Among LLMs: You are the Impostor</title>
    <updated>2025-09-15T05:15:20+00:00</updated>
    <author>
      <name>/u/Foreign_Radio8864</name>
      <uri>https://old.reddit.com/user/Foreign_Radio8864</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhd7x9/i_made_a_game_using_llms_gptoss20b_among_llms_you/"&gt; &lt;img alt="I made a game using LLMs (gpt-oss:20b) -- Among LLMs: You are the Impostor" src="https://preview.redd.it/g8fa7j6rf9pf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e636d781f5602359f2441244fae11049afeee2ae" title="I made a game using LLMs (gpt-oss:20b) -- Among LLMs: You are the Impostor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Note: &lt;strong&gt;Reposting this&lt;/strong&gt; because my account I used for the same &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nfrzbv/i_made_a_game_using_llms_gptoss20b_among_llms_you/"&gt;earlier post &lt;/a&gt;here got &lt;strong&gt;banned from Reddit&lt;/strong&gt; for no apparent reason and I'm not even allowed to login from it now. I hope this is fine.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I made this game in Python (that uses &lt;strong&gt;Ollama&lt;/strong&gt; and &lt;strong&gt;local&lt;/strong&gt; `gpt-oss:20b` / `gpt-oss:120b` models) that runs directly inside your terminal. &lt;strong&gt;Perfect for people who love drama and would love to start fights between AI bots&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github&lt;/strong&gt; &lt;strong&gt;link&lt;/strong&gt; at the end.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Among LLMs turns your &lt;strong&gt;terminal&lt;/strong&gt; into a chaotic chatroom playground where &lt;strong&gt;you’re the only human&lt;/strong&gt; &lt;strong&gt;among a bunch of eccentric AI agents&lt;/strong&gt;, dropped into a common &lt;em&gt;scenario&lt;/em&gt; -- it could be Fantasy, Sci-Fi, Thriller, Crime, or something completely unexpected. Each participant, including you, has a &lt;em&gt;persona&lt;/em&gt; and a &lt;em&gt;backstory&lt;/em&gt;, and all the AI agents share one common goal -- determine and eliminate the human, through &lt;em&gt;voting&lt;/em&gt;. &lt;strong&gt;Your mission: stay hidden, manipulate conversations, and turn the bots against each other with edits, whispers, impersonations, and clever gaslighting&lt;/strong&gt;. Outlast everyone, turn chaos to your advantage, and make it to the final two.&lt;/p&gt; &lt;p&gt;Can you survive the hunt and &lt;em&gt;outsmart&lt;/em&gt; the AI?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I didn't expect that my same &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nfrzbv/i_made_a_game_using_llms_gptoss20b_among_llms_you/"&gt;earlier post&lt;/a&gt; would be received so well in this community and I have &lt;strong&gt;implemented few suggestions&lt;/strong&gt; that I received in my post:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can control the speed of the responses via config file now (no more spammy responses)&lt;/li&gt; &lt;li&gt;You can now use multiple models per-agent (currently &lt;em&gt;experimental&lt;/em&gt; and WIP; Not fully integrated into the UI)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Quick Demo&lt;/strong&gt;: &lt;a href="https://youtu.be/kbNe9WUQe14"&gt;https://youtu.be/kbNe9WUQe14&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github&lt;/strong&gt;: &lt;a href="https://github.com/0xd3ba/among-llms"&gt;https://github.com/0xd3ba/among-llms&lt;/a&gt; (refer to `develop` branch for latest updates)&lt;/p&gt; &lt;h1&gt;Example of a Chatroom Inside the Game&lt;/h1&gt; &lt;p&gt;You can &lt;strong&gt;export your chatroom as JSON files&lt;/strong&gt; anytime during the chatroom and &lt;strong&gt;resume it later on by loading it&lt;/strong&gt;. Similarly, you can load other's JSON files as well. What's more, when you export it, the chat is exported as text file as well. Here's an example of a chat that I recently had inside a Sci-Fi chatroom, to give you an idea of how it is, using Among LLMs:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example Chatroom&lt;/strong&gt;: &lt;a href="https://pastebin.com/ud7mYmH4"&gt;https://pastebin.com/ud7mYmH4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note(s):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Might be lengthy, but you'll get the idea of how these bots behave (lol)&lt;/li&gt; &lt;li&gt;All agents have personas and backstories, which are not visible in the exported chat&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Foreign_Radio8864"&gt; /u/Foreign_Radio8864 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g8fa7j6rf9pf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhd7x9/i_made_a_game_using_llms_gptoss20b_among_llms_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhd7x9/i_made_a_game_using_llms_gptoss20b_among_llms_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T05:15:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhx3jp</id>
    <title>What’s the most cost-effective and best AI model for coding in your experience?</title>
    <updated>2025-09-15T20:25:39+00:00</updated>
    <author>
      <name>/u/Mammoth-Leopard6549</name>
      <uri>https://old.reddit.com/user/Mammoth-Leopard6549</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; I’m curious to hear from developers here: which AI model do you personally find the most cost-effective and reliable for coding tasks?&lt;/p&gt; &lt;p&gt;I know it can depend a lot on use cases (debugging, writing new code, learning, pair programming, etc.), but I’d love to get a sense of what actually works well for you in real projects.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Which model do you use the most?&lt;/li&gt; &lt;li&gt;Do you combine multiple models depending on the task?&lt;/li&gt; &lt;li&gt;If you pay for one, do you feel the price is justified compared to free or open-source options?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I think it’d be really helpful to compare experiences across the community, so please share your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mammoth-Leopard6549"&gt; /u/Mammoth-Leopard6549 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhx3jp/whats_the_most_costeffective_and_best_ai_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhx3jp/whats_the_most_costeffective_and_best_ai_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhx3jp/whats_the_most_costeffective_and_best_ai_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T20:25:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhxlqe</id>
    <title>Is there a newer large corpus of synthetic training data than Cosmopedia v2?</title>
    <updated>2025-09-15T20:44:30+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hoard models and datasets, but am usually limited by my crappy rural home DSL. I'm currently taking advantage of a business trip to download my backlog of large models with someone else's fast internet connection (brought an empty 14TB hard drive with me to fill up and take home).&lt;/p&gt; &lt;p&gt;It's only been a day, and I have already downloaded my backlog of large models. Datasets are next. I've queued up a few TB which are downloading now.&lt;/p&gt; &lt;p&gt;I'm particularly interested in high-quality open source synthetic datasets, but already have copies of Cosmopedia and Cosmopedia v2 from &lt;a href="https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus"&gt;https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus&lt;/a&gt; at home, and various smaller datasets.&lt;/p&gt; &lt;p&gt;Cosmopedia v2 is a year old already, and I'm wondering if anyone can suggest a few newer, high quality synthetic corpus I should nab while I still have access to the faster internet.&lt;/p&gt; &lt;p&gt;I'm particularly interested in open source physics-oriented STEM datasets, persuasion skill datasets, and datasets which have undergone multiple rounds of improvement (complexifying / rarifying via Evol-Instruct, Self-Critique, reward model scoring, and similar techniques). Especially if they have associated open source software repositories, papers, and permissible licenses.&lt;/p&gt; &lt;p&gt;If you have suggestions, I'd love to see them!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhxlqe/is_there_a_newer_large_corpus_of_synthetic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhxlqe/is_there_a_newer_large_corpus_of_synthetic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhxlqe/is_there_a_newer_large_corpus_of_synthetic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T20:44:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhxsg5</id>
    <title>For inference, I'm looking for help to navigate hardware that would support inference across 3 RTX 3090s with the ability to expand to 4 later.</title>
    <updated>2025-09-15T20:51:31+00:00</updated>
    <author>
      <name>/u/fkih</name>
      <uri>https://old.reddit.com/user/fkih</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm finding a lot of conflicting information across Reddit, and the scene/meta seems to move so fast! So I apologize if y'all get a &lt;em&gt;ton&lt;/em&gt; of these kind of questions.&lt;/p&gt; &lt;p&gt;With that said, I've got my FormD TD1 with a mini ITX build inside that I used to use as a gaming PC, but I have since recommissioned it as a home lab. I've had a blast coming up with applications for local LLMs to manage use-cases across the system.&lt;/p&gt; &lt;p&gt;I found someone selling used RTX 3090 FEs locally for C$750 a pop, so I bought all three they were selling at the time after stress testing and benchmarking all of them. Everything checked out.&lt;/p&gt; &lt;p&gt;I have since replaced the RTX 4080 inside with one of them, but obviously I want to leverage all of them. The seller is selling one more as well, so I'd like to see about picking up the fourth - but I've decided to hold off until I've confirmed other components.&lt;/p&gt; &lt;p&gt;My goal is to get the RTX 4080 back in the PC, and come up with a separate build around the GPUs, and I'm having a little bit of a tough time navigating the (niche) information online relating to running a similar setup. Particularly the motherboard &amp;amp; CPU combination. I'd appreciate any insight or pointers for a starting point.&lt;/p&gt; &lt;p&gt;No budget, but I'd like to spend mindfully rather than for the sake of spending. I'm totally okay looking into server hardware.&lt;/p&gt; &lt;p&gt;Thanks so much in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fkih"&gt; /u/fkih &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhxsg5/for_inference_im_looking_for_help_to_navigate/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhxsg5/for_inference_im_looking_for_help_to_navigate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhxsg5/for_inference_im_looking_for_help_to_navigate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T20:51:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhh1v1</id>
    <title>Engineer's Guide to Local LLMs with LLaMA.cpp and QwenCode on Linux</title>
    <updated>2025-09-15T09:20:37+00:00</updated>
    <author>
      <name>/u/Limp_Classroom_2645</name>
      <uri>https://old.reddit.com/user/Limp_Classroom_2645</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhh1v1/engineers_guide_to_local_llms_with_llamacpp_and/"&gt; &lt;img alt="Engineer's Guide to Local LLMs with LLaMA.cpp and QwenCode on Linux" src="https://external-preview.redd.it/dssHtWLh-9QdeM9_9eziiNbmQKbtVyxezJ3v7r7eNmc.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6ed393b15488f0c723126e979b2c6e17807c6ee" title="Engineer's Guide to Local LLMs with LLaMA.cpp and QwenCode on Linux" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;In this write up I will share my local AI setup on Ubuntu that I use for my personal projects as well as professional workflows (local chat, agentic workflows, coding agents, data analysis, synthetic dataset generation, etc).&lt;/p&gt; &lt;p&gt;This setup is particularly useful when I want to generate large amounts of synthetic datasets locally, process large amounts of sensitive data with LLMs in a safe way, use local agents without sending my private data to third party LLM providers, or just use chat/RAGs in complete privacy.&lt;/p&gt; &lt;h1&gt;What you'll learn&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Compile &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;LlamaCPP&lt;/a&gt; on your machine, set it up in your PATH, keep it up to date (compiling from source allows to use the bleeding edge version of llamacpp so you can always get latest features as soon as they are merged into the master branch)&lt;/li&gt; &lt;li&gt;Use llama-server to serve local models with very fast inference speeds&lt;/li&gt; &lt;li&gt;Setup llama-swap to automate model swapping on the fly and use it as your OpenAI compatible API endpoint.&lt;/li&gt; &lt;li&gt;Use systemd to setup llama-swap as a service that boots with your system and automatically restarts when the server config file changes&lt;/li&gt; &lt;li&gt;Integrate local AI in Agent Mode into your terminal with &lt;a href="https://github.com/QwenLM/qwen-code"&gt;QwenCode/OpenCode&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Test some local agentic workflows in Python with &lt;a href="https://www.crewai.com/"&gt;CrewAI&lt;/a&gt; (Part II)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I will also share what models I use for different types of workflows and different advanced configurations for each model (context expansion, parallel batch inference, multi modality, embedding, rereanking, and more.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;This will be a technical write up, and I will skip some things like installing and configuring basic build tools, &lt;a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/"&gt;CUDA toolkit installation&lt;/a&gt;, git, etc, if I do miss some steps that where not obvious to setup, or something doesn't work from your end, please let me know in the comments, I will gladly help you out, and progressively update the article with new information and more details as more people complain about specific aspects of the setup process.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Hardware&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;RTX3090 Founders Edition 24GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;The more VRAM you have the larger models you can load, but if you don't have the same GPU as long at it's an NVIDIA GPU it's fine, you can still load smaller models, just don't expect good agentic and tool usage results from smaller LLMs.&lt;/p&gt; &lt;p&gt;RTX3090 can load a Q5 quantized 30B Qwen3 model entirely into VRAM, with up to 140t/s as inference speed and 24k tokens context window (or up 110K tokens with some flash attention magic)&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Prerequisites&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Experience with working on a Linux Dev Box&lt;/li&gt; &lt;li&gt;&lt;a href="https://ubuntu.com/download"&gt;Ubuntu 24 or 25&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://documentation.ubuntu.com/server/how-to/graphics/install-nvidia-drivers/"&gt;NVIDIA proprietary drivers installed (version 580 at the time of writing)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;amp;target_arch=x86_64&amp;amp;Distribution=Ubuntu&amp;amp;target_version=24.04&amp;amp;target_type=deb_local"&gt;CUDA toolking installed&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Linux build tools + Git installed and configured&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Architecture&lt;/h1&gt; &lt;p&gt;Here is a rough overview of the architecture we will be setting up:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b7i8qycuqapf1.png?width=2278&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7a13c62c675f024cf7db25006b47caf565515b1d"&gt;https://preview.redd.it/b7i8qycuqapf1.png?width=2278&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7a13c62c675f024cf7db25006b47caf565515b1d&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Installing and setting up Llamacpp&lt;/h1&gt; &lt;p&gt;LlamaCpp is a very fast and flexible inference engine, it will allow us to run LLMs in GGUF format locally.&lt;/p&gt; &lt;p&gt;Clone the repo:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone git@github.com:ggml-org/llama.cpp.git &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;cd into the repo:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd llama.cpp &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;compile llamacpp for CUDA:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cmake -B build -DGGML_CUDA=ON -DBUILD_SHARED_LIBS=OFF -DLLAMA_CURL=ON -DGGML_CUDA_FA_ALL_QUANTS=ON &lt;/code&gt;&lt;/pre&gt; &lt;blockquote&gt; &lt;p&gt;If you have a different GPU, checkout the build guide &lt;a href="https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;pre&gt;&lt;code&gt;cmake --build build --config Release -j --clean-first &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will create llama.cpp binaries in &lt;code&gt;build/bin&lt;/code&gt; folder.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;To update llamacpp to bleeding edge just pull the lastes changes from the master branch with &lt;code&gt;git pull origin master&lt;/code&gt; and run the same commands to recompile&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Add llamacpp to PATH&lt;/h1&gt; &lt;p&gt;Depending on your shell, add the following to you bashrc or zshrc config file so we can execute llamacpp binaries in the terminal&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export LLAMACPP=[PATH TO CLONED LLAMACPP FOLDER] export PATH=$LLAMACPP/build/bin:$PATH &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Test that everything works correctly:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server --help &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output should look like this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8m2w9yr5rapf1.png?width=902&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=374b0dfcd1b5791a4a6a6e14c76ebd99e526a96b"&gt;https://preview.redd.it/8m2w9yr5rapf1.png?width=902&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=374b0dfcd1b5791a4a6a6e14c76ebd99e526a96b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Test that inference is working correctly:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-cli -hf ggml-org/gemma-3-1b-it-GGUF &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4mw5gz28rapf1.png?width=1402&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51a98932f0b00fd676a1a92c2c75df954d4ed845"&gt;https://preview.redd.it/4mw5gz28rapf1.png?width=1402&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=51a98932f0b00fd676a1a92c2c75df954d4ed845&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Great! now that we can do inference, let move on to setting up llama swap&lt;/p&gt; &lt;h1&gt;Installing and setting up llama swap&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/mostlygeek/llama-swap"&gt;llama-swap&lt;/a&gt; is a light weight, proxy server that provides automatic model swapping to llama.cpp's server. It will automate the model loading and unloading through a special configuration file and provide us with an openai compatible REST API endpoint.&lt;/p&gt; &lt;h1&gt;Download and install&lt;/h1&gt; &lt;p&gt;Download the latest version from the releases page:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/mostlygeek/llama-swap/releases"&gt;https://github.com/mostlygeek/llama-swap/releases&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;(look for &lt;code&gt;llama-swap_159_linux_amd64.tar.gz&lt;/code&gt; )&lt;/p&gt; &lt;p&gt;Unzip the downloaded archive and put the &lt;code&gt;llama-swap&lt;/code&gt; executable somewhere in your home folder (eg: &lt;code&gt;~/llama-swap/bin/llama-swap&lt;/code&gt;)&lt;/p&gt; &lt;p&gt;Add it to your path :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export PATH=$HOME/llama-swap/bin:$PATH &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;create an empty (for now) config file file in &lt;code&gt;~/llama-swap/config.yaml&lt;/code&gt;&lt;/p&gt; &lt;p&gt;test the executable&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-swap --help&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5pb4i3uarapf1.png?width=567&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44e0c75adc87f071e53c10101f7c2c71f8f80956"&gt;https://preview.redd.it/5pb4i3uarapf1.png?width=567&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44e0c75adc87f071e53c10101f7c2c71f8f80956&lt;/a&gt;&lt;/p&gt; &lt;p&gt;![Image description](&lt;a href="https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kl6iqatvejkec03eeaef.png"&gt;https://dev-to-uploads.s3.amazonaws.com/uploads/articles/kl6iqatvejkec03eeaef.png&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Before setting up llama-swap configuration we first need to download a few GGUF models .&lt;/p&gt; &lt;p&gt;To get started, let's download qwen3-4b and gemma gemma3-4b&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ggml-org/Qwen3-4B-GGUF/blob/main/Qwen3-4B-Q8_0.gguf"&gt;https://huggingface.co/ggml-org/Qwen3-4B-GGUF/blob/main/Qwen3-4B-Q8_0.gguf&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/ggml-org/gemma-3-4b-it-GGUF/blob/main/gemma-3-4b-it-Q8_0.gguf"&gt;https://huggingface.co/ggml-org/gemma-3-4b-it-GGUF/blob/main/gemma-3-4b-it-Q8_0.gguf&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Download and put the GGUF files in the following folder structure&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/models ├── google │ └── Gemma3-4B │ └── Qwen3-4B-Q8_0.gguf └── qwen └── Qwen3-4B └── gemma-3-4b-it-Q8_0.gguf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now that we have some ggufs, let's create a llama-swap config file.&lt;/p&gt; &lt;h1&gt;Llama Swap config file&lt;/h1&gt; &lt;p&gt;Our llama swap config located in &lt;code&gt;~/llama-swap/config.yaml&lt;/code&gt; will look like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;macros: &amp;quot;Qwen3-4b-macro&amp;quot;: &amp;gt; llama-server \ --port ${PORT} \ -ngl 80 \ --ctx-size 8000 \ --temp 0.7 \ --top-p 0.8 \ --top-k 20 \ --min-p 0 \ --repeat-penalty 1.05 \ --no-webui \ --timeout 300 \ --flash-attn on \ --jinja \ --alias Qwen3-4b \ -m /home/[YOUR HOME FOLDER]/models/qwen/Qwen3-4B/Qwen3-4B-Q8_0.gguf &amp;quot;Gemma-3-4b-macro&amp;quot;: &amp;gt; llama-server \ --port ${PORT} \ -ngl 80 \ --top-p 0.95 \ --top-k 64 \ --no-webui \ --timeout 300 \ --flash-attn on \ -m /home/[YOUR HOME FOLDER]/models/google/Gemma3-4B/gemma-3-4b-it-Q8_0.gguf models: &amp;quot;Qwen3-4b&amp;quot;: # &amp;lt;-- this is your model ID when calling the REST API cmd: | ${Qwen3-4b-macro} ttl: 3600 &amp;quot;Gemma3-4b&amp;quot;: cmd: | ${Gemma-3-4b-macro} ttl: 3600 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Start llama-swap&lt;/h1&gt; &lt;p&gt;Now we can start llama-swap with the following command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-swap --listen 0.0.0.0:8083 --config ~/llama-swap/config.yaml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can access llama-swap UI at: &lt;a href="http://localhost:8083"&gt;http://localhost:8083&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mkme3jsdrapf1.png?width=1669&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=841e19ee3a9bba041fa3037f9dd3f46d6a121202"&gt;https://preview.redd.it/mkme3jsdrapf1.png?width=1669&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=841e19ee3a9bba041fa3037f9dd3f46d6a121202&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here you can see all configured models, you can also load or unload them manually.&lt;/p&gt; &lt;h1&gt;Inference&lt;/h1&gt; &lt;p&gt;Let's do some inference via llama-swap REST API completions endpoint&lt;/p&gt; &lt;p&gt;Calling Qwen3:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl -X POST http://localhost:8083/v1/chat/completions \ -H &amp;quot;Content-Type: application/json&amp;quot; \ -d '{ &amp;quot;messages&amp;quot;: [ { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;hello&amp;quot; } ], &amp;quot;stream&amp;quot;: false, &amp;quot;model&amp;quot;: &amp;quot;Qwen3-4b&amp;quot; }' | jq &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Calling Gemma3:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl -X POST http://localhost:8083/v1/chat/completions \ -H &amp;quot;Content-Type: application/json&amp;quot; \ -d '{ &amp;quot;messages&amp;quot;: [ { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;hello&amp;quot; } ], &amp;quot;stream&amp;quot;: false, &amp;quot;model&amp;quot;: &amp;quot;Gemma3-4b&amp;quot; }' | jq &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should see a response from the server that looks something like this, and llamaswap will automatically load the correct model into the memory with each request:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; &amp;quot;choices&amp;quot;: [ { &amp;quot;finish_reason&amp;quot;: &amp;quot;stop&amp;quot;, &amp;quot;index&amp;quot;: 0, &amp;quot;message&amp;quot;: { &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Hello! How can I assist you today? 😊&amp;quot; } } ], &amp;quot;created&amp;quot;: 1757877832, &amp;quot;model&amp;quot;: &amp;quot;Qwen3-4b&amp;quot;, &amp;quot;system_fingerprint&amp;quot;: &amp;quot;b6471-261e6a20&amp;quot;, &amp;quot;object&amp;quot;: &amp;quot;chat.completion&amp;quot;, &amp;quot;usage&amp;quot;: { &amp;quot;completion_tokens&amp;quot;: 12, &amp;quot;prompt_tokens&amp;quot;: 9, &amp;quot;total_tokens&amp;quot;: 21 }, &amp;quot;id&amp;quot;: &amp;quot;chatcmpl-JgolLnFcqEEYmMOu18y8dDgQCEx9PAVl&amp;quot;, &amp;quot;timings&amp;quot;: { &amp;quot;cache_n&amp;quot;: 8, &amp;quot;prompt_n&amp;quot;: 1, &amp;quot;prompt_ms&amp;quot;: 26.072, &amp;quot;prompt_per_token_ms&amp;quot;: 26.072, &amp;quot;prompt_per_second&amp;quot;: 38.35532371893219, &amp;quot;predicted_n&amp;quot;: 12, &amp;quot;predicted_ms&amp;quot;: 80.737, &amp;quot;predicted_per_token_ms&amp;quot;: 6.728083333333333, &amp;quot;predicted_per_second&amp;quot;: 148.63073931406916 } } &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Optional: Adding llamaswap as systemd service and setup auto restart when config file changes&lt;/h1&gt; &lt;p&gt;If you don't want to manually run the llama-swap command everytime you turn on your workstation or manually reload the llama-swap server when you change your config you can leverage systemd to automate that away, create the following files:&lt;/p&gt; &lt;p&gt;Llamaswap service unit (if you are not using zsh adapt the &lt;code&gt;ExecStart&lt;/code&gt; accordingly)&lt;/p&gt; &lt;p&gt;&lt;code&gt;~/.config/systemd/user/llama-swap.service&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Unit] Description=Llama Swap Server After=multi-user.target [Service] Type=simple ExecStart=/usr/bin/zsh -l -c &amp;quot;source ~/.zshrc &amp;amp;&amp;amp; llama-swap --listen 0.0.0.0:8083 --config ~/llama-swap/config.yaml&amp;quot; WorkingDirectory=%h StandardOutput=journal StandardError=journal Restart=always RestartSec=5 [Install] WantedBy=multi-user.target &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Llamaswap restart service unit&lt;/p&gt; &lt;p&gt;&lt;code&gt;~/.config/systemd/user/llama-swap-restart.service&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Unit] Description=Restart llama-swap service After=llama-swap.service [Service] Type=oneshot ExecStart=/usr/bin/systemctl --user restart llama-swap.service &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Llamaswap path unit (will allow to monitor changes in the llama-swap config file and call the restart service whenever the changes are detected):&lt;/p&gt; &lt;p&gt;&lt;code&gt;~/.config/systemd/user/llama-swap-config.path&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[Unit] Description=Monitor llamaswap config file for changes After=multi-user.target [Path] # Monitor the specific file for modifications PathModified=%h/llama-swap/config.yaml Unit=llama-swap-restart.service [Install] WantedBy=default.target &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Enable and start the units:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo systemctl daemon-reload systemctl --user enable llama-swap-restart.service llama-swap.service llama-swap-config.path systemctl --user start llama-swap.service &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Check that the service is running correctly:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;systemctl --user status llama-swap.service &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Monitor llamaswap server logs:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;journalctl --user -u llama-swap.service -f &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Whenever the llama swap config is updated, the llamawap proxy server will automatically restart, you can verify it by monitoring the logs and making an update to the config file.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;If were able to get this far, congrats, you can start downloading and configuring your own models and setting up your own config, you can draw some inspiration from my config available here: &lt;a href="https://gist.github.com/avatsaev/dc302228e6628b3099cbafab80ec8998"&gt;https://gist.github.com/avatsaev/dc302228e6628b3099cbafab80ec8998&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It contains some advanced configurations, like multi-modal inference, parallel inference on the same model, extending context length with flash attention and more&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Connecting QwenCode to local models&lt;/h1&gt; &lt;p&gt;Install &lt;a href="https://github.com/QwenLM/qwen-code"&gt;QwenCode&lt;/a&gt; And let's use it with &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;Qwen3 Coder 30B Instruct&lt;/a&gt; locally (I recommend having at least 24GB of VRAM for this one 😅)&lt;/p&gt; &lt;p&gt;Here is my llama swap config:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;macros: &amp;quot;Qwen3-Coder-30B-A3B-Instruct&amp;quot;: &amp;gt; llama-server \ --api-key qwen \ --port ${PORT} \ -ngl 80 \ --ctx-size 110000 \ --temp 0.7 \ --top-p 0.8 \ --top-k 20 \ --min-p 0 \ --repeat-penalty 1.05 \ --cache-type-k q8_0 \ --cache-type-v q8_0 \ --no-webui \ --timeout 300 \ --flash-attn on \ --alias Qwen3-coder-instruct \ --jinja \ -m ~/models/qwen/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL.gguf models: &amp;quot;Qwen3-coder&amp;quot;: cmd: | ${Qwen3-Coder-30B-A3B-Instruct} ttl: 3600 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'm using &lt;a href="https://docs.unsloth.ai/models/unsloth-dynamic-2.0-ggufs"&gt;Unsloth's Dynamic quants&lt;/a&gt; at Q4 with flash attention and extending the context window to 100k tokens (with --cache-type-k and --cache-type-v flags), this is right at the edge of 24GBs of vram of my RTX3090.&lt;/p&gt; &lt;p&gt;You can download qwen coder ggufs &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/tree/main"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For a test scenario let's create a very simple react app in typescript&lt;/p&gt; &lt;p&gt;Create an empty project folder &lt;code&gt;~/qwen-code-test&lt;/code&gt; Inside this folder create an &lt;code&gt;.env&lt;/code&gt; file with the following contents:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=&amp;quot;qwen&amp;quot; OPENAI_BASE_URL=&amp;quot;http://localhost:8083/v1&amp;quot; OPENAI_MODEL=&amp;quot;Qwen3-coder&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;cd into the test directory and start qwen code:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd ~/qwen-code-test qwen &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;make sure that the model is correctly set from your .env file:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f5xcdisxqapf1.png?width=1378&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7071da3aba42ddd38298e1c53be889ef8cef201"&gt;https://preview.redd.it/f5xcdisxqapf1.png?width=1378&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7071da3aba42ddd38298e1c53be889ef8cef201&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've installed &lt;a href="https://marketplace.visualstudio.com/items?itemName=qwenlm.qwen-code-vscode-ide-companion"&gt;Qwen Code Copmanion extenstion&lt;/a&gt; in VS Code for seamless integration with Qwen Code, and here are the results, a fully local coding agent running in VS Code 😁&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/zucJY57vm1Y"&gt;https://youtu.be/zucJY57vm1Y&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Limp_Classroom_2645"&gt; /u/Limp_Classroom_2645 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhh1v1/engineers_guide_to_local_llms_with_llamacpp_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhh1v1/engineers_guide_to_local_llms_with_llamacpp_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhh1v1/engineers_guide_to_local_llms_with_llamacpp_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T09:20:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhxmlp</id>
    <title>Experience with OS LLM's for agentic coding?</title>
    <updated>2025-09-15T20:45:23+00:00</updated>
    <author>
      <name>/u/Crafty-Wonder-7509</name>
      <uri>https://old.reddit.com/user/Crafty-Wonder-7509</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title suggest I'm wondering how OS LLMS like Kimi K2 (0905) and the new Deepseek or GLM 4.5 are doing for you in comparison to Claude Opus/Sonnet or Codex with ChatGPT?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Crafty-Wonder-7509"&gt; /u/Crafty-Wonder-7509 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhxmlp/experience_with_os_llms_for_agentic_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhxmlp/experience_with_os_llms_for_agentic_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhxmlp/experience_with_os_llms_for_agentic_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T20:45:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhz4dn</id>
    <title>Qwen-next - no gguf yet</title>
    <updated>2025-09-15T21:44:23+00:00</updated>
    <author>
      <name>/u/mgr2019x</name>
      <uri>https://old.reddit.com/user/mgr2019x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;does anyone know why llama.cpp has not implemented the new architecture yet?&lt;/p&gt; &lt;p&gt;I am not complaining, i am just wondering what the reason(s) might be. The feature request on github seems quite stuck to me.&lt;/p&gt; &lt;p&gt;Sadly there is no skill on my side, so i am not able to help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mgr2019x"&gt; /u/mgr2019x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhz4dn/qwennext_no_gguf_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhz4dn/qwennext_no_gguf_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhz4dn/qwennext_no_gguf_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T21:44:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhvxo7</id>
    <title>Looking for advice on finetuning an embedding modell</title>
    <updated>2025-09-15T19:43:08+00:00</updated>
    <author>
      <name>/u/CaptainSnackbar</name>
      <uri>https://old.reddit.com/user/CaptainSnackbar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhvxo7/looking_for_advice_on_finetuning_an_embedding/"&gt; &lt;img alt="Looking for advice on finetuning an embedding modell" src="https://preview.redd.it/imbfqj01tdpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=41ce23670265bb6c6cab59c2ab8ac854415baae3" title="Looking for advice on finetuning an embedding modell" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CaptainSnackbar"&gt; /u/CaptainSnackbar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/imbfqj01tdpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhvxo7/looking_for_advice_on_finetuning_an_embedding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhvxo7/looking_for_advice_on_finetuning_an_embedding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T19:43:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nh5fn0</id>
    <title>Spent 4 months building Unified Local AI Workspace - ClaraVerse v0.2.0 instead of just dealing with 5+ Local AI Setup like everyone else</title>
    <updated>2025-09-14T22:51:26+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh5fn0/spent_4_months_building_unified_local_ai/"&gt; &lt;img alt="Spent 4 months building Unified Local AI Workspace - ClaraVerse v0.2.0 instead of just dealing with 5+ Local AI Setup like everyone else" src="https://preview.redd.it/3bgm4ig3i7pf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=06b067861ce9011b554dcf16b3efc513bcd5b061" title="Spent 4 months building Unified Local AI Workspace - ClaraVerse v0.2.0 instead of just dealing with 5+ Local AI Setup like everyone else" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ClaraVerse v0.2.0 - Unified Local AI Workspace (Chat, Agent, ImageGen, Rag &amp;amp; N8N)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Spent 4 months building ClaraVerse instead of just using multiple AI apps like a normal person&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Posted here in April when it was pretty rough and got some reality checks from the community. Kept me going though - people started posting about it on YouTube and stuff.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The basic idea:&lt;/strong&gt; Everything's just LLMs and diffusion models anyway, so why do we need separate apps for everything? Built ClaraVerse to put it all in one place.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's actually working in v0.2.0:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Chat with local models (built-in llama.cpp) or any provider with MCP, Tools, N8N workflow as tools&lt;/li&gt; &lt;li&gt;Generate images with ComfyUI integration&lt;/li&gt; &lt;li&gt;Build agents with visual editor (drag and drop automation)&lt;/li&gt; &lt;li&gt;RAG notebooks with 3D knowledge graphs&lt;/li&gt; &lt;li&gt;N8N workflows for external stuff&lt;/li&gt; &lt;li&gt;Web dev environment (LumaUI)&lt;/li&gt; &lt;li&gt;Community marketplace for sharing workflows&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The modularity thing:&lt;/strong&gt; Everything connects to everything else. Your chat assistant can trigger image generation, agents can update your knowledge base, workflows can run automatically. It's like LEGO blocks but for AI tools.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Reality check:&lt;/strong&gt; Still has rough edges (it's only 4 months old). But 20k+ downloads and people are building interesting stuff with it, so the core idea seems to work.&lt;/p&gt; &lt;p&gt;Everything runs local, MIT licensed. Built-in llama.cpp with model downloads, manager but works with any provider.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt; GitHub: &lt;a href="http://github.com/badboysm890/ClaraVerse"&gt;github.com/badboysm890/ClaraVerse&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone tried building something similar? Curious if this resonates with other people or if I'm just weird about wanting everything in one app.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3bgm4ig3i7pf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nh5fn0/spent_4_months_building_unified_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nh5fn0/spent_4_months_building_unified_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-14T22:51:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhhmu6</id>
    <title>[AutoBE] built full-level backend applications with "qwen3-next-80b-a3b-instruct" model.</title>
    <updated>2025-09-15T09:57:11+00:00</updated>
    <author>
      <name>/u/jhnam88</name>
      <uri>https://old.reddit.com/user/jhnam88</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhhmu6/autobe_built_fulllevel_backend_applications_with/"&gt; &lt;img alt="[AutoBE] built full-level backend applications with &amp;quot;qwen3-next-80b-a3b-instruct&amp;quot; model." src="https://a.thumbs.redditmedia.com/sdfGHCDLTyQ9-yO-fXToFMJUqEorvpfj2vxTYQoMbv0.jpg" title="[AutoBE] built full-level backend applications with &amp;quot;qwen3-next-80b-a3b-instruct&amp;quot; model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Project&lt;/th&gt; &lt;th&gt;&lt;code&gt;qwen3-next-80b-a3b-instruct&lt;/code&gt;&lt;/th&gt; &lt;th&gt;&lt;code&gt;openai/gpt-4.1-mini&lt;/code&gt;&lt;/th&gt; &lt;th&gt;&lt;code&gt;openai/gpt-4.1&lt;/code&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;To Do List&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-todo-qwen-qwen3-next-80b-a3b-instruct"&gt;Qwen3 To Do&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-todo-openai-gpt-4.1-mini"&gt;GPT 4.1-mini To Do&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-todo-openai-gpt-4.1"&gt;GPT 4.1 To Do&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Reddit Community&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-reddit-qwen-qwen3-next-80b-a3b-instruct"&gt;Qwen3 Reddit&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-reddit-openai-gpt-4.1-mini"&gt;GPT 4.1-mini Reddit&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-reddit-openai-gpt-4.1"&gt;GPT 4.1 Reddit&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Economic Discussion&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-bbs-qwen-qwen3-next-80b-a3b-instruct"&gt;Qwen3 BBS&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-bbs-openai-gpt-4.1-mini"&gt;GPT 4.1-mini BBS&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-bbs-openai-gpt-4.1"&gt;GPT 4.1 BBS&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;E-Commerce&lt;/td&gt; &lt;td&gt;Qwen3 Failed&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-shopping-openai-gpt-4.1-mini"&gt;GPT 4.1-mini Shopping&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://github.com/wrtnlabs/autobe-example-shopping-openai-gpt-4.1"&gt;GPT 4.1 Shopping&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The AutoBE team recently tested the &lt;code&gt;qwen3-next-80b-a3b-instruct&lt;/code&gt; model and successfully generated three full-stack backend applications: To Do List, Reddit Community, and Economic Discussion Board.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;code&gt;qwen3-next-80b-a3b-instruct&lt;/code&gt; failed during the &lt;code&gt;realize&lt;/code&gt; phase, but this was due to our compiler development issues rather than the model itself. AutoBE improves backend development success rates by implementing AI-friendly compilers and providing compiler error feedback to AI agents.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;While some compilation errors remained during API logic implementation (realize phase), these were easily fixable manually, so we consider these successful cases. There are still areas for improvement—AutoBE generates relatively few e2e test functions (the Reddit community project only has 9 e2e tests for 60 API operations)—but we expect these issues to be resolved soon.&lt;/p&gt; &lt;p&gt;Compared to &lt;code&gt;openai/gpt-4.1-mini&lt;/code&gt; and &lt;code&gt;openai/gpt-4.1&lt;/code&gt;, the &lt;code&gt;qwen3-next-80b-a3b-instruct&lt;/code&gt; model generates fewer documents, API operations, and DTO schemas. However, in terms of cost efficiency, &lt;code&gt;qwen3-next-80b-a3b-instruct&lt;/code&gt; is significantly more economical than the other models. As AutoBE is an open-source project, we're particularly interested in leveraging open-source models like &lt;code&gt;qwen3-next-80b-a3b-instruct&lt;/code&gt; for better community alignment and accessibility.&lt;/p&gt; &lt;p&gt;For projects that don't require massive backend applications (like our e-commerce test case), &lt;code&gt;qwen3-next-80b-a3b-instruct&lt;/code&gt; is an excellent choice for building full-stack backend applications with AutoBE.&lt;/p&gt; &lt;p&gt;We AutoBE team are actively working on fine-tuning our approach to achieve 100% success rate with &lt;code&gt;qwen3-next-80b-a3b-instruct&lt;/code&gt; in the near future. We envision a future where backend application prototype development becomes fully automated and accessible to everyone through AI. Please stay tuned for what's coming next!&lt;/p&gt; &lt;h2&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AutoBE GitHub Repository:&lt;/strong&gt; &lt;a href="https://github.com/wrtnlabs/autobe"&gt;https://github.com/wrtnlabs/autobe&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; &lt;a href="https://autobe.dev/docs"&gt;https://autobe.dev/docs&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jhnam88"&gt; /u/jhnam88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nhhmu6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhhmu6/autobe_built_fulllevel_backend_applications_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhhmu6/autobe_built_fulllevel_backend_applications_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T09:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhqm7n</id>
    <title>Qwen2.5-VL 7B: Why is Hugging Face Inference more accurate/faster than my local run?</title>
    <updated>2025-09-15T16:28:41+00:00</updated>
    <author>
      <name>/u/Ok_Television_9000</name>
      <uri>https://old.reddit.com/user/Ok_Television_9000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been experimenting with &lt;strong&gt;Qwen2.5-VL 7B&lt;/strong&gt; for image-based data extraction (e.g. receipts).&lt;br /&gt; When I run it on the &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"&gt;Hugging Face Inference provider&lt;/a&gt;, the results are &lt;strong&gt;highly accurate and quite fast&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;But when I run the same model locally (16 GB VRAM, Q8 quantization, &lt;code&gt;max_new_tokens=512&lt;/code&gt;), the output is noticeably &lt;strong&gt;less accurate&lt;/strong&gt; (wrong digits/letters, small hallucinations) and much &lt;strong&gt;slower&lt;/strong&gt; (~3 tok/s despite FlashAttention 2 enabled)&lt;/p&gt; &lt;p&gt;I assume HF is running this on stronger GPUs behind the scenes, but I’m curious if there’s more to it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Do they wrap Qwen-VL with &lt;strong&gt;extra preprocessing/decoding constraints&lt;/strong&gt; (image normalization, capped &lt;code&gt;max_new_tokens&lt;/code&gt;, schema prompts, etc.)?&lt;/li&gt; &lt;li&gt;Or is the gap mainly my local setup (Q8 + large token budget), versus HF’s serving stack optimizations (fp16/bf16 tuning, TensorRT, fused kernels)?&lt;/li&gt; &lt;li&gt;Any practical tips for &lt;strong&gt;closing the accuracy/speed gap&lt;/strong&gt; locally?&lt;/li&gt; &lt;li&gt;Is it normal to not be able to fit FP32 of Qwen2.5-VL 7B into 16GB VRAM?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear from anyone who’s profiled or replicated these differences.&lt;/p&gt; &lt;p&gt;Edit: * Weights: INT8 (BitsAndBytesConfig(load_in_8bit=True)) * Compute &amp;amp; activations: FP16 (dtype=torch.float16). * I quantized to these values because without it, it kept getting offloaded to CPU. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Television_9000"&gt; /u/Ok_Television_9000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhqm7n/qwen25vl_7b_why_is_hugging_face_inference_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhqm7n/qwen25vl_7b_why_is_hugging_face_inference_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhqm7n/qwen25vl_7b_why_is_hugging_face_inference_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T16:28:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhpy35</id>
    <title>A lightweight and tunable python chat interface to interact with LLM, featuring persistent memory</title>
    <updated>2025-09-15T16:03:39+00:00</updated>
    <author>
      <name>/u/Vicouille6</name>
      <uri>https://old.reddit.com/user/Vicouille6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhpy35/a_lightweight_and_tunable_python_chat_interface/"&gt; &lt;img alt="A lightweight and tunable python chat interface to interact with LLM, featuring persistent memory" src="https://preview.redd.it/olzso2n2qcpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b502586d78f6bcec10593dca6d7a69c2f9f80094" title="A lightweight and tunable python chat interface to interact with LLM, featuring persistent memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I developed a lightweight Python tool that allows local LLM to maintain persistent memory, and I’m sharing it here.&lt;/p&gt; &lt;p&gt;Local models are great for privacy and offline use, but they typically lose all context between sessions unlike online services, as you all know.&lt;/p&gt; &lt;p&gt;Previously, I built a project that captured conversations from LM Studio and stored them in a database to enrich prompts sent to models. This new version is a direct chat interface (leveraging easy-llama by u/master-meal-77, many thanks to him) that makes the memory process completely seamless and invisible to the user.&lt;/p&gt; &lt;h1&gt;Key features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Fully local, no external API dependencies &lt;/li&gt; &lt;li&gt;Short-term and long-term memory for fluid conversations and contextually relevant responses -&lt;/li&gt; &lt;li&gt;Fully customizable depth of memory and model parameters &lt;/li&gt; &lt;li&gt;Workspaces to separate different projects &lt;/li&gt; &lt;li&gt;Built-in visualizations to track memory data and semantic indicators&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Upcoming developments:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Document support (PDF, Word, Excel, images) for targeted queries &lt;/li&gt; &lt;li&gt;Integrated web search to supplement local memory with the most recent information &lt;/li&gt; &lt;li&gt;Selective import/export of personal memory through workspaces for sharing within a team&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I think this project could be of interest to some users of this sub.&lt;/p&gt; &lt;p&gt;The code is here : &lt;a href="https://github.com/victorcarre6/LocalMind"&gt;GitHub repository&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to use it as you want and to share your feedback! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vicouille6"&gt; /u/Vicouille6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/olzso2n2qcpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhpy35/a_lightweight_and_tunable_python_chat_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhpy35/a_lightweight_and_tunable_python_chat_interface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T16:03:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhv42c</id>
    <title>NCSOFT/VARCO-VISION-2.0-14B · Hugging Face</title>
    <updated>2025-09-15T19:12:50+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhv42c/ncsoftvarcovision2014b_hugging_face/"&gt; &lt;img alt="NCSOFT/VARCO-VISION-2.0-14B · Hugging Face" src="https://external-preview.redd.it/Zqwx3E1Z_EElc-Wqav8y07fmTzCY9Mbf2KZGmL-cSJg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8273519f7df58dceaae1c21bddf409931659286d" title="NCSOFT/VARCO-VISION-2.0-14B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;p&gt;&lt;strong&gt;VARCO-VISION-2.0&lt;/strong&gt; is a multimodal AI model capable of understanding both images and text to answer user queries. It supports multi-image inputs, enabling effective processing of complex content such as documents, tables, and charts. The model demonstrates strong comprehension in both Korean and English, with significantly improved text generation capabilities and a deeper understanding of Korean cultural context. Compared to its predecessor, performance has been notably enhanced across various benchmarks, and its usability in real-world scenarios—such as everyday Q&amp;amp;A and information summarization—has also improved.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/NCSOFT/VARCO-VISION-2.0-14B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhv42c/ncsoftvarcovision2014b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhv42c/ncsoftvarcovision2014b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T19:12:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhdi2u</id>
    <title>Update: we got our revenge and now beat Deepmind, Microsoft, Zhipu AI and Alibaba</title>
    <updated>2025-09-15T05:32:12+00:00</updated>
    <author>
      <name>/u/Connect-Employ-4708</name>
      <uri>https://old.reddit.com/user/Connect-Employ-4708</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Three weeks ago we open-sourced our agent that uses mobile apps like a human. At that moment, we were #2 on AndroidWorld (behind Zhipu AI).&lt;/p&gt; &lt;p&gt;Since, we worked hard and improved the performance of our agent: &lt;strong&gt;we’re now officially #1&lt;/strong&gt; on the &lt;a href="https://docs.google.com/spreadsheets/d/1cchzP9dlTZ3WXQTfYNhh3avxoLipqHN75v1Tb86uhHo/edit?pli=1&amp;amp;gid=0#gid=0"&gt;AndroidWorld leaderboard&lt;/a&gt;, surpassing Deepmind, Microsoft Research, Zhipu AI and Alibaba.&lt;/p&gt; &lt;p&gt;It handles mobile tasks: booking rides, ordering food, navigating apps, just like a human would. Still working on improvements and building an RL gym for fine-tuning :)&lt;/p&gt; &lt;p&gt;The agent is completely open-source: &lt;a href="http://github.com/minitap-ai/mobile-use"&gt;github.com/minitap-ai/mobile-use&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What mobile tasks would you want an AI agent to handle for you? Always looking for feedback and contributors!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Connect-Employ-4708"&gt; /u/Connect-Employ-4708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhdi2u/update_we_got_our_revenge_and_now_beat_deepmind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhdi2u/update_we_got_our_revenge_and_now_beat_deepmind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhdi2u/update_we_got_our_revenge_and_now_beat_deepmind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T05:32:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhn5sy</id>
    <title>Testers w/ 4th-6th Generation Xeon CPUs wanted to test changes to llama.cpp</title>
    <updated>2025-09-15T14:19:58+00:00</updated>
    <author>
      <name>/u/DataGOGO</name>
      <uri>https://old.reddit.com/user/DataGOGO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,.&lt;/p&gt; &lt;p&gt;I have been working on improving AMX acceleration in llama.cpp. Currently, even if you have a a supported CPU and have built llama.cpp with all the required build flags, AMX acceleration is disabled if you have a GPU present.&lt;/p&gt; &lt;p&gt;I modified the way that llama.cpp exposes the &amp;quot;extra&amp;quot; CPU buffers so that AMX will remain functional in CPU/GPU hybrids, resulting in a 20-40% increase in performance for CPU offloaded layers / CPU offloaded experts.&lt;/p&gt; &lt;p&gt;Since I have limited hardware to test with I made a temporary fork and I am looking for testers make sure everything is good before I open a PR to roll the changes into mainline llama.cpp.&lt;/p&gt; &lt;p&gt;4th-6th Generation Xeons accelerations supported in hybrid: AVX-512VNNI, AMXInt8, AMXBF16&lt;/p&gt; &lt;p&gt;&lt;em&gt;Note: I have made the changes to AMX.cpp to implement AMXInt4, but since I don't have a 6th generation Xeon, I can't test it, so I left it out for now.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;To enable the new behavior you just place &amp;quot;--amx&amp;quot; in your launch command string, to revert to base behavior, just remove the &amp;quot;--amx&amp;quot; flag.&lt;/p&gt; &lt;p&gt;If you test please leave a comment in the discussions in the Github with your CPU/RAM/GPU hardware information and your results with and without the &amp;quot;--amx&amp;quot; flag using the example llama-bench and llama-cli commands (takes less that 1 min each) it would be very helpful. Feel free to include any other tests that you do, the more the better.&lt;/p&gt; &lt;p&gt;Huge thank you in advance!&lt;/p&gt; &lt;p&gt;Here is the github: Instructions and example commands are in the readme.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Gadflyii/llama.cpp"&gt;https://github.com/Gadflyii/llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataGOGO"&gt; /u/DataGOGO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhn5sy/testers_w_4th6th_generation_xeon_cpus_wanted_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhn5sy/testers_w_4th6th_generation_xeon_cpus_wanted_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhn5sy/testers_w_4th6th_generation_xeon_cpus_wanted_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T14:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhd5ks</id>
    <title>Completed 8xAMD MI50 - 256GB VRAM + 256GB RAM rig for $3k</title>
    <updated>2025-09-15T05:11:27+00:00</updated>
    <author>
      <name>/u/MLDataScientist</name>
      <uri>https://old.reddit.com/user/MLDataScientist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhd5ks/completed_8xamd_mi50_256gb_vram_256gb_ram_rig_for/"&gt; &lt;img alt="Completed 8xAMD MI50 - 256GB VRAM + 256GB RAM rig for $3k" src="https://b.thumbs.redditmedia.com/fnXtmv3xMQuxl9xhdyUEPfnB7lkOh7QN0YQdqGLHvKc.jpg" title="Completed 8xAMD MI50 - 256GB VRAM + 256GB RAM rig for $3k" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;A few months ago I posted about how I was able to purchase 4xMI50 for $600 and run them using my consumer PC. Each GPU could run at PCIE3.0 x4 speed and my consumer PC did not have enough PCIE lanes to support more than 6x GPUs. My final goal was to run all 8 GPUs at proper PCIE4.0 x16 speed. &lt;/p&gt; &lt;p&gt;I was finally able to complete my setup. Cost breakdown:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ASRock ROMED8-2T Motherboard with 8x32GB DDR4 3200Mhz and AMD Epyc 7532 CPU (32 cores), dynatron 2U heatsink - $1000&lt;/li&gt; &lt;li&gt;6xMI50 and 2xMI60 - $1500&lt;/li&gt; &lt;li&gt;10x blower fans (all for $60), 1300W PSU ($120) + 850W PSU (already had this), 6x 300mm riser cables (all for $150), 3xPCIE 16x to 8x8x bifurcation cards (all for $70), 8x PCIE power cables and fan power controller (for $100)&lt;/li&gt; &lt;li&gt;GTX 1650 4GB for video output (already had this)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In total, I spent around ~$3k for this rig. All used parts.&lt;/p&gt; &lt;p&gt;ASRock ROMED8-2T was an ideal motherboard for me due to its seven x16 full physical PCIE4.0 slots.&lt;/p&gt; &lt;p&gt;Attached photos below.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b052o7hi99pf1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=20fb34bd86438c2a2111fb0eb52a70b26b3b9685"&gt;8xMI50/60 32GB with GTX 1650 top view&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cnnr3ixn99pf1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=273be5463afc2508a46f17ea5e63b6e6de51b5fb"&gt;8xMI50/60 32GB in open frame rack with motherboard and PSU. My consumer PC is on the right side (not used here)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have not done many LLM tests yet. PCIE4.0 connection was not stable since I am using longer PCIE risers. So, I kept the speed for each PCIE slot at 3.0 x16. Some initial performance metrics are below. Installed Ubuntu 24.04.3 with ROCm 6.4.3 (needed to copy paste gfx906 tensiles to fix deprecated support).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU alone: gpt-oss 120B (65GB Q8) runs at ~25t/s with ~120t/s prompt processing (llama.cpp)&lt;/li&gt; &lt;li&gt;2xMI50: gpt-oss 120B (65GB Q8) runs at ~58t/s with 750t/s prompt processing (llama.cpp)&lt;/li&gt; &lt;li&gt;8xMI50: qwen3 235B Q4_1 runs at ~21t/s with 350t/s prompt processing (llama.cpp)&lt;/li&gt; &lt;li&gt;2xMI60 vllm gfx906: llama3.3 70B AWQ: 25t/s with ~240 t/s prompt processing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Idle power consumption is around ~400W (20w for each GPU, 15w for each blower fan, ~100W for motherboard, RAM, fan and CPU). llama.cpp inference averages around 750W (using wall meter). For a few seconds during inference, the power spikes up to 1100W&lt;/p&gt; &lt;p&gt;I will do some more performance tests. Overall, I am happy with what I was able to build and run. &lt;/p&gt; &lt;p&gt;Fun fact: the entire rig costs around the same price as a single RTX 5090 (variants like ASUS TUF).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLDataScientist"&gt; /u/MLDataScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhd5ks/completed_8xamd_mi50_256gb_vram_256gb_ram_rig_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhd5ks/completed_8xamd_mi50_256gb_vram_256gb_ram_rig_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhd5ks/completed_8xamd_mi50_256gb_vram_256gb_ram_rig_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T05:11:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhtv5f</id>
    <title>Some GPU (5090,4090,3090,A600) idle power consumption, headless on Linux (Fedora 42), and some undervolt/overclock info.</title>
    <updated>2025-09-15T18:27:22+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhtv5f/some_gpu_509040903090a600_idle_power_consumption/"&gt; &lt;img alt="Some GPU (5090,4090,3090,A600) idle power consumption, headless on Linux (Fedora 42), and some undervolt/overclock info." src="https://preview.redd.it/5difgej3fdpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f699faed3067a46c354771c3653e38f77a492e56" title="Some GPU (5090,4090,3090,A600) idle power consumption, headless on Linux (Fedora 42), and some undervolt/overclock info." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just an small post about some power consumption of those some GPUs if some people are interested.&lt;/p&gt; &lt;p&gt;As extra info, all the cards are both undervolted + power limited, but it shouldn't affect idle power consumption.&lt;/p&gt; &lt;p&gt;Undervolt was done with LACT, and they are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;3090s: 1875Mhz max core clock, +150Mhz core clock offset, +1700Mhz VRAM offset.&lt;/li&gt; &lt;li&gt;A6000: 1740Mhz max core clock, +150Mhz core clock offset, +2000 Mhz VRAM offset.&lt;/li&gt; &lt;li&gt;4090 (1): 2850Mhz max core clock, +150Mhz core clock offset, +2700Mhz VRAM.&lt;/li&gt; &lt;li&gt;4090 (2): 2805Mhz max core clock, +180Mhz core clock offset, +1700Mhz VRAM offset.&lt;/li&gt; &lt;li&gt;5090s: 3010Mhz max core clock, +1000Mhz core clock offset, +4400Mhz VRAM offset.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If someone wants to know how to use LACT just let me know, but I basically use SDDM (sudo systemctl start sddm), LACT for the GUI, set the values and then run&lt;/p&gt; &lt;p&gt;sudo a (it does nothing, but helps for the next command)&lt;br /&gt; (echo suspend | sudo tee /proc/driver/nvidia/suspend ;echo resume | sudo tee /proc/driver/nvidia/suspend)&amp;amp;&lt;/p&gt; &lt;p&gt;Then run sudo systemctl stop sddm.&lt;/p&gt; &lt;p&gt;This mostly puts the 3090s, A6000 and 4090 (2) at 0.9V. 4090 (1) is at 0.915V, and 5090s are at 0.895V.&lt;/p&gt; &lt;p&gt;Also this offset in VRAM is MT/s basically, so on Windows comparatively, it is half of that (+1700Mhz = +850Mhz on MSI Afterburner, +1800 = +900, +2700 = 1350, +4400 = +2200)&lt;/p&gt; &lt;p&gt;EDIT: Just as an info, maybe (not) surprisingly, the GPUs that idle at the lower power are the most efficient.&lt;/p&gt; &lt;p&gt;I.e. 5090 2 is more efficient than 5090 0, or 4090 6 is more efficient than 4090 1.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5difgej3fdpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhtv5f/some_gpu_509040903090a600_idle_power_consumption/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhtv5f/some_gpu_509040903090a600_idle_power_consumption/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T18:27:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhgd9k</id>
    <title>The GLM team dropped me a mail</title>
    <updated>2025-09-15T08:35:28+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhgd9k/the_glm_team_dropped_me_a_mail/"&gt; &lt;img alt="The GLM team dropped me a mail" src="https://preview.redd.it/hfmaz0jjiapf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81b836907f7915edb2fd118d974c6fffd4dad51b" title="The GLM team dropped me a mail" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In case you don't know, GLM 4.5 is one of the best open-sourced LLMs. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hfmaz0jjiapf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhgd9k/the_glm_team_dropped_me_a_mail/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhgd9k/the_glm_team_dropped_me_a_mail/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T08:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
