<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-04T18:25:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lrkeib</id>
    <title>Is fine tuning worth it?</title>
    <updated>2025-07-04T14:41:15+00:00</updated>
    <author>
      <name>/u/ManagementNo5153</name>
      <uri>https://old.reddit.com/user/ManagementNo5153</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have never fine tuned a model before, I want a model/agent to do financial analysis. Can someone help?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ManagementNo5153"&gt; /u/ManagementNo5153 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrkeib/is_fine_tuning_worth_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrkeib/is_fine_tuning_worth_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrkeib/is_fine_tuning_worth_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T14:41:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrkrqu</id>
    <title>12x3090s + 2x EPYC 7282 monstrously slow without full GPU offload</title>
    <updated>2025-07-04T14:57:06+00:00</updated>
    <author>
      <name>/u/cantgetthistowork</name>
      <uri>https://old.reddit.com/user/cantgetthistowork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to run V3 but when I try to offload to CPU to increase the context it slows to a crawl. Right now I can fit 16k context fully on GPU with the smallest UD quant but that's barely usable.&lt;/p&gt; &lt;p&gt;I understand that dual CPU setups have NUMA issues but even using threads=1 results in something like 1t/5s.&lt;/p&gt; &lt;p&gt;Super frustrated because I'm seeing single GPU setups run it blazing fast and wondering why bother with 3090s these days.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cantgetthistowork"&gt; /u/cantgetthistowork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrkrqu/12x3090s_2x_epyc_7282_monstrously_slow_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrkrqu/12x3090s_2x_epyc_7282_monstrously_slow_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrkrqu/12x3090s_2x_epyc_7282_monstrously_slow_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T14:57:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqqxhq</id>
    <title>I have made a True Reasoning LLM</title>
    <updated>2025-07-03T14:25:42+00:00</updated>
    <author>
      <name>/u/moilanopyzedev</name>
      <uri>https://old.reddit.com/user/moilanopyzedev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have created an LLM with my own custom architecture. My architecture uses self correction and Long term memory in vector states which makes it more stable and perform a bit better. And I used phi-3-mini for this project and after finetuning the model with the custom architecture it acheived 98.17% on HumanEval benchmark (you could recommend me other lightweight benchmarks for me) and I have made thee model open source &lt;/p&gt; &lt;p&gt;You can get it here&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/moelanoby/phi-3-M3-coder"&gt;https://huggingface.co/moelanoby/phi-3-M3-coder&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moilanopyzedev"&gt; /u/moilanopyzedev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqxhq/i_have_made_a_true_reasoning_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqxhq/i_have_made_a_true_reasoning_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqqxhq/i_have_made_a_true_reasoning_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T14:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrerwe</id>
    <title>pytorch 2.7.x no longer supports Pascal architecture?</title>
    <updated>2025-07-04T09:46:51+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got these warnings:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; /home/user/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:262: UserWarning: Found GPU0 NVIDIA GeForce GT 1030 which is of cuda capability 6.1. PyTorch no longer supports this GPU because it is too old. The minimum cuda capability supported by this library is 7.5. warnings.warn( /home/user/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:287: UserWarning: NVIDIA GeForce GT 1030 with CUDA capability sm_61 is not compatible with the current PyTorch installation. The current PyTorch install supports CUDA capabilities sm_75 sm_80 sm_86 sm_90 sm_100 sm_120 compute_120. If you want to use the NVIDIA GeForce GT 1030 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And then crash with this error:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;RuntimeError: CUDA error: no kernel image is available for execution on the device CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1 Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I tried the 2.7.0 with both cuda 12.6 and 12.8 and they both gave me this error. So I should stick with 2.6.0?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrerwe/pytorch_27x_no_longer_supports_pascal_architecture/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrerwe/pytorch_27x_no_longer_supports_pascal_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrerwe/pytorch_27x_no_longer_supports_pascal_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T09:46:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrhpl8</id>
    <title>Best iOS app with local OpenAI-like API endpoint?</title>
    <updated>2025-07-04T12:37:30+00:00</updated>
    <author>
      <name>/u/PardusHD</name>
      <uri>https://old.reddit.com/user/PardusHD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'll describe my ideal app on my phone for all my local LLM conversations:&lt;br /&gt; - native iOS app&lt;br /&gt; - OpenAI-like API endpoint (to connect to LM Studio on my local network, when I'm on the go using Tailscale to stay connected)&lt;br /&gt; - multimodal support: images, STT, TTS&lt;br /&gt; - conversation history easily exportable or synced&lt;br /&gt; - on-device models when fully offline &lt;/p&gt; &lt;p&gt;So far I've used these two apps successfully for local API endpoints, however they are not as polished with conversation history or multimodal support:&lt;br /&gt; - &lt;a href="https://apps.apple.com/at/app/pal-chat-ai-chat-client/id6447545085"&gt;&amp;quot;Pal Chat&amp;quot;&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://apps.apple.com/at/app/chatbox-free-ai-client/id6471368056"&gt;&amp;quot;Chatbox&amp;quot;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For on-device models:&lt;br /&gt; - &lt;a href="https://apps.apple.com/at/app/enclave-local-ai-assistant/id6476614556"&gt;&amp;quot;Enclave&amp;quot;&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://apps.apple.com/at/app/local-brain/id6476066730"&gt;&amp;quot;Local Brain&amp;quot;&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://apps.apple.com/at/app/pocket-local-ai-chatbot/id6476791661"&gt;&amp;quot;pocket&amp;quot;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now one that seems to incorporate both is paid: &lt;a href="https://apps.apple.com/at/app/apollo-ai-private-local-ai/id6448019325"&gt;&amp;quot;Apollo AI&amp;quot;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Before I buy random apps to try them out, I wanted to hear which setups already work well for you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PardusHD"&gt; /u/PardusHD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrhpl8/best_ios_app_with_local_openailike_api_endpoint/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrhpl8/best_ios_app_with_local_openailike_api_endpoint/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrhpl8/best_ios_app_with_local_openailike_api_endpoint/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T12:37:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1lro41o</id>
    <title>how can i make langchain stream the same way openai does?</title>
    <updated>2025-07-04T17:15:59+00:00</updated>
    <author>
      <name>/u/Beyond_Birthday_13</name>
      <uri>https://old.reddit.com/user/Beyond_Birthday_13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lro41o/how_can_i_make_langchain_stream_the_same_way/"&gt; &lt;img alt="how can i make langchain stream the same way openai does?" src="https://b.thumbs.redditmedia.com/sFAKLVTAcbNLvBL7FdAJy5tUTa61LfFcVeLI5PwEawI.jpg" title="how can i make langchain stream the same way openai does?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beyond_Birthday_13"&gt; /u/Beyond_Birthday_13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lro41o"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lro41o/how_can_i_make_langchain_stream_the_same_way/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lro41o/how_can_i_make_langchain_stream_the_same_way/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T17:15:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrgdzg</id>
    <title>Is there a rule between alpha (Œ±) and rank (r) for LoRA?</title>
    <updated>2025-07-04T11:25:40+00:00</updated>
    <author>
      <name>/u/TechNerd10191</name>
      <uri>https://old.reddit.com/user/TechNerd10191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meaning, should alpha be double the rank or it doesn't matter much?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechNerd10191"&gt; /u/TechNerd10191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrgdzg/is_there_a_rule_between_alpha_Œ±_and_rank_r_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrgdzg/is_there_a_rule_between_alpha_Œ±_and_rank_r_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrgdzg/is_there_a_rule_between_alpha_Œ±_and_rank_r_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T11:25:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lr0i8p</id>
    <title>Smartphone SoC inference performance by year and series</title>
    <updated>2025-07-03T20:49:52+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr0i8p/smartphone_soc_inference_performance_by_year_and/"&gt; &lt;img alt="Smartphone SoC inference performance by year and series" src="https://b.thumbs.redditmedia.com/zFmb1fKtOa4tHvXdPcDmgPMDWJtC_rwKAXVsetdAeCg.jpg" title="Smartphone SoC inference performance by year and series" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://ai-benchmark.com/ranking_processors.html"&gt;https://ai-benchmark.com/ranking_processors.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lr0i8p"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr0i8p/smartphone_soc_inference_performance_by_year_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lr0i8p/smartphone_soc_inference_performance_by_year_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T20:49:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lr217c</id>
    <title>Cheaper Transcriptions, Pricier Errors!</title>
    <updated>2025-07-03T21:55:12+00:00</updated>
    <author>
      <name>/u/TelloLeEngineer</name>
      <uri>https://old.reddit.com/user/TelloLeEngineer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr217c/cheaper_transcriptions_pricier_errors/"&gt; &lt;img alt="Cheaper Transcriptions, Pricier Errors!" src="https://preview.redd.it/zznx9kqgdqaf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b6c6aa04a999c4484b5ede2e12f9048adef610c" title="Cheaper Transcriptions, Pricier Errors!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There was a post going around recently, &lt;a href="https://george.mand.is/2025/06/openai-charges-by-the-minute-so-make-the-minutes-shorter/"&gt;OpenAI Charges by the Minute, So Make the Minutes Shorter&lt;/a&gt;, proposing to speed up audio to lower inference / api costs for speech recognition / transcription / stt. I for one was intrigued by the results but given that they were based primarily on anecdotal evidence I felt compelled to perform a proper evaluation. &lt;a href="https://github.com/LeonEricsson/stt-speedup-bench"&gt;This repo&lt;/a&gt; contains the full experiments, and below is the TLDR, accompanying the figure.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Performance degradation is exponential, at 2√ó playback most models are already 3‚Äì5√ó worse; push to 2.5√ó and accuracy falls off a cliff, with 20√ó degradation not uncommon. There are still sweet spots, though: Whisper-large-turbo only drifts from 5.39 % to 6.92 % WER (‚âà 28 % relative hit) at 1.5√ó, and GPT-4o tolerates 1.2 √ó with a trivial ~3 % penalty.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TelloLeEngineer"&gt; /u/TelloLeEngineer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zznx9kqgdqaf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr217c/cheaper_transcriptions_pricier_errors/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lr217c/cheaper_transcriptions_pricier_errors/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T21:55:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lr18jg</id>
    <title>Serene Pub v0.3.0 Alpha Released ‚Äî Offline AI Roleplay Client w/ Lorebooks+</title>
    <updated>2025-07-03T21:20:25+00:00</updated>
    <author>
      <name>/u/doolijb</name>
      <uri>https://old.reddit.com/user/doolijb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr18jg/serene_pub_v030_alpha_released_offline_ai/"&gt; &lt;img alt="Serene Pub v0.3.0 Alpha Released ‚Äî Offline AI Roleplay Client w/ Lorebooks+" src="https://b.thumbs.redditmedia.com/dvo_5ERmmo1diIOh_NZezoiMzqhJNXXLibR4Rm1MveY.jpg" title="Serene Pub v0.3.0 Alpha Released ‚Äî Offline AI Roleplay Client w/ Lorebooks+" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;üåü Serene Pub v0.3.0&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Serene Pub&lt;/strong&gt; is an open source, locally hosted AI client built specifically for immersive roleplay and storytelling. It focuses on presenting a clean interface and easy configuration for users who would rather not feel like they need a PHD in AI or software development. With built-in real-time sync and offline-first design, Serene Pub helps you stay in character, not in the configuration menu.&lt;/p&gt; &lt;p&gt;After weeks of refinement and feedback, I‚Äôm excited to announce the &lt;strong&gt;0.3.0 alpha release&lt;/strong&gt; of &lt;strong&gt;Serene Pub&lt;/strong&gt; ‚Äî a modern, open source AI client focused on ease of use and role-playing.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;‚ú® What's New in 0.3.0 Alpha&lt;/h2&gt; &lt;h3&gt;üìö Lorebooks+&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Create and manage &lt;strong&gt;World Lore&lt;/strong&gt;, &lt;strong&gt;Character Lore&lt;/strong&gt;, and &lt;strong&gt;History&lt;/strong&gt; entries.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Character Bindings:&lt;/strong&gt; Hot-swappable character and persona bindings to your lorebook. Bindings are used to dynamically insert names into your lore book entries, or link character lore.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;World Lore:&lt;/strong&gt; Traditional lorebook entries that you are already familiar with. Describe places, items, organizations‚Äîanything relevant to your world.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Character Lore:&lt;/strong&gt; Lore entries that are attached to character bindings. These lore entries extend your character profiles.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;History:&lt;/strong&gt; Chronological lore entries that can represent a year, month or day. Provide summaries of past events or discussions. The latest entry is considered the &amp;quot;current date,&amp;quot; which can be automatically referenced in your context configuration.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;üß∞ Other Updates&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;In-app update notifications&lt;/strong&gt; ‚Äì Serene Pub will now (politely) notify you when a new release is available on GitHub.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Preset connection configurations&lt;/strong&gt; ‚Äì Built-in presets make it easy to connect to services like OpenRouter, Ollama, and other OpenAI-compatible APIs.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;UI polish &amp;amp; bug fixes&lt;/strong&gt; ‚Äì Ongoing improvements to mobile layout, theming, and token/prompt statistics.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;‚ö° Features Recap&lt;/h2&gt; &lt;p&gt;Serene Pub already includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚úÖ &lt;strong&gt;WebSocket-based real-time sync&lt;/strong&gt; across windows/devices&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Custom prompt instruction blocks&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;10+ themes&lt;/strong&gt; and dark mode&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Offline/local-first&lt;/strong&gt; ‚Äî no account or cloud required&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;üöÄ Try It Now&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://github.com/doolijb/serene-pub/releases"&gt;Download the latest release&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Extract the archive and execute &lt;code&gt;run.sh&lt;/code&gt; (Linux/MacOS) or &lt;code&gt;run.cmd&lt;/code&gt; (Windows)&lt;/li&gt; &lt;li&gt;Visit &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Add a model, create a character, and start chatting!&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Reminder: This project is in Alpha. It is being actively developed, expect bugs and significant changes!&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;üÜô Upgrading from 0.2.2 to 0.3.x&lt;/h2&gt; &lt;p&gt;Serene Pub now uses a new database backend powered by &lt;strong&gt;PostgreSQL via pglite&lt;/strong&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Upgrading your data from &lt;strong&gt;0.2.2 to 0.3.x&lt;/strong&gt; is &lt;strong&gt;supported only during the 0.3.x release window&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Future releases (e.g. &lt;strong&gt;0.4.x and beyond&lt;/strong&gt;) &lt;strong&gt;will not support direct migration from 0.2.2&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;‚ö†Ô∏è To preserve your data, please upgrade to 0.3.x before jumping to future versions.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;h2&gt;üìπ Video Guide Coming Soon&lt;/h2&gt; &lt;p&gt;I will try to record an in-depth walk-through in the next week!&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;üß™ Feedback Needed&lt;/h2&gt; &lt;p&gt;This release was only tested on &lt;strong&gt;Linux x64&lt;/strong&gt; and &lt;strong&gt;Windows x64&lt;/strong&gt;. Support for other platforms is experimental and feedback is urgently needed.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If you run into issues, please &lt;a href="https://github.com/doolijb/serene-pub/issues"&gt;open an issue&lt;/a&gt; or reach out.&lt;/li&gt; &lt;li&gt;Bug patches will be released in the coming days/weeks based on feedback and severity.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Your testing and suggestions are extremely appreciated!&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;üêû Known Issues&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;LM Chat support is currently disabled&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;The native LM Chat API has been disabled due to bugs in their SDK.&lt;/li&gt; &lt;li&gt;Their OpenAI-compatible endpoint also has unresolved issues.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Recommendation:&lt;/strong&gt; Use &lt;strong&gt;Ollama&lt;/strong&gt; for the most stable and user-friendly local model experience.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h2&gt;üîÆ Coming Soon (0.4.0 ‚Äì 0.6.0)&lt;/h2&gt; &lt;p&gt;These features are currently being planned and will hopefully make it into upcoming releases:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Seamless chat and lorebook vectorization&lt;/strong&gt; ‚Äì enable smarter memory and retrieval for characters and world info.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama Management Console&lt;/strong&gt; ‚Äì download, manage, and switch models directly within Serene Pub.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Serene Pub Assistant Chat&lt;/strong&gt; ‚Äì get help from a built-in assistant for documentation, feature walkthroughs, or character design.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tags&lt;/strong&gt; ‚Äì organize personas, characters, chats, and lorebooks with flexible tagging.&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h2&gt;üó®Ô∏è Final Thoughts&lt;/h2&gt; &lt;p&gt;Thank you to everyone who has tested, contributed, or shared ideas! Your support continues to shape Serene Pub. Try it out, file an issue, and let me know what features you‚Äôd love to see next. Reach out on Github, Reddit or Discord.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doolijb"&gt; /u/doolijb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lr18jg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lr18jg/serene_pub_v030_alpha_released_offline_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lr18jg/serene_pub_v030_alpha_released_offline_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T21:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrjkx3</id>
    <title>Unmute + Llama.cpp server</title>
    <updated>2025-07-04T14:05:55+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Managed to get unmute to work with llama-server API, (thanks to Gemini 2.5 flash). This modified &lt;code&gt;llm_utils.py&lt;/code&gt; goes into unmute/llm (note, it might make vLLM not work, haven't tested):&lt;/p&gt; &lt;p&gt;&lt;a href="https://gist.github.com/jepjoo/7ab6da43c3e51923eeaf278eac47c9c9"&gt;https://gist.github.com/jepjoo/7ab6da43c3e51923eeaf278eac47c9c9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Run llama-server with --port 8000 (or change settings in docker-compose.yml)&lt;/p&gt; &lt;p&gt;Can fit all unmute parts + Mistral 24B IQ4_XS or Gemma 3 27B IQ3_M into 24GB.&lt;/p&gt; &lt;p&gt;Tips:&lt;/p&gt; &lt;p&gt;System prompt can be edited to your liking, it's in &lt;code&gt;unmute/llm/system_prompt.py&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Characters' prompts can be edited and a different voice can be selected for them by editing &lt;code&gt;voices.yaml&lt;/code&gt;&lt;/p&gt; &lt;p&gt;There's over a 100 voices, they are somewhere in the depths of the docker filesystem in .safetensors format, so I just downloaded them all from here in .wav format to be able to listen to them: &lt;a href="https://huggingface.co/kyutai/tts-voices/tree/main"&gt;https://huggingface.co/kyutai/tts-voices/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To switch to a different voice, just edit the &lt;code&gt;path_on_server&lt;/code&gt; like for example the first charater: &lt;code&gt;path_on_server: unmute-prod-website/p329_022.wav&lt;/code&gt; -&amp;gt; &lt;code&gt;path_on_server: expresso/ex04-ex03_fast_001_channel2_25s.wav&lt;/code&gt;&lt;/p&gt; &lt;p&gt;After you update the &lt;code&gt;llm_utils.py&lt;/code&gt; or edit those other files you gotta:&lt;/p&gt; &lt;p&gt;docker compose up -d --build backend&lt;/p&gt; &lt;p&gt;PS. I'm running on Windows, things could be much smoother on Linux and the llm_utils.py fix might be unnecessary, dunno.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrjkx3/unmute_llamacpp_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrjkx3/unmute_llamacpp_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrjkx3/unmute_llamacpp_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T14:05:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1lroopr</id>
    <title>Llama.cpp and continuous batching for performance</title>
    <updated>2025-07-04T17:40:03+00:00</updated>
    <author>
      <name>/u/Simusid</name>
      <uri>https://old.reddit.com/user/Simusid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an archive of several thousand maintenance documents. They are all very structured and similar but not identical. They cover 5 major classes of big industrial equipment. For a single class there may be 20 or more specific builds but not every build in a class is identical. Sometimes we want information about a whole class, and sometimes we want information about a specific build.&lt;/p&gt; &lt;p&gt;I've had very good luck using an LLM with a well engineered prompt and defined JSON schema. And basically I'm getting the answers I want, but not fast enough. These may take 20 seconds each. &lt;/p&gt; &lt;p&gt;Right now I just do all these in a loop, one at a time and I'm wondering if there is a way to configure the server for better performance. I have &lt;em&gt;plenty&lt;/em&gt; of both CPU and GPU resources. I want to better understand things like continuous batching, kv cache optimizing, threads and anything else that can improve performance when the prompts are nearly the same thing over and over.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Simusid"&gt; /u/Simusid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lroopr/llamacpp_and_continuous_batching_for_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lroopr/llamacpp_and_continuous_batching_for_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lroopr/llamacpp_and_continuous_batching_for_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T17:40:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrjg7t</id>
    <title>What are some locally hosted Killer Apps?</title>
    <updated>2025-07-04T14:00:14+00:00</updated>
    <author>
      <name>/u/AdOne8437</name>
      <uri>https://old.reddit.com/user/AdOne8437</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are your locally hosted killer apps at the moment. What do you show to wow your friends and boss?&lt;/p&gt; &lt;p&gt;I just got asked by a friend since he has been tasked to install a local ai chat but wants to wow his boss and I also realized I have been stuck in the 'helps coding' and 'helps writing' corner for a while.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdOne8437"&gt; /u/AdOne8437 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrjg7t/what_are_some_locally_hosted_killer_apps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrjg7t/what_are_some_locally_hosted_killer_apps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrjg7t/what_are_some_locally_hosted_killer_apps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T14:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqycp0</id>
    <title>Kyutai TTS is here: Real-time, voice-cloning, ultra-low-latency TTS, Robust Longform generation</title>
    <updated>2025-07-03T19:20:57+00:00</updated>
    <author>
      <name>/u/pheonis2</name>
      <uri>https://old.reddit.com/user/pheonis2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/"&gt; &lt;img alt="Kyutai TTS is here: Real-time, voice-cloning, ultra-low-latency TTS, Robust Longform generation" src="https://external-preview.redd.it/W23MXrPmD5xlTsZxRV5EvGHxPsNlgEO6PQGqj7YJHs4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abb529167e09a3692724b342df0121216749b7bd" title="Kyutai TTS is here: Real-time, voice-cloning, ultra-low-latency TTS, Robust Longform generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/46c2vbkrkpaf1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a074dd8ac462e5276d42be14dd98e4b1700f67fd"&gt;https://preview.redd.it/46c2vbkrkpaf1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a074dd8ac462e5276d42be14dd98e4b1700f67fd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Kyutai has open-sourced Kyutai TTS ‚Äî a new real-time text-to-speech model that‚Äôs packed with features and ready to shake things up in the world of TTS.&lt;/p&gt; &lt;p&gt;It‚Äôs super fast, starting to generate audio in just ~220ms after getting the first bit of text. Unlike most ‚Äústreaming‚Äù TTS models out there, it doesn‚Äôt need the whole text upfront ‚Äî it works as you type or as an LLM generates text, making it perfect for live interactions.&lt;/p&gt; &lt;p&gt;You can also clone voices with just 10 seconds of audio.&lt;/p&gt; &lt;p&gt;And yes ‚Äî it handles long sentences or paragraphs without breaking a sweat, going well beyond the usual 30-second limit most models struggle with.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/kyutai-labs/delayed-streams-modeling/"&gt;https://github.com/kyutai-labs/delayed-streams-modeling/&lt;/a&gt;&lt;br /&gt; Huggingface: &lt;a href="https://huggingface.co/kyutai/tts-1.6b-en_fr"&gt;https://huggingface.co/kyutai/tts-1.6b-en_fr&lt;/a&gt;&lt;br /&gt; &lt;a href="https://kyutai.org/next/tts"&gt;https://kyutai.org/next/tts&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pheonis2"&gt; /u/pheonis2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T19:20:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrlmco</id>
    <title>How are the casual users here using LLMs or/and MCPs?</title>
    <updated>2025-07-04T15:31:52+00:00</updated>
    <author>
      <name>/u/man_eating_chicken</name>
      <uri>https://old.reddit.com/user/man_eating_chicken</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been exploring LLMs for a while and have been using Ollama and python to just do some formatting, standardisation and conversions of some private files. Beyond this I use Claude to help me with complex excel functions or to help me collate lists of all podcasts with Richard Thaler, for example. &lt;/p&gt; &lt;p&gt;I'm curious about MCPs and want to know how users here are using AI in their PERSONAL LIVES. &lt;/p&gt; &lt;p&gt;I'm so exhausted by all the posts about vibe coding, hardware and model comparisons because they're all for people who view AI very differently than I do. &lt;/p&gt; &lt;p&gt;I'm more curious about personal usage because I'm not keen on using AI to sort my emails as most people on YouTube do with AI agents and such. I mean, let me try and protect my data while I still can. &lt;/p&gt; &lt;p&gt;It could be as simple as using Image OCR to LLM to make an excel sheet of all the different sneakers you own. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/man_eating_chicken"&gt; /u/man_eating_chicken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrlmco/how_are_the_casual_users_here_using_llms_orand/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrlmco/how_are_the_casual_users_here_using_llms_orand/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrlmco/how_are_the_casual_users_here_using_llms_orand/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T15:31:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1lre3x9</id>
    <title>Apple M4 Max or AMD Ryzen AI Max+ 395 (Framwork Desktop)</title>
    <updated>2025-07-04T09:02:33+00:00</updated>
    <author>
      <name>/u/zeltbrennt</name>
      <uri>https://old.reddit.com/user/zeltbrennt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working on a LLM-Project for my CS Degree where I need to run a models locally, because of sensitive data. My current Desktop PC is quite old now (Windows, i5-6600K, 16GB RAM, GTX 1060 6GB) and only capable of running small models, so I want to upgrade it anyway. I saw a few people reccomending Apples ARM for the job, but they are very expensive. I am looking at &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mac Studio M4 Max&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Apple M4 Max&lt;/li&gt; &lt;li&gt;16 Core CPU&lt;/li&gt; &lt;li&gt;40 Core GPU&lt;/li&gt; &lt;li&gt;16 Core NE&lt;/li&gt; &lt;li&gt;546 GB/s memory bandwidth &lt;/li&gt; &lt;li&gt;128 GB RAM&lt;/li&gt; &lt;li&gt;1TB SSD&lt;/li&gt; &lt;li&gt;MacOS&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the Edu-Store they sell in my country it for &lt;strong&gt;4,160‚Ç¨&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I found another alternative: Framework. I knew they build nice Laptops, but one might also preorder their new Desktops (Charge 11 is estimated to ship in Q3). &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Framework Desktop Max+ 395&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen AI Max+ 395&lt;/li&gt; &lt;li&gt;16 Core CPU&lt;/li&gt; &lt;li&gt;40 Core GPU&lt;/li&gt; &lt;li&gt;265 GB/s memory bandwidth &lt;/li&gt; &lt;li&gt;128 GB RAM&lt;/li&gt; &lt;li&gt;1TB SSD&lt;/li&gt; &lt;li&gt;Fedora&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So with the (on paper) equivalent configuration I arrive at &lt;strong&gt;2,570‚Ç¨&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;That is a lot of money saved! Plus I would be running Linux instead of MacOS. I like not being boxed in an ecosystem. The replacement parts are much cheaper. The only downside would be a few programs like Lightroom are not availabe on Linux (I would cancel my subscription, wich also saves money). Gaming on this thing might also be better.&lt;/p&gt; &lt;p&gt;Has anybody expierence with this System for LLMs? Would this be a good alternative? What benefit am I getting in the Max version and is it worth the premium price?&lt;/p&gt; &lt;p&gt;Edit: fixed CPU core count, added memory bandwidth&lt;/p&gt; &lt;p&gt;Edit2:more Information on the use case: the input prompt will be relativly large (tranacripts of conversations enriched by RAG from a data base of domain specific literarure) and the output small (reccomendations and best practices)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zeltbrennt"&gt; /u/zeltbrennt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lre3x9/apple_m4_max_or_amd_ryzen_ai_max_395_framwork/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lre3x9/apple_m4_max_or_amd_ryzen_ai_max_395_framwork/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lre3x9/apple_m4_max_or_amd_ryzen_ai_max_395_framwork/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T09:02:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1lroonm</id>
    <title>Gemini CLI is open source. Could we fork it to be able to use other models ?</title>
    <updated>2025-07-04T17:39:59+00:00</updated>
    <author>
      <name>/u/SubliminalPoet</name>
      <uri>https://old.reddit.com/user/SubliminalPoet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unlike Claude Code, &lt;a href="https://github.com/google-gemini/gemini-cli/tree/main"&gt;Gemini CLI is open source&lt;/a&gt;. Wouldn‚Äôt it be interesting to fork it and extend it to support other models, similar to what Aider provides?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SubliminalPoet"&gt; /u/SubliminalPoet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lroonm/gemini_cli_is_open_source_could_we_fork_it_to_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lroonm/gemini_cli_is_open_source_could_we_fork_it_to_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lroonm/gemini_cli_is_open_source_could_we_fork_it_to_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T17:39:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lqvovt</id>
    <title>A project to bring CUDA to non-Nvidia GPUs is making major progress</title>
    <updated>2025-07-03T17:35:16+00:00</updated>
    <author>
      <name>/u/OwnWitness2836</name>
      <uri>https://old.reddit.com/user/OwnWitness2836</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/"&gt; &lt;img alt="A project to bring CUDA to non-Nvidia GPUs is making major progress" src="https://external-preview.redd.it/ADNOrzyLUcH9GZiKV8wujT8yD5FQYGEatyugJVGa73E.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b1a092718ebc960a0b1d79e4d2f8bc6ea6c934f" title="A project to bring CUDA to non-Nvidia GPUs is making major progress" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OwnWitness2836"&gt; /u/OwnWitness2836 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/software/a-project-to-bring-cuda-to-non-nvidia-gpus-is-making-major-progress-zluda-update-now-has-two-full-time-developers-working-on-32-bit-physx-support-and-llms-amongst-other-things"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lqvovt/a_project_to_bring_cuda_to_nonnvidia_gpus_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-03T17:35:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrlags</id>
    <title>Kwai Keye VL 8B - Very promising new VL model</title>
    <updated>2025-07-04T15:18:18+00:00</updated>
    <author>
      <name>/u/pol_phil</name>
      <uri>https://old.reddit.com/user/pol_phil</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The model Kwai Keye VL 8B is available on Huggingface with Apache 2.0 license. It has been built by Kuaishou (1st time I hear of them) on top of Qwen 3 8B and combines it with SigLIP-400M.&lt;/p&gt; &lt;p&gt;Their paper is truly a gem as they detail their pretraining and post-training methodology exhaustively. Haven't tested it yet, but their evaluation seems pretty solid.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pol_phil"&gt; /u/pol_phil &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2507.01949"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrlags/kwai_keye_vl_8b_very_promising_new_vl_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrlags/kwai_keye_vl_8b_very_promising_new_vl_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T15:18:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1lreu44</id>
    <title>30-60tok/s on 4bit local LLM, iPhone 16.</title>
    <updated>2025-07-04T09:50:53+00:00</updated>
    <author>
      <name>/u/Specific_Opinion_573</name>
      <uri>https://old.reddit.com/user/Specific_Opinion_573</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lreu44/3060toks_on_4bit_local_llm_iphone_16/"&gt; &lt;img alt="30-60tok/s on 4bit local LLM, iPhone 16." src="https://preview.redd.it/1pi871kgxtaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b45a353c7618f09017647270bb57fb0e37b72933" title="30-60tok/s on 4bit local LLM, iPhone 16." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I‚Äôm an AI/LLM enthusiast coming from a mobile dev background (iOS, Swift). I‚Äôve been building a local inference engine, tailored for Metal-first, real-time inference on iOS (iPhone + iPad).&lt;/p&gt; &lt;p&gt;I‚Äôve been benchmarking on iPhone 16 and hitting what seem to be high token/s rates for 4-bit quantized models.&lt;/p&gt; &lt;p&gt;Current Benchmarks (iPhone 16 Plus, all 4-bit):&lt;/p&gt; &lt;p&gt;Model Size - Token/s Range 0.5B‚Äì1.7B - 30‚Äì64 tok/s 2B - 20‚Äì48 tok/s 3B - 15‚Äì30 tok/s 4B - 7‚Äì16 tok/s 7B - often crashes due to RAM, 5‚Äì12 tok/s max&lt;/p&gt; &lt;p&gt;I haven‚Äôt seen any PrivateLLM, MLC-LLM, or llama.cpp shipping these numbers with live UI streaming, so I‚Äôd love validation: 1. iPhone 16 / 15 Pro users willing to test, can you reproduce these numbers on A17/A18? 2. If you‚Äôve profiled PrivateLLM or MLC at 2-3 B, please drop raw tok/s + device specs.&lt;/p&gt; &lt;p&gt;Happy to share build structure and testing info if helpful. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specific_Opinion_573"&gt; /u/Specific_Opinion_573 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1pi871kgxtaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lreu44/3060toks_on_4bit_local_llm_iphone_16/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lreu44/3060toks_on_4bit_local_llm_iphone_16/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T09:50:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrmxn7</id>
    <title>llama : add high-throughput mode by ggerganov ¬∑ Pull Request #14363 ¬∑ ggml-org/llama.cpp</title>
    <updated>2025-07-04T16:26:56+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrmxn7/llama_add_highthroughput_mode_by_ggerganov_pull/"&gt; &lt;img alt="llama : add high-throughput mode by ggerganov ¬∑ Pull Request #14363 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/IuYm0uiYOGT85fahGmoFFRSSnFzP4A66rCPcA3iycYY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6fff29b1956872ed77766a108a404db46b43026" title="llama : add high-throughput mode by ggerganov ¬∑ Pull Request #14363 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14363"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrmxn7/llama_add_highthroughput_mode_by_ggerganov_pull/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrmxn7/llama_add_highthroughput_mode_by_ggerganov_pull/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T16:26:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrgomi</id>
    <title>MCP 2025-06-18 Spec Update: Security, Structured Output &amp; Elicitation</title>
    <updated>2025-07-04T11:42:49+00:00</updated>
    <author>
      <name>/u/anmolbaranwal</name>
      <uri>https://old.reddit.com/user/anmolbaranwal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Model Context Protocol has faced a lot of criticism due to its security vulnerabilities. Anthropic recently released a new Spec Update (&lt;code&gt;MCP v2025-06-18&lt;/code&gt;) and I have been reviewing it, especially around security. Here are the important changes you should know.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;MCP servers are classified as OAuth 2.0 Resource Servers.&lt;/li&gt; &lt;li&gt;Clients must include a &lt;code&gt;resource&lt;/code&gt; parameter (RFC 8707) when requesting tokens, this explicitly binds each access token to a specific MCP server.&lt;/li&gt; &lt;li&gt;Structured JSON tool output is now supported (&lt;code&gt;structuredContent&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Servers can now ask users for input mid-session by sending an `elicitation/create` request with a message and a JSON schema.&lt;/li&gt; &lt;li&gt;‚ÄúSecurity Considerations‚Äù have been added to prevent token theft, PKCE, redirect URIs, confused deputy issues.&lt;/li&gt; &lt;li&gt;Newly added Security best practices page addresses threats like token passthrough, confused deputy, session hijacking, proxy misuse with concrete countermeasures.&lt;/li&gt; &lt;li&gt;All HTTP requests now must include the &lt;code&gt;MCP-Protocol-Version&lt;/code&gt; header. If the header is missing and the version can‚Äôt be inferred, servers should default to &lt;code&gt;2025-03-26&lt;/code&gt; for backward compatibility.&lt;/li&gt; &lt;li&gt;New &lt;code&gt;resource_link&lt;/code&gt; type lets tools point to URIs instead of inlining everything. The client can then subscribe to or fetch this URI as needed.&lt;/li&gt; &lt;li&gt;They removed JSON-RPC batching (not backward compatible). If your SDK or application was sending multiple JSON-RPC calls in a single batch request (an array), it will now break as MCP servers will reject it starting with version &lt;code&gt;2025-06-18&lt;/code&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In the PR (#416), I found ‚Äúno compelling use cases‚Äù for actually removing it. Official JSON-RPC documentation explicitly says a client &lt;code&gt;MAY send an Array&lt;/code&gt; of requests and the server &lt;code&gt;SHOULD respond with an Array&lt;/code&gt; of results. MCP‚Äôs new rule essentially forbids that.&lt;/p&gt; &lt;p&gt;Detailed writeup: &lt;a href="https://forgecode.dev/blog/mcp-spec-updates/"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What's your experience? Are you satisfied with the changes or still upset with the security risks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anmolbaranwal"&gt; /u/anmolbaranwal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://forgecode.dev/blog/mcp-spec-updates/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrgomi/mcp_20250618_spec_update_security_structured/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrgomi/mcp_20250618_spec_update_security_structured/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T11:42:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrjy15</id>
    <title>Anyone else feel like working with LLM libs is like navigating a minefield ?</title>
    <updated>2025-07-04T14:22:03+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've worked about 7 years in software development companies, and it's &amp;quot;easy&amp;quot; to be a software/backend/web developer because we use tools/frameworks/libs that are mature and battle-tested.&lt;/p&gt; &lt;p&gt;Problem with Django? Update it, the bug was probably fixed ages ago.&lt;/p&gt; &lt;p&gt;With LLMs it's an absolute clusterfuck. You just bought an RTX 5090? Boom, you have to recompile everything to make it work with SM_120. And I'm skipping the hellish Ubuntu installation part with cursed headers just to get it running in degraded mode.&lt;/p&gt; &lt;p&gt;Example from last week: vLLM implemented Dual Chunked Attention for Qwen 7B/14B 1M, THE ONLY (open weight) model that seriously handles long context.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Unmerged bugfix that makes it &lt;strong&gt;UNUSABLE&lt;/strong&gt; &lt;a href="https://github.com/vllm-project/vllm/pull/19084"&gt;https://github.com/vllm-project/vllm/pull/19084&lt;/a&gt;&lt;/li&gt; &lt;li&gt;FP8 wasn't working, I had to make the PR myself &lt;a href="https://github.com/vllm-project/vllm/pull/19420"&gt;https://github.com/vllm-project/vllm/pull/19420&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Some guy broke Dual Chunk attention because of CUDA kernel and division by zero, had to write another PR &lt;a href="https://github.com/vllm-project/vllm/pull/20488"&gt;https://github.com/vllm-project/vllm/pull/20488&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Holy shit, I spend more time at the office hammering away at libraries than actually working on the project that's supposed to use these libraries.&lt;/p&gt; &lt;p&gt;Am I going crazy or do you guys also notice this is a COMPLETE SHITSHOW????&lt;/p&gt; &lt;p&gt;And I'm not even talking about the nightmare of having to use virtualized GPUs with NVIDIA GRID drivers that you can't download yourself and that EXPLODE at the slightest conflict: &lt;/p&gt; &lt;p&gt;&lt;code&gt;driver versions &amp;lt;----&amp;gt; torch version &amp;lt;-----&amp;gt; vLLM version&lt;/code&gt;&lt;/p&gt; &lt;p&gt;It's driving me insane.&lt;/p&gt; &lt;p&gt;I don't understand how Ggerganov can keep working on llama.cpp every single day with no break and not turn INSANE.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrjy15/anyone_else_feel_like_working_with_llm_libs_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T14:22:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1lrbwmz</id>
    <title>Created an Open Source Conversation Response Path Exploration System using Monte Carlo Tree Search</title>
    <updated>2025-07-04T06:36:12+00:00</updated>
    <author>
      <name>/u/ManavTheWorld</name>
      <uri>https://old.reddit.com/user/ManavTheWorld</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/"&gt; &lt;img alt="Created an Open Source Conversation Response Path Exploration System using Monte Carlo Tree Search" src="https://external-preview.redd.it/g_tsBx-sQoZLGvWQ7PFRoKZ5UY7Qfo6eiDBT125d8YE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a18a6b5aef4f1d48310d2918ee6ff6f6c5943c2" title="Created an Open Source Conversation Response Path Exploration System using Monte Carlo Tree Search" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! I'm creating a project that applies Monte Carlo Tree Search to LLM conversations. Instead of just generating the next response, it simulates entire conversation trees to find paths that achieve long-term goals. The initial draft version is up.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/MVPandey/CAE"&gt;https://github.com/MVPandey/CAE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Note: This is a Claude-generated mock UI. The payload is real but the UI is simulated :) I'm a terrible frontend dev)&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/dqws3fzgysaf1.gif"&gt;https://i.redd.it/dqws3fzgysaf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Generates multiple response candidates at each conversation state&lt;/li&gt; &lt;li&gt;Simulates how conversations might unfold down each branch (using the LLM to predict user responses)&lt;/li&gt; &lt;li&gt;Scores each trajectory on metrics like empathy, goal achievement, coherence&lt;/li&gt; &lt;li&gt;Uses MCTS with UCB1 to efficiently explore the most promising paths&lt;/li&gt; &lt;li&gt;Selects the response that leads to the best expected outcome&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Technical implementation:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;FastAPI backend with async SQLAlchemy (PostgreSQL)&lt;/li&gt; &lt;li&gt;Aggressive parallelization - all branch evaluations run concurrently with asyncio.gather()&lt;/li&gt; &lt;li&gt;Works with any OpenAI-compatible endpoint&lt;/li&gt; &lt;li&gt;Dual-purpose: works as both a standard chat API and on-demand analysis engine&lt;/li&gt; &lt;li&gt;No agentic framework dependencies &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Limitations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Scoring is done by the same LLM that generates responses (obviously bad - not very grounded or reproducible or scientific yet)&lt;/li&gt; &lt;li&gt;Branch pruning is naive - just threshold-based instead of something smarter like progressive widening&lt;/li&gt; &lt;li&gt;Memory usage grows with tree size - haven't implemented node recycling yet&lt;/li&gt; &lt;li&gt;The pgvector embedding code is there but commented out (wanted semantic search over conversation history)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Originally thought of this to generate preference data for RL training (converting instruct/response datasets to PPO datasets) and refined the idea into code at a hackathon - the system outputs full JSON showing why certain conversation paths outperform others, with rationales and metrics. Been testing on customer support scenarios and therapeutic conversations.&lt;/p&gt; &lt;p&gt;Example output shows the selected response, rejected alternatives, simulated user reactions, and scoring breakdowns. Pretty interesting to see it reason through de-escalation strategies or teaching approaches.&lt;/p&gt; &lt;p&gt;Curious if anyone's tried similar approaches or has ideas for more grounded scoring methods. The LLM-as-judge problem is real here.&lt;/p&gt; &lt;p&gt;Anyway, please let me know any thoughts, criticisms, feedback, etc! :) &lt;/p&gt; &lt;p&gt;I also am not sure what I want this project to evolve into. This is a very crude first approach and IDK what I wanna do for next steps. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ManavTheWorld"&gt; /u/ManavTheWorld &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lrbwmz/created_an_open_source_conversation_response_path/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T06:36:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1lri12r</id>
    <title>Great price on a 5090</title>
    <updated>2025-07-04T12:53:29+00:00</updated>
    <author>
      <name>/u/psdwizzard</name>
      <uri>https://old.reddit.com/user/psdwizzard</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lri12r/great_price_on_a_5090/"&gt; &lt;img alt="Great price on a 5090" src="https://preview.redd.it/1en1lic1uuaf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7df49758108c1feb283e4286654e01dbd232a219" title="Great price on a 5090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About to pull the trigger on this one I can't believe how cheap it is. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/psdwizzard"&gt; /u/psdwizzard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1en1lic1uuaf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lri12r/great_price_on_a_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lri12r/great_price_on_a_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-04T12:53:29+00:00</published>
  </entry>
</feed>
