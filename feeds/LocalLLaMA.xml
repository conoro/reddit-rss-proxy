<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-27T03:42:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mzwcs8</id>
    <title>Qwen Wan2.2-S2V is coming soon</title>
    <updated>2025-08-25T17:08:46+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"&gt; &lt;img alt="Qwen Wan2.2-S2V is coming soon" src="https://preview.redd.it/9xwkq1az67lf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d418420a969fcd5b88779cc4eb2389257267480c" title="Qwen Wan2.2-S2V is coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/Alibaba_Wan/status/1959963989703880866?t=F29sqn8rWrVM-ia3qz4cvQ&amp;amp;s=19"&gt;https://x.com/Alibaba_Wan/status/1959963989703880866?t=F29sqn8rWrVM-ia3qz4cvQ&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9xwkq1az67lf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mzwcs8/qwen_wan22s2v_is_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T17:08:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0f4hh</id>
    <title>Trying to run offline LLM+RAG feels impossible. What am I doing wrong?</title>
    <updated>2025-08-26T07:12:51+00:00</updated>
    <author>
      <name>/u/caprazli</name>
      <uri>https://old.reddit.com/user/caprazli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been banging my head against the wall trying to get a simple offline LLM+RAG setup running on my laptop (which is plenty powerful). The idea was just a proof of concept: local model + retrieval, able to handle MS Office docs, PDFs, &lt;strong&gt;and&lt;/strong&gt; (that's important) even .eml files.&lt;/p&gt; &lt;p&gt;Instead, it‚Äôs been an absolute nightmare. Nothing works out of the box. Every ‚Äúsolution‚Äù I try turns into endless code-patching across multiple platforms. Half the guides are outdated, half the repos are broken, and when I finally get something running, it chokes on the files I actually need.&lt;/p&gt; &lt;p&gt;I‚Äôm not a total beginner yet I‚Äôm definitely not an expert either. Still, I feel like the bar to entry here is ridiculously high. AI is fantastic for writing, summarizing, and all the fancy cloud-based stuff, but when it comes to coding and local setups, reliability is just‚Ä¶ not there yet.&lt;/p&gt; &lt;p&gt;Am I doing something completely wrong? Does anyone else have similar experiences? Because honestly, &lt;strong&gt;AI might be ‚Äútaking over the world,‚Äù but it‚Äôs definitely&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;not&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;taking over my computer&lt;/strong&gt;. It simply cannot.&lt;/p&gt; &lt;p&gt;Curious to hear from others. What‚Äôs your experience with local LLM+RAG setups? Any success stories or lessons learned?&lt;/p&gt; &lt;p&gt;&lt;em&gt;PS: U7-155H | 32G | 2T | Arc+NPU | W11: Should theoretically be enough to run local LLMs with big context, chew through Office/PDF/&lt;/em&gt;&lt;strong&gt;&lt;em&gt;.eml&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;docs, and push AI-native pipelines with NPU boost, yet...&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/caprazli"&gt; /u/caprazli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0f4hh/trying_to_run_offline_llmrag_feels_impossible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0f4hh/trying_to_run_offline_llmrag_feels_impossible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0f4hh/trying_to_run_offline_llmrag_feels_impossible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T07:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n04bjf</id>
    <title>OpenBNB just released MiniCPM-V 4.5 8B</title>
    <updated>2025-08-25T22:09:58+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n04bjf/openbnb_just_released_minicpmv_45_8b/"&gt; &lt;img alt="OpenBNB just released MiniCPM-V 4.5 8B" src="https://external-preview.redd.it/aDQxdnl1aXBvOGxmMfglwkP6DhCqoPe2rr3dd0QwemhViAoKpUk6qvqn7V19.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=efc8ba97fe359c4e115f528437cc336a6259f86c" title="OpenBNB just released MiniCPM-V 4.5 8B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;claiming it's vision language surpasses GPT-4o, Gemini Pro 2, and Qwen2.5-VL 72B&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Announcement on X: &lt;a href="https://x.com/openbmb/status/1960090703083843712?s=46"&gt;https://x.com/openbmb/status/1960090703083843712?s=46&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5"&gt;https://huggingface.co/openbmb/MiniCPM-V-4_5&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/OpenBMB/MiniCPM-o"&gt;https://github.com/OpenBMB/MiniCPM-o&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5vsd9mlpo8lf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n04bjf/openbnb_just_released_minicpmv_45_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n04bjf/openbnb_just_released_minicpmv_45_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-25T22:09:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1n13rsq</id>
    <title>Most economical way to run GPT-OSS-120B?</title>
    <updated>2025-08-27T01:19:08+00:00</updated>
    <author>
      <name>/u/Mysterious_Bison_907</name>
      <uri>https://old.reddit.com/user/Mysterious_Bison_907</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently running GPT-OSS-120b on my desktop computer. On a good day, it can manage 7 or 8 tokens/sec. I saw Jeff Geerling's video where he was running this model on a Framework Desktop and getting 35-40 tps. Is this the least expensive way to get better performance with this model? Thanks in advance for any advice.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Bison_907"&gt; /u/Mysterious_Bison_907 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n13rsq/most_economical_way_to_run_gptoss120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n13rsq/most_economical_way_to_run_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n13rsq/most_economical_way_to_run_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T01:19:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0z3az</id>
    <title>What I can and can't do with AMD AI Max 395+ and Nvidia RTX 5090? One hardware for all the purposes?</title>
    <updated>2025-08-26T21:52:50+00:00</updated>
    <author>
      <name>/u/Davidvia0x</name>
      <uri>https://old.reddit.com/user/Davidvia0x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone! I almost pulled a trigger on Nvidia RTX 5090 today but started thinking if that is an overkill for me (or not).&lt;/p&gt; &lt;p&gt;Well I am still a newbie in the world of LLM apart from using the Perplexity and NotebookLM. I do want to deep-dive into the LLM and to start with following the 'Build a Large Language Model (From Scratch)' book by Sebastian Raschka. &lt;/p&gt; &lt;p&gt;I work as an automotive engineer, working on powertrain optimisation (so designing e-machines, transmissions and inverter - on high level). I am looking for opportunities to script concept generation and taking inspiration from research papers etc.&lt;/p&gt; &lt;p&gt;I do have an access to the Nvidia RTX 6000 Ada GPU, but Ollama studio is currently blocked for installation (but if I have a strong case I can have it approved).&lt;/p&gt; &lt;p&gt;I want to start from exploring some of the ideas on my personal desktop or laptop, but without spending huge amount of money.&lt;/p&gt; &lt;p&gt;Use-cases:&lt;br /&gt; - LLMs for music generation&lt;br /&gt; - LLMs for research documentation classification and extraction of some of the key-parameters/outcomes&lt;br /&gt; - LLMs fine-tuning (potentially) with all the data I generate to help me make the best decision for powertrain component selection based on requirements (big one, but unsure how LLMs can help me better than Machine Learning)&lt;br /&gt; - LLMs for some kind of engineering knowledge source, could be trained on books, work documentation or data etc.&lt;/p&gt; &lt;p&gt;Optional:&lt;br /&gt; - I want to use the same hardware for coding, engineering design CAE/CAD work + rarely play games (like Civ VII, InZoi, BF6), music production and video editing (but I have Macbook M4 Pro, which could be more useful here)&lt;/p&gt; &lt;p&gt;Options I consider:&lt;br /&gt; - AMD AI Max 395+ 128GB (most reasonably priced and potentially can do all above - unsure about fine-tuning if I need that, low energy consumption, not sure about software compatibility but should be ok), Framework is my fav due to good reviews and company's perception)&lt;br /&gt; - Some bespoke configuration with RTX 5090 as core, if I get one for ~¬£1600, entire cost of desktop should be around ¬£3000 or less. I will sell my laptop (RTX 3070M + 32GB + 5700X CPU), so should get ¬£300 or more for it (due to many scratches etc.) to get some money back.&lt;br /&gt; - Mac Studio with M4 Max 64GB+, but engineering software wouldn't be compatible with it.&lt;/p&gt; &lt;p&gt;Q1) Can I train or fine-tune models on AMD AI Max 395+?&lt;br /&gt; Q2) Do I need speed for my applications?&lt;br /&gt; Q3) Should I look for AMD AI Max 395+ with Oculink for potential future expansion, although system won't be that tidy anymore. Can I pair it with RTX 5080 Ti (24GB if released) without any problem? Would RTX 5090 be overkill?&lt;/p&gt; &lt;p&gt;I back off on RTX 5090 due to power consumption, it would really add-up to my bill if I use it on regular basis.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Davidvia0x"&gt; /u/Davidvia0x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0z3az/what_i_can_and_cant_do_with_amd_ai_max_395_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0z3az/what_i_can_and_cant_do_with_amd_ai_max_395_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0z3az/what_i_can_and_cant_do_with_amd_ai_max_395_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T21:52:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0j56s</id>
    <title>support for Kimi VL model has been merged into llama.cpp (mtmd)</title>
    <updated>2025-08-26T11:23:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0j56s/support_for_kimi_vl_model_has_been_merged_into/"&gt; &lt;img alt="support for Kimi VL model has been merged into llama.cpp (mtmd)" src="https://external-preview.redd.it/cbkhZmPayjB8ku2nI_wAWqat0X8_NNQmx76ZV3jHgSs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4760e99c4781aaef8c2381cf144ea12255d3b5e8" title="support for Kimi VL model has been merged into llama.cpp (mtmd)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;model description:&lt;/p&gt; &lt;p&gt;We present &lt;strong&gt;Kimi-VL&lt;/strong&gt;, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers &lt;strong&gt;advanced multimodal reasoning, long-context understanding, and strong agent capabilities&lt;/strong&gt;‚Äîall while activating only &lt;strong&gt;2.8B&lt;/strong&gt; parameters in its language decoder (Kimi-VL-A3B).&lt;/p&gt; &lt;p&gt;(...)&lt;/p&gt; &lt;p&gt;This is an updated version of &lt;a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Thinking"&gt;Kimi-VL-A3B-Thinking&lt;/a&gt;, with following improved abilities:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;It Thinks Smarter while Consuming Less Tokens&lt;/strong&gt;: The 2506 version reaches better accuracy on multimodal reasoning benchmarks: 56.9 on MathVision (+20.1), 80.1 on MathVista (+8.4), 46.3 on MMMU-Pro (+3.3), 64.0 on MMMU (+2.1), while in average requires 20% reduced thinking length.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It Sees Clearer with Thinking&lt;/strong&gt;: Unlike the previous version that specializes on thinking tasks, the 2506 version can also achieve the same or even better ability on general visual perception and understanding, e.g. MMBench-EN-v1.1 (84.4), MMStar (70.4), RealWorldQA (70.0), MMVet (78.4), surpassing or matching abilties of our non-thinking model (&lt;a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct"&gt;Kimi-VL-A3B-Instruct&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It Extends to Video Scenarios&lt;/strong&gt;: The new 2506 version also improves on video reasoning and understanding benchmarks. It sets new state-of-the-art for open-source models on VideoMMMU (65.2), while also retains good ability on general video understanding (71.9 on Video-MME, matching &lt;a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct"&gt;Kimi-VL-A3B-Instruct&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It Extends to Higher Resolution&lt;/strong&gt;: The new 2506 version supports 3.2 million total pixels in a single image, 4X compared to the previous version. This leads to non-trivial improvements on high-resolution perception and OS-agent grounding benchmarks: 83.2 on V* Benchmark (without extra tools), 52.8 on ScreenSpot-Pro, 52.5 on OSWorld-G (full set with refusal).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GGUF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF"&gt;https://huggingface.co/ggml-org/Kimi-VL-A3B-Thinking-2506-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15458"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0j56s/support_for_kimi_vl_model_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0j56s/support_for_kimi_vl_model_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T11:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0thpa</id>
    <title>Ubuntu Docker Support in Cua with Kasm</title>
    <updated>2025-08-26T18:17:16+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0thpa/ubuntu_docker_support_in_cua_with_kasm/"&gt; &lt;img alt="Ubuntu Docker Support in Cua with Kasm" src="https://external-preview.redd.it/bXVmbTN0ZzNvZWxmMZZrLHb2o7dKB6a-Vq6jd65fuBOTo_G_y7VwV4PvoF0D.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf25a7c9e23a9488a6738ce06d7e4381f29c36e2" title="Ubuntu Docker Support in Cua with Kasm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With our Cua Agent framework, we kept seeing the same pattern: people were excited to try it‚Ä¶ and then lost 20 minutes wrestling with VM setup. Hypervisor configs, nested virt errors, giant image downloads‚Äîby the time a desktop booted, most gave up before an agent ever clicked a button.&lt;/p&gt; &lt;p&gt;So we made the first step stupid-simple: üëâ &lt;strong&gt;Ubuntu desktops in Docker with Kasm.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A full Linux GUI inside Docker, viewable in your browser. Runs the same on macOS, Windows, and Linux. Cold-starts in seconds. You can even spin up multiple desktops in parallel on one machine.&lt;/p&gt; &lt;p&gt;```python from computer import Computer&lt;/p&gt; &lt;p&gt;computer = Computer( os_type=&amp;quot;linux&amp;quot;, provider_type=&amp;quot;docker&amp;quot;, image=&amp;quot;trycua/cua-ubuntu:latest&amp;quot;, name=&amp;quot;my-desktop&amp;quot; )&lt;/p&gt; &lt;p&gt;await computer.run() ```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Docker over QEMU/KVM?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Boots in seconds, not minutes.&lt;/li&gt; &lt;li&gt;No hypervisor or nested virt drama.&lt;/li&gt; &lt;li&gt;Much lighter to operate and script.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We still use VMs when needed (macOS with lume on Apple.Virtualization, Windows Sandbox on Windows) for native OS, kernel features, or GPU passthrough. But for demos and most local agent workflows, containers win.&lt;/p&gt; &lt;p&gt;Point an agent at it like this:&lt;/p&gt; &lt;p&gt;```python from agent import ComputerAgent&lt;/p&gt; &lt;p&gt;agent = ComputerAgent(&amp;quot;openrouter/z-ai/glm-4.5v&amp;quot;, tools=[computer]) async for _ in agent.run(&amp;quot;Click on the search bar and type 'hello world'&amp;quot;): pass ```&lt;/p&gt; &lt;p&gt;That‚Äôs it: a controlled, browser-accessible desktop your model can drive.&lt;/p&gt; &lt;p&gt;üìñ Blog: &lt;a href="https://www.trycua.com/blog/ubuntu-docker-support"&gt;https://www.trycua.com/blog/ubuntu-docker-support&lt;/a&gt; üíª Repo: &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/suts7ap3oelf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0thpa/ubuntu_docker_support_in_cua_with_kasm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0thpa/ubuntu_docker_support_in_cua_with_kasm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T18:17:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1n14i2f</id>
    <title>llama.cpp-CPU with avx-512 support?</title>
    <updated>2025-08-27T01:53:18+00:00</updated>
    <author>
      <name>/u/Dry_Management_8203</name>
      <uri>https://old.reddit.com/user/Dry_Management_8203</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been trying to get llama.cpp-CPU to allow for avx-512 support but, it seems like it takes a bit of rebuilding, and I haven't quite gotten the success I was hoping for whilst trying to do that.&lt;/p&gt; &lt;p&gt;Does anyone by chance have a link to or, could produce a working llama.cpp-CPU with avx-512 support enabled. Maybe, a step-by-step walkthrough would suffice.&lt;/p&gt; &lt;p&gt;Greatly appreciated.&lt;/p&gt; &lt;p&gt;Also, I've been seeing some community concern over performance when compared to AVX2 but, I'd like to test it out for myself no less. &lt;/p&gt; &lt;p&gt;Intel core vPRO i5&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Management_8203"&gt; /u/Dry_Management_8203 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n14i2f/llamacppcpu_with_avx512_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n14i2f/llamacppcpu_with_avx512_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n14i2f/llamacppcpu_with_avx512_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T01:53:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0ozdf</id>
    <title>Challenge: can any visual model figure out why this mistaken switch in newspaper comics is so funny?</title>
    <updated>2025-08-26T15:29:22+00:00</updated>
    <author>
      <name>/u/LightBrightLeftRight</name>
      <uri>https://old.reddit.com/user/LightBrightLeftRight</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0ozdf/challenge_can_any_visual_model_figure_out_why/"&gt; &lt;img alt="Challenge: can any visual model figure out why this mistaken switch in newspaper comics is so funny?" src="https://preview.redd.it/qfb5upk6tdlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c42b7465f6015a481b4b6df52f3db1aebd024a9" title="Challenge: can any visual model figure out why this mistaken switch in newspaper comics is so funny?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This printing mistake, where they mixed up text for The Far Side with Dennis the Mennis, is one of the funniest things I've ever seen.&lt;/p&gt; &lt;p&gt;I've tried all the LLMs installed on my computer (as well as ChatGPT) with various prompts, and none of them have gotten it. I think my best prompt was &amp;quot;This newspaper made a mistake printing these comics. Can you tell me what the mistake is and why the result of the mistake itself is funny?&amp;quot;&lt;/p&gt; &lt;p&gt;My favorite hallucination was from InternVLs version of Qwen 30b-a3b:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;The mistake in the comic strip is that the caption under the second panel should be &amp;quot;I see your little, perked up tail... labeled and resting on a shelf somewhere.&amp;quot; The humor comes from the fact that the father is referring to his son's tail as if it were an object that could be labeled and shelved like a toy or a piece of furniture.&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LightBrightLeftRight"&gt; /u/LightBrightLeftRight &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qfb5upk6tdlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0ozdf/challenge_can_any_visual_model_figure_out_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0ozdf/challenge_can_any_visual_model_figure_out_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T15:29:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0dl84</id>
    <title>Been working on something... A teaser</title>
    <updated>2025-08-26T05:35:04+00:00</updated>
    <author>
      <name>/u/orblabs</name>
      <uri>https://old.reddit.com/user/orblabs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dl84/been_working_on_something_a_teaser/"&gt; &lt;img alt="Been working on something... A teaser" src="https://b.thumbs.redditmedia.com/gJsXHozTMr8Q_MnsobeoZsA017tBJOzE3DqbO7Z1inw.jpg" title="Been working on something... A teaser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty excited about this project i have been working on lately, be back soon with more info, but in the meantime thought a teaser wouldn't hurt&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/orblabs"&gt; /u/orblabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n0dl84"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dl84/been_working_on_something_a_teaser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0dl84/been_working_on_something_a_teaser/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T05:35:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0i2ln</id>
    <title>Is there any way to run 100-120B MoE models at &gt;32k context at 30 tokens/second without spending a lot?</title>
    <updated>2025-08-26T10:23:55+00:00</updated>
    <author>
      <name>/u/vtkayaker</name>
      <uri>https://old.reddit.com/user/vtkayaker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 3090 and a good AM5 socket system. With some tweaking, this is enough to run a 4-bit Qwen3-30B-A3B-Instruct-2507 as a coding model with 32k of context. It's no Claude Sonnet, but it's a cute toy and occasionally useful as a pair programmer.&lt;/p&gt; &lt;p&gt;I can also, with heroic effort and most of my 64GB of RAM, get GLM 4.5 Air to run painfully slowly with 32k context. Adding a &lt;a href="https://huggingface.co/jukofyork/GLM-4.5-DRAFT-0.6B-v3.0-GGUF/blob/main/README.md"&gt;draft model&lt;/a&gt; speeds up diff generation quite a bit, because even an 0.6B can accurately predict 16 tokens of unchanged diff context correctly.&lt;/p&gt; &lt;p&gt;But let's say I want to run a 4-bit quant of GLM 4.5 Air with 48-64k context at 30 tokens/second? What's the cheapest option?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An NVIDIA RTX PRO 6000 Blackwell 96GB costs around $8750. That would pay for &lt;em&gt;years&lt;/em&gt; of Claude MAX.&lt;/li&gt; &lt;li&gt;Lashing together 3 or 4 3090s requires both an EPYC motherboard and buying more 3090s.&lt;/li&gt; &lt;li&gt;Apple has some unified RAM systems. How fast are they &lt;em&gt;really&lt;/em&gt; for models like GLM 4.5 Air or GPT OSS 120B with 32-64k context and a 4-bit quant?&lt;/li&gt; &lt;li&gt;There's also the Ryzen AI MAX+ 395 with 128 GB of RAM, and dedicating 96 GB for the GPU. The few benchmarks I've seen are under 4k context, or not any better than 10 tokens/second.&lt;/li&gt; &lt;li&gt;NVIDIA has the DGX Spark coming out &lt;em&gt;sometime&lt;/em&gt; soon, but it looks like it will start at $3,000 and not actually be &lt;em&gt;that&lt;/em&gt; much better than the Ryzen AI MAX+ 395?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Is there some clever setup that I'm missing? Does anyone have a 4-bit quant of GLM 4.5 Air running at 30 tokens/second with 48-64k context &lt;em&gt;without&lt;/em&gt; going all the way up to a RTX 6000 or 3-4 [345]090 cards and a server motherboard? I suspect the limiting factor here is RAM speed and PCIe lanes, even with the MoE?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vtkayaker"&gt; /u/vtkayaker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0i2ln/is_there_any_way_to_run_100120b_moe_models_at_32k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0i2ln/is_there_any_way_to_run_100120b_moe_models_at_32k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0i2ln/is_there_any_way_to_run_100120b_moe_models_at_32k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T10:23:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1n11q8o</id>
    <title>Made an HF downloader app</title>
    <updated>2025-08-26T23:44:52+00:00</updated>
    <author>
      <name>/u/Sure_Explorer_6698</name>
      <uri>https://old.reddit.com/user/Sure_Explorer_6698</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n11q8o/made_an_hf_downloader_app/"&gt; &lt;img alt="Made an HF downloader app" src="https://b.thumbs.redditmedia.com/NzNUaRKDxGl9LZ5OydWwNislWikxq57EaTCx1ZM-XHU.jpg" title="Made an HF downloader app" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Java based app that i compiled using CodeAssist.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/DroidSpectre/hf-downloader"&gt;https://github.com/DroidSpectre/hf-downloader&lt;/a&gt;&lt;/p&gt; &lt;p&gt;No apk at the moment, as i have to compile on a device that allows acces to app storage (&amp;lt;Android 10).&lt;/p&gt; &lt;p&gt;But it works if you can compile it. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sure_Explorer_6698"&gt; /u/Sure_Explorer_6698 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n11q8o"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n11q8o/made_an_hf_downloader_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n11q8o/made_an_hf_downloader_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T23:44:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0bhd7</id>
    <title>Microsoft VibeVoice TTS : Open-Sourced, Supports 90 minutes speech, 4 distinct speakers at a time</title>
    <updated>2025-08-26T03:36:48+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft just dropped VibeVoice, an Open-sourced TTS model in 2 variants (1.5B and 7B) which can support audio generation upto 90 mins and also supports multiple speaker audio for podcast generation. &lt;/p&gt; &lt;p&gt;Demo Video : &lt;a href="https://youtu.be/uIvx_nhPjl0?si=_pzMrAG2VcE5F7qJ"&gt;https://youtu.be/uIvx_nhPjl0?si=_pzMrAG2VcE5F7qJ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub : &lt;a href="https://github.com/microsoft/VibeVoice"&gt;https://github.com/microsoft/VibeVoice&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bhd7/microsoft_vibevoice_tts_opensourced_supports_90/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bhd7/microsoft_vibevoice_tts_opensourced_supports_90/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0bhd7/microsoft_vibevoice_tts_opensourced_supports_90/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T03:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0wdxz</id>
    <title>How many gpus do you have in your ai setup? How much did it cost?</title>
    <updated>2025-08-26T20:08:13+00:00</updated>
    <author>
      <name>/u/No_Strawberry_8719</name>
      <uri>https://old.reddit.com/user/No_Strawberry_8719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just curiouse how many gpus you guys have and how much it cost? I only have 1 its a 12gb rtx 3060 and im not sure if ill ever be able to upgrade it seems so pricey to have more than 1 gpu...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Strawberry_8719"&gt; /u/No_Strawberry_8719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0wdxz/how_many_gpus_do_you_have_in_your_ai_setup_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0wdxz/how_many_gpus_do_you_have_in_your_ai_setup_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0wdxz/how_many_gpus_do_you_have_in_your_ai_setup_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T20:08:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0sa4p</id>
    <title>Local fashion stylist using Qwen2.5-VL-7B-Instruct-AWQ</title>
    <updated>2025-08-26T17:32:18+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0sa4p/local_fashion_stylist_using_qwen25vl7binstructawq/"&gt; &lt;img alt="Local fashion stylist using Qwen2.5-VL-7B-Instruct-AWQ" src="https://external-preview.redd.it/NWlkeGQzeGRkZWxmMRT4SbhLApKgvQD1owvP5YiaiL2TbzJV_ZyYOyd1qyKC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=240788c7a83334d1b5b97be2f2fa4c2c86332686" title="Local fashion stylist using Qwen2.5-VL-7B-Instruct-AWQ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Testing a fully local AI agent with Qwen 2.5 VL on my 3090. Simple setup: webcam in, on-device reasoning, ~1s TTS out.&lt;/p&gt; &lt;p&gt;For fun I turned it into a ‚Äúfashion stylist.‚Äù Had my buddy stand in front of the camera and receive live outfit advice. Honestly worked better than I expected, although it hallucinated a few times and (like most smaller models) lost the thread on longer convos.&lt;/p&gt; &lt;p&gt;Still, it worked! These local models can actually feel personal and context-aware. Repo in comments if you wanna mess with it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6xelo3xddelf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0sa4p/local_fashion_stylist_using_qwen25vl7binstructawq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0sa4p/local_fashion_stylist_using_qwen25vl7binstructawq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T17:32:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0pkhj</id>
    <title>Wan S2V reelased : 1st open-sourced AI Video Generation model with Audio support</title>
    <updated>2025-08-26T15:51:39+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wan2.2 S2V (14B params) has been dropped recently and the early samples look great. The Audio support is great and can generate sining videos, dialogue deliveries, object sounds (like eating, rain, etc). &lt;strong&gt;I&lt;/strong&gt;t intakes a static image, an audio clip, and a text prompt. Built on a diffusion-based 3D VAE architecture with audio injection via Wav2Vec and motion consistency enabled by FramePack compression, it handles full-body movement, facial expressions, and long-form scene continuity with strong identity preservation and lip-sync accuracy.&lt;/p&gt; &lt;p&gt;Demo : &lt;a href="https://youtu.be/Hw9zaXOlU7I"&gt;https://youtu.be/Hw9zaXOlU7I&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model weights : &lt;a href="https://huggingface.co/Wan-AI/Wan2.2-S2V-14B"&gt;https://huggingface.co/Wan-AI/Wan2.2-S2V-14B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Technical Report : &lt;a href="https://humanaigc.github.io/wan-s2v-webpage/content/wan-s2v.pdf"&gt;https://humanaigc.github.io/wan-s2v-webpage/content/wan-s2v.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0pkhj/wan_s2v_reelased_1st_opensourced_ai_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0pkhj/wan_s2v_reelased_1st_opensourced_ai_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0pkhj/wan_s2v_reelased_1st_opensourced_ai_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T15:51:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0kb1d</id>
    <title>InternVL 3.5 released : Best Open-Sourced Multi-Modal LLM, Ranks 3 overall</title>
    <updated>2025-08-26T12:20:27+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0kb1d/internvl_35_released_best_opensourced_multimodal/"&gt; &lt;img alt="InternVL 3.5 released : Best Open-Sourced Multi-Modal LLM, Ranks 3 overall" src="https://external-preview.redd.it/YcFbNVrfuwRpMZYl10KfE37DrxtDi8fi-29iTcISpUY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c3c3b5d958987fc0ab87ba9f72507692db53ed10" title="InternVL 3.5 released : Best Open-Sourced Multi-Modal LLM, Ranks 3 overall" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;InternVL 3.5 has been released, and given the benchmark, the model looks to be the best multi-model LLM, ranking 3 overall just behind Gemini 2.5 Pro and GPT-5. Multiple variants released ranging from 1B to 241B&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5v5hfeg9wclf1.png?width=1787&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2b06d9da57d572ea4ab90008e2ea2763c904f33"&gt;https://preview.redd.it/5v5hfeg9wclf1.png?width=1787&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2b06d9da57d572ea4ab90008e2ea2763c904f33&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The team has introduced a number of new technical inventions, including &lt;em&gt;Cascade RL, Visual Resolution Router, Decoupled Vision-Language Deployment.&lt;/em&gt; &lt;/p&gt; &lt;p&gt;Model weights : &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-8B"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tech report : &lt;a href="https://arxiv.org/abs/2508.18265"&gt;https://arxiv.org/abs/2508.18265&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video summary : &lt;a href="https://www.youtube.com/watch?v=hYrdHfLS6e0"&gt;https://www.youtube.com/watch?v=hYrdHfLS6e0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0kb1d/internvl_35_released_best_opensourced_multimodal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0kb1d/internvl_35_released_best_opensourced_multimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0kb1d/internvl_35_released_best_opensourced_multimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T12:20:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0nbih</id>
    <title>Wan-AI/Wan2.2-S2V-14B ¬∑ Hugging Face</title>
    <updated>2025-08-26T14:26:43+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0nbih/wanaiwan22s2v14b_hugging_face/"&gt; &lt;img alt="Wan-AI/Wan2.2-S2V-14B ¬∑ Hugging Face" src="https://external-preview.redd.it/4TRGFXGIVFwdwj9_01KulvW5c-oJPbLrLYw7udu9cqc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac2ca6a3cef9ab3cfd3e94820eb94dccc92be218" title="Wan-AI/Wan2.2-S2V-14B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wan-S2V is an AI video generation model that can transform static images and audio into high-quality videos.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Wan-AI/Wan2.2-S2V-14B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0nbih/wanaiwan22s2v14b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0nbih/wanaiwan22s2v14b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T14:26:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0haub</id>
    <title>I pre-trained Gemma3 270m entirely from scratch</title>
    <updated>2025-08-26T09:36:43+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0haub/i_pretrained_gemma3_270m_entirely_from_scratch/"&gt; &lt;img alt="I pre-trained Gemma3 270m entirely from scratch" src="https://external-preview.redd.it/BE2F9tVIKL9AN2T5zS4Z4ig6RgU9hM-QoHxWkSh5XTQ.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cd6fc22120dd0f86f8e67b629bd0ad915a09ad61" title="I pre-trained Gemma3 270m entirely from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/9tmq5sa73clf1.gif"&gt;https://i.redd.it/9tmq5sa73clf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I made a video on this topic here: &lt;a href="https://youtu.be/bLDlwcl6hbA?si=1bxlObPOTw2n1TPB"&gt;https://youtu.be/bLDlwcl6hbA?si=1bxlObPOTw2n1TPB&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here is what I cover in this video: &lt;/p&gt; &lt;p&gt;(1) Introduction&lt;/p&gt; &lt;p&gt;(2) Dataset loading&lt;/p&gt; &lt;p&gt;(3) Tokenisation&lt;/p&gt; &lt;p&gt;(4) Creating input-output pairs&lt;/p&gt; &lt;p&gt;(5) Building the Gemma 3 270M architecture&lt;/p&gt; &lt;p&gt;(6) Pre-training&lt;/p&gt; &lt;p&gt;(7) Inference&lt;/p&gt; &lt;p&gt;Attached is a GIF showing my lecture notes!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0haub/i_pretrained_gemma3_270m_entirely_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0haub/i_pretrained_gemma3_270m_entirely_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0haub/i_pretrained_gemma3_270m_entirely_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T09:36:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0yukc</id>
    <title>Hermes 4 Benchmarks</title>
    <updated>2025-08-26T21:43:19+00:00</updated>
    <author>
      <name>/u/notrdm</name>
      <uri>https://old.reddit.com/user/notrdm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0yukc/hermes_4_benchmarks/"&gt; &lt;img alt="Hermes 4 Benchmarks" src="https://b.thumbs.redditmedia.com/GJEF_PqRT8Gr7CSqzTDDlWsTocV8Govd0ipEB9laC5Y.jpg" title="Hermes 4 Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Technical Report: &lt;a href="https://arxiv.org/pdf/2508.18255"&gt;https://arxiv.org/pdf/2508.18255&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notrdm"&gt; /u/notrdm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n0yukc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0yukc/hermes_4_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0yukc/hermes_4_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T21:43:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1n11e9y</id>
    <title>MarvisTTS - Efficient Real-time Voice Cloning with Streaming Speech Synthesis</title>
    <updated>2025-08-26T23:30:19+00:00</updated>
    <author>
      <name>/u/aratahikaru5</name>
      <uri>https://old.reddit.com/user/aratahikaru5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From the &lt;a href="https://github.com/Marvis-Labs/marvis-tts"&gt;repository&lt;/a&gt;:&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Model Description&lt;/h2&gt; &lt;p&gt;Marvis is built on the &lt;a href="https://huggingface.co/sesame/csm-1b"&gt;Sesame CSM-1B&lt;/a&gt; (Conversational Speech Model) architecture, a multimodal transformer that operates directly on Residual Vector Quantization (RVQ) tokens and uses &lt;a href="https://huggingface.co/kyutai/mimi"&gt;Kyutai's mimi codec&lt;/a&gt;. The architecture enables end-to-end training while maintaining low-latency generation and employs a dual-transformer approach:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Multimodal Backbone (250M parameters)&lt;/strong&gt;: Processes interleaved text and audio sequences to model the zeroth codebook level, providing semantic understanding and context.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Audio Decoder (60M parameters)&lt;/strong&gt;: A smaller, specialized transformer that models the remaining 31 codebook levels to reconstruct high-quality speech from the backbone's representations.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key Architectural Innovation&lt;/strong&gt;: Unlike models that require text chunking based on regex patterns, Marvis processes entire text sequences contextually, resulting in more natural speech flow and intonation.&lt;/p&gt; &lt;h2&gt;Key Features&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Rapid Voice Cloning&lt;/strong&gt;: Clone any voice using just 10 seconds of reference audio&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time Streaming&lt;/strong&gt;: Stream audio chunks as text is processed, enabling natural conversational flow&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Compact Size&lt;/strong&gt;: Only 500MB when quantized, enabling on-device inference&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Edge deployment&lt;/strong&gt;: Optimized for real-time Speech-to-Speech (STS) on mobile devices (i.e., iPad, iPhone and etc)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Natural Audio Flow&lt;/strong&gt;: Process entire text context for coherent speech synthesis without chunking artifacts&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multimodal Architecture&lt;/strong&gt;: Seamlessly handles interleaved text and audio tokens&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Supported Languages&lt;/h2&gt; &lt;p&gt;Currently optimized for English with support for expressive speech synthesis with additional languages such as German, Portuguese, French and Mandarin coming soon.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;a href="https://x.com/Prince_Canuma/status/1960399829290426448"&gt;Announcement thread&lt;/a&gt; | &lt;a href="https://huggingface.co/Marvis-AI/marvis-tts-250m-v0.1"&gt;Model card&lt;/a&gt; | &lt;a href="https://huggingface.co/collections/Marvis-AI/marvis-tts-250m-v01-68adf13f5f59206e3910502a"&gt;Model collection&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aratahikaru5"&gt; /u/aratahikaru5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n11e9y/marvistts_efficient_realtime_voice_cloning_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n11e9y/marvistts_efficient_realtime_voice_cloning_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n11e9y/marvistts_efficient_realtime_voice_cloning_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T23:30:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1n12aqj</id>
    <title>Deepseek changes their API price again</title>
    <updated>2025-08-27T00:10:24+00:00</updated>
    <author>
      <name>/u/Pro-editor-1105</name>
      <uri>https://old.reddit.com/user/Pro-editor-1105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n12aqj/deepseek_changes_their_api_price_again/"&gt; &lt;img alt="Deepseek changes their API price again" src="https://preview.redd.it/x6keqt10fglf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8ec2bcfd599ff48e74e4fe29bfdc5460aeaec90" title="Deepseek changes their API price again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is far less attractive tbh. Basically they said R1 and V3 were going with a price now of 0.07 (0.56 cache miss) and 1.12, now that 1.12 is now 1.68. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pro-editor-1105"&gt; /u/Pro-editor-1105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x6keqt10fglf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n12aqj/deepseek_changes_their_api_price_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n12aqj/deepseek_changes_their_api_price_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T00:10:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0iho2</id>
    <title>LLM speedup breakthrough? 53x faster generation and 6x prefilling from NVIDIA</title>
    <updated>2025-08-26T10:48:28+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0iho2/llm_speedup_breakthrough_53x_faster_generation/"&gt; &lt;img alt="LLM speedup breakthrough? 53x faster generation and 6x prefilling from NVIDIA" src="https://preview.redd.it/g8lwztnlfclf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b45eb7eb720e8c27adcd24d4808bef43e5cb8dad" title="LLM speedup breakthrough? 53x faster generation and 6x prefilling from NVIDIA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;source: &lt;a href="https://arxiv.org/pdf/2508.15884v1"&gt;https://arxiv.org/pdf/2508.15884v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g8lwztnlfclf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0iho2/llm_speedup_breakthrough_53x_faster_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0iho2/llm_speedup_breakthrough_53x_faster_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T10:48:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0tgrr</id>
    <title>nano-banana is a MASSIVE jump forward in image editing</title>
    <updated>2025-08-26T18:16:15+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0tgrr/nanobanana_is_a_massive_jump_forward_in_image/"&gt; &lt;img alt="nano-banana is a MASSIVE jump forward in image editing" src="https://preview.redd.it/7kcykqmxnelf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c71a63e7a49527931c15a14e3dbb88e861587ab4" title="nano-banana is a MASSIVE jump forward in image editing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7kcykqmxnelf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0tgrr/nanobanana_is_a_massive_jump_forward_in_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0tgrr/nanobanana_is_a_massive_jump_forward_in_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T18:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1n0us6p</id>
    <title>Nous Research presents Hermes 4</title>
    <updated>2025-08-26T19:06:53+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0us6p/nous_research_presents_hermes_4/"&gt; &lt;img alt="Nous Research presents Hermes 4" src="https://external-preview.redd.it/NQUFFcCjHt1BJkc3XZx_qrQGOmxnmvDswSz5yNpH4xs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=963a55e599f5d49840779052d831759babb45c21" title="Nous Research presents Hermes 4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Edit: &lt;a href="https://huggingface.co/collections/NousResearch/hermes-4-collection-68a731bfd452e20816725728"&gt;HF collection&lt;/a&gt;&lt;br /&gt; My long-awaited open-source masterpiece&lt;/p&gt; &lt;p&gt;&lt;a href="https://hermes4.nousresearch.com"&gt;https://hermes4.nousresearch.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2508.18255"&gt;Paper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://chat.nousresearch.com/"&gt;Chat&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0us6p/nous_research_presents_hermes_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n0us6p/nous_research_presents_hermes_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n0us6p/nous_research_presents_hermes_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-26T19:06:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
