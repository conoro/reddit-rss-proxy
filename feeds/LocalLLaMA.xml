<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-25T22:06:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p6hrzg</id>
    <title>Sharing my poor experience with Apple's foundation models, positive experiences with Qwen3 8b model, and self hosting it all on an old Mac mini for a website I created</title>
    <updated>2025-11-25T16:58:29+00:00</updated>
    <author>
      <name>/u/busymom0</name>
      <uri>https://old.reddit.com/user/busymom0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6hrzg/sharing_my_poor_experience_with_apples_foundation/"&gt; &lt;img alt="Sharing my poor experience with Apple's foundation models, positive experiences with Qwen3 8b model, and self hosting it all on an old Mac mini for a website I created" src="https://preview.redd.it/ado2pjutof3g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f74f6399121da4d91a20bc417ad36c3e0821f165" title="Sharing my poor experience with Apple's foundation models, positive experiences with Qwen3 8b model, and self hosting it all on an old Mac mini for a website I created" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/busymom0"&gt; /u/busymom0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ado2pjutof3g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6hrzg/sharing_my_poor_experience_with_apples_foundation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6hrzg/sharing_my_poor_experience_with_apples_foundation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T16:58:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6dnkg</id>
    <title>Calling a Finetune/LoRA Wizard: Need Dataset Tips for RP Model</title>
    <updated>2025-11-25T14:21:16+00:00</updated>
    <author>
      <name>/u/AmpedHorizon</name>
      <uri>https://old.reddit.com/user/AmpedHorizon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've always wanted to do my own fine-tune/LoRA/QLoRA and I'm trying to get a better sense of the dataset size needed. The plan is to build a dataset in a specific style, but before committing time (and money), I'd really like to get a better sense of how to start properly without overshooting or undershooting.&lt;/p&gt; &lt;p&gt;Let's assume:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We want to fine-tune a ~12B base model using a new clean dataset&lt;/li&gt; &lt;li&gt;To make a general roleplay model, not tied to a single character, but with a certain structure&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;When we ignore the technical part and focus on creating the dataset in theory, for this kind of project, what's a good starting point? 30k examples in the dataset? More? Less?&lt;/p&gt; &lt;p&gt;If anyone has experience or resources they can share, that would be amazing (even rules of thumb). Or maybe a legendary finetuner around who can offer some guidance or practical tips on planning the dataset? If there's interest, I would also document my journey.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AmpedHorizon"&gt; /u/AmpedHorizon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6dnkg/calling_a_finetunelora_wizard_need_dataset_tips/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6dnkg/calling_a_finetunelora_wizard_need_dataset_tips/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6dnkg/calling_a_finetunelora_wizard_need_dataset_tips/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T14:21:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5wjia</id>
    <title>Opus 4.5 only narrowly reclaims #1 on official SWE-bench leaderboard (independent evaluation); cheaper than previous versions, but still more expensive than others</title>
    <updated>2025-11-24T23:18:13+00:00</updated>
    <author>
      <name>/u/klieret</name>
      <uri>https://old.reddit.com/user/klieret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5wjia/opus_45_only_narrowly_reclaims_1_on_official/"&gt; &lt;img alt="Opus 4.5 only narrowly reclaims #1 on official SWE-bench leaderboard (independent evaluation); cheaper than previous versions, but still more expensive than others" src="https://b.thumbs.redditmedia.com/jl1e-X5a7OxM7ptJPHbpLZFTeFQd-7kAhlFa8Y3ToLA.jpg" title="Opus 4.5 only narrowly reclaims #1 on official SWE-bench leaderboard (independent evaluation); cheaper than previous versions, but still more expensive than others" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm from the SWE-bench team. We maintain a leaderboard where we evaluate all models with the exact same agent and prompts so that we can compare models apple-to-apple.&lt;/p&gt; &lt;p&gt;We just finished evaluating Opus 4.5 and it's back at #1 on the leaderboard. However, it's by quite a small margin (only 0.2%pts ahead of Gemini 3, i.e., just a single task) and it's clearly more expensive than the other models that achieve top scores.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/svt1p1b9fa3g1.png?width=3160&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f4ea5388eebbc540d03bdfa101614411dcb55a62"&gt;https://preview.redd.it/svt1p1b9fa3g1.png?width=3160&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f4ea5388eebbc540d03bdfa101614411dcb55a62&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Interestingly, Opus 4.5 takes fewer steps than Sonnet 4.5. About as many as Gemini 3 Pro, but much more than the GPT-5.1 models.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sx5o0e9cfa3g1.png?width=2251&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=68dd5df936d150ef8b697f150ddcd365f50f909e"&gt;https://preview.redd.it/sx5o0e9cfa3g1.png?width=2251&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=68dd5df936d150ef8b697f150ddcd365f50f909e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you want to get maximum performance, you should set the step limit to at least 100:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/52gyo5pefa3g1.png?width=2009&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfdaf2b849abe875e0693beb08da4d1e9e0a5678"&gt;https://preview.redd.it/52gyo5pefa3g1.png?width=2009&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bfdaf2b849abe875e0693beb08da4d1e9e0a5678&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Limiting the max number of steps also allows you to balance avg cost vs performance (interestingly Opus 4.5 can be more cost-efficient than Sonnet 4.5 for lower step limits). &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gymvl4hffa3g1.png?width=2009&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c6cfd7a42eec0c88d8401fad5cfcef8a2f3a693"&gt;https://preview.redd.it/gymvl4hffa3g1.png?width=2009&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c6cfd7a42eec0c88d8401fad5cfcef8a2f3a693&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can find all other models at &lt;a href="http://swebench.com"&gt;swebench.com&lt;/a&gt; (will be updated in the next hour with the new results). You can also reproduce the numbers by using &lt;a href="https://github.com/SWE-agent/mini-swe-agent/"&gt;https://github.com/SWE-agent/mini-swe-agent/&lt;/a&gt; [MIT license]. There is a tutorial in the documentation on how to evaluate on SWE-bench (it's a 1-liner).&lt;/p&gt; &lt;p&gt;We're also currently evaluating minimax-m2 and other open source models and will be back with a comparison of the most open source models soon (we tend to take a bit longer at evaluating these because it often has more infra/logistics hiccups)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klieret"&gt; /u/klieret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5wjia/opus_45_only_narrowly_reclaims_1_on_official/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5wjia/opus_45_only_narrowly_reclaims_1_on_official/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5wjia/opus_45_only_narrowly_reclaims_1_on_official/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T23:18:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6nf1r</id>
    <title>How I replaced Gemini CLI &amp; Copilot with a local stack using Ollama, Continue.dev and MCP servers</title>
    <updated>2025-11-25T20:25:03+00:00</updated>
    <author>
      <name>/u/aaronsky</name>
      <uri>https://old.reddit.com/user/aaronsky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the last few weeks I‚Äôve been trying to get off the treadmill of cloud AI assistants (Gemini CLI, Copilot, Claude-CLI, etc.) and move everything to a local stack.&lt;/p&gt; &lt;p&gt;Goals:&lt;/p&gt; &lt;p&gt;- Keep code on my machine&lt;/p&gt; &lt;p&gt;- Stop paying monthly for autocomplete&lt;/p&gt; &lt;p&gt;- Still get ‚Äúassistant-level‚Äù help in the editor&lt;/p&gt; &lt;p&gt;The stack I ended up with:&lt;/p&gt; &lt;p&gt;- Ollama for local LLMs (Nemotron-9B, Qwen3-8B, etc.)&lt;/p&gt; &lt;p&gt;- &lt;a href="http://Continue.dev"&gt;Continue.dev&lt;/a&gt; inside VS Code for chat + agents&lt;/p&gt; &lt;p&gt;- MCP servers (Filesystem, Git, Fetch, XRAY, SQLite, Snyk‚Ä¶) as tools&lt;/p&gt; &lt;p&gt;What it can do in practice:&lt;/p&gt; &lt;p&gt;- Web research from inside VS Code (Fetch)&lt;/p&gt; &lt;p&gt;- Multi-file refactors &amp;amp; impact analysis (Filesystem + XRAY)&lt;/p&gt; &lt;p&gt;- Commit/PR summaries and diff review (Git)&lt;/p&gt; &lt;p&gt;- Local DB queries (SQLite)&lt;/p&gt; &lt;p&gt;- Security / error triage (Snyk / Sentry)&lt;/p&gt; &lt;p&gt;I wrote everything up here, including:&lt;/p&gt; &lt;p&gt;- Real laptop specs (Win 11 + RTX 6650M, 8 GB VRAM)&lt;/p&gt; &lt;p&gt;- Model selection tips (GGUF ‚Üí Ollama)&lt;/p&gt; &lt;p&gt;- Step-by-step setup&lt;/p&gt; &lt;p&gt;- Example ‚Äúagent‚Äù workflows (PR triage bot, dep upgrader, docs bot, etc.)&lt;/p&gt; &lt;p&gt;Main article:&lt;/p&gt; &lt;p&gt;&lt;a href="https://aiandsons.com/blog/local-ai-stack-ollama-continue-mcp"&gt;https://aiandsons.com/blog/local-ai-stack-ollama-continue-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo with docs &amp;amp; config:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/aar0nsky/blog-post-local-agent-mcp"&gt;https://github.com/aar0nsky/blog-post-local-agent-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also cross-posted to Medium if that‚Äôs easier to read:&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@a.ankiel/ditch-the-monthly-fees-a-more-powerful-alternative-to-gemini-and-copilot-f4563f6530b7"&gt;https://medium.com/@a.ankiel/ditch-the-monthly-fees-a-more-powerful-alternative-to-gemini-and-copilot-f4563f6530b7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious how other people are doing local-first dev assistants (what models + tools you‚Äôre using).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aaronsky"&gt; /u/aaronsky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6nf1r/how_i_replaced_gemini_cli_copilot_with_a_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6nf1r/how_i_replaced_gemini_cli_copilot_with_a_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6nf1r/how_i_replaced_gemini_cli_copilot_with_a_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T20:25:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5ynpr</id>
    <title>Qwen3-235B-A22B achieves SOTA in EsoBench, Claude 4.5 Opus places 7th. EsoBench tests how well models learn and use a private esolang.</title>
    <updated>2025-11-25T00:50:54+00:00</updated>
    <author>
      <name>/u/neat_space</name>
      <uri>https://old.reddit.com/user/neat_space</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5ynpr/qwen3235ba22b_achieves_sota_in_esobench_claude_45/"&gt; &lt;img alt="Qwen3-235B-A22B achieves SOTA in EsoBench, Claude 4.5 Opus places 7th. EsoBench tests how well models learn and use a private esolang." src="https://b.thumbs.redditmedia.com/OBtMIflKSvaUYAFVz4VdaUGn7N4bjfxcn6myRC9AdSs.jpg" title="Qwen3-235B-A22B achieves SOTA in EsoBench, Claude 4.5 Opus places 7th. EsoBench tests how well models learn and use a private esolang." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is &lt;a href="https://caseys-evals.com/"&gt;my own benchmark.&lt;/a&gt; (Apologies mobile users, I still need to fix the site on mobile D:)&lt;/p&gt; &lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Esoteric_programming_language"&gt;Esolang definition&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I've tested 3 open weights models, and of the course the shiny new Claude 4.5 Opus. New additions:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1)&lt;/strong&gt; Qwen3-235B-A22B thinking, scores 29.4&lt;/p&gt; &lt;p&gt;&lt;strong&gt;7)&lt;/strong&gt; Claude 4.5 Opus, scoring 20.9&lt;/p&gt; &lt;p&gt;&lt;strong&gt;16)&lt;/strong&gt; Deepseek v3.2 exp, scoring 16.2&lt;/p&gt; &lt;p&gt;&lt;strong&gt;17)&lt;/strong&gt; Kimi k2 thinking, scoring 16.1&lt;/p&gt; &lt;p&gt;I was pretty surpised by all results here. Qwen for doing so incredibly well, and the other 3 for underperforming. The Claude models are all run without thinking which kinda handicaps them, so you could argue 4.5 Opus actually did quite well.&lt;/p&gt; &lt;p&gt;The fact that, of the the models I've tested, an open weights model is the current SOTA has really taken me by surprise! Qwen took ages to test though, boy does that model think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neat_space"&gt; /u/neat_space &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p5ynpr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5ynpr/qwen3235ba22b_achieves_sota_in_esobench_claude_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5ynpr/qwen3235ba22b_achieves_sota_in_esobench_claude_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T00:50:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1p683yz</id>
    <title>Thank you all for your contribution with tools and stepping up to help maintain the Epstein 20K dataset</title>
    <updated>2025-11-25T09:26:44+00:00</updated>
    <author>
      <name>/u/tensonaut</name>
      <uri>https://old.reddit.com/user/tensonaut</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are keeping track of any RAG based tools that would help investigative journalists uncover hidden details from the Epstein Files. We got our Github setup earlier today with all your contributions listed: &lt;a href="https://github.com/EF20K/Projects"&gt;https://github.com/EF20K/Projects&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our &lt;a href="https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K"&gt;dataset&lt;/a&gt; is also currently featured on the front page of Hugging Face, so we expect more projects along the way. If you are interested in contributing feel free to reach out - no matter how small it is. Once again we would like to thank all the members of the sub for your support in keeping everything open source!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tensonaut"&gt; /u/tensonaut &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p683yz/thank_you_all_for_your_contribution_with_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p683yz/thank_you_all_for_your_contribution_with_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p683yz/thank_you_all_for_your_contribution_with_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T09:26:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5zz11</id>
    <title>Best Coding LLM as of Nov'25</title>
    <updated>2025-11-25T01:51:36+00:00</updated>
    <author>
      <name>/u/PhysicsPast8286</name>
      <uri>https://old.reddit.com/user/PhysicsPast8286</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Folks, &lt;/p&gt; &lt;p&gt;I have a NVIDIA H100 and have been tasked to find a replacement for Qwen3 32B (non-quantized) model currenly hosted on it. &lt;/p&gt; &lt;p&gt;I‚Äôm looking it to use primarily for Java coding tasks and want the LLM to support atleast 100K context window (input + output). It would be used in a corporate environment so censored models like GPT OSS are also okay if they are good at Java programming.&lt;/p&gt; &lt;p&gt;Can anyone recommend an alternative LLM that would be more suitable for this kind of work?&lt;/p&gt; &lt;p&gt;Appreciate any suggestions or insights!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhysicsPast8286"&gt; /u/PhysicsPast8286 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5zz11/best_coding_llm_as_of_nov25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5zz11/best_coding_llm_as_of_nov25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5zz11/best_coding_llm_as_of_nov25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T01:51:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p66m28</id>
    <title>PipesHub - The Open Source, Self-Hostable Alternative to Microsoft 365 Copilot</title>
    <updated>2025-11-25T07:48:44+00:00</updated>
    <author>
      <name>/u/Effective-Ad2060</name>
      <uri>https://old.reddit.com/user/Effective-Ad2060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I‚Äôm excited to share something we‚Äôve been building for the past few months - &lt;strong&gt;PipesHub&lt;/strong&gt;, a &lt;strong&gt;fully open-source alternative to Microsoft 365 Copilot&lt;/strong&gt; designed to bring powerful Enterprise Search, Agent Builders to every team, without vendor lock-in. The platform brings all your business data together and makes it searchable. It connects with apps like Google Drive, Gmail, Slack, Notion, Confluence, Jira, Outlook, SharePoint, Dropbox, and even local file uploads. You can deploy it and run it with just one docker compose command.&lt;/p&gt; &lt;p&gt;The entire system is built on a &lt;strong&gt;fully event-streaming architecture powered by Kafka&lt;/strong&gt;, making indexing and retrieval scalable, fault-tolerant, and real-time across large volumes of data. PipesHub combines a vector database with a knowledge graph and uses Agentic RAG to deliver highly accurate results. We constrain the LLM to ground truth. Provides Visual citations, reasoning and confidence score. Our implementation says Information not found rather than hallucinating.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep understanding of user, organization and teams with enterprise knowledge graph&lt;/li&gt; &lt;li&gt;Connect to any AI model of your choice including OpenAI, Gemini, Claude, or Ollama (works well with gpt-oss or qwen3 vl)&lt;/li&gt; &lt;li&gt;Use any other provider that supports OpenAI compatible endpoints&lt;/li&gt; &lt;li&gt;Vision-Language Models and OCR for visual or scanned docs&lt;/li&gt; &lt;li&gt;Login with Google, Microsoft, OAuth, or SSO&lt;/li&gt; &lt;li&gt;Rich REST APIs for developers&lt;/li&gt; &lt;li&gt;All major file types support including pdfs with images, diagrams and charts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Features releasing this month&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agent Builder - Perform actions like Sending mails, Schedule Meetings, etc along with Search, Deep research, Internet search and more&lt;/li&gt; &lt;li&gt;Reasoning Agent that plans before executing tasks&lt;/li&gt; &lt;li&gt;40+ Connectors allowing you to connect to your entire business apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check it out and share your thoughts or feedback. Your feedback is immensely valuable and is much appreciated:&lt;br /&gt; &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo Video:&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=xA9m3pwOgz8"&gt;https://www.youtube.com/watch?v=xA9m3pwOgz8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective-Ad2060"&gt; /u/Effective-Ad2060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p66m28/pipeshub_the_open_source_selfhostable_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p66m28/pipeshub_the_open_source_selfhostable_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p66m28/pipeshub_the_open_source_selfhostable_alternative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T07:48:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6eb24</id>
    <title>cyankiwi AWQ v1.0</title>
    <updated>2025-11-25T14:47:56+00:00</updated>
    <author>
      <name>/u/_cpatonn</name>
      <uri>https://old.reddit.com/user/_cpatonn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thank you for using my model from my personal account cpatonn so far. I am happy to introduce cyankiwi AWQ v1.0 with 4bit quantized model achieving accuracy degradation of less than 1%, an improvement from my AWQ quants on my personal account cpatonn. cyankiwi AWQ v1.0 models will be labelled in our modelcards.&lt;/p&gt; &lt;p&gt;The following table compares wikitext byte perplexity (lower is better) of some cyankiwi AWQ v1.0 quantized models. Perplexity increases range from negatives (decreases) to 0.6%!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;/th&gt; &lt;th align="left"&gt;Base Model&lt;/th&gt; &lt;th align="left"&gt;cyankiwi AWQ 8bit&lt;/th&gt; &lt;th align="left"&gt;cyankiwi AWQ 4bit&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3-Next-80B-A3B-Instruct&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1.48256&lt;/td&gt; &lt;td align="left"&gt;1.48258&lt;/td&gt; &lt;td align="left"&gt;1.48602&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Kimi-Linear-48B-A3B-Instruct&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1.54038&lt;/td&gt; &lt;td align="left"&gt;1.54041&lt;/td&gt; &lt;td align="left"&gt;1.54194&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;MiniMax-M2&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1.54984&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;1.54743&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1.80803&lt;/td&gt; &lt;td align="left"&gt;1.80776&lt;/td&gt; &lt;td align="left"&gt;1.79795&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Please, please and please let me know your thoughts on my prior quants, and what you expect in the future, as I always aim to improve my products! For more complex queries or feedback, please get in touch with me at &lt;a href="mailto:ton@cyan.kiwi"&gt;ton@cyan.kiwi&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_cpatonn"&gt; /u/_cpatonn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6eb24/cyankiwi_awq_v10/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6eb24/cyankiwi_awq_v10/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6eb24/cyankiwi_awq_v10/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T14:47:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6mmb1</id>
    <title>Trying to build a "Jarvis" that never phones home - on-device AI with full access to your digital life (free beta, roast us)</title>
    <updated>2025-11-25T19:54:58+00:00</updated>
    <author>
      <name>/u/ipav9</name>
      <uri>https://old.reddit.com/user/ipav9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6mmb1/trying_to_build_a_jarvis_that_never_phones_home/"&gt; &lt;img alt="Trying to build a &amp;quot;Jarvis&amp;quot; that never phones home - on-device AI with full access to your digital life (free beta, roast us)" src="https://preview.redd.it/loj0n38hkg3g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=34d3a49ae932dab6a03739299fbb51a8a2573cf6" title="Trying to build a &amp;quot;Jarvis&amp;quot; that never phones home - on-device AI with full access to your digital life (free beta, roast us)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey r/LocalLLaMA,&lt;/p&gt; &lt;p&gt;I know, I know - another &amp;quot;we built something&amp;quot; post. I'll be upfront: this is about something we made, so feel free to scroll past if that's not your thing. But if you're into local inference and privacy-first AI with a WhatsApp/Signal-grade E2E encryption flavor, maybe stick around for a sec.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who we are&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We're Ivan and Dan - two devs from London who've been boiling in the AI field for a while and got tired of the &amp;quot;trust us with your data&amp;quot; model that every AI company seems to push.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What we built and why&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We believe today's AI assistants are powerful but fundamentally disconnected from your actual life. Sure, you can feed ChatGPT a document or paste an email to get a smart-sounding reply. But that's not where AI gets truly useful. Real usefulness comes when AI has real-time access to your entire digital footprint - documents, notes, emails, calendar, photos, health data, maybe even your journal. That level of context is what makes AI actually proactive instead of just reactive.&lt;/p&gt; &lt;p&gt;But here's the hard sell: who's ready to hand all of that to OpenAI, Google, or Meta in one go? We weren't. So we built Atlantis - a two-app ecosystem (desktop + mobile) where all AI processing happens locally. No cloud calls, no &amp;quot;we promise we won't look at your data&amp;quot; - just on-device inference.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it actually does&lt;/strong&gt; (in beta right now):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Morning briefings&lt;/strong&gt; - your starting point for a true &amp;quot;Jarvis&amp;quot;-like AI experience (see demo video on product's main web page)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HealthKit integration&lt;/strong&gt; - ask about your health data (stays on-device where it belongs)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Document vault &amp;amp; email access&lt;/strong&gt; - full context without the cloud compromise&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-term memory&lt;/strong&gt; - AI that actually remembers your conversation history across the chats&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic search&lt;/strong&gt; - across files, emails, and chat history&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reminders &amp;amp; weather&lt;/strong&gt; - the basics, done privately&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why I'm posting here specifically&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This community actually understands local LLMs, their limitations, and what makes them useful (or not). You're also allergic to BS, which is exactly what we need right now.&lt;/p&gt; &lt;p&gt;We're in beta and it's completely free. No catch, no &amp;quot;free tier with limitations&amp;quot; - we're genuinely trying to figure out what matters to users before we even think about monetization.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What we're hoping for:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Brutal honesty about what works and what doesn't&lt;/li&gt; &lt;li&gt;Ideas on what would make this actually useful for your workflow&lt;/li&gt; &lt;li&gt;Technical questions about our architecture (happy to get into the weeds)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Link if you're curious:&lt;/strong&gt; &lt;a href="https://roia.io/atlantis?utm_source=reddit&amp;amp;utm_medium=social&amp;amp;utm_campaign=atlantis_intro_article&amp;amp;utm_content=r_LocalLLaMA"&gt;https://roia.io&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Not asking for upvotes or smth. Just feedback from people who know what they're talking about. Roast us if we deserve it - we'd rather hear it now than after we've gone down the wrong path.&lt;/p&gt; &lt;p&gt;Happy to answer any questions in the comments.&lt;/p&gt; &lt;p&gt;P.S. Before the tomatoes start flying - yes, we're Mac/iOS only at the moment. Windows, Linux, and Android are on the roadmap after our prod rollout in Q2. We had to start somewhere, and we promise we haven't forgotten about you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ipav9"&gt; /u/ipav9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/loj0n38hkg3g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6mmb1/trying_to_build_a_jarvis_that_never_phones_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6mmb1/trying_to_build_a_jarvis_that_never_phones_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T19:54:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5qzft</id>
    <title>Coursera Founder And AI Pioneer Andrew Ng Just Dropped An AI Reviewer That Performs At Human Level</title>
    <updated>2025-11-24T19:44:03+00:00</updated>
    <author>
      <name>/u/AskGpts</name>
      <uri>https://old.reddit.com/user/AskGpts</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5qzft/coursera_founder_and_ai_pioneer_andrew_ng_just/"&gt; &lt;img alt="Coursera Founder And AI Pioneer Andrew Ng Just Dropped An AI Reviewer That Performs At Human Level" src="https://preview.redd.it/xslefnsmd93g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d03c76fdecbb27a88d2b15a9cd2eaa3d225b151c" title="Coursera Founder And AI Pioneer Andrew Ng Just Dropped An AI Reviewer That Performs At Human Level" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Andrew Ng just announced a new Agentic Reviewer that gives research feedback approaching human-level performance.&lt;/p&gt; &lt;p&gt;It was trained on ICLR 2025 reviews and scored:&lt;/p&gt; &lt;p&gt;0.41 correlation between two human reviewers&lt;/p&gt; &lt;p&gt;0.42 correlation between the AI and a human reviewer&lt;/p&gt; &lt;p&gt;Meaning: The AI reviewer is now effectively as reliable as a human reviewer. And it can potentially replace the 6-month feedback loop researchers normally suffer through when submitting papers.&lt;/p&gt; &lt;p&gt;It searches arXiv for context, analyzes your paper, and returns structured review comments instantly.&lt;/p&gt; &lt;p&gt;For anyone who‚Äôs had a paper rejected multiple times and waited months each round‚Ä¶ this could be game-changing.&lt;/p&gt; &lt;p&gt;Try the tool here:&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="https://paperreview.ai"&gt;https://paperreview.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AskGpts"&gt; /u/AskGpts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xslefnsmd93g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5qzft/coursera_founder_and_ai_pioneer_andrew_ng_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5qzft/coursera_founder_and_ai_pioneer_andrew_ng_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T19:44:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6l1lz</id>
    <title>SearXNG-LDR-Academic: I made a "safe for work" fork of SearXNG optimized for use with LearningCircuit's Local Deep Research Tool.</title>
    <updated>2025-11-25T18:57:25+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: I forked SearXNG and stripped out all the NSFW stuff to keep University/Corporate IT happy (removed Pirate Bay search, Torrent search, shadow libraries, etc). I added several academic research-focused search engines (Semantic Scholar, WolfRam Alpha, PubMed, and others), and made the whole thing super easy to pair with Learning Circuit‚Äôs excellent Local Deep Research tool which works entirely local using local inference. Here‚Äôs my fork: &lt;a href="https://github.com/porespellar/searxng-LDR-academic"&gt;https://github.com/porespellar/searxng-LDR-academic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôve been testing LearningCircuit‚Äôs Local Deep Research tool recently, and frankly, it‚Äôs incredible. When paired with a decent local high-context model (I‚Äôm using gpt-OSS-120b at 128k context), it can produce massive, relatively slop-free, 100+ page coherent deep-dive documents with full clickable citations. It beats the stew out most other ‚Äúdeep research‚Äù offerings I‚Äôve seen (even from commercial model providers). I can't stress enough how good the output of this thing is in its &amp;quot;Detailed Report&amp;quot; mode (after its had about an hour to do its thing). Kudos to the LearningCicuits team for building such an awesome Deep Research tool for us local LLM users! &lt;/p&gt; &lt;p&gt;Anyways, the default SearXNG back-end (used by Local Deep Research) has two major issues that bothered me enough to make a fork for my use case:&lt;/p&gt; &lt;p&gt;Issue 1 - Default SearXNG often routes through engines that search torrents, Pirate Bay, and NSFW content. For my use case, I need to run this for academic-type research on University/Enterprise networks without setting off every alarm in the SOC. I know I can disable these engines manually, but I would rather not have to worry about them in the first place (Btw, Pirate Bay is default-enabled in the default SearXNG container for some unknown reason).&lt;/p&gt; &lt;p&gt;Issue 2 - For deep academic research, having the agent scrape social media or entertainment sites wastes tokens and introduces irrelevant noise.&lt;/p&gt; &lt;p&gt;What my fork does: (searxng-LDR-academic)&lt;/p&gt; &lt;p&gt;I decided to build a pre-configured, single-container fork designed to be a drop-in replacement for the standard SearXNG container. My fork features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sanitized Sources: &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Removed Torrent, Music, Video, and Social Media categories. It‚Äôs pure text/data focus now.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Academic-focus: &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Added several additional search engine choices, including: Semantic Scholar, Wolfram Alpha, PubMed, ArXiv, and other scientific indices (enabled by default, can be disabled in preferences).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Shadow Library Removal: &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Disabled shadow libraries to ensure the output is strictly compliant for workplace/academic citations.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Drop-in Ready: &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Configured to match LearningCircuit‚Äôs expected container names and ports out of the box to make integration with Local Deep Research easy.&lt;/p&gt; &lt;p&gt;Why use this fork?&lt;/p&gt; &lt;p&gt;If you are trying to use agentic research tools in a professional environment or for a class project, this fork minimizes the risk of your agent scraping &amp;quot;dodgy&amp;quot; parts of the web and returning flagged URLs. It also tends to keep the LLM more focused on high-quality literature since the retrieval pool is cleaner.&lt;/p&gt; &lt;p&gt;What‚Äôs in it for you, Porespellar? &lt;/p&gt; &lt;p&gt;Nothing, I just thought maybe someone else might find it useful and I thought I would share it with the community. If you like it, you can give it a star on GitHub to increase its visibility but you don‚Äôt have to. &lt;/p&gt; &lt;p&gt;The Repos:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;My Fork of SearXNG:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/porespellar/searxng-LDR-academic"&gt;https://github.com/porespellar/searxng-LDR-academic&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Tool it's meant to work with: &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Local Deep Research): &lt;a href="https://github.com/LearningCircuit/local-deep-research"&gt;https://github.com/LearningCircuit/local-deep-research&lt;/a&gt; (Highly recommend checking them out).&lt;/p&gt; &lt;p&gt;Feedback Request:&lt;/p&gt; &lt;p&gt;I‚Äôm looking to add more specialized academic or technical search engines to the configuration to make it more useful for Local Deep Research. If you have specific engines you use for academic / scientific retrieval (that work well with SearXNG), let me know in the comments and I'll see about adding them to a future release.&lt;/p&gt; &lt;p&gt;Full Disclosure: &lt;/p&gt; &lt;p&gt;I used Gemini 3 Pro and Claude Code to assist in the development of this fork. I security audited the final Docker builds using Trivy and Grype. I am not affiliated with either the LearningCircuit LDR or SearXNG project (just a big fan of both). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6l1lz/searxngldracademic_i_made_a_safe_for_work_fork_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6l1lz/searxngldracademic_i_made_a_safe_for_work_fork_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6l1lz/searxngldracademic_i_made_a_safe_for_work_fork_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T18:57:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1p69bea</id>
    <title>GLiNER2: Unified Schema-Based Information Extraction</title>
    <updated>2025-11-25T10:43:25+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p69bea/gliner2_unified_schemabased_information_extraction/"&gt; &lt;img alt="GLiNER2: Unified Schema-Based Information Extraction" src="https://b.thumbs.redditmedia.com/jbxiujDqa50qKCnpeAe_kYWcLq_ws_q5F6bgTLhSOkM.jpg" title="GLiNER2: Unified Schema-Based Information Extraction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLiNER2 is an efficient, unified information extraction system that combines named entity recognition, text classification, and hierarchical structured data extraction into a single 205M-parameter model. Built on a pretrained transformer encoder architecture and trained on 254,334 examples of real and synthetic data, it achieves competitive performance with large language models while running efficiently on CPU hardware without requiring GPUs or external APIs.&lt;/p&gt; &lt;p&gt;The system uses a schema-based interface where users can define extraction tasks declaratively through simple Python API calls, supporting features like entity descriptions, multi-label classification, nested structures, and multi-task composition in a single forward pass.&lt;/p&gt; &lt;p&gt;Released as an open-source pip-installable library under Apache 2.0 license with pre-trained models on Hugging Face, GLiNER2 demonstrates strong zero-shot performance across benchmarks‚Äîachieving 0.72 average accuracy on classification tasks and 0.590 F1 on the CrossNER benchmark‚Äîwhile maintaining approximately 2.6√ó speedup over GPT-4o on CPU.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2507.18546"&gt;https://arxiv.org/abs/2507.18546&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code repo: &lt;a href="https://github.com/fastino-ai/GLiNER2"&gt;https://github.com/fastino-ai/GLiNER2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Install: &lt;a href="https://pypi.org/project/gliner2"&gt;https://pypi.org/project/gliner2&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p69bea"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p69bea/gliner2_unified_schemabased_information_extraction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p69bea/gliner2_unified_schemabased_information_extraction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T10:43:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1p61ch2</id>
    <title>NVIDIA RTX PRO 6000 Blackwell desktop GPU drops to $7,999</title>
    <updated>2025-11-25T02:56:35+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p61ch2/nvidia_rtx_pro_6000_blackwell_desktop_gpu_drops/"&gt; &lt;img alt="NVIDIA RTX PRO 6000 Blackwell desktop GPU drops to $7,999" src="https://external-preview.redd.it/YCPQesYDmOPQ_XkQN8p_ciK514B0FKoU6bNyhy9mcvg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc94dfb5840da6920f1ea749a1fc15f8c8d11b76" title="NVIDIA RTX PRO 6000 Blackwell desktop GPU drops to $7,999" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you guys think that a RTX Quadro 8000 situation could happen again?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/nvidia-flagship-rtx-pro-6000-is-now-rtx-5080-cheaper-as-card-price-drops-to-7999"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p61ch2/nvidia_rtx_pro_6000_blackwell_desktop_gpu_drops/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p61ch2/nvidia_rtx_pro_6000_blackwell_desktop_gpu_drops/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T02:56:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6gruv</id>
    <title>I tested a few local hosted coding models with VSCode / cline so that you don't have to</title>
    <updated>2025-11-25T16:21:10+00:00</updated>
    <author>
      <name>/u/DrMicrobit</name>
      <uri>https://old.reddit.com/user/DrMicrobit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been running a bunch of &amp;quot;can I actually code with a local model in VS Code?&amp;quot; experiments over the last weeks, focused on task with moderate complexity. I chose simple, well known games as they help to visualise strengths and shortcomings of the results quite easily, also to a layperson. The tasks at hand: Space Invaders &amp;amp; Galaga in a single HTML file. I also did a more serious run with a ~2.3k- word design doc.&lt;/p&gt; &lt;p&gt;Sharing the main takeaways here for anyone trying to use local models with Cline/Ollama for real coding work, not just completions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt; Ubuntu 24.04, 2x 4060 Ti 16 GB (32 GB total VRAM), VS Code + Cline, models served via Ollama / GGUF. Context for local models was usually ~96k tokens (anything much bigger spilled into RAM and became 7-20x slower). Tasks ranged from YOLO prompts (&amp;quot;Write a Space Invaders game in a single HTML file&amp;quot;) to a moderately detailed spec for a modernized Space Invaders.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Headline result:&lt;/strong&gt; Qwen 3 Coder 30B is the only family I tested that consistently worked well with Cline and produced usable games. At 4-bit it's already solid; quality drops noticeably at 3-bit and 2-bit (more logic bugs, more broken runs). With 4-bit and 32 GB VRAM you can keep ~ 100k context and still be reasorably fast. If you can spare more VRAM or live with reduced context, higher-bit Qwen 3 Coder (e.g. 6-bit) does help. But 4-bit is the practical sweet spot for 32 GiB VRAM.&lt;/p&gt; &lt;p&gt;Merges/prunes of Qwen 3 Coder generally underperformed the original. The cerebras REAP 25B prune and YOYO merges were noticeably buggier and less reliable than vanilla Qwen 3 Coder 30B, even at higher bit widths. They sometimes produced runnable code, but with a much higher &amp;quot;Cline has to rerun / you have to hand-debug or giveup&amp;quot; rate. TL;DR: for coding, the unmodified coder models beat their fancy descendants.&lt;/p&gt; &lt;p&gt;Non-coder 30B models and &amp;quot;hot&amp;quot; general models mostly disappointed in this setup. Qwen 3 30B (base/instruct from various sources), devstral 24B, Skyfall 31B v4, Nemotron Nano 9B v2, and Olmo 3 32B either: (a) fought with Cline (rambling, overwriting their own code, breaking the project), or (b) produced very broken game logic that wasn't fixable in one or two debug rounds. Some also forced me to shrink context so much they stopped being interesting for larger tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Guiding the models:&lt;/strong&gt; I wanted to demonstrate, with examples that can be shown to people without much insights, what development means: YOLO prompts (&amp;quot;Make me a Space Invaders / Galaga game&amp;quot;) will produce widely varying results even for big online models, and doubly so for locals. See &lt;a href="https://drmicrobit.github.io/lllm_suit/tests/01_SpaceInvaders_yolo/online/GPT5/t1/space_invaders.html"&gt;this example&lt;/a&gt; for an interesting YOLO from GPT-5, and &lt;a href="https://drmicrobit.github.io/lllm_suit/tests/01_SpaceInvaders_yolo/online/Opus41/t2/space_invaders.html"&gt;this example&lt;/a&gt; for a barebone one from Opus 4.1. Models differ a lot in what they think &amp;quot;Space Invaders&amp;quot; or &amp;quot;Galaga&amp;quot; is, and leave out key features (bunkers, UFO, proper alien movement, etc.).&lt;/p&gt; &lt;p&gt;With a moderately detailed design doc, Qwen 3 Coder 30B can stick reasonably well to spec: &lt;a href="https://drmicrobit.github.io/lllm_suit/tests/03_SpaceInvaders_ddoc01/local/qwen3-coder-30B-unsloth/6bitUD_t1/space_invaders.html"&gt;Example 1&lt;/a&gt;, &lt;a href="https://drmicrobit.github.io/lllm_suit/tests/03_SpaceInvaders_ddoc01/local/qwen3-coder-30B-unsloth/4bitUD_t1/space_invaders.html"&gt;Example 2&lt;/a&gt;, &lt;a href="https://drmicrobit.github.io/lllm_suit/tests/03_SpaceInvaders_ddoc01/local/qwen3-coder-30B-unsloth/4bit_t2/space_invaders.html"&gt;Example 3&lt;/a&gt;. They still tend to repeat certain logic errors (e.g., invader formation movement, missing config entries) and often can't fix them from a high-level bug description without human help.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My current working hypothesis:&lt;/strong&gt; to do enthusiast-level Al-assisted coding in VS Code with Cline, one really needs to have at least 32 GB VRAM for usable models. Preferably use an untampered Qwen 3 Coder 30B (Ollama's default 4-bit, or an unsloth GGUF at 4-6 bits). Avoid going below 4-bit for coding, be wary of fancy merges/prunes, and don't expect miracles without a decent spec. &lt;/p&gt; &lt;p&gt;I documented all runs (code + notes) in a repo on GitHub (&lt;a href="https://github.com/DrMicrobit/lllm_suit"&gt;https://github.com/DrMicrobit/lllm_suit&lt;/a&gt;) if anyone's interested in. The docs there are linked and, going down the experiments, give an idea of what the results looked like with an image and have direct links runnable HTML files, configs, and model variants.&lt;/p&gt; &lt;p&gt;I'd be happy to hear what others think of this kind of simple experimental evaluation, or what other models I could test.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrMicrobit"&gt; /u/DrMicrobit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gruv/i_tested_a_few_local_hosted_coding_models_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gruv/i_tested_a_few_local_hosted_coding_models_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gruv/i_tested_a_few_local_hosted_coding_models_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T16:21:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6o5hf</id>
    <title>Cheapest $/vRAM GPU right now? Is it a good time?</title>
    <updated>2025-11-25T20:53:01+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an rtx 2080 which only has 8Gb vRAM, and I was thinking of upgrading that GPU to an affordable and good $/vRAM ratio GPU. I don't have 8k to drop on an rtx pro 6000 like suggested a few days ago here, I was thinking more in the &amp;lt;1k range.&lt;/p&gt; &lt;p&gt;Here are some options I've seen from most expensive to cheapest:&lt;/p&gt; &lt;p&gt;$1,546 RTX PRO 4000 Blackwell 24 GB GDDR7 $64/Gb&lt;/p&gt; &lt;p&gt;~$900 wait for 5070 ti super? $37/Gb&lt;/p&gt; &lt;p&gt;$800 RTX titan, $33/Gb&lt;/p&gt; &lt;p&gt;$600-800 used 3090, $25-33/Gb &lt;/p&gt; &lt;p&gt;2x$300 mac mini m1 16g cluster using exolabs? (i've used a mac mini cluster before, but it is limited on what you can run) $18/Gb&lt;/p&gt; &lt;p&gt;Is it a good time to guy a GPU? What are your setups like and what can you run in this price range?&lt;/p&gt; &lt;p&gt;I'm worried that the uptrend of RAM prices means GPUs are going to become more expensive in the coming months. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6o5hf/cheapest_vram_gpu_right_now_is_it_a_good_time/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6o5hf/cheapest_vram_gpu_right_now_is_it_a_good_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6o5hf/cheapest_vram_gpu_right_now_is_it_a_good_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T20:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6ksc6</id>
    <title>I built an AI research platform and just open sourced it.</title>
    <updated>2025-11-25T18:48:11+00:00</updated>
    <author>
      <name>/u/CodingWithSatyam</name>
      <uri>https://old.reddit.com/user/CodingWithSatyam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I've been working on Introlix for some months now. So, today I've open sourced it. It was really hard time building it as an student and a solo developer. This project is not finished yet but its on that stage I can show it to others and ask other for help in developing it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I built:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Introlix is an AI-powered research platform. Think of it as &amp;quot;GitHub Copilot meets Google Docs&amp;quot; for research work.&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Research Desk: It is just like google docs but in right side there is an AI pannel where users can ask questions to LLM. And also it can edit or write document for user. So, it is just like github copilot but it is for text editor. There are two modes: Chat and edit. Chat mode is for asking questions and edit mode is for editing the document using AI agent.&lt;/li&gt; &lt;li&gt;Chat: For quick questions you can create a new chat and ask questions.&lt;/li&gt; &lt;li&gt;Workspace: Every chat, and research desk are managed in workspace. A workspace shares data with every items it have. So, when creating an new desk or chat user need to choose a workspace and every items on that workspace will be sharing same data. The data includes the search results and scraped content.&lt;/li&gt; &lt;li&gt;Multiple AI Agents: There are multiple AI agents like: context agent (to understand user prompt better), planner agent, explorer_agent (to search internet), etc.&lt;/li&gt; &lt;li&gt;Auto Format &amp;amp; Reference manage (coming soon): This is a feature to format the document into blog post style or research paper style or any other style and also automatic citation management with inline references.&lt;/li&gt; &lt;li&gt;Local LLMs (coming soon): Will support local llms&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So, I was working alone on this project and because of that codes are little bit messy. And many feature are not that fast. I've never tried to make it perfect as I was focusing on building the MVP. Now after working demo I'll be developing this project into complete working stable project. And I know I can't do it alone. I also want to learn about how to work on very big projects and this could be one of the big opportunity I have. There will be many other students or every other developers that could help me build this project end to end. To be honest I have never open sourced any project before. I have many small project and made it public but never tired to get any help from open source community. So, this is my first time.&lt;/p&gt; &lt;p&gt;I like to get help from senior developers who can guide me on this project and make it a stable project with a lot of features.&lt;/p&gt; &lt;p&gt;Here is github link for technical details: &lt;a href="https://github.com/introlix/introlix"&gt;https://github.com/introlix/introlix&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discord link: &lt;a href="https://discord.gg/mhyKwfVm"&gt;https://discord.gg/mhyKwfVm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note: I've been still working on adding github issues for development plan.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CodingWithSatyam"&gt; /u/CodingWithSatyam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ksc6/i_built_an_ai_research_platform_and_just_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ksc6/i_built_an_ai_research_platform_and_just_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ksc6/i_built_an_ai_research_platform_and_just_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T18:48:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5u44r</id>
    <title>That's why local models are better</title>
    <updated>2025-11-24T21:42:13+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5u44r/thats_why_local_models_are_better/"&gt; &lt;img alt="That's why local models are better" src="https://preview.redd.it/7s5e59vpy93g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91d4c29a99283e56fcfd8614cc10c6d72a0af91a" title="That's why local models are better" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That is why the local ones are better than the private ones in addition to this model is still expensive, I will be surprised when the US models reach an optimized price like those in China, the price reflects the optimization of the model, did you know ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7s5e59vpy93g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5u44r/thats_why_local_models_are_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5u44r/thats_why_local_models_are_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T21:42:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1p68sjf</id>
    <title>tencent/HunyuanOCR-1B</title>
    <updated>2025-11-25T10:10:33+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p68sjf/tencenthunyuanocr1b/"&gt; &lt;img alt="tencent/HunyuanOCR-1B" src="https://external-preview.redd.it/euNO2VS0UsDEnKIxd8MnYm5CABYmnLN8JLKug1m_WZw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6451703bfbd1fab35e662fdb90c099a069b6d25b" title="tencent/HunyuanOCR-1B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/HunyuanOCR"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p68sjf/tencenthunyuanocr1b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p68sjf/tencenthunyuanocr1b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T10:10:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6h63t</id>
    <title>Ryzen AI and Radeon are ready to run LLMs Locally with Lemonade Software</title>
    <updated>2025-11-25T16:36:04+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6h63t/ryzen_ai_and_radeon_are_ready_to_run_llms_locally/"&gt; &lt;img alt="Ryzen AI and Radeon are ready to run LLMs Locally with Lemonade Software" src="https://external-preview.redd.it/JNLQ1TFljv7ecmJGVkBQs2GgRQ8E7p3qY9QpUEtPmD8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88a44b03e9ee2eeae1607d08f00e67f73244cead" title="Ryzen AI and Radeon are ready to run LLMs Locally with Lemonade Software" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.amd.com/en/developer/resources/technical-articles/2025/ryzen-ai-radeon-llms-with-lemonade.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6h63t/ryzen_ai_and_radeon_are_ready_to_run_llms_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6h63t/ryzen_ai_and_radeon_are_ready_to_run_llms_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T16:36:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6gsjh</id>
    <title>LLaDA2.0 (103B/16B) has been released</title>
    <updated>2025-11-25T16:21:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;LLaDA2.0-flash&lt;/strong&gt; is a diffusion language model featuring a 100BA6B Mixture-of-Experts (MoE) architecture. As an enhanced, instruction-tuned iteration of the LLaDA2.0 series, it is optimized for practical applications.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-flash"&gt;https://huggingface.co/inclusionAI/LLaDA2.0-flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LLaDA2.0-mini&lt;/strong&gt; is a diffusion language model featuring a 16BA1B Mixture-of-Experts (MoE) architecture. As an enhanced, instruction-tuned iteration of the LLaDA series, it is optimized for practical applications.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-mini"&gt;https://huggingface.co/inclusionAI/LLaDA2.0-mini&lt;/a&gt;&lt;/p&gt; &lt;p&gt;llama.cpp support in progress &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17454"&gt;https://github.com/ggml-org/llama.cpp/pull/17454&lt;/a&gt;&lt;/p&gt; &lt;p&gt;previous version of LLaDA is supported &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16003"&gt;https://github.com/ggml-org/llama.cpp/pull/16003&lt;/a&gt; already (please check the comments)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gsjh/llada20_103b16b_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gsjh/llada20_103b16b_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6gsjh/llada20_103b16b_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T16:21:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6ht87</id>
    <title>Flux 2 can be run on 24gb vram!!!</title>
    <updated>2025-11-25T16:59:49+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ht87/flux_2_can_be_run_on_24gb_vram/"&gt; &lt;img alt="Flux 2 can be run on 24gb vram!!!" src="https://preview.redd.it/m9ud0rs8pf3g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=815d30594ab759659c5d269629ebb9cd5bd93a40" title="Flux 2 can be run on 24gb vram!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I dont know why people are complaining......&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m9ud0rs8pf3g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ht87/flux_2_can_be_run_on_24gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6ht87/flux_2_can_be_run_on_24gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T16:59:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p6k0h2</id>
    <title>You can now do FP8 reinforcement learning locally! (&lt;5GB VRAM)</title>
    <updated>2025-11-25T18:19:47+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6k0h2/you_can_now_do_fp8_reinforcement_learning_locally/"&gt; &lt;img alt="You can now do FP8 reinforcement learning locally! (&amp;lt;5GB VRAM)" src="https://preview.redd.it/t5wv1iax1g3g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c2fb5f6ea2413c66c20bbe83efc473ce566ff763" title="You can now do FP8 reinforcement learning locally! (&amp;lt;5GB VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We're getting close to our last release of 2025! Thanks so much for all the support this year. The DeepSeek team back in Jan showcased how powerful FP8 RL can be with GRPO. Well, you can now try it on your local hardware using only 5GB VRAM! RTX 50x, 40x series all work! &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why should you do FP8 training?&lt;/strong&gt;&lt;br /&gt; NVIDIA's research finds FP8 training can match BF16 accuracy whilst getting 1.6x faster inference time. We collabed with TorchAO from PyTorch to introduce FP8 RL training, making FP8 GRPO possible on home GPUs with no accuracy loss!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-4B FP8 GRPO works on just 6GB VRAM. Qwen3-1.7B on 5GB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1.4x faster RL training and 2√ó longer context vs BF16/FP16&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;60% less VRAM and 10√ó longer context than other FP8 RL implementations&lt;/li&gt; &lt;li&gt;Unsloth is the only framework that makes FP8 RL LoRA work on consumer GPUs (e.g. NVIDIA RTX 40 &amp;amp; 50 Series). Also runs on H100, H200, B200.&lt;/li&gt; &lt;li&gt;You may notice &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; now uses much less VRAM than before, enabling even longer context. We‚Äôre also implementing faster training soon. Blog coming soon&lt;/li&gt; &lt;li&gt;Our notebooks use 24GB L4s which fit Qwen3-14B as Tesla T4s don‚Äôt support FP8.&lt;/li&gt; &lt;li&gt;Our FP8 RL incorporates Unsloth‚Äôs weight sharing, Standby, Flex Attention + more.&lt;/li&gt; &lt;li&gt;Works on any NVIDIA RTX 40, 50 series and H100, B200 etc. GPUs&lt;/li&gt; &lt;li&gt;Use &lt;code&gt;load_in_fp8 = True&lt;/code&gt; within &lt;code&gt;FastLanguageModel&lt;/code&gt; to enable FP8 RL.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can read our blogpost for our findings and more: &lt;a href="https://docs.unsloth.ai/new/fp8-reinforcement-learning"&gt;https://docs.unsloth.ai/new/fp8-reinforcement-learning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Llama 3.2 1B FP8 Colab Notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama_FP8_GRPO.ipynb"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama_FP8_GRPO.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the notebook, you can plug in any of our previous reward functions or RL environment examples, including our auto kernel creation and our 2048 game notebooks. To enable fp8:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import os; os.environ['UNSLOTH_VLLM_STANDBY'] = &amp;quot;1&amp;quot; # Saves 30% VRAM from unsloth import FastLanguageModel model, tokenizer = FastLanguageModel.from_pretrained( model_name = &amp;quot;unsloth/Qwen3-8B&amp;quot;, max_seq_length = 2048, load_in_4bit = False, # False for LoRA 16bit fast_inference = True, # Enable vLLM fast inference max_lora_rank = 32, load_in_fp8 = True, # Float8 RL / GRPO! ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hope you all have a lovely Thanksgiving, a lovely rest of the week and I'll be here to answer any and all questions! =)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t5wv1iax1g3g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p6k0h2/you_can_now_do_fp8_reinforcement_learning_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p6k0h2/you_can_now_do_fp8_reinforcement_learning_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-25T18:19:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax ‚Äî Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; ‚Äî Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; ‚Äî Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; ‚Äî LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5retd</id>
    <title>Best Local VLMs - November 2025</title>
    <updated>2025-11-24T20:00:04+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite models are right now and &lt;strong&gt;&lt;em&gt;why&lt;/em&gt;&lt;/strong&gt;. Given the nature of the beast in evaluating VLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (what applications, how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T20:00:04+00:00</published>
  </entry>
</feed>
