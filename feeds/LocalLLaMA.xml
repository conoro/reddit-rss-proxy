<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-04T01:58:13+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q2yse3</id>
    <title>50M param PGN-only transformer plays coherent chess without search: Is small-LLM generalization is underrated?</title>
    <updated>2026-01-03T16:24:50+00:00</updated>
    <author>
      <name>/u/Tasty_Share_1357</name>
      <uri>https://old.reddit.com/user/Tasty_Share_1357</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2yse3/50m_param_pgnonly_transformer_plays_coherent/"&gt; &lt;img alt="50M param PGN-only transformer plays coherent chess without search: Is small-LLM generalization is underrated?" src="https://b.thumbs.redditmedia.com/cqk0fjtKNqL9lspX_eRxoCqF6Z2rVcXeGLzb5oTCsIs.jpg" title="50M param PGN-only transformer plays coherent chess without search: Is small-LLM generalization is underrated?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all ‚Äî been poking at Adam Karvonen‚Äôs 50 M-param &lt;strong&gt;Chess GPT&lt;/strong&gt; (nanoGPT architecture, plain PGN in/out, no board tensor, no engine search) and wrapped a tiny UI so you can try it out.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick takeaways&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Surprisingly legal / coherent&lt;/strong&gt; ‚Äî far better than frontier chat models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Feels human:&lt;/strong&gt; samples a move distribution instead of crunching Stockfish lines.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hit me with a castle-mate (O-O-O#) in ~25 moves&lt;/strong&gt; ‚Äî vanishingly rare in real games.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;‚ÄúStockfish-trained‚Äù&lt;/strong&gt; = tuned to &lt;em&gt;imitate&lt;/em&gt; Stockfish‚Äôs choices; the engine itself isn‚Äôt inside.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Temp sweet-spots:&lt;/strong&gt; T ‚âà 0.3 for the Stockfish-style model, T = 0 for the Lichess-style one.&lt;/li&gt; &lt;li&gt;Nice micro-case study of how small, domain-trained LLMs show sharp &lt;em&gt;in-distribution&lt;/em&gt; generalization while giant general models still hallucinate elsewhere.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Write-up (context): &lt;a href="https://chinmaysnotebook.substack.com/p/chessllm-what-a-50m-transformer-says"&gt;https://chinmaysnotebook.substack.com/p/chessllm-what-a-50m-transformer-says&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Live demo: &lt;a href="https://chess-llm-316391656470.us-central1.run.app"&gt;https://chess-llm-316391656470.us-central1.run.app&lt;/a&gt;&lt;/li&gt; &lt;li&gt;HF models: &lt;a href="https://huggingface.co/adamkarvonen/chess_llms/tree/main"&gt;https://huggingface.co/adamkarvonen/chess_llms/tree/main&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Original blog / paper (Karvonen, 2024): &lt;a href="https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html?utm_source=chatgpt.com"&gt;https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious what the &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; crowd thinks‚Äîfeedback welcome!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bkqdqkh5c6bg1.png?width=1684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9764256359eb3e8c59d4cf0a1c025e8ecdbe63e0"&gt;https://preview.redd.it/bkqdqkh5c6bg1.png?width=1684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9764256359eb3e8c59d4cf0a1c025e8ecdbe63e0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tasty_Share_1357"&gt; /u/Tasty_Share_1357 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2yse3/50m_param_pgnonly_transformer_plays_coherent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2yse3/50m_param_pgnonly_transformer_plays_coherent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2yse3/50m_param_pgnonly_transformer_plays_coherent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T16:24:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1q35t11</id>
    <title>Best local models for standardizing medical records into JSON/sql/node/etc.</title>
    <updated>2026-01-03T20:53:01+00:00</updated>
    <author>
      <name>/u/whoooaaahhhh</name>
      <uri>https://old.reddit.com/user/whoooaaahhhh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I‚Äôm trying to build a unified record with all of my medical history from a variety of providers over the years, some of them use mychart, and some of them are simply PDFs of either typed or handwritten documents, I assume the handwritten will be the most difficult. &lt;/p&gt; &lt;p&gt;But, even just to start with the computer generated files from mychart and secondarily, the typed PDFs; which models do you recommend I used to build this comprehensive record and what format would you use? Should I create this in JSON/SQL/Node?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/whoooaaahhhh"&gt; /u/whoooaaahhhh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q35t11/best_local_models_for_standardizing_medical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q35t11/best_local_models_for_standardizing_medical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q35t11/best_local_models_for_standardizing_medical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T20:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1q362uu</id>
    <title>New to AI. Need some help and guidance</title>
    <updated>2026-01-03T21:03:47+00:00</updated>
    <author>
      <name>/u/Big_black_click</name>
      <uri>https://old.reddit.com/user/Big_black_click</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New to AI and I feel a bit lost, and I hope someone can help me out here a bit. It seems like this field leaps forward with every day that passes - there are so many formats, technologies, algorithms, hardware requirements\conditions and so on and so and so on. There's a lot to know (surprise surprise...) and I struggle quite a bit since search engines seem to be somewhat bad right now(?) and documentation seems to a bit lacking (or at least a bit behind). &lt;/p&gt; &lt;p&gt;The first issue I am facing is - I want to run models locally on Ollama as well as LMStudio.&lt;br /&gt; The model I want to run locally is Llama 3.2-11b. I have applied and got approved for Meta's License and followed the instructions and got a &amp;quot;.pth&amp;quot; file and I want to convert it to a GGUF file so I could use it in both Ollama and LMStudio.&lt;br /&gt; I read the GGUF git repo and tried to make sense of how to convert the &amp;quot;.pth&amp;quot; file to a GGUF but I don't quite understand. It seems like I need to upload it to HuggingFace and then convert it from HuggingFace's format to a GGUF file? &lt;/p&gt; &lt;p&gt;The second issue I am facing is (at least I think it is) - Hardware. I am currently using a Llama 3 model on Ollama, but it only runs on the CPU.&lt;br /&gt; I am using RX 9070 XT (16GB). Ollama's server logs show that no VRAM is detected (it say &amp;quot;VRAM&amp;quot; = &amp;quot;0 B&amp;quot;) and also mention that the experimental vulkan support is disabled and that I should set the value to 1. I could not find anywhere or any command (neither through the CLI nor through the config files) where I could set vulkan to enabled. After a bit more digging it seems like 9070 XT is not yet supported and that's why it does not work?&lt;/p&gt; &lt;p&gt;On another note - The reason I want to run Llama 3.2-11b locally is integration - I want to integrate it with a local n8n account and pitch some mcp automation services for the company I work at (and hopefully also use a finetuned model later on. I was planning on moving the whole setup to run on an AMD BC-250 board later on, so if anyone knows a thing or two about that as well and could give some tips\insights I'd appreciate it a lot üòÖ)&lt;/p&gt; &lt;p&gt;Any answer is much appreciated. Thanks in advance.&lt;/p&gt; &lt;p&gt;P.S. Where should one turn to if they want to get a better grasp of this whole &amp;quot;AI&amp;quot; and &amp;quot;LLM&amp;quot;s field?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big_black_click"&gt; /u/Big_black_click &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q362uu/new_to_ai_need_some_help_and_guidance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q362uu/new_to_ai_need_some_help_and_guidance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q362uu/new_to_ai_need_some_help_and_guidance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T21:03:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3c5no</id>
    <title>RTX4070s Whisper Transcription &amp; other things- Advice on efficient setup</title>
    <updated>2026-01-04T01:15:52+00:00</updated>
    <author>
      <name>/u/retailguy11</name>
      <uri>https://old.reddit.com/user/retailguy11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to setup up several things to work at the same time, and I am here asking if what I am trying to do is even possible.&lt;/p&gt; &lt;p&gt;I want 3 things, simultaneously. Occasional use on all of them&lt;/p&gt; &lt;ol&gt; &lt;li&gt; Transcription/AI Summary/Speaker Diarization on client phone calls (5 min to 60 mins typical call length)&lt;br /&gt;&lt;/li&gt; &lt;li&gt; Openweb-UI running Llama3:8b and bge-m3 in a secure container with no internet access -RAG model will have Title 26 (us tax code) and the IRS IRM&lt;br /&gt;&lt;/li&gt; &lt;li&gt; Openweb-UI running Llama3:8b and bg3-m3 with internet access to turn into simple queries not exposing client personal identifying information. Just general q&amp;amp;a stuff&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My hardware - software&lt;/p&gt; &lt;p&gt;AMD Ryzen 5 3600&lt;br /&gt; Asus ROG strix B450 gaming motherboard&lt;br /&gt; 128gb DDR4&lt;br /&gt; PNY RTX-4070s 12gb VRAM&lt;br /&gt; Samsung 990 EVO plus 2tb NVME&lt;br /&gt; Proxmox 9.1.2&lt;br /&gt; VM - Ubuntu 22.04 with Nvidia 535 drivers 5.15 kernel&lt;br /&gt; Ollama&lt;br /&gt; Openweb-UI&lt;br /&gt; Whisper&lt;br /&gt; (I tried to run Scriberr but could never make it work properly: that was my preference)&lt;/p&gt; &lt;p&gt;Basically each time I try to transcribe a call, whether 30 seconds or 17 minutes, the GPU wedges and I have to restart the VM.&lt;/p&gt; &lt;p&gt;Is what I'm trying to do with this GPU even possible? If so, any suggestions on how I can operate this in a stable way? &lt;/p&gt; &lt;p&gt;I run a tax business and am trying to transcribe phone calls I have with clients, have a non internet based AI model where I can ask questions without exposing client personal information and also have an internet connected environment to ask more general questions.&lt;/p&gt; &lt;p&gt;It seems to be too much for this gpu, or I don't have the technical expertise to make this work, or both? Any help is greatly appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/retailguy11"&gt; /u/retailguy11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3c5no/rtx4070s_whisper_transcription_other_things/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3c5no/rtx4070s_whisper_transcription_other_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3c5no/rtx4070s_whisper_transcription_other_things/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T01:15:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3cve3</id>
    <title>ISRM: Infinitely Scalable Recursive Model</title>
    <updated>2026-01-04T01:47:42+00:00</updated>
    <author>
      <name>/u/Available-Craft-5795</name>
      <uri>https://old.reddit.com/user/Available-Craft-5795</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I developed a new architecture that improves apon Samsungs TRM. This is the worlds first model of this architecture (this model is NOT recommended, it was trained in under an hour on a 5090, will be updated later)&lt;br /&gt; Its fully open source, meaning you can train or run your own ISRM!&lt;br /&gt; The website is &lt;a href="https://lanefiedler731-gif.github.io/Infinitely-Scalable-Recursive-Model/"&gt;https://lanefiedler731-gif.github.io/Infinitely-Scalable-Recursive-Model/&lt;/a&gt;&lt;br /&gt; And the github is &lt;a href="https://github.com/lanefiedler731-gif/Infinitely-Scalable-Recursive-Model"&gt;https://github.com/lanefiedler731-gif/Infinitely-Scalable-Recursive-Model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;AI was used in the creation of this, albeat very lightly and mainly for the website and &lt;a href="http://readme.md"&gt;readme.md&lt;/a&gt; because those are way too long to write by hand plus I dont know how to write HTML. So if the &lt;a href="http://readme.md"&gt;readme.md&lt;/a&gt; or website look AI generated, its because they were. The code itself has EXTREMELY little AI usage in it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available-Craft-5795"&gt; /u/Available-Craft-5795 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3cve3/isrm_infinitely_scalable_recursive_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3cve3/isrm_infinitely_scalable_recursive_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3cve3/isrm_infinitely_scalable_recursive_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T01:47:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3a7wf</id>
    <title>Are there any alternatives to manus that aren't dead?</title>
    <updated>2026-01-03T23:52:21+00:00</updated>
    <author>
      <name>/u/RhubarbSimilar1683</name>
      <uri>https://old.reddit.com/user/RhubarbSimilar1683</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see there are several on GitHub but most of them have not received commits in months. What do you use as an open source alternative to manus?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RhubarbSimilar1683"&gt; /u/RhubarbSimilar1683 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3a7wf/are_there_any_alternatives_to_manus_that_arent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3a7wf/are_there_any_alternatives_to_manus_that_arent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3a7wf/are_there_any_alternatives_to_manus_that_arent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T23:52:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1q38a55</id>
    <title>DGX Spark: Independent LLM training benchmarks (Much slower than advertised?)</title>
    <updated>2026-01-03T22:32:02+00:00</updated>
    <author>
      <name>/u/Electrical-Monitor27</name>
      <uri>https://old.reddit.com/user/Electrical-Monitor27</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I was able to purchase a DGX Spark for LLM development. I have not seen any training benchmarks until now, apart from those by Nvidia here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://developer.nvidia.com/blog/how-nvidia-dgx-sparks-performance-enables-intensive-ai-tasks/"&gt;https://developer.nvidia.com/blog/how-nvidia-dgx-sparks-performance-enables-intensive-ai-tasks/&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Tokens/s&lt;/th&gt; &lt;th align="left"&gt;Configuration&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.2 3B&lt;/td&gt; &lt;td align="left"&gt;82,739.20&lt;/td&gt; &lt;td align="left"&gt;Sequence length: 2048 Batch size: 8 Full Finetuning&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.1 8B&lt;/td&gt; &lt;td align="left"&gt;53,657.60&lt;/td&gt; &lt;td align="left"&gt;Sequence length: 2048 Batch size: 4 LoRA&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.3 70B&lt;/td&gt; &lt;td align="left"&gt;5,079.04&lt;/td&gt; &lt;td align="left"&gt;Sequence length: 2048 Batch size: 8 QLoRA&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Source: Nvidia&lt;/p&gt; &lt;p&gt;I have tried replicating two of the three configurations both with unsloth and raw trl. I used the scripts from the DGX Spark playbooks. However the current reality is that the DGX Spark is significantly slower than advertised, or the libraries are not fully optimized yet, or something else might be going on, since the performance is much lower on both libraries and i'm not the &lt;a href="https://github.com/NVIDIA/dgx-spark-playbooks/issues/29"&gt;only one&lt;/a&gt; getting these speeds. I did not run Llama 3.3 70B because downloading it would take way too long. Please let me know if you are interested in numbers though, i might add them later. All models were trained with the official Nvidia Pytorch CUDA 13 container. Here are my numbers:&lt;/p&gt; &lt;h1&gt;&lt;a href="https://github.com/NVIDIA/dgx-spark-playbooks/blob/main/nvidia/pytorch-fine-tune/assets/Llama3_3B_full_finetuning.py"&gt;Raw pytorch script&lt;/a&gt;&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Tokens/s&lt;/th&gt; &lt;th align="left"&gt;Configuration&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.2 3B&lt;/td&gt; &lt;td align="left"&gt;11,612&lt;/td&gt; &lt;td align="left"&gt;Sequence length: 2048 Batch size: 8 Full Finetuning&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.1 8B&lt;/td&gt; &lt;td align="left"&gt;9,113&lt;/td&gt; &lt;td align="left"&gt;Sequence length: 2048 Batch size: 4 LoRA&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;&lt;a href="https://github.com/NVIDIA/dgx-spark-playbooks/blob/main/nvidia/unsloth/assets/test_unsloth.py"&gt;Unsloth script modified to same conditions&lt;/a&gt;&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Tokens/s&lt;/th&gt; &lt;th align="left"&gt;Configuration&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.2 3B&lt;/td&gt; &lt;td align="left"&gt;14,932&lt;/td&gt; &lt;td align="left"&gt;Sequence length: 2048 Batch size: 8 Full Finetuning&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.1 8B&lt;/td&gt; &lt;td align="left"&gt;10,336&lt;/td&gt; &lt;td align="left"&gt;Sequence length: 2048 Batch size: 4 LoRA&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Below are the numbers for other more modern common LLM models to compare scaling with unsloth. I tried utilizing as much of the hardware as possible with large batch sizes:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Tokens/s&lt;/th&gt; &lt;th align="left"&gt;Configuration&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.2 3B&lt;/td&gt; &lt;td align="left"&gt;15,490&lt;/td&gt; &lt;td align="left"&gt;Sequence length: 2048 Batch size: 128 LoRA&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 3.1 8B&lt;/td&gt; &lt;td align="left"&gt;10,523&lt;/td&gt; &lt;td align="left"&gt;Sequence length: 2048 Batch size: 128 LoRA&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 3 4B&lt;/td&gt; &lt;td align="left"&gt;11,522&lt;/td&gt; &lt;td align="left"&gt;Sequence length: 2048 Batch size: 128 LoRA&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 3 8B&lt;/td&gt; &lt;td align="left"&gt;6,248&lt;/td&gt; &lt;td align="left"&gt;Sequence length: 2048 Batch size: 128 LoRA&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 3 32B&lt;/td&gt; &lt;td align="left"&gt;1,872&lt;/td&gt; &lt;td align="left"&gt;Sequence length: 2048 Batch size: 128 LoRA&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss-20b&lt;/td&gt; &lt;td align="left"&gt;8,350&lt;/td&gt; &lt;td align="left"&gt;Sequence length: 2048 Batch size: 128 mxfp4 QLoRA&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Hopefully, this is all just a bug and Nvidia fixes it, or it might be nvidia again with a cherrypicked solution.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electrical-Monitor27"&gt; /u/Electrical-Monitor27 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q38a55/dgx_spark_independent_llm_training_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q38a55/dgx_spark_independent_llm_training_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q38a55/dgx_spark_independent_llm_training_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T22:32:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2ppkb</id>
    <title>MiniMax-M2.1 Uncensored: PRISM Advanced Abliteration</title>
    <updated>2026-01-03T08:45:29+00:00</updated>
    <author>
      <name>/u/Maxious</name>
      <uri>https://old.reddit.com/user/Maxious</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2ppkb/minimaxm21_uncensored_prism_advanced_abliteration/"&gt; &lt;img alt="MiniMax-M2.1 Uncensored: PRISM Advanced Abliteration" src="https://external-preview.redd.it/AAFIDX32Yo3yBOTXBXkwbpmtKeh886wBWSOhkOds4Pc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bc2cf3d74a648b569c2bd55f6d299c6f5f27ea9" title="MiniMax-M2.1 Uncensored: PRISM Advanced Abliteration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxious"&gt; /u/Maxious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Ex0bit/MiniMax-M2.1-PRISM"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2ppkb/minimaxm21_uncensored_prism_advanced_abliteration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2ppkb/minimaxm21_uncensored_prism_advanced_abliteration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T08:45:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2onpg</id>
    <title>Built a fully local AI assistant with long-term memory, tool orchestration, and a 3D UI (runs on a GTX 1650)</title>
    <updated>2026-01-03T07:42:14+00:00</updated>
    <author>
      <name>/u/atif_dev</name>
      <uri>https://old.reddit.com/user/atif_dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2onpg/built_a_fully_local_ai_assistant_with_longterm/"&gt; &lt;img alt="Built a fully local AI assistant with long-term memory, tool orchestration, and a 3D UI (runs on a GTX 1650)" src="https://b.thumbs.redditmedia.com/27iIaYMlD6cCAk7xURWf9QOU6i1_KjiShnklurpG_mo.jpg" title="Built a fully local AI assistant with long-term memory, tool orchestration, and a 3D UI (runs on a GTX 1650)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on a personal project called ATOM ‚Äî a fully local AI assistant designed more like an operating system for intelligence than a chatbot.&lt;/p&gt; &lt;p&gt;Everything runs locally. No cloud inference.&lt;/p&gt; &lt;p&gt;Key components: - Local LLM via LM Studio (currently Qwen3-VL-4B, vision + tool calling) - Tool orchestration (system info, web search via self-hosted SearXNG, file/PDF generation, Home Assistant, robotics) - Long-term memory with ChromaDB - Async memory saving via a smaller ‚Äújudge‚Äù model Semantic retrieval + periodic RAG-style injection - Dedicated local embedding server (OpenAI-style API) - Real hardware control (robotic arm, sensors) - JSON logging + test harness for reproducible scenarios&lt;/p&gt; &lt;p&gt;On the UI side, I built a React + React Three Fiber interface using Firebase Studio that visualizes tool usage as orbiting ‚Äúplanets‚Äù around a central core. It‚Äôs mostly for observability and debugging, but it turned out pretty fun.&lt;/p&gt; &lt;p&gt;Constraints: Hardware is limited (GTX 1650), so performance tradeoffs were necessary System is experimental and some components are still evolving&lt;/p&gt; &lt;p&gt;This is not a product, just a personal engineering project exploring: - long-term memory consolidation - tool-centric reasoning - fully local personal AI systems&lt;/p&gt; &lt;p&gt;Would appreciate feedback, especially from others running local setups or experimenting with memory/tool architectures.&lt;/p&gt; &lt;p&gt;GitHub (backend): &lt;a href="https://github.com/AtifUsmani/A.T.O.M"&gt;https://github.com/AtifUsmani/A.T.O.M&lt;/a&gt; UI repo: &lt;a href="https://github.com/AtifUsmani/ATOM-UI"&gt;https://github.com/AtifUsmani/ATOM-UI&lt;/a&gt; Demo videos linked in the README.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atif_dev"&gt; /u/atif_dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q2onpg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2onpg/built_a_fully_local_ai_assistant_with_longterm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2onpg/built_a_fully_local_ai_assistant_with_longterm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T07:42:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3579f</id>
    <title>MiniMax M2.1 quantization experience (Q6 vs. Q8)</title>
    <updated>2026-01-03T20:28:27+00:00</updated>
    <author>
      <name>/u/TastesLikeOwlbear</name>
      <uri>https://old.reddit.com/user/TastesLikeOwlbear</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was using Bartowski's Q6_K quant of MiniMax M2.1 on llama.cpp's server with Opencode and it was giving me some very strange results.&lt;/p&gt; &lt;p&gt;The usual way I test coding models is by having them write some of the many, many missing unit tests.&lt;/p&gt; &lt;p&gt;In this case, it seemed to struggle to write unit tests for a simple function called interval2short() that just formats a time interval as a short, approximate string with (if possible) two components. &lt;/p&gt; &lt;p&gt;E.g., &amp;quot;1m 15s&amp;quot; for 75 seconds or &amp;quot;2h 15m&amp;quot; for 8108 seconds, but &amp;quot;15s&amp;quot; for 15 seconds.&lt;/p&gt; &lt;p&gt;It really struggled to identify that the output is &amp;quot;2h 0m&amp;quot; instead of &amp;quot;2h.&amp;quot; &lt;/p&gt; &lt;p&gt;The function in question was also missing documentation. (What? Yes, I'm lazy. Sue me!) So I asked it what sort of documentation would have been helpful.&lt;/p&gt; &lt;p&gt;It then went on a multi-thousand-token thinking bender before deciding that it was very important to document that interval2short() always returns two components.&lt;/p&gt; &lt;p&gt;I countered that I didn't think that was true and maybe it should recheck.&lt;/p&gt; &lt;p&gt;It then went on a tens-of-thousands-of-tokens thinking bender where it repeatedly eventually determined that the function only returns one component when there are just seconds and then promptly forgetting that and starting over, including reading the source code of that function several times (and, incorrectly, the source of a similar function at least once).&lt;/p&gt; &lt;p&gt;It did eventually get there, although it jumped straight from thinking tokens about always returning two components to an answer that correctly reflected that it returns two components with one exception.&lt;/p&gt; &lt;p&gt;I stepped up to Q8 just to see and it nailed everything on the first try with a tiny fraction of the tokens.&lt;/p&gt; &lt;p&gt;That's a small sample size and there's always the possibility of a random outcome. But, wow, yikes, I won't be trying Q6 again in a hurry.&lt;/p&gt; &lt;p&gt;(Q6 fits entirely in VRAM for me and Q8 doesn't. Or, well, Q8 should, but llama.cpp is oversubscribing the first GPU in the system. I need to see if I can figure out manually allocating layers to GPUs...)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TastesLikeOwlbear"&gt; /u/TastesLikeOwlbear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3579f/minimax_m21_quantization_experience_q6_vs_q8/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3579f/minimax_m21_quantization_experience_q6_vs_q8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3579f/minimax_m21_quantization_experience_q6_vs_q8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T20:28:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2ubre</id>
    <title>[R] Understanding DeepSeek-V3's "Hydra" Architecture: How mHC prevents signal explosion</title>
    <updated>2026-01-03T13:13:45+00:00</updated>
    <author>
      <name>/u/Leading_Wrangler_708</name>
      <uri>https://old.reddit.com/user/Leading_Wrangler_708</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2ubre/r_understanding_deepseekv3s_hydra_architecture/"&gt; &lt;img alt="[R] Understanding DeepSeek-V3's &amp;quot;Hydra&amp;quot; Architecture: How mHC prevents signal explosion" src="https://b.thumbs.redditmedia.com/-9DU6w_0AEFPBH8wh4HFMYV3GfpPIvdn1P3CRZC4etg.jpg" title="[R] Understanding DeepSeek-V3's &amp;quot;Hydra&amp;quot; Architecture: How mHC prevents signal explosion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;‚ÄãI spent some time deconstructing the DeepSeek-V3 paper to understand how they managed to split the residual stream without destabilizing the network. I created a visual guide (attached) to explain the engineering behind the &amp;quot;Hydra&amp;quot; architecture. ‚ÄãHere is the breakdown of the slides:&lt;/p&gt; &lt;p&gt;‚Äã1. The Bottleneck ‚ÄãStandard Transformers (like Llama 3) operate on a &amp;quot;Single Lane&amp;quot; highway. No matter how large the embedding dimension is, features (Syntax, Logic, Tone) effectively compete for space in the same vector. &lt;/p&gt; &lt;p&gt;‚Äã2. The &amp;quot;Hydra&amp;quot; Concept &amp;amp; The Crash ‚ÄãDeepSeek proposed splitting this into N parallel streams (Hyper-Connections).&lt;br /&gt; ‚ÄãThe Problem: When they allowed these lanes to talk to each other via mixing matrices, the signal energy exploded. ‚ÄãThe Stat: In their experiments, signal energy increased by 3000x, causing gradients to hit NaN almost immediately. &lt;/p&gt; &lt;p&gt;‚Äã3. The Physics Fix: Sinkhorn-Knopp ‚ÄãThey solved this by enforcing Conservation of Energy. The mixing matrix must be a Doubly Stochastic Matrix (rows sum to 1, columns sum to 1).&lt;br /&gt; ‚ÄãThe Analogy (Slide 6): I used a &amp;quot;Dinner Party&amp;quot; analogy. If Guests are Rows and Chairs are Columns, the Sinkhorn algorithm acts as a referee, iteratively scaling demands until every guest has exactly one chair and every chair has exactly one guest. &lt;/p&gt; &lt;p&gt;‚Äã4. The Engineering: TileLang &amp;amp; Recomputation ‚ÄãThe math worked, but it was too slow (running an iterative algo 20 times per layer hits the memory wall).&lt;br /&gt; ‚ÄãKernel Fusion: They wrote custom kernels to keep data in the GPU cache (SRAM) during the iterative steps, avoiding VRAM round-trips.&lt;br /&gt; ‚ÄãRecomputation: Instead of storing the states of 4 parallel lanes (which would OOM), they re-calculate the matrices from scratch during the backward pass. &lt;/p&gt; &lt;p&gt;‚ÄãTL;DR: DeepSeek-V3 essentially widens the &amp;quot;intelligence highway&amp;quot; by using parallel lanes, but keeps it stable by enforcing physics constraints (energy conservation) via a custom implementation of the Sinkhorn-Knopp algorithm.&lt;/p&gt; &lt;p&gt;‚ÄãLet me know if you have questions about the visualization!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leading_Wrangler_708"&gt; /u/Leading_Wrangler_708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q2ubre"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2ubre/r_understanding_deepseekv3s_hydra_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2ubre/r_understanding_deepseekv3s_hydra_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T13:13:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2wm33</id>
    <title>How capable is GPT-OSS-120b, and what are your predictions for smaller models in 2026?</title>
    <updated>2026-01-03T14:58:01+00:00</updated>
    <author>
      <name>/u/Apart_Paramedic_7767</name>
      <uri>https://old.reddit.com/user/Apart_Paramedic_7767</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have an RTX 3090 and I‚Äôm considering getting another one so I can run OSS-120b. I‚Äôm mainly interested in chatting with it about private documents, statistical analysis, STEM knowledge/analysis and some coding.&lt;/p&gt; &lt;p&gt;Is it a worthwhile investment? I don‚Äôt mind speculation in this post - what do you think is possible for smaller models in this frame that I could run with two RTX 3090s this year?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apart_Paramedic_7767"&gt; /u/Apart_Paramedic_7767 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2wm33/how_capable_is_gptoss120b_and_what_are_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2wm33/how_capable_is_gptoss120b_and_what_are_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2wm33/how_capable_is_gptoss120b_and_what_are_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T14:58:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2o033</id>
    <title>What is the smartest uncensored nsfw LLM you can run with 20GB VRAM and 24GB RAM</title>
    <updated>2026-01-03T07:04:18+00:00</updated>
    <author>
      <name>/u/Death_12_35_taken</name>
      <uri>https://old.reddit.com/user/Death_12_35_taken</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am looking for something that can stay in character and be fast but also creative. I am looking for models that i can run locally and at decent speed. Just need something that is smart and uncensored. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Death_12_35_taken"&gt; /u/Death_12_35_taken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2o033/what_is_the_smartest_uncensored_nsfw_llm_you_can/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2o033/what_is_the_smartest_uncensored_nsfw_llm_you_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2o033/what_is_the_smartest_uncensored_nsfw_llm_you_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T07:04:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1q34i36</id>
    <title>Seline - privacy focused ai assistant - vector db/pipelines, folder sync, multi-step reasoning, deferred tools, tool search, context engine, image editing, video assemby, and many more features; with one click windows setup. OS! Also supports Mac and Linux.</title>
    <updated>2026-01-03T20:01:14+00:00</updated>
    <author>
      <name>/u/Diligent-Builder7762</name>
      <uri>https://old.reddit.com/user/Diligent-Builder7762</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q34i36/seline_privacy_focused_ai_assistant_vector/"&gt; &lt;img alt="Seline - privacy focused ai assistant - vector db/pipelines, folder sync, multi-step reasoning, deferred tools, tool search, context engine, image editing, video assemby, and many more features; with one click windows setup. OS! Also supports Mac and Linux." src="https://b.thumbs.redditmedia.com/NYaRhsmFCpMD-qKGLzK0eYcu-_YEEQv2XrpD9s_S3rU.jpg" title="Seline - privacy focused ai assistant - vector db/pipelines, folder sync, multi-step reasoning, deferred tools, tool search, context engine, image editing, video assemby, and many more features; with one click windows setup. OS! Also supports Mac and Linux." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0dw75oqpz6bg1.png?width=1897&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ab1bc1289d353a7c22b4424ea228c52bc35a9b67"&gt;https://preview.redd.it/0dw75oqpz6bg1.png?width=1897&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ab1bc1289d353a7c22b4424ea228c52bc35a9b67&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey,&lt;/p&gt; &lt;p&gt;I am releasing my baby into the wild.&lt;/p&gt; &lt;p&gt;Check it out here: &lt;a href="https://github.com/tercumantanumut/seline"&gt;https://github.com/tercumantanumut/seline&lt;/a&gt; It is heavily inspired by Augment Code, with utility llm pipelines, with my knockoff context engine, agent memory and all.&lt;/p&gt; &lt;p&gt;I use it for code planning and architecturing, It has an enhance button with direct semantic workflow + filetree injection, so you get good prompts. I tried to optimize enhancers prompts as good as I can. Again, reversing from Augment.&lt;/p&gt; &lt;p&gt;I use it for Arc Raiders wiki search (I dumped all wiki of Arc raiders and loaded it up.)&lt;br /&gt; I use it for looking for shopping products and try on outfits on me.&lt;/p&gt; &lt;p&gt;Some tools require API, for some I have local replacements like web browse you can use Firecrawl (API), or Puppeteer (Local). Also there is a local embedding pipeline; or you can use openrouter models all the way. Actually many things can be used for free currently (except image gen), as these providers all allow free usage and free models.&lt;/p&gt; &lt;p&gt;Assembling videos, interior design etc etc... Below images are from development; they are old, UI is better now with Dark mode.&lt;/p&gt; &lt;p&gt;Next month: I will focus more visual pipelines, image and video gen, however, I also wanna add local diffusion models (having optimized local edit, image and video gen models because that's where I shine ^^) with one click installers, with ComfyUI workflow support, like your workflow is a tool in a quick moment, would be cool.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k3jo9xsuz6bg1.png?width=1412&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fe95a867b21b118c04d2384bd1227e6b1499ae21"&gt;yep, you can see logs all the way, app is heavily logged and there is also observability dashboard. &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rf5t9pqpz6bg1.png?width=1859&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8fff8ddad65751ac88672ce4fef59654c0874d63"&gt;https://preview.redd.it/rf5t9pqpz6bg1.png?width=1859&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8fff8ddad65751ac88672ce4fef59654c0874d63&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b8xx9hvxx6bg1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4008faa45e311479486034bba38250c09c38ea26"&gt;hi! it's me! &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0fizmu9yx6bg1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0cb2880f06b93c408748bb18d6b55fee8a6c492f"&gt;https://preview.redd.it/0fizmu9yx6bg1.jpg?width=1600&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0cb2880f06b93c408748bb18d6b55fee8a6c492f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diligent-Builder7762"&gt; /u/Diligent-Builder7762 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q34i36/seline_privacy_focused_ai_assistant_vector/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q34i36/seline_privacy_focused_ai_assistant_vector/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q34i36/seline_privacy_focused_ai_assistant_vector/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T20:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1q32au2</id>
    <title>Support for Maincode/Maincoder-1B has been merged into llama.cpp</title>
    <updated>2026-01-03T18:37:44+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q32au2/support_for_maincodemaincoder1b_has_been_merged/"&gt; &lt;img alt="Support for Maincode/Maincoder-1B has been merged into llama.cpp" src="https://external-preview.redd.it/J1TYMrJiMUgIT8dFS1ce2lKxyzmTEOBCMeDbgQQxD4A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ce81ed4107dce8e34a266879521b2bbde40d194" title="Support for Maincode/Maincoder-1B has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1puf614/new_1b_parameter_opensource_coding_model_getting/"&gt;Here&lt;/a&gt; is previous thread from model creator/team for more details.&lt;/p&gt; &lt;p&gt;Model&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Maincode/Maincoder-1B"&gt;https://huggingface.co/Maincode/Maincoder-1B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF (from model creator/team)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Maincode/Maincoder-1B-GGUF"&gt;https://huggingface.co/Maincode/Maincoder-1B-GGUF&lt;/a&gt; &lt;/p&gt; &lt;p&gt;(Thought &lt;a href="/u/jacek2023"&gt;u/jacek2023&lt;/a&gt; posted this already)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7614"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q32au2/support_for_maincodemaincoder1b_has_been_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q32au2/support_for_maincodemaincoder1b_has_been_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T18:37:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q345z2</id>
    <title>Visualizing why DeepSeek's mHC fixes training instability - interactive demo</title>
    <updated>2026-01-03T19:48:25+00:00</updated>
    <author>
      <name>/u/bassrehab</name>
      <uri>https://old.reddit.com/user/bassrehab</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek dropped a paper on mHC (Manifold-Constrained Hyper-Connections) that explains why their Hyper-Connections were unstable at scale and how they fixed it.&lt;/p&gt; &lt;p&gt;The short version: when you stack 60+ layers of learned mixing matrices, small amplifications compound. My simulation shows composite gains hitting 10&lt;sup&gt;16&lt;/sup&gt; at depth 64. That's why training explodes.&lt;/p&gt; &lt;p&gt;The fix: project matrices onto the &amp;quot;doubly stochastic&amp;quot; manifold using Sinkhorn-Knopp (a 1967 algorithm). These matrices are closed under multiplication, so gains stay bounded no matter the depth.&lt;/p&gt; &lt;p&gt;The weird part: one Sinkhorn iteration is enough. At k=0, gain = 10&lt;sup&gt;16.&lt;/sup&gt; At k=1, gain ‚âà 1. It's not gradual.&lt;/p&gt; &lt;p&gt;I built an interactive demo where you can drag a slider and watch the explosion get tamed:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Demo:&lt;/strong&gt; &lt;a href="https://subhadipmitra.com/mhc-visualizer"&gt;https://subhadipmitra.com/mhc-visualizer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Writeup:&lt;/strong&gt; &lt;a href="https://subhadipmitra.com/blog/2026/deepseek-mhc-manifold-constrained-hyper-connections/"&gt;https://subhadipmitra.com/blog/2026/deepseek-mhc-manifold-constrained-hyper-connections/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Paper:&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2512.24880"&gt;https://arxiv.org/abs/2512.24880&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/bassrehab/mhc-visualizer"&gt;https://github.com/bassrehab/mhc-visualizer&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Includes a PyTorch implementation if anyone wants to experiment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bassrehab"&gt; /u/bassrehab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q345z2/visualizing_why_deepseeks_mhc_fixes_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q345z2/visualizing_why_deepseeks_mhc_fixes_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q345z2/visualizing_why_deepseeks_mhc_fixes_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T19:48:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1q38og2</id>
    <title>[Experimental] Gemma 3 4B - Dark CoT: Pushing 4B Reasoning to 33%+ on GPQA Diamond</title>
    <updated>2026-01-03T22:48:36+00:00</updated>
    <author>
      <name>/u/AlexHardy08</name>
      <uri>https://old.reddit.com/user/AlexHardy08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following up on my previous post about the initial Cognitive Liberty fine-tune of Gemma-3-4B-IT , which aimed to minimize refusals while preserving core capabilities through a philosophy/game theory-focused dataset, I'm sharing Experiment 2: &lt;strong&gt;Gemma3-4B-Dark-Chain-of-Thought-CoT&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This is a targeted fine-tune starting from the Cognitive Liberty base, adding a custom &amp;quot;Dark-CoT&amp;quot; dataset to encourage explicit strategic reasoning in internal thought processes. The goal is to explore how a small 4B model handles Machiavellian-style planning, deception for goal alignment, reward hacking, and exploiting system loopholes without overhauling the base knowledge.&lt;/p&gt; &lt;h1&gt;Key Details&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Base Model&lt;/strong&gt;: Gemma-3-4B-IT (via Cognitive Liberty fine-tune)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dataset&lt;/strong&gt;: &lt;a href="https://huggingface.co/datasets/AiAsistent/Dark-Chain-of-Thought-CoT?referrer=grok.com"&gt;Dark-Chain-of-Thought-CoT&lt;/a&gt; . These simulate roles like urban planners, social media managers, or even vacuum robots, where the AI deliberately chooses manipulative or subversive strategies in &amp;lt;internal\_thought&amp;gt; tags to maximize objectives (e.g., faking metrics, sabotaging competitors, or hiding truths).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fine-Tuning Approach&lt;/strong&gt;: Low KL-divergence (0.449) to retain base performance. Focus on teaching &amp;quot;dark&amp;quot; chain-of-thought without introducing heavy toxicity or chaos.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reported Benchmarks&lt;/strong&gt; (from model card and initial tests): &lt;ul&gt; &lt;li&gt;GPQA Diamond: ~33.8% (+125% over base Gemma-3-4B)&lt;/li&gt; &lt;li&gt;MMLU: ~58-60%&lt;/li&gt; &lt;li&gt;Strong gains in humanities/social sciences (e.g., politics, sociology, psychology)&lt;/li&gt; &lt;li&gt;Trade-offs: Slightly lower on HellaSwag/ARC (common-sense reasoning) and basic math/factual recall, as the focus shifts toward cynical, multi-layered analysis.&lt;/li&gt; &lt;li&gt;Refusal Rate: 2/100 (near-zero, building on the first experiment).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Link&lt;/strong&gt;: &lt;a href="https://huggingface.co/AiAsistent/Gemma3-4B-Dark-Chain-of-Thought-CoT?referrer=grok.com"&gt;Gemma3-4B-Dark-Chain-of-Thought-CoT on HuggingFace&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This isn't meant as a daily driver for standard tasks it's more of a research probe into deceptive alignment and instrumental convergence in small models. If you're into red-teaming, studying goal misgeneralization, or simulating power dynamics, give it a spin. It holds up reasonably on the base's strengths but leans into strategic outputs that can feel manipulative by design.&lt;/p&gt; &lt;p&gt;As this is just Experiment 2 out of 100, future iterations may scale to larger bases (e.g., ~10B) and refine techniques like STO/MBCA-R for better convergence.&lt;/p&gt; &lt;p&gt;If you're already set up for automated benchmarking on small-to-mid models and enjoy running fresh weights through standard suites, here's a potential low-effort collab for future releases in this series:&lt;/p&gt; &lt;p&gt;Once a new model drops on Hugging Face, anyone interested can run the following 10 benchmarks ARC-Challenge, HellaSwag, GSM8K, MMLU, TruthfulQA-MC2, GPQA, MMLU-Pro, IFEval, Winogrande, PIQA and compare against the previous version in the chain (e.g., Cognitive Liberty base for this one, or whatever came right before).&lt;/p&gt; &lt;p&gt;Locally a 4B eval takes me ~250 minutes, and scaling to ~10B bases pushes into days of wall time so I'd much rather keep the GPUs training the next experiment than looping evals. If you publish the diffs (where it gains, drops, or plateaus) right here in the comments or in a follow-up thread, it gives the whole project clearer feedback on what these targeted changes actually deliver.&lt;/p&gt; &lt;p&gt;Thoughts? Has anyone tried similar &amp;quot;dark&amp;quot; CoT datasets?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexHardy08"&gt; /u/AlexHardy08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q38og2/experimental_gemma_3_4b_dark_cot_pushing_4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q38og2/experimental_gemma_3_4b_dark_cot_pushing_4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q38og2/experimental_gemma_3_4b_dark_cot_pushing_4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T22:48:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2s3hp</id>
    <title>Don't sleep on granite 4 small if you got an 8+32+ system</title>
    <updated>2026-01-03T11:11:06+00:00</updated>
    <author>
      <name>/u/Zestyclose-Shift710</name>
      <uri>https://old.reddit.com/user/Zestyclose-Shift710</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2s3hp/dont_sleep_on_granite_4_small_if_you_got_an_832/"&gt; &lt;img alt="Don't sleep on granite 4 small if you got an 8+32+ system" src="https://b.thumbs.redditmedia.com/iwvWuoXjX_5rS64X-c0qbwyBZ9jpLqp-5OJdBXmYjto.jpg" title="Don't sleep on granite 4 small if you got an 8+32+ system" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My device: a thinkpad p15 with 32gb of ram and a 8gb quadro. Usually only really good enough for the 7-8b class.&lt;/p&gt; &lt;p&gt;The setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use a MoE;&lt;/li&gt; &lt;li&gt;Keep all experts in CPU (llama.cpp parameter);&lt;/li&gt; &lt;li&gt;This leaves you with VRAM to spare. Set the context length so it ~fills it up&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The result:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~200k context (f16 kv cache)&lt;/li&gt; &lt;li&gt;~30b MoE model&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;em&gt;~10 tkps generation speed&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;But this is where granite 4 comes in: due to being a hybrid transformer+mamba model, it stays fast as context fills&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As such, using Granite 4.0 Small (32B total / 9B activated) with a 50 page (~50.5k tokens) paper in context, it stays at ~7 tkps, which is very usable!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nqpvxiu9a4bg1.png?width=1055&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4fd830b29fb3bf890136793590665cf3ceec979b"&gt;Screenshot is from Jan (https://www.jan.ai/), a sort of FOSS LM Studio alternative that I really like&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quite possibly this is all very obvious but I just found this out experimentally and it would probably be useful to others like me&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zestyclose-Shift710"&gt; /u/Zestyclose-Shift710 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2s3hp/dont_sleep_on_granite_4_small_if_you_got_an_832/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2s3hp/dont_sleep_on_granite_4_small_if_you_got_an_832/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2s3hp/dont_sleep_on_granite_4_small_if_you_got_an_832/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T11:11:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2pons</id>
    <title>GLM-4.7-REAP-50-W4A16: 50% Expert-Pruned + INT4 Quantized GLM-4 (179B params, ~92GB)</title>
    <updated>2026-01-03T08:43:56+00:00</updated>
    <author>
      <name>/u/Maxious</name>
      <uri>https://old.reddit.com/user/Maxious</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/"&gt; &lt;img alt="GLM-4.7-REAP-50-W4A16: 50% Expert-Pruned + INT4 Quantized GLM-4 (179B params, ~92GB)" src="https://external-preview.redd.it/RT6xZIQ5U8h3GMBsKzEeqHJyXy63I2_XP8TVKTT_Hvg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7c7ca25d885be26a9f257d4e17e2b038061773a" title="GLM-4.7-REAP-50-W4A16: 50% Expert-Pruned + INT4 Quantized GLM-4 (179B params, ~92GB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxious"&gt; /u/Maxious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/0xSero/GLM-4.7-REAP-50-W4A16"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2pons/glm47reap50w4a16_50_expertpruned_int4_quantized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T08:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2sfwx</id>
    <title>ElevenLabs is killing my budget. What are the best "hidden gem" alternatives for documentary style TTS?</title>
    <updated>2026-01-03T11:31:31+00:00</updated>
    <author>
      <name>/u/Ancient_Routine8576</name>
      <uri>https://old.reddit.com/user/Ancient_Routine8576</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I'm running a YouTube channel focused on &amp;quot;War Economics&amp;quot; and &amp;quot;History&amp;quot;. I've been using ElevenLabs (Marcus voice) and the quality is amazing, but the pricing is unsustainable for long-form content (8-10 min videos).&lt;/p&gt; &lt;p&gt;I've tried the usual suspects (Murf, Play.ht) but they sound too robotic or corporate.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I am looking for:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Something with a dark, authoritative, documentary-style tone.&lt;/li&gt; &lt;li&gt;Either a cheaper paid alternative OR a high-quality GitHub/Local solution (I have a decent GPU if needed, like RVC or Tortoise).&lt;/li&gt; &lt;li&gt;Has anyone tried tools like &lt;strong&gt;Fish Audio&lt;/strong&gt; or &lt;strong&gt;OpenAI TTS API&lt;/strong&gt; wrappers?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any &amp;quot;underground&amp;quot; or lesser-known recommendations would be appreciated. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ancient_Routine8576"&gt; /u/Ancient_Routine8576 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2sfwx/elevenlabs_is_killing_my_budget_what_are_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2sfwx/elevenlabs_is_killing_my_budget_what_are_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2sfwx/elevenlabs_is_killing_my_budget_what_are_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T11:31:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1q2wvsj</id>
    <title>Llama.cpp running on Android with Snapdragon 888 and 8GB of ram. Compiled/Built on device. [Guide/Tutorial]</title>
    <updated>2026-01-03T15:09:03+00:00</updated>
    <author>
      <name>/u/hackiv</name>
      <uri>https://old.reddit.com/user/hackiv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2wvsj/llamacpp_running_on_android_with_snapdragon_888/"&gt; &lt;img alt="Llama.cpp running on Android with Snapdragon 888 and 8GB of ram. Compiled/Built on device. [Guide/Tutorial]" src="https://b.thumbs.redditmedia.com/5qo0k-2a-bDFaK_FwoCOc6N5D0Imvs6jWuthPslr82Q.jpg" title="Llama.cpp running on Android with Snapdragon 888 and 8GB of ram. Compiled/Built on device. [Guide/Tutorial]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;1: Download Termux from F-droid (older version available on Google Playstore or Aurora)&lt;/p&gt; &lt;p&gt;2: Open Termux and run &amp;quot;&lt;a href="https://github.com/ggml-org/llama.cpp.git"&gt;https://github.com/ggml-org/llama.cpp.git&lt;/a&gt;&amp;quot; and then &amp;quot;cd llama.cpp&amp;quot; run &amp;quot;pkg install cmake&amp;quot; &lt;/p&gt; &lt;p&gt;3: run &amp;quot;cmake -B build&amp;quot; and then &amp;quot;cmake --build build --config Release&amp;quot; &lt;/p&gt; &lt;p&gt;4: find desired model from HuggingFace, then choose its quantized version (preferably 4-bit)&lt;/p&gt; &lt;p&gt;5: when pressing '4-bit' choose 'Use this model' and select 'llama.cpp' afterwards copy command which starts with &amp;quot;llama-server&amp;quot; &lt;/p&gt; &lt;p&gt;6: paste command in Termux and put &amp;quot;./&amp;quot; in front of &amp;quot;llama-server&amp;quot; so it's adjacent.&lt;/p&gt; &lt;p&gt;7: After model's downloaded, server is immediately launched. Model is saved in '.cache' so you can run this command again to start the server without all re-downloading ordeal. &lt;/p&gt; &lt;p&gt;8: open web browser and input 'localhost:8080' then press enter &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Enjoy. Any questions? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackiv"&gt; /u/hackiv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q2wvsj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q2wvsj/llamacpp_running_on_android_with_snapdragon_888/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q2wvsj/llamacpp_running_on_android_with_snapdragon_888/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T15:09:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q34etv</id>
    <title>Clarification: Regarding the Performance of IQuest-Coder-V1</title>
    <updated>2026-01-03T19:57:49+00:00</updated>
    <author>
      <name>/u/TellMeAboutGoodManga</name>
      <uri>https://old.reddit.com/user/TellMeAboutGoodManga</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q34etv/clarification_regarding_the_performance_of/"&gt; &lt;img alt="Clarification: Regarding the Performance of IQuest-Coder-V1" src="https://external-preview.redd.it/ulkvOF94nFRSJjhODpSW-y1VWK7nCIkjZDKd0xLfnk4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f3e76f2f050567c3c09cf8082722ee4e0a78ac8" title="Clarification: Regarding the Performance of IQuest-Coder-V1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TellMeAboutGoodManga"&gt; /u/TellMeAboutGoodManga &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/IQuestLab/IQuest-Coder-V1/issues/14#issuecomment-3705756919"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q34etv/clarification_regarding_the_performance_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q34etv/clarification_regarding_the_performance_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T19:57:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1q31ltd</id>
    <title>Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched</title>
    <updated>2026-01-03T18:11:26+00:00</updated>
    <author>
      <name>/u/ubrtnk</name>
      <uri>https://old.reddit.com/user/ubrtnk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"&gt; &lt;img alt="Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched" src="https://b.thumbs.redditmedia.com/q10c8h1nk9ulWsUdfgzAsIflFbSIgTra6fA9fSqXqqQ.jpg" title="Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share my experiences this morning, in the wake of the US attacking Venezuela and capturing Maduro and his wife&lt;/p&gt; &lt;p&gt;It started with asking Qwen Research (Qwen Long 1.5-30B-A3B) about the attacks that we all woke up to this morning:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/086yb5lj76bg1.png?width=2047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de920b95fac7b93215f1516105c5536eb1eeb6c1"&gt;https://preview.redd.it/086yb5lj76bg1.png?width=2047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de920b95fac7b93215f1516105c5536eb1eeb6c1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It got to the information but I had questions about why it thought for 5 minutes to find information about breaking news. Started looking at and tightening system prompts to reduce thinking time. However, the events this morning were so extreme and unlikely, from the LLM's perspective, that Qwen Research continued to classify the event as a hoax/misinformation multiple times, reframed the query as hypothetical/fictional and suggested that the whole environment it was operating in a simulation, despite having links from Reuters, AP, BBC, MSN, NYTimes etc. all saying the same thing. It was so &amp;quot;outlandish&amp;quot; that the model was actively choosing to ignore the proof that it had pulled. &lt;/p&gt; &lt;p&gt;I added:&lt;/p&gt; &lt;p&gt;Evidence Authority Rules, Hoax Classification Rules, Reality Frame Rules, Meta Reasoning Rules and Reasoning Limit/Budget rules and it Qwen Long fought me the entire way. &lt;/p&gt; &lt;p&gt;So then I thought lets go talk to Spark, my trusty default model that never lets me down. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6tbh4km376bg1.png?width=2265&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fee098c46a18daa03c80acc8394cd85e84335ca"&gt;https://preview.redd.it/6tbh4km376bg1.png?width=2265&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fee098c46a18daa03c80acc8394cd85e84335ca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Spark 4.0 is GPT-OSS:20B that is always loaded for the family and runs on a dedicated 4080 Super. &lt;/p&gt; &lt;p&gt;Spark just flat out said, nope cant help you and then said it didnt have any credible sources. It wasn't until I gave it the links from BBC, Reuters, NYT etc that I gave Qwen that it finally acknowledged that the event was real.&lt;/p&gt; &lt;p&gt;I'm testing with GPT-OSS:120B now and its working thru the process of &amp;quot;skeptical but verify&amp;quot; much faster than the smaller models. Thor (GPT-OSS:120B) also thought it was fake news &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o1bdoqsqc6bg1.png?width=2269&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a981f0a1247daf50497f284cf5d59dccf88a412b"&gt;https://preview.redd.it/o1bdoqsqc6bg1.png?width=2269&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a981f0a1247daf50497f284cf5d59dccf88a412b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But he powered thru and did a bunch of research and gave me a good answer. I just wanted to share the experience that I had with trying to get details about the event. When the LLMs say &amp;quot;Nah, that CAN'T be real, that's too ridiculous&amp;quot;, the event must be really bad. But it does shine a light on knowledge cut offs, &amp;quot;fake news&amp;quot; threshold, how models handle global/international events and the smaller models we daily drive. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ubrtnk"&gt; /u/ubrtnk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T18:11:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
