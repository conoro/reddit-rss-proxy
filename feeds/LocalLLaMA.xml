<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-15T06:25:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ox6xt8</id>
    <title>Observed a sharp ‚Äúepoch-wise double descent‚Äù in a small MNIST MLP , associated with overfitting the augmented training data</title>
    <updated>2025-11-14T20:03:07+00:00</updated>
    <author>
      <name>/u/calculatedcontent</name>
      <uri>https://old.reddit.com/user/calculatedcontent</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox6xt8/observed_a_sharp_epochwise_double_descent_in_a/"&gt; &lt;img alt="Observed a sharp ‚Äúepoch-wise double descent‚Äù in a small MNIST MLP , associated with overfitting the augmented training data" src="https://b.thumbs.redditmedia.com/DroEtJPkvTLJBe_Uoxc9iDpijKytJHVRTYgGuTqDAVA.jpg" title="Observed a sharp ‚Äúepoch-wise double descent‚Äù in a small MNIST MLP , associated with overfitting the augmented training data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been training a simple 3-layer MLP on MNIST using standard tricks (light affine augmentation, label smoothing, LR warmup, etc.), and I ran into an interesting pattern. The model reaches its best test accuracy fairly early, then test accuracy &lt;em&gt;declines&lt;/em&gt; for a while, even though training accuracy keeps rising.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w3qwfyqr3a1g1.png?width=989&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d3b76405efc89a4322b8ae8efe88c38efbc44b22"&gt;https://preview.redd.it/w3qwfyqr3a1g1.png?width=989&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d3b76405efc89a4322b8ae8efe88c38efbc44b22&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To understand what was happening, I looked at the weight matrices layer-by-layer and computed the HTSR / weightwatcher power law layer quality metrice (Œ±) during training. At the point of peak test accuracy, Œ± is close to 2 (which usually corresponds to well-fit layers). But as training continues, Œ± drops significantly below 2 ‚Äî right when test accuracy starts declining.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xxpehyfa4a1g1.png?width=989&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1a348445c981e584469ede9489ca6b4b582051fa"&gt;https://preview.redd.it/xxpehyfa4a1g1.png?width=989&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1a348445c981e584469ede9489ca6b4b582051fa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What makes this interesting is that the drop in Œ± lines up almost perfectly with overfitting to the &lt;strong&gt;augmented&lt;/strong&gt; training distribution. In other words, once augmentation no longer provides enough variety, the model seems to ‚Äúmemorize‚Äù these transformed samples and the spectra reflect that shift.&lt;/p&gt; &lt;p&gt;Has anyone else seen this kind of &lt;strong&gt;epoch-wise double descent&lt;/strong&gt; in small models? And especially this tight relationship overfitting on the augmented data?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/calculatedcontent"&gt; /u/calculatedcontent &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox6xt8/observed_a_sharp_epochwise_double_descent_in_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox6xt8/observed_a_sharp_epochwise_double_descent_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ox6xt8/observed_a_sharp_epochwise_double_descent_in_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T20:03:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1owulxd</id>
    <title>Kimi k2 thinking + kilo code really not bad</title>
    <updated>2025-11-14T12:00:43+00:00</updated>
    <author>
      <name>/u/Federal_Spend2412</name>
      <uri>https://old.reddit.com/user/Federal_Spend2412</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm genuinely impressed. Once your AGENTS.md and rules.md are clear enough, kimi k2 thinking + kilo code really seems to be just as capable as Claude 4.0 sonnet, especially when it comes to programming and debugging. It‚Äôs a surprisingly powerful combination.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal_Spend2412"&gt; /u/Federal_Spend2412 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owulxd/kimi_k2_thinking_kilo_code_really_not_bad/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owulxd/kimi_k2_thinking_kilo_code_really_not_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owulxd/kimi_k2_thinking_kilo_code_really_not_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T12:00:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxd3zo</id>
    <title>Slamming my head against the wall with Parakeet</title>
    <updated>2025-11-15T00:14:03+00:00</updated>
    <author>
      <name>/u/PM_ME_ABSOLUTE_UNITZ</name>
      <uri>https://old.reddit.com/user/PM_ME_ABSOLUTE_UNITZ</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ive been trying to get this thing running locally on windows and cant seem to get it to work. I got whisper ai to work in minutes through Vibe. &lt;/p&gt; &lt;p&gt;But parakeet? Nothing close to being as easy. Been trying for over 3 hrs now. Is there an easy app I can install like Vibe or Ollama?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PM_ME_ABSOLUTE_UNITZ"&gt; /u/PM_ME_ABSOLUTE_UNITZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxd3zo/slamming_my_head_against_the_wall_with_parakeet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxd3zo/slamming_my_head_against_the_wall_with_parakeet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxd3zo/slamming_my_head_against_the_wall_with_parakeet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T00:14:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox63cc</id>
    <title>New Nemo tune of creative \ adventure \ roleplay</title>
    <updated>2025-11-14T19:30:43+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I introduce &lt;strong&gt;Sweet_Dreams_12B,&lt;/strong&gt; a Nemo 12B tune with focus on more human and natural responses, with a fun vocabulary and reduced slop.&lt;/p&gt; &lt;p&gt;Here's the TL;DR:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Accepts &lt;strong&gt;wide range&lt;/strong&gt; of character cards formats.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unique&lt;/strong&gt; vocabulary.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Very diverse&lt;/strong&gt; swipes.&lt;/li&gt; &lt;li&gt;Does adventure well.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Morrowind&lt;/strong&gt; knowledge :)&lt;/li&gt; &lt;li&gt;Feels sometimes &lt;strong&gt;very human&lt;/strong&gt; in the way it responds.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamic length&lt;/strong&gt; response with a &lt;strong&gt;slight&lt;/strong&gt; bias towards &lt;strong&gt;more paragraphs&lt;/strong&gt; (2‚Äì5 paragraphs, usually 2‚Äì3). Length is adjustable via 1‚Äì3 examples in the dialogue. &lt;strong&gt;No more rigid short-bias!&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Sweet_Dreams_12B"&gt;https://huggingface.co/SicariusSicariiStuff/Sweet_Dreams_12B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox63cc/new_nemo_tune_of_creative_adventure_roleplay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox63cc/new_nemo_tune_of_creative_adventure_roleplay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ox63cc/new_nemo_tune_of_creative_adventure_roleplay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T19:30:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1owxob9</id>
    <title>Why aren't there cheap NVLink adapters for RTX 3090s?</title>
    <updated>2025-11-14T14:17:43+00:00</updated>
    <author>
      <name>/u/alex_bit_</name>
      <uri>https://old.reddit.com/user/alex_bit_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is the NVLink only a wire jumper linking both cards together?&lt;/p&gt; &lt;p&gt;Can I make my own homemade connections?&lt;/p&gt; &lt;p&gt;Or are there some chips or other things inside the bridge?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alex_bit_"&gt; /u/alex_bit_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owxob9/why_arent_there_cheap_nvlink_adapters_for_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owxob9/why_arent_there_cheap_nvlink_adapters_for_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owxob9/why_arent_there_cheap_nvlink_adapters_for_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T14:17:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ows6z3</id>
    <title>Kimi k2 thinking vs Claude Sonnet</title>
    <updated>2025-11-14T09:41:28+00:00</updated>
    <author>
      <name>/u/sebastianmicu24</name>
      <uri>https://old.reddit.com/user/sebastianmicu24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I will add my personal experience with kimi k2 thinking for my usecase since I saw contrasting opinions. &lt;/p&gt; &lt;p&gt;I needed to cluster some cells from a csv file to see if it would be achievable with my data to do some unsupervised classification of tumor cell/healthy cell. &lt;/p&gt; &lt;p&gt;I tried with claude sonnet 4 and after 2$ in api calls and a bunch of prompts i got no result, it was clustering 99.9% of cells into one group and 0.1% into the other. It was also having difficulties into rendering the cells from the x y positions in the csv. &lt;/p&gt; &lt;p&gt;Kimi k2 thinking achieved a proper clustering in 2 prompts (one for preprocessing of csv data, and one for clustering, maybe it could have done the same in 1 prompt). Total cost 0.17$&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sebastianmicu24"&gt; /u/sebastianmicu24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ows6z3/kimi_k2_thinking_vs_claude_sonnet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ows6z3/kimi_k2_thinking_vs_claude_sonnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ows6z3/kimi_k2_thinking_vs_claude_sonnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T09:41:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1owmtkt</id>
    <title>I brought CUDA back to macOS. Not because it was useful ‚Äî because nobody else could.</title>
    <updated>2025-11-14T04:20:16+00:00</updated>
    <author>
      <name>/u/Adept_Tip8375</name>
      <uri>https://old.reddit.com/user/Adept_Tip8375</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owmtkt/i_brought_cuda_back_to_macos_not_because_it_was/"&gt; &lt;img alt="I brought CUDA back to macOS. Not because it was useful ‚Äî because nobody else could." src="https://external-preview.redd.it/YHp6xAwqBe8oZ_OrMdwTyJjRYCv9-wbk4V-lSqlUI3I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e28774fdaaba15ba19d667058a3967b4695ebc8" title="I brought CUDA back to macOS. Not because it was useful ‚Äî because nobody else could." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just resurrected CUDA on High Sierra in 2025&lt;br /&gt; Apple killed it 2018, NVIDIA killed drivers 2021&lt;br /&gt; now my 1080 Ti is doing 11 TFLOPs under PyTorch again&lt;br /&gt; ‚Äúimpossible‚Äù they said&lt;br /&gt; &lt;a href="https://github.com/careunix/PyTorch-HighSierra-CUDA-Revival"&gt;https://github.com/careunix/PyTorch-HighSierra-CUDA-Revival&lt;/a&gt;&lt;br /&gt; who still runs 10.13 in 2025 üòÇ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adept_Tip8375"&gt; /u/Adept_Tip8375 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owmtkt/i_brought_cuda_back_to_macos_not_because_it_was/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owmtkt/i_brought_cuda_back_to_macos_not_because_it_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owmtkt/i_brought_cuda_back_to_macos_not_because_it_was/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T04:20:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox8oth</id>
    <title>Build RAG Evals from your Docs with Synthetic Data Generation (plus reranking, semantic chunking, and RAG over MCP) [Kiln AI]</title>
    <updated>2025-11-14T21:11:30+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox8oth/build_rag_evals_from_your_docs_with_synthetic/"&gt; &lt;img alt="Build RAG Evals from your Docs with Synthetic Data Generation (plus reranking, semantic chunking, and RAG over MCP) [Kiln AI]" src="https://external-preview.redd.it/bjdidTgyMHpmYTFnMZ0LLxhMcl41SYgtW1Imv6oloK9nZBtq79RPecQv5U1A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a89fd99ce0c9bc576923ecc1a7d79cbae20c194" title="Build RAG Evals from your Docs with Synthetic Data Generation (plus reranking, semantic chunking, and RAG over MCP) [Kiln AI]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just created an interactive tool for building RAG evals, as part of &lt;a href="https://github.com/Kiln-AI/Kiln"&gt;the Github Project Kiln&lt;/a&gt;. It generates a RAG eval from your documents using synthetic data generation, through a fully interactive UI.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; Evaluating RAG is tricky. An LLM-as-judge doesn't have the knowledge from your documents, so it can't tell if a response is actually correct. But giving the judge access to RAG biases the evaluation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The solution:&lt;/strong&gt; Reference-answer evals. The judge compares results to a known correct answer. Building these datasets used to be a long manual process.&lt;/p&gt; &lt;p&gt;Kiln can now build Q&amp;amp;A datasets for evals by iterating over your document store. The process is fully interactive and takes just a few minutes to generate hundreds of reference answers. Use it to evaluate RAG accuracy end-to-end, including whether your agent calls RAG at the right times with quality queries. &lt;a href="https://docs.kiln.tech/docs/evaluations/evaluate-rag-accuracy-q-and-a-evals"&gt;&lt;strong&gt;Learn more in our docs&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Other new features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Semantic chunking&lt;/strong&gt;: Splits documents by meaning rather than length, improving retrieval accuracy&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reranking&lt;/strong&gt;: Add a reranking model to any RAG system you build in Kiln&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAG over MCP&lt;/strong&gt;: Expose your Kiln RAG tools to any MCP client with a CLI command&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Appropriate Tool Use Eval&lt;/strong&gt;: Verify tools are called at the right times and not when they shouldn't be&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/Kiln-AI/Kiln"&gt;GitHub repo&lt;/a&gt; (4.4k stars)&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.kiln.tech/docs/documents-and-search-rag"&gt;RAG/docs Guide&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.kiln.tech/docs/evaluations/evaluate-rag-accuracy-q-and-a-evals"&gt;RAG Q&amp;amp;A Eval Guide&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://getkiln.ai/discord"&gt;Discord&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://kiln.tech/"&gt;Kiln Homepage&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions or hear feature requests! Let me know if you want support for specific reranking models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7wy9a01zfa1g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox8oth/build_rag_evals_from_your_docs_with_synthetic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ox8oth/build_rag_evals_from_your_docs_with_synthetic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T21:11:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox1x54</id>
    <title>Risk of LLM Judges in Paper Review: Scores Could Mask Poor Quality</title>
    <updated>2025-11-14T16:55:58+00:00</updated>
    <author>
      <name>/u/pier4r</name>
      <uri>https://old.reddit.com/user/pier4r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See this twitter thread: &lt;a href="https://nitter.net/micahgoldblum/status/1989088547777966512"&gt;https://nitter.net/micahgoldblum/status/1989088547777966512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A couple of quotes&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;An LLM-generated paper is in the top 17% of ICLR submissions in terms of average reviewer score, having received two 8's. The paper has tons of BS jargon and hallucinated references. Fortunately, one reviewer actually looked at the paper and gave it a zero. &lt;/p&gt; &lt;p&gt;Do you think the other 2 reviewers who gave it 8 just used LLMs to review as well?&lt;/p&gt; &lt;p&gt;Likely&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;There are other discussions that also mentions: peer reviews are free (one can submit a ton of those). What if people simply produce a ton of paperslop to review and humans peer reviewers get fatigued, use LLMs as judges and those don't know better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pier4r"&gt; /u/pier4r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox1x54/risk_of_llm_judges_in_paper_review_scores_could/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox1x54/risk_of_llm_judges_in_paper_review_scores_could/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ox1x54/risk_of_llm_judges_in_paper_review_scores_could/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T16:55:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox9fzy</id>
    <title>Is there a self-hosted, open-source plug-and-play RAG solution?</title>
    <updated>2025-11-14T21:41:09+00:00</updated>
    <author>
      <name>/u/anedisi</name>
      <uri>https://old.reddit.com/user/anedisi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know about Ollama, llama-server, vLLM and all the other options for hosting LLMs, but I‚Äôm looking for something similar for RAG that I can self-host.&lt;/p&gt; &lt;p&gt;Basically: I want to store scraped websites, upload PDF files, and similar documents ‚Äî and have a simple system that handles: ‚Ä¢ vector DB storage ‚Ä¢ chunking ‚Ä¢ data ingestion ‚Ä¢ querying the vector DB when a user asks something ‚Ä¢ sending that to the LLM for final output&lt;/p&gt; &lt;p&gt;I know RAG gets complicated with PDFs containing tables, images, etc., but I just need a starting point so I don‚Äôt have to build all the boilerplate myself.&lt;/p&gt; &lt;p&gt;Is there any open-source, self-hosted solution that‚Äôs already close to this? Something I can install, run locally/server, and extend from?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anedisi"&gt; /u/anedisi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox9fzy/is_there_a_selfhosted_opensource_plugandplay_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox9fzy/is_there_a_selfhosted_opensource_plugandplay_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ox9fzy/is_there_a_selfhosted_opensource_plugandplay_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T21:41:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxdkdt</id>
    <title>Best local model to learn from?</title>
    <updated>2025-11-15T00:33:39+00:00</updated>
    <author>
      <name>/u/agreeduponspring</name>
      <uri>https://old.reddit.com/user/agreeduponspring</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently trying to learn quantum physics, and it's been invaluable having a model to talk to to get my own personal understanding sorted out. However, this is a subject where the risk of hallucinations I can't catch is quite high, so I'm wondering if there are any models known for being particularly good in this area.&lt;/p&gt; &lt;p&gt;The only constraint I have personally is that it needs to fit in 96GB of RAM - I can tolerate extremely slow token generation, but running from disk is the realm of the unhinged.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/agreeduponspring"&gt; /u/agreeduponspring &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxdkdt/best_local_model_to_learn_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxdkdt/best_local_model_to_learn_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxdkdt/best_local_model_to_learn_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T00:33:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxj2mq</id>
    <title>I just realized 20 tokens per second is a decent speed in token generation.</title>
    <updated>2025-11-15T05:04:06+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I can ever afford a mac studio with 512 unified memory, I will happily take it. I just want inference and even 20 tokens per second is not bad. At least I‚Äôll be able to locally run models on it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxj2mq/i_just_realized_20_tokens_per_second_is_a_decent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxj2mq/i_just_realized_20_tokens_per_second_is_a_decent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxj2mq/i_just_realized_20_tokens_per_second_is_a_decent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T05:04:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxhqk5</id>
    <title>Kimi K2 Thinking 1bit just 0.22 tokens/s on 512GB RAM RTX 4090 EPYC 64 core machine</title>
    <updated>2025-11-15T03:53:14+00:00</updated>
    <author>
      <name>/u/eesahe</name>
      <uri>https://old.reddit.com/user/eesahe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxhqk5/kimi_k2_thinking_1bit_just_022_tokenss_on_512gb/"&gt; &lt;img alt="Kimi K2 Thinking 1bit just 0.22 tokens/s on 512GB RAM RTX 4090 EPYC 64 core machine" src="https://b.thumbs.redditmedia.com/NQDCmAQpV2nGU4fI3h8biPWwPrMM18Wnea4JKLst44s.jpg" title="Kimi K2 Thinking 1bit just 0.22 tokens/s on 512GB RAM RTX 4090 EPYC 64 core machine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As per the &lt;a href="https://docs.unsloth.ai/models/kimi-k2-thinking-how-to-run-locally"&gt;unsloth guide &lt;/a&gt;it seems I should be expecting around an order of magnitude faster speeds with the UD-TQ1_0 quant.&lt;/p&gt; &lt;p&gt;I wonder if there's anything simple I might be doing wrong.&lt;/p&gt; &lt;p&gt;This is how I'm running it:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Build latest llama.cpp (15th Nov)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd ~/src git clone https://github.com/ggml-org/llama.cpp cmake llama.cpp -B llama.cpp/build \ -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON cmake \ --build llama.cpp/build \ --config Release -j --clean-first \ --target llama-quantize llama-cli llama-gguf-split llama-mtmd-cli llama-server cp llama.cpp/build/bin/llama-* llama.cpp/ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Run llama-server&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; ./llama.cpp/llama-server \ --model ~/models/UD-TQ1_0/Kimi-K2-Thinking-UD-TQ1_0-00001-of-00006.gguf \ --alias &amp;quot;unsloth/Kimi-K2-Thinking&amp;quot; \ --threads -1 \ -fa on \ --n-gpu-layers 999 \ -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; \ --min_p 0.01 \ --ctx-size 16384 \ --port 8002 \ --jinja &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is the performance I'm getting in the web UI:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2pbs2j75ec1g1.png?width=1714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=853b85d19909c0fab57cfd2295b7e1b07e255369"&gt;https://preview.redd.it/2pbs2j75ec1g1.png?width=1714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=853b85d19909c0fab57cfd2295b7e1b07e255369&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From another request:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 17950.58 ms / 26 tokens ( 690.41 ms per token, 1.45 tokens per second) eval time = 522630.84 ms / 110 tokens ( 4751.19 ms per token, 0.21 tokens per second) total time = 540581.43 ms / 136 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;nvidia-smi while generating:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ nvidia-smi Sat Nov 15 03:51:35 2025 +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 560.28.03 Driver Version: 560.28.03 CUDA Version: 12.6 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 4090 On | 00000000:83:00.0 Off | Off | | 0% 55C P0 69W / 450W | 12894MiB / 24564MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ +-----------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=========================================================================================| | 0 N/A N/A 1332381 C ./llama.cpp/llama-server 12884MiB | +-----------------------------------------------------------------------------------------+ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;llama-server in top while generating:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1332381 eesahe 20 0 281.3g 229.4g 229.1g S 11612 45.5 224:01.19 llama-server &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eesahe"&gt; /u/eesahe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxhqk5/kimi_k2_thinking_1bit_just_022_tokenss_on_512gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxhqk5/kimi_k2_thinking_1bit_just_022_tokenss_on_512gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxhqk5/kimi_k2_thinking_1bit_just_022_tokenss_on_512gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T03:53:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxexii</id>
    <title>I benchmarked "vanilla" and REAP'd Qwen3-Coder models locally, do my results match your experience?</title>
    <updated>2025-11-15T01:37:15+00:00</updated>
    <author>
      <name>/u/MutantEggroll</name>
      <uri>https://old.reddit.com/user/MutantEggroll</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxexii/i_benchmarked_vanilla_and_reapd_qwen3coder_models/"&gt; &lt;img alt="I benchmarked &amp;quot;vanilla&amp;quot; and REAP'd Qwen3-Coder models locally, do my results match your experience?" src="https://b.thumbs.redditmedia.com/9XKrNlj46QmQKY1rIwpmR8xy9WXbqxRPmqi7u7IoUDw.jpg" title="I benchmarked &amp;quot;vanilla&amp;quot; and REAP'd Qwen3-Coder models locally, do my results match your experience?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been curious about REAPs, and how they might compare to Unsloth Dynamic quants (my current go-to). So, I ran a few iterations of aider polyglot locally to get a sense of which gives the best bang-for-VRAM. Test setup and results below:&lt;/p&gt; &lt;p&gt;TL;DR: Statistically speaking, with my small sample size, I did not find a benefit to the REAP variant of Qwen3-Coder-30B-A3B.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;&lt;br /&gt; Determine whether the higher quants enabled by REAP'd models' smaller initial size provides benefits to coding performance, which tends to be heavily impacted by quantization. In this case, pitting Unsloth's UD-Q6_K_XL of &amp;quot;vanilla&amp;quot; Qwen3-Coder-30B-A3B against bartowski's Q8_0 of Qwen3-Coder-REAP-25B-A3B, both of which fit fully in a 5090's VRAM with room for 40k context.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Configuration&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Unsloth Dynamic&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;qwen3-coder-30b-a3b-instruct&amp;quot;: cmd: | ${LLAMA_SERVER_CMD} ${BOILERPLATE_SETTINGS} --model &amp;quot;${MODEL_BASE_DIR}\unsloth\Qwen3-Coder-30B-A3B-Instruct-GGUF\Qwen3-Coder-30B-A3B-Instruct-UD-Q6_K_XL.gguf&amp;quot; --ctx-size 40960 --temp 0.7 --min-p 0.0 --top-p 0.8 --top-k 20 --repeat-penalty 1.05 --jinja &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;REAP&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;qwen3-coder-REAP-25B-A3B&amp;quot;: cmd: | ${LLAMA_SERVER_CMD} ${BOILERPLATE_SETTINGS} --model &amp;quot;${MODEL_BASE_DIR}\bartowski\cerebras_Qwen3-Coder-REAP-25B-A3B-GGUF\cerebras_Qwen3-Coder-REAP-25B-A3B-Q8_0.gguf&amp;quot; --ctx-size 40960 --temp 0.7 --min-p 0.0 --top-p 0.8 --top-k 20 --repeat-penalty 1.05 --jinja &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Aider Command&lt;/strong&gt;&lt;br /&gt; &lt;code&gt;OPENAI_BASE_URL=http://&amp;lt;llama-swap host IP&amp;gt;:8080/v1 OPENAI_API_KEY=&amp;quot;none&amp;quot; ./benchmark/benchmark.py &amp;lt;results dir name&amp;gt; --model openai/&amp;lt;model name&amp;gt; --num-ctx 40960 --edit-format whole --threads 1 --sleep 5 --exercises-dir polyglot-benchmark --new&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vsahw6pqkb1g1.png?width=1359&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0ec3f9432e1cb55c5d1edbe86b271f1a9c3e6a09"&gt;aider-polyglot 0.86.2.dev results&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;/th&gt; &lt;th align="left"&gt;Unsloth Dynamic&lt;/th&gt; &lt;th align="left"&gt;REAP&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Pass 1 Average&lt;/td&gt; &lt;td align="left"&gt;12.0%&lt;/td&gt; &lt;td align="left"&gt;10.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Pass 1 Std. Dev.&lt;/td&gt; &lt;td align="left"&gt;0.77%&lt;/td&gt; &lt;td align="left"&gt;2.45%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Pass 2 Average&lt;/td&gt; &lt;td align="left"&gt;29.9%&lt;/td&gt; &lt;td align="left"&gt;28.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Pass 2 Std. Dev.&lt;/td&gt; &lt;td align="left"&gt;1.56%&lt;/td&gt; &lt;td align="left"&gt;2.31%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;This amounts to a tie, since each model's average Pass 2 results fall within the other's standard deviation. Meaning, for this benchmark, there is no benefit to using the higher quant of the REAP'd model. And it's possible that it's a detriment, given the higher variability of results from the REAP'd model.&lt;/p&gt; &lt;p&gt;That said, I'd caution reading too much into this result. Though aider polyglot is in my opinion a good benchmark, and each run at 40k context contains 225 test cases, 3 runs on 2 models is not peer-review-worthy research.&lt;/p&gt; &lt;p&gt;For those of you who've used both &amp;quot;vanilla&amp;quot; and REAP'd models for coding, does this match your experience? Do you notice other things that wouldn't show up in this kind of benchmark?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MutantEggroll"&gt; /u/MutantEggroll &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxexii/i_benchmarked_vanilla_and_reapd_qwen3coder_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxexii/i_benchmarked_vanilla_and_reapd_qwen3coder_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxexii/i_benchmarked_vanilla_and_reapd_qwen3coder_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T01:37:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1owx1nh</id>
    <title>The company gmktec made a comparison of the EVO-X2 that has a Ryzen AI Max+ 395 processor vs NVIDIA DGX SPARK</title>
    <updated>2025-11-14T13:52:15+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owx1nh/the_company_gmktec_made_a_comparison_of_the_evox2/"&gt; &lt;img alt="The company gmktec made a comparison of the EVO-X2 that has a Ryzen AI Max+ 395 processor vs NVIDIA DGX SPARK" src="https://preview.redd.it/pl1lqj8r981g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1855485e8cb6f5d5b69639209615733627982830" title="The company gmktec made a comparison of the EVO-X2 that has a Ryzen AI Max+ 395 processor vs NVIDIA DGX SPARK" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My point is that they should make comparisons with small models that have come out lately because they are enough for most people and because the inference is also faster&lt;/p&gt; &lt;p&gt;Info :&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.gmktec.com/blog/evo-x2-vs-nvidia-dgx-spark-redefining-local-ai-performance"&gt;https://www.gmktec.com/blog/evo-x2-vs-nvidia-dgx-spark-redefining-local-ai-performance&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pl1lqj8r981g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owx1nh/the_company_gmktec_made_a_comparison_of_the_evox2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owx1nh/the_company_gmktec_made_a_comparison_of_the_evox2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T13:52:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1owocd2</id>
    <title>Is it normal to hear weird noises when running an LLM on 4√ó Pro 6000 Max-Q cards?</title>
    <updated>2025-11-14T05:41:14+00:00</updated>
    <author>
      <name>/u/PlusProfession9245</name>
      <uri>https://old.reddit.com/user/PlusProfession9245</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owocd2/is_it_normal_to_hear_weird_noises_when_running_an/"&gt; &lt;img alt="Is it normal to hear weird noises when running an LLM on 4√ó Pro 6000 Max-Q cards?" src="https://external-preview.redd.it/MnFzdzJ0b3l0NTFnMbphl7ifhldDVQJssqSE3uLNJKqrQJ4o9dG0SGtQf767.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b2e8e94666721a90be11f2cea3b9f593dc28f21" title="Is it normal to hear weird noises when running an LLM on 4√ó Pro 6000 Max-Q cards?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It doesn‚Äôt sound like normal coil whine.&lt;br /&gt; In a Docker environment, when I run gpt-oss-120b across 4 GPUs, I hear a strange noise.&lt;br /&gt; The sound is also different depending on the model.&lt;br /&gt; Is this normal??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PlusProfession9245"&gt; /u/PlusProfession9245 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9eez1soyt51g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owocd2/is_it_normal_to_hear_weird_noises_when_running_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owocd2/is_it_normal_to_hear_weird_noises_when_running_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T05:41:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxeif6</id>
    <title>Premise: MoE models have exploitable locality in expert activation patterns, and LRU caching with profiling could cut VRAM requirements in half.</title>
    <updated>2025-11-15T01:16:49+00:00</updated>
    <author>
      <name>/u/CodeSlave9000</name>
      <uri>https://old.reddit.com/user/CodeSlave9000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently doing some brainstorming - and a few back-of-the-page calculations, and came up with this. The premise is that with some profiling based on actual user workload, we should be able to determine expert activation patterns and locality for caching. TLDR; A &amp;quot;smart&amp;quot; MOE caching size could reduce VRAM needs by up to half. I'm sure I'm not the first to think about this, and I'm sure I've got a screw loose, but maybe someone can set me straight.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MoE models have exploitable locality in expert activation patterns, and LRU caching with profiling could cut VRAM requirements in half&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Meaning, that:&lt;/p&gt; &lt;p&gt;Total VRAM budget: X&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Expert size: &lt;strong&gt;E&lt;/strong&gt; (some fraction of total model Y)&lt;/li&gt; &lt;li&gt;Can fit in cache: &lt;strong&gt;C = X / E&lt;/strong&gt; experts&lt;/li&gt; &lt;li&gt;Experts activated per token across all layers: &lt;strong&gt;A&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;LRU cache hit rate: &lt;strong&gt;H&lt;/strong&gt; (empirically ~70-80% with temporal locality)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Cost Model&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Without swapping&lt;/strong&gt;: Need all experts in VRAM = can't run the model if total experts &amp;gt; X&lt;/p&gt; &lt;p&gt;&lt;strong&gt;With swapping&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cache hits: free (already in VRAM)&lt;/li&gt; &lt;li&gt;Cache misses: pay PCIe transfer cost&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Per-token cost&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Expert activations needed: A&lt;/li&gt; &lt;li&gt;Cache hits: A √ó H (free)&lt;/li&gt; &lt;li&gt;Cache misses: A √ó (1 - H) √ó transfer_cost&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Transfer cost&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PCIe bandwidth: ~25 GB/s practical&lt;/li&gt; &lt;li&gt;Expert size: E&lt;/li&gt; &lt;li&gt;Transfer time: E / 25 GB/s&lt;/li&gt; &lt;li&gt;Token generation time target: ~10-50ms (20-100 tokens/sec)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Break-even -&lt;/p&gt; &lt;p&gt;You want: &lt;code&gt;cache_miss_overhead &amp;lt; token_generation_time_savings&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Simple threshold&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;If C ‚â• A / (1 - target_miss_rate) then swapping is likely worth it&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Per layer&lt;/strong&gt; (assuming 8 experts per layer):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If C_layer = 2: you can only fit exactly what's needed, 0% cache benefit&lt;/li&gt; &lt;li&gt;If C_layer = 4: ~50-60% hit rate&lt;/li&gt; &lt;li&gt;If C_layer = 6: ~75-85% hit rate&lt;/li&gt; &lt;li&gt;If C_layer = 8: 100% hit rate (all experts cached)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Break-even point&lt;/strong&gt;: When &lt;code&gt;(1 - H) √ó E / 25GB/s &amp;lt; token_budget&lt;/code&gt;&lt;/p&gt; &lt;p&gt;If E = 1GB, token_budget = 20ms:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;With H = 75%: 0.25 √ó 1GB / 25GB/s = 10ms ‚úì Worth it&lt;/li&gt; &lt;li&gt;With H = 50%: 0.50 √ó 1GB / 25GB/s = 20ms ‚âà Break-even&lt;/li&gt; &lt;li&gt;With H = 25%: 0.75 √ó 1GB / 25GB/s = 30ms ‚úó Too slow&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you can fit at least &lt;strong&gt;half the experts&lt;/strong&gt; in VRAM, LRU swapping is likely a win because temporal locality gives you 70-80% hit rates.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Not worth it when&lt;/strong&gt;: C &amp;lt; 0.25 √ó total_experts - you're thrashing too much&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Sweet spot&lt;/strong&gt;: Models where you can fit 50-75% of experts - you get most of the benefit of the full model at a fraction of the VRAM cost.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CodeSlave9000"&gt; /u/CodeSlave9000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxeif6/premise_moe_models_have_exploitable_locality_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxeif6/premise_moe_models_have_exploitable_locality_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxeif6/premise_moe_models_have_exploitable_locality_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T01:16:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1owskm6</id>
    <title>Windows llama.cpp is 20% faster</title>
    <updated>2025-11-14T10:05:21+00:00</updated>
    <author>
      <name>/u/johannes_bertens</name>
      <uri>https://old.reddit.com/user/johannes_bertens</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owskm6/windows_llamacpp_is_20_faster/"&gt; &lt;img alt="Windows llama.cpp is 20% faster" src="https://preview.redd.it/tfdcbkf6571g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f97a0d3f3c6a2519462ab5e159f2045396e9409" title="Windows llama.cpp is 20% faster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;But why?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Windows: 1000+ PP&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;llama-bench -m C:\Users\johan\.lmstudio\models\unsloth\Qwen3-VL-30B-A3B-Instruct-GGUF\Qwen3-VL-30B-A3B-Instruct-UD-Q8_K_XL.gguf -p 512,1024,2048,4096 -n 0 -fa 0 --mmap 0&lt;br /&gt; load_backend: loaded RPC backend from C:\Users\johan\Downloads\llama-b7032-bin-win-vulkan-x64\ggml-rpc.dll&lt;br /&gt; ggml_vulkan: Found 1 Vulkan devices:&lt;br /&gt; ggml_vulkan: 0 = AMD Radeon(TM) 8060S Graphics (AMD proprietary driver) | uma: 1 | fp16: 1 | bf16: 1 | warp size: 64 | shared memory: 32768 | int dot: 1 | matrix cores: KHR_coopmat&lt;br /&gt; load_backend: loaded Vulkan backend from C:\Users\johan\Downloads\llama-b7032-bin-win-vulkan-x64\ggml-vulkan.dll&lt;br /&gt; load_backend: loaded CPU backend from C:\Users\johan\Downloads\llama-b7032-bin-win-vulkan-x64\ggml-cpu-icelake.dll &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model &lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend &lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;mmap&lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp512&lt;/td&gt; &lt;td align="right"&gt; 1079.12 ¬± 4.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp1024&lt;/td&gt; &lt;td align="right"&gt; 975.04 ¬± 4.46&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 892.94 ¬± 2.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp4096&lt;/td&gt; &lt;td align="right"&gt; 806.84 ¬± 2.89&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Linux: 880 PP&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; [johannes@toolbx ~]$ llama-bench -m models/Qwen3-VL-30B-A3B-Instruct-UD-Q8_K_XL.gguf -p 512,1024,2048,4096 -n 0 -fa 0 --mmap 0&lt;br /&gt; ggml_vulkan: Found 1 Vulkan devices:&lt;br /&gt; ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model &lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend &lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;mmap&lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp512&lt;/td&gt; &lt;td align="right"&gt; 876.79 ¬± 4.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp1024&lt;/td&gt; &lt;td align="right"&gt; 797.87 ¬± 1.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 757.55 ¬± 2.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp4096&lt;/td&gt; &lt;td align="right"&gt; 686.61 ¬± 0.89&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Obviously it's not 20% over the board, but still a very big difference. Is the &amp;quot;AMD proprietary driver&amp;quot; such a big deal?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johannes_bertens"&gt; /u/johannes_bertens &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tfdcbkf6571g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owskm6/windows_llamacpp_is_20_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owskm6/windows_llamacpp_is_20_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T10:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1owyp8q</id>
    <title>The Big LLM Architecture Comparison: From DeepSeek-V3 to Kimi K2 Thinking</title>
    <updated>2025-11-14T14:58:02+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owyp8q/the_big_llm_architecture_comparison_from/"&gt; &lt;img alt="The Big LLM Architecture Comparison: From DeepSeek-V3 to Kimi K2 Thinking" src="https://external-preview.redd.it/5N8z_mXiAneWfY6B3hrkRbiDD5IkgsvFJWMT1AAURS8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c88626d25ad555fca14567f2825f2de4449a35ff" title="The Big LLM Architecture Comparison: From DeepSeek-V3 to Kimi K2 Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owyp8q/the_big_llm_architecture_comparison_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owyp8q/the_big_llm_architecture_comparison_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T14:58:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox3e1f</id>
    <title>Drummer's Precog 24B and 123B v1 - AI that writes a short draft before responding</title>
    <updated>2025-11-14T17:50:02+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox3e1f/drummers_precog_24b_and_123b_v1_ai_that_writes_a/"&gt; &lt;img alt="Drummer's Precog 24B and 123B v1 - AI that writes a short draft before responding" src="https://external-preview.redd.it/vl2ei1-FehJR-7jZHQXuFZ_Y0kemf2CP216W8qh6VxE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2798025eaf994dc8b7c090c13aa6bdefb4507a02" title="Drummer's Precog 24B and 123B v1 - AI that writes a short draft before responding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! &lt;/p&gt; &lt;p&gt;I wanted to explore a different way of thinking where the AI uses the &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; block to plan ahead and create a short draft so that its &lt;em&gt;actual&lt;/em&gt; response has &lt;strong&gt;basis&lt;/strong&gt;. It seems like a good way to have the AI pan out its start, middle, and end before writing the entire thing. Kind of like a synopsis or abstract. &lt;/p&gt; &lt;p&gt;I'm hoping it could strengthen consistency and flow since the AI doesn't have to &lt;em&gt;wing it&lt;/em&gt; and write a thousand tokens from the get-go. It's a cheaper, more effective alternative to reasoning, especially when it comes to story / RP. You can also make adjustments to the draft to steer it a certain way. Testers have been happy with it.&lt;/p&gt; &lt;p&gt;24B: &lt;a href="https://huggingface.co/TheDrummer/Precog-24B-v1"&gt;https://huggingface.co/TheDrummer/Precog-24B-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;123B: &lt;a href="https://huggingface.co/TheDrummer/Precog-123B-v1"&gt;https://huggingface.co/TheDrummer/Precog-123B-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Examples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1li2viecf91g1.png?width=2264&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af225606b23751beaf3076b1a58140b1c77b1a4f"&gt;https://preview.redd.it/1li2viecf91g1.png?width=2264&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af225606b23751beaf3076b1a58140b1c77b1a4f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7iu4m7zcf91g1.png?width=887&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4de7655654340ec91216d8a61c93c474571b1dc0"&gt;https://preview.redd.it/7iu4m7zcf91g1.png?width=887&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4de7655654340ec91216d8a61c93c474571b1dc0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3qo833ndf91g1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0cac98a5e93dd87baa885bda58574385b8e73c11"&gt;https://preview.redd.it/3qo833ndf91g1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0cac98a5e93dd87baa885bda58574385b8e73c11&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox3e1f/drummers_precog_24b_and_123b_v1_ai_that_writes_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox3e1f/drummers_precog_24b_and_123b_v1_ai_that_writes_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ox3e1f/drummers_precog_24b_and_123b_v1_ai_that_writes_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T17:50:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxb9zp</id>
    <title>Local models handle tools way better when you give them a code sandbox instead of individual tools</title>
    <updated>2025-11-14T22:54:49+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxb9zp/local_models_handle_tools_way_better_when_you/"&gt; &lt;img alt="Local models handle tools way better when you give them a code sandbox instead of individual tools" src="https://preview.redd.it/83hx5w1txa1g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e2fbc834e05621ee050a05b0ee016fd280ff683" title="Local models handle tools way better when you give them a code sandbox instead of individual tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/83hx5w1txa1g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxb9zp/local_models_handle_tools_way_better_when_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxb9zp/local_models_handle_tools_way_better_when_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T22:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxbgo2</id>
    <title>New Chinese optical quantum chip allegedly 1,000x faster than Nvidia GPUs for processing AI workloads - firm reportedly producing 12,000 wafers per year</title>
    <updated>2025-11-14T23:02:23+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxbgo2/new_chinese_optical_quantum_chip_allegedly_1000x/"&gt; &lt;img alt="New Chinese optical quantum chip allegedly 1,000x faster than Nvidia GPUs for processing AI workloads - firm reportedly producing 12,000 wafers per year" src="https://external-preview.redd.it/wyPTYhjvk1ZkI6QWcppW9U0hNBCsQcC_EN7LGK2SdOo.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35f766b5512f0a0b22b8d34fceb1428ec2479e57" title="New Chinese optical quantum chip allegedly 1,000x faster than Nvidia GPUs for processing AI workloads - firm reportedly producing 12,000 wafers per year" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/tech-industry/quantum-computing/new-chinese-optical-quantum-chip-allegedly-1-000x-faster-than-nvidia-gpus-for-processing-ai-workloads-but-yields-are-low"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxbgo2/new_chinese_optical_quantum_chip_allegedly_1000x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxbgo2/new_chinese_optical_quantum_chip_allegedly_1000x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T23:02:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oximzj</id>
    <title>Anthropic pushing again for regulation of open source models?</title>
    <updated>2025-11-15T04:40:56+00:00</updated>
    <author>
      <name>/u/MasterDragon_</name>
      <uri>https://old.reddit.com/user/MasterDragon_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oximzj/anthropic_pushing_again_for_regulation_of_open/"&gt; &lt;img alt="Anthropic pushing again for regulation of open source models?" src="https://preview.redd.it/623qojxaoc1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd955c46ca05077bed949b46643bd7061e16d04c" title="Anthropic pushing again for regulation of open source models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MasterDragon_"&gt; /u/MasterDragon_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/623qojxaoc1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oximzj/anthropic_pushing_again_for_regulation_of_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oximzj/anthropic_pushing_again_for_regulation_of_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T04:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oth5pw</id>
    <title>AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model</title>
    <updated>2025-11-10T15:44:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt; &lt;img alt="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" src="https://b.thumbs.redditmedia.com/yz9_FpdLcHNiCkaH5fLEIoXS2f5u5twNBr7SQ9Go3AI.jpg" title="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Moonshot AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/ComfortableAsk4494"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/zxytim"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ppwwyyxx"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87"&gt;https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:44:10+00:00</published>
  </entry>
</feed>
