<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-23T11:08:26+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qknay6</id>
    <title>Need On-Site GPU Cluster Engineer in Delhi NCR - Grace Blackwell EdgeXpert Setup</title>
    <updated>2026-01-23T10:25:46+00:00</updated>
    <author>
      <name>/u/Soft_Ad6760</name>
      <uri>https://old.reddit.com/user/Soft_Ad6760</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for an experienced GPU cluster engineer for on-site work in Faridabad (Delhi NCR).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;2√ó MSI EdgeXpert (NVIDIA Grace Blackwell GB10)&lt;/li&gt; &lt;li&gt;MSI Raider 18 (RTX 5090)&lt;/li&gt; &lt;li&gt;ConnectX-7 QSFP56 interconnect&lt;/li&gt; &lt;li&gt;10G networking switch&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I Need:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Physical installation and cabling&lt;/li&gt; &lt;li&gt;DGX OS setup on both nodes&lt;/li&gt; &lt;li&gt;Multi-node clustering configuration (MPI/NCCL)&lt;/li&gt; &lt;li&gt;Performance validation and testing&lt;/li&gt; &lt;li&gt;Basic documentation/handover&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real hands-on experience with GPU clusters (DGX, ConnectX, InfiniBand, etc.)&lt;/li&gt; &lt;li&gt;Available within 1-2 weeks for 1-2 days on-site&lt;/li&gt; &lt;li&gt;Based in or can travel to Delhi NCR&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why I'm posting here:&lt;/strong&gt; Got an Upwork response from an agency sending &amp;quot;a guy&amp;quot; with generic IT experience. Want to work directly with someone who actually knows this hardware.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Soft_Ad6760"&gt; /u/Soft_Ad6760 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qknay6/need_onsite_gpu_cluster_engineer_in_delhi_ncr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qknay6/need_onsite_gpu_cluster_engineer_in_delhi_ncr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qknay6/need_onsite_gpu_cluster_engineer_in_delhi_ncr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T10:25:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjrsur</id>
    <title>GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp</title>
    <updated>2026-01-22T11:10:42+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjrsur/glm_47_flash_fa_fix_for_cuda_has_been_merged_into/"&gt; &lt;img alt="GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp" src="https://external-preview.redd.it/TcGseIeP3Z00NB4otbKR8-_fs_ssjxg6HC4Fv_lVbUU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d37072f08acdffcd4c3617847f00d2a9b9bafcf4" title="GLM 4.7 flash FA fix for CUDA has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18953"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjrsur/glm_47_flash_fa_fix_for_cuda_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjrsur/glm_47_flash_fa_fix_for_cuda_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T11:10:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkjrrc</id>
    <title>Whisper.cpp update: answering common questions + prototype progress (alignment, UI, free access)</title>
    <updated>2026-01-23T06:47:39+00:00</updated>
    <author>
      <name>/u/Curious_File7648</name>
      <uri>https://old.reddit.com/user/Curious_File7648</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkjrrc/whispercpp_update_answering_common_questions/"&gt; &lt;img alt="Whisper.cpp update: answering common questions + prototype progress (alignment, UI, free access)" src="https://preview.redd.it/mebiju8co1fg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b19d92795b34d2d94f3a17cba490081dbf46d875" title="Whisper.cpp update: answering common questions + prototype progress (alignment, UI, free access)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, following up on my earlier posts about building a &lt;strong&gt;Whisper.cpp-based local transcription and subtitle editor&lt;/strong&gt;. A lot of people asked questions in comments and DMs, so I wanted to answer them properly and share where things stand now.&lt;/p&gt; &lt;p&gt;Older Post:-&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1q8m9lq/building_a_whispercpp_transcription_app_focused/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;Building a Whisper.cpp transcription app focused on accurate alignment ‚Äî need thoughts&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Q: Is this still just a backend experiment, or a real usable tool now?&lt;/h1&gt; &lt;p&gt;It‚Äôs now very much a &lt;strong&gt;usable prototype&lt;/strong&gt;. The core pipeline is stable and working end-to-end, not just demos or tests.&lt;/p&gt; &lt;p&gt;What‚Äôs solid now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Local &lt;strong&gt;Whisper.cpp transcription&lt;/strong&gt; (CPU + GPU)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Proper word to word alignment&lt;/strong&gt; that holds up across languages&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Manual alignment tools&lt;/strong&gt; to fix words or segments when auto alignment isn‚Äôt perfect&lt;/li&gt; &lt;li&gt;A smooth &lt;strong&gt;editor-style UI&lt;/strong&gt; instead of a raw timeline&lt;/li&gt; &lt;li&gt;Built-in subtitle styles, effects, and clean export flow&lt;/li&gt; &lt;li&gt;Runs smoothly on normal PCs, no cloud required&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Q: Did you improve the UI? A few people said it felt rough earlier.&lt;/h1&gt; &lt;p&gt;Yes , that feedback was valid.&lt;/p&gt; &lt;p&gt;The early UI was very raw because the focus was accuracy and alignment first. The current build feels much closer to a &lt;strong&gt;proper editor&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;smoother timeline interaction&lt;/li&gt; &lt;li&gt;easier controls for non-technical users&lt;/li&gt; &lt;li&gt;manual fixing doesn‚Äôt feel painful anymore&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The screenshots shared earlier were from testing builds. The UI/UX is now much more polished, and still improving.&lt;/p&gt; &lt;h1&gt;Q: Why local Whisper instead of cloud APIs?&lt;/h1&gt; &lt;p&gt;This hasn‚Äôt changed.&lt;/p&gt; &lt;p&gt;Local Whisper gives:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;full control over words, timestamps, and languages&lt;/li&gt; &lt;li&gt;consistent results for &lt;strong&gt;non-English and mixed languages&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;no hallucinations caused by black-box APIs&lt;/li&gt; &lt;li&gt;no dependency on internet or usage limits&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I did test cloud options (like Groq). They‚Äôre fast and fine for English, but once you move to other languages, accuracy and alignment become unreliable.&lt;/p&gt; &lt;h1&gt;Q: Will this be paid?&lt;/h1&gt; &lt;p&gt;This is an important one.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The plan is to keep this free for the community.&lt;/strong&gt;&lt;br /&gt; Accessibility is the main reason this exists good transcription and alignment shouldn‚Äôt be locked behind expensive subscriptions.&lt;/p&gt; &lt;p&gt;That said, I‚Äôm being careful about licensing.&lt;/p&gt; &lt;h1&gt;Q: How do you keep it free without it being misused?&lt;/h1&gt; &lt;p&gt;This is something I‚Äôm actively looking for input on.&lt;/p&gt; &lt;p&gt;I‚Äôm trying to figure out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;how to keep it &lt;strong&gt;free for individuals and creators&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;while avoiding obvious misuse (reselling, bundling into paid tools, etc.)&lt;/li&gt; &lt;li&gt;what kind of &lt;strong&gt;license model&lt;/strong&gt; makes sense here&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If anyone has experience with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;open-source vs source-available licenses&lt;/li&gt; &lt;li&gt;community-friendly licensing&lt;/li&gt; &lt;li&gt;or similar projects that handled this well&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôd really appreciate pointers.&lt;/p&gt; &lt;p&gt;At this stage, I‚Äôm mainly looking for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;honest feedback on features that actually matter&lt;/li&gt; &lt;li&gt;whether manual alignment + editing tools are as important as people said&lt;/li&gt; &lt;li&gt;thoughts on licensing from people who‚Äôve been through this&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions and keep sharing updates as things move forward.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Curious_File7648"&gt; /u/Curious_File7648 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mebiju8co1fg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkjrrc/whispercpp_update_answering_common_questions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkjrrc/whispercpp_update_answering_common_questions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T06:47:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkirpl</id>
    <title>GLM4.7 Flash numbers on Apple Silicon?</title>
    <updated>2026-01-23T05:52:46+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious what folk are seeing for GLM4.7 flash on Apple silicone with MLX and llama.cpp? &lt;/p&gt; &lt;p&gt;(I'm holding off on trying it till things settle down a little bit more with the llama.cpp integration or conversely will finally pull the trigger with MLX if its showing significantly higher tok/s)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkirpl/glm47_flash_numbers_on_apple_silicon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkirpl/glm47_flash_numbers_on_apple_silicon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkirpl/glm47_flash_numbers_on_apple_silicon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T05:52:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkgxzk</id>
    <title>For coding, is it worth spinning to bigger models using heavy RAM, or staying small for speed? 48GB VRAM/120GB RAM</title>
    <updated>2026-01-23T04:21:21+00:00</updated>
    <author>
      <name>/u/CharlesStross</name>
      <uri>https://old.reddit.com/user/CharlesStross</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this is sort of a &amp;quot;how long is a length of string&amp;quot; question because ultimately it comes down to speed vs. quality, but wondered if anyone felt like there was a sufficient-enough-win using something like qwen 3 235b a22b that will just barely fit a quant in VRAM+RAM vs devstral that's going to fit entirely in VRAM. I'm kinda leading towards &amp;quot;code async and use the quality,&amp;quot; but maybe there's a better solution. I'm coming from Claude Code (can't keep spending $200/mo lol) so know there's gonna be a downgrade, but care a lot about code quality I'm working on primarily backend python as well as a smattering of very boring frontend, and occasionally systems work (ansible, terraform, etc.).&lt;/p&gt; &lt;p&gt;Any obvious thoughts or is it just a reality of &amp;quot;well, it's a trade off&amp;quot;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CharlesStross"&gt; /u/CharlesStross &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkgxzk/for_coding_is_it_worth_spinning_to_bigger_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkgxzk/for_coding_is_it_worth_spinning_to_bigger_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkgxzk/for_coding_is_it_worth_spinning_to_bigger_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T04:21:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkckmc</id>
    <title>Mistral Small Creative just beat Claude Opus 4.5, Sonnet 4.5, and GPT-OSS-120B on practical communication tasks</title>
    <updated>2026-01-23T01:02:44+00:00</updated>
    <author>
      <name>/u/Silver_Raspberry_811</name>
      <uri>https://old.reddit.com/user/Silver_Raspberry_811</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkckmc/mistral_small_creative_just_beat_claude_opus_45/"&gt; &lt;img alt="Mistral Small Creative just beat Claude Opus 4.5, Sonnet 4.5, and GPT-OSS-120B on practical communication tasks" src="https://a.thumbs.redditmedia.com/MkBdgYy1VwuwU-cAJXDpHPy8VKhmYf9muQ0eqpiQkV0.jpg" title="Mistral Small Creative just beat Claude Opus 4.5, Sonnet 4.5, and GPT-OSS-120B on practical communication tasks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run daily peer evaluations called The Multivac ‚Äî frontier models judging each other blind. Today's test: write 3 versions of an API outage message (internal Slack, enterprise email, public status page).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Mistral Small Creative‚Äîa model that gets a fraction of the attention of frontier giants‚Äîtook first place on a practical business task.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pre2wmf600fg1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d61bcbd4f368918233a544dfd5311bf596431c6d"&gt;https://preview.redd.it/pre2wmf600fg1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d61bcbd4f368918233a544dfd5311bf596431c6d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What made it win:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Its internal Slack message felt like an actual engineering lead wrote it. Specific, blameless, with concrete action items:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;That's the kind of language that actually helps teams improve.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The meta observation:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For practical communication tasks, raw parameter count isn't everything. Mistral seems to have strong instincts for tone and audience calibration‚Äîskills that don't necessarily scale linearly with model size.&lt;/p&gt; &lt;p&gt;Full methodology + all responses: &lt;a href="http://themultivac.com"&gt;themultivac.com&lt;/a&gt;&lt;br /&gt; LINK: &lt;a href="https://open.substack.com/pub/themultivac/p/a-small-model-just-beat-claude-opus?r=72olj0&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=true"&gt;https://open.substack.com/pub/themultivac/p/a-small-model-just-beat-claude-opus?r=72olj0&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=true&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Phase 3 coming soon:&lt;/strong&gt; We're working on the next evolution of evals. Datasets and outputs will be available for everyone to test and play with directly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silver_Raspberry_811"&gt; /u/Silver_Raspberry_811 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkckmc/mistral_small_creative_just_beat_claude_opus_45/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkckmc/mistral_small_creative_just_beat_claude_opus_45/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkckmc/mistral_small_creative_just_beat_claude_opus_45/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T01:02:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qk18y6</id>
    <title>Unsloth announces support for finetuning embedding models</title>
    <updated>2026-01-22T17:44:56+00:00</updated>
    <author>
      <name>/u/-Cubie-</name>
      <uri>https://old.reddit.com/user/-Cubie-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk18y6/unsloth_announces_support_for_finetuning/"&gt; &lt;img alt="Unsloth announces support for finetuning embedding models" src="https://external-preview.redd.it/ksRJC2bKGwjrMfOqsioi-B4oIm5QWQUM7Vf03KwieGM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f6fc5d8f727ab6f86a8ca5f94a5091bbe81d025" title="Unsloth announces support for finetuning embedding models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Daniel Han from Unsloth just announced finetuning embedding models with Unsloth and Sentence Transformers together:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Unsloth now has 1.8x-3.3x faster 20% less VRAM embedding finetuning! EmbeddingGemma, Qwen3 Embedding &amp;amp; all others work!&lt;br /&gt; We made 6 notebooks showing how you can customize for RAG, semantic similarity tasks &amp;amp; more. Transformers v5 works as well. Thanks huggingface for the collab!&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I've heard really good things about Unsloth for finetuning LLMs, so I have high hopes for this as well. Very promising for retrieval models for RAG etc, I think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cubie-"&gt; /u/-Cubie- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://unsloth.ai/docs/new/embedding-finetuning"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk18y6/unsloth_announces_support_for_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qk18y6/unsloth_announces_support_for_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T17:44:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qk93ol</id>
    <title>Built a mobile app (KernelAI) that runs 43+ models 100% on-device, 100 offline &amp; very well optimized AND it includes Gemma 3, llama 3, and other sick models like Phi and uncensored models like Dolphin. For fun I have included GPT-2 if you were ever wondering what AI looked like couple of years ago</title>
    <updated>2026-01-22T22:37:06+00:00</updated>
    <author>
      <name>/u/Better_Comment_7749</name>
      <uri>https://old.reddit.com/user/Better_Comment_7749</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To begin with, I hope you are having a wonderful day.&lt;/p&gt; &lt;p&gt;I got nerd snipped into build this app, I'm well aware that there is at least 2 other local ai apps in mobile. The goal of the current app is to offer a much higher model selection with a better UI experience (hopefully), and include as many IOS versions/phone models as possible. The app also include vision models (Qwen) that can read images, and TTS. I have put a LOT of efforts in trying to optimize the RAM consumption as much as possible, and the battery as well. So far, the recommended models (Llama 3.2, Gemma 3, IBM granite 4.0 micro etc..) are only consuming around 400 to 600 MB RAM.&lt;/p&gt; &lt;p&gt;If there is anything missing, or if you notice a bug, please do not hesitate to reach out. My current objective is to release the android version in the next days (It's a bit more challenging given that android have a ton of phone models).&lt;/p&gt; &lt;p&gt;kernelai in the appstore, link : &lt;a href="https://apps.apple.com/ca/app/kernelai/id6757350731"&gt;https://apps.apple.com/ca/app/kernelai/id6757350731&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd appreciate a lot a positive review in the app store!&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;p&gt;edit : 100% free &amp;amp; no friction&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Better_Comment_7749"&gt; /u/Better_Comment_7749 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk93ol/built_a_mobile_app_kernelai_that_runs_43_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk93ol/built_a_mobile_app_kernelai_that_runs_43_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qk93ol/built_a_mobile_app_kernelai_that_runs_43_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T22:37:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjul2g</id>
    <title>Qwen3 TTS just dropped üó£Ô∏èüîà</title>
    <updated>2026-01-22T13:31:10+00:00</updated>
    <author>
      <name>/u/Reasonable-Fun-7078</name>
      <uri>https://old.reddit.com/user/Reasonable-Fun-7078</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/QwenLM/Qwen3-TTS"&gt;https://github.com/QwenLM/Qwen3-TTS&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/collections/Qwen/qwen3-tts"&gt;https://huggingface.co/collections/Qwen/qwen3-tts&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable-Fun-7078"&gt; /u/Reasonable-Fun-7078 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul2g/qwen3_tts_just_dropped/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul2g/qwen3_tts_just_dropped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul2g/qwen3_tts_just_dropped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T13:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qklgft</id>
    <title>Leetcode for ML</title>
    <updated>2026-01-23T08:31:15+00:00</updated>
    <author>
      <name>/u/Big-Stick4446</name>
      <uri>https://old.reddit.com/user/Big-Stick4446</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qklgft/leetcode_for_ml/"&gt; &lt;img alt="Leetcode for ML" src="https://external-preview.redd.it/aXh0azY3bTE4MmZnMTOpzGvWJtEh0vETIlq5fXCK2iSGrkk3885uvQgRyBSE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=98a423fa3a1ac9af3d9a0980270ed0de108c48ef" title="Leetcode for ML" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, I built a platform called TensorTonic where you can implement 100+ ML algorithms from scratch.&lt;/p&gt; &lt;p&gt;Additionally, I added more than 60+ topics on mathematics fundamentals required to know ML.&lt;/p&gt; &lt;p&gt;I started this 2.5 months ago and already gained 7000 users. I will be shipping a lot of cool stuff ahead and would love the feedback from community on this.&lt;/p&gt; &lt;p&gt;Ps - Its completely free to use&lt;/p&gt; &lt;p&gt;Check it out here - tensortonic.com&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big-Stick4446"&gt; /u/Big-Stick4446 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cmffd63282fg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qklgft/leetcode_for_ml/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qklgft/leetcode_for_ml/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T08:31:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkbv12</id>
    <title>Finnaly I am in the club, rate my set up üòú</title>
    <updated>2026-01-23T00:31:35+00:00</updated>
    <author>
      <name>/u/black7stone</name>
      <uri>https://old.reddit.com/user/black7stone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkbv12/finnaly_i_am_in_the_club_rate_my_set_up/"&gt; &lt;img alt="Finnaly I am in the club, rate my set up üòú" src="https://preview.redd.it/dda95brpuzeg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0691e250029de84aec9a48bcb7ab1cd35596c718" title="Finnaly I am in the club, rate my set up üòú" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys finnaly I managed to get my own server PC, here a screenshot of the specifics.&lt;/p&gt; &lt;p&gt;At the moment I have an 3060 of 12 gb VRAM but I have ordered the 5060 ti 16gb Vram (ordered on the 3rd of January and will arrive on the 20th of Feb XD) then later I will keep both in my set up.&lt;/p&gt; &lt;p&gt;So what do you think about? I have 36 cores and 72 threads, 128 gb ram DDR 4 all on a nvme V4 of 1tb and running Ubuntu 24.&lt;/p&gt; &lt;p&gt;Any suggestions? Now I would like to profit from this set up some how, any tip? So I can make more more money and upgrade slowly.&lt;/p&gt; &lt;p&gt;I am installing llama 70b any other LLM worth it?&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/black7stone"&gt; /u/black7stone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dda95brpuzeg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkbv12/finnaly_i_am_in_the_club_rate_my_set_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkbv12/finnaly_i_am_in_the_club_rate_my_set_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T00:31:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkjjif</id>
    <title>Qwen3-TTS: Qwen Team Apache'd Their TTS Model</title>
    <updated>2026-01-23T06:34:18+00:00</updated>
    <author>
      <name>/u/Lopsided_Dot_4557</name>
      <uri>https://old.reddit.com/user/Lopsided_Dot_4557</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üîπ Design custom voices from natural language descriptions&lt;/p&gt; &lt;p&gt;üîπ Clone any voice from just 3 seconds of audio&lt;/p&gt; &lt;p&gt;üîπ 10 languages supported&lt;/p&gt; &lt;p&gt;üîπ 97ms end-to-end latency for real-time generation&lt;/p&gt; &lt;p&gt;üîπ Instruction-based control over emotion, tone &amp;amp; prosody&lt;/p&gt; &lt;p&gt;üîπ 1.7B params, runs locally with streaming support&lt;/p&gt; &lt;p&gt;HF Model: &lt;a href="https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice"&gt;https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Install and Test Demo: &lt;a href="https://youtu.be/gR5dyKaxpEk?si=Kjye6ubN3iwIjhTD"&gt;https://youtu.be/gR5dyKaxpEk?si=Kjye6ubN3iwIjhTD&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Dot_4557"&gt; /u/Lopsided_Dot_4557 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkjjif/qwen3tts_qwen_team_apached_their_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkjjif/qwen3tts_qwen_team_apached_their_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkjjif/qwen3tts_qwen_team_apached_their_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T06:34:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkef77</id>
    <title>I pre-trained and instruction tuned a 394M parameter LM from scratch :)</title>
    <updated>2026-01-23T02:25:42+00:00</updated>
    <author>
      <name>/u/SadEqual5367</name>
      <uri>https://old.reddit.com/user/SadEqual5367</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is the link to my repo: &lt;a href="https://github.com/pradyGn/zoof"&gt;https://github.com/pradyGn/zoof&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am reading about reasoning in SLMs and planning to add those capabilities into zoof. Any suggestions on interesting papers / repositories that I can read?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SadEqual5367"&gt; /u/SadEqual5367 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkef77/i_pretrained_and_instruction_tuned_a_394m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkef77/i_pretrained_and_instruction_tuned_a_394m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkef77/i_pretrained_and_instruction_tuned_a_394m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T02:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qk9vmv</id>
    <title>1.8-3.3x faster Embedding finetuning now in Unsloth (~3GB VRAM)</title>
    <updated>2026-01-22T23:09:04+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk9vmv/1833x_faster_embedding_finetuning_now_in_unsloth/"&gt; &lt;img alt="1.8-3.3x faster Embedding finetuning now in Unsloth (~3GB VRAM)" src="https://preview.redd.it/wwwlbq9ffzeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08d147840833d98030a378036e90b68b7a8dd2ff" title="1.8-3.3x faster Embedding finetuning now in Unsloth (~3GB VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLLaMA! We added embedding fine-tuning support in Unsloth! &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; trains embedding models &lt;strong&gt;1.8-3.3x faster with 20% less VRAM&lt;/strong&gt;, 2x longer context &amp;amp; no accuracy loss vs. FA2 setups. Most need only 3GB of VRAM for 4bit QLoRA. 6GB for 16bit LoRA.&lt;/p&gt; &lt;p&gt;Full finetuning, LoRA (16bit) and QLoRA (4bit) are all faster by default!&lt;/p&gt; &lt;p&gt;Fine-tuning embedding models can improve retrieval &amp;amp; RAG by aligning vectors to your domain-specific notion of similarity, improving search, clustering, and recommendations on your data.&lt;/p&gt; &lt;p&gt;Blog + Guide: &lt;a href="https://unsloth.ai/docs/new/embedding-finetuning"&gt;https://unsloth.ai/docs/new/embedding-finetuning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After finetuning, you can deploy your fine-tuned model anywhere: transformers, LangChain, Ollama, vLLM, llama.cpp&lt;/p&gt; &lt;p&gt;We'd like to thank Hugging Face and Unsloth contributor: electroglyph for making this possible!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Try the &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/EmbeddingGemma_(300M"&gt;EmbeddingGemma notebook&lt;/a&gt;.ipynb) in a free Colab T4 instance&lt;/li&gt; &lt;li&gt;We support ModernBERT, Qwen Embedding, Embedding Gemma, MiniLM-L6-v2, mpnet, BGE and all other models are supported automatically!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And code for doing EmbeddingGemma:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from unsloth import FastSentenceTransformer model = FastSentenceTransformer.from_pretrained( model_name = &amp;quot;unsloth/embeddinggemma-300m&amp;quot;, max_seq_length = 1024, # Choose any for long context! full_finetuning = False, # [NEW!] We have full finetuning now! ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Update Unsloth via &lt;code&gt;pip install --upgrade unsloth unsloth_zoo&lt;/code&gt; to get the latest updates. Thanks everyone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wwwlbq9ffzeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk9vmv/1833x_faster_embedding_finetuning_now_in_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qk9vmv/1833x_faster_embedding_finetuning_now_in_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T23:09:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qk68n8</id>
    <title>vLLM raising $150M confirms it: We have moved from the "Throughput Era" to the "Latency(Cold Starts)."</title>
    <updated>2026-01-22T20:45:42+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The news today that the team behind vLLM (Inferact) raised a $150M Seed Round at an $800M valuation is a massive signal for everyone in this space.&lt;/p&gt; &lt;p&gt;For the last two years, all the capital flowed into &lt;strong&gt;Training&lt;/strong&gt; (Foundation Models, massive clusters). This raise signals that the bottleneck has officially shifted to &lt;strong&gt;Serving&lt;/strong&gt; (Efficiency, Latency, Throughput).&lt;/p&gt; &lt;p&gt;It validates a few things we've been seeing in the open-source community:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Software &amp;gt; Hardware:&lt;/strong&gt; buying more H100s isn't enough anymore. You need the software stack (PagedAttention, specialized kernels) to actually utilize them. The &amp;quot;Software Tax&amp;quot; on inference is real.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &amp;quot;Standardization&amp;quot; Race:&lt;/strong&gt; vLLM is clearly aiming to be the &amp;quot;Linux of Inference&amp;quot;‚Äîthe default engine that runs on NVIDIA, AMD, and Intel. I wonder though, With this kind of war chest, do we think they go for &lt;strong&gt;Horizontal Compatibility&lt;/strong&gt; (making AMD/Intel usable) or &lt;strong&gt;Vertical Optimization&lt;/strong&gt; (squeezing more latency out of CUDA)?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Personally, I think &amp;quot;Throughput&amp;quot; (Batched tokens) is largely solved. The next massive hurdle is &lt;strong&gt;Latency&lt;/strong&gt; (Cold starts and Time-to-First-Token).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk68n8/vllm_raising_150m_confirms_it_we_have_moved_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk68n8/vllm_raising_150m_confirms_it_we_have_moved_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qk68n8/vllm_raising_150m_confirms_it_we_have_moved_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T20:45:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjtyw8</id>
    <title>Qwen dev on Twitter!!</title>
    <updated>2026-01-22T13:03:26+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/"&gt; &lt;img alt="Qwen dev on Twitter!!" src="https://preview.redd.it/avu4mhyvfweg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60312577cba6dc65c74da0313ab4d31252bd6be2" title="Qwen dev on Twitter!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/avu4mhyvfweg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T13:03:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjul5t</id>
    <title>Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp; 1.8B), Support for 10 languages</title>
    <updated>2026-01-22T13:31:16+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/"&gt; &lt;img alt="Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp;amp; 1.8B), Support for 10 languages" src="https://preview.redd.it/wo9tqflvkweg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75bf194547e68a1bb648f530175a2ec826899fd0" title="Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &amp;amp; 1.8B), Support for 10 languages" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Github: &lt;a href="https://github.com/QwenLM/Qwen3-TTS"&gt;https://github.com/QwenLM/Qwen3-TTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/collections/Qwen/qwen3-tts"&gt;https://huggingface.co/collections/Qwen/qwen3-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://qwen.ai/blog?id=qwen3tts-0115"&gt;https://qwen.ai/blog?id=qwen3tts-0115&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://github.com/QwenLM/Qwen3-TTS/blob/main/assets/Qwen3_TTS.pdf"&gt;https://github.com/QwenLM/Qwen3-TTS/blob/main/assets/Qwen3_TTS.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face Demo: &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-TTS"&gt;https://huggingface.co/spaces/Qwen/Qwen3-TTS&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wo9tqflvkweg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T13:31:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkimzg</id>
    <title>Nvidia Introduces PersonaPlex: An Open-Source, Real-Time Conversational AI Voice</title>
    <updated>2026-01-23T05:46:00+00:00</updated>
    <author>
      <name>/u/44th--Hokage</name>
      <uri>https://old.reddit.com/user/44th--Hokage</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkimzg/nvidia_introduces_personaplex_an_opensource/"&gt; &lt;img alt="Nvidia Introduces PersonaPlex: An Open-Source, Real-Time Conversational AI Voice" src="https://external-preview.redd.it/MTBpcnh0Y3RlMWZnMZIsTFZLbt9sVhZK1iJgvS1KPC08YlewNjml1NOE_YRL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6e21790c025d36b7ace41a33c90e43761f4b274" title="Nvidia Introduces PersonaPlex: An Open-Source, Real-Time Conversational AI Voice" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;PersonaPlex is a real-time, full-duplex speech-to-speech conversational model that enables persona control through text-based role prompts and audio-based voice conditioning. Trained on a combination of synthetic and real conversations, it produces natural, low-latency spoken interactions with a consistent persona. &lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Link to the Project Page with Demos: &lt;a href="https://research.nvidia.com/labs/adlr/personaplex/"&gt;https://research.nvidia.com/labs/adlr/personaplex/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;####Link to the Open-Sourced Code: &lt;a href="https://github.com/NVIDIA/personaplex"&gt;https://github.com/NVIDIA/personaplex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;####Link To Try Out PersonaPlex: &lt;a href="https://colab.research.google.com/#fileId=https://huggingface.co/nvidia/personaplex-7b-v1.ipynb"&gt;https://colab.research.google.com/#fileId=https://huggingface.co/nvidia/personaplex-7b-v1.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;####Link to the HuggingFace: &lt;a href="https://huggingface.co/nvidia/personaplex-7b-v1"&gt;https://huggingface.co/nvidia/personaplex-7b-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;####Link to the PersonaPlex Preprint: &lt;a href="https://research.nvidia.com/labs/adlr/files/personaplex/personaplex%5C_preprint.pdf"&gt;https://research.nvidia.com/labs/adlr/files/personaplex/personaplex\_preprint.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/44th--Hokage"&gt; /u/44th--Hokage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r8hfqlcte1fg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkimzg/nvidia_introduces_personaplex_an_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkimzg/nvidia_introduces_personaplex_an_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T05:46:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkghpk</id>
    <title>Quiet Threadripper AI Workstation - 768GB DDR5 and 160GB VRAM (RTX 5090 + 4x R9700)</title>
    <updated>2026-01-23T04:00:22+00:00</updated>
    <author>
      <name>/u/sloptimizer</name>
      <uri>https://old.reddit.com/user/sloptimizer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkghpk/quiet_threadripper_ai_workstation_768gb_ddr5_and/"&gt; &lt;img alt="Quiet Threadripper AI Workstation - 768GB DDR5 and 160GB VRAM (RTX 5090 + 4x R9700)" src="https://b.thumbs.redditmedia.com/OvcCfJivz4D4HOjUCQinn-sUra3cRaaS32dKiyRmuRM.jpg" title="Quiet Threadripper AI Workstation - 768GB DDR5 and 160GB VRAM (RTX 5090 + 4x R9700)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seeing all the quad R9700 builds inspired me to post mine!&lt;/p&gt; &lt;p&gt;I managed to squeeze in RTX 5090 and four R9700 into a workstation build by fitting some GPUs vertically in the front section. Two power supplies: 1600W for the main system and most of the components, and a smaller 850W power supply for 3 of the Radeons (the power cable is threaded through the system popping out through a small gap left by RTX 5090).&lt;/p&gt; &lt;p&gt;DeepSeek-V3.1-Terminus with context = 37279 tokens: PP = 151.76 tps, TG = 10.85 tps&lt;/p&gt; &lt;p&gt;Some things I discovered running local LLMs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For water-cooled CPU systems, there is not enough air circulation to cool the RAM! &lt;ul&gt; &lt;li&gt;Adding RAM fans got me a 30% performance boost with DeepSeek&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Turning off remote management on WRX90E-SAGE makes it boot much faster&lt;/li&gt; &lt;li&gt;You can combine Nvidia and AMD cards in llama.cpp by compiling with &lt;code&gt;-DGGML_BACKEND_DL=ON&lt;/code&gt;&lt;/li&gt; &lt;li&gt;No significant performance penalty running RTX 5090 at 400W, but much cooler and quieter &lt;ul&gt; &lt;li&gt;To fix, run: &lt;code&gt;sudo nvidia-smi -pl 400&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;R9700 has crazy auto-overclocking by default, draining power and making a lot of noise for little gain &lt;ul&gt; &lt;li&gt;To fix, run: &lt;code&gt;sudo amd-smi set --perf-level=HIGH&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Despite aggressive auto-overclocking, R9700's default mode is sub-optimal for MoE offloading (perf-level=HIGH fixes that as well)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Component List:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Motherboard - Pro WS WRX90E-SAGE SE&lt;/li&gt; &lt;li&gt;CPU - AMD Ryzen Threadripper PRO 7975WX&lt;/li&gt; &lt;li&gt;RAM - 8x KINGSTON 96GB DDR5 5600MHz CL46&lt;/li&gt; &lt;li&gt;GPU1 - ASUS TUF GeForce RTX 5090&lt;/li&gt; &lt;li&gt;GPU2 - 4x ASRock Creator Radeon AI Pro R9700&lt;/li&gt; &lt;li&gt;NVMe - 4x Samsung 9100 PRO 2TB&lt;/li&gt; &lt;li&gt;HDD - 2x Seagate Exos 16TB Enterprise&lt;/li&gt; &lt;li&gt;Power1 - Dark Power Pro 13 1600W 80+ Titanium&lt;/li&gt; &lt;li&gt;Power2 - Seasonic FOCUS V3 GX-850, 850W 80+ Gold&lt;/li&gt; &lt;li&gt;Case - Fractal Design Define 7 XL&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sloptimizer"&gt; /u/sloptimizer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qkghpk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkghpk/quiet_threadripper_ai_workstation_768gb_ddr5_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkghpk/quiet_threadripper_ai_workstation_768gb_ddr5_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T04:00:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkmn4l</id>
    <title>DeepSeek-V3.2 Matches GPT-5 at 10x Lower Cost | Introl Blog</title>
    <updated>2026-01-23T09:45:43+00:00</updated>
    <author>
      <name>/u/EchoOfOppenheimer</name>
      <uri>https://old.reddit.com/user/EchoOfOppenheimer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkmn4l/deepseekv32_matches_gpt5_at_10x_lower_cost_introl/"&gt; &lt;img alt="DeepSeek-V3.2 Matches GPT-5 at 10x Lower Cost | Introl Blog" src="https://external-preview.redd.it/egKcSDfb7_eR6zvCmQcIQCLzo1D62E9bPVJSY-qk8TU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e934f165af819bde727b9fabcde448d350ae932" title="DeepSeek-V3.2 Matches GPT-5 at 10x Lower Cost | Introl Blog" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek has released V3.2, an open-source model that reportedly matches GPT-5 on math reasoning while costing 10x less to run ($0.028/million tokens). By using a new 'Sparse Attention' architecture, the Chinese lab has achieved frontier-class performance for a total training cost of just ~$5.5 million‚Äîcompared to the $100M+ spent by US tech giants.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EchoOfOppenheimer"&gt; /u/EchoOfOppenheimer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://introl.com/blog/deepseek-v3-2-open-source-ai-cost-advantage"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkmn4l/deepseekv32_matches_gpt5_at_10x_lower_cost_introl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkmn4l/deepseekv32_matches_gpt5_at_10x_lower_cost_introl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T09:45:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkj9zh</id>
    <title>GLM4.7-Flash REAP @ 25% live on HF + agentic coding evals</title>
    <updated>2026-01-23T06:19:37+00:00</updated>
    <author>
      <name>/u/ilzrvch</name>
      <uri>https://old.reddit.com/user/ilzrvch</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkj9zh/glm47flash_reap_25_live_on_hf_agentic_coding_evals/"&gt; &lt;img alt="GLM4.7-Flash REAP @ 25% live on HF + agentic coding evals" src="https://b.thumbs.redditmedia.com/sU6QdQb5WuTlrLDYXYowgPhhH5wsTdVh6s0BAol-lqA.jpg" title="GLM4.7-Flash REAP @ 25% live on HF + agentic coding evals" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;We're releasing a 25% REAP'd version of GLM4.7-Flash: &lt;a href="http://hf.co/cerebras/GLM-4.7-Flash-REAP-23B-A3B"&gt;hf.co/cerebras/GLM-4.7-Flash-REAP-23B-A3B&lt;/a&gt;&lt;br /&gt; and MiniMax-M2.1 is in the works!&lt;/p&gt; &lt;p&gt;We've gotten a lot of feedback that REAP pruning affects creative writing / multi-lingual capabilities of the model - this is expected for our REAPs with calibration set curated for agentic coding.&lt;/p&gt; &lt;p&gt;We wanted to see how our REAPs are doing vs. other models of comparable size. We ran the mini-swe-agent flow on SWE-rebench leaderboard for October 2025 and found (see attached image) that GLM4.7 REAPs are a big jump over GLM4.6's and are in the Pareto frontier of agentic coding vs. model size efficiency. MiniMax-M2.1 is in between GLM4.7 REAPs @ 25% and 40%, so we think REAPs MiniMax-M2.1 will shine!&lt;/p&gt; &lt;p&gt;Additionally, based on your feedback, we're considering to drop experimental REAPs for creative writing. Do let us know which datasets and evals we should explore for this. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pw1zn8zsk1fg1.png?width=2700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57bacd1248548a329fca9aecaa81b4cc1a8c3c44"&gt;https://preview.redd.it/pw1zn8zsk1fg1.png?width=2700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57bacd1248548a329fca9aecaa81b4cc1a8c3c44&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilzrvch"&gt; /u/ilzrvch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkj9zh/glm47flash_reap_25_live_on_hf_agentic_coding_evals/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkj9zh/glm47flash_reap_25_live_on_hf_agentic_coding_evals/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkj9zh/glm47flash_reap_25_live_on_hf_agentic_coding_evals/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T06:19:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkm9zb</id>
    <title>Llama.cpp merges in OpenAI Responses API Support</title>
    <updated>2026-01-23T09:22:40+00:00</updated>
    <author>
      <name>/u/SemaMod</name>
      <uri>https://old.reddit.com/user/SemaMod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkm9zb/llamacpp_merges_in_openai_responses_api_support/"&gt; &lt;img alt="Llama.cpp merges in OpenAI Responses API Support" src="https://external-preview.redd.it/jaCRAxUnJ2FTFmnP1XEivypPWJS55V8E63eMDNFL6mg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61f06c5517f67a03da7c9a1bf80c4717a8acae9f" title="Llama.cpp merges in OpenAI Responses API Support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally! Took some fussing around to get this to work with unsloth/GLM-4.7-Flash:UD-Q4_K_XL in llama.cpp (ROCm) and Codex CLI, but once set up it works great! I'm super impressed with GLM-4.7-Flash capability in the Codex CLI harness. Haven't tried any big feature implementations yet, but for exploring (large) codebases it has been surprisingly effective&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SemaMod"&gt; /u/SemaMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18486"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkm9zb/llamacpp_merges_in_openai_responses_api_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkm9zb/llamacpp_merges_in_openai_responses_api_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T09:22:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qk8zj1</id>
    <title>Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing?</title>
    <updated>2026-01-22T22:31:28+00:00</updated>
    <author>
      <name>/u/Empty_Enthusiasm_167</name>
      <uri>https://old.reddit.com/user/Empty_Enthusiasm_167</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lately I go on Reddit and I keep seeing the same idea repeated over and over again. Another chat app, another assistant, another ‚ÄúAI tool‚Äù that, in reality, already exists ‚Äî or worse, already exists in a better and more polished form.&lt;/p&gt; &lt;p&gt;Many of these are applications that could be solved perfectly with an extension, a plugin, or a simple feature inside an app we already use. I‚Äôm not saying AI is bad ‚Äî quite the opposite, it‚Äôs incredible. But there are people pouring all their money into Anthropic subscriptions or increasing their electricity bill just to build a less polished version of things like OpenWebUI, Open Code, Cline, etc&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Empty_Enthusiasm_167"&gt; /u/Empty_Enthusiasm_167 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T22:31:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkiylw</id>
    <title>OpenAI CFO hinting at "Outcome-Based Pricing" (aka royalties on your work)? Makes the case for local even stronger.</title>
    <updated>2026-01-23T06:02:31+00:00</updated>
    <author>
      <name>/u/distalx</name>
      <uri>https://old.reddit.com/user/distalx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw some screenshots floating around about OpenAI planning to &amp;quot;take a cut&amp;quot; of customer discoveries (like pharma drugs, etc).&lt;/p&gt; &lt;p&gt;I tried to dig up the primary source to see if it‚Äôs just clickbait. The closest official thing is a recent blog post from their CFO Sarah Friar talking about &amp;quot;outcome-based pricing&amp;quot; and &amp;quot;sharing in the value created&amp;quot; for high-value industries.&lt;/p&gt; &lt;p&gt;Even if the &amp;quot;royalty&amp;quot; headlines are sensationalized by tech media, the direction is pretty clear. They are signaling a shift from &amp;quot;paying for electricity&amp;quot; (tokens) to &amp;quot;taxing the factory output&amp;quot; (value).&lt;/p&gt; &lt;p&gt;It kind of reminds me of the whole Grid vs. Solar debate. relying on the Grid (Cloud APIs) is cheap and powerful, but you don't control the terms. If they decide your specific use case is &amp;quot;high value&amp;quot; and want a percentage, you're locked in.&lt;/p&gt; &lt;p&gt;Building a local stack is like installing solar/batteries. Expensive upfront, pain in the ass to maintain, but at least nobody knocks on your door asking for 5% of your project revenue just because you used their weights to run the math.&lt;/p&gt; &lt;p&gt;Link to article: &lt;a href="https://www.gizmochina.com/2026/01/21/openai-wants-a-cut-of-your-profits-inside-its-new-royalty-based-plan-and-other-business-models/"&gt;https://www.gizmochina.com/2026/01/21/openai-wants-a-cut-of-your-profits-inside-its-new-royalty-based-plan-and-other-business-models/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/distalx"&gt; /u/distalx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkiylw/openai_cfo_hinting_at_outcomebased_pricing_aka/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkiylw/openai_cfo_hinting_at_outcomebased_pricing_aka/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkiylw/openai_cfo_hinting_at_outcomebased_pricing_aka/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T06:02:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
