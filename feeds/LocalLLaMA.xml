<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-09T02:53:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nbq3y7</id>
    <title>New research preprint: Evolving Transformers with NEMoE</title>
    <updated>2025-09-08T15:01:47+00:00</updated>
    <author>
      <name>/u/Desperate_Contact102</name>
      <uri>https://old.reddit.com/user/Desperate_Contact102</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I just uploaded a new research preprint called NEMoE (Neuro-Evolutionary Mixture of Experts Transformer).&lt;/p&gt; &lt;p&gt;Instead of using a standard Transformer with fixed experts, NEMoE applies ideas from evolutionary algorithms (mutation, crossover, selection) to improve how experts are chosen and combined.&lt;/p&gt; &lt;p&gt;üîπ Early results show:&lt;/p&gt; &lt;p&gt;Lower perplexity (better language modeling performance)&lt;/p&gt; &lt;p&gt;More stable training compared to Switch Transformer&lt;/p&gt; &lt;p&gt;Better use of experts without adding compute cost&lt;/p&gt; &lt;p&gt;Here‚Äôs the preprint (open access on Zenodo): üëâ &lt;a href="https://doi.org/10.5281/zenodo.17073715"&gt;https://doi.org/10.5281/zenodo.17073715&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Contact102"&gt; /u/Desperate_Contact102 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbq3y7/new_research_preprint_evolving_transformers_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbq3y7/new_research_preprint_evolving_transformers_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbq3y7/new_research_preprint_evolving_transformers_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T15:01:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc6rpj</id>
    <title>I tried out the newly released Qwen3-ASR</title>
    <updated>2025-09-09T02:06:17+00:00</updated>
    <author>
      <name>/u/Timely_Rain_9284</name>
      <uri>https://old.reddit.com/user/Timely_Rain_9284</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc6rpj/i_tried_out_the_newly_released_qwen3asr/"&gt; &lt;img alt="I tried out the newly released Qwen3-ASR" src="https://external-preview.redd.it/PFxugutkAk8L3sZMz44hO5HCsrR-7_5sdJKWlm59VAo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6edf3b78cfd9e9c31dc183d4fb6dba44f8049470" title="I tried out the newly released Qwen3-ASR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-ASR is built upon Qwen3's multimodal dataset and tens of millions of hours of ASR training data. It currently delivers speech recognition performance, supporting 11 languages and various accents.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Supported Languages:&lt;/strong&gt;&lt;br /&gt; The Qwen3-ASR single model accurately transcribes multiple languages, dialects, and accents (including English and Chinese): English, Chinese, French, German, Russian, Italian, Spanish, Portuguese, Japanese, Korean, and Arabic.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Singing Recognition Support: Capable of recognizing a cappella and full songs with background music, with an actual measured error rate below 8%.&lt;/li&gt; &lt;li&gt;Customized Recognition: Users can provide contextual text in any format (e.g., word lists, paragraphs, or full documents). The model intelligently leverages this context to recognize and match named entities and key terms, delivering tailored transcription results.&lt;/li&gt; &lt;li&gt;Language Identification &amp;amp; Non-Speech Filtering: The model accurately identifies speech languages and automatically filters out non-speech segments, including silence and background noise.&lt;/li&gt; &lt;li&gt;Robust Performance: Maintains high accuracy even with challenging text patterns such as long complex sentences, mid-sentence language switching, repetitive phrases, and in acoustically complex environments.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;But here's the catch for our community, IT'S API-ONLY right now. No local weights available yet, which is a bummer for offline use.I tested with a 1.5-minute audio clip of the song &amp;quot;My All&amp;quot; (3 minutes was too long for the API), and the accuracy was impressive. What do you think? worth waiting for a local version? Actually open-source models like Whisper and Voxtral are quite mature now,though I haven't done a detailed comparison myself. Would love to hear about your opinion.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-ASR-Demo"&gt;https://huggingface.co/spaces/Qwen/Qwen3-ASR-Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4rn7ft57r1of1.png?width=2620&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f8d9528736e05650500390a350c855c25bcc3d4e"&gt;https://preview.redd.it/4rn7ft57r1of1.png?width=2620&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f8d9528736e05650500390a350c855c25bcc3d4e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7asd7b7ar1of1.jpg?width=1256&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=703d37f1c5d91aaf5ecac2a3b5ce717ce91bd4c8"&gt;https://preview.redd.it/7asd7b7ar1of1.jpg?width=1256&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=703d37f1c5d91aaf5ecac2a3b5ce717ce91bd4c8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/w9bm2ozar1of1.jpg?width=1256&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fa956f80b4d1b81cf4159eb41093cac38d6b4c40"&gt;https://preview.redd.it/w9bm2ozar1of1.jpg?width=1256&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fa956f80b4d1b81cf4159eb41093cac38d6b4c40&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Timely_Rain_9284"&gt; /u/Timely_Rain_9284 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc6rpj/i_tried_out_the_newly_released_qwen3asr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc6rpj/i_tried_out_the_newly_released_qwen3asr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc6rpj/i_tried_out_the_newly_released_qwen3asr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T02:06:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc3hc3</id>
    <title>Local LLM and Home Assistant</title>
    <updated>2025-09-08T23:35:30+00:00</updated>
    <author>
      <name>/u/j0ker31m</name>
      <uri>https://old.reddit.com/user/j0ker31m</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ive been a google home user since the day they launched. However, ive also been watching home Assistant grow over the years. Im wanting to switch to HA with a local LLM, and storing all of my wireless security camera footage all running on the same box.&lt;/p&gt; &lt;p&gt;I dont know what size of an llm is considered too small, or considered overkill, so its hard for me to determine what kind of hardware im going to need. Do you guys have any suggestion on what the simplest and cheapest method would be without having to sacrifice this for that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/j0ker31m"&gt; /u/j0ker31m &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc3hc3/local_llm_and_home_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc3hc3/local_llm_and_home_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc3hc3/local_llm_and_home_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T23:35:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc74g8</id>
    <title>Open Source Alternative to NotebookLM</title>
    <updated>2025-09-09T02:23:33+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt; &lt;li&gt;Hierarchical Indices (2-tiered RAG setup)&lt;/li&gt; &lt;li&gt;Combines Semantic + Full-Text Search with Reciprocal Rank Fusion (Hybrid Search)&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Podcasts&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Support for local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Blazingly fast podcast generation agent (3-minute podcast in under 20 seconds)&lt;/li&gt; &lt;li&gt;Convert chat conversations into engaging audio&lt;/li&gt; &lt;li&gt;Multiple TTS providers supported&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;External Sources Integration&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search Engines (Tavily, LinkUp)&lt;/li&gt; &lt;li&gt;Slack&lt;/li&gt; &lt;li&gt;Linear&lt;/li&gt; &lt;li&gt;Jira&lt;/li&gt; &lt;li&gt;ClickUp&lt;/li&gt; &lt;li&gt;Gmail&lt;/li&gt; &lt;li&gt;Confluence&lt;/li&gt; &lt;li&gt;Notion&lt;/li&gt; &lt;li&gt;Youtube Videos&lt;/li&gt; &lt;li&gt;GitHub&lt;/li&gt; &lt;li&gt;Discord&lt;/li&gt; &lt;li&gt;Airtable&lt;/li&gt; &lt;li&gt;Google Calandar&lt;/li&gt; &lt;li&gt;and more to come.....&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Cross-Browser Extension&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The SurfSense extension lets you save any dynamic webpage you want, including authenticated content.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc74g8/open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc74g8/open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc74g8/open_source_alternative_to_notebooklm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T02:23:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbqb3o</id>
    <title>Will you use this new Qwen3-ASR?</title>
    <updated>2025-09-08T15:09:21+00:00</updated>
    <author>
      <name>/u/LuozhuZhang</name>
      <uri>https://old.reddit.com/user/LuozhuZhang</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqb3o/will_you_use_this_new_qwen3asr/"&gt; &lt;img alt="Will you use this new Qwen3-ASR?" src="https://preview.redd.it/1ytcylkfiynf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c83369e0e599a5317d04c7536881ab848342f5d" title="Will you use this new Qwen3-ASR?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Supporting 11 languages &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LuozhuZhang"&gt; /u/LuozhuZhang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1ytcylkfiynf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqb3o/will_you_use_this_new_qwen3asr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqb3o/will_you_use_this_new_qwen3asr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T15:09:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbqa34</id>
    <title>üöÄ What model should we build next? YOU DECIDE! üöÄ</title>
    <updated>2025-09-08T15:08:12+00:00</updated>
    <author>
      <name>/u/GuiltyBookkeeper4849</name>
      <uri>https://old.reddit.com/user/GuiltyBookkeeper4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLLaMA!&lt;/p&gt; &lt;p&gt;After the amazing support we received in our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1n3xxm5/introducing_art08b_reasoning_the_way_you_want_it/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;last post&lt;/a&gt; with Art-0-8B, we're ready to tackle our next project and want YOU to decide what it should be! (Art-1 8B and 20B versions are coming soon btw)&lt;/p&gt; &lt;p&gt;For those who missed it, we're AGI-0 Labs - a decentralized research lab building open-source AGI through democratic community input. Our mission is simple: create AI that belongs to everyone, developed openly and guided by the community. Check us out at &lt;a href="http://AGI-0.com"&gt;AGI-0.com&lt;/a&gt; if you want to learn more about our approach.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here's how this works:&lt;/strong&gt; The most upvoted comment below describing a model idea will be our next development target. Whether it's a specialized fine-tune, a novel architecture experiment, or something completely wild - if the community wants it, we'll build it.&lt;/p&gt; &lt;p&gt;We're also open to collaborating with any sponsors who'd like to help us get more compute resources - feel free to reach out if you're interested in supporting open-source AI development!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Drop your model ideas below and let's see what the community wants most! The highest upvoted suggestion gets built. üó≥Ô∏è&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Looking forward to seeing what creative ideas you all come up with!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuiltyBookkeeper4849"&gt; /u/GuiltyBookkeeper4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa34/what_model_should_we_build_next_you_decide/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa34/what_model_should_we_build_next_you_decide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa34/what_model_should_we_build_next_you_decide/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T15:08:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbmz92</id>
    <title>NotebookLM is amazing - how can I replicate it locally and keep data private?</title>
    <updated>2025-09-08T12:55:27+00:00</updated>
    <author>
      <name>/u/Hot-Independence-197</name>
      <uri>https://old.reddit.com/user/Hot-Independence-197</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like how &lt;strong&gt;NotebookLM&lt;/strong&gt; works - I just upload a file, ask any question, and it provides high-quality answers. How could one build a similar system locally? Would this be considered a RAG (Retrieval-Augmented Generation) pipeline, or something else? Could you recommend good &lt;strong&gt;open-source&lt;/strong&gt; versions that can be run locally, while keeping data secure and private?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hot-Independence-197"&gt; /u/Hot-Independence-197 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmz92/notebooklm_is_amazing_how_can_i_replicate_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmz92/notebooklm_is_amazing_how_can_i_replicate_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmz92/notebooklm_is_amazing_how_can_i_replicate_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T12:55:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc0dgg</id>
    <title>ROCm 7.0.0 nightly based apps for Ryzen AI - unsloth, bitsandbytes and llama-cpp</title>
    <updated>2025-09-08T21:25:34+00:00</updated>
    <author>
      <name>/u/Recent-Success-1520</name>
      <uri>https://old.reddit.com/user/Recent-Success-1520</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc0dgg/rocm_700_nightly_based_apps_for_ryzen_ai_unsloth/"&gt; &lt;img alt="ROCm 7.0.0 nightly based apps for Ryzen AI - unsloth, bitsandbytes and llama-cpp" src="https://external-preview.redd.it/HwAJGIWkuuQRHBpEXp2R4CbrQfzKoASLgWeZFT1sFIQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=131ad86b79c662a9208b9b395f33e7b7dfcbb355" title="ROCm 7.0.0 nightly based apps for Ryzen AI - unsloth, bitsandbytes and llama-cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HI all,&lt;/p&gt; &lt;p&gt;A few days ago I posted if anyone had any fine tuning working on Strix Halo and many people like me were looking.&lt;br /&gt; I have got a working setup now that allows me to use ROCm based fine tuining and inferencing.&lt;/p&gt; &lt;p&gt;For now the following tools are working with latest ROCm 7.0.0 nightly and available in my repo (linked). From the limited testing unsloth seems to be working and llama-cpp inference is working too.&lt;/p&gt; &lt;p&gt;This is initial setup and I will keep adding more tools all ROCm compiled.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# make help Available targets: all: Installs everything bitsandbytes: Install bitsandbytes from source flash-attn: Install flash-attn from source help: Prints all available targets install-packages: Installs required packages llama-cpp: Installs llama.cpp from source pytorch: Installs torch torchvision torchaudio pytorch-triton-rcom from ROCm nightly rocWMMA: Installs rocWMMA library from source theRock: Installs ROCm in /opt/rocm from theRock Nightly unsloth: Installs unsloth from source &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Sample bench&lt;/p&gt; &lt;p&gt;&lt;code&gt;root@a7aca9cd63bc:/strix-rocm-all# llama-bench -m ~/.cache/llama.cpp/ggml-org_gpt-oss-120b-GGUF_gpt-oss-120b-mxfp4-00001-of-00003.gguf -ngl 999 -mmp 0 -fa 0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ggml_cuda_init: found 1 ROCm devices:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Device 0: AMD Radeon Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | mmap | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm | 999 | 0 | pp512 | 698.26 ¬± 7.31 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm | 999 | 0 | tg128 | 46.20 ¬± 0.47 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Got mixed up with &lt;a href="/r/LocalLLM"&gt;r/LocalLLM&lt;/a&gt; so posting here too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recent-Success-1520"&gt; /u/Recent-Success-1520 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/shantur/strix-rocm-all"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc0dgg/rocm_700_nightly_based_apps_for_ryzen_ai_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc0dgg/rocm_700_nightly_based_apps_for_ryzen_ai_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T21:25:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbo0bz</id>
    <title>KittenML released a mini version (80M) of their text to speech model.</title>
    <updated>2025-09-08T13:39:19+00:00</updated>
    <author>
      <name>/u/Yorn2</name>
      <uri>https://old.reddit.com/user/Yorn2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo0bz/kittenml_released_a_mini_version_80m_of_their/"&gt; &lt;img alt="KittenML released a mini version (80M) of their text to speech model." src="https://external-preview.redd.it/6tEU3HFyV9wrAIlbgWYDqTicViQ2PFk-H0trsfrB-TE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ae1cdc19684b4f8a1d7922e2097495effc92e03" title="KittenML released a mini version (80M) of their text to speech model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Yorn2"&gt; /u/Yorn2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/KittenML/kitten-tts-mini-0.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo0bz/kittenml_released_a_mini_version_80m_of_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo0bz/kittenml_released_a_mini_version_80m_of_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T13:39:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbt82m</id>
    <title>Drummer's Valkyrie 49B v2 - A finetune of Nemotron Super 49B v1.5, a pack puncher.</title>
    <updated>2025-09-08T16:58:12+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbt82m/drummers_valkyrie_49b_v2_a_finetune_of_nemotron/"&gt; &lt;img alt="Drummer's Valkyrie 49B v2 - A finetune of Nemotron Super 49B v1.5, a pack puncher." src="https://external-preview.redd.it/G0pBb0RCd46QXI7ZBwlDv7ScyXXHaae0jNeNWtdfkbk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6496fcee3c3c0005184df9b3cbe9dceaa06a0c4a" title="Drummer's Valkyrie 49B v2 - A finetune of Nemotron Super 49B v1.5, a pack puncher." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Also updated my FAQ. Preparing a release on a Largestral 2407 and Small 22B tune too! (If anyone's interested, they're a bit smarter with the 'modern' tuning.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Valkyrie-49B-v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbt82m/drummers_valkyrie_49b_v2_a_finetune_of_nemotron/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbt82m/drummers_valkyrie_49b_v2_a_finetune_of_nemotron/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T16:58:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbi95c</id>
    <title>Tilde AI Releases TildeOpen LLM: An Open-Source Large Language Model with Over 30 Billion Parameters and Support Most European Languages</title>
    <updated>2025-09-08T08:32:01+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbi95c/tilde_ai_releases_tildeopen_llm_an_opensource/"&gt; &lt;img alt="Tilde AI Releases TildeOpen LLM: An Open-Source Large Language Model with Over 30 Billion Parameters and Support Most European Languages" src="https://external-preview.redd.it/aoPAPmODv59RqOF8q1zUghKheD5cO88KxVLhosHPVZE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d56ba07b04f6a26463fb99f2d29054bf135f506a" title="Tilde AI Releases TildeOpen LLM: An Open-Source Large Language Model with Over 30 Billion Parameters and Support Most European Languages" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TildeOpen LLM is an open-source foundational language model built to serve underrepresented Nordic and Eastern European languages. Developed with European Commission funding and trained on the LUMI supercomputer, this 30B+ parameter model addresses the performance gaps that speakers of 19 focus languages‚Äîrepresenting over 165 million people‚Äîface with existing AI systems.&lt;/p&gt; &lt;p&gt;The model employs an equitable tokeniser and curriculum-learning approach to ensure fair representation across less-resourced languages, moving beyond the typical English-centric design of most language models. As an open-source project, TildeOpen LLM enables transparent research and community-driven development while maintaining European technological independence.&lt;/p&gt; &lt;p&gt;This foundational model is not yet adapted to follow instructions or aligned with safety features. The next version being built on top of this model will be a specialised translation model, leveraging TildeOpen LLM's multilingual foundation to provide high-quality translation capabilities across the supported European language pairs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Languages:&lt;/strong&gt; Albanian, Bosnian, Bulgarian, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Hungarian, Icelandic, Irish, Italian, Latgalian, Latvian, Lithuanian, Macedonian, Maltese, Montenegrin, Norwegian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovene, Spanish, Swedish, Turkish, Ukrainian as well of mathematical proofs, programming code and XML documents containing translation data&lt;/p&gt; &lt;p&gt;GGUF:&lt;br /&gt; &lt;a href="https://huggingface.co/mradermacher/TildeOpen-30b-GGUF"&gt;https://huggingface.co/mradermacher/TildeOpen-30b-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TildeAI/TildeOpen-30b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbi95c/tilde_ai_releases_tildeopen_llm_an_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbi95c/tilde_ai_releases_tildeopen_llm_an_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T08:32:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbly7o</id>
    <title>MiniCPM4.1-8B</title>
    <updated>2025-09-08T12:08:09+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model: &lt;a href="https://huggingface.co/openbmb/MiniCPM4.1-8B"&gt;https://huggingface.co/openbmb/MiniCPM4.1-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;8B hybrid reasoning model (/think vs /no_think)&lt;/li&gt; &lt;li&gt;InfLLM v2 sparse attention, natively supports 65K, RoPE scaling validated to 131K&lt;/li&gt; &lt;li&gt;BitCPM ternary quantization, FP8 and multi-token prediction&lt;/li&gt; &lt;li&gt;Eagle3 speculative decoding integrated in vLLM, SGLang, and CPM .cu with up to 3x faster reasoning&lt;/li&gt; &lt;li&gt;On Jetson Orin achieves approximately 7x faster decoding compared to Qwen3-8B and 3x reasoning speedup over MiniCPM4&lt;/li&gt; &lt;li&gt;Available in GPTQ, AutoAWQ, Marlin, GGUF, MLX, and Eagle3 draft variants&lt;/li&gt; &lt;li&gt;Apache 2.0&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbly7o/minicpm418b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbly7o/minicpm418b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbly7o/minicpm418b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T12:08:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbfy60</id>
    <title>Finishing touches on dual RTX 6000 build</title>
    <updated>2025-09-08T06:04:43+00:00</updated>
    <author>
      <name>/u/ikkiyikki</name>
      <uri>https://old.reddit.com/user/ikkiyikki</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfy60/finishing_touches_on_dual_rtx_6000_build/"&gt; &lt;img alt="Finishing touches on dual RTX 6000 build" src="https://preview.redd.it/sez83piasvnf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=599c86ee050439c49f07c7056ff67e6c48ef8381" title="Finishing touches on dual RTX 6000 build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a dream build: 192 gigs of fast VRAM (and another 128 of RAM) but worried I'll burn the house down because of the 15A breakers.&lt;/p&gt; &lt;p&gt;Downloading Qwen 235B q4 :-)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ikkiyikki"&gt; /u/ikkiyikki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sez83piasvnf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfy60/finishing_touches_on_dual_rtx_6000_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfy60/finishing_touches_on_dual_rtx_6000_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T06:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbslxu</id>
    <title>native tool calling support for DeepSeek V3.1 just merged in llama.cpp</title>
    <updated>2025-09-08T16:35:22+00:00</updated>
    <author>
      <name>/u/createthiscom</name>
      <uri>https://old.reddit.com/user/createthiscom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbslxu/native_tool_calling_support_for_deepseek_v31_just/"&gt; &lt;img alt="native tool calling support for DeepSeek V3.1 just merged in llama.cpp" src="https://external-preview.redd.it/AaDrVOkhJbB5T7DYbjublcje7S3TXjWZXxoeBvGkbYY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bdc3f7c6f3e61642c4575d3c207d062eb6dd4b4" title="native tool calling support for DeepSeek V3.1 just merged in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I doubt many people are using it, but just FYI: native tool calling support (OpenAI style JSON request/response) for DeepSeek V3.1 was just merged into llama.cpp. To use, I think you have to start the server with `--jinja` and unset `--response_format`, or set it to `auto`. I personally use this feature quite a bit with Open Hands AI via docker with `-e LLM_NATIVE_TOOL_CALLING=true`, but you'll have to check your documentation to see if it is supported and how to enable it if you use a different client. Benefits include reduced context length and possibly better agentic reliability (time will tell).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/createthiscom"&gt; /u/createthiscom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15533"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbslxu/native_tool_calling_support_for_deepseek_v31_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbslxu/native_tool_calling_support_for_deepseek_v31_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T16:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc5k36</id>
    <title>ParaThinker: Native Parallel Thinking as a New Paradigm to Scale LLM Test-time Compute</title>
    <updated>2025-09-09T01:09:13+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Recent advances in Large Language Models (LLMs) have been driven by test-time compute scaling - a strategy that improves reasoning by generating longer, sequential thought processes. While effective, this approach encounters a significant bottleneck as computation increases, where further computation offers only marginal performance gains. We argue this ceiling is not an inherent limit of the model's capability but a flaw in the scaling strategy itself, a phenomenon we term &amp;quot;Tunnel Vision&amp;quot;, where a model's imperfect initial steps lock it into a suboptimal reasoning path. To overcome this, we introduce a new scaling paradigm: native thought parallelism. We present ParaThinker, an end-to-end framework that trains an LLM to generate multiple, diverse reasoning paths in parallel and synthesize them into a superior final answer. By exploring different lines of thoughts simultaneously, ParaThinker effectively sidesteps the Tunnel Vision issue and unlocks the model's latent reasoning potential. Our approach demonstrates that scaling compute in parallel (width) is a more effective and efficient way to superior reasoning than simply scaling sequentially (depth). On challenging reasoning benchmarks, ParaThinker achieves substantial accuracy improvements over sequential LLMs (12.3% for 1.5B and 7.5% for 7B models on average with 8 parallel paths), while adding only negligible latency overhead (7.1%). This enables smaller models to surpass much larger counterparts and establishes parallel thinking as a critical, efficient dimension for scaling future LLMs.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2509.04475"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc5k36/parathinker_native_parallel_thinking_as_a_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc5k36/parathinker_native_parallel_thinking_as_a_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T01:09:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbgosx</id>
    <title>Apocalyptic scenario: If you could download only one LLM before the internet goes down, which one would it be?</title>
    <updated>2025-09-08T06:50:43+00:00</updated>
    <author>
      <name>/u/sado361</name>
      <uri>https://old.reddit.com/user/sado361</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, a thought crossed my mind and I've been thinking about it for a few days. Let's say we have an apocalyptic scenario, like a zombie apocalypse. You have a Mac Studio with an M3 chip and 512 GB of RAM (it uses little power and can run large models). If such an apocalypse happened today, which local LLM would you download before the internet disappears? You only have a chance to download one. Electricity is not a problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sado361"&gt; /u/sado361 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgosx/apocalyptic_scenario_if_you_could_download_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgosx/apocalyptic_scenario_if_you_could_download_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgosx/apocalyptic_scenario_if_you_could_download_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T06:50:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc57zo</id>
    <title>Confusion about VRAM</title>
    <updated>2025-09-09T00:53:34+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I understand that having more GPU‚Äôs is good for inference, but if I remember from the days of SLI and Crossfire, the VRAM doesn‚Äôt stack. So why is it I see some people say that two 20GB cards are going to give them 40GB of VRAM. When I swear VRAM doesn‚Äôt work like that. Am I wrong or not?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc57zo/confusion_about_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc57zo/confusion_about_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc57zo/confusion_about_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T00:53:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc2w7h</id>
    <title>5090 vs 6000</title>
    <updated>2025-09-08T23:10:04+00:00</updated>
    <author>
      <name>/u/That-Thanks3889</name>
      <uri>https://old.reddit.com/user/That-Thanks3889</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;a student asked me which rig for learning and training models - i recommended the 6000 but with new hardware every month i'm taking it back ... wondering everyone else's opinion ? 5090 seems sufficient to learn and fine tune mistral etc ... and once they proficient they can rent cloud or spend money &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/That-Thanks3889"&gt; /u/That-Thanks3889 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc2w7h/5090_vs_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc2w7h/5090_vs_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc2w7h/5090_vs_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T23:10:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbkxnm</id>
    <title>Introducing IndexTTS-2.0: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech</title>
    <updated>2025-09-08T11:16:22+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are thrilled to announce the official open-sourcing of IndexTTS-2.0 - an emotionally rich and duration-controllable autoregressive zero-shot text-to-speech system. &lt;/p&gt; &lt;p&gt;- We innovatively propose a &amp;quot;time encoding&amp;quot; mechanism applicable to autoregressive systems, solving for the first time the challenge of precise speech duration control in traditional autoregressive models. &lt;/p&gt; &lt;p&gt;- The system also introduces a timbre-emotion decoupling modeling mechanism, offering diverse and flexible emotional control methods. Beyond single-audio reference, it enables precise adjustment of synthesized speech's emotional expression through standalone emotional reference audio, emotion vectors, or text descriptions, significantly enhancing the expressiveness and adaptability of generated speech. &lt;/p&gt; &lt;p&gt;The architecture of IndexTTS-2.0 makes it widely suitable for various creative and application scenarios, including but not limited to: AI voiceovers, audiobooks, dynamic comics, video translation, voice dialogues, podcasts, and more. We believe this system marks a crucial milestone in advancing zero-shot TTS technology toward practical applications. &lt;/p&gt; &lt;p&gt;Currently, the project paper, full code, model weights, and online demo page are all open-sourced. We warmly invite developers, researchers, and content creators to explore and provide valuable feedback. In the future, we will continue optimizing model performance and gradually release more resources and tools, looking forward to collaborating with the developer community to build an open and thriving technology ecosystem. &lt;/p&gt; &lt;p&gt;üëâ Repository: &lt;a href="https://github.com/index-tts/index-tts"&gt;https://github.com/index-tts/index-tts&lt;/a&gt; &lt;/p&gt; &lt;p&gt;üëâ Paper: &lt;a href="https://arxiv.org/abs/2506.21619"&gt;https://arxiv.org/abs/2506.21619&lt;/a&gt; &lt;/p&gt; &lt;p&gt;üëâ Demo: &lt;a href="https://index-tts.github.io/index-tts2.github.io/"&gt;https://index-tts.github.io/index-tts2.github.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbkxnm/introducing_indextts20_a_breakthrough_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbkxnm/introducing_indextts20_a_breakthrough_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbkxnm/introducing_indextts20_a_breakthrough_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T11:16:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbyw3b</id>
    <title>3090 is it still a good buy?</title>
    <updated>2025-09-08T20:28:10+00:00</updated>
    <author>
      <name>/u/Ideabile</name>
      <uri>https://old.reddit.com/user/Ideabile</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got the opportunity to buy 2 Nvidia 3090 RTX 24GB for 600‚Ç¨ each.&lt;/p&gt; &lt;p&gt;I want to be run a bunch of llm workflows: this to self host some Claude code and to automate some burocracies I got.&lt;/p&gt; &lt;p&gt;Additionally I want to step up in the llm experimental path, so I can learn more about it and have the ML skill set.&lt;/p&gt; &lt;p&gt;Currently other video cards seems much more expensive I hardly believe it will ever get cheaper.&lt;/p&gt; &lt;p&gt;I saw some people recommending 2 x 3090 which would make 48gb of vram.&lt;/p&gt; &lt;p&gt;Is there any other budget friendly alternatives? Is this a good lasting investment?&lt;/p&gt; &lt;p&gt;Thank you in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ideabile"&gt; /u/Ideabile &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbyw3b/3090_is_it_still_a_good_buy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbyw3b/3090_is_it_still_a_good_buy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbyw3b/3090_is_it_still_a_good_buy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T20:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbqa1p</id>
    <title>Qwen released API (only) Qwen3-ASR ‚Äî the all-in-one speech recognition model!</title>
    <updated>2025-09-08T15:08:10+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa1p/qwen_released_api_only_qwen3asr_the_allinone/"&gt; &lt;img alt="Qwen released API (only) Qwen3-ASR ‚Äî the all-in-one speech recognition model!" src="https://preview.redd.it/et1syg58iynf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ab48cb0e9692e8d94764a7f031fd34d0db1ae95" title="Qwen released API (only) Qwen3-ASR ‚Äî the all-in-one speech recognition model!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üéôÔ∏è Meet Qwen3-ASR ‚Äî the all-in-one speech recognition model!&lt;/p&gt; &lt;p&gt;‚úÖ High-accuracy EN/CN + 9 more languages: ar, de, en, es, fr, it, ja, ko, pt, ru, zh&lt;/p&gt; &lt;p&gt;‚úÖ Auto language detection&lt;/p&gt; &lt;p&gt;‚úÖ Songs? Raps? Voice with BGM? No problem. &amp;lt;8% WER&lt;/p&gt; &lt;p&gt;‚úÖ Works in noise, low quality, far-field&lt;/p&gt; &lt;p&gt;‚úÖ Custom context? Just paste ANY text ‚Äî names, jargon, even gibberish üß†&lt;/p&gt; &lt;p&gt;‚úÖ One model. Zero hassle.Great for edtech, media, customer service &amp;amp; more.&lt;/p&gt; &lt;p&gt;API: &lt;a href="https://bailian.console.alibabacloud.com/?tab=doc#/doc/?type=model&amp;amp;url=2979031"&gt;https://bailian.console.alibabacloud.com/?tab=doc#/doc/?type=model&amp;amp;url=2979031&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Modelscope Demo: &lt;a href="https://modelscope.cn/studios/Qwen/Qwen3-ASR-Demo"&gt;https://modelscope.cn/studios/Qwen/Qwen3-ASR-Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face Demo: &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-ASR-Demo"&gt;https://huggingface.co/spaces/Qwen/Qwen3-ASR-Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://qwen.ai/blog?id=41e4c0f6175f9b004a03a07e42343eaaf48329e7&amp;amp;from=research.latest-advancements-list"&gt;https://qwen.ai/blog?id=41e4c0f6175f9b004a03a07e42343eaaf48329e7&amp;amp;from=research.latest-advancements-list&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/et1syg58iynf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa1p/qwen_released_api_only_qwen3asr_the_allinone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa1p/qwen_released_api_only_qwen3asr_the_allinone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T15:08:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc79yg</id>
    <title>baidu/ERNIE-4.5-21B-A3B-Thinking ¬∑ Hugging Face</title>
    <updated>2025-09-09T02:31:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc79yg/baiduernie4521ba3bthinking_hugging_face/"&gt; &lt;img alt="baidu/ERNIE-4.5-21B-A3B-Thinking ¬∑ Hugging Face" src="https://external-preview.redd.it/PVc8HBAyReu1sVKS98fa6WZXbf4lkkgSEZVgozf_73w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6d7b67af3eb2cf9bcf96edfb103c94299b667a8" title="baidu/ERNIE-4.5-21B-A3B-Thinking ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Model Highlights&lt;/h1&gt; &lt;p&gt;Over the past three months, we have continued to scale the &lt;strong&gt;thinking capability&lt;/strong&gt; of ERNIE-4.5-21B-A3B, improving both the &lt;strong&gt;quality and depth&lt;/strong&gt; of reasoning, thereby advancing the competitiveness of ERNIE &lt;strong&gt;lightweight models&lt;/strong&gt; in complex reasoning tasks. We are pleased to introduce &lt;strong&gt;ERNIE-4.5-21B-A3B-Thinking&lt;/strong&gt;, featuring the following key enhancements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Significantly improved performance&lt;/strong&gt; on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient tool usage&lt;/strong&gt; capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced 128K long-context understanding&lt;/strong&gt; capabilities.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GGUF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF"&gt;https://huggingface.co/gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-Thinking"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc79yg/baiduernie4521ba3bthinking_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc79yg/baiduernie4521ba3bthinking_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T02:31:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbo33p</id>
    <title>UAE Preparing to Launch K2 Think, "the world‚Äôs most advanced open-source reasoning model"</title>
    <updated>2025-09-08T13:42:30+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo33p/uae_preparing_to_launch_k2_think_the_worlds_most/"&gt; &lt;img alt="UAE Preparing to Launch K2 Think, &amp;quot;the world‚Äôs most advanced open-source reasoning model&amp;quot;" src="https://external-preview.redd.it/3A4olwwXC7kAmitvVkfkfzLywUYc6IvJ9He-QlxgRLY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2afac7f2d1366e35e6945533e06a6756d060e202" title="UAE Preparing to Launch K2 Think, &amp;quot;the world‚Äôs most advanced open-source reasoning model&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;In the coming week, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) and G42 will release K2 Think, the world‚Äôs most advanced open-source reasoning model. &lt;strong&gt;Designed to be leaner and smarter, K2 Think delivers frontier-class performance in a remarkably compact form&lt;/strong&gt; ‚Äì often matching, or even surpassing, the results of models an order of magnitude larger. The result: greater efficiency, more flexibility, and broader real-world applicability.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wam.ae/en/article/bll7llv-recognition-sheikh-khalifa%E2%80%99s-contribution"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo33p/uae_preparing_to_launch_k2_think_the_worlds_most/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo33p/uae_preparing_to_launch_k2_think_the_worlds_most/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T13:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbr45v</id>
    <title>Poor man‚Äôs FlashAttention: Llama.cpp-gfx906 fork!</title>
    <updated>2025-09-08T15:39:53+00:00</updated>
    <author>
      <name>/u/CornerLimits</name>
      <uri>https://old.reddit.com/user/CornerLimits</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbr45v/poor_mans_flashattention_llamacppgfx906_fork/"&gt; &lt;img alt="Poor man‚Äôs FlashAttention: Llama.cpp-gfx906 fork!" src="https://external-preview.redd.it/PecTYmNSbm5tb-T9OW67-xyMoNn-SzofgAKif5I3sUI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e84ff63d5e5bbf73f86b2641f6f74955f0a20dbe" title="Poor man‚Äôs FlashAttention: Llama.cpp-gfx906 fork!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just released a fork of llama.cpp that implements some strong optimizations for the MI50/MI60/Vega7 series. &lt;/p&gt; &lt;p&gt;Thanks to the outstanding work of open source community I made a final effort to actually make flash attention FASTER than no flash attention in almost every case. Yeah‚Ä¶ almost.&lt;/p&gt; &lt;p&gt;The goal is to run ~30B models with ~30K ctx on a single card at decent speed.&lt;/p&gt; &lt;p&gt;You can find benchmarks, compile/launch/bench scripts, references to the original works and explanations of my new kernel in the repo.&lt;/p&gt; &lt;p&gt;Have fun!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CornerLimits"&gt; /u/CornerLimits &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/iacopPBK/llama.cpp-gfx906"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbr45v/poor_mans_flashattention_llamacppgfx906_fork/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbr45v/poor_mans_flashattention_llamacppgfx906_fork/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T15:39:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc1p0a</id>
    <title>Where are people finding RTX PRO 6000 96gb cards for under 7k</title>
    <updated>2025-09-08T22:18:44+00:00</updated>
    <author>
      <name>/u/devshore</name>
      <uri>https://old.reddit.com/user/devshore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everywhere ive seen, they are like 8.5k, but people comstantly mention that they can be had for around 6.5k. How? Where? I want to start moving away from paid services like claude and start moving towards self-hosting, starting with an rtx pro 6000 + 3090. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devshore"&gt; /u/devshore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc1p0a/where_are_people_finding_rtx_pro_6000_96gb_cards/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc1p0a/where_are_people_finding_rtx_pro_6000_96gb_cards/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc1p0a/where_are_people_finding_rtx_pro_6000_96gb_cards/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T22:18:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
