<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-07T21:07:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q6cx7x</id>
    <title>A.X-K1 - New korean LLM benchmark released</title>
    <updated>2026-01-07T11:26:34+00:00</updated>
    <author>
      <name>/u/Leather-Term-30</name>
      <uri>https://old.reddit.com/user/Leather-Term-30</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6cx7x/axk1_new_korean_llm_benchmark_released/"&gt; &lt;img alt="A.X-K1 - New korean LLM benchmark released" src="https://preview.redd.it/54p3jxjywwbg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f245d4be5450f8919f9889e8b73dbcdd62f0142" title="A.X-K1 - New korean LLM benchmark released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leather-Term-30"&gt; /u/Leather-Term-30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/54p3jxjywwbg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6cx7x/axk1_new_korean_llm_benchmark_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6cx7x/axk1_new_korean_llm_benchmark_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T11:26:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5vk9m</id>
    <title>200ms search over 40 million texts using just a CPU server + demo: binary search with int8 rescoring</title>
    <updated>2026-01-06T21:24:42+00:00</updated>
    <author>
      <name>/u/-Cubie-</name>
      <uri>https://old.reddit.com/user/-Cubie-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5vk9m/200ms_search_over_40_million_texts_using_just_a/"&gt; &lt;img alt="200ms search over 40 million texts using just a CPU server + demo: binary search with int8 rescoring" src="https://external-preview.redd.it/hCm8D9e9AzrbuM1MK1zF3wZVeIaff34g_KhZMmvJyGM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e9c0e4b8d5b4eb54a2abe96ddff2503d4b7f22f" title="200ms search over 40 million texts using just a CPU server + demo: binary search with int8 rescoring" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the inference strategy:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Embed your query using a dense embedding model into a 'standard' fp32 embedding&lt;/li&gt; &lt;li&gt;Quantize the fp32 embedding to binary: 32x smaller&lt;/li&gt; &lt;li&gt;Use an approximate (or exact) binary index to retrieve e.g. 40 documents (~20x faster than a fp32 index)&lt;/li&gt; &lt;li&gt;Load int8 embeddings for the 40 top binary documents from disk.&lt;/li&gt; &lt;li&gt;Rescore the top 40 documents using the fp32 query embedding and the 40 int8 embeddings&lt;/li&gt; &lt;li&gt;Sort the 40 documents based on the new scores, grab the top 10&lt;/li&gt; &lt;li&gt;Load the titles/texts of the top 10 documents&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This requires:&lt;br /&gt; - Embedding all of your documents once, and using those embeddings for:&lt;br /&gt; - A binary index, I used a IndexBinaryFlat for exact and IndexBinaryIVF for approximate&lt;br /&gt; - A int8 &amp;quot;view&amp;quot;, i.e. a way to load the int8 embeddings from disk efficiently given a document ID&lt;/p&gt; &lt;p&gt;Instead of having to store fp32 embeddings, you only store binary index (32x smaller) and int8 embeddings (4x smaller). Beyond that, you only keep the binary index in memory, so you're also saving 32x on memory compared to a fp32 search index.&lt;/p&gt; &lt;p&gt;By loading e.g. 4x as many documents with the binary index and rescoring those with int8, you restore ~99% of the performance of the fp32 search, compared to ~97% when using purely the binary index: &lt;a href="https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring"&gt;https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Check out the demo that allows you to test this technique on 40 million texts from Wikipedia: &lt;a href="https://huggingface.co/spaces/sentence-transformers/quantized-retrieval"&gt;https://huggingface.co/spaces/sentence-transformers/quantized-retrieval&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It would be simple to add a sparse component here as well: e.g. bm25s for a BM25 variant or an inference-free SparseEncoder with e.g. 'splade-index'.&lt;/p&gt; &lt;p&gt;In short: your retrieval doesn't need to be so expensive!&lt;/p&gt; &lt;p&gt;Sources:&lt;br /&gt; - &lt;a href="https://www.linkedin.com/posts/tomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a"&gt;https://www.linkedin.com/posts/tomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://huggingface.co/blog/embedding-quantization"&gt;https://huggingface.co/blog/embedding-quantization&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://cohere.com/blog/int8-binary-embeddings"&gt;https://cohere.com/blog/int8-binary-embeddings&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cubie-"&gt; /u/-Cubie- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/sentence-transformers/quantized-retrieval"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5vk9m/200ms_search_over_40_million_texts_using_just_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5vk9m/200ms_search_over_40_million_texts_using_just_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T21:24:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6k82y</id>
    <title>Fine-tuning OSS-120B / Qwen3-30B on 90k surgical Q&amp;A: SFT vs DPO, multi-turn, and RAG integration?</title>
    <updated>2026-01-07T16:37:34+00:00</updated>
    <author>
      <name>/u/Patient_Ad1095</name>
      <uri>https://old.reddit.com/user/Patient_Ad1095</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm planning to fine-tune OSS-20B (or Qwen3-30B-A3B-Thinking-2507) on a mixed corpus: ~10k human-written Q&amp;amp;A pairs plus ~80k carefully curated synthetic Q&amp;amp;A pairs that we spent a few months generating and validating. The goal is to publish an open-weight model on Hugging Face and submit the work to an upcoming surgical conference in my country. The model is intended to help junior surgeons with clinical reasoning/support and board-style exam prep.&lt;/p&gt; &lt;p&gt;I‚Äôm very comfortable with RAG + inference/deployment, but this is my first time running a fine-tuning effort at this scale. I‚Äôm also working with a tight compute budget, so I‚Äôm trying to be deliberate and avoid expensive trial-and-error. I‚Äôd really appreciate input from anyone who‚Äôs done this in practice:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Multi-turn behavior: If I fine-tune on this dataset, will it noticeably degrade multi-turn / follow-up handling? Should I explicitly add another 5‚Äì10k dialog-style, multi-turn examples (with coreference + follow-ups), or will the base model generally preserve conversational robustness without increased hallucination?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;SFT vs RL: The dataset is ~25% MCQs and ~75% open-ended answers; MCQs include rationales/explanations. Would you recommend RL after SFT here? If yes, what approach makes the most sense (e.g., DPO/IPO/KTO/ORPO vs PPO-style RLHF), and what data format + rough scale would you target for the preference/reward step?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Two inference modes: I want two user-facing modes: clinical support and exam preparation. Would you bake the mode-specific system prompts into SFT/RL (i.e., train with explicit instruction headers), and if so, would you attach them to every example or only a subset to avoid over-conditioning?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;RAG / tool use at inference: If I‚Äôm going to pair the model with RAG and/or a web-search tool at inference time, should that change how I structure fine-tuning or RL? For example: training with retrieved context, citations, tool-call patterns, refusal policies, or ‚Äúanswer only from context‚Äù constraints.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Model choice: Between OSS-20B and Qwen3-30B-A3B, which would you pick for this use case? I slightly prefer OSS-20B for general non-coding performance, but I‚Äôm unsure whether its chat/harmony formatting or any architecture/format constraints create extra friction or difficulties during SFT/RL.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Patient_Ad1095"&gt; /u/Patient_Ad1095 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6k82y/finetuning_oss120b_qwen330b_on_90k_surgical_qa/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6k82y/finetuning_oss120b_qwen330b_on_90k_surgical_qa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6k82y/finetuning_oss120b_qwen330b_on_90k_surgical_qa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T16:37:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6cihe</id>
    <title>I built a mobile game where a local Qwen3-VL acts as an "Oracle" that analyzes player photos</title>
    <updated>2026-01-07T11:03:08+00:00</updated>
    <author>
      <name>/u/franke777</name>
      <uri>https://old.reddit.com/user/franke777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on a solo project called Lenswalker a walking RPG where players physically walk to charge mana, then photograph real-world subjects. The interesting part: a locally-hosted vision model analyzes each photo and determines what they found.&lt;/p&gt; &lt;p&gt;The setup:&lt;/p&gt; &lt;p&gt;- Ollama running Qwen3-VL on my home server (RTX 4090)&lt;/p&gt; &lt;p&gt;- FastAPI backend, PWA frontend&lt;/p&gt; &lt;p&gt;- Everything self-hosted, no cloud APIs, no data leaving my network&lt;/p&gt; &lt;p&gt;What the Oracle does:&lt;/p&gt; &lt;p&gt;- Analyzes the photo and identifies the subject&lt;/p&gt; &lt;p&gt;- Assigns a &amp;quot;rarity&amp;quot; (1-10) based on how interesting/unusual it is (a trash can = 1, a wild fox = 9)&lt;/p&gt; &lt;p&gt;- Determines capture quality (composition, lighting, focus)&lt;/p&gt; &lt;p&gt;- Extracts dominant color -&amp;gt; maps to game element (green -&amp;gt; Nature, white -&amp;gt; Light, etc.)&lt;/p&gt; &lt;p&gt;- Generates flavor text for the discovery&lt;/p&gt; &lt;p&gt;What surprised me:&lt;/p&gt; &lt;p&gt;- Qwen3-VL is remarkably consistent at judging &amp;quot;interestingness&amp;quot; - mundane objects score low, genuinely unusual finds score high&lt;/p&gt; &lt;p&gt;- Color extraction works well for element assignment&lt;/p&gt; &lt;p&gt;- ~15-45s per analysis on first load, ~5-10s when model is warm&lt;/p&gt; &lt;p&gt;- Running OLLAMA_MAX_CONCURRENT=4 handles multiple players fine&lt;/p&gt; &lt;p&gt;The whole thing started because I wanted a game where the AI couldn't be cheated by googling answers, you have to actually go outside and find something worth photographing.&lt;/p&gt; &lt;p&gt;Currently in pre-alpha with ~25 testers. Happy to answer questions about the vision model integration or the prompt engineering approach. &lt;/p&gt; &lt;p&gt;If anyone in Europe wants to try it out, DM me, server's hosted in Germany so latency is best for EU players.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/franke777"&gt; /u/franke777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6cihe/i_built_a_mobile_game_where_a_local_qwen3vl_acts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6cihe/i_built_a_mobile_game_where_a_local_qwen3vl_acts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6cihe/i_built_a_mobile_game_where_a_local_qwen3vl_acts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T11:03:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6lt19</id>
    <title>[Project] I built a complete ui for Fine-Tuning LLMs on Mac (MLX) ‚Äì No more CLI arguments! (Open Source and Non-profit)</title>
    <updated>2026-01-07T17:34:38+00:00</updated>
    <author>
      <name>/u/Datapotagia</name>
      <uri>https://old.reddit.com/user/Datapotagia</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6lt19/project_i_built_a_complete_ui_for_finetuning_llms/"&gt; &lt;img alt="[Project] I built a complete ui for Fine-Tuning LLMs on Mac (MLX) ‚Äì No more CLI arguments! (Open Source and Non-profit)" src="https://b.thumbs.redditmedia.com/Gt2UOarOLAX4tmrIZ2R0EkZ9u0I4JKy2BLelQiDknUU.jpg" title="[Project] I built a complete ui for Fine-Tuning LLMs on Mac (MLX) ‚Äì No more CLI arguments! (Open Source and Non-profit)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;We all love Apple's MLX for its speed, but running fine-tunes usually means juggling endless CLI flags (&lt;code&gt;python&lt;/code&gt; &lt;a href="http://lora.py"&gt;&lt;code&gt;lora.py&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--model ... --learning_rate ...&lt;/code&gt;). It feels fragile and hard to track.&lt;/p&gt; &lt;p&gt;So I built a full &lt;strong&gt;Fine-Tuning Engine with a visual UI&lt;/strong&gt; for Apple Silicon.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/santos-sanz/mlx-lora-finetune-template"&gt;https://github.com/santos-sanz/mlx-lora-finetune-template&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;br /&gt; It wraps the raw MLX training scripts into a clean UI using &lt;strong&gt;Streamlit UI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Visual Configuration:&lt;/strong&gt; Select models (Mistral or Qwen)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data Preparation:&lt;/strong&gt; Integrated with OpenRouter to prepare training and validation data,&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hyperparameter Tuning:&lt;/strong&gt; Sliders for LoRA rank, learning rate, and epochs with default configs if you are not an expert.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time Monitoring:&lt;/strong&gt; Watch your loss curves visually as it trains.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat Tester:&lt;/strong&gt; Test your adapter immediately in a chat interface after training to see if it worked.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy HF Upload:&lt;/strong&gt; Upload your model directly to HuggingFace after testing it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Under the hood:&lt;/strong&gt;&lt;br /&gt; It still uses native MLX optimization (LoRA), so you get full M1/M2/M3 speed, just without the headache of terminal commands.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I‚Äôd love to know what you think. Is a UI helpful for your workflow, or do you prefer raw scripts?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6s4noxgppybg1.png?width=3344&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=77cdf3776362c235cc54e635af260d458115fccb"&gt;Data Preparation Tab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qgqtxavspybg1.png?width=3344&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=50b513a428f74d8d9cf970cae0bc08e38f814dcc"&gt;Training Tab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Datapotagia"&gt; /u/Datapotagia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6lt19/project_i_built_a_complete_ui_for_finetuning_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6lt19/project_i_built_a_complete_ui_for_finetuning_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6lt19/project_i_built_a_complete_ui_for_finetuning_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T17:34:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6pof8</id>
    <title>Minimizing ElevenLabs Latency for Custom LLM (OpenAI Fine-Tune)</title>
    <updated>2026-01-07T19:51:51+00:00</updated>
    <author>
      <name>/u/Common-Feeling7380</name>
      <uri>https://old.reddit.com/user/Common-Feeling7380</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've fine-tuned GPT 4.1 mini through OpenAI's browser SFT system. I want to use it as the Custom LLM for an Eleven Labs agent. I set up a Cloudflare worker proxy server to normalize input and strip reasoning.effort and forward the request to the OpenAI server. This adds maybe 10-50 ms. However, we don't get speech output in ElevenLabs for a full 7 seconds on average with this Custom LLM setup. When I switch the LLM to ElevenLabs integration with the 4.1 mini base model, it takes a couple seconds max.&lt;/p&gt; &lt;p&gt;Has anyone run into a similar issue? Any advice for minimizing this latency, it's just way too long.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Common-Feeling7380"&gt; /u/Common-Feeling7380 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6pof8/minimizing_elevenlabs_latency_for_custom_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6pof8/minimizing_elevenlabs_latency_for_custom_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6pof8/minimizing_elevenlabs_latency_for_custom_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T19:51:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6izof</id>
    <title>The Personality of Open Source: How Llama, Mistral, and Qwen Compare to GPT-5.2 and Claude</title>
    <updated>2026-01-07T15:52:34+00:00</updated>
    <author>
      <name>/u/dimethyldumbass</name>
      <uri>https://old.reddit.com/user/dimethyldumbass</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dimethyldumbass"&gt; /u/dimethyldumbass &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.lindr.io/blog/open-source-benchmark"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6izof/the_personality_of_open_source_how_llama_mistral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6izof/the_personality_of_open_source_how_llama_mistral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T15:52:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5m2n6</id>
    <title>A 30B Qwen Model Walks Into a Raspberry Pi‚Ä¶ and Runs in Real Time</title>
    <updated>2026-01-06T15:45:12+00:00</updated>
    <author>
      <name>/u/ali_byteshape</name>
      <uri>https://old.reddit.com/user/ali_byteshape</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/"&gt; &lt;img alt="A 30B Qwen Model Walks Into a Raspberry Pi‚Ä¶ and Runs in Real Time" src="https://preview.redd.it/52juwyqq0rbg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39e3a291db2422f84c16930831ae926a4cb20240" title="A 30B Qwen Model Walks Into a Raspberry Pi‚Ä¶ and Runs in Real Time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;We‚Äôre back with another &lt;strong&gt;ShapeLearn&lt;/strong&gt; GGUF release (&lt;a href="https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/"&gt;Blog&lt;/a&gt;, &lt;a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF"&gt;Models&lt;/a&gt;), this time for a model that &lt;em&gt;should not&lt;/em&gt; feel this usable on small hardware‚Ä¶ and yet here we are:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-Instruct-2507&lt;/strong&gt; (device-optimized quant variants, llama.cpp-first).&lt;/p&gt; &lt;p&gt;We‚Äôre optimizing for TPS on a specific device without output quality falling off a cliff.&lt;/p&gt; &lt;p&gt;Instead of treating ‚Äúsmaller‚Äù as the goal, we treat memory as a budget: Fit first, then optimize TPS vs quality.&lt;/p&gt; &lt;p&gt;Why? Because llama.cpp has a quirk: ‚ÄúFewer bits‚Äù does &lt;em&gt;not&lt;/em&gt; automatically mean ‚Äúmore speed.‚Äù&lt;/p&gt; &lt;p&gt;Different quant formats trigger different kernels + decode overheads, and on GPUs you can absolutely end up with &lt;strong&gt;smaller and slower&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Yes, a 30B runs on a Raspberry Pi 5 (16GB). We achieve &lt;strong&gt;8.03 TPS&lt;/strong&gt; at 2.70 BPW, while retaining &lt;strong&gt;94.18% of BF16 quality&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Across devices, the pattern repeats: ShapeLearn tends to find better TPS/quality tradeoffs versus alternatives (we compare against Unsloth and MagicQuant as requested in our previous post).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What‚Äôs new/interesting in this one&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1) CPU behavior is‚Ä¶ sane (mostly)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;On CPUs, once you‚Äôre past ‚Äúit fits,‚Äù &lt;strong&gt;smaller tends to be faster&lt;/strong&gt; in a fairly monotonic way. The tradeoff curve behaves like you‚Äôd expect.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2) GPU behavior is‚Ä¶ quirky (kernel edition)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;On GPUs, performance depends as much on &lt;strong&gt;kernel choice&lt;/strong&gt; as on memory footprint. So you often get &lt;strong&gt;sweet spots&lt;/strong&gt; (especially around ~4b) where the kernels are ‚Äúgolden path,‚Äù and pushing lower-bit can get weird.&lt;/p&gt; &lt;h1&gt;Request to the community üôè&lt;/h1&gt; &lt;p&gt;We‚Äôd &lt;em&gt;love&lt;/em&gt; feedback and extra testing from folks here, especially if you can run:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;different llama.cpp builds / CUDA backends,&lt;/li&gt; &lt;li&gt;weird batch sizes / context lengths,&lt;/li&gt; &lt;li&gt;real workloads (coding assistants, long-form, tool-ish prompts),&lt;/li&gt; &lt;li&gt;or non-NVIDIA setups (we‚Äôre aware this is where it gets spicy).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also: we heard you on the previous Reddit post and are actively working to improve our evaluation and reporting. Evaluation is currently our bottleneck, not quantization, so if you have strong opinions on what benchmarks best match real usage, we‚Äôre all ears.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ali_byteshape"&gt; /u/ali_byteshape &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/52juwyqq0rbg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T15:45:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6m4yw</id>
    <title>Nvidia RTP PRO proxmox VM GPU passtrough problem</title>
    <updated>2026-01-07T17:46:12+00:00</updated>
    <author>
      <name>/u/Rich_Artist_8327</name>
      <uri>https://old.reddit.com/user/Rich_Artist_8327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else has this ?&lt;br /&gt; When a VM is rebooted, Nvidia RTX Pro is not anymore recognized. The VM boots fine, and the lspci finds the card but nvidia-smi does not find, or nvtop. I always need to reboot the whole Proxmox host and then the GPU works in the VM as passed trough. But if the VM is rebooted once, its all gone and needs the whole server reboot.&lt;br /&gt; I have another similar server but with consumer RTX 5090 and in same ubuntu version and all works after VM reboots. So is there a known RTX PRO related issue with GPU passtrough?&lt;/p&gt; &lt;p&gt;EDIT: fixe with &lt;/p&gt; &lt;p&gt;sudo nano /etc/modprobe.d/nvidia-modeset.conf&lt;/p&gt; &lt;p&gt;add this line in the VM:&lt;/p&gt; &lt;p&gt;options nvidia-drm modeset=0&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rich_Artist_8327"&gt; /u/Rich_Artist_8327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6m4yw/nvidia_rtp_pro_proxmox_vm_gpu_passtrough_problem/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6m4yw/nvidia_rtp_pro_proxmox_vm_gpu_passtrough_problem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6m4yw/nvidia_rtp_pro_proxmox_vm_gpu_passtrough_problem/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T17:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6j58w</id>
    <title>[HW TUNING] Finding the best GPU power limit for inference</title>
    <updated>2026-01-07T15:58:20+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So in preparation for my multi-GPU setup I wanted to actually test the &amp;quot;limit the power bro, after a specific limit the increase is marginal...&amp;quot; and it seems to have a large kernel of truth in it. So the pre-conditions are RTX4090 with main usage as a single user.&lt;/p&gt; &lt;p&gt;The vLLM server line was: vllm serve allenai/Olmo-3-7B-Instruct --trust-remote-code --max-model-len 32768&lt;/p&gt; &lt;p&gt;The benchmark command line was: vllm bench serve --backend openai --host 127.0.0.1 --port 8000 --endpoint /v1/completions --model allenai/Olmo-3-7B-Instruct --dataset-name random --num-prompts 200 --seed 0 --input-len 1024 --output-len 128 --request-rate 1 --max-concurrency 1 --metric-percentiles 50,90,95,99 --percentile-metrics ttft,tpot,itl,e2el --save-result --result-dir ./bench_results --result-filename &amp;quot;xxxW_interactive_c1_rps1.json&amp;quot;, where xxxW is the set power limit where the benchmark was done, i.e 300W.&lt;/p&gt; &lt;p&gt;The results are:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Median TTFT (lower is better) 250W: 139.17 ms 300W: 100.97 ms (huge win) 350W: 100.28 ms (basically same as 300W) 400W: 96.51 ms (small gain) 450W: 94.09 ms (tiny gain) P99 TTFT (tail latency / ‚Äúhitching‚Äù) 250W: 143.02 ms 300W: 118.56 ms 350W: 101.97 ms (big tail improvement) 400W: 98.05 ms 450W: 95.06 ms Decode smoothness (ITL / TPOT) Median ITL is basically flat after 300W: 250W: 16.455 ms 300W: 16.250 ms 350W: 16.198 ms 400W: 16.196 ms 450W: 16.196 ms P99 ITL improves a bit up to ~350W then flattens: 250W: 17.38 ms 300W: 16.90 ms 350W: 16.46 ms 400W: 16.41 ms 450W: 16.38 ms Sweet spot #1 (best value / best perf-per-watt): 300W Sweet spot #2 (best ‚Äúsmoothness‚Äù / best tails): 350W Median barely changes vs 300W, but P99 TTFT and P99 ITL improve noticeably, i.e. fewer little ‚Äúhiccups.‚Äù Costs you only +50W vs 300W. Not worth it: &amp;gt;350W 350‚Üí450W buys you ~6 ms median TTFT and tiny ITL gains for +100W. That‚Äôs classic waste. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The comments are form the friendly ChatGPT, so how you find your optimal power level for your setup ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6j58w/hw_tuning_finding_the_best_gpu_power_limit_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6j58w/hw_tuning_finding_the_best_gpu_power_limit_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6j58w/hw_tuning_finding_the_best_gpu_power_limit_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T15:58:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1q617ug</id>
    <title>Razer is demonstrating a ‚ÄúAI accelerator‚Äù box with a Wormhole n150 processor from Tenstorrent at CES</title>
    <updated>2026-01-07T01:07:27+00:00</updated>
    <author>
      <name>/u/Hasuto</name>
      <uri>https://old.reddit.com/user/Hasuto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q617ug/razer_is_demonstrating_a_ai_accelerator_box_with/"&gt; &lt;img alt="Razer is demonstrating a ‚ÄúAI accelerator‚Äù box with a Wormhole n150 processor from Tenstorrent at CES" src="https://external-preview.redd.it/55S8efCmWR_UNwVQwnrBhqr5tzC73SFwKgXTxGt7lKs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d858518876fe54114627febc2a60feabfd21a89" title="Razer is demonstrating a ‚ÄúAI accelerator‚Äù box with a Wormhole n150 processor from Tenstorrent at CES" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There is a press release from Tenstorrent as well, but I haven‚Äôt seen anyone test it out.&lt;/p&gt; &lt;p&gt;From what I‚Äôve seen before the hardware isn‚Äôt super impressive. The n150 usually comes as a PCIe dev board with 12GB memory for $1000.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hasuto"&gt; /u/Hasuto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/razer-partners-tenstorrent-goes-into-full-ai-mode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q617ug/razer_is_demonstrating_a_ai_accelerator_box_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q617ug/razer_is_demonstrating_a_ai_accelerator_box_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T01:07:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1q64f26</id>
    <title>llama.cpp vs Ollama: ~70% higher code generation throughput on Qwen-3 Coder 32B (FP16)</title>
    <updated>2026-01-07T03:27:09+00:00</updated>
    <author>
      <name>/u/Shoddy_Bed3240</name>
      <uri>https://old.reddit.com/user/Shoddy_Bed3240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm seeing a significant throughput difference between &lt;strong&gt;llama.cpp&lt;/strong&gt; and &lt;strong&gt;Ollama&lt;/strong&gt; when running the same model locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: &lt;strong&gt;Qwen-3 Coder 32B&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Precision: &lt;strong&gt;FP16&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Hardware: &lt;strong&gt;RTX 5090 + RTX 3090 Ti&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Task: code generation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;llama.cpp:&lt;/strong&gt; ~52 tokens/sec&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama:&lt;/strong&gt; ~30 tokens/sec&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both runs use the same model weights and hardware. The gap is ~70% in favor of llama.cpp.&lt;/p&gt; &lt;p&gt;Has anyone dug into why this happens? Possibilities I‚Äôm considering:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;different CUDA kernels / attention implementations&lt;/li&gt; &lt;li&gt;default context or batching differences&lt;/li&gt; &lt;li&gt;scheduler or multi-GPU utilization differences&lt;/li&gt; &lt;li&gt;overhead from Ollama‚Äôs runtime / API layer&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious if others have benchmarked this or know which knobs in Ollama might close the gap.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shoddy_Bed3240"&gt; /u/Shoddy_Bed3240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q64f26/llamacpp_vs_ollama_70_higher_code_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q64f26/llamacpp_vs_ollama_70_higher_code_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q64f26/llamacpp_vs_ollama_70_higher_code_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T03:27:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6a32c</id>
    <title>NousCoder-14B-GGUF is here!</title>
    <updated>2026-01-07T08:33:13+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6a32c/nouscoder14bgguf_is_here/"&gt; &lt;img alt="NousCoder-14B-GGUF is here!" src="https://external-preview.redd.it/DMiOkJBVtF0q2KjwqWnPlke-IPF1A5a0Z_XDB7tGLn8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b59c49d178c2f341a958665b9686d84962973bbe" title="NousCoder-14B-GGUF is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RL post training on Qwen 3 14B&lt;/p&gt; &lt;p&gt;&amp;quot;On LiveCodeBench v6 (08/01/2024 - 05/01/2025), we achieve a Pass@1 accuracy of 67.87%, up 7.08% from the baseline Pass@1 accuracy of 60.79% of Qwen3-14B. We trained on 24k verifiable coding problems using 48 B200s over the course of four days.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AaryanK/NousCoder-14B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6a32c/nouscoder14bgguf_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6a32c/nouscoder14bgguf_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T08:33:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6edb2</id>
    <title>AI agents for searching and reasoning over internal documents</title>
    <updated>2026-01-07T12:42:45+00:00</updated>
    <author>
      <name>/u/Effective-Ad2060</name>
      <uri>https://old.reddit.com/user/Effective-Ad2060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I‚Äôm excited to share something we‚Äôve been building for the past few months - &lt;strong&gt;PipesHub&lt;/strong&gt;, a &lt;strong&gt;fully open-source alternative to Glean,&lt;/strong&gt; designed to bring powerful Enterprise Search, Agent Builders to every team, without vendor lock-in. The platform brings all your business data together and makes it searchable. It connects with apps like Google Drive, Gmail, Slack, Notion, Confluence, Jira, OneDrive, Outlook, SharePoint Online, Dropbox, and even local file uploads. You can deploy it and run it with just one docker compose command.&lt;/p&gt; &lt;p&gt;The entire system is built on a &lt;strong&gt;fully event-streaming architecture powered by Kafka&lt;/strong&gt;, making indexing and retrieval scalable, fault-tolerant, and real-time across large volumes of data. PipesHub combines a vector database with a knowledge graph and uses Agentic RAG to deliver highly accurate results. We constrain the LLM to ground truth. Provides Visual citations, reasoning and confidence score. Our implementation says Information not found rather than hallucinating.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep understanding of user, organization and teams with enterprise knowledge graph&lt;/li&gt; &lt;li&gt;Connect to any AI model of your choice including OpenAI, Gemini, Claude, or Ollama&lt;/li&gt; &lt;li&gt;Use any other provider that supports OpenAI compatible endpoints&lt;/li&gt; &lt;li&gt;Vision-Language Models and OCR for visual or scanned docs&lt;/li&gt; &lt;li&gt;Login with Google, Microsoft, OAuth, or SSO&lt;/li&gt; &lt;li&gt;Rich REST APIs for developers&lt;/li&gt; &lt;li&gt;All major file types support including pdfs with images, diagrams and charts&lt;/li&gt; &lt;li&gt;Agent Builder - Perform actions like Sending mails, Schedule Meetings, etc along with Search, Deep research, Internet search and more&lt;/li&gt; &lt;li&gt;Reasoning Agent that plans before executing tasks&lt;/li&gt; &lt;li&gt;40+ Connectors allowing you to connect to your entire business apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check it out and share your thoughts or feedback. Your feedback is immensely valuable and is much appreciated:&lt;br /&gt; &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo Video:&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=xA9m3pwOgz8"&gt;https://www.youtube.com/watch?v=xA9m3pwOgz8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective-Ad2060"&gt; /u/Effective-Ad2060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6edb2/ai_agents_for_searching_and_reasoning_over/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6edb2/ai_agents_for_searching_and_reasoning_over/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6edb2/ai_agents_for_searching_and_reasoning_over/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T12:42:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q61wpv</id>
    <title>NousResearch/NousCoder-14B ¬∑ Hugging Face</title>
    <updated>2026-01-07T01:37:25+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q61wpv/nousresearchnouscoder14b_hugging_face/"&gt; &lt;img alt="NousResearch/NousCoder-14B ¬∑ Hugging Face" src="https://external-preview.redd.it/5B9NQ05vM8G6MB5IJCuanVCmVkz4L0DmuyDbK4fKfHU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d0894b4bfc8c3fc81d8f7c59a878b6ad54d6614" title="NousResearch/NousCoder-14B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;from NousResearch:&lt;/p&gt; &lt;p&gt;&amp;quot;We introduce &lt;em&gt;NousCoder-14B&lt;/em&gt;, a competitive programming model post-trained on &lt;a href="https://huggingface.co/Qwen/Qwen3-14B"&gt;Qwen3-14B&lt;/a&gt; via reinforcement learning. On LiveCodeBench v6 (08/01/2024 - 05/01/2025), we achieve a Pass@1 accuracy of 67.87%, up 7.08% from the baseline Pass@1 accuracy of 60.79% of Qwen3-14B. We trained on 24k verifiable coding problems using 48 B200s over the course of four days.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/NousResearch/NousCoder-14B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q61wpv/nousresearchnouscoder14b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q61wpv/nousresearchnouscoder14b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T01:37:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6cuh5</id>
    <title>In NVIDIA's announcement of Rubin (successor to Blackwell) what do you think is meant by "adaptive compression"?</title>
    <updated>2026-01-07T11:22:07+00:00</updated>
    <author>
      <name>/u/michaelmalak</name>
      <uri>https://old.reddit.com/user/michaelmalak</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6cuh5/in_nvidias_announcement_of_rubin_successor_to/"&gt; &lt;img alt="In NVIDIA's announcement of Rubin (successor to Blackwell) what do you think is meant by &amp;quot;adaptive compression&amp;quot;?" src="https://external-preview.redd.it/5t8jfpe67v909kH8kONfOcl8j7uy46XjufTzHSp5p-I.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81304f3376532fa1f1d936cef16bcd151043d65a" title="In NVIDIA's announcement of Rubin (successor to Blackwell) what do you think is meant by &amp;quot;adaptive compression&amp;quot;?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/michaelmalak"&gt; /u/michaelmalak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developer.nvidia.com/blog/inside-the-nvidia-rubin-platform-six-new-chips-one-ai-supercomputer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6cuh5/in_nvidias_announcement_of_rubin_successor_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6cuh5/in_nvidias_announcement_of_rubin_successor_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T11:22:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6b6u7</id>
    <title>Has anyone tested how the newest Rocm does in llms?</title>
    <updated>2026-01-07T09:43:40+00:00</updated>
    <author>
      <name>/u/Eden1506</name>
      <uri>https://old.reddit.com/user/Eden1506</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6b6u7/has_anyone_tested_how_the_newest_rocm_does_in_llms/"&gt; &lt;img alt="Has anyone tested how the newest Rocm does in llms?" src="https://preview.redd.it/z3s6igomewbg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56b025d3c97db74a9cc2754f7f890856d8f441b4" title="Has anyone tested how the newest Rocm does in llms?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been using Vulkan but the newest rocm is supposed to be quite a Performance jump and wanted to know if its worth the headache to install?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eden1506"&gt; /u/Eden1506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z3s6igomewbg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6b6u7/has_anyone_tested_how_the_newest_rocm_does_in_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6b6u7/has_anyone_tested_how_the_newest_rocm_does_in_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T09:43:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6khuh</id>
    <title>Arguably, the best web search MCP server for Claude Code, Codex, and other coding tools</title>
    <updated>2026-01-07T16:47:30+00:00</updated>
    <author>
      <name>/u/Quirky_Category5725</name>
      <uri>https://old.reddit.com/user/Quirky_Category5725</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6khuh/arguably_the_best_web_search_mcp_server_for/"&gt; &lt;img alt="Arguably, the best web search MCP server for Claude Code, Codex, and other coding tools" src="https://a.thumbs.redditmedia.com/EG9ZWvZPaVnuT_0-o6x3NK2RjHDh4xMBCJn-AOk8ei0.jpg" title="Arguably, the best web search MCP server for Claude Code, Codex, and other coding tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôve officially open-sourced &lt;a href="https://github.com/Shelpuk-AI-Technology-Consulting/kindly-web-search-mcp-server"&gt;Kindly&lt;/a&gt; - the Web Search MCP server we built internally for tools like Claude Code, Cursor, and Codex.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tpiz0zg0iybg1.png?width=1498&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=498c083702c62f798ae1d7af434b3e920bb9a7f4"&gt;https://preview.redd.it/tpiz0zg0iybg1.png?width=1498&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=498c083702c62f798ae1d7af434b3e920bb9a7f4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Why build another search tool? Because the existing ones were frustrating us.&lt;/p&gt; &lt;p&gt;When you are debugging a complex issue, you don‚Äôt just need a URL or a 2-sentence snippet (which is what wrappers like Tavily or Serper usually provide). You need the context. You need the &amp;quot;Accepted Answer&amp;quot; on StackOverflow, the specific GitHub Issue comment saying &amp;quot;this workaround fixed it,&amp;quot; or the actual content of an arXiv paper.&lt;/p&gt; &lt;p&gt;Standard search MCPs usually fail here. They either return insufficient snippets or dump raw HTML full of navigation bars and ads that confuse the LLM and waste context window.&lt;/p&gt; &lt;p&gt;Kindly solves this by being smarter about retrieval, not just search:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Intelligent Parsing: It doesn‚Äôt just scrape. If the search result is a StackOverflow thread, Kindly uses the StackExchange API to fetch the question, all answers, and metadata (likes/accepted status) and formats it into clean Markdown.&lt;/li&gt; &lt;li&gt;GitHub Native: If the result is a GitHub Issue, it pulls the full conversation via the API.&lt;/li&gt; &lt;li&gt;ArXiv Ready: It grabs the full PDF content and converts it to text.&lt;/li&gt; &lt;li&gt;Headless Browser Fallback: For everything else, it spins up an invisible browser to render the page and extract the main content.&lt;/li&gt; &lt;li&gt;One-Shot: It returns the full, structured content with the search results. No need for the AI to make a second tool call to &amp;quot;read page.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For us, this replaced our need for separate generic web search, StackOverflow, and scraping MCP servers. It‚Äôs the only setup we‚Äôve found that allows AI coding assistants to actually research a bug the way a human engineer would.&lt;/p&gt; &lt;p&gt;It works with Claude Code, Codex, Cursor, and others.&lt;/p&gt; &lt;p&gt;P.S. If you give it a try or like the idea, please drop us a star on GitHub - it‚Äôs always huge motivation for us to keep improving it! ‚≠êÔ∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quirky_Category5725"&gt; /u/Quirky_Category5725 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6khuh/arguably_the_best_web_search_mcp_server_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6khuh/arguably_the_best_web_search_mcp_server_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6khuh/arguably_the_best_web_search_mcp_server_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T16:47:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q694ic</id>
    <title>Don't put off hardware purchases: GPUs, SSDs, and RAM are going to skyrocket in price soon</title>
    <updated>2026-01-07T07:32:28+00:00</updated>
    <author>
      <name>/u/Eisenstein</name>
      <uri>https://old.reddit.com/user/Eisenstein</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In case you thought it was going to get better:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPU&lt;/strong&gt; prices are going up. &lt;a href="https://www.trendforce.com/news/2026/01/05/news-nvidia-amd-reportedly-plan-price-hikes-starting-1q26-geforce-rtx-5090-may-reach-5000/"&gt;AMD and NVIDIA are planning to increase prices every month starting soon.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NAND flash&lt;/strong&gt; contract price &lt;a href="https://www.trendforce.com/price/flash/flash_contract"&gt;went up 20% in November&lt;/a&gt;, with &lt;a href="https://www.trendforce.com/research/download/RP251231KM"&gt;further increases in December&lt;/a&gt;. This means SSDs will be a lot more expensive soon.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DRAM&lt;/strong&gt; &lt;a href="https://www.trendforce.com/news/2026/01/07/news-memory-shortages-reportedly-spark-csp-buying-spree-2027-supply-contracts-eyed-as-early-as-q1/"&gt;prices are going to skyrocket&lt;/a&gt;, with no increase in production capacity and datacenters and OEMs competing for everything. &lt;/p&gt; &lt;p&gt;Even &lt;strong&gt;Consoles&lt;/strong&gt; are &lt;a href="https://insider-gaming.com/ram-prices-next-gen/"&gt;going to be delayed due to the shortages.&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;According to TrendForce, conventional DRAM contract prices in 1Q26 are forecast to rise 55‚Äì60% quarter over quarter, while server DRAM prices are projected to surge by more than 60% QoQ. Meanwhile, NAND Flash prices are expected to increase 33‚Äì38% QoQ&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://www.trendforce.com/news/2026/01/07/news-memory-shortages-reportedly-spark-csp-buying-spree-2027-supply-contracts-eyed-as-early-as-q1/"&gt;Source.&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Industry sources cited by Kbench believe the latest price hikes will broadly affect NVIDIA‚Äôs RTX 50 series and AMD‚Äôs Radeon RX 9000 lineup. The outlet adds that NVIDIA‚Äôs flagship GeForce RTX 5090 could see its price climb to as high as $5,000 later in 2026.&lt;/p&gt; &lt;p&gt;NVIDIA is also reportedly weighing a 30% to 40% reduction in output for parts of its midrange lineup, including the RTX 5070 and RTX 5060 Ti, according to Kbench.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://www.trendforce.com/news/2026/01/05/news-nvidia-amd-reportedly-plan-price-hikes-starting-1q26-geforce-rtx-5090-may-reach-5000/"&gt;Source.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eisenstein"&gt; /u/Eisenstein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q694ic/dont_put_off_hardware_purchases_gpus_ssds_and_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q694ic/dont_put_off_hardware_purchases_gpus_ssds_and_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q694ic/dont_put_off_hardware_purchases_gpus_ssds_and_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T07:32:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6nm6a</id>
    <title>Liquid AI releases LFM2-2.6B-Transcript, an incredibly fast open-weight meeting transcribing AI model on-par with closed-source giants.</title>
    <updated>2026-01-07T18:38:08+00:00</updated>
    <author>
      <name>/u/KaroYadgar</name>
      <uri>https://old.reddit.com/user/KaroYadgar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6nm6a/liquid_ai_releases_lfm226btranscript_an/"&gt; &lt;img alt="Liquid AI releases LFM2-2.6B-Transcript, an incredibly fast open-weight meeting transcribing AI model on-par with closed-source giants." src="https://b.thumbs.redditmedia.com/8ZC6Dgn6yBO0SzygMNeu1n_550XBpbDxYLQVCtnNdCw.jpg" title="Liquid AI releases LFM2-2.6B-Transcript, an incredibly fast open-weight meeting transcribing AI model on-par with closed-source giants." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://x.com/liquidai/status/2008954886659166371"&gt;https://x.com/liquidai/status/2008954886659166371&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hugging Face page:&lt;/strong&gt; &lt;a href="https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript"&gt;https://huggingface.co/LiquidAI/LFM2-2.6B-Transcript&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GGUFs:&lt;/strong&gt; &lt;a href="https://huggingface.co/models?other=base_model:quantized:LiquidAI/LFM2-2.6B-Transcript"&gt;https://huggingface.co/models?other=base_model:quantized:LiquidAI/LFM2-2.6B-Transcript&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;First image:&lt;/strong&gt;&lt;br /&gt; &amp;quot;This week at &lt;a href="https://x.com/hashtag/CES?src=hashtag_click"&gt;#CES&lt;/a&gt;, we‚Äôre showcasing what‚Äôs next for on-device intelligence alongside our partners &lt;a href="https://x.com/AMD"&gt;@AMD&lt;/a&gt;: fast, private, and entirely secure AI summarization that runs fully on-device.&lt;/p&gt; &lt;p&gt;Meetings are foundational to business, creating mission critical and sensitive information. Too often, that data leaves the room to be processed in the cloud, introducing latency, unpredictable costs, and real security and compliance risks.&lt;/p&gt; &lt;p&gt;With &lt;a href="https://x.com/AMD"&gt;@AMD&lt;/a&gt;, we‚Äôve broken that barrier with a cloud-quality summarization model that runs locally across the AMD Ryzen‚Ñ¢ AI platform, delivering enterprise-grade accuracy in seconds.&lt;/p&gt; &lt;p&gt;Today, we‚Äôre expanding access to this model to everyone.&lt;/p&gt; &lt;p&gt;Meet LFM2-2.6B-Transcript: a purpose-built Liquid Nano designed for long-form meeting transcripts and real operational use.&lt;/p&gt; &lt;p&gt;&amp;gt; Cloud-level summarization quality&lt;br /&gt; &amp;gt; Summaries generated in seconds&lt;br /&gt; &amp;gt; &amp;lt;3 GB RAM usage \&amp;gt; Lower latency and energy consumption than larger transformer baselines&lt;br /&gt; &amp;gt; Fully local execution across CPU, GPU, and NPU&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Second image:&lt;/strong&gt;&lt;br /&gt; &amp;quot;LFM2-2.6B-Transcript delivers accuracy ratings on par with cloud models that are orders of magnitude larger. Delivering similar quality for a fraction of the memory use and compute. It completes a 60-minute meeting summarization in 16 seconds!&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Third Image:&lt;/strong&gt;&lt;br /&gt; &amp;quot;Leveraging our efficient LFM2 backbone, LFM2-2.6B-Transcript uses significantly less RAM than other models. This gap is what makes full on-device deployment on 16GB AI PCs practical for LFM2‚Äîbut effectively out of reach for many traditional transformer models.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KaroYadgar"&gt; /u/KaroYadgar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q6nm6a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6nm6a/liquid_ai_releases_lfm226btranscript_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6nm6a/liquid_ai_releases_lfm226btranscript_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T18:38:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6o39r</id>
    <title>Plea for testers - Llama.cpp autoparser</title>
    <updated>2026-01-07T18:54:18+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6o39r/plea_for_testers_llamacpp_autoparser/"&gt; &lt;img alt="Plea for testers - Llama.cpp autoparser" src="https://external-preview.redd.it/JcpYn8Cz-OyGGRpBPLbq_dE9J31OX_9QcJOnJvqOM3Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e9e9cfdda72c8105276d541c870c4c4c454caad" title="Plea for testers - Llama.cpp autoparser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would like to ask the community to aid in the testing of the new autoparser mechanism that I've been cooking for llama.cpp for the past month or so. &lt;/p&gt; &lt;p&gt;The idea is to scrap the existing buggy mess of the chat parsers and replace it with a layered mechanism:&lt;br /&gt; -&amp;gt; autoparser that handles 95%+ of typical chat templates for models&lt;br /&gt; -&amp;gt; manual parsers / handlers for models that need something extra&lt;/p&gt; &lt;p&gt;Currently of all models that I've tested, only Ministral and GPT-OSS have shown the need to use a dedicated parser. I've tested the approach as extensively with as many models as I could, but I'm just a single dev doing this after hours, so I obviously can't do long coding sessions on all possible models. Therefore, I'd ask everyone who's able to test it with their favorite coding agent (I mostly used OpenCode and Roo, it's important to use an agent that actually uses tool calls, so Aider is out) because I'm quite sure there will be quite a few bugs.&lt;/p&gt; &lt;p&gt;Since I don't want to clutter the main repo, please report all bugs with the autoparser to &lt;a href="https://github.com/pwilkin/llama.cpp/issues"&gt;https://github.com/pwilkin/llama.cpp/issues&lt;/a&gt; instead.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18675"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6o39r/plea_for_testers_llamacpp_autoparser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6o39r/plea_for_testers_llamacpp_autoparser/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T18:54:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6c9wc</id>
    <title>DeepSeek-R1‚Äôs paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.</title>
    <updated>2026-01-07T10:49:12+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/"&gt; &lt;img alt="DeepSeek-R1‚Äôs paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail." src="https://b.thumbs.redditmedia.com/DUX7Mp3MJPzZfCIE1yl1lv9VDENeyLGC6ZkgyRLizOw.jpg" title="DeepSeek-R1‚Äôs paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;arXiv:2501.12948 [cs.CL]: &lt;a href="https://arxiv.org/abs/2501.12948"&gt;https://arxiv.org/abs/2501.12948&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q6c9wc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T10:49:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6n5vl</id>
    <title>16x AMD MI50 32GB at 10 t/s (tg) &amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)</title>
    <updated>2026-01-07T18:22:05+00:00</updated>
    <author>
      <name>/u/ai-infos</name>
      <uri>https://old.reddit.com/user/ai-infos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/"&gt; &lt;img alt="16x AMD MI50 32GB at 10 t/s (tg) &amp;amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)" src="https://preview.redd.it/lor8ccu2xybg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=857ab421f987d81024114a5c2bc2cf35859061b4" title="16x AMD MI50 32GB at 10 t/s (tg) &amp;amp; 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek 3.2 AWQ 4bit @ 10 tok/s (output) // 2000 tok/s (input of 23k tok)&lt;/p&gt; &lt;p&gt;on vllm-gfx906-deepseek with 69000 context length&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Power draw&lt;/strong&gt;: 550W (idle) / 2400W (peak inference)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: run Deepseek V3.2 AWQ 4-bit on most cost effective hardware like 16*MI50 at decent speed (token generation &amp;amp; prompt processing)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Coming next&lt;/strong&gt;: open source a future test setup of 32 AMD MI50 32GB for Kimi K2 Thinking&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Credits&lt;/strong&gt;: BIG thanks to the Global Open source Community!&lt;/p&gt; &lt;p&gt;All setup details here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32"&gt;https://github.com/ai-infos/guidances-setup-16-mi50-deepseek-v32&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Feel free to ask any questions and/or share any comments.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;ps: it might be a good alternative to CPU hardwares as RAM price increases and the prompt processing speed will be much better with 16 TB/s bandwidth + tensor parallelism! &lt;/p&gt; &lt;p&gt;ps2: i'm just a random guy with average software dev background using LLMs to make it run. Goal is to be ready for LOCAL AGI without spending +300k$... &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai-infos"&gt; /u/ai-infos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lor8ccu2xybg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T18:22:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
