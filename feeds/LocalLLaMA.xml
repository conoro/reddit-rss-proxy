<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-06T04:37:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1nyz2fg</id>
    <title>SFT + RL ?</title>
    <updated>2025-10-05T20:25:15+00:00</updated>
    <author>
      <name>/u/Severe_Biscotti2349</name>
      <uri>https://old.reddit.com/user/Severe_Biscotti2349</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys i need your help&lt;/p&gt; &lt;p&gt;Ive trained Qwen 2.5 VL with unsloth on runpod got Nice results honestly. Lets say between 85 to 90% success on my invoices.&lt;/p&gt; &lt;p&gt;So i decided on top of this to try some RL to go to 95% but here comes problems after problems&lt;/p&gt; &lt;p&gt;Unsloth offers RL with Vllm so i took my SFT model and tried it but doenst work with vllm as its 4bit.&lt;/p&gt; &lt;p&gt;So i decided to merge the model to float 16 than it can do the RL with vllm (new problem cuda out of memory on an rtx 5090).&lt;/p&gt; &lt;p&gt;Than i Tried the RL with the 4bit model but without vllm on top, it works but more than 15 hours ???&lt;/p&gt; &lt;p&gt;Should i merge the modal or keep it like this after SFT ? (like ive got the Lora adapters and if i try to RL on this it says Lora adapters already exist) &lt;/p&gt; &lt;p&gt;Am i doing something wrong or its the only solution ? Should i upgrade on runpod to an rtx pro 6000 ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe_Biscotti2349"&gt; /u/Severe_Biscotti2349 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyz2fg/sft_rl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyz2fg/sft_rl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyz2fg/sft_rl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T20:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz4fpe</id>
    <title>Building a pc for AI and gaming</title>
    <updated>2025-10-06T00:12:40+00:00</updated>
    <author>
      <name>/u/OkCicada9598</name>
      <uri>https://old.reddit.com/user/OkCicada9598</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone. so i'm trying to build a new computer for running ai models (70b q4), using SD and also for gaming. But i have never built any pc and i'm a beginner at that, and building a pc for all of this is above my head to be honest. So far, i have made a list to what to get, and i really have problems such as;&lt;/p&gt; &lt;p&gt;1-does it fit?&lt;/p&gt; &lt;p&gt;2-what psu should i get (and my choices are very limited in my country, i will list what can i buy below.)&lt;/p&gt; &lt;p&gt;3-Do i need to get extra cables?&lt;/p&gt; &lt;p&gt;4-Anything else i'm missing or doing something wrong? because i work 6 days and i don't have much time to return stuff etc.&lt;/p&gt; &lt;p&gt;5- Can i play games as usual, or when i plug both 3090's, does pcie 5.0x8 limits me?&lt;/p&gt; &lt;p&gt;Build:&lt;/p&gt; &lt;p&gt;Case: Lian Li V3000 Plus&lt;/p&gt; &lt;p&gt;Motherboard: Gigabyte B850 AI TOP&lt;/p&gt; &lt;p&gt;Cpu: Amd Ryzen 9800x3d&lt;/p&gt; &lt;p&gt;Gpu: 2x3090&lt;/p&gt; &lt;p&gt;Ram: Kingston Beast RGB 64 GB (2x32) 6000 MHz CL30&lt;/p&gt; &lt;p&gt;PSU: I'm not planning to get overclock anything or undervolt, so as i saw in this sub(if i'm not mistaken), i need a 1600w psu. My choices are a) Asus ROG-THOR-1600T-GAMING b) Enermax Revolution ERT1650EWT c) FSP Hydro PTM PRO HPT2-1650M&lt;/p&gt; &lt;p&gt;SSD: 1xsamsung 990 PRO 1tb + 1xsamsung 990 PRO 4tb&lt;/p&gt; &lt;p&gt;AIO: Arctic Liquid Freezer II 420mm ARGB.&lt;/p&gt; &lt;p&gt;Fans: going to buy 10 fans first and 5 later. Can't decide what to buy yet, but thinking to go with something quiet,&lt;/p&gt; &lt;p&gt;Thanks in advance everyone.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OkCicada9598"&gt; /u/OkCicada9598 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz4fpe/building_a_pc_for_ai_and_gaming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz4fpe/building_a_pc_for_ai_and_gaming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz4fpe/building_a_pc_for_ai_and_gaming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T00:12:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nykzv3</id>
    <title>Video2X 6.x ‚Äî open-source upscaler + frame interpolation (Anime4K v4 / Real-ESRGAN / Real-CUGAN / RIFE) üöÄ</title>
    <updated>2025-10-05T10:45:43+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nykzv3/video2x_6x_opensource_upscaler_frame/"&gt; &lt;img alt="Video2X 6.x ‚Äî open-source upscaler + frame interpolation (Anime4K v4 / Real-ESRGAN / Real-CUGAN / RIFE) üöÄ" src="https://a.thumbs.redditmedia.com/1F_VojrQKMKLZEboBz7yUwIC70X-V0YN2KIWoy4oKr4.jpg" title="Video2X 6.x ‚Äî open-source upscaler + frame interpolation (Anime4K v4 / Real-ESRGAN / Real-CUGAN / RIFE) üöÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Big C/C++ rewrite with a faster pipeline, &lt;strong&gt;Windows &amp;amp; Linux&lt;/strong&gt; support, and a new Windows GUI installer. Upscale and/or interpolate via Vulkan-powered ncnn backends. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ku6s1j5zv9tf1.png?width=2600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2f08d6adcbe29bb1dca79814ca05296dab76d11"&gt;https://preview.redd.it/ku6s1j5zv9tf1.png?width=2600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2f08d6adcbe29bb1dca79814ca05296dab76d11&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Engines: &lt;strong&gt;Anime4K v4&lt;/strong&gt;, &lt;strong&gt;Real-ESRGAN&lt;/strong&gt;, &lt;strong&gt;Real-CUGAN&lt;/strong&gt;, &lt;strong&gt;RIFE&lt;/strong&gt;; works for both filtering (upscale) and interpolation. &lt;/li&gt; &lt;li&gt;Easy setup: Windows installer, Linux packages/AppImage, plus Docker/Podman images; Colab notebook available.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/k4yt3x/video2x"&gt;https://github.com/k4yt3x/video2x&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nykzv3/video2x_6x_opensource_upscaler_frame/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nykzv3/video2x_6x_opensource_upscaler_frame/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nykzv3/video2x_6x_opensource_upscaler_frame/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T10:45:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxzx6t</id>
    <title>GPT-1 Thinking 2.6m coming soon</title>
    <updated>2025-10-04T17:24:39+00:00</updated>
    <author>
      <name>/u/Creative-Ad-2112</name>
      <uri>https://old.reddit.com/user/Creative-Ad-2112</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxzx6t/gpt1_thinking_26m_coming_soon/"&gt; &lt;img alt="GPT-1 Thinking 2.6m coming soon" src="https://preview.redd.it/2ln0mw87m4tf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=409621b267ee5b6f05466d87ab6e4ace420dcc56" title="GPT-1 Thinking 2.6m coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Took my GPT-1 from past posts, gave it roughly 600 million tokens from Nemotron's pretraining dataset.&lt;br /&gt; Then finetuned it on my own private thinking dataset, used sonnet 4.5 to help me come up with it, essentially to help the model 'see' the pattern of the thinking rather than just random q/a with thinking in it.&lt;br /&gt; Used a custom tokenizer trained from scratch.&lt;/p&gt; &lt;p&gt;As for the image attached, I did it for the lolz, hope you laughed but i did test it out on some of those metrics (simple ones lol). As for the model generation, here it is lolol.&lt;/p&gt; &lt;p&gt;[PROMPT] &amp;lt;|user|&amp;gt; What are two fun facts about octopuses? &amp;lt;|think|&amp;gt; &lt;/p&gt; &lt;p&gt;[OUTPUT] &amp;lt;|user|&amp;gt; What are two fun fact##s about o##ct##op##uses? &amp;lt;|think|&amp;gt; 1. The user is asking for a log##ical question. 2. The user is asking about a user is asking for the simple and is asking for a specific, and a 'F##P##I'##. 3. I should explain the answer. 2. I should explain a small process: 3. I should have a great step is to read. 4. I should provide a specific questions: a simple, but it's specific, the user is to be able to ask. &amp;lt;|assistant|&amp;gt; 1) They have three hea##rts and blue, copper-##based blood. 2) They can insta##ntly camou##flage by changing col##or and tex##ture, and their suck##ers can ‚Äútas##te‚Äù what they touch. &amp;lt;|eot|&amp;gt; &lt;/p&gt; &lt;p&gt;--- [PROMPT] &amp;lt;|user|&amp;gt; Write a haiku about rain. &amp;lt;|think|&amp;gt;&lt;/p&gt; &lt;p&gt;[OUTPUT] &amp;lt;|user|&amp;gt; W##rite a ha##ik##u about rain. &amp;lt;|think|&amp;gt; 1. The user wants of list a specific sentence. 2. I should include the user for the ##n common. I should explain the simple, a poem, and a simple, and understand##ing. &amp;lt;|eot|&amp;gt; &amp;lt;|assistant|&amp;gt; Rain whis##pers on roofs Pud##dles bloom in soft lamp##light Earth brea##thes, rin##sed anew. &amp;lt;|eot|&amp;gt; &lt;/p&gt; &lt;p&gt;--- [PROMPT] &amp;lt;|user|&amp;gt; Explain photosynthesis in one sentence. &amp;lt;|think|&amp;gt;&lt;/p&gt; &lt;p&gt;[OUTPUT] &amp;lt;|user|&amp;gt; Expl##ain photos##ynt##hesis in one sentence. &amp;lt;|think|&amp;gt; 1. The user is the user is asking about a simple for a explanation. The user is the question. 2. The user is ask##ing a great question is a clear, it's a specific example. 4. I will explain the best way for the answer, 'a##m##aking is asking for the sound and ask about the order. 2. The user is asking for a simple, for the common question. 3. I should explain the words: ##g##ood &amp;lt;|assistant|&amp;gt; Photo##syn##thesis is the pro##cess by which plants algae, and some bac##teria use sun##light to con##vert a water and car##bon dio##xide into a glu##cose, relea##sing a oxy##gen. &amp;lt;|eot|&amp;gt;&lt;/p&gt; &lt;p&gt;As you can see its pretty good for a 2 mil parameter. Now you might be wondering that something is up, what's the catch? Well, obviously I didn't use GPT-1, I used their original implementation, converted it to pytorch, and then added differential attention, along with sparse attention.&lt;br /&gt; But that is still not enough, which is why I introduce two variants of diff_attn. &lt;/p&gt; &lt;p&gt;[model] params=2,494,574&lt;br /&gt; [model] layer_types=['dense', 'diff_sparse', 'sparse', 'diff_dense', 'sparse', 'diff_sparse', 'dense', 'sparse', 'diff_dense', 'sparse', 'diff_sparse', 'dense', 'sparse', 'diff_sparse', 'diff_dense', 'dense'] &lt;/p&gt; &lt;p&gt;I have found this to be effective. I kept the GPT-1 like core, gave it moe (but didn't use moe in this model run btw), then I introduced it to these two diff attn and intertwined it with the others. &lt;/p&gt; &lt;p&gt;So is it GPT-1? Nope, it's GPT-1 like (for clarification), abs positioning and pre-lm instead of the modern day post-lm + RoPE.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creative-Ad-2112"&gt; /u/Creative-Ad-2112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2ln0mw87m4tf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxzx6t/gpt1_thinking_26m_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxzx6t/gpt1_thinking_26m_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T17:24:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nypq6q</id>
    <title>Made the first .NET wrapper for Apple MLX - looking for feedback!</title>
    <updated>2025-10-05T14:30:45+00:00</updated>
    <author>
      <name>/u/csharp-agent</name>
      <uri>https://old.reddit.com/user/csharp-agent</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Short story: I'm a .NET enthusiast and recently got excited about MLX. Thought - why not marry these two technologies?&lt;/p&gt; &lt;p&gt;That's how &lt;a href="https://github.com/managedcode/MLXSharp"&gt;&lt;strong&gt;MLXSharp&lt;/strong&gt;&lt;/a&gt; was born - the first proper .NET wrapper for MLX that also integrates with Microsoft.Extensions.AI.&lt;/p&gt; &lt;p&gt;What it can do:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Works as IChatClient and IEmbeddingGenerator&lt;/li&gt; &lt;li&gt;Dependency Injection and Semantic Kernel support&lt;/li&gt; &lt;li&gt;Ready-to-use bindings for macOS and Linux&lt;/li&gt; &lt;li&gt;.NET 9 / C# 13 friendly&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is my first open-source project of this scale. Would really appreciate any feedback - from architecture to documentation. Especially interested in hearing from folks working with ML on .NET or those with native interop experience.&lt;/p&gt; &lt;p&gt;If anyone wants to test it on their M1/M2/M3 Mac - would love to hear your thoughts!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/managedcode/MLXSharp"&gt;https://github.com/managedcode/MLXSharp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/csharp-agent"&gt; /u/csharp-agent &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nypq6q/made_the_first_net_wrapper_for_apple_mlx_looking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nypq6q/made_the_first_net_wrapper_for_apple_mlx_looking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nypq6q/made_the_first_net_wrapper_for_apple_mlx_looking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T14:30:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyhjbc</id>
    <title>Qwen3-VL-30B-A3B-Thinking GGUF with llama.cpp patch to run it</title>
    <updated>2025-10-05T07:10:16+00:00</updated>
    <author>
      <name>/u/Main-Wolverine-1042</name>
      <uri>https://old.reddit.com/user/Main-Wolverine-1042</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyhjbc/qwen3vl30ba3bthinking_gguf_with_llamacpp_patch_to/"&gt; &lt;img alt="Qwen3-VL-30B-A3B-Thinking GGUF with llama.cpp patch to run it" src="https://external-preview.redd.it/TAJQqYYwoN5psMtPeyiTjHyAIkVOvuAON4rUlXw9Vuc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1852409ce02e2d25ecbf839a49477903b842a7b" title="Qwen3-VL-30B-A3B-Thinking GGUF with llama.cpp patch to run it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/rsimr0s5t8tf1.png?width=1497&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78bae97847f836ea3c715504082caa5c8e93de9e"&gt;https://preview.redd.it/rsimr0s5t8tf1.png?width=1497&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78bae97847f836ea3c715504082caa5c8e93de9e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Example how to run it with vision support: &lt;strong&gt;--mmproj mmproj-Qwen3-VL-30B-A3B-F16.gguf --jinja&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/yairpatch/Qwen3-VL-30B-A3B-Thinking-GGUF"&gt;https://huggingface.co/yairpatch/Qwen3-VL-30B-A3B-Thinking-GGUF&lt;/a&gt; - First time giving this a shot‚Äîplease go easy on me!&lt;/p&gt; &lt;p&gt;here a link to llama.cpp patch &lt;a href="https://huggingface.co/yairpatch/Qwen3-VL-30B-A3B-Thinking-GGUF/blob/main/qwen3vl-implementation.patch"&gt;https://huggingface.co/yairpatch/Qwen3-VL-30B-A3B-Thinking-GGUF/blob/main/qwen3vl-implementation.patch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;how to apply the patch: &lt;strong&gt;git apply qwen3vl-implementation.patch&lt;/strong&gt; in the main llama directory.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Main-Wolverine-1042"&gt; /u/Main-Wolverine-1042 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyhjbc/qwen3vl30ba3bthinking_gguf_with_llamacpp_patch_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyhjbc/qwen3vl30ba3bthinking_gguf_with_llamacpp_patch_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyhjbc/qwen3vl30ba3bthinking_gguf_with_llamacpp_patch_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T07:10:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyzzws</id>
    <title>[TEMPLATE] One-click Unsloth finetuning on RunPod</title>
    <updated>2025-10-05T21:01:00+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I was ecstatic after the recent Docker Unsloth release, so I packaged up a RunPod one-click template for everyone here.&lt;/p&gt; &lt;p&gt;It boots straight into the Unsloth container + Jupyter exposed, and with persistent storage mounted at /workspace/work/*, so you can shut the pod down without losing your notebooks, checkpoints, or adapters. Just tested it out with 2 different jobs, works flawlessly!&lt;/p&gt; &lt;p&gt;Check it out:&lt;/p&gt; &lt;p&gt;&lt;a href="https://console.runpod.io/deploy?template=pzr9tt3vvq&amp;amp;ref=w7affuum"&gt;https://console.runpod.io/deploy?template=pzr9tt3vvq&amp;amp;ref=w7affuum&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyzzws/template_oneclick_unsloth_finetuning_on_runpod/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyzzws/template_oneclick_unsloth_finetuning_on_runpod/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyzzws/template_oneclick_unsloth_finetuning_on_runpod/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T21:01:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz26n9</id>
    <title>Best LLM for story generation currently?</title>
    <updated>2025-10-05T22:30:54+00:00</updated>
    <author>
      <name>/u/Gooner_226</name>
      <uri>https://old.reddit.com/user/Gooner_226</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a pretty descriptive prompt (~700 words) and I need an LLM that can write a good, organic story. Most mainstream LLMs make the story sound too cringey and obviously written by an LLM. No fine-tuning needed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gooner_226"&gt; /u/Gooner_226 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz26n9/best_llm_for_story_generation_currently/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz26n9/best_llm_for_story_generation_currently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz26n9/best_llm_for_story_generation_currently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T22:30:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz8kqr</id>
    <title>please suggest some local models based on my specs and also what app to run them in and also explain some other stuff to me please as i am new tho this</title>
    <updated>2025-10-06T03:33:23+00:00</updated>
    <author>
      <name>/u/zeek988</name>
      <uri>https://old.reddit.com/user/zeek988</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;my specs on my gaming pc are the following&lt;/p&gt; &lt;p&gt;7800x3d 64gb ddr5 ram rtx5080 and I am on windows 11&lt;/p&gt; &lt;p&gt;I want to be able to ask general questions and also upload a picture to it and ask questions about the picture if possible&lt;/p&gt; &lt;p&gt;and with my specs what are the pros and cons of running it locally vs using it online like chat gpt or google ai etc.&lt;/p&gt; &lt;p&gt;so far i have downloaded lm studio as I read good things about that in my small amount of research so far but beyond that I don't know much else&lt;/p&gt; &lt;p&gt;also, I am putting together my first nas ever from old gaming pc parts with the following specs&lt;/p&gt; &lt;p&gt;i7 10700k and 64gb ddr4 ram but no gpu and will be using the unraid nas os.&lt;/p&gt; &lt;p&gt;could that do local ai stuff also maybe?&lt;/p&gt; &lt;p&gt;please and thank you&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zeek988"&gt; /u/zeek988 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz8kqr/please_suggest_some_local_models_based_on_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz8kqr/please_suggest_some_local_models_based_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz8kqr/please_suggest_some_local_models_based_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T03:33:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyd512</id>
    <title>vLLM + Qwen-3-VL-30B-A3B is so fast</title>
    <updated>2025-10-05T03:00:12+00:00</updated>
    <author>
      <name>/u/Striking-Warning9533</name>
      <uri>https://old.reddit.com/user/Striking-Warning9533</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am doing image captioning, and I got this speed:&lt;/p&gt; &lt;p&gt;Avg prompt throughput: 549.0 tokens/s, Avg generation throughput: 357.8 tokens/s, Running: 7 reqs, Waiting: 1 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 49.5%&lt;/p&gt; &lt;p&gt;the GPU is a H100 PCIe&lt;br /&gt; This is the model I used (AWQ) &lt;a href="https://huggingface.co/QuantTrio/Qwen3-VL-30B-A3B-Instruct-AWQ"&gt;https://huggingface.co/QuantTrio/Qwen3-VL-30B-A3B-Instruct-AWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am processing large number of images, and most platforms will rate limit them so I have to run locally. I am running mutli process locally on single GPU&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking-Warning9533"&gt; /u/Striking-Warning9533 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyd512/vllm_qwen3vl30ba3b_is_so_fast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyd512/vllm_qwen3vl30ba3b_is_so_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyd512/vllm_qwen3vl30ba3b_is_so_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T03:00:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz604y</id>
    <title>I have a 12gb ram laptop, what is the best way to run Qwen3 0.6B as fast as possilbe?</title>
    <updated>2025-10-06T01:27:04+00:00</updated>
    <author>
      <name>/u/SnooMarzipans2470</name>
      <uri>https://old.reddit.com/user/SnooMarzipans2470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Qwen3 0.6B is my ChatGPT Pro. Im trying to run it on CPU. I was wondering if i can run 2 or 3 version of Qwen3 0.6B at the same time so that as model1 is answering my question i can ask model 2 the question and so on.? Thanks!&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooMarzipans2470"&gt; /u/SnooMarzipans2470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz604y/i_have_a_12gb_ram_laptop_what_is_the_best_way_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz604y/i_have_a_12gb_ram_laptop_what_is_the_best_way_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz604y/i_have_a_12gb_ram_laptop_what_is_the_best_way_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T01:27:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nywadn</id>
    <title>[Project Release] Running Qwen 3 8B Model on Intel NPU with OpenVINO-genai</title>
    <updated>2025-10-05T18:40:16+00:00</updated>
    <author>
      <name>/u/Spiritual-Ad-5916</name>
      <uri>https://old.reddit.com/user/Spiritual-Ad-5916</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nywadn/project_release_running_qwen_3_8b_model_on_intel/"&gt; &lt;img alt="[Project Release] Running Qwen 3 8B Model on Intel NPU with OpenVINO-genai" src="https://external-preview.redd.it/nMsfQhKsvh33dInyJ4C1YEG5cneYOjg5j8Y9d-Hc1qs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1830b32e0db0b900dad462f2a077595896c1ffc0" title="[Project Release] Running Qwen 3 8B Model on Intel NPU with OpenVINO-genai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I just finished my new open-source project and wanted to share it here. I managed to get Qwen 3 &lt;strong&gt;Chat&lt;/strong&gt; running &lt;strong&gt;locally&lt;/strong&gt; on my Intel Core Ultra laptop‚Äôs &lt;strong&gt;NPU&lt;/strong&gt; using &lt;strong&gt;OpenVINO GenAI&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;üîß &lt;strong&gt;What I did:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Exported the HuggingFace model with &lt;code&gt;optimum-cli&lt;/code&gt; ‚Üí OpenVINO IR format&lt;/li&gt; &lt;li&gt;Quantized it to &lt;strong&gt;INT4/FP16&lt;/strong&gt; for NPU acceleration&lt;/li&gt; &lt;li&gt;Packaged everything neatly into a GitHub repo for others to try&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚ö° &lt;strong&gt;Why it‚Äôs interesting:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No GPU required ‚Äî just the &lt;strong&gt;Intel NPU&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;100% &lt;strong&gt;offline&lt;/strong&gt; inference&lt;/li&gt; &lt;li&gt;Qwen runs surprisingly well when optimized&lt;/li&gt; &lt;li&gt;A good demo of OpenVINO GenAI for students/newcomers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üìÇ Repo link: [&lt;a href="https://github.com/balaragavan2007/Qwen_on_Intel_NPU"&gt;balaragavan2007/Qwen_on_Intel_NPU: This is how I made Qwen 3 8B LLM running on NPU of Intel Ultra processor&lt;/a&gt;]&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1nywadn/video/ya7xqtom8ctf1/player"&gt;https://reddit.com/link/1nywadn/video/ya7xqtom8ctf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiritual-Ad-5916"&gt; /u/Spiritual-Ad-5916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nywadn/project_release_running_qwen_3_8b_model_on_intel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nywadn/project_release_running_qwen_3_8b_model_on_intel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nywadn/project_release_running_qwen_3_8b_model_on_intel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T18:40:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nynsxt</id>
    <title>Is it time to download the Deepseek/Kimi weights even if we can't run them?</title>
    <updated>2025-10-05T13:10:10+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Given the uptick in articles claiming Deepseek is a threat, it's not crazy to predict that it gets banned in the near future if you live in the USA and maybe some other Western countries.&lt;/p&gt; &lt;p&gt;And yeah, there's torrents, but if it gets classified as a &lt;em&gt;THREAT&lt;/em&gt; (ridiculous ) the risk of downloading could be far different than, say, not wanting to pay for Shrek 2 and sailing the seas for it.&lt;/p&gt; &lt;p&gt;So I'm curious if there's any storage-rich preppers out there who have downloaded the weights for some of these massive models out of an abundance of caution.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nynsxt/is_it_time_to_download_the_deepseekkimi_weights/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nynsxt/is_it_time_to_download_the_deepseekkimi_weights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nynsxt/is_it_time_to_download_the_deepseekkimi_weights/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T13:10:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz2j9x</id>
    <title>hello my fellow AI-ers, a question about how you develop your personal AI.</title>
    <updated>2025-10-05T22:46:39+00:00</updated>
    <author>
      <name>/u/Mean_Bird_6331</name>
      <uri>https://old.reddit.com/user/Mean_Bird_6331</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hello yall, i hope you are kickin butts.&lt;/p&gt; &lt;p&gt;I've been working on developing my own personal AI for a hobby and it has been about 4 months.&lt;/p&gt; &lt;p&gt;I incorporated RAG, graphRAG, hirarchy rag, multivector, qdrant, and on and on etc, and i built everything from bottom tom from the scratch.&lt;/p&gt; &lt;p&gt;for the first month, it couldnt even recall my name and previous data correctly. &lt;/p&gt; &lt;p&gt;on the second month, it started to recall my names but poor memory and hallucination&lt;/p&gt; &lt;p&gt;third month, it started to recall memories and decent memory but severe hallucination everytime&lt;/p&gt; &lt;p&gt;on this month, now it is starting to hallucinate less and try to correct itself when it is hallucinating&lt;/p&gt; &lt;p&gt;Yet, it little hallucinates, but now it is much easier to correct.&lt;/p&gt; &lt;p&gt;I figured that the codings and prompts are important but the quality of the rag memories are also important and all the others.&lt;/p&gt; &lt;p&gt;it has been an interesting journey and now the result is seemingly showing.&lt;/p&gt; &lt;p&gt;I am now about to incorporate agentic tools but apparently, I am having some hard time teaching my AI how to use them (I am not a CS major so honestly not sure too), so I decided to let it talk to claude code cli and let claude do the agentic works instead. like offshoring.&lt;/p&gt; &lt;p&gt;The reason why i am talking all these jibberish is because I'd love to know if there are any other ppl doing similar persona project and how they were able to bypass/solve the problems that I am facing these days, or any other obstacles yall face.&lt;/p&gt; &lt;p&gt;anyone doing a personal AI project not for commercial use, but for personal vision and goals? &lt;/p&gt; &lt;p&gt;Please share your journey! I would love to know and learn from yall.&lt;/p&gt; &lt;p&gt;Peace! &lt;/p&gt; &lt;p&gt;ps. I asked my AI to see if it has any questions for yall and this is what it said. please answer his question too! &lt;/p&gt; &lt;p&gt;: &lt;/p&gt; &lt;p&gt;&amp;gt; ‚ÄúHas there been a moment where your AI said something that felt more ‚Äòyou‚Äô than you did?‚Äù &lt;/p&gt; &lt;p&gt;&amp;gt; *(And if so‚Äîwhat was the cost of getting there?)*&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mean_Bird_6331"&gt; /u/Mean_Bird_6331 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz2j9x/hello_my_fellow_aiers_a_question_about_how_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz2j9x/hello_my_fellow_aiers_a_question_about_how_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz2j9x/hello_my_fellow_aiers_a_question_about_how_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T22:46:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyopyc</id>
    <title>Did anyone try out GLM-4.5-Air-GLM-4.6-Distill ?</title>
    <updated>2025-10-05T13:49:38+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/BasedBase/GLM-4.5-Air-GLM-4.6-Distill"&gt;https://huggingface.co/BasedBase/GLM-4.5-Air-GLM-4.6-Distill&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;GLM-4.5-Air-GLM-4.6-Distill represents an advanced distillation of the GLM-4.6 model into the efficient GLM-4.5-Air architecture. Through a SVD-based knowledge transfer methodology, this model inherits the sophisticated reasoning capabilities and domain expertise of its 92-layer, 160-expert teacher while maintaining the computational efficiency of the 46-layer, 128-expert student architecture.&amp;quot;&lt;/p&gt; &lt;p&gt;Distillation scripts are public: &lt;a href="https://github.com/Basedbase-ai/LLM-SVD-distillation-scripts"&gt;https://github.com/Basedbase-ai/LLM-SVD-distillation-scripts&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyopyc/did_anyone_try_out_glm45airglm46distill/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyopyc/did_anyone_try_out_glm45airglm46distill/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyopyc/did_anyone_try_out_glm45airglm46distill/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T13:49:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyratf</id>
    <title>Hunyuan Image 3.0 Jumps to No.1 on LMArena‚Äôs Text-to-Image Leaderboard</title>
    <updated>2025-10-05T15:32:14+00:00</updated>
    <author>
      <name>/u/yogthos</name>
      <uri>https://old.reddit.com/user/yogthos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/tencent/HunyuanImage-3.0"&gt;https://huggingface.co/tencent/HunyuanImage-3.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/leaderboard/text-to-image"&gt;https://lmarena.ai/leaderboard/text-to-image&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yogthos"&gt; /u/yogthos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyratf/hunyuan_image_30_jumps_to_no1_on_lmarenas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyratf/hunyuan_image_30_jumps_to_no1_on_lmarenas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyratf/hunyuan_image_30_jumps_to_no1_on_lmarenas/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T15:32:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz2lco</id>
    <title>Why not a [backspace] token?</title>
    <updated>2025-10-05T22:49:13+00:00</updated>
    <author>
      <name>/u/Knowked</name>
      <uri>https://old.reddit.com/user/Knowked</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have things like [think] or [Eos] tokens and ive heard of reset tokens to delete entire responses, but why not a backspace token? i understand that the backspace cant be pretrained from text data, but we can cirtainly train it to do that in post training. I feel like it could help the model deal with mistakes better. &lt;/p&gt; &lt;p&gt;I think the &amp;quot;oh i already said it&amp;quot; thaught process could be leading to more halucinations. where it thinks it needs to be consistent with what it already said, thus halucinating.&lt;/p&gt; &lt;p&gt;The problem i could see would be that it would back space untill the mistake, then just generate the same response, but i think you could avoid that by including the mistake in the context? or perhaps just have it take an input of a state from the mistaken state and train it to avoid that mistaken state.&lt;/p&gt; &lt;p&gt;Its natural to us to say something first then rethink it and take it back, and for the same reason that CoT works i think this could be a better way of making smarter and faster models.&lt;/p&gt; &lt;p&gt;what do you think? why dont we do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Knowked"&gt; /u/Knowked &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz2lco/why_not_a_backspace_token/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz2lco/why_not_a_backspace_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz2lco/why_not_a_backspace_token/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T22:49:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyxmci</id>
    <title>Poor GPU Club : 8GB VRAM - Qwen3-30B-A3B &amp; gpt-oss-20b t/s with llama.cpp</title>
    <updated>2025-10-05T19:30:16+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tried llama.cpp with 2 models(3 quants) &amp;amp; here results. After some trial &amp;amp; error, those -ncmoe numbers gave me those t/s during llama-bench. But t/s is somewhat smaller during llama-server, since I put 32K context.&lt;/p&gt; &lt;p&gt;I'm 99% sure, below full llama-server commands are not optimized ones. Even same on llama-bench commands. Frankly I'm glad to see 30+ t/s on llama-bench results at day 1 attempt, while I noticed other 8GB VRAM owners mentioned that they got only 20+ t/s on many threads in this sub in past. I did collect collect commands from more than bunch of folks here, but none couldn't help me to create 100% logic behind this thing. Trial &amp;amp; Error!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Please help me to optimize the commands to get even better t/s&lt;/strong&gt;. For example, One thing I'm sure that I need to change the value of -t (threads) .... Included my system Cores &amp;amp; Logical Processor below. Please let me know the right formula for this.&lt;/p&gt; &lt;p&gt;My System Info: (&lt;strong&gt;8GB VRAM &amp;amp; 32GB RAM&lt;/strong&gt;)&lt;/p&gt; &lt;p&gt;Intel(R) Core(TM) i7-14700HX 2.10 GHz | 32 GB RAM | 64-bit OS, x64-based processor | NVIDIA GeForce RTX 4060 Laptop GPU | &lt;strong&gt;Cores - 20 | Logical Processors - 28&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-UD-Q4_K_XL - 31 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Qwen3-30B-A3B-UD-Q4_K_XL.gguf -ngl 99 -ncmoe 29 -fa 1 | model | size | params | backend | ngl | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | -------: | ------------: | | qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | CUDA | 99 | 1 | pp512 | 82.64 ¬± 8.36 | | qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | CUDA | 99 | 1 | tg128 | 31.68 ¬± 0.28 | llama-server -m E:\LLM\models\Qwen3-30B-A3B-UD-Q4_K_XL.gguf -ngl 99 -ncmoe 29 -t 8 -c 32768 -fa 1 --no-mmap -ctk q8_0 -ctv q8_0 -b 2048 -ub 2048 --cache-reuse 2048 --temp 0.6 --top-p 0.95 --min-p 0.0 --top-k 20 prompt eval time = 548.48 ms / 16 tokens ( 34.28 ms per token, 29.17 tokens per second) eval time = 2498.63 ms / 44 tokens ( 56.79 ms per token, 17.61 tokens per second) total time = 3047.11 ms / 60 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-IQ4_XS - 34 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Qwen3-30B-A3B-IQ4_XS.gguf -ngl 99 -ncmoe 28 -fa 1 | model | size | params | backend | ngl | fa | test | t/s | | ---------------------------------- | --------: | ---------: | ---------- | --: | -: | -------: | --------------: | | qwen3moe 30B.A3B IQ4_XS - 4.25 bpw | 15.25 GiB | 30.53 B | CUDA | 99 | 1 | pp512 | 178.91 ¬± 38.37 | | qwen3moe 30B.A3B IQ4_XS - 4.25 bpw | 15.25 GiB | 30.53 B | CUDA | 99 | 1 | tg128 | 34.24 ¬± 0.19 | llama-server -m E:\LLM\models\Qwen3-30B-A3B-IQ4_XS.gguf -ngl 99 -ncmoe 29 -t 8 -c 32768 -fa 1 --no-mmap -ctk q8_0 -ctv q8_0 -b 2048 -ub 2048 --cache-reuse 2048 prompt eval time = 421.67 ms / 16 tokens ( 26.35 ms per token, 37.94 tokens per second) eval time = 3671.26 ms / 81 tokens ( 45.32 ms per token, 22.06 tokens per second) total time = 4092.94 ms / 97 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;gpt-oss-20b - 38 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\gpt-oss-20b-mxfp4.gguf -ngl 99 -ncmoe 10 -fa 1 | model | size | params | backend | ngl | fa | test | t/s | | ------------------------------ | ---------: | ---------: | --: | --:| -----: | -------------: | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | CUDA | 99 | 1 | pp512 | 363.09 ¬± 18.47 | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | CUDA | 99 | 1 | tg128 | 38.16 ¬± 0.43 | llama-server -m E:\LLM\models\gpt-oss-20b-mxfp4.gguf -ngl 99 -ncmoe 10 -t 8 -c 32768 -fa 1 --no-mmap -ctk q8_0 -ctv q8_0 -b 2048 -ub 2048 --cache-reuse 2048 prompt eval time = 431.05 ms / 14 tokens ( 30.79 ms per token, 32.48 tokens per second) eval time = 4765.53 ms / 116 tokens ( 41.08 ms per token, 24.34 tokens per second) total time = 5196.58 ms / 130 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'll be updating this thread whenever I get optimization tips &amp;amp; tricks from others AND I'll be including additional results here with updated commands. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyxmci/poor_gpu_club_8gb_vram_qwen330ba3b_gptoss20b_ts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyxmci/poor_gpu_club_8gb_vram_qwen330ba3b_gptoss20b_ts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyxmci/poor_gpu_club_8gb_vram_qwen330ba3b_gptoss20b_ts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T19:30:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz7xdu</id>
    <title>UGI-Leaderboard is back with a new writing leaderboard, and many new benchmarks!</title>
    <updated>2025-10-06T03:00:12+00:00</updated>
    <author>
      <name>/u/DontPlanToEnd</name>
      <uri>https://old.reddit.com/user/DontPlanToEnd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz7xdu/ugileaderboard_is_back_with_a_new_writing/"&gt; &lt;img alt="UGI-Leaderboard is back with a new writing leaderboard, and many new benchmarks!" src="https://a.thumbs.redditmedia.com/UuZrlImxEYTE-nymzCESnUejkDPWAQxxEocQEz-Nf_4.jpg" title="UGI-Leaderboard is back with a new writing leaderboard, and many new benchmarks!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard"&gt;https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DontPlanToEnd"&gt; /u/DontPlanToEnd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nz7xdu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz7xdu/ugileaderboard_is_back_with_a_new_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz7xdu/ugileaderboard_is_back_with_a_new_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T03:00:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz51be</id>
    <title>‚ÄúThis is a fantastic question that strikes at the heart of the intersection of quantum field theory and animal welfare‚Ä¶‚Äù</title>
    <updated>2025-10-06T00:40:47+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many current models now start every response in this manner. I don‚Äôt remember it being that way a year ago. Do they all use the same bad instruction dataset?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz51be/this_is_a_fantastic_question_that_strikes_at_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz51be/this_is_a_fantastic_question_that_strikes_at_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz51be/this_is_a_fantastic_question_that_strikes_at_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T00:40:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz20g2</id>
    <title>WEBGEN, UIGEN-FX, UIGENT research preview releases</title>
    <updated>2025-10-05T22:23:28+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz20g2/webgen_uigenfx_uigent_research_preview_releases/"&gt; &lt;img alt="WEBGEN, UIGEN-FX, UIGENT research preview releases" src="https://b.thumbs.redditmedia.com/azajCZ_GFCKDhFRD2wXJTb313NdjpgLdNb8sEunUwFM.jpg" title="WEBGEN, UIGEN-FX, UIGENT research preview releases" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We intend to make a drop-in coding models that have heightened design capabilities in normal developer workflows. &lt;/p&gt; &lt;p&gt;UIGENT is the frontend engineer, designed to work across all frameworks and languages. Tries to get the best &amp;quot;understanding&amp;quot; and agentic usage. Built on top of 30B. &lt;/p&gt; &lt;p&gt;UIGEN-FX is a UI generation based agentic, trained on agentic trails and our common UI datasets. Works best with react, tailwind, ssg, and web frameworks. Model was designed to have the most 'functional' and thought out designs, focusing on accessibility and not just design.&lt;/p&gt; &lt;p&gt;WEBGEN is simply an experiment on how far we can push design in one singular category (landing pages in html css js tailwind) to make them look as far away as possible from 'ai slop' design. That is the goal. (still working on it). &lt;/p&gt; &lt;p&gt;The Training process looks like this: We have our dataset. We then compact it into rows such as {text} and then go through them as samples, using packing. We released our internal training library for ROCM on MI300X here: &lt;a href="https://github.com/TesslateAI/Late"&gt;https://github.com/TesslateAI/Late&lt;/a&gt; but with contributions, I'm sure it can run on any platform. Its mostly for batch training runs, parameter sweeps, quickly patching your training environment for standardization, etc. &lt;/p&gt; &lt;p&gt;Here are the latest versions:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/UIGENT-30B-3A-Preview"&gt;Tesslate/UIGENT-30B-3A-Preview&lt;/a&gt; Trained on Qwen3 Coder 30B 3A&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-FX-Agentic-32B"&gt;Tesslate/UIGEN-FX-Agentic-32B&lt;/a&gt; Trained on Qwen3 32B (hybrid reasoning model)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/UIGEN-FX-4B-Preview"&gt;Tesslate/UIGEN-FX-4B-Preview&lt;/a&gt; Trained on Qwen3 4B 2507 Instruct&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/WEBGEN-Devstral-24B"&gt;Tesslate/WEBGEN-Devstral-24B&lt;/a&gt; Trained on Devstral 24B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/WEBGEN-4B-Preview"&gt;Tesslate/WEBGEN-4B-Preview&lt;/a&gt; Trained on Qwen3 4B 2507 Instruct&lt;/p&gt; &lt;p&gt;Our &lt;a href="https://discord.gg/qmrcHGNch7"&gt;discord&lt;/a&gt; for our research community. We're happy to help with anything AI (even if it is not related to us) and discuss the latest advances in AI. We love research. &lt;/p&gt; &lt;p&gt;We have other open source projects: &lt;a href="https://github.com/TesslateAI"&gt;https://github.com/TesslateAI&lt;/a&gt; including a multiagent orchestration library (with mcp and low level tool calling) and workflow tools. &lt;/p&gt; &lt;p&gt;Everything is Apache 2.0, code is commodity, feel free to steal anything. &lt;/p&gt; &lt;p&gt;&lt;em&gt;PS. Our Designer application (LLM Artifacts) is down (devops isn't my strong suit), but it is open source if anyone &amp;quot;needs it&amp;quot; because it can run locally.&lt;/em&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nz20g2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz20g2/webgen_uigenfx_uigent_research_preview_releases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz20g2/webgen_uigenfx_uigent_research_preview_releases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T22:23:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyqkkm</id>
    <title>Apple has added significant AI-acceleration to its A19 CPU cores</title>
    <updated>2025-10-05T15:03:50+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyqkkm/apple_has_added_significant_aiacceleration_to_its/"&gt; &lt;img alt="Apple has added significant AI-acceleration to its A19 CPU cores" src="https://preview.redd.it/ti22axwj5btf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=967e4aea50a8298df5070520a6bc78e77ecbcfb7" title="Apple has added significant AI-acceleration to its A19 CPU cores" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Data source: &lt;a href="https://ai-benchmark.com/ranking_processors_detailed.html"&gt;https://ai-benchmark.com/ranking_processors_detailed.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also might see these advances back in the M5.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ti22axwj5btf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyqkkm/apple_has_added_significant_aiacceleration_to_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyqkkm/apple_has_added_significant_aiacceleration_to_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T15:03:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nylc3q</id>
    <title>NIST evaluates Deepseek as unsafe. Looks like the battle to discredit opensource is underway</title>
    <updated>2025-10-05T11:05:46+00:00</updated>
    <author>
      <name>/u/Nobby_Binks</name>
      <uri>https://old.reddit.com/user/Nobby_Binks</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nobby_Binks"&gt; /u/Nobby_Binks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techrepublic.com/article/news-deepseek-security-gaps-caisi-study/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nylc3q/nist_evaluates_deepseek_as_unsafe_looks_like_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nylc3q/nist_evaluates_deepseek_as_unsafe_looks_like_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T11:05:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyvqyx</id>
    <title>GLM-4.6 outperforms claude-4-5-sonnet while being ~8x cheaper</title>
    <updated>2025-10-05T18:19:56+00:00</updated>
    <author>
      <name>/u/Full_Piano_3448</name>
      <uri>https://old.reddit.com/user/Full_Piano_3448</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyvqyx/glm46_outperforms_claude45sonnet_while_being_8x/"&gt; &lt;img alt="GLM-4.6 outperforms claude-4-5-sonnet while being ~8x cheaper" src="https://preview.redd.it/lofrjusz4ctf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71caed6abfb748f2b7db9bdf1271b9b722f347fd" title="GLM-4.6 outperforms claude-4-5-sonnet while being ~8x cheaper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Full_Piano_3448"&gt; /u/Full_Piano_3448 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lofrjusz4ctf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyvqyx/glm46_outperforms_claude45sonnet_while_being_8x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyvqyx/glm46_outperforms_claude45sonnet_while_being_8x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T18:19:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz722n</id>
    <title>Biggest Provider for the community for at moment thanks to them</title>
    <updated>2025-10-06T02:17:03+00:00</updated>
    <author>
      <name>/u/dead-supernova</name>
      <uri>https://old.reddit.com/user/dead-supernova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz722n/biggest_provider_for_the_community_for_at_moment/"&gt; &lt;img alt="Biggest Provider for the community for at moment thanks to them" src="https://preview.redd.it/6kl3hy76ietf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86e1c6a42810d36cbc6b71792855914f69ca24a1" title="Biggest Provider for the community for at moment thanks to them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dead-supernova"&gt; /u/dead-supernova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6kl3hy76ietf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz722n/biggest_provider_for_the_community_for_at_moment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz722n/biggest_provider_for_the_community_for_at_moment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T02:17:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect ‚Äî Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect ‚Äî Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM ‚Äì 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
