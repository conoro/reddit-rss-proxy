<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-13T17:48:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ng0grl</id>
    <title>RTX 3060 with cpu offloading rig</title>
    <updated>2025-09-13T15:32:06+00:00</updated>
    <author>
      <name>/u/PloscaruRadu</name>
      <uri>https://old.reddit.com/user/PloscaruRadu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So right now I have a workstation with an rtx 3060 12 gb and 24 gb of ddr3 ram I've been using for running small models like qwen 3 14b and gemma 3 12b but i've been thinking about upgrading to a rig with 64/128 gb of ddr4 ram, mainly for using MoE models like the new qwen 3-next 80b or gpt-oss 120b. Loading them into ram the active experts on the gpu. Will the performance be abysmal or usable? I mean like 3-5 tks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PloscaruRadu"&gt; /u/PloscaruRadu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng0grl/rtx_3060_with_cpu_offloading_rig/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng0grl/rtx_3060_with_cpu_offloading_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ng0grl/rtx_3060_with_cpu_offloading_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T15:32:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfqna6</id>
    <title>MoE Total/Active parameter coefficient. How much further can it go?</title>
    <updated>2025-09-13T06:49:17+00:00</updated>
    <author>
      <name>/u/ihatebeinganonymous</name>
      <uri>https://old.reddit.com/user/ihatebeinganonymous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. So far, with Qwen 30B-A3B etc, the ratio between active and total parameters was at a certain range. But with the new Next model, that range has broken.&lt;/p&gt; &lt;p&gt;We have jumped from 10x to ~27x. How much further can it go? What are the limiting factors? Do you imagine e.g. a 300B-3B MoE model? If yes, what would be the equivalent dense parameter count?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihatebeinganonymous"&gt; /u/ihatebeinganonymous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqna6/moe_totalactive_parameter_coefficient_how_much/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqna6/moe_totalactive_parameter_coefficient_how_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqna6/moe_totalactive_parameter_coefficient_how_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T06:49:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfrz5y</id>
    <title>Anyone had any success running local LLMs on a console?</title>
    <updated>2025-09-13T08:11:30+00:00</updated>
    <author>
      <name>/u/Junior-Ad-2186</name>
      <uri>https://old.reddit.com/user/Junior-Ad-2186</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This morning I got a random thought. I haven't really been playing my Xbox (Series S) recently, but wondered if I could use it for some type of small LLM.&lt;/p&gt; &lt;p&gt;I get that this is more of a software limitation more than anything, but it'd be pretty cool if some type of jailbroken version could run Ollama and/or LMStudio, etc..&lt;/p&gt; &lt;p&gt;I feel like the hardware is there! It just sucks that the software is holding it back (as is common in tech lol)&lt;/p&gt; &lt;p&gt;I know it only has ~10GB of RAM, but you could probably run 8B models on this pretty happily? It's got a decent GPU afaict (and the Xbox Series X would be even better)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Junior-Ad-2186"&gt; /u/Junior-Ad-2186 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfrz5y/anyone_had_any_success_running_local_llms_on_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfrz5y/anyone_had_any_success_running_local_llms_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfrz5y/anyone_had_any_success_running_local_llms_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T08:11:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf6s0w</id>
    <title>Qwen3 Next and DeepSeek V3.1 share an identical Artificial Analysis Intelligence Index Score for both their reasoning and non-reasoning modes.</title>
    <updated>2025-09-12T15:48:49+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf6s0w/qwen3_next_and_deepseek_v31_share_an_identical/"&gt; &lt;img alt="Qwen3 Next and DeepSeek V3.1 share an identical Artificial Analysis Intelligence Index Score for both their reasoning and non-reasoning modes." src="https://preview.redd.it/hb62e80c7rof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45fbd3055204b4282742bcaf6567d07ade494ed5" title="Qwen3 Next and DeepSeek V3.1 share an identical Artificial Analysis Intelligence Index Score for both their reasoning and non-reasoning modes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hb62e80c7rof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf6s0w/qwen3_next_and_deepseek_v31_share_an_identical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf6s0w/qwen3_next_and_deepseek_v31_share_an_identical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T15:48:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfxshl</id>
    <title>Private browser AI chatbot</title>
    <updated>2025-09-13T13:40:51+00:00</updated>
    <author>
      <name>/u/Acceptable-Staff271</name>
      <uri>https://old.reddit.com/user/Acceptable-Staff271</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, recently I came across the idea of building a PWA to run open source AI models like LLama and Deepseek, while all your chats and information stay on your device.&lt;/p&gt; &lt;p&gt;It'll be a PWA because I still like the idea of accessing the AI from a browser, and there's no downloading or complex setup process (so you can also use it in public computers on incognito mode).&lt;/p&gt; &lt;p&gt;Curious as to whether people would want to use it over existing options like ChatGPT and Ollama + Open webUI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable-Staff271"&gt; /u/Acceptable-Staff271 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfxshl/private_browser_ai_chatbot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfxshl/private_browser_ai_chatbot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfxshl/private_browser_ai_chatbot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T13:40:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfaaik</id>
    <title>GPT-OSS:20b &amp; Qwen 4b are a match made in heaven for 24GB VRAM builds</title>
    <updated>2025-09-12T18:05:08+00:00</updated>
    <author>
      <name>/u/No_Information9314</name>
      <uri>https://old.reddit.com/user/No_Information9314</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just wanted to share that after experimenting with several models, most recently Qwen-30b-a3b, I found that gpt-oss:20b and qwen4b loaded into vram together provide a perfect balance of intelligence and speed, with space for about 30k of KV cache. I use gpt-oss for most of my work-related queries that require reasoning, and Qwen 4B generate web search queries. I also have Qwen4 running perplexica which runs very fast - (gpt-oss rather quite slow returning results). &lt;/p&gt; &lt;p&gt;Obviously YMMV but wanted to share this setup in case it may be helpful to others.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Information9314"&gt; /u/No_Information9314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfaaik/gptoss20b_qwen_4b_are_a_match_made_in_heaven_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfaaik/gptoss20b_qwen_4b_are_a_match_made_in_heaven_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfaaik/gptoss20b_qwen_4b_are_a_match_made_in_heaven_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T18:05:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng15en</id>
    <title>Where can I find training data for intent classification (chat-to-SQL bot)?</title>
    <updated>2025-09-13T15:59:53+00:00</updated>
    <author>
      <name>/u/Small-Inevitable6185</name>
      <uri>https://old.reddit.com/user/Small-Inevitable6185</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m building a &lt;strong&gt;chat-to-SQL system&lt;/strong&gt; (read-only, no inserts/updates/deletes). I want to train a &lt;strong&gt;DistilBERT-based intent classifier&lt;/strong&gt; that categorizes user queries into three classes:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Description type answer&lt;/strong&gt; → user asks about schema (e.g., “What columns are in the customers table?”)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SQL-based query filter answer&lt;/strong&gt; → user asks for data retrieval (e.g., “Show me all customers from New York.”)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Both&lt;/strong&gt; → user wants explanation + query together (e.g., “Which column stores customer age, and show me all customers older than 30?”)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My problem: I’m not sure where to get a &lt;strong&gt;dataset&lt;/strong&gt; to train this classifier. Most datasets I’ve found (ATIS, Spider, WikiSQL) are great for text-to-SQL mapping, but they don’t label queries into “description / query / both.”&lt;/p&gt; &lt;p&gt;Should I:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Try adapting text-to-SQL datasets (Spider/WikiSQL) by manually labeling a subset into my categories?&lt;/li&gt; &lt;li&gt;Or are there existing intent classification datasets closer to this use case that I might be missing?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any guidance or pointers to datasets/resources would be super helpful&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Small-Inevitable6185"&gt; /u/Small-Inevitable6185 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng15en/where_can_i_find_training_data_for_intent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng15en/where_can_i_find_training_data_for_intent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ng15en/where_can_i_find_training_data_for_intent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T15:59:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfjljo</id>
    <title>RELEASE inclusionAI/Ling-mini-2.0</title>
    <updated>2025-09-13T00:30:10+00:00</updated>
    <author>
      <name>/u/juanlndd</name>
      <uri>https://old.reddit.com/user/juanlndd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfjljo/release_inclusionailingmini20/"&gt; &lt;img alt="RELEASE inclusionAI/Ling-mini-2.0" src="https://b.thumbs.redditmedia.com/oS8LL88rsyR-C5jtj0L74JwL83T5bIh5Z_l1ri9Sv7o.jpg" title="RELEASE inclusionAI/Ling-mini-2.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys, finally a CPU-ONLY model, just need to quantize!&lt;/p&gt; &lt;p&gt;Inclusion AI released Ling-mini four days ago, and now Ring (the latter is a thought experiment).&lt;/p&gt; &lt;p&gt;16B total parameters, but only 1.4B are activated per input token (non-embedding 789M).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/npuinnm4utof1.jpg?width=2418&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8ee2e634e123fe35e911642324050e4ef1d471d5"&gt;https://preview.redd.it/npuinnm4utof1.jpg?width=2418&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8ee2e634e123fe35e911642324050e4ef1d471d5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is great news for those looking for functional solutions for use without a GPU.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanlndd"&gt; /u/juanlndd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfjljo/release_inclusionailingmini20/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfjljo/release_inclusionailingmini20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfjljo/release_inclusionailingmini20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T00:30:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfo1j4</id>
    <title>Building a Personal AI Assistant Without the Cloud (2025 Guide)</title>
    <updated>2025-09-13T04:17:06+00:00</updated>
    <author>
      <name>/u/nanhewa</name>
      <uri>https://old.reddit.com/user/nanhewa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfo1j4/building_a_personal_ai_assistant_without_the/"&gt; &lt;img alt="Building a Personal AI Assistant Without the Cloud (2025 Guide)" src="https://external-preview.redd.it/q0pbuZw5m9aebEJ2_iKV8VSARNL8LZSN6tvGWaFYcmY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac83b0c7db47906d8f0584938346fdc0b4901ecc" title="Building a Personal AI Assistant Without the Cloud (2025 Guide)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cloud assistants are convenient, but they send your data to third-party servers. In 2025 the landscape changed: lightweight open-source LLMs, efficient runtimes, and offline speech stacks make it possible to run a capable AI assistant entirely on your device. This guide walks you through planning, tools, code, and deployment so you can build a privacy-first, offline assistant that understands text and voice, controls local devices, and stays fully under your control.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nanhewa"&gt; /u/nanhewa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.lktechacademy.com/2025/09/building-personal-ai-assistant-without-cloud.html?m=1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfo1j4/building_a_personal_ai_assistant_without_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfo1j4/building_a_personal_ai_assistant_without_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T04:17:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf9x9m</id>
    <title>Apple stumbled into succes with MLX</title>
    <updated>2025-09-12T17:50:47+00:00</updated>
    <author>
      <name>/u/Alarming-Ad8154</name>
      <uri>https://old.reddit.com/user/Alarming-Ad8154</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-next 80b-a3b is out in mlx on hugging face, MLX already supports it. Open source contributors got this done within 24 hrs. Doing things apple itself couldn’t ever do quickly, simply because the call to support, or not support, specific Chinese AI companies, who’s parent company may or may not be under specific US sanctions would take months if it had the apple brand anywhere near it If apple hadn’t let MLX sort of evolve in its research arm while they tried, and failed, to manage “apple intelligence”, and pulled it into the company, closed it, centralized it, they would be nowhere now. It’s really quite a story arc and I feel with their new M5 chip design having matmul cores (faster prompt processing) they’re actually leaning into it! Apple is never the choice for sort of “go at it on your own” tinkerers, but now it actually is…&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarming-Ad8154"&gt; /u/Alarming-Ad8154 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf9x9m/apple_stumbled_into_succes_with_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf9x9m/apple_stumbled_into_succes_with_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf9x9m/apple_stumbled_into_succes_with_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T17:50:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng0fmv</id>
    <title>PSA/RFC: KV Cache quantization forces excess processing onto CPU in llama.cpp</title>
    <updated>2025-09-13T15:30:51+00:00</updated>
    <author>
      <name>/u/MutantEggroll</name>
      <uri>https://old.reddit.com/user/MutantEggroll</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking for additional comments/suggestions for optimization, since I have a very small sample size and have only been playing with GPT-OSS-120B.&lt;/p&gt; &lt;p&gt;I was struggling with GPT-OSS-120B despite my relatively high-spec hardware, only getting ~90tk/s prompt and ~10tk/s inference at 10k context. Turns out this was because quantizing the KV cache in llama.cpp seems to force the CPU to take on much more responsibility than the GPU. After only removing the KV cache quantization options, I'm now getting ~1200tk/s prompt and ~35tk/s inference at 50k context. System specs/llama.cpp commands below for reference:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;System:&lt;/strong&gt;&lt;br /&gt; CPU: Intel i9-13900K (Hyper-Threading disabled)&lt;br /&gt; RAM: 64GB DDR5-6000 (OC'd from DDR5-5400)&lt;br /&gt; GPU: NVIDIA RTX 5090 (undervolted to 890mV, driver 581.15)&lt;br /&gt; OS: Windows 11 Pro 24H2 (Build 26100.6584)&lt;br /&gt; llama.cpp Release: CUDA-12 B6318&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Initial Command (90tk/s prompt, 10tk/s inference @ 10k context):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server --threads 8 --cpu-range 0-7 --cpu-strict 1 --prio 2 --flash-attn --n-gpu-layers 999 --offline --model &amp;quot;\path\to\unsloth\gpt-oss-120b-GGUF\gpt-oss-120b-F16.gguf&amp;quot; --no-mmap --n-cpu-moe 22 --ctx-size 65536 --cache-type-k q4_0 --cache-type-v q4_0 --batch-size 2048 --ubatch-size 2048 --jinja &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Improved Command (1200tk/s prompt, 35tk/s inference @ 50k context):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server --threads 8 --cpu-range 0-7 --cpu-strict 1 --prio 2 --flash-attn --n-gpu-layers 999 --offline --model &amp;quot;\path\to\unsloth\gpt-oss-120b-GGUF\gpt-oss-120b-F16.gguf&amp;quot; --no-mmap --n-cpu-moe 22 --ctx-size 65536 --batch-size 2048 --ubatch-size 2048 --jinja &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hope this helps someone eke out a few more tk/s!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MutantEggroll"&gt; /u/MutantEggroll &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng0fmv/psarfc_kv_cache_quantization_forces_excess/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng0fmv/psarfc_kv_cache_quantization_forces_excess/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ng0fmv/psarfc_kv_cache_quantization_forces_excess/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T15:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfhbzv</id>
    <title>Ring-mini-2.0 16B 1.4b MoE</title>
    <updated>2025-09-12T22:46:15+00:00</updated>
    <author>
      <name>/u/HilLiedTroopsDied</name>
      <uri>https://old.reddit.com/user/HilLiedTroopsDied</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfhbzv/ringmini20_16b_14b_moe/"&gt; &lt;img alt="Ring-mini-2.0 16B 1.4b MoE" src="https://external-preview.redd.it/k_1ZiAjClo_PWHpZM0iAYeW3wPAsQ_ZQE2cc_xW7-3o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e8a42ffff0966a52b0cd60044e86cd61473738b" title="Ring-mini-2.0 16B 1.4b MoE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HilLiedTroopsDied"&gt; /u/HilLiedTroopsDied &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-mini-2.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfhbzv/ringmini20_16b_14b_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfhbzv/ringmini20_16b_14b_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T22:46:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfzgge</id>
    <title>Anyone put together an “oversight agent” on top of Roo Code?</title>
    <updated>2025-09-13T14:52:13+00:00</updated>
    <author>
      <name>/u/Sluggerjt44</name>
      <uri>https://old.reddit.com/user/Sluggerjt44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just came across the idea of agentic swarms and it sounds amazing. The way I understand it, you give a high-level goal and the agents keep working (coding, testing, fixing) until the thing is done.&lt;/p&gt; &lt;p&gt;Right now, I’m using Roo Code with Gemini inside VS Code and it’s pretty great, but I feel like I’m acting as the oversight layer. I have to keep nudging it step by step, almost like being the manager. What I’d love is something that's one level higher like a lightweight “boss agent” that just watches Roo, retries/re-prompts when things fail, and keeps pushing toward the end goal until the small project or app is finished.&lt;/p&gt; &lt;p&gt;From my limited understanding at this point, I'm not looking for a full LangChain/CrewAI setup, just something glue-code simple that could give me that extra hierarchy layer. Has anyone here already built something like this, or is everyone still handling oversight manually?&lt;/p&gt; &lt;p&gt;Would be very help for the little apps I’m trying to build instead of having to watch it constantly for the next step. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sluggerjt44"&gt; /u/Sluggerjt44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfzgge/anyone_put_together_an_oversight_agent_on_top_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfzgge/anyone_put_together_an_oversight_agent_on_top_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfzgge/anyone_put_together_an_oversight_agent_on_top_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T14:52:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nflbh4</id>
    <title>Qwen3max feels like a manager that had to attend sensitivity training</title>
    <updated>2025-09-13T01:54:45+00:00</updated>
    <author>
      <name>/u/Coldaine</name>
      <uri>https://old.reddit.com/user/Coldaine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nflbh4/qwen3max_feels_like_a_manager_that_had_to_attend/"&gt; &lt;img alt="Qwen3max feels like a manager that had to attend sensitivity training" src="https://preview.redd.it/371xjrd89uof1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d1fc0fb0a562a6ddb6d774061259f8ebdfe2969e" title="Qwen3max feels like a manager that had to attend sensitivity training" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really did have someone like this in real life. He was definitely a little bit on the spectrum and didn't get humor at all. People told him to lighten up, and it somehow got even worse when he was trying to be funny. &lt;/p&gt; &lt;p&gt;The rest of my code review did not go as well as the first line, but at least qwen was able to find one good thing about my code. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Coldaine"&gt; /u/Coldaine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/371xjrd89uof1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nflbh4/qwen3max_feels_like_a_manager_that_had_to_attend/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nflbh4/qwen3max_feels_like_a_manager_that_had_to_attend/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T01:54:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf7zhq</id>
    <title>Meta released MobileLLM-R1 on Hugging Face</title>
    <updated>2025-09-12T16:35:23+00:00</updated>
    <author>
      <name>/u/Illustrious_Row_9971</name>
      <uri>https://old.reddit.com/user/Illustrious_Row_9971</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf7zhq/meta_released_mobilellmr1_on_hugging_face/"&gt; &lt;img alt="Meta released MobileLLM-R1 on Hugging Face" src="https://preview.redd.it/huchm6bahrof1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f091dea3c1b3bd8cc946d3ae61d24b3e9a2e3a3b" title="Meta released MobileLLM-R1 on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;model: &lt;a href="https://huggingface.co/facebook/MobileLLM-R1-950M"&gt;https://huggingface.co/facebook/MobileLLM-R1-950M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;app (vibe coded): &lt;a href="https://huggingface.co/spaces/akhaliq/MobileLLM-R1-950M"&gt;https://huggingface.co/spaces/akhaliq/MobileLLM-R1-950M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;app was made in: &lt;a href="https://huggingface.co/spaces/akhaliq/anycoder"&gt;https://huggingface.co/spaces/akhaliq/anycoder&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious_Row_9971"&gt; /u/Illustrious_Row_9971 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/huchm6bahrof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf7zhq/meta_released_mobilellmr1_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf7zhq/meta_released_mobilellmr1_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T16:35:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfieif</id>
    <title>vLLM Now Supports Qwen3-Next: Hybrid Architecture with Extreme Efficiency</title>
    <updated>2025-09-12T23:33:53+00:00</updated>
    <author>
      <name>/u/chisleu</name>
      <uri>https://old.reddit.com/user/chisleu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfieif/vllm_now_supports_qwen3next_hybrid_architecture/"&gt; &lt;img alt="vLLM Now Supports Qwen3-Next: Hybrid Architecture with Extreme Efficiency" src="https://external-preview.redd.it/K3rGlpkjbDPCdSyb_xOk55T-rqiVrUIviv6vZoP3TV0.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16235ad425456d8b12ef7e3e92930529dc005885" title="vLLM Now Supports Qwen3-Next: Hybrid Architecture with Extreme Efficiency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let's fire it up!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chisleu"&gt; /u/chisleu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.vllm.ai/2025/09/11/qwen3-next.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfieif/vllm_now_supports_qwen3next_hybrid_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfieif/vllm_now_supports_qwen3next_hybrid_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T23:33:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfyefq</id>
    <title>Built an OpenWebUI Mobile Companion (Conduit): Alternative to Commercial Chat Apps</title>
    <updated>2025-09-13T14:07:55+00:00</updated>
    <author>
      <name>/u/cogwheel0</name>
      <uri>https://old.reddit.com/user/cogwheel0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfyefq/built_an_openwebui_mobile_companion_conduit/"&gt; &lt;img alt="Built an OpenWebUI Mobile Companion (Conduit): Alternative to Commercial Chat Apps" src="https://external-preview.redd.it/bmM2eHJndWN1eG9mMYOZirCPOS6YBPWlrGphmOW5xVKSUfZTzrO9LgRxJEFF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16d62db6f900a25a3678e55cff44f818a3b54ee1" title="Built an OpenWebUI Mobile Companion (Conduit): Alternative to Commercial Chat Apps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I have been building this for the past month. &lt;a href="https://www.reddit.com/r/selfhosted/comments/1mo9w3t/built_a_native_openwebui_client_for_ios_android/"&gt;After announcing it on different sub&lt;/a&gt; and receiving incredible feedback, I have been iterating. It's currently quite stable for daily use, even for non savvy users. This remains a primary goal with this project as it's difficult to move family off of commercial chat apps like ChatGPT, Gemini, etc without a viable alternative.&lt;/p&gt; &lt;p&gt;It's fully opensource and private: &lt;a href="https://github.com/cogwheel0/conduit"&gt;https://github.com/cogwheel0/conduit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please try it out if you're already selfhosting OpenWebUI and open an issue on GitHub for any problems!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cogwheel0"&gt; /u/cogwheel0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6eh7mfucuxof1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfyefq/built_an_openwebui_mobile_companion_conduit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfyefq/built_an_openwebui_mobile_companion_conduit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T14:07:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfqr69</id>
    <title>WarLlama: 2x MI50 LLM MicroATX Server</title>
    <updated>2025-09-13T06:55:54+00:00</updated>
    <author>
      <name>/u/__E8__</name>
      <uri>https://old.reddit.com/user/__E8__</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqr69/warllama_2x_mi50_llm_microatx_server/"&gt; &lt;img alt="WarLlama: 2x MI50 LLM MicroATX Server" src="https://b.thumbs.redditmedia.com/h9qMX2YlXQIKQi4YG23oXTimROWCT0LUz66l7zJkwKg.jpg" title="WarLlama: 2x MI50 LLM MicroATX Server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some ppl on this sub have Ahab-class dreadnoughts rocking a DeepSeek/Kimi high quant. Other have a warhorse w a giant gpu or six (or 16x?). This is my sleek lil &lt;em&gt;warllama&lt;/em&gt;. &lt;/p&gt; &lt;p&gt;It's is not abt the bling-bling; it's abt the ching-ching: how little money I spend building a little power house. It came out comely, but it was meant to be minimalist-- a pure headless Linux box running llama.cpp + rocm (which needs freq reboots from lots of llm usage) w a comfy 64gb vram. Cost of main parts: &lt;span class="md-spoiler-text"&gt;$730&lt;/span&gt;. The bells &amp;amp; whistles prob costs another $200+ nowadays but I bought most of it bf the recent (hyper)inflation/tariff BS. YMMV. &lt;/p&gt; &lt;p&gt;WARNING: I flout every sensible guideline in the &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j09qk1/dell_t640_for_a_4x_3090_build/"&gt;LocalLlama build guidebook&lt;/a&gt;: super tight case, ancient desktop mobo, weird gpus, buggy drivers, even buggier vbioxen, cramped airflow. You'll prob be eaten by a Grue.&lt;/p&gt; &lt;h2&gt;Write-Up Sections:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;PC Parts &amp;amp; Costs&lt;/li&gt; &lt;li&gt;Benchmarks &amp;amp; Temperatures&lt;/li&gt; &lt;li&gt;Notes&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;PC HW/SW Parts &amp;amp; Costs&lt;/h1&gt; &lt;h2&gt;HW&lt;/h2&gt; &lt;p&gt;It's all abt the models, then the gpus. The main computer is an afterthought.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Price&lt;/th&gt; &lt;th align="left"&gt;Part&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;$400&lt;/td&gt; &lt;td align="left"&gt;2x mi50 32gb&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$130&lt;/td&gt; &lt;td align="left"&gt;Asus Maximus VIII Gene + 32gb ddr4 + i5-6600k&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$35&lt;/td&gt; &lt;td align="left"&gt;Powertrain X100 PC case&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$60&lt;/td&gt; &lt;td align="left"&gt;ESGaming 750w modular PSU&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$50&lt;/td&gt; &lt;td align="left"&gt;1tb nvme&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$17&lt;/td&gt; &lt;td align="left"&gt;ARGB CPU fan&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$8&lt;/td&gt; &lt;td align="left"&gt;2x delta fans&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;?&lt;/td&gt; &lt;td align="left"&gt;various 3D printer parts: fan shroud, i/o shield, gpu stand, psu mount&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;$4&lt;/td&gt; &lt;td align="left"&gt;18pin ribbon cable for extending mobo front panels pins around mi50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;TOTAL: $731&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Bells &amp;amp; Whistles (no idea what these cost nowadays)&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Razer Chroma ARGB controller (6ch, perfect openrgb ctrl)&lt;/li&gt; &lt;li&gt;lcd 2004 + i2c adap&lt;/li&gt; &lt;li&gt;ch341: usb to i2c/gpio&lt;/li&gt; &lt;li&gt;ARGB 120mm case fan&lt;/li&gt; &lt;li&gt;usb cables/adap for internal usb devs&lt;/li&gt; &lt;li&gt;2x ARGB magnetic led strips&lt;/li&gt; &lt;li&gt;2x pcie Y-splitter for gpus&lt;/li&gt; &lt;li&gt;vga/hdmi car-rearview monitor&lt;/li&gt; &lt;li&gt;ezOutlet5 (poor man's bmc)&lt;/li&gt; &lt;li&gt;keyboard&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Smaller than a 24pack of soda. Heavy like a chonky cat.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dim: 349 x 185 x 295mm (19L, I think)&lt;/li&gt; &lt;li&gt;Total Weight: 19.3lb (8.68kg)&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;SW&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Ubuntu 22.04 + 6.8 hwe kernel &lt;/li&gt; &lt;li&gt;rocm 6.4.1 (6.4.4 ripped out mi50 supp!)&lt;/li&gt; &lt;li&gt;llama.cpp -&amp;gt; build_rocm&lt;/li&gt; &lt;li&gt;vbios: 113-D1631700-111 (orig hacky vbios that shipped w mi50).&lt;/li&gt; &lt;li&gt;bios: v0402 (mobo had first oem bios bf update)&lt;/li&gt; &lt;li&gt;openrgb (for python argb ctrl)&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/frank-zago/ch341-i2c-spi-gpio"&gt;ch341 linux driver&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Benchmarks &amp;amp; Temperatures&lt;/h1&gt; &lt;p&gt;Put into comment below&lt;/p&gt; &lt;h1&gt;Notes&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m389gi/32gb_mi50_but_llamacpp_vulkan_sees_only_16gb/"&gt;mi50 vbios misadventures&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1j09qk1/dell_t640_for_a_4x_3090_build/"&gt;Building a chonker multi-gpu rig considerations&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mo92ou/rtx_5090_vs_rtx_4090_48gb_or_rtx_6000/"&gt;How much HW do I rly need??? Vram Eaters vs the Gpu Cartel&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;you cant dress trash until you spend a lotta money. building smthg like this can only be done w v clear sw req assessment and a whole lotta hw expertise. multi-gpu compat on old hw is v arcane; esp w mi50s.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;target model: qwen family. v versatile, hq, instructable. v lil refusal bs.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;usecases: filing cooking recipes, modernizing Rolodex, doing arithmetic on dozens (!) of tabular cells. Or how abt: erp, dank memes, navigation calcs (dont wanna fly thru a star when i hit lightspeed)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;mobo is 10yro but is one of the slickest boards i've ever owned&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;its miraculous i was able to fit everything into case. the gpus, the fans &amp;amp; mounts. the normal atx cable lengths. the long (160mm) full sized atx psu. sff builds take more parts bc need to get evryhting to fit. either custom 3d printed plastic or workarounds like ribbon cables&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;similarly there's enough airflow thru such smol spaces to keep things undr 70C during llama-bench&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;i needed to ext the pin headers on the bottom edge of the mobo. 2.54mm pitch ribbon cables to the rescue. still needed to grind a few edges, but it works&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;i pray my nvme will last forevaaaaaah bc id need to tear the whole thing apart to swap drives.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;econ of cheap hw are terrible outside of hobbyests. for viable business, a comp builder would need to make thousands per box. but nobody is gonna pay that for less than multi-gpu behemoths. DIY or DIE.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;the mi50 appears to be the second coming of the P40 due to software advances from gents like these. thanks guys! &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15884"&gt;Flash attn for mi50&lt;/a&gt;. &lt;a href="https://github.com/iacopPBK/llama.cpp-gfx906"&gt;Part2&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;a 4x mi50 rig would be excellent, but exps w 2x tell me sorting out the pcie rsrc alloc issues would be more work than usual for multi-gpu. and still too smol for deepseek&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__E8__"&gt; /u/__E8__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nfqr69"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqr69/warllama_2x_mi50_llm_microatx_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqr69/warllama_2x_mi50_llm_microatx_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T06:55:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nftdeo</id>
    <title>appreciation post for qwen3 0.6b llm model</title>
    <updated>2025-09-13T09:42:11+00:00</updated>
    <author>
      <name>/u/iamzooook</name>
      <uri>https://old.reddit.com/user/iamzooook</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, For the last few days I was trying out all the low param llm models which would run on cpu.&lt;/p&gt; &lt;p&gt;I have tested from openai oss 20b, gemma 270m, 1b, 4b, deepseek 1.5b, qwen3 0.6b, 1.7b, 4b, 8b, granite 2b, and many more.&lt;/p&gt; &lt;p&gt;the performance and the reliability of qwen3 0.6b is unmatched to any other models. gemma isn't reliable at all even its 4b model. at the same time qwen3 4b beats oss 20b easily. granite 2b is good backup.&lt;/p&gt; &lt;p&gt;I got rid of all the models and just kept qwen3 0.6b, 4b and granite 2b. this would be my doomsday llm models running on cpu.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamzooook"&gt; /u/iamzooook &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nftdeo/appreciation_post_for_qwen3_06b_llm_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nftdeo/appreciation_post_for_qwen3_06b_llm_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nftdeo/appreciation_post_for_qwen3_06b_llm_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T09:42:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfzwag</id>
    <title>Local LLaMA Users: The 0~1000 Stars Semantic Firewall, Now In Human Words</title>
    <updated>2025-09-13T15:09:35+00:00</updated>
    <author>
      <name>/u/onestardao</name>
      <uri>https://old.reddit.com/user/onestardao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfzwag/local_llama_users_the_01000_stars_semantic/"&gt; &lt;img alt="Local LLaMA Users: The 0~1000 Stars Semantic Firewall, Now In Human Words" src="https://preview.redd.it/zsbiv9717yof1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81460bae6ca22a0919a3d9c5d49cf8458fe97bde" title="Local LLaMA Users: The 0~1000 Stars Semantic Firewall, Now In Human Words" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;The Semantic Firewall, In Plain Words&lt;/h1&gt; &lt;p&gt;Last time I posted a paper-like write-up. Today is the update that many asked for. No math, no jargon. I also brought a Grandma version so anyone can follow.&lt;/p&gt; &lt;p&gt;Idea&lt;/p&gt; &lt;p&gt;Most of us fix AI problems after the model has already spoken. You add a reranker or a regex or a guard. The same failure returns with a new face.&lt;/p&gt; &lt;p&gt;A semantic firewall flips the order. It checks the semantic state before the model is allowed to answer. If the state is unstable, it loops, narrows, or resets. Only a stable state may speak.&lt;/p&gt; &lt;p&gt;Think of it like pre-flight checks for your model instead of search-and-rescue after a crash.&lt;/p&gt; &lt;p&gt;—&lt;/p&gt; &lt;h2&gt;Before vs After&lt;/h2&gt; &lt;p&gt;After&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model talks first, you patch later.&lt;/li&gt; &lt;li&gt;New patches conflict. Debug time grows.&lt;/li&gt; &lt;li&gt;Failures resurface when context or seed changes.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Before&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Inspect the field first. We look at drift and hazard signals.&lt;/li&gt; &lt;li&gt;If unstable, the pipeline loops or resets, then tries again.&lt;/li&gt; &lt;li&gt;Once a failure mode is mapped, it stays fixed across runs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;—&lt;/p&gt; &lt;h2&gt;The 60-Second Test&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Load your usual local setup. Ollama, LM Studio, text-gen-webui, vLLM, exllama, GPTQ. Offline is fine.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Paste the firewall text block from your notes.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Ask your model: &lt;code&gt;answer using the firewall on: &amp;lt;your real task&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Watch stability. If it is unstable, the script tells you which failure bucket you hit. You fix that one bucket instead of guessing.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;—&lt;/p&gt; &lt;h2&gt;What Lives Where&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Problem Map 1.0 The starter tree of 16 repeatable failure modes. Great for first fixes and teaching juniors what to watch for.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Problem Map 2.0 RAG-heavy view. Chunking contracts, embedding traps, hybrid retriever weights, index hygiene. If your answers cite the wrong paragraph while retrieval “looks fine”, start here.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Semantic Clinic Symptom-first navigation when you are unsure. You describe the strange behavior, it routes you to the right fix.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Global Fix Map A layer that covers real stacks and tools. FAISS, Chroma, Qdrant, Weaviate, Milvus, LangChain, LangGraph, Ollama quirks, serverless boot orders, and more. No vendor switch needed. You keep your stack, you add a reasoning firewall on top.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;AI Doctor A pre-trained share window. Paste your bug or screenshot. It maps to the right page and gives a minimal repair plan. It is just text and reasoning, so it runs fine with local models too.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why this matters to Local LLaMA users You do not always have cloud evals or enterprise rerankers. You need something light and offline that reduces “whack-a-mole” time. The firewall is just text. No SDK, no keys, no infra change.&lt;/p&gt; &lt;h2&gt;Start Here. Grandma Edition&lt;/h2&gt; &lt;p&gt;If you want the simplest path, use this first:&lt;/p&gt; &lt;p&gt;Grandma Clinic&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/onestardao/WFGY/blob/main/ProblemMap/GrandmaClinic/README.md"&gt;https://github.com/onestardao/WFGY/blob/main/ProblemMap/GrandmaClinic/README.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;How to use it&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Scroll the Grandma stories until you see your symptom.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It tells you which failure you likely hit and what to try.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If it helps, save it. Next time you will fix a different class in minutes.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I built all this during a one-person cold start that went from 0 to 1000 stars in one season. The single biggest lesson: install guardrails at the reasoning layer before generation. Debugging becomes a process, not a fire.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;FAQ for &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;Q1. How is a semantic firewall different from just adding a reranker or a regex?&lt;/p&gt; &lt;p&gt;Rerankers and regex are after-the-fact filters. The firewall blocks unstable states before words are produced. That is why once a failure mode is mapped, it does not come back in a new form.&lt;/p&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;Q2. Can this run fully offline with local models?&lt;/p&gt; &lt;p&gt;Yes. It is plain text logic. Paste into your local loop. Works with Ollama, LM Studio, text-gen-webui, vLLM, exllama, GPTQ. No internet required.&lt;/p&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;Q3. What signals do you check to decide “stable enough to speak”?&lt;/p&gt; &lt;p&gt;Three families: drift of the current path, hazard of continuing the same path, and coverage of the retrieved or reasoned field. If any looks risky, the loop narrows scope or resets.&lt;/p&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;Q4. Will this slow down inference on my small GPU?&lt;/p&gt; &lt;p&gt;It adds short reasoning loops when unstable. In practice total time drops because you avoid redo cycles and long, wrong chains. On laptops you still see net savings when bugs are frequent.&lt;/p&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;Q5. My problem is RAG. Retrieval looks fine but answers cite the wrong line. Where do I begin?&lt;/p&gt; &lt;p&gt;Use the RAG view from Problem Map 2.0. Check chunking contracts first, then embedding normalization, then hybrid weights. The firewall will keep the model from answering until those are consistent.&lt;/p&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;Q6. I already have eval scripts. Why add this?&lt;/p&gt; &lt;p&gt;Eval measures after output. The firewall prevents unstable output from appearing. Keep both. Measure with eval, guarantee with the firewall.&lt;/p&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;Q7. I only want the easy route. Can I stick to Grandma Clinic?&lt;/p&gt; &lt;p&gt;Yes. Many teams run only the Grandma layer to start. When you want deeper control, open the main Problem Map and the RAG view later.&lt;/p&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;Q8. Does this require me to switch from my current stack or provider?&lt;/p&gt; &lt;p&gt;No. The firewall is stack-agnostic. It sits on top. Keep your loaders, your vector store, your agent framework. You just add reasoning checks before speaking.&lt;/p&gt; &lt;p&gt;—&lt;/p&gt; &lt;p&gt;Q9. I have a weird failure that is not on the list.&lt;/p&gt; &lt;p&gt;Treat it as a new class. Describe the symptom precisely, make a tiny repro, then map it once. After that, it is sealed for good.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;If this update helps you see your pipeline with new eyes, bookmark the Grandma page above. Even if you only ever hit one of the failure modes, learning the map early usually prevents three others you would have met next month.&lt;/p&gt; &lt;p&gt;Thanks for reading my work &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onestardao"&gt; /u/onestardao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zsbiv9717yof1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfzwag/local_llama_users_the_01000_stars_semantic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfzwag/local_llama_users_the_01000_stars_semantic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T15:09:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfqe2c</id>
    <title>What's with the obsession with reasoning models?</title>
    <updated>2025-09-13T06:33:35+00:00</updated>
    <author>
      <name>/u/HadesThrowaway</name>
      <uri>https://old.reddit.com/user/HadesThrowaway</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is just a mini rant so I apologize beforehand. Why are practically all AI model releases in the last few months all reasoning models? Even those that aren't are now &amp;quot;hybrid thinking&amp;quot; models. It's like every AI corpo is obsessed with reasoning models currently. &lt;/p&gt; &lt;p&gt;I personally dislike reasoning models, it feels like their only purpose is to help answer tricky riddles at the cost of a huge waste of tokens. &lt;/p&gt; &lt;p&gt;It also feels like everything is getting increasingly benchmaxxed. Models are overfit on puzzles and coding at the cost of creative writing and general intelligence. I think a good example is Deepseek v3.1 which, although technically benchmarking better than v3-0324, feels like a worse model in many ways.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HadesThrowaway"&gt; /u/HadesThrowaway &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqe2c/whats_with_the_obsession_with_reasoning_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqe2c/whats_with_the_obsession_with_reasoning_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfqe2c/whats_with_the_obsession_with_reasoning_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T06:33:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfktdg</id>
    <title>To The Qwen Team, Kindly Contribute to Qwen3-Next GGUF Support!</title>
    <updated>2025-09-13T01:29:35+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfktdg/to_the_qwen_team_kindly_contribute_to_qwen3next/"&gt; &lt;img alt="To The Qwen Team, Kindly Contribute to Qwen3-Next GGUF Support!" src="https://b.thumbs.redditmedia.com/FW-gGORXDRiCTDczp9dCooqVX1C79rwrf1Z5V7TuZEM.jpg" title="To The Qwen Team, Kindly Contribute to Qwen3-Next GGUF Support!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you haven't noticed already, Qwen3-Next hasn't yet been supported in llama.cpp, and that's because it comes with a custom SSM archiecture. Without the support of the Qwen team, this amazing model might not be supported for weeks or even months. By now, I strongly believe that llama.cpp day one support is an absolute must.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uxcnskn54uof1.png?width=938&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d39e54d6738531f99913794f3f2917609ff47b3"&gt;https://preview.redd.it/uxcnskn54uof1.png?width=938&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d39e54d6738531f99913794f3f2917609ff47b3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfktdg/to_the_qwen_team_kindly_contribute_to_qwen3next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfktdg/to_the_qwen_team_kindly_contribute_to_qwen3next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfktdg/to_the_qwen_team_kindly_contribute_to_qwen3next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T01:29:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng1fa5</id>
    <title>New Qwen 3 Next 80B A3B</title>
    <updated>2025-09-13T16:10:18+00:00</updated>
    <author>
      <name>/u/Haruki_090</name>
      <uri>https://old.reddit.com/user/Haruki_090</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng1fa5/new_qwen_3_next_80b_a3b/"&gt; &lt;img alt="New Qwen 3 Next 80B A3B" src="https://b.thumbs.redditmedia.com/H83OV_9-rVuIMILDJ4WSO054RM2o8R-_wKJXYnVERiQ.jpg" title="New Qwen 3 Next 80B A3B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Benchmarks&lt;/p&gt; &lt;p&gt;Model Card: &lt;a href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Instruct Model Card: &lt;a href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source of benchmarks: &lt;a href="https://artificialanalysis.ai"&gt;https://artificialanalysis.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Haruki_090"&gt; /u/Haruki_090 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ng1fa5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng1fa5/new_qwen_3_next_80b_a3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ng1fa5/new_qwen_3_next_80b_a3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T16:10:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfy5pv</id>
    <title>WEBGEN-OSS Web Design Model - a model that runs on a laptop and generates clean responsive websites from a single prompt</title>
    <updated>2025-09-13T13:57:18+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfy5pv/webgenoss_web_design_model_a_model_that_runs_on_a/"&gt; &lt;img alt="WEBGEN-OSS Web Design Model - a model that runs on a laptop and generates clean responsive websites from a single prompt" src="https://external-preview.redd.it/MmZ0eTk4bzlyeG9mMVpKx18zu2Al60haJHCIipoecy1_uH38KnrawZp01IuI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=27f7dd2294fdcf4c328eea19464490c2b249a9e9" title="WEBGEN-OSS Web Design Model - a model that runs on a laptop and generates clean responsive websites from a single prompt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Tesslate/WEBGEN-OSS-20B"&gt;https://huggingface.co/Tesslate/WEBGEN-OSS-20B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm excited to share WEBGEN-OSS-20B, a new 20B open-weight model focused exclusively on generating responsive websites. It’s small enough to run locally for fast iteration and is fine-tuned to produce modern HTML/CSS with Tailwind.&lt;/p&gt; &lt;p&gt;It prefers semantic HTML, sane spacing, and modern component blocks (hero sections, pricing tables, FAQs, etc.). Released under the Apache 2.0 license.&lt;/p&gt; &lt;p&gt;This is a research preview. Use it as you wish but we will be improving the model series greatly in the coming days. (Its very opinionated).&lt;/p&gt; &lt;p&gt;Key Links:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hugging Face Model: &lt;a href="https://huggingface.co/Tesslate/WEBGEN-OSS-20B"&gt;Tesslate/WEBGEN-OSS-20B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Example Outputs: &lt;a href="https://uigenoutput.tesslate.com/"&gt;uigenoutput.tesslate.com&lt;/a&gt; (will be updated within 24 hours)&lt;/li&gt; &lt;li&gt;Join the Tesslate Community to talk about AI and vote for upcoming models: &lt;a href="https://discord.com/invite/EcCpcTv93U"&gt;Discord&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r8vjb8o9rxof1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfy5pv/webgenoss_web_design_model_a_model_that_runs_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfy5pv/webgenoss_web_design_model_a_model_that_runs_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T13:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ng0nia</id>
    <title>4x 3090 local ai workstation</title>
    <updated>2025-09-13T15:39:55+00:00</updated>
    <author>
      <name>/u/monoidconcat</name>
      <uri>https://old.reddit.com/user/monoidconcat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng0nia/4x_3090_local_ai_workstation/"&gt; &lt;img alt="4x 3090 local ai workstation" src="https://preview.redd.it/0ug26v2gcyof1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6919babc61209c9648169dac26dd08693e7e02e2" title="4x 3090 local ai workstation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;4x RTX 3090($2500) 2x evga 1600w PSU($200) WRX80E + 3955wx($900) 8x 64gb RAM($500) 1x 2tb nvme($200)&lt;/p&gt; &lt;p&gt;All bought from used market, in total $4300, and I got 96gb of VRAM in total.&lt;/p&gt; &lt;p&gt;Currently considering to acquire two more 3090s and maybe one 5090, but I think the price of 3090s right now is a great deal to build a local AI workstation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/monoidconcat"&gt; /u/monoidconcat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0ug26v2gcyof1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ng0nia/4x_3090_local_ai_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ng0nia/4x_3090_local_ai_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-13T15:39:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nct0z8</id>
    <title>Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)</title>
    <updated>2025-09-09T19:47:12+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt; &lt;img alt="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" src="https://preview.redd.it/7vh8enuu07of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0f499252613d5bcf478fb1b75c594bb50f43436" title="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7vh8enuu07of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T19:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndjxdt</id>
    <title>AMA with the Unsloth team</title>
    <updated>2025-09-10T17:02:18+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;, I'm Daniel from &lt;a href="https://docs.unsloth.ai/"&gt;Unsloth&lt;/a&gt;! You might know us from our RL &amp;amp; fine-tuning open-source framework, our GGUFs, kernels or bug fixes. We’re super excited to answer all your questions!! 🦥 Our GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To celebrate the AMA, we’re releasing Aider Polyglot benchmarks comparing our DeepSeek-V3.1 Dynamic GGUFs to other models and quants. We also made a Localllama post here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our participants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Daniel, u/danielhanchen&lt;/li&gt; &lt;li&gt;Michael, u/yoracale&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 10AM – 1PM PST, with the Unsloth team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thanks so much!🥰&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T17:02:18+00:00</published>
  </entry>
</feed>
