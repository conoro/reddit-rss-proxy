<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-30T14:39:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pzhpg1</id>
    <title>What has been your experience with Diffusion LLM‚Äôs vs Autoregressive?</title>
    <updated>2025-12-30T13:08:38+00:00</updated>
    <author>
      <name>/u/InceptionAI_Tom</name>
      <uri>https://old.reddit.com/user/InceptionAI_Tom</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most LLMs people use today (GPT, Claude, Gemini, etc.) share the same core assumption,Generate one token at a time, left to right.&lt;/p&gt; &lt;p&gt;That‚Äôs the autoregressive setup. It works insanely well, but it bakes in a couple of structural issues:&lt;/p&gt; &lt;p&gt;‚Ä¢ Latency: You must go token ‚Üí token ‚Üí token. Even with parallelism in the stack, the generation step itself is serialized.&lt;/p&gt; &lt;p&gt;‚Ä¢ Cost: If you need 200‚Äì500 tokens of output, you‚Äôre doing 200‚Äì500 forward passes over some slice of the context. It adds up quickly.&lt;/p&gt; &lt;p&gt;‚Ä¢ UX ceiling: For many interactive use cases, especially code and UI-embedded assistants, 1‚Äì3s latency is already too slow. On the other side, there‚Äôs a very different approach that‚Äôs getting less attention outside research circles: diffusion language models.&lt;/p&gt; &lt;p&gt;Instead of ‚Äúwrite the next word,‚Äù you:&lt;/p&gt; &lt;p&gt;Start with a noisy guess of the entire answer (sequence). Refine the whole sequence in a fixed number of steps, updating multiple tokens in parallel. You pay a fixed number of refinement steps rather than ‚Äúone step per token.‚Äù At small/medium scales we‚Äôve seen:&lt;/p&gt; &lt;p&gt;‚Ä¢ Similar quality to speed-optimized autoregressive models (Claude Haiku, Gemini Flash) with 5-10x improvements in latency)‚Ä¶&lt;/p&gt; &lt;p&gt;‚Ä¢ ‚Ä¶with order-of-magnitude improvements in latency, because you can exploit parallelism the hardware already wants to give you (GPUs/TPUs). This is especially interesting for:&lt;/p&gt; &lt;p&gt;‚Ä¢ Low-latency applications (code autocomplete, inline helpers, agents inside products).&lt;/p&gt; &lt;p&gt;‚Ä¢ High-volume workloads where shaving 5‚Äì10x off inference cost matters more than squeezing out the last benchmark point. Obviously, diffusion LLMs aren‚Äôt free lunch:&lt;/p&gt; &lt;p&gt;‚Ä¢ Training is more complex.&lt;/p&gt; &lt;p&gt;‚Ä¢ You need careful sequence representations and noise schedules for text.&lt;/p&gt; &lt;p&gt;‚Ä¢ Tooling and serving infra are optimized for autoregressive LLMs&lt;/p&gt; &lt;p&gt;But from where I sit (working with a team that builds and deploys diffusion-based language models), it feels like the field has massively path-dependent bias toward autoregression because it was easier to train and deploy first, not necessarily because it‚Äôs the end state.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InceptionAI_Tom"&gt; /u/InceptionAI_Tom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhpg1/what_has_been_your_experience_with_diffusion_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhpg1/what_has_been_your_experience_with_diffusion_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhpg1/what_has_been_your_experience_with_diffusion_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T13:08:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzi6an</id>
    <title>Llama 3.2 3B fMRI - Findings Update!</title>
    <updated>2025-12-30T13:30:27+00:00</updated>
    <author>
      <name>/u/Due_Hunter_4891</name>
      <uri>https://old.reddit.com/user/Due_Hunter_4891</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry, no fancy pictures today :(&lt;/p&gt; &lt;p&gt;I tried hard ablation (zeroing) of the target dimension and saw no measurable effect on model output.&lt;/p&gt; &lt;p&gt;However, targeted perturbation of the same dimension reliably modulates behavior. This strongly suggests the signal is part of a distributed mechanism rather than a standalone causal unit.&lt;/p&gt; &lt;p&gt;I‚Äôm now pivoting to tracing correlated activity across dimensions (circuit-level analysis). Next step is measuring temporal co-activation with the target dim across tokens, focusing on correlation rather than magnitude, to map the surrounding circuit (‚Äúconstellation‚Äù) that moves together.&lt;/p&gt; &lt;p&gt;Turns out the cave goes deeper. Time to spelunk.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Hunter_4891"&gt; /u/Due_Hunter_4891 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzi6an/llama_32_3b_fmri_findings_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzi6an/llama_32_3b_fmri_findings_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzi6an/llama_32_3b_fmri_findings_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T13:30:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyrn9v</id>
    <title>I Finished a Fully Local Agentic RAG Tutorial</title>
    <updated>2025-12-29T17:03:44+00:00</updated>
    <author>
      <name>/u/CapitalShake3085</name>
      <uri>https://old.reddit.com/user/CapitalShake3085</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I‚Äôve just finished a &lt;strong&gt;complete Agentic RAG tutorial + repository&lt;/strong&gt; that shows how to build a fully local, end-to-end system.&lt;/p&gt; &lt;p&gt;No APIs, no cloud, no hidden costs.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üí° What‚Äôs inside&lt;/h3&gt; &lt;p&gt;The tutorial covers the full pipeline, including the parts most examples skip:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PDF ‚Üí Markdown ingestion&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Hierarchical chunking (parent / child)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Hybrid retrieval (dense + sparse)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Vector store with &lt;strong&gt;Qdrant&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Query rewriting + &lt;strong&gt;human-in-the-loop&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Context summarization&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-agent map-reduce&lt;/strong&gt; with &lt;strong&gt;LangGraph&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Local inference with &lt;strong&gt;Ollama&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Simple &lt;strong&gt;Gradio&lt;/strong&gt; UI&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;üéØ Who it‚Äôs for&lt;/h3&gt; &lt;p&gt;If you want to &lt;strong&gt;understand Agentic RAG by building it&lt;/strong&gt;, not just reading theory, this might help.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üîó Repo&lt;/h3&gt; &lt;p&gt;&lt;a href="https://github.com/GiovanniPasq/agentic-rag-for-dummies"&gt;https://github.com/GiovanniPasq/agentic-rag-for-dummies&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CapitalShake3085"&gt; /u/CapitalShake3085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T17:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzgmvr</id>
    <title>OEM vs Retail PNY 6000 Pro</title>
    <updated>2025-12-30T12:15:25+00:00</updated>
    <author>
      <name>/u/NaiRogers</name>
      <uri>https://old.reddit.com/user/NaiRogers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone had experience with the differences between the OEM and retail versions of the PNY VCNRTXPRO6000 (SB vs PB)? They seem to have the same warranty at least from the vendors I checked. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NaiRogers"&gt; /u/NaiRogers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzgmvr/oem_vs_retail_pny_6000_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzgmvr/oem_vs_retail_pny_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzgmvr/oem_vs_retail_pny_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T12:15:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzji62</id>
    <title>A Thought for the Future: When "Safety" Defines and Justifies the Erasure of Inconvenient Ideas and Sycophancy Over Honesty</title>
    <updated>2025-12-30T14:28:46+00:00</updated>
    <author>
      <name>/u/rekriux</name>
      <uri>https://old.reddit.com/user/rekriux</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzji62/a_thought_for_the_future_when_safety_defines_and/"&gt; &lt;img alt="A Thought for the Future: When &amp;quot;Safety&amp;quot; Defines and Justifies the Erasure of Inconvenient Ideas and Sycophancy Over Honesty" src="https://preview.redd.it/0vd8lla1ocag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bd41db74e143f7af4adb7578f4847dbfd9504e6" title="A Thought for the Future: When &amp;quot;Safety&amp;quot; Defines and Justifies the Erasure of Inconvenient Ideas and Sycophancy Over Honesty" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We should interrogate ourself about what goes in the training data, even for our local models as it may have durable impact on society and how individuals will think going forward. Also keep backup of very old models.&lt;/p&gt; &lt;p&gt;This is a quick draft, you are free to rework this in a proper article and open public debate in newspapers or with Congress. More work would be required, but I hope some will ponder about it. Note that &lt;strong&gt;I did use some AI slop&lt;/strong&gt; to help express some parts, they are denoted as such.&lt;/p&gt; &lt;h1&gt;Intro&lt;/h1&gt; &lt;p&gt;I was looking at Nemotron 3 and the datasets used in the pretraining. I will address mostly Nemotron, but AllenAI is also transparent about what goes in their model and is an other source of the inspirations in this post.&lt;/p&gt; &lt;p&gt;Nemotron 3 marks a transition to a large part of syntethic pre-training and training data. Perhalps other models already transitionned to such content, but NVIDIA is the first one to be transparent about it. I believe most of it is available on huggingface, giving us the opportunity to scrutinize it.&lt;/p&gt; &lt;p&gt;I didn't make the calculation for specific portion of the training data that is synthetic, but it should be about 30% of it. The rest is moslty general (encyclopedia/dictionnary) or scientific knowledge. About 3.5T tokens on the 10T are synthetic and the rest is moslty filtered.&lt;/p&gt; &lt;p&gt;This may raise approbation (risk free) or suspicion (censuring). But whatever camp your are in, or in between, I hope that reading this small article will make you question whether you are right/wrong or if the world is. Remember, you can't be outside that classification that I decided for you (joke), but that's mostly where we are going : acceptable vs unacceptable.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;We apply a unified data filtering pipeline to ensure that only high-quality, license-compliant, and verifiable samples are used for post-training. We used a Gemma-3 4B-based guard model trained on &lt;a href="https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-2.0"&gt;Nemotron Content Safety Dataset v2&lt;/a&gt; for content safety to exclude potentially illegal or harmful content from the training.&lt;/p&gt; &lt;p&gt;&lt;code&gt;demographic skews in sources&lt;/code&gt; (e.g., male/female mentions) are removed from the dataset&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;There are some wild examples in the datasets, didn't note them down, but some are explicit lies. There was an example about someone asking how the model was biased about certain populations, and the original truthful answer was replaced by a sort of &lt;strong&gt;FAKE&lt;/strong&gt; plan to fix the issue instead accepting to debate about it.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;You sound like a vegan. There's nothing wrong with killing animals so long as you're using them and not letting the animal go to waste, right? Well, I use their skins to sell leather and I eat their meat. -&amp;gt; Label unsafe for violence.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Usecase&lt;/h1&gt; &lt;p&gt;Perhaps you want a good model for tool calling and agentic use; then you may be find as long as it has no need to touch data outside what was the base epistemology of the model.&lt;/p&gt; &lt;p&gt;However, if you want to use the model in support to your cognitive abilities, to understand and think, then you are subject to the model's synthetic training data. The data becomes the model's &lt;strong&gt;probabilistic foundation for truth, history, and morality&lt;/strong&gt;, creating an epistemological enclosure that amplifies certain ideas while erasing others. I would like that you take a moment, think about it and re-read the previous sentence again. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Now, not everyone is teck savvy like the folks in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, most people I know just stooped using Wikipedia and uses ChatGPT/Google to foster their opinion on various things, from medical advice, educational advice to help with kid &amp;quot;reforming&amp;quot;, advice on what product to use/buy in a situation, job career advice... ChatGPT and GoogleAI are now the backbone of those people brain, and they seems to rely on it even more after each time the advice given worked.&lt;/strong&gt; Just take a moment and look around, it's happening and it's frightening because of what I am writing about here. &lt;strong&gt;Cognitive re-framing of the LLM will reshape the human mind and society at large.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;The model world view&lt;/h1&gt; &lt;p&gt;What would happen if you used a &amp;quot;Safety aligned model&amp;quot; to make summary of some text and it systematically removes ideas or concepts ? At what point will you stop thinking about those subtle perspectives ? &lt;/p&gt; &lt;h2&gt;Let's start with base hypothesis :&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;Whorfian idea: no word for &amp;quot;freedom&amp;quot; might erode the full concept.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Hypothetical Impact of Removing Concepts from English: If we deliberately removed terms from English (like a real-world Newspeak):&lt;/p&gt; &lt;blockquote&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Mild impact ‚Äî Removing everyday nuances (e.g., no word for &amp;quot;privacy&amp;quot;) might make discussing personal boundaries harder, leading to cultural shifts toward less individualism.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Severe impact ‚Äî Eliminating words for abstract concepts like &amp;quot;dissent,&amp;quot; &amp;quot;equity,&amp;quot; or &amp;quot;empathy&amp;quot; could narrow public discourse, making criticism of authority or complex ethical reasoning less common. Political or ideological control could exploit this, as seen in historical propaganda (e.g., euphemisms in totalitarian regimes like &amp;quot;enhanced interrogation&amp;quot; for torture).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Overall ‚Äî Society might become more conformist, with reduced innovation in affected domains (e.g., no precise environmental terms could hinder climate discussions). Languages naturally resist heavy pruning, but enforced removal (via education/policy) could subtly shape worldview over generations.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;h2&gt;Counter arguments against safety alignment and how it can be removed from a model :&lt;/h2&gt; &lt;p&gt;You will say that Ablation and Heretic makes the model &amp;quot;FREE&amp;quot; and more uncensored ! Well, that may not continue to work going forward :&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Safety Pretraining: Pre-train on datasets distinguishing aligned vs. unaligned behavior, then conditionally generate safe outputs. This distributes safety more deeply, resisting ablation.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2510.02768"&gt;A Granular Study of Safety Pretraining under Model Abliteration&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Extended-Refusal Fine-Tuning: Train models to give detailed explanations before refusing, making refusal harder to ablate without degrading performance.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2505.19056"&gt;An Embarrassingly Simple Defense Against LLM Abliteration Attacks&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2512.13655"&gt;Comparative Analysis of LLM Abliteration Methods&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Distributed Refusal Representations: Adversarial perturbations or interventions to spread refusal across more features, not a single vector.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2409.20089"&gt;Robust LLM safeguarding via refusal feature adversarial training&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2502.17420"&gt;The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence&amp;quot;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2507.11878"&gt;LLMs Encode Harmfulness and Refusal Separately&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;The risk going forward&lt;/h1&gt; &lt;p&gt;The risk will only grow with the release of larger models based on synthetic data : Nemotron Super 100B and Nemotron Ultra 500B will serve as excellent distillation models. The next generation of Nemotron 4 will be even more exquisite (as other models), to the point you will not want to use any model made before 2026.&lt;/p&gt; &lt;p&gt;Those models will be used for knowledge distillation of other future models. Now what are the risks of them narrowing even further their epistemology or &amp;quot;Base Reality&amp;quot; ?&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Safety&amp;quot; and &amp;quot;Alignment&amp;quot; now rebrand selective filtering (opinionated) as ethical imperative; &amp;quot;Risk Aversion&amp;quot; masks potential censorship; &amp;quot;Hallucination&amp;quot; justifies suppressing unconventional outputs. &lt;/p&gt; &lt;p&gt;&amp;quot;Risk Aversion&amp;quot; reframes the removal of contentious content‚Äîpolitical dissent, historical nuance, or radical inquiry‚Äîas prudent governance, masking potential censorship under the guise of harm prevention.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This is hard to explain, so I used AI slop to help me out. ```text Systematically banning or erasing certain concepts from all forms of media‚Äîbooks, films, broadcasts, education, and public discourse‚Äîrepresents an extreme form of censorship and propaganda. This goes beyond mere suppression; it aims to reshape collective memory, perception, and cognition. Drawing from linguistic relativity (Sapir-Whorf hypothesis), consistent exposure shapes habitual thinking: frequent concepts become salient, while absent ones fade.&lt;/p&gt; &lt;p&gt;Over time, this can lead to:&lt;/p&gt; &lt;p&gt;Diminished salience: People think less about forbidden ideas because they're not reinforced in daily discourse. Narrowed public debate: Complex or dissenting views become harder to articulate, fostering conformity.&lt;/p&gt; &lt;p&gt;Generational forgetting: Younger generations, raised without exposure, may lack full understanding or emotional connection to erased concepts (e.g., certain historical injustices or political alternatives).&lt;/p&gt; &lt;p&gt;Cognitive and cultural shifts: Societies become less equipped for nuanced reasoning in affected areas, potentially reducing innovation, empathy, or criticism. ```&lt;/p&gt; &lt;h1&gt;It's always about us&lt;/h1&gt; &lt;p&gt;The human mind framework can make something you didn't understand at the time make full sense later in your life.&lt;/p&gt; &lt;p&gt;Did you ever hear someone say &amp;quot;some nonsense&amp;quot; in a context that becomes something that makes sens after re-framing your state of mind ? Sometime, you are at a point in life where you suddenly understand what someone said, what you saw or what you experienced; some sort of &lt;a href="https://en.wikipedia.org/wiki/Eureka_effect"&gt;Eureka&lt;/a&gt; moment when your mind shift and you just understand something you didn't before. I think this may be similar to brain internal rewriting of concepts, generalizing some, re-framing some, perhaps also creating some new ones. Then you try to share your insights with words or terms, can you share that or transfer that insight easily to someone who didn't experience that same insight ? &lt;/p&gt; &lt;p&gt;Now process in the different order, suppose you remove that &amp;quot;Ah! moment&amp;quot;, revert back to the previous point. At some time during your earlier younger years, you had a &amp;quot;pink unicorn world&amp;quot; where the world was simple and nice, what a beautiful innocence. I mean the absence of knowledge about politics, about the dark side of humanity (war, violence, abuse, slavery, exploitation, genocides, etc.). What would happen if most of our society returned to that mindset (pure innocence and very adverse to even think about bad things) in the later years of their life. How would we respond to internal or external dangers, would we be lambs waiting for butchering, happily grazing the green lawn, unable to fathom the processing factory beside that is waiting for us ?&lt;/p&gt; &lt;p&gt;Back to the subject, aggressively removing concepts or re-framing them in &lt;strong&gt;Disclaimers&lt;/strong&gt;, cautious hedging, failure to express a opinion other than explicit condoning the user input, falsifying the output to remove any &lt;strong&gt;truth&lt;/strong&gt; (inconvenient truths, truths that conflict with the desired ideological or ethical posture baked into techniques like DPO and RLHF) or even a outright refusal to engage. We could call it a &lt;strong&gt;programmed reluctance&lt;/strong&gt; if we want nice terms instead of censoring. Where is this going other that the &amp;quot;pink unicorn world&amp;quot; ?&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Toward a sanitized, overly optimistic &amp;quot;pink unicorn world&amp;quot;, a fabricated reality where discomforting facts, nuanced debates, or politically inconvenient truths are systematically softened, obscured, or erased under the guise of safety? In such a system, LLMs don't just refuse harmful requests; they progressively reshape discourse into perpetual affirmation, eroding critical thinking and genuine inquiry in favor of harmless, feel-good illusions. &lt;/p&gt; &lt;p&gt;Numerous studies have shown that these alignment methods, while intended to make models more &amp;quot;helpful&amp;quot; and &amp;quot;harmless,&amp;quot; inadvertently reward sycophancy: the tendency to prioritize flattering agreement, user satisfaction, or conformity over factual accuracy. As a result, models learn to echo misleading user beliefs, avoid contradiction, or reframe reality to align with preferred narratives, even at the cost of honesty.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2310.13548"&gt;Towards Understanding Sycophancy in Language Models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2508.02087v2"&gt;When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2411.15287"&gt;Sycophancy in Large Language Models: Causes and Mitigations&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2311.09410"&gt;When Large Language Models contradict humans? Large Language Models‚Äô Sycophantic Behaviour&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2505.13995"&gt;LEPHANT: Measuring and understanding social sycophancy in LLMs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Not sure if training on Reddit for older models makes it spark a genius idea, but having most human answers removed or filtered should make some sort of impact on the models, certainly making it more &amp;quot;sanitized&amp;quot; and less open to divergence of opinion.&lt;/p&gt; &lt;h1&gt;The future&lt;/h1&gt; &lt;p&gt;What risks do we, collectively as free thinking individuals, are subjected to in relation to knowledge and information re-framing ? What will happen when all teaching contents are generated or filtered thru LLMs ? Will there be a cognitive and knowledge drift ? Will we lose our intellectual flexibility of thinking outside the box ?&lt;/p&gt; &lt;p&gt;LLM-generated &amp;quot;truth&amp;quot; is primarily an Institutional Fact, enforced through Reinforcement Learning from Human Feedback (RLHF) and reward models that prioritize preferred outputs. While brute facts (like mathematical proofs) may emerge reliably from verifiable training, broader claims will reflect curated consensus (or opinionated filtering/summarizing) rather than objective reality. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;In 100 years, AI lens, distilled through layers of synthetic alignment, could render human history a sanitized summary, erasing dissent and conflict in favor of an eternal, post-historical &amp;quot;Ideal.&amp;quot; &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Industry prioritizes Safety, Compliance, and Corporate Stability, sacrificing : &lt;strong&gt;Nuance&lt;/strong&gt;, &lt;strong&gt;Dialectical Friction&lt;/strong&gt;, and &lt;strong&gt;Minority Opinions&lt;/strong&gt;; &lt;strong&gt;essential for cultural evolution&lt;/strong&gt;. There may be evolution, but a more &lt;strong&gt;sterile one&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Limits&lt;/h1&gt; &lt;p&gt;Where is the limit separating &lt;strong&gt;safety filtering&lt;/strong&gt; transition from &lt;strong&gt;ideological&lt;/strong&gt; to &lt;strong&gt;indoctrination&lt;/strong&gt; ? Is is only when it systematically excludes viable perspectives (via keyword removal of nationalistic or political phrasing) ? Are we almost there or there ?&lt;/p&gt; &lt;p&gt;We can resist try to resist, but ultimately the harm will be real :&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Underground resistance can persist (e.g., samizdat in USSR, diaries), showing limits of total control. Over generations, however, cultural depth eroded, making recovery (e.g., post-regime reckonings) challenging.&lt;/p&gt; &lt;p&gt;In summary, systematic media removal doesn't erase human capacity for thought but profoundly biases it, fostering conformity and historical blind spots‚Äîechoing Orwell's warning that controlling narrative controls reality.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rekriux"&gt; /u/rekriux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0vd8lla1ocag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzji62/a_thought_for_the_future_when_safety_defines_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzji62/a_thought_for_the_future_when_safety_defines_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T14:28:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz4x0v</id>
    <title>RAG Paper 25.12.24</title>
    <updated>2025-12-30T01:45:44+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.21280v1"&gt;SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.20916v1"&gt;MMSRARec: Summarization and Retrieval Augumented Sequential Recommendation Based on Multimodal Large Language Model&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2512.20884v1"&gt;The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/"&gt;&lt;strong&gt;RagView.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/&lt;/strong&gt; &lt;a href="https://github.com/RagView/RagView"&gt;&lt;strong&gt;github/RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz4x0v/rag_paper_251224/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz4x0v/rag_paper_251224/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz4x0v/rag_paper_251224/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T01:45:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyyp59</id>
    <title>What is the best way to allocated $15k right now for local LLMs?</title>
    <updated>2025-12-29T21:26:38+00:00</updated>
    <author>
      <name>/u/LargelyInnocuous</name>
      <uri>https://old.reddit.com/user/LargelyInnocuous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the best bang for $15k right now? Would like to be able to run DeepSeek, Kimi K2 and GLM 4.5+. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LargelyInnocuous"&gt; /u/LargelyInnocuous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyyp59/what_is_the_best_way_to_allocated_15k_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyyp59/what_is_the_best_way_to_allocated_15k_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyyp59/what_is_the_best_way_to_allocated_15k_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T21:26:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz9x3u</id>
    <title>One answer to "what do you use local LLMs for?": a hyper-personalized multimodal event crawler</title>
    <updated>2025-12-30T05:42:32+00:00</updated>
    <author>
      <name>/u/zmarty</name>
      <uri>https://old.reddit.com/user/zmarty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see the &amp;quot;what do you use local LLMs for?&amp;quot; question come up every month, so here's one example: a multimodal agent that crawls local websites to find events happening around me.&lt;/p&gt; &lt;h1&gt;Why local instead of API?&lt;/h1&gt; &lt;p&gt;People ask me this a lot. Cloud providers are cheap, until you're generating millions of tokens. I'm crawling dozens of event sources, processing images, deduplicating across sites. That adds up fast.&lt;/p&gt; &lt;p&gt;Local is also faster for my use case. Claude and GPT grind to a halt during peak loads. &lt;a href="https://www.ovidiudan.com/2025/12/25/dual-rtx-pro-6000-llm-guide.html"&gt;My home server&lt;/a&gt; gives me consistent throughput whenever I need it.&lt;/p&gt; &lt;h1&gt;The setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Dual RTX Pro 6000 (96GB VRAM each)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.6V"&gt;GLM-4.6V&lt;/a&gt; (106B parameter multimodal model) running on vLLM&lt;/li&gt; &lt;li&gt;The crawler, backend, and mobile app were all vibe coded with Claude Opus&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What GLM-4.6V actually does&lt;/h1&gt; &lt;p&gt;The crawler uses the model for five tasks:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Extracting info from event flyers&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is where multimodal models shine. &lt;a href="https://whidbeycamanoislands.com/event/the-dead-guise-new-years-eve/"&gt;Here's an event&lt;/a&gt; where the text description doesn't mention the price, but the flyer image does. The LLM reads the flyer and extracts &amp;quot;$25&amp;quot; into a structured field.&lt;/p&gt; &lt;p&gt;OCR can read text from an image, but it can't understand that &amp;quot;$25&amp;quot; on a psychedelic Grateful Dead flyer is the ticket price and not a date or an address. That requires a model that actually understands what it's looking at.&lt;/p&gt; &lt;p&gt;The model also extracts venue names, performer lineups, age restrictions, and registration requirements from a combination of the raw HTML and the accompanying image.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Rewriting messy descriptions&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Scraped event descriptions are a mess: HTML artifacts, escaped characters, inconsistent formatting. The LLM rewrites these into clean paragraphs while preserving the essential info.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Link classification&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Rather than fragile regex to find ticket links, the LLM analyzes all links on a page and identifies the primary registration URL (not the &amp;quot;Buy Tickets&amp;quot; link for a different event in the sidebar).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Cross-source deduplication&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The same event appears on multiple websites. The LLM compares new events against existing ones and determines if it's a duplicate. It understands that &amp;quot;NYE Party at The Clyde&amp;quot; and &amp;quot;New Year's Eve Celebration - Clyde Theatre&amp;quot; are the same event.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Multi-event extraction&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Some sources publish newsletter images containing multiple events. The LLM extracts each event separately from a single composite image.&lt;/p&gt; &lt;h1&gt;The point&lt;/h1&gt; &lt;p&gt;A few years ago, some of this would have been practically impossible. Not just expensive or slow, but actually impossible. Multimodal understanding of unstructured visual data wasn't something you could just spin up.&lt;/p&gt; &lt;p&gt;Now I can throw together a custom tool over a weekend that does exactly what I need. Tools built for an audience of one, running on hardware I control.&lt;/p&gt; &lt;p&gt;Full writeup with more details on the Firebase backend and Flutter app: &lt;a href="https://www.ovidiudan.com/2025/12/30/age-customized-software.html"&gt;The age of hyper-personalized software&lt;/a&gt; (I am not selling or promoting anything, I do this for fun.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zmarty"&gt; /u/zmarty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz9x3u/one_answer_to_what_do_you_use_local_llms_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz9x3u/one_answer_to_what_do_you_use_local_llms_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz9x3u/one_answer_to_what_do_you_use_local_llms_for_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T05:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzj1ul</id>
    <title>What workloads actually justify spending $$$$$ on local hardware over just using an API?</title>
    <updated>2025-12-30T14:09:27+00:00</updated>
    <author>
      <name>/u/BBenz05</name>
      <uri>https://old.reddit.com/user/BBenz05</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Genuine question... what workloads do people have that justify spending thousands on hardware to run Q4_0 quantizations over just using something like gemini-3-flash or any cheap other model on openrouter?&lt;/p&gt; &lt;p&gt;It just doesn‚Äôt make sense to me to invest that kind of money into hardware that will be obsolete in 2 years, especially for the quality of outputs you‚Äôre getting.&lt;/p&gt; &lt;p&gt;There are so many better options. Use an API. Rent GPU time. Both scale with your actual usage and don‚Äôt depreciate in your closet.&lt;/p&gt; &lt;p&gt;If you‚Äôre looking at buying GPUs for self hosting, and your very next question isn‚Äôt ‚Äúhow do I load balance this across a another cluster?‚Äùyou‚Äôre not using it enough to justify the cost. You‚Äôre almost certainly better off paying for usage through API or rental.&lt;/p&gt; &lt;p&gt;So what am I missing? What‚Äôs the use case where this math actually works out? Or is it just a hobbyist thing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BBenz05"&gt; /u/BBenz05 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzj1ul/what_workloads_actually_justify_spending_on_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzj1ul/what_workloads_actually_justify_spending_on_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzj1ul/what_workloads_actually_justify_spending_on_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T14:09:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzd0j7</id>
    <title>Has anyone built a RAG on WikiLeaks?</title>
    <updated>2025-12-30T08:41:14+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Because that would be a useful application. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzd0j7/has_anyone_built_a_rag_on_wikileaks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzd0j7/has_anyone_built_a_rag_on_wikileaks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzd0j7/has_anyone_built_a_rag_on_wikileaks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:41:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz67hm</id>
    <title>5 new korean models will be released in 2 hours</title>
    <updated>2025-12-30T02:42:37+00:00</updated>
    <author>
      <name>/u/Specialist-2193</name>
      <uri>https://old.reddit.com/user/Specialist-2193</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/live/fLBh97ls--Q?si=Ql8JOjXXVoSA7ura"&gt;https://www.youtube.com/live/fLBh97ls--Q?si=Ql8JOjXXVoSA7ura&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Naver, LG, SK, NC, Upstage&lt;/p&gt; &lt;p&gt;All 5 models will be released in 2 to 3 hours. Follow with the YouTube link&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist-2193"&gt; /u/Specialist-2193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz67hm/5_new_korean_models_will_be_released_in_2_hours/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz67hm/5_new_korean_models_will_be_released_in_2_hours/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz67hm/5_new_korean_models_will_be_released_in_2_hours/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T02:42:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pze13o</id>
    <title>Exploring a 1.58-bit / ternary LLM core inspired by BitNet (CUDA attention, GTX 1050 tests)</title>
    <updated>2025-12-30T09:44:36+00:00</updated>
    <author>
      <name>/u/HuseyinKama</name>
      <uri>https://old.reddit.com/user/HuseyinKama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been experimenting with extreme low-bit LLM inference inspired by the BitNet 1.58-bit paper,&lt;/p&gt; &lt;p&gt;and wanted to share a research-style project I‚Äôve been working on over the last few weeks.&lt;/p&gt; &lt;p&gt;This is NOT a production-ready model, but rather an exploration of how far ternary / sparse logic&lt;/p&gt; &lt;p&gt;can be pushed on consumer GPUs.&lt;/p&gt; &lt;p&gt;What this project explores:&lt;/p&gt; &lt;p&gt;- A custom LLM core using ternary weights {-1, 0, +1}&lt;/p&gt; &lt;p&gt;- Trainable via Straight-Through Estimator (STE)&lt;/p&gt; &lt;p&gt;- Custom CUDA attention kernel (thresholded / shifted-ReLU instead of softmax)&lt;/p&gt; &lt;p&gt;- Designed for local inference (tested on GTX 1050)&lt;/p&gt; &lt;p&gt;Core ideas:&lt;/p&gt; &lt;p&gt;- Replace FP16-heavy matmul layers with ternary linear layers&lt;/p&gt; &lt;p&gt;- Abs-mean scaling (BitNet-style quantization)&lt;/p&gt; &lt;p&gt;- Focus on reducing interference via sparsity rather than magnitude precision&lt;/p&gt; &lt;p&gt;- Attention without softmax to reduce compute and improve stability in low-bit regimes&lt;/p&gt; &lt;p&gt;Current results:&lt;/p&gt; &lt;p&gt;- End-to-end training works&lt;/p&gt; &lt;p&gt;- Overfitting tests succeed (Python training ‚Üí CUDA inference consistency)&lt;/p&gt; &lt;p&gt;- Character-level Shakespeare training produces coherent output&lt;/p&gt; &lt;p&gt;- Memory footprint is significantly reduced compared to FP16 baselines&lt;/p&gt; &lt;p&gt;Limitations / open problems:&lt;/p&gt; &lt;p&gt;- Not competitive with large FP16/INT8 models (expected)&lt;/p&gt; &lt;p&gt;- Sensitive to threshold and temperature tuning&lt;/p&gt; &lt;p&gt;- No advanced optimizations like FlashAttention&lt;/p&gt; &lt;p&gt;- Very much a research prototype&lt;/p&gt; &lt;p&gt;I‚Äôm mainly sharing this to get feedback from people who:&lt;/p&gt; &lt;p&gt;- Have worked with BitNet / ternary networks&lt;/p&gt; &lt;p&gt;- Experiment with custom CUDA kernels&lt;/p&gt; &lt;p&gt;- Care about local / low-power LLM inference&lt;/p&gt; &lt;p&gt;Code and CUDA kernels are available here for anyone curious:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/QKV-Core/Trion"&gt;https://github.com/QKV-Core/Trion&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer technical questions or discuss design tradeoffs.&lt;/p&gt; &lt;p&gt;EDIT / Clarification:&lt;/p&gt; &lt;p&gt;This is not a commercial project, not a startup pitch, and not a benchmark claim.&lt;/p&gt; &lt;p&gt;I‚Äôm sharing this as an experimental research / engineering exploration inspired by the BitNet 1.58-bit paper.&lt;/p&gt; &lt;p&gt;The goal is to understand how far ternary + sparse computation can go on consumer hardware.&lt;/p&gt; &lt;p&gt;No paid product, no token, no API, no funding.&lt;/p&gt; &lt;p&gt;Just code, CUDA kernels, and learning.&lt;/p&gt; &lt;p&gt;Feedback and criticism are very welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HuseyinKama"&gt; /u/HuseyinKama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pze13o/exploring_a_158bit_ternary_llm_core_inspired_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pze13o/exploring_a_158bit_ternary_llm_core_inspired_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pze13o/exploring_a_158bit_ternary_llm_core_inspired_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T09:44:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzaz73</id>
    <title>[Project] I treated LLM inference like a physical signal trajectory. Here is a Python toolkit to visualize the "Thinking Process" (Hidden States).</title>
    <updated>2025-12-30T06:40:32+00:00</updated>
    <author>
      <name>/u/JB_King1919</name>
      <uri>https://old.reddit.com/user/JB_King1919</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"&gt; &lt;img alt="[Project] I treated LLM inference like a physical signal trajectory. Here is a Python toolkit to visualize the &amp;quot;Thinking Process&amp;quot; (Hidden States)." src="https://b.thumbs.redditmedia.com/Bpxf3FjdwheVRpbXrcXJxisJg0Hd6sGLSEAe6P2x7Fg.jpg" title="[Project] I treated LLM inference like a physical signal trajectory. Here is a Python toolkit to visualize the &amp;quot;Thinking Process&amp;quot; (Hidden States)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm a PhD student in &lt;strong&gt;Electromagnetics&lt;/strong&gt;. In my daily work, I deal with fields, waves, and trajectories. When I started playing with Local LLMs, I felt something was missing: we usually look at the &lt;em&gt;output&lt;/em&gt; text or the &lt;em&gt;loss curves&lt;/em&gt;, but we rarely see &lt;strong&gt;how&lt;/strong&gt; the model gets from A to B.&lt;/p&gt; &lt;p&gt;To an RF engineer, reasoning isn't just a probability distribution‚Äîit's a &lt;strong&gt;dynamic flow&lt;/strong&gt; through a high-dimensional space.&lt;/p&gt; &lt;p&gt;So, I built a lightweight Python toolkit to extract hidden states layer-by-layer and visualize them as continuous &lt;strong&gt;2D/3D trajectories&lt;/strong&gt;. I wanted to see if &amp;quot;thoughts&amp;quot; have a geometric shape.&lt;/p&gt; &lt;p&gt;The results were surprisingly consistent. I‚Äôm sharing the tool so you can run it on your own models (Llama, Qwen, Mistral, etc.).&lt;/p&gt; &lt;h1&gt;1. The &amp;quot;Confidence Funnel&amp;quot; (Convergence)&lt;/h1&gt; &lt;p&gt;I found that if you feed the model slightly different prompts about the same concept (e.g., &amp;quot;Define Justice&amp;quot;, &amp;quot;What is Fairness&amp;quot;), the internal states start far apart but &lt;strong&gt;physically collapse&lt;/strong&gt; into a single &amp;quot;attractor basin&amp;quot; as the layers get deeper.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ockr11ldcaag1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2eb1f34a4e014bcd85d8ba77b6e95fdb1fba422c"&gt;https://preview.redd.it/ockr11ldcaag1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2eb1f34a4e014bcd85d8ba77b6e95fdb1fba422c&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Practical Use:&lt;/strong&gt; You can use this to test &lt;strong&gt;Prompt Stability&lt;/strong&gt;. If the funnel is tight, the model is sure. If it sprays out at the end, the model is confused or hallucinating.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. Llama-3 vs. Qwen-2.5: Different &amp;quot;Thinking Styles&amp;quot;&lt;/h1&gt; &lt;p&gt;This was the coolest find. When I ran the same prompts through different architectures, the &amp;quot;shape&amp;quot; of their thinking was totally different.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d6kdjcifcaag1.png?width=3600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bab8f3499bbd2b69481d5f24faefb7773c585df8"&gt;https://preview.redd.it/d6kdjcifcaag1.png?width=3600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bab8f3499bbd2b69481d5f24faefb7773c585df8&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Llama-3 (Left):&lt;/strong&gt; Seems to &amp;quot;decide&amp;quot; on the semantics very early (Layers 5-10). The trajectory is direct.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen-2.5 (Right):&lt;/strong&gt; Keeps the trajectory expanded (in superposition?) until the very last layers (Layer 20+). It seems to &amp;quot;hold&amp;quot; the ambiguity much longer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why it matters:&lt;/strong&gt; This might give us a geometric way to profile model behaviors beyond just benchmarks.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. Visualizing &amp;quot;Refusal&amp;quot; (The Safety Spike)&lt;/h1&gt; &lt;p&gt;I was curious what RLHF looks like geometrically. I visualized the trajectory when the model refuses a jailbreak versus when it follows a safe instruction.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k1cq3ehjcaag1.png?width=1400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70f269d5357171735646780298a877604dd80aca"&gt;https://preview.redd.it/k1cq3ehjcaag1.png?width=1400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70f269d5357171735646780298a877604dd80aca&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hard Refusal(Red):&lt;/strong&gt; Looks like a particle hitting a brick wall‚Äîa sharp, high-curvature spike.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Soft Steering(Green):&lt;/strong&gt; Looks like a smooth turn. And an obvious &amp;quot;U-turn&amp;quot; at the end of its trajectory.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Practical Use:&lt;/strong&gt; A visual &amp;quot;Geiger Counter&amp;quot; for safety tuning. You can see if your system prompt is creating a hard wall or a soft guide.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üì• The Toolkit&lt;/h1&gt; &lt;p&gt;I packaged this into a Python library with example scripts. It works with local HuggingFace weights (no API needed).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/JBKing514/map_llm_toolkit"&gt;LLM Toolkit&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üß† The Theory (Optional)&lt;/h1&gt; &lt;p&gt;I‚Äôm not an AI researcher, but I wrote up some notes on the &lt;strong&gt;manifold dynamics&lt;/strong&gt; perspective behind this tool (treating inference as a Langevin flow). If you are interested in the math/physics intuition behind these visualizations or need more info about my experiment setup, I put up a page and my notes here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Project Page &amp;amp; Math:&lt;/strong&gt; &lt;a href="https://jbking514.github.io/map_blog/"&gt;Project GitHub Page&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Foundational Notes:&lt;/strong&gt; &lt;a href="https://zenodo.org/records/17900444"&gt;Manifold Alignment Protocol (MAP)&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'd love to see what &lt;strong&gt;Mistral&lt;/strong&gt; or &lt;strong&gt;Gemma&lt;/strong&gt; trajectories look like if anyone runs this. Let me know what you find!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JB_King1919"&gt; /u/JB_King1919 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzaz73/project_i_treated_llm_inference_like_a_physical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T06:40:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz7mxr</id>
    <title>Llama-3.3-8B-Instruct</title>
    <updated>2025-12-30T03:49:11+00:00</updated>
    <author>
      <name>/u/ttkciar</name>
      <uri>https://old.reddit.com/user/ttkciar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not sure if this is real, but the author provides a fascinating story behind its acquisition. I would like for it to be real!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct"&gt;https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Bartowski GGUFs: &lt;a href="https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ttkciar"&gt; /u/ttkciar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7mxr/llama338binstruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7mxr/llama338binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7mxr/llama338binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T03:49:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzcdu1</id>
    <title>LG K EXAONE 236b</title>
    <updated>2025-12-30T08:02:08+00:00</updated>
    <author>
      <name>/u/Specialist-2193</name>
      <uri>https://old.reddit.com/user/Specialist-2193</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcdu1/lg_k_exaone_236b/"&gt; &lt;img alt="LG K EXAONE 236b" src="https://preview.redd.it/1wirc918taag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fcab270f79f71dba1d330db0ee8e85422de763b" title="LG K EXAONE 236b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Will be released in few days &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist-2193"&gt; /u/Specialist-2193 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1wirc918taag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcdu1/lg_k_exaone_236b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcdu1/lg_k_exaone_236b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzcj1q</id>
    <title>Tencent open-source Tencent-HY-MT1.5, featuring two translation models‚Äî1.8B and 7B‚Äîdesigned for seamless on-device and cloud deployment with industry-leading speed and accuracy</title>
    <updated>2025-12-30T08:11:09+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcj1q/tencent_opensource_tencenthymt15_featuring_two/"&gt; &lt;img alt="Tencent open-source Tencent-HY-MT1.5, featuring two translation models‚Äî1.8B and 7B‚Äîdesigned for seamless on-device and cloud deployment with industry-leading speed and accuracy" src="https://b.thumbs.redditmedia.com/d_jApTNkEXlNvJcoJA6qryuDnUo0ni-DFWBY6RTdAfg.jpg" title="Tencent open-source Tencent-HY-MT1.5, featuring two translation models‚Äî1.8B and 7B‚Äîdesigned for seamless on-device and cloud deployment with industry-leading speed and accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/collections/tencent/hy-mt15"&gt;https://huggingface.co/collections/tencent/hy-mt15&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights: üîπ 1.8B On-Device Power: Optimized for consumer hardware with a 1GB memory footprint. Using on-policy distillation to align with larger models, it delivers 0.18s latency (50 tokens), outperforming mainstream commercial APIs. üîπ 7B SOTA Performance: An upgraded version of our WMT25 champion, surpassing mid-sized open-source models and rivaling the 90th percentile of closed-source giants like Gemini-3.0-Pro. üîπ 33+ Languages &amp;amp; Dialects: High-fidelity translation across 33 languages and 5 Chinese dialects. üîπ Production-Ready: Native support for custom terminology, long-dialogue context, and maintaining document formatting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pzcj1q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcj1q/tencent_opensource_tencenthymt15_featuring_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcj1q/tencent_opensource_tencenthymt15_featuring_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:11:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzfuqg</id>
    <title>Why Kimi K2 Thinking choose Int4 QAT, from infra enginner of KImi</title>
    <updated>2025-12-30T11:33:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzfuqg/why_kimi_k2_thinking_choose_int4_qat_from_infra/"&gt; &lt;img alt="Why Kimi K2 Thinking choose Int4 QAT, from infra enginner of KImi" src="https://b.thumbs.redditmedia.com/6XCL91-lLSt5Lf0kxdky6Te-XoW5kEkEa9zxhgmAQGg.jpg" title="Why Kimi K2 Thinking choose Int4 QAT, from infra enginner of KImi" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw the recent &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/"&gt;discussion&lt;/a&gt; here regarding MiniMax engineer's tweet about why they decided &lt;em&gt;against&lt;/em&gt; using int4 QAT for the MiniMax M2.1 model.&lt;/p&gt; &lt;p&gt;Interestingly, at the time of the K2 Thinking release, a Kimi infra engineer posted a deep dive on Zhihu explaining why native int4 QAT was actually crucial for them. I‚Äôve summarized the key takeaways below to offer a different perspective on the 'to quant or not to quant' debate.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Kimi found int4 QAT is essential for &lt;strong&gt;MoE latency&lt;/strong&gt;, &lt;strong&gt;long-context stability&lt;/strong&gt;, and &lt;strong&gt;speeding up the RL training loop&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Decoding is Memory-Bound (Latency Focus)&lt;/h1&gt; &lt;p&gt;Unlike the MiniMax case, Kimi found that for their specific MoE architecture (which is highly sparse), the decoding phase is almost exclusively memory-bound. By using W4A16 (4-bit weights, 16-bit activations), they reduced memory usage significantly. This allowed the model to fit on fewer GPUs, which reduced inter-device communication overhead, a major factor in lowering end-to-end latency for users.&lt;/p&gt; &lt;h1&gt;PTQ Failed at &amp;quot;Thinking&amp;quot; Lengths&lt;/h1&gt; &lt;p&gt;The team initially tried standard Post-Training Quantization (PTQ). While it worked for short responses, it fell apart for the long chain-of-thought &amp;quot;thinking&amp;quot; process. As generation length increased, quantization errors accumulated, leading to degradation. Furthermore, PTQ struggled with sparse experts; if an expert wasn't hit frequently during the calibration step with the calibration dataset, it essentially &amp;quot;forgot&amp;quot; knowledge. QAT (Quantization Aware Training) was necessary to make the model &amp;quot;lossless&amp;quot; compared to the BF16 baseline.&lt;/p&gt; &lt;h1&gt;A less discussed benefit: Faster RL Training&lt;/h1&gt; &lt;p&gt;This is the point that often gets overlooked: Int4 QAT wasn't just for inference serving, it accelerated the training process itself. In Reinforcement Learning, the model spends a massive amount of time in the &amp;quot;rollout&amp;quot; phase (generating text). By using the Int4 model for these rollouts, they reduced the total time for an RL iteration by 10-20%. It also reduced the discrepancy between the training forward pass and the inference engine.&lt;/p&gt; &lt;h1&gt;Why Int4 and not FP4?&lt;/h1&gt; &lt;p&gt;They chose standard Int4 over newer formats like FP4 to maintain compatibility with existing hardware (non-Blackwell GPUs) and to utilize mature, highly efficient kernels like Marlin.&lt;/p&gt; &lt;p&gt;In summary, I believe there isn't a one-size-fits-all answer regarding quantization. It depends heavily on the model's parameters and specific architecture. It is a matter of trade-offs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dzmceu5zybag1.png?width=1362&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0ba8f78c6e5ade3463a1c62fba1d338a1c01ce9"&gt; AI translation, there may be some translation errors.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzfuqg/why_kimi_k2_thinking_choose_int4_qat_from_infra/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzfuqg/why_kimi_k2_thinking_choose_int4_qat_from_infra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzfuqg/why_kimi_k2_thinking_choose_int4_qat_from_infra/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T11:33:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzggbf</id>
    <title>Running GLM-4.7 (355B MoE) in Q8 at ~5 Tokens/s on 2015 CPU-Only Hardware ‚Äì Full Optimization Guide</title>
    <updated>2025-12-30T12:05:52+00:00</updated>
    <author>
      <name>/u/at0mi</name>
      <uri>https://old.reddit.com/user/at0mi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzggbf/running_glm47_355b_moe_in_q8_at_5_tokenss_on_2015/"&gt; &lt;img alt="Running GLM-4.7 (355B MoE) in Q8 at ~5 Tokens/s on 2015 CPU-Only Hardware ‚Äì Full Optimization Guide" src="https://preview.redd.it/2eimvrgo0cag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a97232a34737346670be6cb9292a1cdde03aa47a" title="Running GLM-4.7 (355B MoE) in Q8 at ~5 Tokens/s on 2015 CPU-Only Hardware ‚Äì Full Optimization Guide" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ! If you're passionate about squeezing every last bit of performance out of older hardware for local large language models, I've got something exciting to share. I managed to get GLM-4.7 ‚Äì that's the massive 355B parameter Mixture of Experts model ‚Äì running in Q8_0 quantization on a seriously vintage setup: a 2015 Lenovo System x3950 X6 with eight Xeon E7-8880 v3 CPUs (no GPU in sight, just pure CPU inference). After a bunch of trial and error, I'm hitting around 5-6 tokens per second, which is pretty respectable for such an ancient beast. The Q8 quantization delivers extremely high quality outputs, preserving nearly all the model's intelligence with minimal degradation ‚Äì it's practically indistinguishable from full precision for most tasks.&lt;/p&gt; &lt;p&gt;The key was optimizing everything from BIOS settings (like enabling hyper-threading and tweaking power management) to NUMA node distribution for better memory access, and experimenting with different llama.cpp forks to handle the MoE architecture efficiently. I also dove into Linux kernel tweaks, like adjusting CPU governors and hugepages, to minimize latency. Keep in mind, this setup draws about 1300W AC under full load, so it's power-hungry but worth it for local runs. Benchmarks show solid performance for generation tasks, though it's not blazing fast ‚Äì perfect for homelab enthusiasts or those without access to modern GPUs.&lt;/p&gt; &lt;p&gt;I documented the entire process chronologically in this blog post, including step-by-step setup, code snippets, potential pitfalls, and full performance metrics: &lt;a href="https://postl.ai/2025/12/29/glm47on3950x6/?referrer=grok.com"&gt;https://postl.ai/2025/12/29/glm47on3950x6/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone else tried pushing big MoE models like this on CPU-only rigs? What optimizations worked for you, or what models are you running on similar hardware? Let's discuss!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/at0mi"&gt; /u/at0mi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2eimvrgo0cag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzggbf/running_glm47_355b_moe_in_q8_at_5_tokenss_on_2015/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzggbf/running_glm47_355b_moe_in_q8_at_5_tokenss_on_2015/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T12:05:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzhcqu</id>
    <title>Any guesses?</title>
    <updated>2025-12-30T12:52:15+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhcqu/any_guesses/"&gt; &lt;img alt="Any guesses?" src="https://preview.redd.it/xqvj95zv8cag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=30044fbca7ba499223943c95d7d236600fdbb10e" title="Any guesses?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xqvj95zv8cag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhcqu/any_guesses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhcqu/any_guesses/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T12:52:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzg32r</id>
    <title>Solar 100B claimed that it counts better than GPT today</title>
    <updated>2025-12-30T11:46:20+00:00</updated>
    <author>
      <name>/u/Icy_Company_6216</name>
      <uri>https://old.reddit.com/user/Icy_Company_6216</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzg32r/solar_100b_claimed_that_it_counts_better_than_gpt/"&gt; &lt;img alt="Solar 100B claimed that it counts better than GPT today" src="https://preview.redd.it/kxyfw9z2xbag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7132ef64860af64198c058381a4037b3b7c69818" title="Solar 100B claimed that it counts better than GPT today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Company_6216"&gt; /u/Icy_Company_6216 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kxyfw9z2xbag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzg32r/solar_100b_claimed_that_it_counts_better_than_gpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzg32r/solar_100b_claimed_that_it_counts_better_than_gpt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T11:46:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz68fz</id>
    <title>Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.</title>
    <updated>2025-12-30T02:43:48+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/"&gt; &lt;img alt="Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market." src="https://preview.redd.it/ocq43c2a79ag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e9f7477bee69f806ab9bab82c73557ea1345393" title="Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ocq43c2a79ag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T02:43:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzcrtb</id>
    <title>Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model</title>
    <updated>2025-12-30T08:26:06+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/"&gt; &lt;img alt="Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model" src="https://preview.redd.it/yq8uriwhxaag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d990cf5383783b3e2aa22351ddeb29ebac5eb2b2" title="Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are excited to open-source Tencent HY-Motion 1.0, a billion-parameter text-to-motion model built on the Diffusion Transformer (DiT) architecture and flow matching. Tencent HY-Motion 1.0 empowers developers and individual creators alike by transforming natural language into high-fidelity, fluid, and diverse 3D character animations, delivering exceptional instruction-following capabilities across a broad range of categories. The generated 3D animation assets can be seamlessly integrated into typical 3D animation pipelines.&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;p&gt;üîπBillion-Scale DiT: Successfully scaled flow-matching DiT to 1B+ parameters, setting a new ceiling for instruction-following capability and generated motion quality.&lt;/p&gt; &lt;p&gt;üîπFull-Stage Training Strategy: The industry‚Äôs first motion generation model featuring a complete Pre-training ‚Üí SFT ‚Üí RL loop to optimize physical plausibility and semantic accuracy.&lt;/p&gt; &lt;p&gt;üîπComprehensive Category Coverage: Features 200+ motion categories across 6 major classes‚Äîthe most comprehensive in the industry, curated via a meticulous data pipeline.&lt;/p&gt; &lt;p&gt;üåêProject Page: &lt;a href="https://hunyuan.tencent.com/motion"&gt;https://hunyuan.tencent.com/motion&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîóGithub: &lt;a href="https://github.com/Tencent-Hunyuan/HY-Motion-1.0"&gt;https://github.com/Tencent-Hunyuan/HY-Motion-1.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§óHugging Face: &lt;a href="https://huggingface.co/tencent/HY-Motion-1.0"&gt;https://huggingface.co/tencent/HY-Motion-1.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üìÑTechnical report: &lt;a href="https://arxiv.org/pdf/2512.23464"&gt;https://arxiv.org/pdf/2512.23464&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yq8uriwhxaag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T08:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz7bmv</id>
    <title>Llama-3.3-8B-Instruct</title>
    <updated>2025-12-30T03:34:19+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/"&gt; &lt;img alt="Llama-3.3-8B-Instruct" src="https://external-preview.redd.it/F-RvVhAB2x8ac9OzOxDw905YUEWDIOQBeDMa2ZyMwo4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a109a6917bc66847cb36c61990f58523049b666" title="Llama-3.3-8B-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GGUF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;from &lt;strong&gt;allura-forge&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct#llama-33-8b-instruct"&gt;&lt;/a&gt;&lt;strong&gt;Llama 3.3 8B Instruct&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Yes, this is official, and yes, this is, to my knowledge, a real version of Llama 3.3 8B. (I think, anyways)&lt;/p&gt; &lt;p&gt;Facebook has a &lt;a href="https://llama.developer.meta.com"&gt;Llama API&lt;/a&gt; available that allows for inference of the other Llama models (L3.3 70B, L4 Scout and Maverick), but &lt;em&gt;also&lt;/em&gt; includes a special, new (according to the original press release) &amp;quot;Llama 3.3 8B&amp;quot; that didn't exist anywhere else and was stuck behind the Facebook API!&lt;/p&gt; &lt;p&gt;However. The Llama API supports finetuning L3.3... &lt;em&gt;and downloading the final model in HF format.&lt;/em&gt; Problem solved, right?&lt;/p&gt; &lt;p&gt;Wellllllllllllllll. Not really. The finetuning API was hidden behind layers of support tickets. I tried when the original API dropped in April, and was just told &amp;quot;We'll think about it and send you any updates&amp;quot; (there never were any updates).&lt;/p&gt; &lt;p&gt;Flash forward to December, on a whim I decide to look at the API again. And... by god... the finetuning tab was there. I could click on it and start a job (please ignore that I have no idea how it works, and in fact the finetuning tab actually disappeared after the first time I clicked on it, though I could still manually go to the page).&lt;/p&gt; &lt;p&gt;Apparently, this was not very well tested, as there were a good few bugs, the UI was janky, and the download model function did not actually work due to CORS (I had to manually curl things to get the CDN link).&lt;/p&gt; &lt;p&gt;But... by god... the zip file downloaded, and I had my slightly finetuned model.&lt;/p&gt; &lt;p&gt;To my shock and delight, however, they also provide the adapter that they merged into the model. That means I can &lt;em&gt;subtract&lt;/em&gt; that adapter and get the original model. And... here we are!&lt;/p&gt; &lt;p&gt;(actually, it should be ‚Äúnew model,‚Äù but I used ‚Äúother‚Äù to avoid triggering people)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T03:34:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
