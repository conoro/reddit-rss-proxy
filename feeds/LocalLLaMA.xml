<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-27T10:39:56+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ohalnb</id>
    <title>Running OrKa GraphScout plus Plan Validator locally with small models</title>
    <updated>2025-10-27T10:15:40+00:00</updated>
    <author>
      <name>/u/marcosomma-OrKA</name>
      <uri>https://old.reddit.com/user/marcosomma-OrKA</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohalnb/running_orka_graphscout_plus_plan_validator/"&gt; &lt;img alt="Running OrKa GraphScout plus Plan Validator locally with small models" src="https://preview.redd.it/jx3x5vyhqmxf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3aae748096ea511d02e40e27ade18f6692538e9" title="Running OrKa GraphScout plus Plan Validator locally with small models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I paired two parts of OrKa to make local agent workflows less brittle on CPU only setups.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GraphScout proposes a minimal plan that satisfies an intent with cost awareness&lt;/li&gt; &lt;li&gt;Plan Validator grades that plan across completeness, efficiency, safety, coherence, and fallback, then returns structured fixes&lt;/li&gt; &lt;li&gt;A short loop applies fixes and revalidates until the score clears a threshold, then the executor runs&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why this helps on local boxes&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lower variance: validator runs at low temperature and prefers consistent grading&lt;/li&gt; &lt;li&gt;Cost control: efficiency is a first class dimension, so you catch high token defaults before execution&lt;/li&gt; &lt;li&gt;Safer tool use: validator blocks plans that call the network or code without limits&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Practical tips&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use 3B to 8B instruction models for both scout and validator&lt;/li&gt; &lt;li&gt;Validator temperature 0.1, top p 0.9&lt;/li&gt; &lt;li&gt;Keep validator outputs compact JSON to reduce tokens&lt;/li&gt; &lt;li&gt;Loop budget 3 rounds, threshold 0.85 to 0.88&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Docs and examples: &lt;a href="https://github.com/marcosomma/orka-reasoning"&gt;https://github.com/marcosomma/orka-reasoning&lt;/a&gt;&lt;br /&gt; If you want a minimal local config, say your CPU class and I will reply with a tuned YAML and token limits.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marcosomma-OrKA"&gt; /u/marcosomma-OrKA &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jx3x5vyhqmxf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohalnb/running_orka_graphscout_plus_plan_validator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohalnb/running_orka_graphscout_plus_plan_validator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T10:15:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogznvh</id>
    <title>Tested a few small models on a local CLI agent. I was surprised by the results.</title>
    <updated>2025-10-26T23:46:37+00:00</updated>
    <author>
      <name>/u/dsartori</name>
      <uri>https://old.reddit.com/user/dsartori</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building a CLI-based tool-using agent for my own purposes. &lt;/p&gt; &lt;p&gt;I've mostly used cloud models for this work up until now, but I had a little time today and decided to run some benchmark tests against the small models I have on my PC with a 16 GB 4060. &lt;/p&gt; &lt;p&gt;My agent has a number of categorized tools at its disposal (categories: web, files, system, dev, containers). These tools do things like list processes, measure memory usage, examine git repositories and so on - all kinds of stuff you can do with read-only access to the local system.&lt;/p&gt; &lt;p&gt;I ran a small suite of prompts through each of the models I had on hand to assess their ability to select the correct tools and provide a useful response. &lt;/p&gt; &lt;p&gt;These are the models I tested, in order of viability for this purpose: &lt;/p&gt; &lt;p&gt;- Qwen3:4b is the clear leader with excellent quality outputs&lt;br /&gt; - Llama3.2:3b provides pretty solid responses but needs heavier prompting to select the right tools&lt;br /&gt; - Granite3.3:8b, which has excellent quality when it works (about half the time)&lt;br /&gt; - Qwen3:0.6b just doesn't have the &amp;quot;brain power&amp;quot; to figure out complex tool chains&lt;br /&gt; - Phi4:14b, which couldn't use any tools at all&lt;/p&gt; &lt;p&gt;None of this is to say that my results are gospel for anyone else, but I think it's really surprising and interesting how useful that little llama model is for my agent. Goes to show that benchmarks are one thing but testing for your own use case is critical. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dsartori"&gt; /u/dsartori &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogznvh/tested_a_few_small_models_on_a_local_cli_agent_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogznvh/tested_a_few_small_models_on_a_local_cli_agent_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogznvh/tested_a_few_small_models_on_a_local_cli_agent_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T23:46:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohaq6f</id>
    <title>My Model's Latest Status</title>
    <updated>2025-10-27T10:23:34+00:00</updated>
    <author>
      <name>/u/Patience2277</name>
      <uri>https://old.reddit.com/user/Patience2277</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohaq6f/my_models_latest_status/"&gt; &lt;img alt="My Model's Latest Status" src="https://b.thumbs.redditmedia.com/cwMduvGyYWoO3ap1ztdoNSP63cRQ06lvKHn1cPTWtKo.jpg" title="My Model's Latest Status" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is how it always responds whenever I ask about upgrades, lol. It seems to be slightly &lt;strong&gt;overfitted&lt;/strong&gt;, but I think it's fine for now, haha.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uf4rhyczrmxf1.png?width=877&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9584a43d4cceb6337ed4cb17217c467475b8e5fd"&gt;https://preview.redd.it/uf4rhyczrmxf1.png?width=877&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9584a43d4cceb6337ed4cb17217c467475b8e5fd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It actually refused to answer at the end &lt;strong&gt;ㅋㅋㅋㅋ&lt;/strong&gt;! The reason given was &lt;strong&gt;&amp;quot;Bad Request&amp;quot;&lt;/strong&gt; &lt;strong&gt;zzzzzzzzzzzzzzzzzzz&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qbrlqmd1smxf1.png?width=1181&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b28b30fddc066b6e6180007e13741d708c038729"&gt;https://preview.redd.it/qbrlqmd1smxf1.png?width=1181&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b28b30fddc066b6e6180007e13741d708c038729&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's pretty entertaining how it acts like it has consciousness!&lt;/p&gt; &lt;p&gt;Of course, it's just a &lt;strong&gt;lump of differentiation (or 'a bunch of matrices'),&lt;/strong&gt; though!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Patience2277"&gt; /u/Patience2277 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohaq6f/my_models_latest_status/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohaq6f/my_models_latest_status/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohaq6f/my_models_latest_status/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T10:23:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh7fze</id>
    <title>Fall of GPTQ and Rise of AWQ. Why exactly?</title>
    <updated>2025-10-27T06:42:22+00:00</updated>
    <author>
      <name>/u/everyoneisodd</name>
      <uri>https://old.reddit.com/user/everyoneisodd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So was looking for qwen3-VL-30BA3B GPTQ quant on huggingface, but was only able to find AWQ. For comparison qwen-2.5-vl did have GPTQ quant. Checked for other versions of the model as well, same issue.&lt;/p&gt; &lt;p&gt;Can someone explain why this is the case? &lt;/p&gt; &lt;p&gt;Based on my personal testing, latency wise GPTQ and AWQ were on par and performance wise GPTQ was better (tested on qwen-2.5-vl-7b and llama3-8b on vLLM) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/everyoneisodd"&gt; /u/everyoneisodd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh7fze/fall_of_gptq_and_rise_of_awq_why_exactly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh7fze/fall_of_gptq_and_rise_of_awq_why_exactly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh7fze/fall_of_gptq_and_rise_of_awq_why_exactly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T06:42:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogy9lh</id>
    <title>Quantizing MoE models to MXFP4</title>
    <updated>2025-10-26T22:43:53+00:00</updated>
    <author>
      <name>/u/noctrex</name>
      <uri>https://old.reddit.com/user/noctrex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lately its like my behind is on fire, and I'm downloading and quantizing models like crazy, but into this specific MXFP4 format only.&lt;/p&gt; &lt;p&gt;And cause of this format, it can be done only on Mixture-of-Expert models.&lt;/p&gt; &lt;p&gt;Why, you ask?&lt;/p&gt; &lt;p&gt;Why not!, I respond.&lt;/p&gt; &lt;p&gt;Must be my ADHD brain cause I couldn't find a MXFP4 model quant I wanted to test out, and I said to myself, why not quantize some more and uplaod them to hf?&lt;/p&gt; &lt;p&gt;So here we are.&lt;/p&gt; &lt;p&gt;I just finished quantizing one of the huge models, DeepSeek-V3.1-Terminus, and the MXFP4 is a cool 340GB... &lt;/p&gt; &lt;p&gt;But I can't run this on my PC! I've got a bunch of RAM, but it reads most of it from disk and the speed is like 1 token per day.&lt;/p&gt; &lt;p&gt;Anyway, I'm uploading it.&lt;/p&gt; &lt;p&gt;And I want to ask you, would you like me to quantize other such large models? Or is it just a waste?&lt;/p&gt; &lt;p&gt;You know the other large ones, like Kimi-K2-Instruct-0905, or DeepSeek-R1-0528, or cogito-v2-preview-deepseek-671B-MoE&lt;/p&gt; &lt;p&gt;Do you have any suggestion for other MoE ones that are not in MXFP4 yet?&lt;/p&gt; &lt;p&gt;Ah yes here is the link:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/noctrex"&gt;https://huggingface.co/noctrex&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noctrex"&gt; /u/noctrex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogy9lh/quantizing_moe_models_to_mxfp4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogy9lh/quantizing_moe_models_to_mxfp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogy9lh/quantizing_moe_models_to_mxfp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T22:43:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh4k7k</id>
    <title>How to Quantize TTS and ASR models to fit in VRAM ?</title>
    <updated>2025-10-27T03:46:59+00:00</updated>
    <author>
      <name>/u/bull_bear25</name>
      <uri>https://old.reddit.com/user/bull_bear25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have created conversational bot system it is working fine from backend but it is failing in the application due to VRAM overflow (8 GB VRAM)&lt;/p&gt; &lt;p&gt;I am working on tight budget. How do I quantize both these models from FP16 to Q8 or Q6 to manage the memory budget?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bull_bear25"&gt; /u/bull_bear25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh4k7k/how_to_quantize_tts_and_asr_models_to_fit_in_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh4k7k/how_to_quantize_tts_and_asr_models_to_fit_in_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh4k7k/how_to_quantize_tts_and_asr_models_to_fit_in_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T03:46:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogo2jv</id>
    <title>I made a 1B model to generate 3d files (barely)</title>
    <updated>2025-10-26T15:52:22+00:00</updated>
    <author>
      <name>/u/ThomasPhilli</name>
      <uri>https://old.reddit.com/user/ThomasPhilli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;2 weeks ago, I finetuned Gemma3 1B on Synthetic 3D file data. I called the model K-1B.&lt;/p&gt; &lt;p&gt;Yesterday I packaged it into an app, hosting the model on Modal.&lt;/p&gt; &lt;p&gt;I would appreciate any feedback as this is a hobby project that I will keep on training the model etc.&lt;/p&gt; &lt;p&gt;Thanks :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasPhilli"&gt; /u/ThomasPhilli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://cadmonkey.web.app"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogo2jv/i_made_a_1b_model_to_generate_3d_files_barely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogo2jv/i_made_a_1b_model_to_generate_3d_files_barely/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T15:52:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogg6sz</id>
    <title>Why didn't LoRA catch on with LLMs?</title>
    <updated>2025-10-26T09:22:15+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;Explanation of LoRA for the folks at home&lt;/h2&gt; &lt;p&gt;(skip to next section if you already know what Lora is)&lt;/p&gt; &lt;p&gt;I only know it from the image generation Stable Diffusion world, and I only tried that briefly, so this won't be 100% exact.&lt;/p&gt; &lt;p&gt;Let's say your image generation model is Stable Diffusion 1.5, which came out a few years ago. It can't know the artstyle of a new artist that came up in the past year, let's say his name his Bobsolete.&lt;/p&gt; &lt;p&gt;What lora creators did is create a small dataset of Bobsolete's art, and use it to train SD 1.5 for like 1-2 days. This outputs a small lora file (the SD 1.5 model is 8GB, a lora is like 20MB). Users can download this lora, and when loading SD 1.5, say &amp;quot;also attach Bobsolete.lora to the model&amp;quot;. Now the user is interacting with SD 1.5 that has been augmented with knowledge of Bobsolete. The user can specify &amp;quot;drawn in the style of Bobsolete&amp;quot; and it will work.&lt;/p&gt; &lt;p&gt;Loras are used to add new styles to a model, new unique characters, and so on.&lt;/p&gt; &lt;h2&gt;Back to LLMs&lt;/h2&gt; &lt;p&gt;LLMs apparently support loras, but no one seems to use them. I've never ever seen them discussed on this sub in my 2 years of casual browsing, although I see they exist in the search results.&lt;/p&gt; &lt;p&gt;I was wondering why this hasn't caught on. People could add little bodies of knowledge to an already-released model. For example, you take a solid general model like Gemma 3 27B. Someone could release a lora trained on all scifi books, another based on all major movie scripts, etc. You could then &amp;quot;./llama.cpp -m models/gemma3.gguf --lora models/scifi-books-rev6.lora --lora models/movie-scripts.lora&amp;quot; and try to get Gemma 3 to help you write a modern scifi movie script. You could even focus even more on specific authors, cormac-mccarthy.lora etc.&lt;/p&gt; &lt;p&gt;A more useful/legal example would be attaching current-events-2025.lora to a model whose cutoff date was December 2024. &lt;/p&gt; &lt;p&gt;So why didn't this catch on the way it did in the image world? Is this technology inherently more limited on LLMs? Why does it seem like companies interested in integrating their doc with AI are more focused on RAG than training a Lora on their internal docs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogg6sz/why_didnt_lora_catch_on_with_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogg6sz/why_didnt_lora_catch_on_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogg6sz/why_didnt_lora_catch_on_with_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T09:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogz0b7</id>
    <title>Oh my REAP-ness. Qwen3-Coder-30B-A3B-Instruct_Pruned_REAP-15B-A3B-GGUF on BC-250</title>
    <updated>2025-10-26T23:16:46+00:00</updated>
    <author>
      <name>/u/MachineZer0</name>
      <uri>https://old.reddit.com/user/MachineZer0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR: AMD BC-250 running Vulkan Llama.cpp with REAP Qwen3-Coder-30B-A3B-Instruct Q4 clocking in at 100/70 tok/s&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here is a post I did a while back super impressed with Llama 3.1 running ~27 tok/s tg on An AMD BC-250 with Vulkan drivers.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1h8el9m/metallama318binstructq8_0gguf_2689_toks_for_20/"&gt;Meta-Llama-3.1-8B-Instruct-Q8_0.gguf - 26.89 tok/s for $20 : r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For giggles today I dusted off my bench BC-250 and recompiled the latest llama.cpp and was pleasantly surprised to see almost 30% uplift in pp &amp;amp; tg. See below:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;slot launch_slot_: id 0 | task 513 | processing task slot update_slots: id 0 | task 513 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 45 slot update_slots: id 0 | task 513 | old: ... are an expert of | food and food preparation. What slot update_slots: id 0 | task 513 | new: ... are an expert of | agentic coding systems. If slot update_slots: id 0 | task 513 | 527 459 6335 315 3691 323 3691 18459 13 3639 slot update_slots: id 0 | task 513 | 527 459 6335 315 945 4351 11058 6067 13 1442 slot update_slots: id 0 | task 513 | n_past = 10, memory_seq_rm [10, end) slot update_slots: id 0 | task 513 | prompt processing progress, n_past = 45, n_tokens = 35, progress = 1.000000 slot update_slots: id 0 | task 513 | prompt done, n_past = 45, n_tokens = 35 slot print_timing: id 0 | task 513 | prompt eval time = 282.75 ms / 35 tokens ( 8.08 ms per token, 123.78 tokens per second) eval time = 23699.99 ms / 779 tokens ( 30.42 ms per token, 32.87 tokens per second) total time = 23982.74 ms / 814 tokens slot release: id 0 | task 513 | stop processing: n_past = 823, truncated = 0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I thought I would give the 50% REAP Qwen3-Coder-30B-A3B-Instruct a shot with Q4_K_M which should fit within the 10gb of 16gb visible to llama.cpp&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/12bitmisfit/Qwen3-Coder-30B-A3B-Instruct_Pruned_REAP-15B-A3B-GGUF"&gt;12bitmisfit/Qwen3-Coder-30B-A3B-Instruct_Pruned_REAP-15B-A3B-GGUF · Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;YOOOO! nearly 100 tok/s pp and 70 tok/s tg&lt;/p&gt; &lt;pre&gt;&lt;code&gt;slot update_slots: id 0 | task 2318 | new: ... &amp;lt;|im_start|&amp;gt;user | You are a master of the slot update_slots: id 0 | task 2318 | 151644 872 198 14374 5430 510 31115 264 63594 slot update_slots: id 0 | task 2318 | 151644 872 198 2610 525 264 7341 315 279 slot update_slots: id 0 | task 2318 | n_past = 3, memory_seq_rm [3, end) slot update_slots: id 0 | task 2318 | prompt processing progress, n_past = 54, n_tokens = 51, progress = 1.000000 slot update_slots: id 0 | task 2318 | prompt done, n_past = 54, n_tokens = 51 slot print_timing: id 0 | task 2318 | prompt eval time = 520.59 ms / 51 tokens ( 10.21 ms per token, 97.97 tokens per second) eval time = 22970.01 ms / 1614 tokens ( 14.23 ms per token, 70.27 tokens per second) total time = 23490.60 ms / 1665 tokens slot release: id 0 | task 2318 | stop processing: n_past = 1667, truncated = 0 srv update_slots: all slots are idle &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;You are a master of the Pyspark eco system. At work we have a full blown Enterprise Databricks deployment. We want to practice at home. We already have a Kubernetes Cluster. Walk me through deployment and configuration.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Output pastebin:&lt;br /&gt; &lt;a href="https://pastebin.com/728Pw4Y9"&gt;Oh my REAP-ness. Qwen3-Coder-30B-A3B-Instruct_Pruned_REAP-15B-A3B-GGUF on BC-250 - Pastebin.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Proof of speed:&lt;br /&gt; &lt;a href="https://youtu.be/n1qEnGSk6-c"&gt;https://youtu.be/n1qEnGSk6-c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks to u/&lt;a href="https://www.reddit.com/user/12bitmisfit/"&gt;12bitmisfit&lt;/a&gt;&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1octe2s/pruned_moe_reap_quants_for_testing/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1octe2s/pruned_moe_reap_quants_for_testing/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MachineZer0"&gt; /u/MachineZer0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogz0b7/oh_my_reapness_qwen3coder30ba3binstruct_pruned/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogz0b7/oh_my_reapness_qwen3coder30ba3binstruct_pruned/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogz0b7/oh_my_reapness_qwen3coder30ba3binstruct_pruned/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T23:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh06du</id>
    <title>Model named "ernie-exp-251022" spotted on Lmarena. Baidu cooking?</title>
    <updated>2025-10-27T00:10:45+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh06du/model_named_ernieexp251022_spotted_on_lmarena/"&gt; &lt;img alt="Model named &amp;quot;ernie-exp-251022&amp;quot; spotted on Lmarena. Baidu cooking?" src="https://preview.redd.it/aebg6a4zojxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5347258deaa2d8b9fd41517456f1916402365502" title="Model named &amp;quot;ernie-exp-251022&amp;quot; spotted on Lmarena. Baidu cooking?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those wondering, the prompt was to create a retro game character in html, single file. Nothing fancy. Usually models add some basic mechanics akin to the side scrollers.&lt;/p&gt; &lt;p&gt;There were some bugs in the code this model created, but so were in the code created by the model on the right side.&lt;/p&gt; &lt;p&gt;I must say apart from the bugs, the output was pretty impressive anyway on the left and felt much different than anything I encountered before. That and it was actually better than the output on the right overall, so I voted for it just to see which model it was and there you have it.&lt;/p&gt; &lt;p&gt;Model named ernie-exp-251022. What do you guys think it is? Baidu cooking, or something else entirely? Something cloud only, or perhaps open weight? So many questions...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aebg6a4zojxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh06du/model_named_ernieexp251022_spotted_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh06du/model_named_ernieexp251022_spotted_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T00:10:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh1kfe</id>
    <title>Built a full voice AI assistant running locally on my RX 6700 with Vulkan - Proof AMD cards excel at LLM inference</title>
    <updated>2025-10-27T01:16:28+00:00</updated>
    <author>
      <name>/u/Straight_Issue279</name>
      <uri>https://old.reddit.com/user/Straight_Issue279</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share something I've been working on that I think showcases what AMD hardware can really do for local AI.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I Built:&lt;/strong&gt; A complete AI assistant named Aletheia that runs 100% locally on my AMD RX 6700 10GB using Vulkan acceleration. She has: - Real-time voice interaction (speaks and listens) - Persistent memory across sessions - Emotional intelligence system - Vector memory for semantic recall - 20+ integrated Python modules&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Setup:&lt;/strong&gt; - GPU: AMD Radeon RX 6700 10GB - CPU: AMD Ryzen 7 9800X3D - RAM: 32GB DDR5 - OS: Windows 11 Pro - Backend: llama.cpp with Vulkan (45 GPU layers) - Model: Mistral-7B Q6_K quantization&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why This Matters:&lt;/strong&gt; Everyone assumes you need a $2000 NVIDIA GPU for local AI. I'm proving that's wrong. Consumer AMD cards with Vulkan deliver excellent performance without needing ROCm (which doesn't support consumer cards anyway).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Unique Part:&lt;/strong&gt; I'm not a programmer. I built this entire system using AI-assisted development - ChatGPT and Claude helped me write the code while I provided the vision and troubleshooting. This represents the democratization of AI that AMD enables with accessible hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance:&lt;/strong&gt; Running Mistral-7B with full voice integration, persistent memory, and real-time processing. The RX 6700 handles it beautifully with Vulkan acceleration.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I'm Posting:&lt;/strong&gt; 1. To show AMD users that local LLM inference works great on consumer cards 2. To document that Windows + AMD + Vulkan is a viable path 3. To prove you don't need to be a developer to build amazing things with AMD hardware&lt;/p&gt; &lt;p&gt;I'm documenting the full build process and considering reaching out to AMD to showcase what their hardware enables. If there's interest, I'm happy to share technical details, the prompts I used with AI tools, or my troubleshooting process.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Built a fully functional voice AI assistant on a mid-range AMD GPU using Vulkan. Proves AMD is the accessible choice for local AI.&lt;/p&gt; &lt;p&gt;Happy to answer questions about the build process, performance, or how I got Vulkan working on Windows!&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Specs for the curious: - Motherboard: ASRock X870 Pro RS - Vulkan SDK: 1.3.290.0 - TTS: Coqui TTS (Jenny voice) - STT: Whisper Small with DirectML - Total project cost: ~$1200 (all AMD)&lt;/p&gt; &lt;p&gt;UPDATE Thanks for the feedback, all valid points:&lt;/p&gt; &lt;p&gt;Re: GitHub - You're right, I should share code. Sanitizing personal memory files and will push this week.&lt;/p&gt; &lt;p&gt;Re: 3060 vs 6700 - Completely agree 3060 12GB is better value for pure AI workloads. I already owned the 6700 for gaming. My angle is &amp;quot;if you already have AMD consumer hardware, here's how to make it work with Vulkan&amp;quot; not &amp;quot;buy AMD for AI.&amp;quot; Should have been clearer.&lt;/p&gt; &lt;p&gt;Re: &amp;quot;Nothing special&amp;quot; - Fair. The value I'm offering is: (1) Complete Windows/AMD/Vulkan documentation (less common than Linux/NVIDIA guides), (2) AI-assisted development process for non-programmers, (3) Full troubleshooting guide. If that's not useful to you, no problem.&lt;/p&gt; &lt;p&gt;Re: Hardware choice - Yeah, AMD consumer cards aren't optimal for AI. But lots of people already have them and want to try local LLMs without buying new hardware. That's who this is for.&lt;/p&gt; &lt;p&gt;My original post overstated the &amp;quot;AMD excels&amp;quot; angle. More accurate: &amp;quot;AMD consumer cards are serviceable for local &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Straight_Issue279"&gt; /u/Straight_Issue279 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh1kfe/built_a_full_voice_ai_assistant_running_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh1kfe/built_a_full_voice_ai_assistant_running_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh1kfe/built_a_full_voice_ai_assistant_running_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T01:16:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh9gai</id>
    <title>How powerful are phones for AI workloads today?</title>
    <updated>2025-10-27T09:00:58+00:00</updated>
    <author>
      <name>/u/Henrie_the_dreamer</name>
      <uri>https://old.reddit.com/user/Henrie_the_dreamer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran a quick experiment to understand how many activated params a model needs to perform optimally on phones. &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;File size&lt;/th&gt; &lt;th&gt;Nothing 3a &amp;amp; Pixel 6a CPU&lt;/th&gt; &lt;th&gt;Galaxy S25 Ultra &amp;amp; iPhone 17 Pro CPU&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Gemma3-270M-INT8&lt;/td&gt; &lt;td&gt;170mb&lt;/td&gt; &lt;td&gt;~30 toks/sec&lt;/td&gt; &lt;td&gt;~148 toks/sec&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LFM2-350M-INT8&lt;/td&gt; &lt;td&gt;233mb&lt;/td&gt; &lt;td&gt;~26 toks/sec&lt;/td&gt; &lt;td&gt;~130 toks/sec&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-600M-INT8&lt;/td&gt; &lt;td&gt;370mb&lt;/td&gt; &lt;td&gt;~20 toks/sec&lt;/td&gt; &lt;td&gt;~75 toks/sec&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LFM2-750M-INT8&lt;/td&gt; &lt;td&gt;467mb&lt;/td&gt; &lt;td&gt;~20 toks/sec&lt;/td&gt; &lt;td&gt;~75 toks/sec&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma3-1B-INT8&lt;/td&gt; &lt;td&gt;650mb&lt;/td&gt; &lt;td&gt;~14 toks/sec&lt;/td&gt; &lt;td&gt;~48 toks/sec&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LFM-1.2B-INT8&lt;/td&gt; &lt;td&gt;722mb&lt;/td&gt; &lt;td&gt;~13 toks/sec&lt;/td&gt; &lt;td&gt;~44 toks/sec&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-1.7B-INT8&lt;/td&gt; &lt;td&gt;1012mb&lt;/td&gt; &lt;td&gt;~8 toks/sec&lt;/td&gt; &lt;td&gt;~27 toks/sec&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;So, it might be tempting to suggest 8B-A1B model, but battery drain and heating makes it unusable in reality.&lt;/p&gt; &lt;p&gt;MOE makes sense since Qwen3-Next showed that 80B-A3B can beat dense 32B Qwen. &lt;/p&gt; &lt;p&gt;Task-specific models make sense because most mobile tasks are not that massive to need frontier models, and SLMs trained on specific tasks compete with generalist models 20x their size on the tasks. &lt;/p&gt; &lt;p&gt;An ideal setup would be 1B-A200m task-specific models. The file size at INT4 would be 330mb and the speed will go from 80-350 tokens/sec depending on the device. &lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;p&gt;N/B: The benchmarks were computed using &lt;a href="https://github.com/cactus-compute/cactus"&gt;Cactus&lt;/a&gt;. Context size for benchmarks 128, simple KVCache. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Henrie_the_dreamer"&gt; /u/Henrie_the_dreamer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh9gai/how_powerful_are_phones_for_ai_workloads_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh9gai/how_powerful_are_phones_for_ai_workloads_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh9gai/how_powerful_are_phones_for_ai_workloads_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T09:00:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh8jt8</id>
    <title>Lightweight coding model for 4 GB Vram</title>
    <updated>2025-10-27T07:58:44+00:00</updated>
    <author>
      <name>/u/HiqhAim</name>
      <uri>https://old.reddit.com/user/HiqhAim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, i was wondering if there is lightweight model for writing code that works on 4 GB Vram and 16 GB ram. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HiqhAim"&gt; /u/HiqhAim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh8jt8/lightweight_coding_model_for_4_gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh8jt8/lightweight_coding_model_for_4_gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh8jt8/lightweight_coding_model_for_4_gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T07:58:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogoyza</id>
    <title>Qwen3-VL-32B is really good. Quick test vs several other local models I keep on my workstation (details in comments)</title>
    <updated>2025-10-26T16:28:19+00:00</updated>
    <author>
      <name>/u/EmPips</name>
      <uri>https://old.reddit.com/user/EmPips</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogoyza/qwen3vl32b_is_really_good_quick_test_vs_several/"&gt; &lt;img alt="Qwen3-VL-32B is really good. Quick test vs several other local models I keep on my workstation (details in comments)" src="https://preview.redd.it/8a00jiy4ghxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d362439263cafe886a82048ec21177d435463df4" title="Qwen3-VL-32B is really good. Quick test vs several other local models I keep on my workstation (details in comments)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmPips"&gt; /u/EmPips &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8a00jiy4ghxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogoyza/qwen3vl32b_is_really_good_quick_test_vs_several/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogoyza/qwen3vl32b_is_really_good_quick_test_vs_several/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T16:28:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh079j</id>
    <title>Any Linux distro better than others for AI use?</title>
    <updated>2025-10-27T00:11:58+00:00</updated>
    <author>
      <name>/u/otto_delmar</name>
      <uri>https://old.reddit.com/user/otto_delmar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m choosing a new Linux distro for these use cases:&lt;/p&gt; &lt;p&gt;• Python development&lt;br /&gt; • Running “power-user” AI tools (e.g., Claude Desktop or similar)&lt;br /&gt; • Local LLM inference - small, optimized models only&lt;br /&gt; • Might experiment with inference optimization frameworks (TensorRT, etc.).&lt;br /&gt; • Potentially local voice recognition (Whisper?) if my hardware is good enough&lt;br /&gt; • General productivity use&lt;br /&gt; • Casual gaming (no high expectations)&lt;/p&gt; &lt;p&gt;For the type of AI tooling I mentioned, does any of the various Linux tribes have an edge over the others? ChatGPT - depending on how I ask it - has recommended either an Arch-based distro (e.g., Garuda) - or Ubuntu. Which seems.... decidedly undecided.&lt;/p&gt; &lt;p&gt;My setup is an HP Elitedesk 800 G4 SFF with i5-8500, currently 16GB RAM (can be expanded to 64GB), and a RTX-3050 low-profile GPU. I can also upgrade the CPU when needed.&lt;/p&gt; &lt;p&gt;Any and all thoughts greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/otto_delmar"&gt; /u/otto_delmar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh079j/any_linux_distro_better_than_others_for_ai_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh079j/any_linux_distro_better_than_others_for_ai_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh079j/any_linux_distro_better_than_others_for_ai_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T00:11:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogtdbg</id>
    <title>What is the real world hit of using PCIe 4.0 instead of PCIe 5.0 with a 5090?</title>
    <updated>2025-10-26T19:21:18+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m trying to be a bit “cheap” and just buy a 5090 for my desktop that is currently running a 3060. It’s a high end build 128gb RAM, video card is the worst part. I’ll probably slowly end up upgrading everything, but I would like to start with the GPU. &lt;/p&gt; &lt;p&gt;I’m assuming someone might have tried this already?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogtdbg/what_is_the_real_world_hit_of_using_pcie_40/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogtdbg/what_is_the_real_world_hit_of_using_pcie_40/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogtdbg/what_is_the_real_world_hit_of_using_pcie_40/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T19:21:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogwjdj</id>
    <title>What is the best local Large Language Model setup for coding on a budget of approximately $2,000?</title>
    <updated>2025-10-26T21:29:22+00:00</updated>
    <author>
      <name>/u/Independent-Band7571</name>
      <uri>https://old.reddit.com/user/Independent-Band7571</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My initial research has highlighted three main hardware options:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;A dedicated GPU with 16–32GB of VRAM.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;A Mac Ultra with 64GB+ of Unified Memory.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;An AMD Strix Halo system with 64–128GB of RAM.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My understanding is that all three options can run similar models at an acceptable t/s speed. In fact, they might even be overpowered if we are focusing on Mixture-of-Experts (MoE) models.&lt;/p&gt; &lt;p&gt;I'm also weighing the following trade-offs:&lt;/p&gt; &lt;p&gt;Mac Ultra: Appears to be the &amp;quot;sweet spot&amp;quot; due to its ease of setup and strong all-around performance, but I have a strong preference against the Apple ecosystem.&lt;/p&gt; &lt;p&gt;Strix Halo: The fully-specced mini-PC versions, often from Chinese manufacturers, already push the $2,000 budget limit. While the lower power consumption is appealing, I'm concerned about a potentially complicated setup and performance bottlenecks from its memory bandwidth and/or throttling due to thermals.&lt;/p&gt; &lt;p&gt;Multi-GPU PC: Building a system with multiple GPUs seems the most future-proof, but the high peak power consumption is a significant concern and hard limits on the models it can run.&lt;/p&gt; &lt;p&gt;What other considerations should I keep in mind? Are there any exciting new developments coming soon (either hardware or models), and should I hold off on buying anything right now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Band7571"&gt; /u/Independent-Band7571 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogwjdj/what_is_the_best_local_large_language_model_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogwjdj/what_is_the_best_local_large_language_model_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogwjdj/what_is_the_best_local_large_language_model_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T21:29:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh6vqf</id>
    <title>Token-Oriented Object Notation (TOON) - JSON for LLMs at half the token cost</title>
    <updated>2025-10-27T06:04:44+00:00</updated>
    <author>
      <name>/u/monnef</name>
      <uri>https://old.reddit.com/user/monnef</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh6vqf/tokenoriented_object_notation_toon_json_for_llms/"&gt; &lt;img alt="Token-Oriented Object Notation (TOON) - JSON for LLMs at half the token cost" src="https://external-preview.redd.it/VrvLLB_yWC1wqD45_cvqK9l_om9VPq0R0Yc-ww1E9Aw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=500c22355aceea8d22944381237591529ec9d1ae" title="Token-Oriented Object Notation (TOON) - JSON for LLMs at half the token cost" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/monnef"&gt; /u/monnef &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/johannschopplich/toon"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh6vqf/tokenoriented_object_notation_toon_json_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh6vqf/tokenoriented_object_notation_toon_json_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T06:04:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogxo2l</id>
    <title>New text diffusion model from inclusionAI - LLaDA2.0-flash-preview</title>
    <updated>2025-10-26T22:17:35+00:00</updated>
    <author>
      <name>/u/Finanzamt_Endgegner</name>
      <uri>https://old.reddit.com/user/Finanzamt_Endgegner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogxo2l/new_text_diffusion_model_from_inclusionai/"&gt; &lt;img alt="New text diffusion model from inclusionAI - LLaDA2.0-flash-preview" src="https://external-preview.redd.it/eSP0omdugNqFpul8K2DNpGSQodYj7BcPVqbrd3f9DmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=83c616737b8550a99d38eaf9d5ae4ca19a76a068" title="New text diffusion model from inclusionAI - LLaDA2.0-flash-preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/LLaDA2.0-flash-preview"&gt;https://huggingface.co/inclusionAI/LLaDA2.0-flash-preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As its smaller brother LLaDA2-mini-preview this is a text diffusion mixture of experts model but instead of only 16b total parameters this one comes with 100b total non embedding and 6b active parameters, which as far as I know makes it the biggest opensource text diffusion models out there.&lt;/p&gt; &lt;p&gt;**edit&lt;/p&gt; &lt;p&gt;The model does in fact work with longer contexts, though the official number is 4k, 128k could work, but I cant test that /:&lt;/p&gt; &lt;p&gt;So this isnt really a model for people who seek the best of the best (yet), but its certainly extremely cool that inclusionai decided to open source this experimental model (;&lt;/p&gt; &lt;p&gt;I think they released a new framework to run such diffusion models recently, otherwise there is no support outside of transformers as far as I know.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n0b8dgyg4jxf1.png?width=489&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02ca366acc269b87059dd2b0878e47650cb553c4"&gt;https://preview.redd.it/n0b8dgyg4jxf1.png?width=489&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02ca366acc269b87059dd2b0878e47650cb553c4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Finanzamt_Endgegner"&gt; /u/Finanzamt_Endgegner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogxo2l/new_text_diffusion_model_from_inclusionai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogxo2l/new_text_diffusion_model_from_inclusionai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogxo2l/new_text_diffusion_model_from_inclusionai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T22:17:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogybvr</id>
    <title>Qwen's VLM is strong!</title>
    <updated>2025-10-26T22:46:45+00:00</updated>
    <author>
      <name>/u/dulldata</name>
      <uri>https://old.reddit.com/user/dulldata</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogybvr/qwens_vlm_is_strong/"&gt; &lt;img alt="Qwen's VLM is strong!" src="https://preview.redd.it/jc97wpepbjxf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0895935673cc7bbe35bc8ea3a71d20d4837c8861" title="Qwen's VLM is strong!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dulldata"&gt; /u/dulldata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jc97wpepbjxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogybvr/qwens_vlm_is_strong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogybvr/qwens_vlm_is_strong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T22:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh6k6u</id>
    <title>Some usage notes on low-end CPU LLMs and home applications (/r/frugal meets /r/localLlama)</title>
    <updated>2025-10-27T05:44:43+00:00</updated>
    <author>
      <name>/u/___positive___</name>
      <uri>https://old.reddit.com/user/___positive___</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So a few weeks ago I discovered that Qwen3-4b is actually usable on any old laptop with CPU-only inference. Since then, I've been working on getting a simple home smart station set up using small LLMs. These are some notes on the LLMs and their usage that will hopefully be useful for anyone else thinking of doing similar hobby projects with dirt cheap components.&lt;/p&gt; &lt;p&gt;I scored a used Thinkpad for $200 with a Ryzen 4650U and 32GB DDR4 3200, perfect cosmetic condition. The key here is the 32GB RAM. I installed Ubuntu 24.04. I'm not a big Linux guy but it was painless and everything worked perfectly on the first try. The idea is to have a small self-contained system with a built-in monitor and keyboard to act like a smart whiteboard + Alexa.&lt;/p&gt; &lt;p&gt;Here are some inference numbers , pardon the plain formatting, all run with llama.cpp built for CPU only, all q4, using short test prompts:&lt;/p&gt; &lt;p&gt;Qwen3-4B-Instruct-2507 (q4): 29 tok/sec (PP), 11 tok/sec (TG), 1 sec (model load time). Running in Balanced Mode versus Performance Mode power settings had negligible difference.&lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B-Instruct-2507 (q4): 38 tok/sec (PP), 15 tok/sec (TG), 26 sec (model load time) for Balanced Mode. 44 tok/sec (PP), 15 tok/sec (TG), 17 sec (model load time) for Performance Mode.&lt;/p&gt; &lt;p&gt;Mistral-Small-3.2-24B-Instruct-2506 (q4): 5 tok/sec (PP), 2 tok/sec (TG), 12 sec (model load time) for Balanced mode. 5 tok/sec (PP), 2 tok/sec (TG), 4 sec (model load time) for Performance Mode.&lt;/p&gt; &lt;p&gt;Qwen3-30b-a3b is actually FASTER than Qwen3-4b and also performed better in my benchmarks for relevant tasks. But you need a lot of RAM to load it, which is why I specifically looked for the cheapest 32GB RAM laptop. Also, in my testing I found that the Qwen3-4b Thinking model would think for 3000 tokens to give a final 100 token result, which gave an effective generation rate of 0.1-0.2 tok/sec. So I would actually prefer a super slow non-instruct model like Mistral 24b at 2 tok/sec to a thinking model. However, Qwen3-30b-a3b is a nice compromise between speed and reliability.&lt;/p&gt; &lt;p&gt;Most of my use cases are non-interactive, like giving it an email to process and update a calendar. I do not need real time responses. For that reason, I didn't care about slow inference times within reason.&lt;/p&gt; &lt;p&gt;To get reliable performance, I had to split up tasks into simple subtasks. For example, I will ask the LLM to simply list all the topics from an email in the first step. In a second step, I ask the LLM to evaluate the relevancy of each topic in small batches. Then, I ask the LLM to extract JSON structures for each relevant event in order to update the calendar. On a 1000 word email with very high topic density (like a newsletter), Qwen3-30b-a3b would take roughly 9 minutes to process the entire workflow. I tweaked the workflow with various optimizations and could cut it down to about half. That's good enough for me.&lt;/p&gt; &lt;p&gt;I want to keep the power usage low, which means I'm not keeping the models warm. (I also stick to Balanced Mode.) That's why I wanted to record model load times as well. Again, most use cases are non-interactive. If I input a single event, like type &amp;quot;add this event on this time at this date&amp;quot;, the LLM will spin up and add it in under a minute.&lt;/p&gt; &lt;p&gt;I do have some light interactive uses. An example of that is asking for a timer while cooking. I might say &amp;quot;Alexa, set the timer for five minutes.&amp;quot; So here are some notes on that.&lt;/p&gt; &lt;p&gt;First, I use Openwakeword to trigger the whole process so that my laptop is not always running models and recording sound. Openwakeword is pre-tuned for a few wake words, which is why I am using &amp;quot;Alexa&amp;quot; as the wake word for now. I believe this can be tuned in the future. As soon as the wake word is detected, I immediately fire up faster-distil-whisper-small.en and LFM2-8b-a1b. They only take a second each to load, and I'm talking for a few seconds, so there is no lag this way.&lt;/p&gt; &lt;p&gt;LFM2-8b-a1b loads in about 1 second for me and runs at about 25 tok/sec TG (forgot to write down the PP but it is fast too). It is much faster than the other models but not as good with anything requiring reasoning. However, I was surprised at how well it performs in two tasks: topic identification and JSON extraction. So in a 1000 word newsletter filled with 18 topics, LFM2-8b-a1b can reliably extract all 18 topics pretty much as well as Qwen3-30b-a3b. So it's great at summarization, essentially. LFM2-8b-a1b can also reliably form JSON structures. By the way, I am using the model at q8. q4 definitely performs worse. This model, however, is not good at reasoning. For example, if I ask the model to determine if a certain event is relevant or not, it does not perform well. So it is good for fast topic identification and JSON extraction.&lt;/p&gt; &lt;p&gt;I tried various whisper models. I ended up finding the faster-distil-whisper-small.en to be a good compromise between speed and reliability. A sentence like &amp;quot;Alexa, set the timer for 5 minutes&amp;quot; will get parsed in 1 sec, but not as well as I would like. However, if I set the beam_size to 10 (5 is the default, typically), then it takes 2 seconds but with decent reliability. The medium model is too slow, around 5+ seconds even with reduced beam_size, and the base model has horrible accuracy. So that worked for me.&lt;/p&gt; &lt;p&gt;However, to boost the reliability further, I take the output from faster-distil-whisper-small.en and pass it to LFM2-8b-a1b, which gives me a JSON with an action field and a parameter field or two. That gets used to trigger the downstream python script. The LFM2 inference adds about an additional second or so. I don't care about waiting a tiny amount in this case, so that works for me.&lt;/p&gt; &lt;p&gt;For voice commands for adding reminders or calendar events, I will use the LFM2 JSON extraction to trigger re-transcription of the recorded voice message with whisper-largev3. Then, throw it to Qwen3-30b-a3b for processing, since quality is more important than speed.&lt;/p&gt; &lt;p&gt;I almost forgot! Super important, but the built-in mic quality isn't great on laptops. I ended getting a cheap USB wired conference speakerphone for &amp;lt;$20 off ebay. The brand is EMEET, but I think any modern one probably works. Python interacts with the microphone using Pipewire. The microphone made a big difference in transcription quality. It has hardware level sound processing, noise cancellation, etc.&lt;/p&gt; &lt;p&gt;Basically, I am using Qwen3-30b-a3b to process messy inputs (typing, voice, emails) slowly and LFM2-8b-a1b to process messy voice transcription quickly. Again, this all runs on a dirt cheap, old 4650U processor.&lt;/p&gt; &lt;p&gt;This is an ongoing hobby project. I want to eventually see if I can take pictures with the built-in webcam of physical mail or receipts and get one of the VL models or an OCR model to process it. There are trivial things to add, like verbal commands to check the weather and such. A whole bunch of other ideas.&lt;/p&gt; &lt;p&gt;I am loving the low-end LLM ecosystem. The cool part is that the stuff you make actually affects people around you! Like it actually gets used! The Qwen3 and LFM2 models I use are my favorites so far.&lt;/p&gt; &lt;p&gt;Okay, now back to you guys with your 8 x H100 basement setups...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/___positive___"&gt; /u/___positive___ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh6k6u/some_usage_notes_on_lowend_cpu_llms_and_home/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh6k6u/some_usage_notes_on_lowend_cpu_llms_and_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh6k6u/some_usage_notes_on_lowend_cpu_llms_and_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T05:44:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogwf6b</id>
    <title>M5 Neural Accelerator benchmark results from Llama.cpp</title>
    <updated>2025-10-26T21:24:33+00:00</updated>
    <author>
      <name>/u/auradragon1</name>
      <uri>https://old.reddit.com/user/auradragon1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;LLaMA 7B&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;SoC&lt;/th&gt; &lt;th align="right"&gt;BW [GB/s]&lt;/th&gt; &lt;th align="right"&gt;GPU Cores&lt;/th&gt; &lt;th align="right"&gt;F16 PP [t/s]&lt;/th&gt; &lt;th align="right"&gt;F16 TG [t/s]&lt;/th&gt; &lt;th align="right"&gt;Q8_0 PP [t/s]&lt;/th&gt; &lt;th align="right"&gt;Q8_0 TG [t/s]&lt;/th&gt; &lt;th align="right"&gt;Q4_0 PP [t/s]&lt;/th&gt; &lt;th align="right"&gt;Q4_0 TG [t/s]&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M1 [1]&lt;/td&gt; &lt;td align="right"&gt;68&lt;/td&gt; &lt;td align="right"&gt;7&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;108.21&lt;/td&gt; &lt;td align="right"&gt;7.92&lt;/td&gt; &lt;td align="right"&gt;107.81&lt;/td&gt; &lt;td align="right"&gt;14.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M1 [1]&lt;/td&gt; &lt;td align="right"&gt;68&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;117.25&lt;/td&gt; &lt;td align="right"&gt;7.91&lt;/td&gt; &lt;td align="right"&gt;117.96&lt;/td&gt; &lt;td align="right"&gt;14.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M1 Pro [1]&lt;/td&gt; &lt;td align="right"&gt;200&lt;/td&gt; &lt;td align="right"&gt;14&lt;/td&gt; &lt;td align="right"&gt;262.65&lt;/td&gt; &lt;td align="right"&gt;12.75&lt;/td&gt; &lt;td align="right"&gt;235.16&lt;/td&gt; &lt;td align="right"&gt;21.95&lt;/td&gt; &lt;td align="right"&gt;232.55&lt;/td&gt; &lt;td align="right"&gt;35.52&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M1 Pro [1]&lt;/td&gt; &lt;td align="right"&gt;200&lt;/td&gt; &lt;td align="right"&gt;16&lt;/td&gt; &lt;td align="right"&gt;302.14&lt;/td&gt; &lt;td align="right"&gt;12.75&lt;/td&gt; &lt;td align="right"&gt;270.37&lt;/td&gt; &lt;td align="right"&gt;22.34&lt;/td&gt; &lt;td align="right"&gt;266.25&lt;/td&gt; &lt;td align="right"&gt;36.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M1 Max [1]&lt;/td&gt; &lt;td align="right"&gt;400&lt;/td&gt; &lt;td align="right"&gt;24&lt;/td&gt; &lt;td align="right"&gt;453.03&lt;/td&gt; &lt;td align="right"&gt;22.55&lt;/td&gt; &lt;td align="right"&gt;405.87&lt;/td&gt; &lt;td align="right"&gt;37.81&lt;/td&gt; &lt;td align="right"&gt;400.26&lt;/td&gt; &lt;td align="right"&gt;54.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M1 Max [1]&lt;/td&gt; &lt;td align="right"&gt;400&lt;/td&gt; &lt;td align="right"&gt;32&lt;/td&gt; &lt;td align="right"&gt;599.53&lt;/td&gt; &lt;td align="right"&gt;23.03&lt;/td&gt; &lt;td align="right"&gt;537.37&lt;/td&gt; &lt;td align="right"&gt;40.20&lt;/td&gt; &lt;td align="right"&gt;530.06&lt;/td&gt; &lt;td align="right"&gt;61.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M1 Ultra [1]&lt;/td&gt; &lt;td align="right"&gt;800&lt;/td&gt; &lt;td align="right"&gt;48&lt;/td&gt; &lt;td align="right"&gt;875.81&lt;/td&gt; &lt;td align="right"&gt;33.92&lt;/td&gt; &lt;td align="right"&gt;783.45&lt;/td&gt; &lt;td align="right"&gt;55.69&lt;/td&gt; &lt;td align="right"&gt;772.24&lt;/td&gt; &lt;td align="right"&gt;74.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M1 Ultra [1]&lt;/td&gt; &lt;td align="right"&gt;800&lt;/td&gt; &lt;td align="right"&gt;64&lt;/td&gt; &lt;td align="right"&gt;1168.89&lt;/td&gt; &lt;td align="right"&gt;37.01&lt;/td&gt; &lt;td align="right"&gt;1042.95&lt;/td&gt; &lt;td align="right"&gt;59.87&lt;/td&gt; &lt;td align="right"&gt;1030.04&lt;/td&gt; &lt;td align="right"&gt;83.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M2 [2]&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;147.27&lt;/td&gt; &lt;td align="right"&gt;12.18&lt;/td&gt; &lt;td align="right"&gt;145.91&lt;/td&gt; &lt;td align="right"&gt;21.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M2 [2]&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;10&lt;/td&gt; &lt;td align="right"&gt;201.34&lt;/td&gt; &lt;td align="right"&gt;6.72&lt;/td&gt; &lt;td align="right"&gt;181.40&lt;/td&gt; &lt;td align="right"&gt;12.21&lt;/td&gt; &lt;td align="right"&gt;179.57&lt;/td&gt; &lt;td align="right"&gt;21.91&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M2 Pro [2]&lt;/td&gt; &lt;td align="right"&gt;200&lt;/td&gt; &lt;td align="right"&gt;16&lt;/td&gt; &lt;td align="right"&gt;312.65&lt;/td&gt; &lt;td align="right"&gt;12.47&lt;/td&gt; &lt;td align="right"&gt;288.46&lt;/td&gt; &lt;td align="right"&gt;22.70&lt;/td&gt; &lt;td align="right"&gt;294.24&lt;/td&gt; &lt;td align="right"&gt;37.87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M2 Pro [2]&lt;/td&gt; &lt;td align="right"&gt;200&lt;/td&gt; &lt;td align="right"&gt;19&lt;/td&gt; &lt;td align="right"&gt;384.38&lt;/td&gt; &lt;td align="right"&gt;13.06&lt;/td&gt; &lt;td align="right"&gt;344.50&lt;/td&gt; &lt;td align="right"&gt;23.01&lt;/td&gt; &lt;td align="right"&gt;341.19&lt;/td&gt; &lt;td align="right"&gt;38.86&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M2 Max [2]&lt;/td&gt; &lt;td align="right"&gt;400&lt;/td&gt; &lt;td align="right"&gt;30&lt;/td&gt; &lt;td align="right"&gt;600.46&lt;/td&gt; &lt;td align="right"&gt;24.16&lt;/td&gt; &lt;td align="right"&gt;540.15&lt;/td&gt; &lt;td align="right"&gt;39.97&lt;/td&gt; &lt;td align="right"&gt;537.60&lt;/td&gt; &lt;td align="right"&gt;60.99&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M2 Max [2]&lt;/td&gt; &lt;td align="right"&gt;400&lt;/td&gt; &lt;td align="right"&gt;38&lt;/td&gt; &lt;td align="right"&gt;755.67&lt;/td&gt; &lt;td align="right"&gt;24.65&lt;/td&gt; &lt;td align="right"&gt;677.91&lt;/td&gt; &lt;td align="right"&gt;41.83&lt;/td&gt; &lt;td align="right"&gt;671.31&lt;/td&gt; &lt;td align="right"&gt;65.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M2 Ultra [2]&lt;/td&gt; &lt;td align="right"&gt;800&lt;/td&gt; &lt;td align="right"&gt;60&lt;/td&gt; &lt;td align="right"&gt;1128.59&lt;/td&gt; &lt;td align="right"&gt;39.86&lt;/td&gt; &lt;td align="right"&gt;1003.16&lt;/td&gt; &lt;td align="right"&gt;62.14&lt;/td&gt; &lt;td align="right"&gt;1013.81&lt;/td&gt; &lt;td align="right"&gt;88.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M2 Ultra [2]&lt;/td&gt; &lt;td align="right"&gt;800&lt;/td&gt; &lt;td align="right"&gt;76&lt;/td&gt; &lt;td align="right"&gt;1401.85&lt;/td&gt; &lt;td align="right"&gt;41.02&lt;/td&gt; &lt;td align="right"&gt;1248.59&lt;/td&gt; &lt;td align="right"&gt;66.64&lt;/td&gt; &lt;td align="right"&gt;1238.48&lt;/td&gt; &lt;td align="right"&gt;94.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;🟨 M3 [3]&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;10&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;187.52&lt;/td&gt; &lt;td align="right"&gt;12.27&lt;/td&gt; &lt;td align="right"&gt;186.75&lt;/td&gt; &lt;td align="right"&gt;21.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;🟨 M3 Pro [3]&lt;/td&gt; &lt;td align="right"&gt;150&lt;/td&gt; &lt;td align="right"&gt;14&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;272.11&lt;/td&gt; &lt;td align="right"&gt;17.44&lt;/td&gt; &lt;td align="right"&gt;269.49&lt;/td&gt; &lt;td align="right"&gt;30.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M3 Pro [3]&lt;/td&gt; &lt;td align="right"&gt;150&lt;/td&gt; &lt;td align="right"&gt;18&lt;/td&gt; &lt;td align="right"&gt;357.45&lt;/td&gt; &lt;td align="right"&gt;9.89&lt;/td&gt; &lt;td align="right"&gt;344.66&lt;/td&gt; &lt;td align="right"&gt;17.53&lt;/td&gt; &lt;td align="right"&gt;341.67&lt;/td&gt; &lt;td align="right"&gt;30.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M3 Max [3]&lt;/td&gt; &lt;td align="right"&gt;300&lt;/td&gt; &lt;td align="right"&gt;30&lt;/td&gt; &lt;td align="right"&gt;589.41&lt;/td&gt; &lt;td align="right"&gt;19.54&lt;/td&gt; &lt;td align="right"&gt;566.40&lt;/td&gt; &lt;td align="right"&gt;34.30&lt;/td&gt; &lt;td align="right"&gt;567.59&lt;/td&gt; &lt;td align="right"&gt;56.58&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M3 Max [3]&lt;/td&gt; &lt;td align="right"&gt;400&lt;/td&gt; &lt;td align="right"&gt;40&lt;/td&gt; &lt;td align="right"&gt;779.17&lt;/td&gt; &lt;td align="right"&gt;25.09&lt;/td&gt; &lt;td align="right"&gt;757.64&lt;/td&gt; &lt;td align="right"&gt;42.75&lt;/td&gt; &lt;td align="right"&gt;759.70&lt;/td&gt; &lt;td align="right"&gt;66.31&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M3 Ultra [3]&lt;/td&gt; &lt;td align="right"&gt;800&lt;/td&gt; &lt;td align="right"&gt;60&lt;/td&gt; &lt;td align="right"&gt;1121.80&lt;/td&gt; &lt;td align="right"&gt;42.24&lt;/td&gt; &lt;td align="right"&gt;1085.76&lt;/td&gt; &lt;td align="right"&gt;63.55&lt;/td&gt; &lt;td align="right"&gt;1073.09&lt;/td&gt; &lt;td align="right"&gt;88.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M3 Ultra [3]&lt;/td&gt; &lt;td align="right"&gt;800&lt;/td&gt; &lt;td align="right"&gt;80&lt;/td&gt; &lt;td align="right"&gt;1538.34&lt;/td&gt; &lt;td align="right"&gt;39.78&lt;/td&gt; &lt;td align="right"&gt;1487.51&lt;/td&gt; &lt;td align="right"&gt;63.93&lt;/td&gt; &lt;td align="right"&gt;1471.24&lt;/td&gt; &lt;td align="right"&gt;92.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M4 [4]&lt;/td&gt; &lt;td align="right"&gt;120&lt;/td&gt; &lt;td align="right"&gt;10&lt;/td&gt; &lt;td align="right"&gt;230.18&lt;/td&gt; &lt;td align="right"&gt;7.43&lt;/td&gt; &lt;td align="right"&gt;223.64&lt;/td&gt; &lt;td align="right"&gt;13.54&lt;/td&gt; &lt;td align="right"&gt;221.29&lt;/td&gt; &lt;td align="right"&gt;24.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M4 Pro [4]&lt;/td&gt; &lt;td align="right"&gt;273&lt;/td&gt; &lt;td align="right"&gt;16&lt;/td&gt; &lt;td align="right"&gt;381.14&lt;/td&gt; &lt;td align="right"&gt;17.19&lt;/td&gt; &lt;td align="right"&gt;367.13&lt;/td&gt; &lt;td align="right"&gt;30.54&lt;/td&gt; &lt;td align="right"&gt;364.06&lt;/td&gt; &lt;td align="right"&gt;49.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M4 Pro [4]&lt;/td&gt; &lt;td align="right"&gt;273&lt;/td&gt; &lt;td align="right"&gt;20&lt;/td&gt; &lt;td align="right"&gt;464.48&lt;/td&gt; &lt;td align="right"&gt;17.18&lt;/td&gt; &lt;td align="right"&gt;449.62&lt;/td&gt; &lt;td align="right"&gt;30.69&lt;/td&gt; &lt;td align="right"&gt;439.78&lt;/td&gt; &lt;td align="right"&gt;50.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M4 Max [4]&lt;/td&gt; &lt;td align="right"&gt;546&lt;/td&gt; &lt;td align="right"&gt;40&lt;/td&gt; &lt;td align="right"&gt;922.83&lt;/td&gt; &lt;td align="right"&gt;31.64&lt;/td&gt; &lt;td align="right"&gt;891.94&lt;/td&gt; &lt;td align="right"&gt;54.05&lt;/td&gt; &lt;td align="right"&gt;885.68&lt;/td&gt; &lt;td align="right"&gt;83.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ &lt;strong&gt;M5 (Neural Accel)&lt;/strong&gt; [5]&lt;/td&gt; &lt;td align="right"&gt;153&lt;/td&gt; &lt;td align="right"&gt;10&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;608.05&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;26.59&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ &lt;strong&gt;M5 (no Accel)&lt;/strong&gt; [5]&lt;/td&gt; &lt;td align="right"&gt;153&lt;/td&gt; &lt;td align="right"&gt;10&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;252.82&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;27.55&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;M5 source: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16634"&gt;https://github.com/ggml-org/llama.cpp/pull/16634&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All Apple Silicon results: &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/4167"&gt;https://github.com/ggml-org/llama.cpp/discussions/4167&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/auradragon1"&gt; /u/auradragon1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogwf6b/m5_neural_accelerator_benchmark_results_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogwf6b/m5_neural_accelerator_benchmark_results_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogwf6b/m5_neural_accelerator_benchmark_results_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T21:24:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh5asg</id>
    <title>🚀 New Model from the MiniMax team: MiniMax-M2, an impressive 230B-A10B LLM.</title>
    <updated>2025-10-27T04:28:24+00:00</updated>
    <author>
      <name>/u/chenqian615</name>
      <uri>https://old.reddit.com/user/chenqian615</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh5asg/new_model_from_the_minimax_team_minimaxm2_an/"&gt; &lt;img alt="🚀 New Model from the MiniMax team: MiniMax-M2, an impressive 230B-A10B LLM." src="https://a.thumbs.redditmedia.com/b3_JzXejnThTVzO8xn7-iIWxAt3NTQCnaQo4XEvZSC0.jpg" title="🚀 New Model from the MiniMax team: MiniMax-M2, an impressive 230B-A10B LLM." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Officially positioned as an “end-to-end coding + tool-using agent.” From the public evaluations and model setup, it looks well-suited for teams that need end to end development and toolchain agents, prioritizing lower latency and higher throughput. For real engineering workflows that advance in small but continuous steps, it should offer strong cost-effectiveness. I’ve collected a few points to help with evaluation:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;End-to-end workflow oriented, emphasizing multi-file editing, code, run, fix loops, testing/verification, and long-chain tool orchestration across terminal/browser/retrieval/code execution. These capabilities matter more than just chatting when deploying agents.&lt;/li&gt; &lt;li&gt;Publicly described as “~10B activated parameters (total ~200B).” The design aims to reduce inference latency and per unit cost while preserving coding and tool-calling capabilities, making it suitable for high concurrency and batch sampling.&lt;/li&gt; &lt;li&gt;Benchmark coverage spans end-to-end software engineering (SWE-bench, Terminal-Bench, ArtifactsBench), browsing/retrieval tasks (BrowseComp, FinSearchComp), and holistic intelligence profiling (AA Intelligence).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Position in public benchmarks (not the absolute strongest, but well targeted)&lt;/p&gt; &lt;p&gt;Here are a few developer-relevant metrics I pulled from public tables:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SWE-bench Verified: 69.4&lt;/li&gt; &lt;li&gt;Terminal-Bench: 46.3&lt;/li&gt; &lt;li&gt;ArtifactsBench: 66.8&lt;/li&gt; &lt;li&gt;BrowseComp: 44.0 (BrowseComp-zh in Chinese: 48.5)&lt;/li&gt; &lt;li&gt;τ²-Bench: 77.2&lt;/li&gt; &lt;li&gt;FinSearchComp-global: 65.5&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;From the scores, on tasks that require real toolchain collaboration, this model looks like a balanced choice prioritizing efficiency and stability. Some closed-source models score higher on certain benchmarks, but for end to end development/ agent pipelines, its price performance orientation is appealing. On SWE-bench / Multi-SWE-Bench, steadily completing the modify test modify again loop is often more important than a one-shot perfect fix. These scores and its positioning suggest it can keep pushing the loop toward a runnable solution. A Terminal-Bench score of 46.3 indicates decent robustness in command execution, error recovery, and retries worth trying in a real CI sandbox for small-scale tasks.&lt;/p&gt; &lt;p&gt;References&lt;/p&gt; &lt;p&gt;HF:&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chenqian615"&gt; /u/chenqian615 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oh5asg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh5asg/new_model_from_the_minimax_team_minimaxm2_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh5asg/new_model_from_the_minimax_team_minimaxm2_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T04:28:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh57ys</id>
    <title>MiniMaxAI/MiniMax-M2 · Hugging Face</title>
    <updated>2025-10-27T04:23:41+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh57ys/minimaxaiminimaxm2_hugging_face/"&gt; &lt;img alt="MiniMaxAI/MiniMax-M2 · Hugging Face" src="https://external-preview.redd.it/UWFNDndMvPJsO1Z9iKM9CbvnTGrRp8W6-SXVbMO4N1g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f00892c38fccd0c77d2f3f510b6bc20576cdae9" title="MiniMaxAI/MiniMax-M2 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh57ys/minimaxaiminimaxm2_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh57ys/minimaxaiminimaxm2_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T04:23:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oh9hx6</id>
    <title>The performance of Minimax-m2 is truly impressive!</title>
    <updated>2025-10-27T09:04:06+00:00</updated>
    <author>
      <name>/u/contportvas</name>
      <uri>https://old.reddit.com/user/contportvas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh9hx6/the_performance_of_minimaxm2_is_truly_impressive/"&gt; &lt;img alt="The performance of Minimax-m2 is truly impressive!" src="https://preview.redd.it/x4l7z579dmxf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7ce839a8c6c31eed987024c074f4c3c82c027cd" title="The performance of Minimax-m2 is truly impressive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Came across this on X today, and I have to say, the model's performance looks super impressive! Has anyone tested it out yet? This showcase is from a post by a user on X: &lt;a href="https://x.com/ivanfioravanti/status/1982469771481497856?s=46"&gt;https://x.com/ivanfioravanti/status/1982469771481497856?s=46&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/contportvas"&gt; /u/contportvas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x4l7z579dmxf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oh9hx6/the_performance_of_minimaxm2_is_truly_impressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oh9hx6/the_performance_of_minimaxm2_is_truly_impressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T09:04:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
