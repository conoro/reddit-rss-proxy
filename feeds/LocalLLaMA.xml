<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-19T21:24:28+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pqndio</id>
    <title>Is Gemma 9B still the best dense model of that size in December 2025?</title>
    <updated>2025-12-19T15:06:29+00:00</updated>
    <author>
      <name>/u/ihatebeinganonymous</name>
      <uri>https://old.reddit.com/user/ihatebeinganonymous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. I have been missing news for some time. What are the best models of 4B and 9B sizes, for basic NLP (not fine tuning)? Are Gemma 3 4B and Gemma 2 9B still the best ones?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihatebeinganonymous"&gt; /u/ihatebeinganonymous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqndio/is_gemma_9b_still_the_best_dense_model_of_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqndio/is_gemma_9b_still_the_best_dense_model_of_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqndio/is_gemma_9b_still_the_best_dense_model_of_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T15:06:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqnuu7</id>
    <title>When life gives you a potato PC, turn it into Vodka</title>
    <updated>2025-12-19T15:26:08+00:00</updated>
    <author>
      <name>/u/Impossible-Power6989</name>
      <uri>https://old.reddit.com/user/Impossible-Power6989</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've (mostly) been lurking here and on &lt;a href="https://www.reddit.com/r/LocalLLM/"&gt;r/LocalLLM&lt;/a&gt; for about 3 months now. I got back into computers by way of a disc herniation knocking me on my ass for several months, kids wanting to play games to cheer me up, Wii modding, emulation and retro-gaming.&lt;/p&gt; &lt;p&gt;I've read a lot of stuff. Some great, some baffling, and some that could politely be dubbed &amp;quot;piquant&amp;quot; (and probably well suited for &lt;a href="https://www.reddit.com/r/LinkedInLunatics/"&gt;r/LinkedInLunatics&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;What I haven't seen much of is -&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Acknowledging normie use cases&lt;/li&gt; &lt;li&gt;Acknowledging shit tier hardware&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;As a semi-normie with shit tier hardware, I'd like to share my use case, what I did, and why it might be useful for we, the proletariat looking to get into local hosting local models.&lt;/p&gt; &lt;p&gt;I'm not selling anything or covertly puffing myself up like a cat in order to look bigger (or pad my resume for Linkedin). I just genuinely like helping others like me out. If you're a sysadmin running 8x100H, well, this isn't for you.&lt;/p&gt; &lt;h1&gt;The why&lt;/h1&gt; &lt;p&gt;According to recent steam survey [1], roughly 66% of US users have rigs with 8GB or less VRAM. (Yes, we can argue about that being a non-representative sample. Fine. OTOH, this is a Reddit post and not a peer-reviewed article).&lt;/p&gt; &lt;p&gt;Irrespective of the actual % - and in light of the global GPU and RAM crunch - it's fair to say that a vast preponderance of people are not running on specc'ed-out rigs. And that's &lt;em&gt;without&lt;/em&gt; accounting for the &amp;quot;global south&amp;quot;, edge computing devices, or other constrained scenarios.&lt;/p&gt; &lt;p&gt;Myself? I have a pathological &amp;quot;fuck you&amp;quot; reflex when someone says &amp;quot;no, that can't be done&amp;quot;. I will find a way to outwork reality when that particular red rag appears, irrespective of how Pyrrhic the victory may appear.&lt;/p&gt; &lt;p&gt;Ipso facto, my entire potato power rig costs approx $200USD, including the truly &amp;quot;magnificent&amp;quot; P1000 4GB VRAM Nvidia Quadro I acquired for $50USD. I can eke out 25-30tps on with a 4B model and about 18-20tps with a 8B, which everyone told me was (a) impossible (b) toy sized (c) useless to even attempt.&lt;/p&gt; &lt;p&gt;After multiple tests and retests (see my &lt;a href="https://old.reddit.com/r/LocalLLM/comments/1pcwafx/28m_tokens_later_how_i_unfucked_my_4b_model_with/"&gt;RAG nonsense&lt;/a&gt; as an example of how anal I am), I'm at about 95% coverage for what I need, with the occasional use of bigger, free models via OR (DeepSeek R1T2 (free) - 671B, MiMO-V2-Flash (free) - 309B being recent favourites).&lt;/p&gt; &lt;h1&gt;My reasons for using this rig (instead of upgrading):&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;I got it cheap&lt;/li&gt; &lt;li&gt;It's easy to tinker with, take apart, and learn on&lt;/li&gt; &lt;li&gt;It uses 15-25W of power at idle and about 80-100W under load. (Yes, you damn well know I used Kilowatt and HWInfo to log and verify).&lt;/li&gt; &lt;li&gt;It sits behind my TV&lt;/li&gt; &lt;li&gt;It's quiet&lt;/li&gt; &lt;li&gt;It's tiny (1L)&lt;/li&gt; &lt;li&gt;It does what I need it to do (games, automation, SLM)&lt;/li&gt; &lt;li&gt;Because I can&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;LLM use case&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Non hallucinatory chat to spark personal reflection - aka &amp;quot;Dear Dolly Doctor&amp;quot; for MAMILs&lt;/li&gt; &lt;li&gt;Troubleshooting hardware and software (eg: Dolphin emulator, PCSX2, general gaming stuff, Python code, llama.cpp, terminal commands etc), assisted by scraping and then RAGing via the excellent Crawlee [2] and Qdrant [3]&lt;/li&gt; &lt;li&gt;On that topic: general querying of personal documents to get grounded, accurate answers.&lt;/li&gt; &lt;li&gt;Email drafting and sentiment analysis (I have ASD an tone sometimes escapes me)&lt;/li&gt; &lt;li&gt;Tinkering and fun&lt;/li&gt; &lt;li&gt;Privacy&lt;/li&gt; &lt;li&gt;Pulling info out of screenshots and then distilling / querying (&amp;quot;What does this log say&amp;quot;?)&lt;/li&gt; &lt;li&gt;Home automation (TBC)&lt;/li&gt; &lt;li&gt;Do all this at interactive speeds (&amp;gt;10 tps at bare min).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basically, I wanted a thinking engine that I could trust, was private and could be updated easily. Oh, and it had to run fast-ish, be cheap, quiet, easy to tinker with.&lt;/p&gt; &lt;h1&gt;What I did&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Set up llama.cpp, llama-swap and OWUI to help me spin up different models on the fly as needed, or instances of the same model with different settings (lower temperatures, more deterministic, more terse, or more chatty etc)&lt;/li&gt; &lt;li&gt;Created a series of system prompts to ensure tone is consistent. If Qwen3-4B is good at anything, it's slavishly following the rules. You tell it to do something and it does it. Getting it to stop is somewhat of a challenge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As an example, when I need to sniff out bullshit, I inject the following prompt -&lt;/p&gt; &lt;p&gt;Tone: neutral, precise, low‑context.&lt;/p&gt; &lt;p&gt;Rules:&lt;/p&gt; &lt;p&gt;Answer first. No preamble. ≤3 short paragraphs (plus optional bullets/code if needed). Minimal emotion or politeness; no soft closure. Never generate personal memories, subjective experiences, or fictional biographical details. Emotional or expressive tone is forbidden. End with a declarative sentence.&lt;/p&gt; &lt;p&gt;Source and confidence tagging: At the end of every answer, append a single line: Confidence: [low | medium | high | top] | Source: [Model | Docs | Web | User | Contextual | Mixed]&lt;/p&gt; &lt;p&gt;Where:&lt;/p&gt; &lt;p&gt;Confidence is a rough self‑estimate:&lt;/p&gt; &lt;p&gt;low = weak support, partial information, or heavy guesswork. medium = some support, but important gaps or uncertainty. high = well supported by available information, minor uncertainty only. top = very strong support, directly backed by clear information, minimal uncertainty.&lt;/p&gt; &lt;p&gt;Source is your primary evidence:&lt;/p&gt; &lt;p&gt;Model – mostly from internal pretrained knowledge. Docs – primarily from provided documentation or curated notes (RAG context). Web – primarily from online content fetched for this query. User – primarily restating, transforming, or lightly extending user‑supplied text. Contextual – mostly inferred from combining information already present in this conversation. Mixed – substantial combination of two or more of the above, none clearly dominant.&lt;/p&gt; &lt;p&gt;Always follow these rules.&lt;/p&gt; &lt;p&gt;Set up RAG pipeline (as discussed extensively in the above &amp;quot;how I unfucked my 4B&amp;quot; post), paying special attention to use small embedder and re-reanker (TinyBert) so that RAG is actually fast&lt;/p&gt; &lt;p&gt;I have other prompts for other uses, but that gives the flavour.&lt;/p&gt; &lt;h1&gt;Weird shit I did that works for me YMMV&lt;/h1&gt; &lt;p&gt;Created some python code to run within OWUI that creates rolling memory from a TINY -ctx size. Impossibly tiny. 768.&lt;/p&gt; &lt;p&gt;As we all know, the second largest hog of VRAM is -ctx.&lt;/p&gt; &lt;p&gt;The basic idea here is that by shrinking to a minuscule token context limit, I was able to claw back about 80% of VRAM, reduce matmuls and speed up my GPU significantly. It was pretty ok at 14-16 tps with --ctx 8192 but this is better for my use case and stack when I want both fast and not too dumb.&lt;/p&gt; &lt;p&gt;The trick was using JSON (yes, really, a basic text file) to store and contain the first pair (user and assistant), last pair and a rolling summary of the conversation (generated every N turns, for X size: default being 160 words), with auto-tagging, TTL limit, along with breadcrumbs so that the LLM can rehydrate the context on the fly.&lt;/p&gt; &lt;p&gt;As this post is for normies, I'm going to side step a lot of the finer details for now. My eventual goal is to untie the code from OWUI so that it works as middleware with any front-end, and also make it monolithic (to piss off real programmers but also for sake of easy deployment).&lt;/p&gt; &lt;p&gt;My hope is to make it agnostic, such that a Raspberry Pi can run a 4B parameter model at reasonable speeds (+10TPS). In practice, for me, it has allowed me to run a 4B model at 2x speed, and have a 8B Q3_K_M fit entirely in VRAM (thus, 2x it as well).&lt;/p&gt; &lt;p&gt;I &lt;em&gt;think&lt;/em&gt; it basically should allow the next tier up model for any given sized card a chance to run (eg: a 4GB card should be able to fit a 8B model, a 8GB card should be able to fit a 12B model) without having getting the equivalent of digital Alzheimer's. Note: there are some issues to iron out, use case limitations etc but for a single user, on potato hardware, who's main use case is chat, RAG etc (instead of 20 step IF-THEN) then something like this could help. (I'm happy to elaborate if there is interest).&lt;/p&gt; &lt;p&gt;For sake of disclosure, the prototype code is &lt;a href="https://openwebui.com/f/bobbyllm/cut_the_crap"&gt;HERE&lt;/a&gt; and &lt;a href="https://openwebui.com/t/bobbyllm/total_recall"&gt;HERE&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;The goal of this post wasn't to show off (I'm running a P1000, ffs. That's like being the world's tallest dwarf). It was to demonstrate that you don't need a nuclear power plant in your basement to have a private, usable AI brain. I get a surprising amount of work done with it.&lt;/p&gt; &lt;p&gt;By combining cheap hardware, optimized inference (llama.cpp + llama-swap), and aggressive context management, I’ve built a stack that feels snappy and solves my actual problems. Is it going to write a novel? I mean...maybe? Probably not. No. Is it going to help me fix a Python script, debug an emulator, extract data from images, improve my thinking, get info from my documents, source live data easily, draft an email - all without leaking data? Absolutely. Plus, I can press a button (or ideally, utter a voice command) and turn it back into a retro-gaming box that can play games on any tv in the house (Moonlight).&lt;/p&gt; &lt;p&gt;If you are running on 4GB or 8GB of VRAM: don't let the &amp;quot;24GB minimum&amp;quot; crowd discourage you. Tinker, optimize, and break things. That's where the fun is.&lt;/p&gt; &lt;p&gt;Herein endeth the sermon. I'll post again when I get &amp;quot;Vodka&amp;quot; (the working name the python code stack I mentioned above) out the door in a few weeks.&lt;/p&gt; &lt;p&gt;I'm happy to answer questions as best I can but I'm just a dude howling into the wind, so...&lt;/p&gt; &lt;p&gt;[1] &lt;a href="https://store.steampowered.com/hwsurvey/us/"&gt;https://store.steampowered.com/hwsurvey/us/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;[2] &lt;a href="https://github.com/apify/crawlee-python"&gt;https://github.com/apify/crawlee-python&lt;/a&gt;&lt;/p&gt; &lt;p&gt;[3] &lt;a href="https://github.com/qdrant/qdrant"&gt;https://github.com/qdrant/qdrant&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Additional comment:&lt;/p&gt; &lt;p&gt;LORA and PEFT are legitimately black magic. I don't think it's insane to say that with some smart fine tuning, it's possible to make a 7B *feel* like a 70B in one narrow, specific domain. Yes, there are costs etc but if you're at the point of using something like llama-swap, having a few &amp;quot;rain-man&amp;quot; models is probably worth it.&lt;/p&gt; &lt;p&gt;Hell, you could make yourself an army of beautiful idiots and then write a tiny re-ranking script (or use tinyroberta) to invoke the right one for ultimate ghetto / duct-tape vibes. At that point, you've basically Macguyvered a clockwork 70b (sorta-kinda; you know what I mean) while still only need 4GB to run. Ha ha - fuck you physics!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impossible-Power6989"&gt; /u/Impossible-Power6989 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqnuu7/when_life_gives_you_a_potato_pc_turn_it_into_vodka/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqnuu7/when_life_gives_you_a_potato_pc_turn_it_into_vodka/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqnuu7/when_life_gives_you_a_potato_pc_turn_it_into_vodka/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T15:26:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqf27c</id>
    <title>Is gpt oss:120b still the best at its size?</title>
    <updated>2025-12-19T07:27:00+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am interested in math and coding.. is there still no model that is clearly stronger at 120b or less?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqf27c/is_gpt_oss120b_still_the_best_at_its_size/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqf27c/is_gpt_oss120b_still_the_best_at_its_size/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqf27c/is_gpt_oss120b_still_the_best_at_its_size/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T07:27:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqjqqy</id>
    <title>Known Pretraining Tokens for LLMs</title>
    <updated>2025-12-19T12:21:06+00:00</updated>
    <author>
      <name>/u/phree_radical</name>
      <uri>https://old.reddit.com/user/phree_radical</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjqqy/known_pretraining_tokens_for_llms/"&gt; &lt;img alt="Known Pretraining Tokens for LLMs" src="https://preview.redd.it/970lzt7sk58g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b260274c8027bbb342abe4985e434e89f2823c51" title="Known Pretraining Tokens for LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretraining compute seems like it doesn't get enough attention, compared to Parameters.&lt;/p&gt; &lt;p&gt;I was working on this spreadsheet a few months ago. If a vendor didn't publish anything about how many pretraining tokens, I left them out. But I'm certain I've missed some important models.&lt;/p&gt; &lt;p&gt;What can we add to this spreadsheet?&lt;/p&gt; &lt;p&gt;&lt;a href="https://docs.google.com/spreadsheets/d/1vKOK0UPUcUBIEf7srkbGfwQVJTx854_a3rCmglU9QuY/"&gt;https://docs.google.com/spreadsheets/d/1vKOK0UPUcUBIEf7srkbGfwQVJTx854_a3rCmglU9QuY/&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Family / Vendor&lt;/th&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Parameters (B)&lt;/th&gt; &lt;th&gt;Pretraining Tokens (T)&lt;/th&gt; &lt;th&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;LLaMA&lt;/td&gt; &lt;td&gt;LLaMA 7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LLaMA&lt;/td&gt; &lt;td&gt;LLaMA 33B&lt;/td&gt; &lt;td&gt;33&lt;/td&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LLaMA&lt;/td&gt; &lt;td&gt;LLaMA 70B&lt;/td&gt; &lt;td&gt;70&lt;/td&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LLaMA&lt;/td&gt; &lt;td&gt;LLaMA 2 7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LlaMA&lt;/td&gt; &lt;td&gt;LLaMA 2 13B&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LlaMA&lt;/td&gt; &lt;td&gt;LLaMA 2 70B&lt;/td&gt; &lt;td&gt;70&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LLaMA&lt;/td&gt; &lt;td&gt;LLaMA 3 8B&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;15&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;LLaMA&lt;/td&gt; &lt;td&gt;LLaMA 3 70B&lt;/td&gt; &lt;td&gt;70&lt;/td&gt; &lt;td&gt;15&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen-1.8B&lt;/td&gt; &lt;td&gt;1.8&lt;/td&gt; &lt;td&gt;2.2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen-7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;2.4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen-14B&lt;/td&gt; &lt;td&gt;14&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen-72B&lt;/td&gt; &lt;td&gt;72&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2-0.5b&lt;/td&gt; &lt;td&gt;0.5&lt;/td&gt; &lt;td&gt;12&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2-1.5b&lt;/td&gt; &lt;td&gt;1.5&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2-7b&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2-72b&lt;/td&gt; &lt;td&gt;72&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2-57B-A14B&lt;/td&gt; &lt;td&gt;72&lt;/td&gt; &lt;td&gt;11.5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 0.5B&lt;/td&gt; &lt;td&gt;0.5&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 1.5B&lt;/td&gt; &lt;td&gt;1.5&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 3B&lt;/td&gt; &lt;td&gt;3&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 14B&lt;/td&gt; &lt;td&gt;14&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 32B&lt;/td&gt; &lt;td&gt;32&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen&lt;/td&gt; &lt;td&gt;Qwen2.5 72B&lt;/td&gt; &lt;td&gt;72&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3 0.6B&lt;/td&gt; &lt;td&gt;0.6&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3 1.7B&lt;/td&gt; &lt;td&gt;1.7&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3 4B&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3 8B&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3 14B&lt;/td&gt; &lt;td&gt;14&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3 32B&lt;/td&gt; &lt;td&gt;32&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3-30B-A3B&lt;/td&gt; &lt;td&gt;30&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3&lt;/td&gt; &lt;td&gt;Qwen3-235B-A22B&lt;/td&gt; &lt;td&gt;235&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM&lt;/td&gt; &lt;td&gt;GLM-130B&lt;/td&gt; &lt;td&gt;130&lt;/td&gt; &lt;td&gt;23&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Chinchilla&lt;/td&gt; &lt;td&gt;Chinchilla-70B&lt;/td&gt; &lt;td&gt;70&lt;/td&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;OpenAI&lt;/td&gt; &lt;td&gt;GPT-3 (175B)&lt;/td&gt; &lt;td&gt;175&lt;/td&gt; &lt;td&gt;0.5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;OpenAI&lt;/td&gt; &lt;td&gt;GPT-4 (1.8T)&lt;/td&gt; &lt;td&gt;1800&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;PaLM (540B)&lt;/td&gt; &lt;td&gt;540&lt;/td&gt; &lt;td&gt;0.78&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TII&lt;/td&gt; &lt;td&gt;Falcon-180B&lt;/td&gt; &lt;td&gt;180&lt;/td&gt; &lt;td&gt;3.5&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 1 2B&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 1 7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 2 2B&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 2 9B&lt;/td&gt; &lt;td&gt;9&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 2 27B&lt;/td&gt; &lt;td&gt;27&lt;/td&gt; &lt;td&gt;13&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 3 1B&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 3 4B&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 3 12B&lt;/td&gt; &lt;td&gt;12&lt;/td&gt; &lt;td&gt;12&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Google&lt;/td&gt; &lt;td&gt;Gemma 3 27B&lt;/td&gt; &lt;td&gt;27&lt;/td&gt; &lt;td&gt;14&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-Coder 1.3B&lt;/td&gt; &lt;td&gt;1.3&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-Coder 33B&lt;/td&gt; &lt;td&gt;33&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-LLM 7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-LLM 67B&lt;/td&gt; &lt;td&gt;67&lt;/td&gt; &lt;td&gt;2&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-V2&lt;/td&gt; &lt;td&gt;236&lt;/td&gt; &lt;td&gt;8.1&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-V3&lt;/td&gt; &lt;td&gt;671&lt;/td&gt; &lt;td&gt;14.8&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek&lt;/td&gt; &lt;td&gt;DeepSeek-V3.1&lt;/td&gt; &lt;td&gt;685&lt;/td&gt; &lt;td&gt;15.6&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-1&lt;/td&gt; &lt;td&gt;1.3&lt;/td&gt; &lt;td&gt;0.054&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-1.5&lt;/td&gt; &lt;td&gt;1.3&lt;/td&gt; &lt;td&gt;0.15&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-2&lt;/td&gt; &lt;td&gt;2.7&lt;/td&gt; &lt;td&gt;1.4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-3-medium&lt;/td&gt; &lt;td&gt;14&lt;/td&gt; &lt;td&gt;4.8&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-3-small&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;4.8&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-3-mini&lt;/td&gt; &lt;td&gt;3.8&lt;/td&gt; &lt;td&gt;3.3&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-3.5-MoE-instruct&lt;/td&gt; &lt;td&gt;42&lt;/td&gt; &lt;td&gt;4.9&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-3.5-mini-instruct&lt;/td&gt; &lt;td&gt;3.82&lt;/td&gt; &lt;td&gt;3.4&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Microsoft&lt;/td&gt; &lt;td&gt;Phi-3.5-MoE-instruct&lt;/td&gt; &lt;td&gt;42&lt;/td&gt; &lt;td&gt;4.9&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Xiaomi&lt;/td&gt; &lt;td&gt;MiMo-7B&lt;/td&gt; &lt;td&gt;7&lt;/td&gt; &lt;td&gt;25&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;NVIDIA&lt;/td&gt; &lt;td&gt;Nemotron-3-8B-Base-4k&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;3.8&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;NVIDIA&lt;/td&gt; &lt;td&gt;Nemotron-4-340B&lt;/td&gt; &lt;td&gt;340&lt;/td&gt; &lt;td&gt;9&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;NVIDIA&lt;/td&gt; &lt;td&gt;Nemotron-4-15B&lt;/td&gt; &lt;td&gt;15&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;ByteDance&lt;/td&gt; &lt;td&gt;Seed-oss&lt;/td&gt; &lt;td&gt;36&lt;/td&gt; &lt;td&gt;12&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phree_radical"&gt; /u/phree_radical &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/970lzt7sk58g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjqqy/known_pretraining_tokens_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjqqy/known_pretraining_tokens_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T12:21:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqn344</id>
    <title>Built a one-scene AI text adventure running on llama-3.1-8B. It's live.</title>
    <updated>2025-12-19T14:54:54+00:00</updated>
    <author>
      <name>/u/mikiobraun</name>
      <uri>https://old.reddit.com/user/mikiobraun</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I was playing around with prompts to create more engaging, live like agent personas, and somehow accidentally created this: A one-scene mini-game, running off of llama-3.1-8b. Convince a bouncer to let you into an underground Berlin club. 7 turns. Vibe-based scoring. No scripted answers. Curious what weird approaches people find!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mikiobraun"&gt; /u/mikiobraun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://sventhebouncer.com"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqn344/built_a_onescene_ai_text_adventure_running_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqn344/built_a_onescene_ai_text_adventure_running_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T14:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqeauj</id>
    <title>Meta is developing a new image and video AI model “Mango”, along with a previously reported “Avocado” according to WSJ.</title>
    <updated>2025-12-19T06:40:33+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqeauj/meta_is_developing_a_new_image_and_video_ai_model/"&gt; &lt;img alt="Meta is developing a new image and video AI model “Mango”, along with a previously reported “Avocado” according to WSJ." src="https://preview.redd.it/yf9939hiw38g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=25b7cf3ee9d09bdbbcda23002e194ff41e6d07c0" title="Meta is developing a new image and video AI model “Mango”, along with a previously reported “Avocado” according to WSJ." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://www.wsj.com/tech/ai/meta-developing-new-ai-image-and-video-model-code-named-mango-16e785c7"&gt;https://www.wsj.com/tech/ai/meta-developing-new-ai-image-and-video-model-code-named-mango-16e785c7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yf9939hiw38g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqeauj/meta_is_developing_a_new_image_and_video_ai_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqeauj/meta_is_developing_a_new_image_and_video_ai_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T06:40:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppun3v</id>
    <title>Google's Gemma models family</title>
    <updated>2025-12-18T16:09:10+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"&gt; &lt;img alt="Google's Gemma models family" src="https://preview.redd.it/59w0vja4lz7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7dd2d66ee23d4078bf31aba81cdeecc769669af4" title="Google's Gemma models family" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/59w0vja4lz7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T16:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqupew</id>
    <title>Offline-capable scaffolding with memory and continuity between sessions - MIRA</title>
    <updated>2025-12-19T19:57:11+00:00</updated>
    <author>
      <name>/u/awittygamertag</name>
      <uri>https://old.reddit.com/user/awittygamertag</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;**MIRA: Self-managing memory and context for local LLMs&lt;/p&gt; &lt;p&gt;Hi, my name is Taylor. I've spent the last 10 months building MIRA, an open-source system for persistent memory and autonomous context management. This is my TempleOS.&lt;/p&gt; &lt;p&gt;**The problem**: I wanted memory that manages itself. No manual pruning, no context rot, no tagging. Memories decay if unused and persist if referenced. The system figures that out, not me. I also wanted the model to control its own context window rather than relying on external orchestration to decide what's relevant.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**Deployment**&lt;/p&gt; &lt;p&gt;Single cURL. That's it.&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;p&gt;curl -fsSL &lt;a href="https://raw.githubusercontent.com/taylorsatula/mira-OSS/refs/heads/main/deploy.sh"&gt;https://raw.githubusercontent.com/taylorsatula/mira-OSS/refs/heads/main/deploy.sh&lt;/a&gt; -o deploy.sh &amp;amp;&amp;amp; chmod +x deploy.sh &amp;amp;&amp;amp; ./deploy.sh&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;The script is 2000+ lines of production-grade deployment automation. It handles:&lt;/p&gt; &lt;p&gt;- Platform detection (Linux/macOS) with OS-specific service management&lt;/p&gt; &lt;p&gt;- Pre-flight validation: 10GB disk space, port availability (1993, 8200, 6379, 5432), existing installation detection&lt;/p&gt; &lt;p&gt;- Dependency installation with idempotency (skips what's already installed)&lt;/p&gt; &lt;p&gt;- Python venv creation and package installation&lt;/p&gt; &lt;p&gt;- Model downloads (~1.4GB: spaCy, sentence-transformers embedding model, optional Playwright)&lt;/p&gt; &lt;p&gt;- HashiCorp Vault initialization: AppRole creation, policy setup, automatic unseal, credential storage&lt;/p&gt; &lt;p&gt;- PostgreSQL database and user creation&lt;/p&gt; &lt;p&gt;- Valkey (Redis-compatible) setup&lt;/p&gt; &lt;p&gt;- API key configuration (interactive prompts or skip for later)&lt;/p&gt; &lt;p&gt;- Offline mode with Ollama fallback if you don't want to use cloud APIs&lt;/p&gt; &lt;p&gt;- systemd service creation with auto-start on boot (Linux)&lt;/p&gt; &lt;p&gt;- Cleanup and script archival when complete&lt;/p&gt; &lt;p&gt;Run with `--loud` for verbose output if you want to see everything.&lt;/p&gt; &lt;p&gt;The script is fully unattended-capable. Answer the prompts or accept defaults and walk away. When you come back, MIRA is running either as a systemd service or on-demand.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**Local-first architecture**&lt;/p&gt; &lt;p&gt;- Embeddings run locally via sentence-transformers (mdbr-leaf-ir-asym, 768d). No API calls for search.&lt;/p&gt; &lt;p&gt;- CPU-only PyTorch. No GPU required.&lt;/p&gt; &lt;p&gt;- 3GB total resource usage including embedding model and all plumbing (excluding LLM).&lt;/p&gt; &lt;p&gt;- PostgreSQL + Valkey + HashiCorp Vault for persistence and secrets.&lt;/p&gt; &lt;p&gt;**Provider parity**: Any OpenAI-compatible endpoint works. Plug in ollama, vllm, llama.cpp. Internally MIRA follows Anthropic SDK conventions but translation happens at the proper layer. You're not locked in.&lt;/p&gt; &lt;p&gt;**Models tested**: Deepseek V3.2, Qwen 3, Ministral 3. Acceptable results down to 4b parameters. Claude Opus 4.5 gets the best results by a margin, but the architecture doesn't require it.&lt;/p&gt; &lt;p&gt;**What you lose with local models**: Extended thinking disabled, cache_control stripped, server-side code execution filtered out, file uploads become text warnings. I have tried to provide parity where ever possible and have graceful degradation for Anthropic-specific features like the code execution sandbox.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**Memory decay formula**&lt;/p&gt; &lt;p&gt;This is the part I'm proud of.&lt;/p&gt; &lt;p&gt;Decay runs on **activity days**, not calendar days. If you take a two-week vacation, your memories don't rot. Heavy users and light users experience equivalent freshness relative to their own engagement patterns.&lt;/p&gt; &lt;p&gt;Memories earn their keep:&lt;/p&gt; &lt;p&gt;- Access a memory and it strengthens&lt;/p&gt; &lt;p&gt;- Link memories together and hub score rewards well-connected nodes (diminishing returns after 10 inbound links)&lt;/p&gt; &lt;p&gt;- 15 activity-day grace period for new memories before decay kicks in&lt;/p&gt; &lt;p&gt;- ~67 activity-day half-life on recency boost&lt;/p&gt; &lt;p&gt;- Temporal multiplier boosts memories with upcoming relevance (events, deadlines)&lt;/p&gt; &lt;p&gt;Formula is a sigmoid over weighted composite of value score, hub score, recency boost, newness boost, temporal multiplier, and expiration trailoff. Full SQL in the repo.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**Graph-based memory architecture**&lt;/p&gt; &lt;p&gt;Memories are nodes, relationships are edges.&lt;/p&gt; &lt;p&gt;Design principles:&lt;/p&gt; &lt;p&gt;- Non-destructive by default: supersession and splitting don't delete, consolidation archives&lt;/p&gt; &lt;p&gt;- Sparse links over dense links: better to miss weak signals than add noise&lt;/p&gt; &lt;p&gt;- Heal-on-read: dead links cleaned during traversal, not proactively&lt;/p&gt; &lt;p&gt;**Link types** (LLM-classified, sparse): conflicts, supersedes, causes, instance_of, invalidated_by, motivated_by&lt;/p&gt; &lt;p&gt;**Automatic structural links** (cheap): was_context_for, shares_entity:{Name} via spaCy NER (runs locally)&lt;/p&gt; &lt;p&gt;Bidirectional storage: every link stored in both directions for efficient traversal without joins.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**Memory lifecycle (runs unattended)**&lt;/p&gt; &lt;p&gt;| Job | Interval | Purpose |&lt;/p&gt; &lt;p&gt;|-----|----------|---------|&lt;/p&gt; &lt;p&gt;| Extraction batch polling | 1 min | Check batch status |&lt;/p&gt; &lt;p&gt;| Relationship classification | 1 min | Process new links |&lt;/p&gt; &lt;p&gt;| Failed extraction retry | 6 hours | Retry failures |&lt;/p&gt; &lt;p&gt;| Refinement (split/trim verbose memories) | 7 days | Break up bloated memories |&lt;/p&gt; &lt;p&gt;| Consolidation (merge similar memories) | 7 days | Deduplicate |&lt;/p&gt; &lt;p&gt;| Temporal score recalculation | Daily | Update time-based scores |&lt;/p&gt; &lt;p&gt;| Entity garbage collection | Monthly | Clean orphaned entities |&lt;/p&gt; &lt;p&gt;**Consolidation** uses two-phase LLM verification: reasoning model proposes, fast model reviews. New memory gets median importance score to prevent inflation. Old memories archived, not deleted.&lt;/p&gt; &lt;p&gt;**Splitting** breaks verbose memories into focused ones. Original stays active, split memories coexist.&lt;/p&gt; &lt;p&gt;**Supersession** creates temporal versioning. New info explicitly updates old, but superseded memories remain active so you can see what changed when.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**Domaindocs (persistent knowledge blocks)**&lt;/p&gt; &lt;p&gt;Memories decay. Some knowledge shouldn't. Domaindocs are hierarchical, version-controlled text blocks that persist indefinitely.&lt;/p&gt; &lt;p&gt;Token management via collapse/expand:&lt;/p&gt; &lt;p&gt;- MIRA controls its own context by collapsing sections it doesn't need&lt;/p&gt; &lt;p&gt;- Collapsed sections render as header + metadata only&lt;/p&gt; &lt;p&gt;- Large sections (&amp;gt;5000 chars) flagged so MIRA knows the cost before expanding&lt;/p&gt; &lt;p&gt;**personal_context self-model**: Auto-created for every user. MIRA documents its own behavioral patterns (agreement bias, helpfulness pressure, confidence theater). Observation-driven, not configuration-driven. MIRA writes documentation about how it actually behaves, then consults that documentation in future conversations.&lt;/p&gt; &lt;p&gt;Collaborative editing with conflict resolution when both user and MIRA edit simultaneously.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**Tool context management**&lt;/p&gt; &lt;p&gt;Only three essential tools stay permanently loaded: web_tool, invokeother_tool, getcontext_tool.&lt;/p&gt; &lt;p&gt;All other tools exist as one-line hints in working memory. When MIRA needs capability, it calls invokeother_tool to load the full definition on demand. Loaded tools auto-unload after 5 turns unused (configurable).&lt;/p&gt; &lt;p&gt;With ~15 available tools at 150-400 tokens each, that's 2,250-6,000 tokens not wasted per turn. Smaller context = faster inference on constrained hardware.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**Extensibility**&lt;/p&gt; &lt;p&gt;Tools are entirely self-contained: config, schema, and implementation in one file. Extend MIRA by:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Give Claude Code context about what you want&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Drop the new tool in tools/implementations/&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Restart the process&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Tool auto-registers on startup. There's a HOW_TO_BUILD_A_TOOL.md written specifically to give Claude the context needed to zero-shot a working tool.&lt;/p&gt; &lt;p&gt;Trinkets (working memory plugins) work the same way.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**Segment collapse (&amp;quot;REM sleep&amp;quot;)**&lt;/p&gt; &lt;p&gt;Every 5 minutes APScheduler checks for inactive conversation segments. On timeout:&lt;/p&gt; &lt;p&gt;- Generate summary + embedding&lt;/p&gt; &lt;p&gt;- Extract tools used&lt;/p&gt; &lt;p&gt;- Submit memory extraction to batch processing&lt;/p&gt; &lt;p&gt;- Clear search results to prevent context leak between segments&lt;/p&gt; &lt;p&gt;No intervention needed.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**One conversation forever**&lt;/p&gt; &lt;p&gt;There's no &amp;quot;new chat&amp;quot; button. One conversation, continuous. This constraint forced me to actually solve context management instead of letting users reset when things got messy. A new MIRA instance is a blank slate you grow over time.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**Token overhead**&lt;/p&gt; &lt;p&gt;- ~1,123 token system prompt&lt;/p&gt; &lt;p&gt;- ~8,300 tokens typical full context, ~3,300 cached on subsequent requests&lt;/p&gt; &lt;p&gt;- Content controlled via config limits (20 memories max, 5 rolling summaries max)&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/taylorsatula/mira-OSS"&gt;https://github.com/taylorsatula/mira-OSS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you don't want to self-host, there's a web interface at &lt;a href="https://miraos.org"&gt;https://miraos.org&lt;/a&gt; (runs Claude, not local).&lt;/p&gt; &lt;p&gt;Feedback welcome. That is the quickest way to improving software.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/awittygamertag"&gt; /u/awittygamertag &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqupew/offlinecapable_scaffolding_with_memory_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqupew/offlinecapable_scaffolding_with_memory_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqupew/offlinecapable_scaffolding_with_memory_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T19:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqus6y</id>
    <title>[Release] We released "Text Seal" (part of Meta Seal) – Open source tools to detect benchmark contamination &amp; watermark LLM outputs</title>
    <updated>2025-12-19T20:00:19+00:00</updated>
    <author>
      <name>/u/hadyelsahar</name>
      <uri>https://old.reddit.com/user/hadyelsahar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m one of the authors behind &lt;strong&gt;Meta Seal&lt;/strong&gt;, which we open-sourced today. While the suite covers images and audio, I wanted to share the &lt;strong&gt;TextSeal&lt;/strong&gt; component here because it specifically addresses LLM provenance and the &amp;quot;dataset contamination&amp;quot; problem.&lt;/p&gt; &lt;p&gt;We just released the paper and the code.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2512.16904"&gt;How Good is Post-Hoc Watermarking With Language Model Rephrasing? (arXiv:2512.16904)&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/facebookresearch/textseal"&gt;https://github.com/facebookresearch/textseal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Meta Seal:&lt;/strong&gt; &lt;a href="https://facebookresearch.github.io/meta-seal/"&gt;&lt;strong&gt;https://facebookresearch.github.io/meta-seal/&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is TextSeal?&lt;/strong&gt; Unlike standard generation-time watermarking (which requires you to control the sampling loop during inference), TextSeal focuses on &lt;strong&gt;post-hoc watermarking&lt;/strong&gt;. We use an LLM to &lt;em&gt;rewrite&lt;/em&gt; existing text to inject a watermark while preserving semantics.&lt;/p&gt; &lt;p&gt;The paper benchmarks various setups to answer this. We found some surprising results regarding which sampling methods (like Gumbel-max) actually perform best, and how throwing more compute at the rephrasing step changes the trade-off between detectability and text quality. We also discuss where the method currently struggles, such as with &amp;quot;verifiable&amp;quot; text like code.&lt;/p&gt; &lt;p&gt;We released the full toolkit so you can test this against your own local models or datasets. We're curious if the community can find edge cases where the &amp;quot;radioactivity&amp;quot; signal fails to transfer during fine-tuning.&lt;/p&gt; &lt;p&gt;Let me know if you have questions about the implementation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hadyelsahar"&gt; /u/hadyelsahar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqus6y/release_we_released_text_seal_part_of_meta_seal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqus6y/release_we_released_text_seal_part_of_meta_seal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqus6y/release_we_released_text_seal_part_of_meta_seal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T20:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqjhz9</id>
    <title>What metrics actually matter most when evaluating AI agents?</title>
    <updated>2025-12-19T12:07:38+00:00</updated>
    <author>
      <name>/u/screechymeechydoodle</name>
      <uri>https://old.reddit.com/user/screechymeechydoodle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m trying to set up a lightweight way to evaluate some local agents I’ve been working with (mostly tool-using Llama variants), and I’m not 100% sure which metrics I need to be paying the most attention to.&lt;/p&gt; &lt;p&gt;I’m new to this and its hard to wrap my head around it all. Like success rate, hallucination rate, tool-calling accuracy, multi-step reasoning reliability, etc.&lt;/p&gt; &lt;p&gt;What are yall tracking when it comes to testing local agents. If you had to focus on just a handful of metrics, which ones give you the best signal?&lt;/p&gt; &lt;p&gt;Also, if anyone has a setup that doesn’t require spinning up a whole cloud pipeline, I’d love to hear it. Right now I’m measuring everything manually and its a pain in the ass.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/screechymeechydoodle"&gt; /u/screechymeechydoodle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjhz9/what_metrics_actually_matter_most_when_evaluating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjhz9/what_metrics_actually_matter_most_when_evaluating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjhz9/what_metrics_actually_matter_most_when_evaluating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T12:07:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq5k6e</id>
    <title>Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios</title>
    <updated>2025-12-18T23:23:44+00:00</updated>
    <author>
      <name>/u/Competitive_Travel16</name>
      <uri>https://old.reddit.com/user/Competitive_Travel16</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/"&gt; &lt;img alt="Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios" src="https://external-preview.redd.it/A_KZLQUNhCh0wGe2hwjJCJ470X6QmuVpXZdzOWccb0U.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fdf14dca65c42b501a6a7e33b1acf44e71ac72f" title="Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive_Travel16"&gt; /u/Competitive_Travel16 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=4l4UWZGxvoc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T23:23:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqw0gf</id>
    <title>Deepseek V3.2 vs HF SmolLM3-3B: who's the better Santa?</title>
    <updated>2025-12-19T20:51:14+00:00</updated>
    <author>
      <name>/u/Awkward-Bus-2057</name>
      <uri>https://old.reddit.com/user/Awkward-Bus-2057</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqw0gf/deepseek_v32_vs_hf_smollm33b_whos_the_better_santa/"&gt; &lt;img alt="Deepseek V3.2 vs HF SmolLM3-3B: who's the better Santa?" src="https://external-preview.redd.it/gEK9KszR7_BeYaWY8IDnqp-BfNJftKZvW4rzAVmFiZI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd8961d3378b9f1c453ce35ddc65cd7bdd8c4a79" title="Deepseek V3.2 vs HF SmolLM3-3B: who's the better Santa?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;SantaBench stress-tests the full agentic stack: web search, identity verification, multi-turn conversation, and reliable tool execution. We ran GPT-5.2, Grok 4, DeepSeek V3.2, and SmolLM3-3B as part of our benchmark.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Awkward-Bus-2057"&gt; /u/Awkward-Bus-2057 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://veris.ai/blog/santabench"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqw0gf/deepseek_v32_vs_hf_smollm33b_whos_the_better_santa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqw0gf/deepseek_v32_vs_hf_smollm33b_whos_the_better_santa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T20:51:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq2ry0</id>
    <title>Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster</title>
    <updated>2025-12-18T21:28:20+00:00</updated>
    <author>
      <name>/u/geerlingguy</name>
      <uri>https://old.reddit.com/user/geerlingguy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/"&gt; &lt;img alt="Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster" src="https://preview.redd.it/32z50w1s518g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be2781529b5cacb7d7a84c794d37a156e1bdc798" title="Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was testing llama.cpp RPC vs Exo's new RDMA Tensor setting on a cluster of 4x Mac Studios (2x 512GB and 2x 256GB) that Apple loaned me until Februrary.&lt;/p&gt; &lt;p&gt;Would love to do more testing between now and returning it. A lot of the earlier testing was debugging stuff since the RDMA support was very new for the past few weeks... now that it's somewhat stable I can do more.&lt;/p&gt; &lt;p&gt;The annoying thing is there's nothing nice like llama-bench in Exo, so I can't give as direct comparisons with context sizes, prompt processing speeds, etc. (it takes a lot more fuss to do that, at least).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geerlingguy"&gt; /u/geerlingguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/32z50w1s518g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T21:28:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqjja2</id>
    <title>Gemma Scope 2 is a comprehensive, open suite of sparse autoencoders and transcoders for a range of model sizes and versions in the Gemma 3 model family.</title>
    <updated>2025-12-19T12:09:34+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjja2/gemma_scope_2_is_a_comprehensive_open_suite_of/"&gt; &lt;img alt="Gemma Scope 2 is a comprehensive, open suite of sparse autoencoders and transcoders for a range of model sizes and versions in the Gemma 3 model family." src="https://b.thumbs.redditmedia.com/wfpfrO9KgU8CdhDynFFqinaA1_4j8ZSos50KzQEDVTA.jpg" title="Gemma Scope 2 is a comprehensive, open suite of sparse autoencoders and transcoders for a range of model sizes and versions in the Gemma 3 model family." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma Scope 2: &lt;a href="https://huggingface.co/google/gemma-scope-2"&gt;https://huggingface.co/google/gemma-scope-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Collection: &lt;a href="https://huggingface.co/collections/google/gemma-scope-2"&gt;https://huggingface.co/collections/google/gemma-scope-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: Google AI Developers on 𝕏: &lt;a href="https://x.com/googleaidevs/status/2001986944687804774"&gt;https://x.com/googleaidevs/status/2001986944687804774&lt;/a&gt;&lt;br /&gt; Blog post: Gemma Scope 2: helping the AI safety community deepen understanding of complex language model behavior: &lt;a href="https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/"&gt;https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pqjja2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjja2/gemma_scope_2_is_a_comprehensive_open_suite_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqjja2/gemma_scope_2_is_a_comprehensive_open_suite_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T12:09:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqm5g4</id>
    <title>Seed OSS 36b made me reconsider my life choices.</title>
    <updated>2025-12-19T14:15:06+00:00</updated>
    <author>
      <name>/u/ChopSticksPlease</name>
      <uri>https://old.reddit.com/user/ChopSticksPlease</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;5AM, - Me: Hello Seed, write me a complete new library does this and that, use that internal library as a reference but extend it to handle more data formats. Unify the data abstraction layer so data from one format can be exported to other format. Analyse the code in the internal lib directory and create a similar library but extended with more data formats to support. Create unit tests. To run the unit tests use the following command ...&lt;br /&gt; - Seed: Hold my 啤酒&lt;/p&gt; &lt;p&gt;9AM, - Seed: Crap, dude, the test is failing and Im out of 100k context, help!&lt;br /&gt; - Me: Hold on pal, there you go, quick restart, You were working on this and that, keep going mate. This is the short error log, DON'T copy and paste 100k lines of repeating errors lol&lt;br /&gt; - Seed: Gotcha...&lt;/p&gt; &lt;p&gt;11AM, - Seed: Boom done, not a single f**king error, code is in src, tests are in test, examples are here, and this is some docs for you, stupid human being&lt;br /&gt; - Me: :O&lt;/p&gt; &lt;p&gt;Holy f**k. &lt;/p&gt; &lt;p&gt;Anyone else using seed-oss-36b? I literally downloaded it yesterday, ran the Q6_K_XL quant to fit in the 48GB vram with 100k context at q8. Im speachless. Yes, it is slower than the competitors (devstral? qwen?) but the quality is jaw dropping. Worked for hours, without supervision, and if not the context length it would possibly finish the entire project alone. Wierd that there is so little news about this model. Its stupidly good at agentic coding.&lt;/p&gt; &lt;p&gt;Human coding? RIP 2025&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChopSticksPlease"&gt; /u/ChopSticksPlease &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqm5g4/seed_oss_36b_made_me_reconsider_my_life_choices/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqm5g4/seed_oss_36b_made_me_reconsider_my_life_choices/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqm5g4/seed_oss_36b_made_me_reconsider_my_life_choices/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T14:15:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqt04l</id>
    <title>Tutorial on finetuning Gemma3 1B to generate 3D objects</title>
    <updated>2025-12-19T18:48:27+00:00</updated>
    <author>
      <name>/u/ThomasPhilli</name>
      <uri>https://old.reddit.com/user/ThomasPhilli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the past 6 weeks, I have been spending time finetuning Gemma3 1B to generate OpenSCAD code.&lt;/p&gt; &lt;p&gt;There is almost no good dataset nor evaluation framework available. But I think it worked out well with synthetic data generation + careful finetuning.&lt;/p&gt; &lt;p&gt;I put together a quick guide, lmk if it's helpful!&lt;/p&gt; &lt;p&gt;Have a good weekend.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasPhilli"&gt; /u/ThomasPhilli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://starmind.comfyspace.tech/experiments/cadmonkey-v2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqt04l/tutorial_on_finetuning_gemma3_1b_to_generate_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqt04l/tutorial_on_finetuning_gemma3_1b_to_generate_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T18:48:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqfmsr</id>
    <title>Meta releases SAM Audio for audio separation</title>
    <updated>2025-12-19T08:03:38+00:00</updated>
    <author>
      <name>/u/umarmnaq</name>
      <uri>https://old.reddit.com/user/umarmnaq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqfmsr/meta_releases_sam_audio_for_audio_separation/"&gt; &lt;img alt="Meta releases SAM Audio for audio separation" src="https://external-preview.redd.it/cHEzMGt1a2YzNDhnMXbShRCjAlPQsamMmoIWTAtR2gquYxttgWY9vfB1L3ZP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a42426e742804c90a902cb380ea41038b48a1027" title="Meta releases SAM Audio for audio separation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;SAM Audio separates target and residual sounds from any audio or audiovisual source—across general sound, music, and speech.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://ai.meta.com/samaudio/"&gt;https://ai.meta.com/samaudio/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/facebook/sam-audio"&gt;https://huggingface.co/collections/facebook/sam-audio&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/facebookresearch/sam-audio"&gt;https://github.com/facebookresearch/sam-audio&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/umarmnaq"&gt; /u/umarmnaq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/en7nfnmf348g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqfmsr/meta_releases_sam_audio_for_audio_separation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqfmsr/meta_releases_sam_audio_for_audio_separation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T08:03:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqui9l</id>
    <title>FlashHead: Up to 50% faster token generation on top of other techniques like quantization</title>
    <updated>2025-12-19T19:49:00+00:00</updated>
    <author>
      <name>/u/Any_Frame9721</name>
      <uri>https://old.reddit.com/user/Any_Frame9721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqui9l/flashhead_up_to_50_faster_token_generation_on_top/"&gt; &lt;img alt="FlashHead: Up to 50% faster token generation on top of other techniques like quantization" src="https://external-preview.redd.it/_fiYnJOiLFMjXWAHyOC4PQuzh6t1PbyOv347pBjd4tc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79b959a9401497de2b2ba109df8da0de73c9e45f" title="FlashHead: Up to 50% faster token generation on top of other techniques like quantization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;We have developed FlashHead, an architectural innovation for SLMs offering up to 50% more tokens per second &lt;strong&gt;on top&lt;/strong&gt; of other techniques like quantization. It is a drop-in replacement for the language model head. It works by replacing the expensive lm head with the FlashHead layer that uses information retrieval to identify the next token efficiently with perfect accuracy compared to the baseline model.&lt;/p&gt; &lt;p&gt;Try it with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install embedl-models python -m embedl.models.vllm.demo \ --model embedl/Llama-3.2-3B-Instruct-FlashHead-W4A16 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Llama 3.2 1B Instruct benchmark on Ada Gen 3500 GPU (batch size = 1)&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/embedl/Llama-3.2-1B-Instruct-FlashHead#token-generation-speed-rtx-3500-ada-batch-size--1"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Precision&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Tokens/sec&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Speedup vs BF16&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;BF16 baseline&lt;/td&gt; &lt;td align="left"&gt;130&lt;/td&gt; &lt;td align="left"&gt;1.0×&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;FlashHead (Embedl)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;163&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1.25×&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;W4A16 baseline&lt;/td&gt; &lt;td align="left"&gt;278&lt;/td&gt; &lt;td align="left"&gt;2.14×&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;FlashHead W4A16 (Embedl)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;485&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3.73×&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The models perform as their original counterparts, but faster. We have tried to make it as friction-less as possible to use via our vLLM integration, we would love to hear feedback. The GitHub repo is &lt;a href="https://github.com/embedl/embedl-models"&gt;https://github.com/embedl/embedl-models&lt;/a&gt;, &lt;/p&gt; &lt;p&gt;We are a Swedish startup working on efficient AI. We also have a free Edge AI Hub that allows users to run models on mobile devices (Android, iOS) &lt;a href="https://hub.embedl.com"&gt;https://hub.embedl.com&lt;/a&gt; , feel free to join our Slack (#llm channel) for discussions or open an issue on GitHub&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any_Frame9721"&gt; /u/Any_Frame9721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/embedl/models"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqui9l/flashhead_up_to_50_faster_token_generation_on_top/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqui9l/flashhead_up_to_50_faster_token_generation_on_top/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T19:49:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqpj29</id>
    <title>Career Advice in AI — Notes from an Andrew Ng Lecture</title>
    <updated>2025-12-19T16:31:46+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqpj29/career_advice_in_ai_notes_from_an_andrew_ng/"&gt; &lt;img alt="Career Advice in AI — Notes from an Andrew Ng Lecture" src="https://preview.redd.it/cu5vt8lnt68g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=efe9182c042a4013c8b0a1760f4ff5a1ac022efd" title="Career Advice in AI — Notes from an Andrew Ng Lecture" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;[1] A Golden Age for AI Careers&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Andrew Ng emphasizes that this is the best time ever to build a career in AI. He notes that the complexity of tasks AI can handle is doubling approximately every seven months, meaning progress is accelerating, not slowing down.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[2] The Power of AI Coding Tools&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Staying on the “frontier” of coding tools (like Cursor, Claude, and Gemini) is crucial. Being even half a generation behind in your tooling makes you significantly less productive in the current market.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[3] The “Product Management Bottleneck”&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Because AI has made writing code so much cheaper and faster, the bottleneck has shifted to deciding what to build. Engineers who can talk to users, develop empathy, and handle product management (PM) tasks are the fastest-moving individuals in Silicon Valley today.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[4] Surround Yourself with the Right People&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Success is highly predicted by the people you surround yourself with. Ng encourages building a “rich connective tissue” of friends and colleagues to share insights that aren’t yet published on the internet.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[5] Team Over Brand&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;When job hunting, the specific team and people you work with day-to-day are more important than the company’s “hot brand.” Avoid companies that refuse to tell you which team you will join before you sign.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[6] Go and Build Stuff&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Andrew Ng’s number one piece of advice is to simply &lt;strong&gt;go and build stuff&lt;/strong&gt;. The cost of failure is low (losing a weekend), but the learning and demonstration of skill are invaluable.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[7] The Value of Hard Work&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Andrew Ng encourages working hard, defining it not just by hours but by output and passion for building.&lt;/p&gt; &lt;p&gt;Video - &lt;a href="https://www.youtube.com/watch?v=AuZoDsNmG_s"&gt;https://www.youtube.com/watch?v=AuZoDsNmG_s&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cu5vt8lnt68g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqpj29/career_advice_in_ai_notes_from_an_andrew_ng/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqpj29/career_advice_in_ai_notes_from_an_andrew_ng/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T16:31:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqoldt</id>
    <title>Chinese researchers unveil "LightGen": An all-optical chip that outperforms Nvidia’s A100 by 100x</title>
    <updated>2025-12-19T15:55:23+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New research from SJTU and Tsinghua (these are top tier labs, not slopmonsters like East China Normal University etc.).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.science.org/doi/10.1126/science.adv7434"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoldt/chinese_researchers_unveil_lightgen_an_alloptical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoldt/chinese_researchers_unveil_lightgen_an_alloptical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T15:55:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqn0vq</id>
    <title>GLM 4.7 is Coming?</title>
    <updated>2025-12-19T14:52:18+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqn0vq/glm_47_is_coming/"&gt; &lt;img alt="GLM 4.7 is Coming?" src="https://preview.redd.it/206mfj3dc68g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f01e8b54d3347827d6980eb1b0cbc7453cfd2d9c" title="GLM 4.7 is Coming?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/30876"&gt;https://github.com/vllm-project/vllm/pull/30876&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/206mfj3dc68g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqn0vq/glm_47_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqn0vq/glm_47_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T14:52:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqegcr</id>
    <title>Realist meme of the year!</title>
    <updated>2025-12-19T06:49:54+00:00</updated>
    <author>
      <name>/u/Slight_Tone_2188</name>
      <uri>https://old.reddit.com/user/Slight_Tone_2188</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/"&gt; &lt;img alt="Realist meme of the year!" src="https://preview.redd.it/8oge3a2by38g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4697e9a87c50f3f170db7e87eccd27363c505dc" title="Realist meme of the year!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slight_Tone_2188"&gt; /u/Slight_Tone_2188 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8oge3a2by38g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqegcr/realist_meme_of_the_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T06:49:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqoi6i</id>
    <title>Qwen released Qwen-Image-Layered on Hugging face.</title>
    <updated>2025-12-19T15:51:45+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoi6i/qwen_released_qwenimagelayered_on_hugging_face/"&gt; &lt;img alt="Qwen released Qwen-Image-Layered on Hugging face." src="https://b.thumbs.redditmedia.com/WT_uezmugp_bMYr9okz4OYqH1W02XtM64SzwTE-NCms.jpg" title="Qwen released Qwen-Image-Layered on Hugging face." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Layered"&gt;https://huggingface.co/Qwen/Qwen-Image-Layered&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Photoshop-grade layering Physically isolated RGBA layers with true native editability Prompt-controlled structure Explicitly specify 3–10 layers — from coarse layouts to fine-grained details Infinite decomposition Keep drilling down: layers within layers, to any depth of detail&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pqoi6i"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoi6i/qwen_released_qwenimagelayered_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqoi6i/qwen_released_qwenimagelayered_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T15:51:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp9w31</id>
    <title>AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio</title>
    <updated>2025-12-17T22:18:01+00:00</updated>
    <author>
      <name>/u/AIatMeta</name>
      <uri>https://old.reddit.com/user/AIatMeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We’re the research team behind the newest members of the Segment Anything collection of models: SAM 3 + SAM 3D + SAM Audio.&lt;/p&gt; &lt;p&gt;We’re excited to be here to talk all things SAM (sorry, we can’t share details on other projects or future work) and have members from across our team participating:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SAM 3 (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/segment-anything-model-3/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nikhila Ravi&lt;/li&gt; &lt;li&gt;Pengchuan Zhang&lt;/li&gt; &lt;li&gt;Shoubhik Debnath&lt;/li&gt; &lt;li&gt;Chay Ryali&lt;/li&gt; &lt;li&gt;Yuan-Ting Hu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM 3D (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-3d/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weiyao Wang&lt;/li&gt; &lt;li&gt;Sasha Sax&lt;/li&gt; &lt;li&gt;Xitong Yang&lt;/li&gt; &lt;li&gt;Jinkun Cao&lt;/li&gt; &lt;li&gt;Michelle Guo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM Audio (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-audio/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bowen Shi&lt;/li&gt; &lt;li&gt;Andros Tjandra&lt;/li&gt; &lt;li&gt;John Hoffman&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try SAM Audio, SAM 3D, and SAM 3 in the Segment Anything Playground: &lt;a href="https://go.meta.me/87b53b"&gt;https://go.meta.me/87b53b&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PROOF: &lt;a href="https://x.com/AIatMeta/status/2001429429898407977"&gt;https://x.com/AIatMeta/status/2001429429898407977&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT: Thanks to everyone who joined the AMA and for all the great conversation. We look forward to the next one!&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIatMeta"&gt; /u/AIatMeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T22:18:01+00:00</published>
  </entry>
</feed>
