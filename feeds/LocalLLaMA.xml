<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-20T19:18:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r9vsye</id>
    <title>Nice interactive explanation of Speculative Decoding</title>
    <updated>2026-02-20T13:47:19+00:00</updated>
    <author>
      <name>/u/individual_kex</name>
      <uri>https://old.reddit.com/user/individual_kex</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vsye/nice_interactive_explanation_of_speculative/"&gt; &lt;img alt="Nice interactive explanation of Speculative Decoding" src="https://external-preview.redd.it/EhW4bQWT9WIeRw5amz2pS-lzd3lb6K6qLMCB-e4QXzU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb9aba232415d2dd8db7afab8bd1d38bfcb06a5d" title="Nice interactive explanation of Speculative Decoding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/individual_kex"&gt; /u/individual_kex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.adaptive-ml.com/post/speculative-decoding-visualized"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vsye/nice_interactive_explanation_of_speculative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vsye/nice_interactive_explanation_of_speculative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:47:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9rboa</id>
    <title>Buying cheap 'no display' gpus from ebay?</title>
    <updated>2026-02-20T09:57:35+00:00</updated>
    <author>
      <name>/u/getpodapp</name>
      <uri>https://old.reddit.com/user/getpodapp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm finding these RTX 4080/90's for like 200-300GBP on ebay marked as 'no display', clearly theres a risk that they're completely fucked. &lt;/p&gt; &lt;p&gt;If its literally just 'no display' but compute works it seems a stupid easy way of getting a bunch of vRAM on modern GPUs...?&lt;/p&gt; &lt;p&gt;Does anyone experience with this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getpodapp"&gt; /u/getpodapp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9rboa/buying_cheap_no_display_gpus_from_ebay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9rboa/buying_cheap_no_display_gpus_from_ebay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9rboa/buying_cheap_no_display_gpus_from_ebay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T09:57:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9mcjw</id>
    <title>GPT-OSS-120b on 2X RTX5090</title>
    <updated>2026-02-20T05:02:12+00:00</updated>
    <author>
      <name>/u/Interesting-Ad4922</name>
      <uri>https://old.reddit.com/user/Interesting-Ad4922</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mcjw/gptoss120b_on_2x_rtx5090/"&gt; &lt;img alt="GPT-OSS-120b on 2X RTX5090" src="https://preview.redd.it/atfvw7c10lkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c2e6695819ff38848b821da3efa31d5b86b8bb9" title="GPT-OSS-120b on 2X RTX5090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got GPT-OSS-120b deployed on dual RTX5090 rig. 128k context (Significant CPU offloading ~10t/s) I know it's nothing amazing I'm just a little proud of myself and needed to tell someone! Thanks for lookin!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Ad4922"&gt; /u/Interesting-Ad4922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/atfvw7c10lkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mcjw/gptoss120b_on_2x_rtx5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mcjw/gptoss120b_on_2x_rtx5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T05:02:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9mkgj</id>
    <title>PaddleOCR-VL now in llama.cpp</title>
    <updated>2026-02-20T05:13:34+00:00</updated>
    <author>
      <name>/u/PerfectLaw5776</name>
      <uri>https://old.reddit.com/user/PerfectLaw5776</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b8110"&gt;https://github.com/ggml-org/llama.cpp/releases/tag/b8110&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far this is the best performing open-source multilingual OCR model I've seen, would appreciate if other people can share their findings. It's 0.9b so it shouldn't brick our machines. &lt;a href="https://huggingface.co/octopusmegalopod/some-paddleocr1.5-vl-ggufs"&gt;Some GGUFs&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PerfectLaw5776"&gt; /u/PerfectLaw5776 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mkgj/paddleocrvl_now_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mkgj/paddleocrvl_now_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mkgj/paddleocrvl_now_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T05:13:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ra0nz9</id>
    <title>If you're building hierarchical/tree-based RAG, this might be helpful.</title>
    <updated>2026-02-20T16:52:49+00:00</updated>
    <author>
      <name>/u/auditsu</name>
      <uri>https://old.reddit.com/user/auditsu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent a few days building and benchmarking a hierarchical retrieval system â€” routing queries through a tree of LLM-generated summaries instead of flat vector search. The idea: save tokens by pruning irrelevant branches early, only retrieve what matters.&lt;/p&gt; &lt;p&gt;It doesn't work. At least not with embedding-based routing.&lt;/p&gt; &lt;p&gt;At ~300 chunks it looked decent. At ~22k chunks it scored 0.094 nDCG vs 0.749 for plain dense retrieval + cross-encoder reranking. Completely unusable.&lt;/p&gt; &lt;p&gt;The core problem is simple: routing errors at each tree level compound multiplicatively. If you've got even a 15% miss rate per level, after 5 levels you're correctly routing less than half your queries. The deeper the tree (i.e. the larger your corpus â€” exactly when you need this most), the worse it gets.&lt;/p&gt; &lt;p&gt;Things I tested that didn't fix it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Wider beam search (helps, but just delays the collapse)&lt;/li&gt; &lt;li&gt;Better embeddings (mpnet vs MiniLM â€” marginal)&lt;/li&gt; &lt;li&gt;Richer summaries, contrastive prompts, content snippets (all plateau at the same ceiling)&lt;/li&gt; &lt;li&gt;Cross-encoder routing (actually made it worse â€” MS-MARCO models aren't trained on structured summary text)&lt;/li&gt; &lt;li&gt;BM25 hybrid routing (summaries are too sparse for lexical matching)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The tree structure itself is fine â€” beam width sweep proved the correct branches exist at every level. The routing mechanism just can't reliably pick them.&lt;/p&gt; &lt;p&gt;If you're using RAPTOR-style retrieval, this explains why collapsed tree mode (flat search over all nodes) beats top-down traversal. Don't fight the compounding â€” skip it entirely.&lt;/p&gt; &lt;p&gt;Paper and full code/benchmarks: &lt;a href="https://doi.org/10.5281/zenodo.18714001"&gt;https://doi.org/10.5281/zenodo.18714001&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/auditsu"&gt; /u/auditsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ra0nz9/if_youre_building_hierarchicaltreebased_rag_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ra0nz9/if_youre_building_hierarchicaltreebased_rag_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ra0nz9/if_youre_building_hierarchicaltreebased_rag_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T16:52:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9h3g8</id>
    <title>Qwen3 Coder Next 8FP in the process of converting the entire Flutter documentation for 12 hours now with just 3 sentence prompt with 64K max tokens at around 102GB memory (out of 128GB)...</title>
    <updated>2026-02-20T00:54:46+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9h3g8/qwen3_coder_next_8fp_in_the_process_of_converting/"&gt; &lt;img alt="Qwen3 Coder Next 8FP in the process of converting the entire Flutter documentation for 12 hours now with just 3 sentence prompt with 64K max tokens at around 102GB memory (out of 128GB)..." src="https://preview.redd.it/5ouemzagqjkg1.png?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=ae25e9c86a516f23c1f47828293a3fbe972468b8" title="Qwen3 Coder Next 8FP in the process of converting the entire Flutter documentation for 12 hours now with just 3 sentence prompt with 64K max tokens at around 102GB memory (out of 128GB)..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A remarkable LLM -- we really have a winner.&lt;/p&gt; &lt;p&gt;(Most of the models below were NVFP4)&lt;/p&gt; &lt;p&gt;GPT OSS 120B can't do this (though it's a bit outdated now)&lt;br /&gt; GLM 4.7 Flash can't do this&lt;br /&gt; SERA 32B tokens too slow&lt;br /&gt; Devstral 2 Small can't do this&lt;br /&gt; SEED OSS freezes while thinking&lt;br /&gt; Nemotron 3 Nano can't do this &lt;/p&gt; &lt;p&gt;(Unsure if it's Cline (when streaming &amp;lt;think&amp;gt;) or the LLM, but GPT OSS, GLM, Devstral, and Nemotron go on an insanity loop, for thinking, coding, or both)&lt;/p&gt; &lt;p&gt;Markdown isn't exactly coding, but for multi-iteration (because it runs out of context tokens) conversions, it's flawless.&lt;/p&gt; &lt;p&gt;Now I just wish VS Codium + Cline handles all these think boxes (on the right side of the UI) better. It's impossible to scroll even with 32GB RAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r9h3g8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9h3g8/qwen3_coder_next_8fp_in_the_process_of_converting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9h3g8/qwen3_coder_next_8fp_in_the_process_of_converting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T00:54:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9gve8</id>
    <title>I feel left behind. What is special about OpenClaw?</title>
    <updated>2026-02-20T00:44:48+00:00</updated>
    <author>
      <name>/u/Recent_Jellyfish2190</name>
      <uri>https://old.reddit.com/user/Recent_Jellyfish2190</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While there are tools like Manus ai, It seems like everyone is excited about OpenClaw lately, and I genuinely donâ€™t fully understand the differentiation. What exactly is the shift here? Is it UX, architecture, control layer, distribution? Not criticizing, just trying to understand what Iâ€™m missing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recent_Jellyfish2190"&gt; /u/Recent_Jellyfish2190 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9gve8/i_feel_left_behind_what_is_special_about_openclaw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9gve8/i_feel_left_behind_what_is_special_about_openclaw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9gve8/i_feel_left_behind_what_is_special_about_openclaw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T00:44:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9fkks</id>
    <title>We will have Gemini 3.1 before Gemma 4...</title>
    <updated>2026-02-19T23:49:53+00:00</updated>
    <author>
      <name>/u/xandep</name>
      <uri>https://old.reddit.com/user/xandep</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9fkks/we_will_have_gemini_31_before_gemma_4/"&gt; &lt;img alt="We will have Gemini 3.1 before Gemma 4..." src="https://preview.redd.it/hd5oal2ngjkg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c08bae0f338dab67384ce398502fe29f5b06645" title="We will have Gemini 3.1 before Gemma 4..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Appeared on Antigravity...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xandep"&gt; /u/xandep &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hd5oal2ngjkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9fkks/we_will_have_gemini_31_before_gemma_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9fkks/we_will_have_gemini_31_before_gemma_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T23:49:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9wbl3</id>
    <title>ggml / llama.cpp joining Hugging Face â€” implications for local inference?</title>
    <updated>2026-02-20T14:08:56+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ggml / llama.cpp joining HF feels like a significant moment for local inference.&lt;/p&gt; &lt;p&gt;On one hand, this could massively accelerate tooling, integration, and long-term support for local AI. On the other, it concentrates even more of the open model stack under one umbrella.&lt;/p&gt; &lt;p&gt;Is this a net win for the community?&lt;/p&gt; &lt;p&gt;What does this mean for alternative runtimes and independent inference stacks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wbl3/ggml_llamacpp_joining_hugging_face_implications/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wbl3/ggml_llamacpp_joining_hugging_face_implications/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wbl3/ggml_llamacpp_joining_hugging_face_implications/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T14:08:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ra1wxm</id>
    <title>I got 45-46 tok/s on IPhone 14 Pro Max using BitNet</title>
    <updated>2026-02-20T17:37:38+00:00</updated>
    <author>
      <name>/u/Middle-Hurry4718</name>
      <uri>https://old.reddit.com/user/Middle-Hurry4718</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ra1wxm/i_got_4546_toks_on_iphone_14_pro_max_using_bitnet/"&gt; &lt;img alt="I got 45-46 tok/s on IPhone 14 Pro Max using BitNet" src="https://external-preview.redd.it/MnpoZng3cWFyb2tnMag_nQlaOiUb75GBHB5vo6hyb1PC6uSB2BeZWzIId6Ao.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56ac4a23fdd3e309d449ba68f654ccf3d70eee7b" title="I got 45-46 tok/s on IPhone 14 Pro Max using BitNet" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ported Microsoftâ€™s BitNet to iOS. Getting 45 tok/s on iPhone 14 Pro Max with the 0.7B model, ~200MB memory. BitNet uses 1-bit weights (-1, 0, +1) instead of 16-bit floats so the model is tiny and runs fast. The ARM NEON kernels already worked on M-series Macs so getting it on iPhone was mostly build system wrangling. I am currently running a base model (outputs are nonsense), next step is the instruction-tuned 2B model for actual usable chat. I will open source eventually, but sooner rather than later if thereâ€™s interest.â€‹â€‹â€‹â€‹â€‹&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Middle-Hurry4718"&gt; /u/Middle-Hurry4718 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/whlo0jrarokg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ra1wxm/i_got_4546_toks_on_iphone_14_pro_max_using_bitnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ra1wxm/i_got_4546_toks_on_iphone_14_pro_max_using_bitnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T17:37:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9ours</id>
    <title>Qwen3.5 Plus, GLM 5, Gemini 3.1 Pro, Sonnet 4.6, three new open source agents, and a lot more added to SanityBoard</title>
    <updated>2026-02-20T07:24:18+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link: &lt;a href="https://sanityboard.lr7.dev/"&gt;https://sanityboard.lr7.dev/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yeah I've been running evals and working on this for over 3 days straight all day to get this all finished. Too tired to do a proper writeup, so I will give some bullet points and a disclaimer.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;27 New eval results added in total&lt;/li&gt; &lt;li&gt;Got our first 4 community submissions, which brings us GPT 5.3 Codex Spark results, and a few Droid + Skills results to show us how big of a difference a suitable skills file can make.&lt;/li&gt; &lt;li&gt;3 New OSS coding agents; kilocode cli, cline cli, and pi*&lt;/li&gt; &lt;li&gt;Some site UI improvements, like date slider filter, being able to expand the filter options window, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Interesting pattern I realized. GPT-codex models do really well cause they like to iterate, a lot. These kinds of evals favor models with this kind of tendency. Claude models don't iterate as much, so they sometimes get edged out in these kinds of evals. In an actual interactive coding scenario, I do believe the claude models are still better. Now if you want to just assign a long running task and forget it, that's where the gpt-codex models shine. They just keep going and going until done, they're good at that.&lt;/p&gt; &lt;p&gt;A somewhat important note, the infra used makes a HUGE difference in scores. I noticed this very early on, back when I used to run a ton of terminal bench evals, and especially when I decided to run it against as many different providers as I could to see which one was the best for Kimi K2 thinking. Even the speed affected scores a lot. My bench is no different in this regard, although I tried my best to work around this by having generous retry limits, and manually vetting every run for infra issues (which probably takes up the majority of my time), and rerunning any evals that looked like they may have suffered infra issues. This however isn't perfect, I am human. The reason I mention this is cause &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt; infra is dying. It made it almost impossible to bench against the official api. It was actually more expensive to use than paying standard api rates to claude for opus lol. They ghosted after I asked if I could have credits back for the wasted tokens I never got.. but that's neither here nor there. And also you might see some of the same models but from different providers score differently for infra reasons. Even the date of eval might matter for this, since sometimes providers change, either improving and fixing things, or otherwise. Also worth noting since some runs are older than others, some things might not score as well, being on an older agent version. Hopefully the filter by date slider I added can help with this.&lt;/p&gt; &lt;p&gt;*Pi was a large part of why this took me so much time and reruns. The retry logic had to be changed cause it's the only agent that does not have streaming stdout for some reason, and buffers it all until it's done. It also has 0 iteration whatsoever, it just does everything on one shot and never iterates on it again, leading to very poor scores. No other agents behave like this. These changes introduced bugs, which meant a lot of time spent fixing things and having to rerun things for fair evals. Pi I think is really cool, but since it's headless mode or whatever you want to call it is only a half complete implementation at best, it's almost impossible to get a fair evaluation of it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9ours/qwen35_plus_glm_5_gemini_31_pro_sonnet_46_three/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9ours/qwen35_plus_glm_5_gemini_31_pro_sonnet_46_three/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9ours/qwen35_plus_glm_5_gemini_31_pro_sonnet_46_three/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T07:24:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9e27i</id>
    <title>Free ASIC Llama 3.1 8B inference at 16,000 tok/s - no, not a joke</title>
    <updated>2026-02-19T22:48:03+00:00</updated>
    <author>
      <name>/u/Easy_Calligrapher790</name>
      <uri>https://old.reddit.com/user/Easy_Calligrapher790</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;A fast inference hardware startup, Taalas, has released a free chatbot interface and API endpoint running on their chip. They chose a small model intentionally as proof of concept. Well, it worked out really well, it runs at 16k tps! I know this model is quite limited but there likely exists a group of users who find it sufficient and would benefit from hyper-speed on offer.&lt;/p&gt; &lt;p&gt;Anyways, they are of course moving on to bigger and better models, but are giving free access to their proof-of-concept to people who want it.&lt;/p&gt; &lt;p&gt;More info: &lt;a href="https://taalas.com/the-path-to-ubiquitous-ai/"&gt;https://taalas.com/the-path-to-ubiquitous-ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chatbot demo: &lt;a href="https://chatjimmy.ai/"&gt;https://chatjimmy.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Inference API service: &lt;a href="https://taalas.com/api-request-form"&gt;https://taalas.com/api-request-form&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's worth trying out the chatbot even just for a bit, the speed is really something to experience. Cheers!&lt;/p&gt; &lt;p&gt;EDIT: It's worth noting that the chatbot demo actually undersells the speed on display. Anything over a few hundred tps is perceived as instantaneous, so the experience of 1k tps vs 16k tps should be pretty similar. So you are only seeing the bottom few percent of the speed on offer. A proper demo would be using a token-intensive workload with their API. Now THAT would be something to see.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Easy_Calligrapher790"&gt; /u/Easy_Calligrapher790 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9e27i/free_asic_llama_31_8b_inference_at_16000_toks_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9e27i/free_asic_llama_31_8b_inference_at_16000_toks_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9e27i/free_asic_llama_31_8b_inference_at_16000_toks_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T22:48:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r99yda</id>
    <title>Pack it up guys, open weight AI models running offline locally on PCs aren't real. ðŸ˜ž</title>
    <updated>2026-02-19T20:11:42+00:00</updated>
    <author>
      <name>/u/CesarOverlorde</name>
      <uri>https://old.reddit.com/user/CesarOverlorde</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r99yda/pack_it_up_guys_open_weight_ai_models_running/"&gt; &lt;img alt="Pack it up guys, open weight AI models running offline locally on PCs aren't real. ðŸ˜ž" src="https://preview.redd.it/ogkdei4udikg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8834b06cb1ae3aaa95c27230b622dd640e7d9634" title="Pack it up guys, open weight AI models running offline locally on PCs aren't real. ðŸ˜ž" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CesarOverlorde"&gt; /u/CesarOverlorde &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ogkdei4udikg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r99yda/pack_it_up_guys_open_weight_ai_models_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r99yda/pack_it_up_guys_open_weight_ai_models_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T20:11:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9x0l2</id>
    <title>We replaced the LLM in a voice assistant with a fine-tuned 0.6B model. 90.9% tool call accuracy vs. 87.5% for the 120B teacher. ~40ms inference.</title>
    <updated>2026-02-20T14:37:00+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9x0l2/we_replaced_the_llm_in_a_voice_assistant_with_a/"&gt; &lt;img alt="We replaced the LLM in a voice assistant with a fine-tuned 0.6B model. 90.9% tool call accuracy vs. 87.5% for the 120B teacher. ~40ms inference." src="https://preview.redd.it/lh8p2xv0vnkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fbabc4097ccac2e5d448a628acbee7068bf698ee" title="We replaced the LLM in a voice assistant with a fine-tuned 0.6B model. 90.9% tool call accuracy vs. 87.5% for the 120B teacher. ~40ms inference." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Voice assistants almost always use a cloud LLM for the &amp;quot;brain&amp;quot; stage (intent routing, slot extraction, dialogue state). The LLM stage alone adds 375-750ms per turn, which pushes total pipeline latency past the 500-800ms threshold where conversations feel natural.&lt;/p&gt; &lt;p&gt;For bounded workflows like banking, insurance, or telecom, that's a lot of unnecessary overhead. The task is not open-ended generation -- it's classifying intent and extracting structured slots from what the user said. That's exactly where fine-tuned SLMs shine.&lt;/p&gt; &lt;p&gt;We built VoiceTeller, a banking voice assistant that swaps the LLM for a locally-running fine-tuned Qwen3-0.6B. Numbers:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Params&lt;/th&gt; &lt;th&gt;Single-Turn Tool Call Accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;GPT-oss-120B (teacher)&lt;/td&gt; &lt;td&gt;120B&lt;/td&gt; &lt;td&gt;87.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-0.6B (fine-tuned)&lt;/td&gt; &lt;td&gt;0.6B&lt;/td&gt; &lt;td&gt;&lt;strong&gt;90.9%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-0.6B (base)&lt;/td&gt; &lt;td&gt;0.6B&lt;/td&gt; &lt;td&gt;48.7%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;And the pipeline latency breakdown:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Stage&lt;/th&gt; &lt;th&gt;Cloud LLM&lt;/th&gt; &lt;th&gt;SLM&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;ASR&lt;/td&gt; &lt;td&gt;200-350ms&lt;/td&gt; &lt;td&gt;~200ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Brain&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;375-750ms&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;~40ms&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TTS&lt;/td&gt; &lt;td&gt;75-150ms&lt;/td&gt; &lt;td&gt;~75ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;680-1300ms&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;~315ms&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The fine-tuned model beats the 120B teacher by ~3 points while being 200x smaller. The base model at 48.7% is unusable -- over a 3-turn conversation that compounds to about 11.6% success rate.&lt;/p&gt; &lt;p&gt;Architecture note: the SLM never generates user-facing text. It only outputs structured JSON (function name + slots). A deterministic orchestrator handles slot elicitation and response templates. This keeps latency bounded and responses well-formed regardless of what the model outputs.&lt;/p&gt; &lt;p&gt;The whole thing runs locally: Qwen3-ASR-0.6B for speech-to-text, the fine-tuned Qwen3-0.6B via llama.cpp for intent routing, Qwen3-TTS for speech synthesis. Full pipeline on Apple Silicon with MPS.&lt;/p&gt; &lt;p&gt;GitHub (code + training data + pre-trained GGUF): &lt;a href="https://github.com/distil-labs/distil-voice-assistant-banking"&gt;https://github.com/distil-labs/distil-voice-assistant-banking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace model: &lt;a href="https://huggingface.co/distil-labs/distil-qwen3-0.6b-voice-assistant-banking"&gt;https://huggingface.co/distil-labs/distil-qwen3-0.6b-voice-assistant-banking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog post with the full write-up: &lt;a href="https://www.distillabs.ai/blog/the-llm-in-your-voice-assistant-is-the-bottleneck-replace-it-with-an-slm"&gt;https://www.distillabs.ai/blog/the-llm-in-your-voice-assistant-is-the-bottleneck-replace-it-with-an-slm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the training setup, the multi-turn tool calling format, or why the student beats the teacher.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lh8p2xv0vnkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9x0l2/we_replaced_the_llm_in_a_voice_assistant_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9x0l2/we_replaced_the_llm_in_a_voice_assistant_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T14:37:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9uu5h</id>
    <title>Qwen3 Coder Next on 8GB VRAM</title>
    <updated>2026-02-20T13:05:21+00:00</updated>
    <author>
      <name>/u/Juan_Valadez</name>
      <uri>https://old.reddit.com/user/Juan_Valadez</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I have a PC with 64 GB of RAM and an RTX 3060 12 GB, and I'm running Qwen3 Coder Next in MXFP4 with 131,072 context tokens.&lt;/p&gt; &lt;p&gt;I get a sustained speed of around 23 t/s throughout the entire conversation.&lt;/p&gt; &lt;p&gt;I mainly use it for front-end and back-end web development, and it works perfectly.&lt;/p&gt; &lt;p&gt;I've stopped paying for my Claude Max plan ($100 USD per month) to use only Claude Code with the following configuration:&lt;/p&gt; &lt;p&gt;&lt;code&gt;set GGML_CUDA_GRAPH_OPT=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server -m ../GGUF/qwen3-coder-next-mxfp4.gguf -ngl 999 -sm none -mg 0 -t 12 -fa on -cmoe -c 131072 -b 512 -ub 512 -np 1 --jinja --temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01 --repeat-penalty 1.0 --host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--port 8080&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I promise you it works fast enough and with incredible quality to work with complete SaaS applications (I know how to program, obviously, but I'm delegating practically everything to AI).&lt;/p&gt; &lt;p&gt;If you have at least 64 GB of RAM and 8 GB of VRAM, I recommend giving it a try; you won't regret it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juan_Valadez"&gt; /u/Juan_Valadez &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uu5h/qwen3_coder_next_on_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uu5h/qwen3_coder_next_on_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uu5h/qwen3_coder_next_on_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9y6s8</id>
    <title>TranscriptionSuite - A fully local, private &amp; open source audio transcription for Linux, Windows &amp; macOS</title>
    <updated>2026-02-20T15:22:24+00:00</updated>
    <author>
      <name>/u/TwilightEncoder</name>
      <uri>https://old.reddit.com/user/TwilightEncoder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9y6s8/transcriptionsuite_a_fully_local_private_open/"&gt; &lt;img alt="TranscriptionSuite - A fully local, private &amp;amp; open source audio transcription for Linux, Windows &amp;amp; macOS" src="https://external-preview.redd.it/ZjVodnR2dGoyb2tnMfrHn1-Z1IlbM1M-CdvVLf1S0fx3BvVT39BjZwD6xxr6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c01a41fb0c91487d97d9e6bbd7ba58c3750d09f" title="TranscriptionSuite - A fully local, private &amp;amp; open source audio transcription for Linux, Windows &amp;amp; macOS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! This is a short presentation for my hobby project, &lt;a href="https://github.com/homelab-00/TranscriptionSuite"&gt;TranscriptionSuite&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; A fully local &amp;amp; private Speech-To-Text app for Linux, Windows &amp;amp; macOS. Python backend + Electron frontend, utilizing faster-whisper and CUDA acceleration.&lt;/p&gt; &lt;p&gt;If you're interested in the boring dev stuff, go to the bottom section.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;I'm releasing a major UI upgrade today. Enjoy!&lt;/p&gt; &lt;p&gt;Short sales pitch:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;100% Local&lt;/strong&gt;: &lt;em&gt;Everything&lt;/em&gt; runs on your own computer, the app doesn't need internet beyond the initial setup&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Truly Multilingual&lt;/strong&gt;: Supports &lt;a href="https://github.com/openai/whisper/blob/main/whisper/tokenizer.py"&gt;90+ languages&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully featured GUI&lt;/strong&gt;: Electron desktop app for Linux, Windows, and macOS (Apple Silicon)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU + CPU Mode&lt;/strong&gt;: NVIDIA CUDA acceleration (recommended), or CPU-only mode for any platform including macOS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Longform Transcription&lt;/strong&gt;: Record as long as you want and have it transcribed in seconds&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live Mode&lt;/strong&gt;: Real-time sentence-by-sentence transcription for continuous dictation workflows&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speaker Diarization&lt;/strong&gt;: PyAnnote-based speaker identification&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Static File Transcription&lt;/strong&gt;: Transcribe existing audio/video files with multi-file import queue, retry, and progress tracking&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Remote Access&lt;/strong&gt;: Securely access your desktop at home running the model from anywhere (utilizing Tailscale)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Audio Notebook&lt;/strong&gt;: An Audio Notebook mode, with a calendar-based view, full-text search, and LM Studio integration (chat about your notes with the AI)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;System Tray Control&lt;/strong&gt;: Quickly start/stop a recording, plus a lot of other controls, available via the system tray.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;ðŸ“Œ&lt;em&gt;Half an hour of audio transcribed in under a minute (RTX 3060)!&lt;/em&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;The seed of the project was my desire to quickly and reliably interface with AI chatbots using my voice. That was about a year ago. Though less prevalent back then, still plenty of AI services like GhatGPT offered voice transcription. However the issue is that, like every other AI-infused company, they &lt;em&gt;always&lt;/em&gt; do it shittily. Yes is works fine for 30s recordings, but what if I want to ramble on for 10 minutes? The AI is smart enough to decipher what I mean and I can speak to it like a smarter rubber ducky, helping me work through the problem.&lt;/p&gt; &lt;p&gt;Well, from my testing back then speak more than 5 minutes and they all start to crap out. And you feel doubly stupid because not only did you get your transcription but you also wasted 10 minutes talking to the wall.&lt;/p&gt; &lt;p&gt;Moreover, there's the privacy issue. They already collect a ton of text data, giving them my voice feels like too much.&lt;/p&gt; &lt;p&gt;So I first looking at any existing solutions, but couldn't find any decent option that could run locally. Then I came across &lt;a href="https://github.com/KoljaB/RealtimeSTT"&gt;RealtimeSTT&lt;/a&gt;, an extremely impressive and efficient Python project that offered real-time transcription. It's more of a library or framework with only sample implementations.&lt;/p&gt; &lt;p&gt;So I started building around that package, stripping it down to its barest of bones in order to understand how it works so that I could modify it. This whole project grew out of that idea.&lt;/p&gt; &lt;p&gt;I built this project to satisfy my needs. I thought about releasing it only when it was decent enough where someone who doesn't know anything about it can just download a thing and run it. That's why I chose to Dockerize the server portion of the code.&lt;/p&gt; &lt;p&gt;The project was originally written in pure Python. Essentially it's a fancy wrapper around &lt;code&gt;faster-whisper&lt;/code&gt;. At some point I implemented a &lt;em&gt;server-client&lt;/em&gt; architecture and added a notebook mode (think of it like calendar for your audio notes).&lt;/p&gt; &lt;p&gt;And recently I decided to upgrade the frontend UI from Python to React + Typescript. Built all in Google AI Studio - App Builder mode for free believe it or not. No need to shell out the big bucks for Lovable, daddy Google's got you covered.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Don't hesitate to contact me here or open an issue on GitHub for any technical issues or other ideas!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TwilightEncoder"&gt; /u/TwilightEncoder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gxbrs1rj2okg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9y6s8/transcriptionsuite_a_fully_local_private_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9y6s8/transcriptionsuite_a_fully_local_private_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T15:22:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9tdvr</id>
    <title>Kimi K2.5 better than Opus 4.6 on hallucination benchmark in pharmaceutical domain</title>
    <updated>2026-02-20T11:54:25+00:00</updated>
    <author>
      <name>/u/aiprod</name>
      <uri>https://old.reddit.com/user/aiprod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9tdvr/kimi_k25_better_than_opus_46_on_hallucination/"&gt; &lt;img alt="Kimi K2.5 better than Opus 4.6 on hallucination benchmark in pharmaceutical domain" src="https://preview.redd.it/c1z228f22nkg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57a4ecba13b26df8634c1b123271ef9c3a609c4f" title="Kimi K2.5 better than Opus 4.6 on hallucination benchmark in pharmaceutical domain" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know the benchmark is mostly commercial models but Kimi K2.5 was part of it and I was actually surprised how well it did against its commercial counterparts.&lt;/p&gt; &lt;p&gt;The benchmark test 7 recent models for hallucinations on a realistic use case and data from the pharmaceutical domain.&lt;/p&gt; &lt;p&gt;Surprisingly, Opus 4.6 has the highest hallucination rate.&lt;/p&gt; &lt;p&gt;I labeled a good chunk of the data and from my impressions, it just invented clinical protocols or tests that werenâ€™t in the source data (probably trying to be helpful).&lt;/p&gt; &lt;p&gt;Kimi K2.5 did much better (albeit still not great).&lt;/p&gt; &lt;p&gt;You can read the full benchmark here: &lt;a href="https://www.blueguardrails.com/en/blog/placebo-bench-an-llm-hallucination-benchmark-for-pharma"&gt;https://www.blueguardrails.com/en/blog/placebo-bench-an-llm-hallucination-benchmark-for-pharma&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Dataset is also available on hugging face.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aiprod"&gt; /u/aiprod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c1z228f22nkg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9tdvr/kimi_k25_better_than_opus_46_on_hallucination/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9tdvr/kimi_k25_better_than_opus_46_on_hallucination/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T11:54:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9zt8m</id>
    <title>The top 3 models on openrouter this week ( Chinese models are dominating!)</title>
    <updated>2026-02-20T16:21:50+00:00</updated>
    <author>
      <name>/u/keb_37</name>
      <uri>https://old.reddit.com/user/keb_37</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9zt8m/the_top_3_models_on_openrouter_this_week_chinese/"&gt; &lt;img alt="The top 3 models on openrouter this week ( Chinese models are dominating!)" src="https://preview.redd.it/h4l8zr4rdokg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1cb3201433eea5c7cd862fbc8c0f259e4e6b134" title="The top 3 models on openrouter this week ( Chinese models are dominating!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the first time i see a model exceed 3 trillion tokens per week on openrouter!&lt;/p&gt; &lt;p&gt;the first time i see more than one model exceed a trillion token per week ( it was only grok 4 fast month ago)&lt;/p&gt; &lt;p&gt;the first time i see chinese models destroying US ones like this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/keb_37"&gt; /u/keb_37 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h4l8zr4rdokg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9zt8m/the_top_3_models_on_openrouter_this_week_chinese/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9zt8m/the_top_3_models_on_openrouter_this_week_chinese/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T16:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9wvg4</id>
    <title>GGML and llama.cpp join HF to ensure the long-term progress of Local AI</title>
    <updated>2026-02-20T14:31:22+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wvg4/ggml_and_llamacpp_join_hf_to_ensure_the_longterm/"&gt; &lt;img alt="GGML and llama.cpp join HF to ensure the long-term progress of Local AI" src="https://external-preview.redd.it/tLGg2WMvFn2R5w7Nf2m6oJPphAYJILLSWaWPLPoW8i4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6bb0cd5000a00c0e28c8ae17203068e5acfb352" title="GGML and llama.cpp join HF to ensure the long-term progress of Local AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;article by Georgi Gerganov, Xuan-Son Nguyen, Aleksander Grygier, Lysandre, Victor Mustar, Julien Chaumond&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/ggml-joins-hf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wvg4/ggml_and_llamacpp_join_hf_to_ensure_the_longterm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wvg4/ggml_and_llamacpp_join_hf_to_ensure_the_longterm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T14:31:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9qa7l</id>
    <title>Kimi has context window expansion ambitions</title>
    <updated>2026-02-20T08:54:10+00:00</updated>
    <author>
      <name>/u/omarous</name>
      <uri>https://old.reddit.com/user/omarous</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9qa7l/kimi_has_context_window_expansion_ambitions/"&gt; &lt;img alt="Kimi has context window expansion ambitions" src="https://preview.redd.it/3cvl2bdh5mkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e22f6604997ccccf6f6215ae239ab8f8b1dd09c3" title="Kimi has context window expansion ambitions" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omarous"&gt; /u/omarous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3cvl2bdh5mkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9qa7l/kimi_has_context_window_expansion_ambitions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9qa7l/kimi_has_context_window_expansion_ambitions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T08:54:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ra3erl</id>
    <title>16.000 tokens/second - Taalas: LLMs baked into hardware. No HBM, weights and model architecture in silicon</title>
    <updated>2026-02-20T18:31:56+00:00</updated>
    <author>
      <name>/u/CharacterAd9057</name>
      <uri>https://old.reddit.com/user/CharacterAd9057</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ra3erl/16000_tokenssecond_taalas_llms_baked_into/"&gt; &lt;img alt="16.000 tokens/second - Taalas: LLMs baked into hardware. No HBM, weights and model architecture in silicon" src="https://preview.redd.it/3ivt7c1h0pkg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e7595a8007c52e29510def46505ff98750ab5d0" title="16.000 tokens/second - Taalas: LLMs baked into hardware. No HBM, weights and model architecture in silicon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever experienced 16K tokens per second? It's insanely instant. Try their Lllama 3.1 8B demo here: &lt;a href="https://chatjimmy.ai/"&gt;chat jimmy&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;They have a very radical approach to solve the compute problem - albeit a risky one in a landscape where model architectures evolve in weeks instead of years: Etch the model and all the weights onto a single silicon chip.&lt;br /&gt; Normally that would take ages, but they seem to have found a way to go from model to ASIC in 60 days - which might make their approach appealing for domains where raw intelligence is not so much of importance, but latency is super important, like real-time speech models, real-time avatar generation, computer vision etc.&lt;/p&gt; &lt;p&gt;Here are their claims:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;&amp;lt; 1 Millisecond Latency&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;gt; 17k Tokens per Second per User&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;20x Cheaper to Produce&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;10x More Power Efficient&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;60 Days from Unseen Software to Custom Silicon:&lt;/strong&gt; This part is crazyâ€”it normally takes months...&lt;/li&gt; &lt;li&gt;&lt;strong&gt;0% Exotic Hardware Required, thus cheap&lt;/strong&gt;: They ditch HBM, advanced packaging, 3D stacking, liquid cooling, high speed IO - because they put everything into one chip to achieve ultimate simplicity.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoRA Support:&lt;/strong&gt; Despite the model being &amp;quot;baked&amp;quot; in silicon, you can adapt it constrained to the arch and param count. Their demonstrator uses Lllama 3.1 8B, but supports LoRa fine-tuning.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Just 24 Engineers and $30M&lt;/strong&gt;: That's what they spent on the first demonstrator.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bigger Reasoning Model Coming this Spring&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontier LLM Coming this Winter&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now that's for their claims taken from their website: &lt;a href="https://taalas.com/the-path-to-ubiquitous-ai/"&gt;The path to ubiquitous AI | Taalas&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Original Post : &lt;a href="https://www.reddit.com/r/singularity/comments/1r9frzk/taalas_llms_baked_into_hardware_no_hbm_weights/"&gt;https://www.reddit.com/r/singularity/comments/1r9frzk/taalas_llms_baked_into_hardware_no_hbm_weights/&lt;/a&gt; , can't cross post.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CharacterAd9057"&gt; /u/CharacterAd9057 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3ivt7c1h0pkg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ra3erl/16000_tokenssecond_taalas_llms_baked_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ra3erl/16000_tokenssecond_taalas_llms_baked_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T18:31:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9vywq</id>
    <title>GGML.AI has got acquired by Huggingface</title>
    <updated>2026-02-20T13:54:26+00:00</updated>
    <author>
      <name>/u/Time_Reaper</name>
      <uri>https://old.reddit.com/user/Time_Reaper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vywq/ggmlai_has_got_acquired_by_huggingface/"&gt; &lt;img alt="GGML.AI has got acquired by Huggingface" src="https://external-preview.redd.it/l687iazpdDZhrDlIbQBxf8OTcfiJg6WGdsBpv03NqVo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9e45ab199a5cdbdf8c5eb1968743c094b946e98" title="GGML.AI has got acquired by Huggingface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Time_Reaper"&gt; /u/Time_Reaper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/19759"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vywq/ggmlai_has_got_acquired_by_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vywq/ggmlai_has_got_acquired_by_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:54:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9uuc6</id>
    <title>Deepseek and Gemma ??</title>
    <updated>2026-02-20T13:05:36+00:00</updated>
    <author>
      <name>/u/ZeusZCC</name>
      <uri>https://old.reddit.com/user/ZeusZCC</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uuc6/deepseek_and_gemma/"&gt; &lt;img alt="Deepseek and Gemma ??" src="https://preview.redd.it/84ph0pirenkg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d2b363b1900aae44bcfc12c0eeb9d8e2caa7d08" title="Deepseek and Gemma ??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZeusZCC"&gt; /u/ZeusZCC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/84ph0pirenkg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uuc6/deepseek_and_gemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uuc6/deepseek_and_gemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:05:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60qu9</id>
    <title>AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)</title>
    <updated>2026-02-16T05:11:16+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" src="https://preview.redd.it/u11uh8jfisjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afc30b2b6ae673f2e940109e2001bb498bd818ad" title="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ðŸ‘‹&lt;/p&gt; &lt;p&gt;We're excited for Thursday's guests: &lt;strong&gt;The StepFun Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Thursday, Feb. 19th, 8 AMâ€“11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;âš ï¸ &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please donâ€™t post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u11uh8jfisjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T05:11:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
