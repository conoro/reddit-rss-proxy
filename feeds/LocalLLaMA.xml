<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-16T10:07:20+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o7v8bi</id>
    <title>This is how I track usage and improve my AI assistant without exposing sensitive data</title>
    <updated>2025-10-16T03:01:05+00:00</updated>
    <author>
      <name>/u/opensourcecolumbus</name>
      <uri>https://old.reddit.com/user/opensourcecolumbus</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7v8bi/this_is_how_i_track_usage_and_improve_my_ai/"&gt; &lt;img alt="This is how I track usage and improve my AI assistant without exposing sensitive data" src="https://external-preview.redd.it/gaIvmxSSr1iv7t7zNtM70CmHx8jl4ZdjsE3bg8Z2WCw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72b0f5f01a0afd4937a40ceb983ac6e548166416" title="This is how I track usage and improve my AI assistant without exposing sensitive data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The learning, sample schema/dashboard/sql, and the overall approach below. AMA and share your learning. Coming from a data engineering background, I want to share something I recently did and feel proud of. And I'm sure many of us will find this practice of privacy-first tracking useful in building better AI assistants/copilots/agents faster.&lt;/p&gt; &lt;p&gt;As I stepped into Engineering Manager role (a transition from all day of developing/hacking/analyzing/cleaning data pipelines to limited time of doing that and more time on connecting engineering efforts to business output), it became my duty to prove ROI of the engineering efforts I and my team puts in. I realized the importance of tracking key metrics for the project because&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;You can't improve what you don't measure&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;AI copilots and agents need a bit more love in this regard IMO. Instead of running in the never-ending loops to continue coding and postponing the public release to ship that additional improvement we might need (which is usually inspired from the gut-feel), a better approach is to ship early, start tracking usage, and take informed decisions on what you prioritize. Also I needed to measure ROI to get the needed resources and confidence from the business to continue investing more on that AI product/feature my team was building.&lt;/p&gt; &lt;p&gt;So this is what I ended up doing and learning&lt;/p&gt; &lt;h3&gt;Track from day 1&lt;/h3&gt; &lt;blockquote&gt; &lt;p&gt;Don't wait until things &amp;quot;settle down&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This will help you uncover real-world edge cases, weird behaviors, bottlenecks, who is more interested in this, which features get used more, etc. early in the development cycle. And this will help focus on the things that matter the most (as opposed to imaginary and not-so-important issues that we usually end up working on when we don't track). Do this on day 1, things never settle down, and the analytics instrumentation is pushed to another date.&lt;/p&gt; &lt;p&gt;I follow this approach for all my projects&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Collect the minimal real-time events data from clients (web app, mobile app, etc.)&lt;/li&gt; &lt;li&gt;Store the events data in a central warehouse e.g. Postgres, BigQuery, Snowflake, etc. (the single source of truth) &lt;/li&gt; &lt;li&gt;Transform the event data for downstream analytics tools (remove PII)&lt;/li&gt; &lt;li&gt;Route the transformed data to downstream tools for analysis e.g. Mixpanel, Power BI, Google Data Studio, etc.&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Standardize the tracking schema&lt;/h3&gt; &lt;p&gt;Don't reinvent the wheel in each project, save time and energy with the standardized tracking schema for tracking events. These are the key events and their properties that I track&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;&lt;strong&gt;Event Name&lt;/strong&gt;&lt;/th&gt; &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; &lt;th&gt;&lt;strong&gt;Key Properties&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;ai_user_prompt_created&lt;/code&gt;&lt;/td&gt; &lt;td&gt;Tracks when a user submits a prompt to your AI system&lt;/td&gt; &lt;td&gt;&lt;code&gt;prompt_text&lt;/code&gt;, &lt;code&gt;timestamp&lt;/code&gt;, &lt;code&gt;user_id&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;ai_llm_response_received&lt;/code&gt;&lt;/td&gt; &lt;td&gt;Captures AI system responses and performance metrics&lt;/td&gt; &lt;td&gt;&lt;code&gt;response_text&lt;/code&gt;, &lt;code&gt;response_time&lt;/code&gt;, &lt;code&gt;model_version&lt;/code&gt;, &lt;code&gt;user_id&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;ai_user_action&lt;/code&gt;&lt;/td&gt; &lt;td&gt;Measures user interactions with AI responses&lt;/td&gt; &lt;td&gt;&lt;code&gt;action_type&lt;/code&gt;, &lt;code&gt;timestamp&lt;/code&gt;, &lt;code&gt;user_id&lt;/code&gt;, &lt;code&gt;response_id&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I track following metrics primarily&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Engagement metrics&lt;/li&gt; &lt;li&gt;Latency and cost&lt;/li&gt; &lt;li&gt;Ratings and feedback&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can find the &lt;a href="https://www.rudderstack.com/blog/ai-product-analytics-privacy/"&gt;&lt;strong&gt;SQL queries for these metrics here&lt;/strong&gt;&lt;/a&gt; and a &lt;a href="https://claude.ai/public/artifacts/b2a5c6bc-3e6a-4e94-af20-8b322abe3624?fullscreen=true"&gt;&lt;strong&gt;sample dashboard here&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Deal with privacy challenges with LLM-powered intent-classification&lt;/h3&gt; &lt;p&gt;AI assistants contain prompts which has a lots of PII and we do need to send the tracking data to downstream tools (e.g. mixpanel, power BI, etc.) for different kinds of analysis such as user behavior, conversion, ROI, engineering metrics, etc. Sending PII to these downstream tools is not only a privacy nightmare on pricinples but it also creates a regulatory challenge for businesses.&lt;/p&gt; &lt;p&gt;So, in order to avoid sending this PII to these downstream tools, I used LLM to classify intent from the prompt, and replaced prompt with that intent category, good enough for the analytics I need and does not expose my customer's sensitive data with these downstream tools.&lt;/p&gt; &lt;p&gt;Here's the sample code to do this in JavaScript&lt;/p&gt; &lt;p&gt;``` function shouldClassifyIntent(event, metadata) { &lt;em&gt;// Always classify for high-value customers&lt;/em&gt; if (fetchUserProfile().plan === 'enterprise') { return true; }&lt;/p&gt; &lt;p&gt;&lt;em&gt;// Classify all events for new users (first 7 days)&lt;/em&gt; const daysSinceSignup = (Date.now() - fetchUserProfile()?.created_at) / (1000 * 60 * 60 * 24); if (daysSinceSignup &amp;lt;= 7) { return true; }&lt;/p&gt; &lt;p&gt;&lt;em&gt;// Sample 10% of other users based on consistent hash&lt;/em&gt; const userIdHash = simpleHash(event.userId); if (userIdHash % 100 &amp;lt; 10) { return true; }&lt;/p&gt; &lt;p&gt;&lt;em&gt;// Skip classification for this event&lt;/em&gt; return false; }&lt;/p&gt; &lt;p&gt;&lt;em&gt;// In your transformation&lt;/em&gt; export async function transformEvent(event, metadata) { if (event.event !== 'ai_user_prompt_created') { return event; }&lt;/p&gt; &lt;p&gt;&lt;em&gt;// Add sampling decision to event for analysis&lt;/em&gt; event.properties.intent_sampled = shouldClassifyIntent(event, metadata);&lt;/p&gt; &lt;p&gt;if (!event.properties.intent_sampled) { event.properties.classified_intent = 'not_sampled'; return event; }&lt;/p&gt; &lt;p&gt;&lt;em&gt;// Continue with classification...&lt;/em&gt; } ```&lt;/p&gt; &lt;p&gt;Keeping this post concise, I'd leave other details for now. Ask me and I will answer your curiosity. Let's take this discussion one step further by sharing your experience in measuring your AI agent/copilot usage. What metrics do you track, how do you keep it quick to instrument analytics, do you go beyond what basic analytics agent frameworks and observability tools provide, do you think about privacy when implementing analytics, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/opensourcecolumbus"&gt; /u/opensourcecolumbus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.rudderstack.com/blog/ai-product-analytics-privacy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7v8bi/this_is_how_i_track_usage_and_improve_my_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7v8bi/this_is_how_i_track_usage_and_improve_my_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T03:01:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7l1io</id>
    <title>LM Studio and VL models</title>
    <updated>2025-10-15T19:43:32+00:00</updated>
    <author>
      <name>/u/egomarker</name>
      <uri>https://old.reddit.com/user/egomarker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;LM Studio currently downsizes images for VL inference, which can significantly hurt OCR performance. &lt;/p&gt; &lt;p&gt;v0.3.6 release notes: &lt;strong&gt;&amp;quot;Added image auto-resizing for vision model inputs, hardcoded to 500px width while keeping the aspect ratio.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmstudio.ai/blog/lmstudio-v0.3.6"&gt;https://lmstudio.ai/blog/lmstudio-v0.3.6&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Related GitHub reports:&lt;br /&gt; &lt;a href="https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/941"&gt;https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/941&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/880"&gt;https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/880&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/967"&gt;https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/967&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/990"&gt;https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/990&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If your image is a dense page of text and the VL model seems to underperform, LM Studio preprocessing is likely the culprit. Consider using a different app.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/egomarker"&gt; /u/egomarker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7l1io/lm_studio_and_vl_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7l1io/lm_studio_and_vl_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7l1io/lm_studio_and_vl_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T19:43:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7b1i3</id>
    <title>Looks like the DGX Spark a bad 4K investment vs Mac</title>
    <updated>2025-10-15T13:29:44+00:00</updated>
    <author>
      <name>/u/meshreplacer</name>
      <uri>https://old.reddit.com/user/meshreplacer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b1i3/looks_like_the_dgx_spark_a_bad_4k_investment_vs/"&gt; &lt;img alt="Looks like the DGX Spark a bad 4K investment vs Mac" src="https://b.thumbs.redditmedia.com/pPegjJ4GiV-jDUO1MueuQ5ieZJJkj24-yiTMqhNj4TQ.jpg" title="Looks like the DGX Spark a bad 4K investment vs Mac" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/om6zy3z42avf1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=31dff7de8ac355eff8c2962f8f03084cec0ada0c"&gt;https://preview.redd.it/om6zy3z42avf1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=31dff7de8ac355eff8c2962f8f03084cec0ada0c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looks like 4K gets you a slower more expensive product limited In what you can do. I could just imagine how bad it would compare to an M4 128gb Mac Studio. Day late dollar short.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/meshreplacer"&gt; /u/meshreplacer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b1i3/looks_like_the_dgx_spark_a_bad_4k_investment_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b1i3/looks_like_the_dgx_spark_a_bad_4k_investment_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b1i3/looks_like_the_dgx_spark_a_bad_4k_investment_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T13:29:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1o80ack</id>
    <title>How can I implement key frame selection using object detection and tracking for a Vision-Language Model on an edge device?</title>
    <updated>2025-10-16T08:02:26+00:00</updated>
    <author>
      <name>/u/Wraithraisrr</name>
      <uri>https://old.reddit.com/user/Wraithraisrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; I'm currently working on a project that uses a Vision-Language Model (VLM) for risk monitoring on the edge. I want to integrate key frame selection to reduce redundant video frames before sending them to the VLM for reasoning.&lt;/p&gt; &lt;p&gt;My idea is to use object detection and object tracking to identify frames that contain significant changes or new objects (e.g., anomaly events).&lt;/p&gt; &lt;p&gt;I have a few questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;What would be the best approach or algorithm for key frame selection in this kind of setup? &lt;/li&gt; &lt;li&gt;Can object detection and tracking (e.g., YOLOv8, SORT, DeepSORT, ByteTrack) run efficiently on edge devices like a Raspberry Pi 5 with an AI HAT (26 TOPS)?&lt;/li&gt; &lt;li&gt;Are there any optimized lightweight models or frameworks you'd recommend for running this pipeline in real-time?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Any practical insights, papers, or example projects would be greatly appreciated. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wraithraisrr"&gt; /u/Wraithraisrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o80ack/how_can_i_implement_key_frame_selection_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o80ack/how_can_i_implement_key_frame_selection_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o80ack/how_can_i_implement_key_frame_selection_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T08:02:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7kkf0</id>
    <title>Poor GPU Club : 8GB VRAM - MOE models' t/s with llama.cpp</title>
    <updated>2025-10-15T19:25:03+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Continuation to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nyxmci/poor_gpu_club_8gb_vram_qwen330ba3b_gptoss20b_ts/"&gt;my previous thread&lt;/a&gt;. This time I got better pp numbers with tg because of additional parameters. Tried with latest llama.cpp. &lt;/p&gt; &lt;p&gt;&lt;sup&gt;My System Info: (&lt;/sup&gt;&lt;strong&gt;&lt;sup&gt;8GB VRAM &amp;amp; 32GB RAM&lt;/sup&gt;&lt;/strong&gt;)&lt;/p&gt; &lt;p&gt;&lt;sup&gt;Intel(R&lt;/sup&gt; Core(TM) i7-14700HX 2.10 GHz | 32 GB RAM | 64-bit OS, x64-based processor | NVIDIA GeForce RTX 4060 Laptop GPU |) &lt;strong&gt;&lt;sup&gt;Cores - 20 | Logical Processors - 28&lt;/sup&gt;&lt;/strong&gt;&lt;sup&gt;.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-UD-Q4_K_XL - 33 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Qwen3-30B-A3B-UD-Q4_K_XL.gguf -ngl 99 -ncmoe 29 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 160.45 ¬± 18.06 | | qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 33.73 ¬± 0.74 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;gpt-oss-20b-mxfp4 - 42 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\gpt-oss-20b-mxfp4.gguf -ngl 99 -ncmoe 10 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 823.93 ¬± 109.69 | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 42.06 ¬± 0.56 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Ling-lite-1.5-2507.i1-Q6_K - 34 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Ling-lite-1.5-2507.i1-Q6_K.gguf -ngl 99 -ncmoe 15 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | bailingmoe 16B Q6_K | 14.01 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 585.52 ¬± 18.03 | | bailingmoe 16B Q6_K | 14.01 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 34.38 ¬± 1.54 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Ling-lite-1.5-2507.i1-Q5_K_M - 50 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Ling-lite-1.5-2507.i1-Q5_K_M.gguf -ngl 99 -ncmoe 12 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | bailingmoe 16B Q5_K - Medium | 11.87 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 183.79 ¬± 16.55 | | bailingmoe 16B Q5_K - Medium | 11.87 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 50.03 ¬± 0.46 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Ling-Coder-lite.i1-Q6_K - 35 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Ling-Coder-lite.i1-Q6_K.gguf -ngl 99 -ncmoe 15 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | bailingmoe 16B Q6_K | 14.01 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 470.17 ¬± 113.93 | | bailingmoe 16B Q6_K | 14.01 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 35.05 ¬± 3.33 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Ling-Coder-lite.i1-Q5_K_M - 47 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Ling-Coder-lite.i1-Q5_K_M.gguf -ngl 99 -ncmoe 14 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | bailingmoe 16B Q5_K - Medium | 11.87 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 593.95 ¬± 91.55 | | bailingmoe 16B Q5_K - Medium | 11.87 GiB | 16.80 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 47.39 ¬± 0.68 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;SmallThinker-21B-A3B-Instruct-QAT.Q4_K_M - 34 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\SmallThinker-21B-A3B-Instruct-QAT.Q4_K_M.gguf -ngl 99 -ncmoe 27 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | smallthinker 20B Q4_K - Medium | 12.18 GiB | 21.51 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 512.92 ¬± 109.33 | | smallthinker 20B Q4_K - Medium | 12.18 GiB | 21.51 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 34.75 ¬± 0.22 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;SmallThinker-21BA3B-Instruct-IQ4_XS - 38 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\SmallThinker-21BA3B-Instruct-IQ4_XS.gguf -ngl 99 -ncmoe 25 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | smallthinker 20B IQ4_XS - 4.25 bpw | 10.78 GiB | 21.51 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 635.01 ¬± 105.46 | | smallthinker 20B IQ4_XS - 4.25 bpw | 10.78 GiB | 21.51 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 37.47 ¬± 0.37 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;ERNIE-4.5-21B-A3B-PT-UD-Q4_K_XL - 44 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\ERNIE-4.5-21B-A3B-PT-UD-Q4_K_XL.gguf -ngl 99 -ncmoe 14 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | ernie4_5-moe 21B.A3B Q4_K - Medium | 11.91 GiB | 21.83 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 568.99 ¬± 134.16 | | ernie4_5-moe 21B.A3B Q4_K - Medium | 11.91 GiB | 21.83 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 44.83 ¬± 1.72 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Phi-mini-MoE-instruct-Q8_0 - 65 t/s&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-bench -m E:\LLM\models\Phi-mini-MoE-instruct-Q8_0.gguf -ngl 99 -ncmoe 4 -fa 1 -ctk q8_0 -ctv q8_0 -b 2048 -ub 512 -t 8 | model | size | params | backend | ngl | threads | type_k | type_v | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ------: | -----: | -----: | -: | --------------: | -------------------: | | phimoe 16x3.8B Q8_0 | 7.58 GiB | 7.65 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | pp512 | 2570.72 ¬± 48.54 | | phimoe 16x3.8B Q8_0 | 7.58 GiB | 7.65 B | CUDA | 99 | 8 | q8_0 | q8_0 | 1 | tg128 | 65.41 ¬± 0.19 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I'll be updating this thread whenever I get optimization tips &amp;amp; tricks from others AND I'll be including additional results here with updated commands. Also whenever new MOE models get released. Currently I'm checking bunch more MOE models, I'll add those here in this week. Thanks&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Updates : To be updated&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;sup&gt;My Upcoming threads (Planned&lt;/sup&gt; :)&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;sup&gt;8GB VRAM - Dense models' t/s with llama.cpp&lt;/sup&gt;&lt;/li&gt; &lt;li&gt;&lt;sup&gt;8GB VRAM - MOE &amp;amp; Dense models' t/s with llama.cpp - CPU only&lt;/sup&gt;&lt;/li&gt; &lt;li&gt;&lt;sup&gt;8GB VRAM - MOE &amp;amp; Dense models' t/s with ik\&lt;/sup&gt;llama.cpp (Still I'm looking for help on ik_llama.cpp))&lt;/li&gt; &lt;li&gt;&lt;sup&gt;8GB VRAM - MOE &amp;amp; Dense models' t/s with ik\&lt;/sup&gt;llama.cpp - CPU only)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7kkf0/poor_gpu_club_8gb_vram_moe_models_ts_with_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7kkf0/poor_gpu_club_8gb_vram_moe_models_ts_with_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7kkf0/poor_gpu_club_8gb_vram_moe_models_ts_with_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T19:25:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7j6ri</id>
    <title>Microcenter has RTX3090Ti‚Äôs</title>
    <updated>2025-10-15T18:34:30+00:00</updated>
    <author>
      <name>/u/flanconleche</name>
      <uri>https://old.reddit.com/user/flanconleche</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7j6ri/microcenter_has_rtx3090tis/"&gt; &lt;img alt="Microcenter has RTX3090Ti‚Äôs" src="https://b.thumbs.redditmedia.com/Tx7CGhbxlRoqyuJQwDyZMIXkM4c-MnruHZbkvMAEyJk.jpg" title="Microcenter has RTX3090Ti‚Äôs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not sure if anyone cares but my local Microcenter has refurb RTX 3090Ti‚Äôs for $800. If your on the market for 3090‚Äôs it might be worth checking your local Microcenter. The used market prices have gone up to $900 and at Least you have some sort of warranty. &lt;/p&gt; &lt;p&gt;Also got a chance to play with the dgx spark, that thing is really cool. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/flanconleche"&gt; /u/flanconleche &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o7j6ri"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7j6ri/microcenter_has_rtx3090tis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7j6ri/microcenter_has_rtx3090tis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T18:34:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1o810id</id>
    <title>Anyone found a Open Source Voice Changer (not Voice Cloner like Vibevoice or Chatterbox) ?</title>
    <updated>2025-10-16T08:51:18+00:00</updated>
    <author>
      <name>/u/quantier</name>
      <uri>https://old.reddit.com/user/quantier</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Elevenlabs has a voice changer but I havent found anything open source where I can record myself and then just changed that piece to a new voice. &lt;/p&gt; &lt;p&gt;What I am after is emotion and rhythm - It takes a long time to create dialogue with the TTS models including voice cloning and it might take me 5-20 tries before I am happy with one line of voice. Creating dialogue becomes very difficult- however with voice cloning (reference audio) and then adding a prerecorded voice recording and changing voice would be a game changer.&lt;/p&gt; &lt;p&gt;Has anyone ran into something that can do this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantier"&gt; /u/quantier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o810id/anyone_found_a_open_source_voice_changer_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o810id/anyone_found_a_open_source_voice_changer_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o810id/anyone_found_a_open_source_voice_changer_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T08:51:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7xwio</id>
    <title>Need advice on what to do with H200</title>
    <updated>2025-10-16T05:28:07+00:00</updated>
    <author>
      <name>/u/AggressiveMention359</name>
      <uri>https://old.reddit.com/user/AggressiveMention359</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey sub, this question is serious. I'm getting lucky to get free access to H200 that no one was using in my university. I've been learning AI Engineering and Machine Learning, but have never touched one of these. I'd really really love to make the most of it - and decided to post it here for advice.&lt;/p&gt; &lt;p&gt;What are some must-do things? Build Andrej Karpathy's nanoGPT? Try local models? &lt;/p&gt; &lt;p&gt;Any advice is appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AggressiveMention359"&gt; /u/AggressiveMention359 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7xwio/need_advice_on_what_to_do_with_h200/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7xwio/need_advice_on_what_to_do_with_h200/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7xwio/need_advice_on_what_to_do_with_h200/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T05:28:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7x6oi</id>
    <title>SillyTavern for Academic RAG or Alternatives for RAG GUI</title>
    <updated>2025-10-16T04:46:06+00:00</updated>
    <author>
      <name>/u/combrade</name>
      <uri>https://old.reddit.com/user/combrade</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm honestly kinda tempted with SillyTavern‚Äôs Lore and World features . It‚Äôs kinda like isolating an LLM with an advanced system prompt and persona . I sometimes have an issue with LLMs where they often refuse to report something that is ahead of their knowledge base such as ‚Äúwho is President‚Äù even if I give it several articles for RAG with the latest news(just an example not my use case). I feel like it‚Äôs Lorebook and World kinda can isolate and refine an LLM output to avoid that . &lt;/p&gt; &lt;p&gt;ST has the most advanced GUI I‚Äôve ever seen with all its neat features like Persona and World . &lt;/p&gt; &lt;p&gt;I‚Äôve been working on this project for my PhD building a RAG vector DB for this research question . I have a MCP tool Vector server running local that‚Äôs almost done . The final setup is just a front end so I can give a demo to my department. In the backend , I‚Äôll be using MLflow for reporting the RAG metrics we need .&lt;/p&gt; &lt;p&gt;OpenWebUI is kinda 50-60% there , it was a little annoying setting up the MCP but it works and might require a slightly more powerful Cloud Instance for more users in the future . I‚Äôve been going through SillyTavern‚Äôs custom features and it seems really advanced the way you can customize things . &lt;/p&gt; &lt;p&gt;Please be upfront and tell me if this a badshit idea that will have my department head requesting my API logs (Just kidding about this ). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/combrade"&gt; /u/combrade &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x6oi/sillytavern_for_academic_rag_or_alternatives_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x6oi/sillytavern_for_academic_rag_or_alternatives_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x6oi/sillytavern_for_academic_rag_or_alternatives_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T04:46:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7liam</id>
    <title>Llamacpp Model Loader GUI for noobs</title>
    <updated>2025-10-15T20:01:21+00:00</updated>
    <author>
      <name>/u/CabinetNational3461</name>
      <uri>https://old.reddit.com/user/CabinetNational3461</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7liam/llamacpp_model_loader_gui_for_noobs/"&gt; &lt;img alt="Llamacpp Model Loader GUI for noobs" src="https://preview.redd.it/msr7wyiwxbvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0cf3cb84527273f0b3b22fdcb8c887bfed231273" title="Llamacpp Model Loader GUI for noobs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I a noob at this LLM stuff and recently switched from LM Studio/Ollama to llamacpp and loving it so far as far as speed/performance. One thing I dislike is how tedious it is to modify and play around with the parameters and using command line so I vibe coded some python code using Gemini 2.5 Pro for something easier to mess around with. I attached the code, sample model files and commands. I am using window 10 FYI. I had Gemini gen up some doc as am not much of a writer so here it is:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Introduction&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Llama.cpp Model Launcher is a powerful desktop GUI that transforms the complex llama-server.exe command line into an intuitive, point-and-click experience. Effortlessly launch models, dynamically edit every parameter in a visual editor, and manage a complete library of your model configurations. Designed for both beginners and power users, it provides a centralized dashboard to streamline your workflow and unlock the full potential of Llama.cpp without ever touching a terminal.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Intuitive Graphical Control:&lt;/strong&gt; Ditch the terminal. Launch, manage, and shut down the llama-server with simple, reliable button clicks, eliminating the risk of command-line typos.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamic Parameter Editor:&lt;/strong&gt; Visually build and modify launch commands in real-time. Adjust values in text fields, toggle flags with checkboxes, and add new parameters on the fly without memorizing syntax.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full Configuration Management:&lt;/strong&gt; Build and maintain a complete library of your models. Effortlessly add new profiles, edit names and parameters, and delete old configurations, all from within the application.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-Time Monitoring:&lt;/strong&gt; Instantly know the server's status with a colored indicator (Red, Yellow, Green) and watch the live output log to monitor model loading, API requests, and potential errors as they happen.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Integrated Documentation:&lt;/strong&gt; Access a complete Llama.cpp command reference and a formatted user guide directly within the interface, eliminating the need to search for external help.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. Running the Application&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There are two primary ways to run this application:&lt;/p&gt; &lt;p&gt;Method 1: Run from Python Source&lt;/p&gt; &lt;p&gt;This method is ideal for developers or users who have Python installed and are comfortable with a code editor.&lt;/p&gt; &lt;p&gt;Method 2: Compile to a Standalone Executable (.exe)&lt;/p&gt; &lt;p&gt;This method packages the application into a single `.exe` file that can be run on any Windows machine without needing Python installed.&lt;/p&gt; &lt;p&gt;code: &lt;a href="https://drive.google.com/file/d/1NWU1Kp_uVLmhErqgaSv5pGHwqy5BUUdp/view?usp=drive_link"&gt;https://drive.google.com/file/d/1NWU1Kp_uVLmhErqgaSv5pGHwqy5BUUdp/view?usp=drive_link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;help_file: &lt;a href="https://drive.google.com/file/d/1556aMxnNxoaZFzJyAw_ZDgfwkrkK7kTP/view?usp=drive_link"&gt;https://drive.google.com/file/d/1556aMxnNxoaZFzJyAw_ZDgfwkrkK7kTP/view?usp=drive_link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;sample_moldel_commands: &lt;a href="https://drive.google.com/file/d/1ksDD1wcEA27LCVqTOnQrzU9yZe1iWjd_/view?usp=drive_link"&gt;https://drive.google.com/file/d/1ksDD1wcEA27LCVqTOnQrzU9yZe1iWjd_/view?usp=drive_link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope someone find it useful&lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CabinetNational3461"&gt; /u/CabinetNational3461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/msr7wyiwxbvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7liam/llamacpp_model_loader_gui_for_noobs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7liam/llamacpp_model_loader_gui_for_noobs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T20:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7z3sn</id>
    <title>Use evaluations to find the best local model for your use case!</title>
    <updated>2025-10-16T06:43:30+00:00</updated>
    <author>
      <name>/u/evalProtocol</name>
      <uri>https://old.reddit.com/user/evalProtocol</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7z3sn/use_evaluations_to_find_the_best_local_model_for/"&gt; &lt;img alt="Use evaluations to find the best local model for your use case!" src="https://external-preview.redd.it/5Yyi7FZfBglJXdTAq5ctyvLjdHtxUhbYkAAZztvAOSg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=555cf9a9171ccbc0dd2a187ee6851a61b8931671" title="Use evaluations to find the best local model for your use case!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey I am Benny, I have been working on &lt;a href="http://evalprotocol.io"&gt;evalprotocol.io&lt;/a&gt; for a while now, and we recently published a post on using evaluations to pick the best local model to get your job done &lt;a href="https://fireworks.ai/blog/llm-judge-eval-protocol-ollama"&gt;https://fireworks.ai/blog/llm-judge-eval-protocol-ollama&lt;/a&gt; . The SDK is here &lt;a href="https://github.com/eval-protocol/python-sdk"&gt;https://github.com/eval-protocol/python-sdk&lt;/a&gt; , totally open source, and would love to figure out how to best work together with everyone. Please give it a try and let me know if you have any feedback!&lt;/p&gt; &lt;p&gt;(btw not familiar with the self promotion rule here, the SDK is totally open source, if this is not ok feel free to delete the post)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x5fupedf6fvf1.png?width=2454&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3087b9d6f9c43b534cb38ac8f513e5f66b4ea005"&gt;https://preview.redd.it/x5fupedf6fvf1.png?width=2454&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3087b9d6f9c43b534cb38ac8f513e5f66b4ea005&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/evalProtocol"&gt; /u/evalProtocol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7z3sn/use_evaluations_to_find_the_best_local_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7z3sn/use_evaluations_to_find_the_best_local_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7z3sn/use_evaluations_to_find_the_best_local_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T06:43:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1o81cpp</id>
    <title>Claude Haiku 4.5 vs. Chinese models: Is it better?</title>
    <updated>2025-10-16T09:13:54+00:00</updated>
    <author>
      <name>/u/hemokwang</name>
      <uri>https://old.reddit.com/user/hemokwang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed that Claude just released their latest model ‚Äî the Claude Haiku 4.5, which scored 73.3% on SWE-bench verified. This is really impressive, considering it‚Äôs the beginner model in the Claude series.&lt;/p&gt; &lt;p&gt;Claude has always been strong in coding, but its previous models have also been quite expensive. Now they have Haiku 4.5 ‚Äî a cheaper option that still delivers solid performance. Models like Qwen 3 Coder, GLM 4.6, KIMI K2, and DeepSeek V3.2 are undoubtedly its main competitors.&lt;/p&gt; &lt;p&gt;I haven‚Äôt had the chance to try Haiku 4.5 in depth yet. For those who have, what‚Äôs your first impression? How does it perform compared to these cheap Chinese models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hemokwang"&gt; /u/hemokwang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o81cpp/claude_haiku_45_vs_chinese_models_is_it_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o81cpp/claude_haiku_45_vs_chinese_models_is_it_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o81cpp/claude_haiku_45_vs_chinese_models_is_it_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T09:13:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7x25o</id>
    <title>Ollama v0.12.6 finally includes Vulkan support</title>
    <updated>2025-10-16T04:39:06+00:00</updated>
    <author>
      <name>/u/geerlingguy</name>
      <uri>https://old.reddit.com/user/geerlingguy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x25o/ollama_v0126_finally_includes_vulkan_support/"&gt; &lt;img alt="Ollama v0.12.6 finally includes Vulkan support" src="https://external-preview.redd.it/cjCC50drSVvsSjC6BG0LlHPfYq9pihhGvZz2PN90ZFQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c878fa25eb39194203fe7a493bd33e19b95d026c" title="Ollama v0.12.6 finally includes Vulkan support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geerlingguy"&gt; /u/geerlingguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.12.6-rc0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x25o/ollama_v0126_finally_includes_vulkan_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x25o/ollama_v0126_finally_includes_vulkan_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T04:39:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7miyx</id>
    <title>Just ordered new 3090 TI from MicroCenter ü§î</title>
    <updated>2025-10-15T20:39:56+00:00</updated>
    <author>
      <name>/u/GravyPoo</name>
      <uri>https://old.reddit.com/user/GravyPoo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7miyx/just_ordered_new_3090_ti_from_microcenter/"&gt; &lt;img alt="Just ordered new 3090 TI from MicroCenter ü§î" src="https://preview.redd.it/mzozs3957cvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb5a83e22f624acd437f0414ec334d5a460f063d" title="Just ordered new 3090 TI from MicroCenter ü§î" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GravyPoo"&gt; /u/GravyPoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mzozs3957cvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7miyx/just_ordered_new_3090_ti_from_microcenter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7miyx/just_ordered_new_3090_ti_from_microcenter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T20:39:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7rchv</id>
    <title>LLama.cpp GPU Support on Android Device</title>
    <updated>2025-10-15T23:56:47+00:00</updated>
    <author>
      <name>/u/DarkEngine774</name>
      <uri>https://old.reddit.com/user/DarkEngine774</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7rchv/llamacpp_gpu_support_on_android_device/"&gt; &lt;img alt="LLama.cpp GPU Support on Android Device" src="https://a.thumbs.redditmedia.com/VeG6UZmL3mBW6TmARr_WVpKD3xQ0T0XIPkiAj730lQ8.jpg" title="LLama.cpp GPU Support on Android Device" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have figured out a way to Use Android - GPU for LLAMA.CPP&lt;br /&gt; I mean it is not what you would expect like boost in tk/s but it is good for background work mostly&lt;/p&gt; &lt;p&gt;and i didn't saw much of a difference in both GPU and CPU mode&lt;/p&gt; &lt;p&gt;i was using &lt;a href="https://huggingface.co/Menlo/Lucy-128k-gguf/tree/main"&gt;lucy-128k&lt;/a&gt; model, i mean i am also using k-v cache + state file saving so yaa that's all that i got&lt;br /&gt; love to hear more about it from you guys : )&lt;/p&gt; &lt;p&gt;here is the relevant post : &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o7p34f/for_those_building_llamacpp_for_android/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1o7p34f/for_those_building_llamacpp_for_android/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkEngine774"&gt; /u/DarkEngine774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o7rchv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7rchv/llamacpp_gpu_support_on_android_device/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7rchv/llamacpp_gpu_support_on_android_device/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T23:56:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7ep8a</id>
    <title>Apple M5 Officially Announced: is this a big deal?</title>
    <updated>2025-10-15T15:48:34+00:00</updated>
    <author>
      <name>/u/ontorealist</name>
      <uri>https://old.reddit.com/user/ontorealist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(&lt;em&gt;Edit: To be clear, only the *&lt;/em&gt;base** M5 has been announced. My question is primarily about whether M5 &lt;strong&gt;Pro&lt;/strong&gt; and higher-end M5 chips with more high bandwidth memory, etc. are more compelling compared to PC builds for inference given the confirmed specs for the base M5.*)&lt;/p&gt; &lt;p&gt;If I‚Äôm understanding correctly:&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;3.5x faster AI performance&lt;/strong&gt; compared to the M4 (though the exact neural engine improvements aren‚Äôt yet confirmed)&lt;br /&gt; ‚Ä¢ &lt;strong&gt;153 GB/s memory bandwidth&lt;/strong&gt; (~30% improvement)&lt;br /&gt; ‚Ä¢ &lt;strong&gt;4x increase in GPU compute&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ &lt;strong&gt;Unified memory architecture&lt;/strong&gt;, eliminating the need for CPU‚ÜîGPU data transfers, as with previous gens&lt;/p&gt; &lt;p&gt;Even if the neural accelerators on the base M5 aren‚Äôt dedicated matmul units (which seems unlikely given the A19 Pro), will this translate into noticeably faster prompt processing speeds?&lt;/p&gt; &lt;p&gt;At $1,600 for an entry-level 16GB M5 ($2K for 32GB), serious inference workloads feels limiting, especially when compared to refurbished M-series models with more RAM. That said, it seems like a solid choice for new users exploring local AI experiences, particularly when working with sub-30B models for RAG or large context windows at faster speeds. That, along with another LM Studio feature in the press release, is a good sign, no? &lt;/p&gt; &lt;p&gt;Do the specs / pricing represent a meaningful upgrade for anyone considering the M5 Pro, Max, or Ultra? I‚Äôd love to hear others‚Äô thoughts.&lt;/p&gt; &lt;p&gt;Read the announcement &lt;a href="https://www.apple.com/newsroom/2025/10/apple-unleashes-m5-the-next-big-leap-in-ai-performance-for-apple-silicon/"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ontorealist"&gt; /u/ontorealist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7ep8a/apple_m5_officially_announced_is_this_a_big_deal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7ep8a/apple_m5_officially_announced_is_this_a_big_deal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7ep8a/apple_m5_officially_announced_is_this_a_big_deal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T15:48:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7pe3g</id>
    <title>Matthew McConaughey LLaMa</title>
    <updated>2025-10-15T22:34:38+00:00</updated>
    <author>
      <name>/u/ContextualNina</name>
      <uri>https://old.reddit.com/user/ContextualNina</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We thought it would be fun to build something for Matthew McConaughey, based on his recent Rogan podcast interview.&lt;/p&gt; &lt;p&gt;&amp;quot;Matthew McConaughey says he wants a private LLM, fed only with his books, notes, journals, and aspirations, so he can ask it questions and get answers based solely on that information, without any outside influence.&amp;quot;&lt;/p&gt; &lt;p&gt;Pretty classic RAG/context engineering challenge, right? And we use a fine-tuned Llama model in this setup, which also happens to be the most factual and grounded LLM according to the FACTS benchmark (link in comment), Llama-3-Glm-V2. &lt;/p&gt; &lt;p&gt;Here's how we built it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;We found public writings, podcast transcripts, etc, as our base materials to upload as a proxy for the all the information Matthew mentioned in his interview (of course our access to such documents is very limited compared to his).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The agent ingested those to use as a source of truth&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;We configured the agent to the specifications that Matthew asked for in his interview. Note that we already have the most grounded language model (GLM) as the generator, and multiple guardrails against hallucinations, but additional response qualities can be configured via prompt.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Now, when you converse with the agent, it knows to only pull from those sources instead of making things up or use its other training data.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;However, the model retains its overall knowledge of how the world works, and can reason about the responses, in addition to referencing uploaded information verbatim.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The agent is powered by Contextual AI's APIs, and we deployed the full web application on Vercel to create a publicly accessible demo.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ContextualNina"&gt; /u/ContextualNina &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.alrightalrightalright.ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe3g/matthew_mcconaughey_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe3g/matthew_mcconaughey_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T22:34:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7mhf5</id>
    <title>Google &amp; Yale release C2S Scale, a Gemma-based model for cell analysis</title>
    <updated>2025-10-15T20:38:17+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! This is Omar, from the Gemma team.&lt;/p&gt; &lt;p&gt;I'm super excited to share this research based on Gemma. Today, we're releasing a 27B model for single-cell analysis. This model generated hypotheses about how cancer cells behave, and we were able to confirm the predictions with experimental validation in living cells. This reveals a promising new pathway for developing therapies to fight cancer. &lt;/p&gt; &lt;p&gt;This applications of open models for medical use cases are super exciting for me. It's one of many examples of how open models can change the world&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B"&gt;https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2"&gt;https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/"&gt;https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7mhf5/google_yale_release_c2s_scale_a_gemmabased_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7mhf5/google_yale_release_c2s_scale_a_gemmabased_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7mhf5/google_yale_release_c2s_scale_a_gemmabased_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T20:38:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1o81rvs</id>
    <title>Google C2S-Scale 27B (based on Gemma) built with Yale generated a novel hypothesis about cancer cellular behavior - Model + resources are now on Hugging Face and GitHub</title>
    <updated>2025-10-16T09:41:04+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o81rvs/google_c2sscale_27b_based_on_gemma_built_with/"&gt; &lt;img alt="Google C2S-Scale 27B (based on Gemma) built with Yale generated a novel hypothesis about cancer cellular behavior - Model + resources are now on Hugging Face and GitHub" src="https://b.thumbs.redditmedia.com/h0BE1gNO8S-6xv6b1X5IIoHb8CSHWZoS7YxS0LFbuxA.jpg" title="Google C2S-Scale 27B (based on Gemma) built with Yale generated a novel hypothesis about cancer cellular behavior - Model + resources are now on Hugging Face and GitHub" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog post: How a Gemma model helped discover a new potential cancer therapy pathway - We‚Äôre launching a new 27 billion parameter foundation model for single-cell analysis built on the Gemma family of open models.: &lt;a href="https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/"&gt;https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/&lt;/a&gt;&lt;br /&gt; Hugging Face: &lt;a href="https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B"&gt;https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B&lt;/a&gt;&lt;br /&gt; Scientific preprint on bioRxiv: &lt;a href="https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2"&gt;https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2&lt;/a&gt;&lt;br /&gt; Code on GitHub: &lt;a href="https://github.com/vandijklab/cell2sentence"&gt;https://github.com/vandijklab/cell2sentence&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o81rvs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o81rvs/google_c2sscale_27b_based_on_gemma_built_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o81rvs/google_c2sscale_27b_based_on_gemma_built_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T09:41:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o808av</id>
    <title>I fine-tuned Qwen3-VL (4B &amp; 8B) on a free Colab instance using TRL (SFT and GRPO)!</title>
    <updated>2025-10-16T07:58:52+00:00</updated>
    <author>
      <name>/u/External-Rub5414</name>
      <uri>https://old.reddit.com/user/External-Rub5414</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've created a couple of notebook that work for free on Colab (T4 GPU) to fine-tune the new Qwen3-VL small and dense vision-language models (4B and 8B). Both the Instruct and Thinking variants are supported.&lt;/p&gt; &lt;p&gt;They use &lt;strong&gt;TRL&lt;/strong&gt;, which handles most of the training complexity so you can focus entirely on the specific task you want to fine-tune for.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;SFT&lt;/strong&gt; notebook: fine-tunes with a dataset to refine the model's response style: &lt;a href="https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_qwen_vl.ipynb"&gt;https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_qwen_vl.ipynb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GRPO&lt;/strong&gt; notebook: includes two reward functions to make the non-reasoning model learn to reason (&lt;a href="https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/grpo_qwen3_vl.ipynb"&gt;https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/grpo_qwen3_vl.ipynb&lt;/a&gt;): &lt;ol&gt; &lt;li&gt;A tag-based reward that checks for &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;answer&amp;gt;&lt;/code&gt; sections.&lt;/li&gt; &lt;li&gt;A length-based reward that discourages overthinking and checks correctness.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both notebooks can be run on a free Colab instance, but can also be scaled up for more advanced setups. The notebooks can also be accessed here: &lt;a href="https://github.com/huggingface/trl/tree/main/examples/notebooks"&gt;https://github.com/huggingface/trl/tree/main/examples/notebooks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feedback and experiments are welcome!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External-Rub5414"&gt; /u/External-Rub5414 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o808av/i_finetuned_qwen3vl_4b_8b_on_a_free_colab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o808av/i_finetuned_qwen3vl_4b_8b_on_a_free_colab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o808av/i_finetuned_qwen3vl_4b_8b_on_a_free_colab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T07:58:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1o75kkb</id>
    <title>AI has replaced programmers‚Ä¶ totally.</title>
    <updated>2025-10-15T08:37:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"&gt; &lt;img alt="AI has replaced programmers‚Ä¶ totally." src="https://preview.redd.it/bnnb2fb9m8vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1a55140b6915df726dfa4932943df64e43e7d94" title="AI has replaced programmers‚Ä¶ totally." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bnnb2fb9m8vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T08:37:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7x7ss</id>
    <title>GLM 4.5 Air AWQ 4bit on RTX Pro 6000 with vllm</title>
    <updated>2025-10-16T04:47:49+00:00</updated>
    <author>
      <name>/u/notaDestroyer</name>
      <uri>https://old.reddit.com/user/notaDestroyer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x7ss/glm_45_air_awq_4bit_on_rtx_pro_6000_with_vllm/"&gt; &lt;img alt="GLM 4.5 Air AWQ 4bit on RTX Pro 6000 with vllm" src="https://b.thumbs.redditmedia.com/vckUDC6hgDdCUoZhjQ0jn7hdsNydHHYam8wvadpbBSo.jpg" title="GLM 4.5 Air AWQ 4bit on RTX Pro 6000 with vllm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/g45oegzplevf1.png?width=5379&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44f1bf58f80336a1524c4fd128d5e07a6034f517"&gt;https://preview.redd.it/g45oegzplevf1.png?width=5379&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44f1bf58f80336a1524c4fd128d5e07a6034f517&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ran benchmark of cpatonn/GLM-4.5-Air-AWQ-4bit on a single Pro 6000 with vllm. Nvidia Driver Version: 580.95.05 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notaDestroyer"&gt; /u/notaDestroyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x7ss/glm_45_air_awq_4bit_on_rtx_pro_6000_with_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x7ss/glm_45_air_awq_4bit_on_rtx_pro_6000_with_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x7ss/glm_45_air_awq_4bit_on_rtx_pro_6000_with_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T04:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7b5i4</id>
    <title>Apple unveils M5</title>
    <updated>2025-10-15T13:34:26+00:00</updated>
    <author>
      <name>/u/Agreeable-Rest9162</name>
      <uri>https://old.reddit.com/user/Agreeable-Rest9162</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b5i4/apple_unveils_m5/"&gt; &lt;img alt="Apple unveils M5" src="https://preview.redd.it/5ehnojlm2avf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbc46c6e19f88c18588d2f5384d7fb2dd4717f50" title="Apple unveils M5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following the iPhone 17 AI accelerators, most of us were expecting the same tech to be added to M5. Here it is! Lets see what M5 Pro &amp;amp; Max will add. The speedup from M4 to M5 seems to be around 3.5x for prompt processing. &lt;/p&gt; &lt;p&gt;Faster SSDs &amp;amp; RAM:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Additionally, with up to 2x faster SSD performance than the prior generation, the new 14-inch MacBook Pro lets users load a local LLM faster, and they can now choose up to 4TB of storage. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;150GB/s of unified memory bandwidth&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agreeable-Rest9162"&gt; /u/Agreeable-Rest9162 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5ehnojlm2avf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b5i4/apple_unveils_m5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b5i4/apple_unveils_m5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T13:34:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7gpr8</id>
    <title>Got the DGX Spark - ask me anything</title>
    <updated>2025-10-15T17:02:50+00:00</updated>
    <author>
      <name>/u/sotech117</name>
      <uri>https://old.reddit.com/user/sotech117</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"&gt; &lt;img alt="Got the DGX Spark - ask me anything" src="https://preview.redd.it/9mr835ne4bvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42dc8e85dcff8b55d4174e98495bb8d2d144fd7d" title="Got the DGX Spark - ask me anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If there‚Äôs anything you want me to benchmark (or want to see in general), let me know, and I‚Äôll try to reply to your comment. I will be playing with this all night trying a ton of different models I‚Äôve always wanted to run. &lt;/p&gt; &lt;p&gt;(&amp;amp; shoutout to microcenter my goats!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sotech117"&gt; /u/sotech117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9mr835ne4bvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T17:02:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7pe1u</id>
    <title>gigaResearch</title>
    <updated>2025-10-15T22:34:35+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"&gt; &lt;img alt="gigaResearch" src="https://preview.redd.it/nb2hmgqircvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71c101f2683e8df117cbc2a9abd685bcac5cbce0" title="gigaResearch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nb2hmgqircvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T22:34:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
