<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-11T22:23:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1neelit</id>
    <title>KV cache f32 - Are there any benefits?</title>
    <updated>2025-09-11T17:14:40+00:00</updated>
    <author>
      <name>/u/Daniokenon</name>
      <uri>https://old.reddit.com/user/Daniokenon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The default value for the KV cache in llamacpp is f16. I've noticed that reducing the precision negatively affects the model's ability to remember facts, for example, in conversations or roleplay. Does increasing the precision to f32 have the opposite effect? ​​I recently tested Mistral 3.2 Q8 with a KV cache of f32 and I'm not sure. The model was obviously much slower, and it surprised me interestingly a few times (but whether that was due to f32 or just the random seed—I don't know).&lt;/p&gt; &lt;p&gt;I tried to find some tests, but I can't find anything meaningful. Does f32 positively affect the stability/size of the context window?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daniokenon"&gt; /u/Daniokenon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neelit/kv_cache_f32_are_there_any_benefits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neelit/kv_cache_f32_are_there_any_benefits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1neelit/kv_cache_f32_are_there_any_benefits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T17:14:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nej2e7</id>
    <title>RX9070 vs M4 pro 20core GPU speed comparison</title>
    <updated>2025-09-11T20:05:35+00:00</updated>
    <author>
      <name>/u/Only_Comfortable_224</name>
      <uri>https://old.reddit.com/user/Only_Comfortable_224</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just to share a datapoint, I tried open AI oss 20b q4 quantitization. 9070 can easily respond at 103~110 tps, while m4 pro is only 67tps. So 9070 (non-XT) is +50~60% faster!&lt;/p&gt; &lt;p&gt;Ofc that’s only when both models can be fully offloaded to GPU. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Only_Comfortable_224"&gt; /u/Only_Comfortable_224 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nej2e7/rx9070_vs_m4_pro_20core_gpu_speed_comparison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nej2e7/rx9070_vs_m4_pro_20core_gpu_speed_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nej2e7/rx9070_vs_m4_pro_20core_gpu_speed_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T20:05:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne0o5m</id>
    <title>top reads from last week</title>
    <updated>2025-09-11T05:35:19+00:00</updated>
    <author>
      <name>/u/External_Mushroom978</name>
      <uri>https://old.reddit.com/user/External_Mushroom978</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne0o5m/top_reads_from_last_week/"&gt; &lt;img alt="top reads from last week" src="https://preview.redd.it/kio1vvck2hof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=922bbaa42c50e0e800de692fba6af534988fd0b6" title="top reads from last week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mushroom978"&gt; /u/External_Mushroom978 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kio1vvck2hof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne0o5m/top_reads_from_last_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne0o5m/top_reads_from_last_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T05:35:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nejogz</id>
    <title>How do you actually test new local models for your own tasks?</title>
    <updated>2025-09-11T20:29:19+00:00</updated>
    <author>
      <name>/u/Fabulous_Pollution10</name>
      <uri>https://old.reddit.com/user/Fabulous_Pollution10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Beyond leaderboards and toy checks like “how many r’s in strawberries?”, how do you decide a model is worth switching to for your real workload?&lt;/p&gt; &lt;p&gt;Would love to see the practical setups, rules of thumb – that help you say this model is good.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Pollution10"&gt; /u/Fabulous_Pollution10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nejogz/how_do_you_actually_test_new_local_models_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nejogz/how_do_you_actually_test_new_local_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nejogz/how_do_you_actually_test_new_local_models_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T20:29:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne9zn5</id>
    <title>In need of real life community in the space</title>
    <updated>2025-09-11T14:16:51+00:00</updated>
    <author>
      <name>/u/SolidRemote8316</name>
      <uri>https://old.reddit.com/user/SolidRemote8316</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I went down the AI rabbit hole not too long ago and I must say it’s been quite exciting and challenging. I don’t have programming experience, so a lot of things I have explored have been more from a vibe coding standpoint, and I know some of my previous posts have received some pokes due to that. &lt;/p&gt; &lt;p&gt;Everyone brings a different lens and I’m not trying to reduce my inability to code. However, my biggest challenge is that in my circle of friends, I’m the most “advanced” and it sucks cos I know I don’t know a lot. I am using this post as a smoke signal to search for a mentor, peer or community that can help in this quest for knowledge and further understanding of this space. This sub is helpful, but it’s not the same as bouncing thoughts, ideas and all in real time. &lt;/p&gt; &lt;p&gt;When I started out, I bought the domain - &lt;a href="https://www.mindmeetsmodel.com"&gt;https://www.mindmeetsmodel.com&lt;/a&gt; with the goal of documenting my journey and being able to look back and point at what I was able to accomplish. The site was vibe coded by the way. &lt;/p&gt; &lt;p&gt;I hope someone who is willing to help a stranger stumbled on this post. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SolidRemote8316"&gt; /u/SolidRemote8316 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne9zn5/in_need_of_real_life_community_in_the_space/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne9zn5/in_need_of_real_life_community_in_the_space/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne9zn5/in_need_of_real_life_community_in_the_space/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T14:16:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndoxxa</id>
    <title>Why should I **not** buy an AMD AI Max+ 395 128GB right away ?</title>
    <updated>2025-09-10T20:10:55+00:00</updated>
    <author>
      <name>/u/StyMaar</name>
      <uri>https://old.reddit.com/user/StyMaar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the rise of medium-sized MoE (gpt-oss-120B, GLM-4.5-air, and now the incoming Qwen3-80B-A3B) and their excellent performance for local models (well at least for the two first), the relatively low compute and memory bandwidth of the Strix Halo doesn't sounds too much of a problem anymore (because of the low active parameters count) and the 128GB of VRAM for $2k is unbeatable.&lt;/p&gt; &lt;p&gt;So now I'm very tempted to buy one, but I'm also aware that I don't really &lt;em&gt;need&lt;/em&gt; one, so please give me arguments about why I should not buy it.&lt;/p&gt; &lt;p&gt;My wallet thanks you in advance.&lt;/p&gt; &lt;p&gt;Edit: thanks for your response. Unfortunately no one was really able to convinced me out of this purchase.&lt;/p&gt; &lt;p&gt;Now only my procrastination can save me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StyMaar"&gt; /u/StyMaar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndoxxa/why_should_i_not_buy_an_amd_ai_max_395_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndoxxa/why_should_i_not_buy_an_amd_ai_max_395_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndoxxa/why_should_i_not_buy_an_amd_ai_max_395_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T20:10:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1neh91h</id>
    <title>Python agent framework focused on library integration (not tools)</title>
    <updated>2025-09-11T18:55:27+00:00</updated>
    <author>
      <name>/u/Impressive-Glass-523</name>
      <uri>https://old.reddit.com/user/Impressive-Glass-523</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been exploring agentic architectures and felt that the tool-calling loop, while powerful, led to unnecessary abstraction between the libraries I wanted to use and the agent.&lt;/p&gt; &lt;p&gt;So, I've been building an open-source alternative called &lt;a href="https://ashenfad.github.io/agex/"&gt;agex&lt;/a&gt;. The core idea is to bypass the tool-layer and give agents direct, sandboxed access to Python libraries. The agent &amp;quot;thinks-in-code&amp;quot; and can compose functions, classes, and methods from the modules you give it.&lt;/p&gt; &lt;p&gt;The project is somewhere in-between toy and production-ready, but I'd love feedback from folks interested in kicking the tires. It's closest cousin is Huggingface's smol-agents, but again, with an emphasis on library integration.&lt;/p&gt; &lt;p&gt;Some links:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A &lt;a href="https://www.youtube.com/watch?v=-LaY_QBfkf8"&gt;video of a NiceGUI integration&lt;/a&gt; where the agent creates UI on the fly&lt;/li&gt; &lt;li&gt;A &lt;a href="https://ashenfad.github.io/agex/examples/routing/"&gt;notebook&lt;/a&gt; for routing via OSMnx &amp;amp; Folium&lt;/li&gt; &lt;li&gt;A &lt;a href="https://github.com/ashenfad/agex/blob/main/benchmarks/excel_analysis_bench.py"&gt;small benchmark&lt;/a&gt; testing the qwen3 family of models (qwen3-coder works well)&lt;/li&gt; &lt;li&gt;Project repo: &lt;a href="https://github.com/ashenfad/agex"&gt;https://github.com/ashenfad/agex&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Repo for NiceGUI demos: &lt;a href="https://github.com/ashenfad/agex-ui"&gt;https://github.com/ashenfad/agex-ui&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive-Glass-523"&gt; /u/Impressive-Glass-523 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neh91h/python_agent_framework_focused_on_library/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neh91h/python_agent_framework_focused_on_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1neh91h/python_agent_framework_focused_on_library/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T18:55:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndz1k4</id>
    <title>PNY preorder listing shows Nvidia DGX Spark at $4,299.99</title>
    <updated>2025-09-11T04:00:41+00:00</updated>
    <author>
      <name>/u/DeliciousBelt9520</name>
      <uri>https://old.reddit.com/user/DeliciousBelt9520</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;code&gt;PNY has opened preorders for the Nvidia DGX Spark, a compact desktop AI system powered by the Grace Blackwell GB10 Superchip. It combines Arm Cortex-X925 and Cortex-A725 CPU cores with a Blackwell GPU, delivering up to 1,000 AI TOPS, or 1 petaFLOP of FP4 performance, for local model inference and fine-tuning.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://linuxgizmos.com/pny-preorder-listing-shows-nvidia-dgx-spark-at-4299-99/"&gt;https://linuxgizmos.com/pny-preorder-listing-shows-nvidia-dgx-spark-at-4299-99/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeliciousBelt9520"&gt; /u/DeliciousBelt9520 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndz1k4/pny_preorder_listing_shows_nvidia_dgx_spark_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndz1k4/pny_preorder_listing_shows_nvidia_dgx_spark_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndz1k4/pny_preorder_listing_shows_nvidia_dgx_spark_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T04:00:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nej5b6</id>
    <title>Powering GPUs with an extra power supply</title>
    <updated>2025-09-11T20:08:48+00:00</updated>
    <author>
      <name>/u/DevestatingHemorhoid</name>
      <uri>https://old.reddit.com/user/DevestatingHemorhoid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got my hands on some additional V100s. Sadly the PSUs in my workstations cannot fully power more than one at the same time. Instead of running two full blown PC PSUs to power multiple GPUs in one workstation I thought why not buy some PCIe 6+2 cables and use one of my 12 V 600 W power supplies (grounded to the chassis so that it shares ground with the PC PSU) to supply the required ~200 W to each card (75 W come from the PC PSU via the PCI pins).&lt;/p&gt; &lt;p&gt;My question is: has anyone here tried something like this? I am a bit hesistant since I am unsure what kind of ripple/instability/voltage fluctuations the cards can handle and how the 12 V supply compares to the 12 V delivered by a &amp;quot;real&amp;quot; PC PSU. I can obviously add a capacitor in parallel to smooth things out, but I would have to know what kind of spikes, dips I have to filter out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DevestatingHemorhoid"&gt; /u/DevestatingHemorhoid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nej5b6/powering_gpus_with_an_extra_power_supply/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nej5b6/powering_gpus_with_an_extra_power_supply/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nej5b6/powering_gpus_with_an_extra_power_supply/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T20:08:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndxsja</id>
    <title>GPT-OSS 20b (high) consistently does FAR better than gpt5-thinking on my engineering Hw</title>
    <updated>2025-09-11T02:54:33+00:00</updated>
    <author>
      <name>/u/InevitableWay6104</name>
      <uri>https://old.reddit.com/user/InevitableWay6104</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just found this super interesting, but gpt-oss 20b gets almost every problem right, while gpt5-thinking, something I can only query like 5 times before getting rate limited (free tier), only gets it right about 50% of the time.&lt;/p&gt; &lt;p&gt;pretty interesting that a open weights 20b model is better than the closed flagship model on the free tier. I often use these models to verify my work, and both are free, but I can spam the 20b as much as I want and it's right more often.&lt;/p&gt; &lt;p&gt;granted, gpt5-thinking on the free tier is probably on the lowest setting, bc gpt-oss thinks ALOT longer than gpt5 did, on average it was about 20-30k tokens per question. &lt;/p&gt; &lt;p&gt;qwen3-30b-2507-thinking is also really good, but I don't think it's as good for this specific task, and gpt-oss is way smaller.&lt;/p&gt; &lt;p&gt;just still found it super interesting and wanted to share.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InevitableWay6104"&gt; /u/InevitableWay6104 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndxsja/gptoss_20b_high_consistently_does_far_better_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndxsja/gptoss_20b_high_consistently_does_far_better_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndxsja/gptoss_20b_high_consistently_does_far_better_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T02:54:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne4xok</id>
    <title>What would be the most budget-friendly PC to run LLMs larger than 72B?</title>
    <updated>2025-09-11T10:12:32+00:00</updated>
    <author>
      <name>/u/pitchblackfriday</name>
      <uri>https://old.reddit.com/user/pitchblackfriday</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was thinking, if a 5-year-old gaming laptop can run Qwen 3 30B A3B at a slow but functional speed, what about bigger MoE models?&lt;/p&gt; &lt;p&gt;Let's add some realistic expectations.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Serving 1~5 users only, without much concurrency.&lt;/li&gt; &lt;li&gt;Speed matters less, as long as it's &amp;quot;usable at least&amp;quot;. Parameter size and knowledge matter more.&lt;/li&gt; &lt;li&gt;Running MoE-based models only, like the upcoming Qwen 3 Next 80B A3B, to improve inference speed.&lt;/li&gt; &lt;li&gt;(optional) Utilizing APU and unified memory architecture for accommodating sufficient GPU offloading, and keeping the cost lower&lt;/li&gt; &lt;li&gt;Reasonable power consumption and supply for lower electricity bill.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What would be the lowest-cost and yet usable desktop build for running such LLMs locally? I'm just wondering about ideas and opinions for ordinary users, outside those first-world, upper-class, multi-thousand-dollars realm.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pitchblackfriday"&gt; /u/pitchblackfriday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne4xok/what_would_be_the_most_budgetfriendly_pc_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne4xok/what_would_be_the_most_budgetfriendly_pc_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne4xok/what_would_be_the_most_budgetfriendly_pc_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T10:12:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne7f0c</id>
    <title>Qwen Code CLI affected by the debug-js compromise</title>
    <updated>2025-09-11T12:25:10+00:00</updated>
    <author>
      <name>/u/mestar12345</name>
      <uri>https://old.reddit.com/user/mestar12345</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On 2025-09-08 the maintainer of some popular JS libraries was compromised, and new versions of some popular libraries were released with some crypto stealing code. qwen code cli was one of the programs that was updated since then, and windows defender will detect Malgent!MSR trojan in some JS libraries when you start qwen.&lt;/p&gt; &lt;p&gt;The payload was for the browser environment of javascript, and I don't know if there is any impact if you run the compromised code in the node.js context. Still, I hope this gets cleaned up soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mestar12345"&gt; /u/mestar12345 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne7f0c/qwen_code_cli_affected_by_the_debugjs_compromise/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne7f0c/qwen_code_cli_affected_by_the_debugjs_compromise/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne7f0c/qwen_code_cli_affected_by_the_debugjs_compromise/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T12:25:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1neb35p</id>
    <title>New VS Code release allows extensions to contribute language models to Chat</title>
    <updated>2025-09-11T14:58:58+00:00</updated>
    <author>
      <name>/u/isidor_n</name>
      <uri>https://old.reddit.com/user/isidor_n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neb35p/new_vs_code_release_allows_extensions_to/"&gt; &lt;img alt="New VS Code release allows extensions to contribute language models to Chat" src="https://external-preview.redd.it/ub1r8snDnE0gcJQ1X4KunDUU8G23q7XAyTmHzNrnIvQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8124d3dd9bed2000f7513f33ea18cc6440b75db" title="New VS Code release allows extensions to contribute language models to Chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Extensions can now contribute language models that are used in the Chat view. This is the first step (we have a bunch more work to do). But if you have any feedback let me know (vscode pm here).&lt;/p&gt; &lt;p&gt;Docs &lt;a href="https://code.visualstudio.com/api/extension-guides/ai/language-model-chat-provider"&gt;https://code.visualstudio.com/api/extension-guides/ai/language-model-chat-provider&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/isidor_n"&gt; /u/isidor_n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://code.visualstudio.com/updates/v1_104"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neb35p/new_vs_code_release_allows_extensions_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1neb35p/new_vs_code_release_allows_extensions_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T14:58:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne58kw</id>
    <title>Thinking Machines Lab dropped a new research: Defeating Nondeterminism in LLM Inference</title>
    <updated>2025-09-11T10:30:09+00:00</updated>
    <author>
      <name>/u/Snoo_64233</name>
      <uri>https://old.reddit.com/user/Snoo_64233</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR; LLM inference nondeterminism isn't just floating-point non-associativity or GPU concurrent execution, the core culprit is batching variance, where server load unpredictably alters numeric. Batch-invariant kernels unlock true reproducibility. Non-determinism is an issue in all sort of places, but non-determinism stemming from GPU kernels not being batch size invariant is pretty specific to machine learning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snoo_64233"&gt; /u/Snoo_64233 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne58kw/thinking_machines_lab_dropped_a_new_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne58kw/thinking_machines_lab_dropped_a_new_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T10:30:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne3v7m</id>
    <title>Celebrating 1 year anniversary of the revolutionary game changing LLM that was Reflection 70b</title>
    <updated>2025-09-11T09:03:14+00:00</updated>
    <author>
      <name>/u/LosEagle</name>
      <uri>https://old.reddit.com/user/LosEagle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is now a year since the release of Reflection-70B that genius inventor Matt Shumer marketted as state-of-the-art hallucination-free llm that outperforms both gpt-4o and claude 3.5 with its new way of thinking as well as world's top open-source model. &lt;/p&gt; &lt;p&gt;World hasn't been the same since then indeed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LosEagle"&gt; /u/LosEagle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne3v7m/celebrating_1_year_anniversary_of_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne3v7m/celebrating_1_year_anniversary_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne3v7m/celebrating_1_year_anniversary_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T09:03:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nekqze</id>
    <title>I Trained an AI to rewrite text like Nietzsche. Turned out pretty funny.</title>
    <updated>2025-09-11T21:11:17+00:00</updated>
    <author>
      <name>/u/Heralax_Tekran</name>
      <uri>https://old.reddit.com/user/Heralax_Tekran</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nekqze/i_trained_an_ai_to_rewrite_text_like_nietzsche/"&gt; &lt;img alt="I Trained an AI to rewrite text like Nietzsche. Turned out pretty funny." src="https://b.thumbs.redditmedia.com/kIPaQBUAf19y3HS0iiYuMDNinnPDdsEDJiWbucDMs-I.jpg" title="I Trained an AI to rewrite text like Nietzsche. Turned out pretty funny." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I like writing, and I like AI. But &lt;strong&gt;because of AI's writing style, I and many other people have been unwilling to use these text generators for our actual writing,&lt;/strong&gt; which is absurd. So today I'm open-sourcing &lt;strong&gt;a proof-of-concept LLM&lt;/strong&gt;, trained to write like a specific person from history — the German philosopher, &lt;strong&gt;Friedrich Nietzsche&lt;/strong&gt;!&lt;/p&gt; &lt;h1&gt;Model link: &lt;a href="https://huggingface.co/Heralax/RewriteLikeMe-FriedrichNietzsche"&gt;https://huggingface.co/Heralax/RewriteLikeMe-FriedrichNietzsche&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;(The model page includes the original LoRA, as well as the merged model files, and those same model files quantized to q8)&lt;/p&gt; &lt;h1&gt;Running it&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;You have options:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can take the normal-format LoRA files and run them as normal with your favorite inference backend. Base model == Mistral 7b v0.2. Running LoRAs is not as common as full models these days, so here are some instructions: &lt;ol&gt; &lt;li&gt;Download adapter_config, adapter_model, chat_template, config, any anything with &amp;quot;token&amp;quot; in the name&lt;/li&gt; &lt;li&gt;Put them all in the same directory&lt;/li&gt; &lt;li&gt;Download Mistral 7b v0.2 (.safetensors and its accompanying config files etc., not a quant like .gguf). Put all these in another dir.&lt;/li&gt; &lt;li&gt;Use inference software like the text-generation-webui and point it at that directory. It should know what to do. For instance, in textgenwebui/ooba you'll see a selector called &amp;quot;LoRA(s)&amp;quot; next to the model selector, to the right of the Save settings button. First pick the base model, then pick the LoRA to apply to it.&lt;/li&gt; &lt;li&gt;Alternatively, lora files can actually be quantized with llama.cpp -- see &lt;code&gt;convert_lora_to_gguf.py&lt;/code&gt;. The result + a quantized mistral 7b v0.2 can be run with koboldcpp easily enough.&lt;/li&gt; &lt;li&gt;If you want to use quantized LoRA files, which honestly is ideal because no one wants to run anything in f16, KoboldCPP supports this kind of inference. I have not found many others that do.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Alternatively, you can &lt;strong&gt;take the quantized full model files&lt;/strong&gt; (the base model with the LoRA merged onto it) and run them as you would any other local LLM. It's a q8 7b so it should be relatively easy to manage on most hardware.&lt;/li&gt; &lt;li&gt;Or take the merged model files still in .safetensors format, and prepare them in whatever format you like (e.g., exllama, gptq, or just leave them as is for inference and use with vLLM or something)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Since you have the model files in pretty much any format you can imagine, &lt;strong&gt;you can use all the wonderful tricks devised by the open source community&lt;/strong&gt; to make this thing &lt;strong&gt;ance the way you want it to!&lt;/strong&gt; Please let me know if you come across any awesome sampling parameter improvements actually, I haven't iterated too much there.&lt;/p&gt; &lt;p&gt;Anyway, by taking one of these routes you ought to be able to start rephrasing AI text to sound like Nietzsche! Since you have the original lora, &lt;strong&gt;you could possibly also do things like do additional training or merge with RP models,&lt;/strong&gt; which could, possibly (have not tried it) produce character-specific RP bots. Lots of exciting options!&lt;/p&gt; &lt;p&gt;Now for a brief moment I need to talk about the slightly-less-exciting subject of where things will break. This system ain't perfect yet.&lt;/p&gt; &lt;h1&gt;Rough Edges&lt;/h1&gt; &lt;p&gt;One of my goals was to be able to train this model, and future models like it, while using very little text from the original authors. Hunting down input data is annoying after all! I managed to achieve this, but the corners I cut are still a little rough:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Expect having to re-roll the occasional response when it goes off the rails. Because I trained on a very small amount of data that was remixed in a bunch of ways, some memorization crept in despite measures to the contrary.&lt;/li&gt; &lt;li&gt;This model can &lt;strong&gt;only rephrase AI-written text to sound like a person&lt;/strong&gt;. It cannot write the original draft of some text by itself yet. It is a rephraser, not a writer.&lt;/li&gt; &lt;li&gt;Finally, to solve the problem where the LLM might veer off topic if the thing it is rephrasing is too long, I recommend breaking longer texts up into chunks of smaller ones.&lt;/li&gt; &lt;li&gt;The model will be more adept at rephrasing text more or less in the same area as the original data was written in. This Nietzche model will therefore be more apt at rephrasing critical philosophically-oriented things than it would fiction, say. Feeding very out of domain things to the model will still probably work, it's just that the model has to guess a bit more, and therefore might sound less convincing.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Note: the prompt you &lt;strong&gt;must use&lt;/strong&gt;, and some good-ish sampling parameters, are provided as well. This model is very overfit on the specific system prompt so don't use a different one.&lt;/p&gt; &lt;p&gt;Also, there's a funny anecdote from training I want to share: hilariously, the initial training loss for certain people is MUCH higher than others. Friedrich Nietzsche's training run starts off like a good &lt;strong&gt;1.0 or 0.5 loss higher than someone like Paul Graham.&lt;/strong&gt; This is a significant increase! Which makes sense given his unique style.&lt;/p&gt; &lt;p&gt;I hope you find this proof of concept interesting, and possibly entertaining! I also hope that the model files are useful, and that they serve as good fodder for experiments if you do that sorta thing as well. The problem of awful LLM writing styles has had a lot of progress made on it over the years due to a lot of people here in this community, but the challenge of cloning specific styles is sometimes underappreciated and underserved. Especially since I need the AI to write like &lt;em&gt;me&lt;/em&gt; if I'm going to, say, use it to write work emails. This is meant as a first step in that direction.&lt;/p&gt; &lt;p&gt;In case you've had to scroll down a lot because of my rambling, here's the model link again&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Heralax/RewriteLikeMe-FriedrichNietzsche"&gt;https://huggingface.co/Heralax/RewriteLikeMe-FriedrichNietzsche&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thank you for your time, I hope you enjoy the model! Please consider checking it out on Hugging Face :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Heralax_Tekran"&gt; /u/Heralax_Tekran &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nekqze"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nekqze/i_trained_an_ai_to_rewrite_text_like_nietzsche/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nekqze/i_trained_an_ai_to_rewrite_text_like_nietzsche/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T21:11:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nefy3g</id>
    <title>Alibaba's homegrown chips are now competitive with Nvidia H20</title>
    <updated>2025-09-11T18:05:29+00:00</updated>
    <author>
      <name>/u/tengo_harambe</name>
      <uri>https://old.reddit.com/user/tengo_harambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nefy3g/alibabas_homegrown_chips_are_now_competitive_with/"&gt; &lt;img alt="Alibaba's homegrown chips are now competitive with Nvidia H20" src="https://external-preview.redd.it/3GSznXQc2v2aM2IifWcymKhFoVUIu_yfZrGgwEMCPBw.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a1612f97051237e10ed9d34a74d93f77252d00f" title="Alibaba's homegrown chips are now competitive with Nvidia H20" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tengo_harambe"&gt; /u/tengo_harambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reuters.com/world/china/alibaba-baidu-begin-using-own-chips-train-ai-models-information-reports-2025-09-11/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nefy3g/alibabas_homegrown_chips_are_now_competitive_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nefy3g/alibabas_homegrown_chips_are_now_competitive_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T18:05:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne4h62</id>
    <title>Qwen3-Next is coming soon</title>
    <updated>2025-09-11T09:43:38+00:00</updated>
    <author>
      <name>/u/Ok_Ninja7526</name>
      <uri>https://old.reddit.com/user/Ok_Ninja7526</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne4h62/qwen3next_is_coming_soon/"&gt; &lt;img alt="Qwen3-Next is coming soon" src="https://preview.redd.it/1mdp7l72biof1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2cd5da8e1e448326bed9df91f518aadffe866432" title="Qwen3-Next is coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Ninja7526"&gt; /u/Ok_Ninja7526 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1mdp7l72biof1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne4h62/qwen3next_is_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne4h62/qwen3next_is_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T09:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ned2ai</id>
    <title>Building RAG systems at enterprise scale (20K+ docs): lessons from 10+ enterprise implementations</title>
    <updated>2025-09-11T16:16:40+00:00</updated>
    <author>
      <name>/u/Low_Acanthisitta7686</name>
      <uri>https://old.reddit.com/user/Low_Acanthisitta7686</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been building RAG systems for mid-size enterprise companies in the regulated space (100-1000 employees) for the past year and to be honest, this stuff is way harder than any tutorial makes it seem. Worked with around 10+ clients now - pharma companies, banks, law firms, consulting shops. Thought I'd share what actually matters vs all the basic info you read online.&lt;/p&gt; &lt;p&gt;Quick context: most of these companies had 10K-50K+ documents sitting in SharePoint hell or document management systems from 2005. Not clean datasets, not curated knowledge bases - just decades of business documents that somehow need to become searchable.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Document quality detection: the thing nobody talks about&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This was honestly the biggest revelation for me. Most tutorials assume your PDFs are perfect. Reality check: enterprise documents are absolute garbage.&lt;/p&gt; &lt;p&gt;I had one pharma client with research papers from 1995 that were scanned copies of typewritten pages. OCR barely worked. Mixed in with modern clinical trial reports that are 500+ pages with embedded tables and charts. Try applying the same chunking strategy to both and watch your system return complete nonsense.&lt;/p&gt; &lt;p&gt;Spent weeks debugging why certain documents returned terrible results while others worked fine. Finally realized I needed to score document quality before processing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clean PDFs (text extraction works perfectly): full hierarchical processing&lt;/li&gt; &lt;li&gt;Decent docs (some OCR artifacts): basic chunking with cleanup&lt;/li&gt; &lt;li&gt;Garbage docs (scanned handwritten notes): simple fixed chunks + manual review flags&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Built a simple scoring system looking at text extraction quality, OCR artifacts, formatting consistency. Routes documents to different processing pipelines based on score. This single change fixed more retrieval issues than any embedding model upgrade.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why fixed-size chunking is mostly wrong&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Every tutorial: &amp;quot;just chunk everything into 512 tokens with overlap!&amp;quot;&lt;/p&gt; &lt;p&gt;Reality: documents have structure. A research paper's methodology section is different from its conclusion. Financial reports have executive summaries vs detailed tables. When you ignore structure, you get chunks that cut off mid-sentence or combine unrelated concepts.&lt;/p&gt; &lt;p&gt;Had to build hierarchical chunking that preserves document structure:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Document level (title, authors, date, type)&lt;/li&gt; &lt;li&gt;Section level (Abstract, Methods, Results)&lt;/li&gt; &lt;li&gt;Paragraph level (200-400 tokens)&lt;/li&gt; &lt;li&gt;Sentence level for precision queries&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The key insight: query complexity should determine retrieval level. Broad questions stay at paragraph level. Precise stuff like &amp;quot;what was the exact dosage in Table 3?&amp;quot; needs sentence-level precision.&lt;/p&gt; &lt;p&gt;I use simple keyword detection - words like &amp;quot;exact&amp;quot;, &amp;quot;specific&amp;quot;, &amp;quot;table&amp;quot; trigger precision mode. If confidence is low, system automatically drills down to more precise chunks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Metadata architecture matters more than your embedding model&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is where I spent 40% of my development time and it had the highest ROI of anything I built.&lt;/p&gt; &lt;p&gt;Most people treat metadata as an afterthought. But enterprise queries are crazy contextual. A pharma researcher asking about &amp;quot;pediatric studies&amp;quot; needs completely different documents than someone asking about &amp;quot;adult populations.&amp;quot;&lt;/p&gt; &lt;p&gt;Built domain-specific metadata schemas:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For pharma docs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Document type (research paper, regulatory doc, clinical trial)&lt;/li&gt; &lt;li&gt;Drug classifications&lt;/li&gt; &lt;li&gt;Patient demographics (pediatric, adult, geriatric)&lt;/li&gt; &lt;li&gt;Regulatory categories (FDA, EMA)&lt;/li&gt; &lt;li&gt;Therapeutic areas (cardiology, oncology)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;For financial docs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Time periods (Q1 2023, FY 2022)&lt;/li&gt; &lt;li&gt;Financial metrics (revenue, EBITDA)&lt;/li&gt; &lt;li&gt;Business segments&lt;/li&gt; &lt;li&gt;Geographic regions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Avoid using LLMs for metadata extraction - they're inconsistent as hell. Simple keyword matching works way better. Query contains &amp;quot;FDA&amp;quot;? Filter for regulatory_category: &amp;quot;FDA&amp;quot;. Mentions &amp;quot;pediatric&amp;quot;? Apply patient population filters.&lt;/p&gt; &lt;p&gt;Start with 100-200 core terms per domain, expand based on queries that don't match well. Domain experts are usually happy to help build these lists.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;When semantic search fails (spoiler: a lot)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Pure semantic search fails way more than people admit. In specialized domains like pharma and legal, I see 15-20% failure rates, not the 5% everyone assumes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Main failure modes that drove me crazy:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Acronym confusion:&lt;/strong&gt; &amp;quot;CAR&amp;quot; means &amp;quot;Chimeric Antigen Receptor&amp;quot; in oncology but &amp;quot;Computer Aided Radiology&amp;quot; in imaging papers. Same embedding, completely different meanings. This was a constant headache.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Precise technical queries:&lt;/strong&gt; Someone asks &amp;quot;What was the exact dosage in Table 3?&amp;quot; Semantic search finds conceptually similar content but misses the specific table reference.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Cross-reference chains:&lt;/strong&gt; Documents reference other documents constantly. Drug A study references Drug B interaction data. Semantic search misses these relationship networks completely.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; Built hybrid approaches. Graph layer tracks document relationships during processing. After semantic search, system checks if retrieved docs have related documents with better answers.&lt;/p&gt; &lt;p&gt;For acronyms, I do context-aware expansion using domain-specific acronym databases. For precise queries, keyword triggers switch to rule-based retrieval for specific data points.&lt;/p&gt; &lt;h1&gt;Why I went with open source models (Qwen specifically)&lt;/h1&gt; &lt;p&gt;Most people assume GPT-4o or o3-mini are always better. But enterprise clients have weird constraints:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cost:&lt;/strong&gt; API costs explode with 50K+ documents and thousands of daily queries&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data sovereignty:&lt;/strong&gt; Pharma and finance can't send sensitive data to external APIs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Domain terminology:&lt;/strong&gt; General models hallucinate on specialized terms they weren't trained on&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Qwen QWQ-32B ended up working surprisingly well after domain-specific fine-tuning:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;85% cheaper than GPT-4o for high-volume processing&lt;/li&gt; &lt;li&gt;Everything stays on client infrastructure&lt;/li&gt; &lt;li&gt;Could fine-tune on medical/financial terminology&lt;/li&gt; &lt;li&gt;Consistent response times without API rate limits&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Fine-tuning approach was straightforward - supervised training with domain Q&amp;amp;A pairs. Created datasets like &amp;quot;What are contraindications for Drug X?&amp;quot; paired with actual FDA guideline answers. Basic supervised fine-tuning worked better than complex stuff like RAFT. Key was having clean training data.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Table processing: the hidden nightmare&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Enterprise docs are full of complex tables - financial models, clinical trial data, compliance matrices. Standard RAG either ignores tables or extracts them as unstructured text, losing all the relationships.&lt;/p&gt; &lt;p&gt;Tables contain some of the most critical information. Financial analysts need exact numbers from specific quarters. Researchers need dosage info from clinical tables. If you can't handle tabular data, you're missing half the value.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My approach:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Treat tables as separate entities with their own processing pipeline&lt;/li&gt; &lt;li&gt;Use heuristics for table detection (spacing patterns, grid structures)&lt;/li&gt; &lt;li&gt;For simple tables: convert to CSV. For complex tables: preserve hierarchical relationships in metadata&lt;/li&gt; &lt;li&gt;Dual embedding strategy: embed both structured data AND semantic description&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For the bank project, financial tables were everywhere. Had to track relationships between summary tables and detailed breakdowns too.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Production infrastructure reality check&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Tutorials assume unlimited resources and perfect uptime. Production means concurrent users, GPU memory management, consistent response times, uptime guarantees.&lt;/p&gt; &lt;p&gt;Most enterprise clients already had GPU infrastructure sitting around - unused compute or other data science workloads. Made on-premise deployment easier than expected.&lt;/p&gt; &lt;p&gt;Typically deploy 2-3 models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Main generation model (Qwen 32B) for complex queries&lt;/li&gt; &lt;li&gt;Lightweight model for metadata extraction&lt;/li&gt; &lt;li&gt;Specialized embedding model&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Used quantized versions when possible. Qwen QWQ-32B quantized to 4-bit only needed 24GB VRAM but maintained quality. Could run on single RTX 4090, though A100s better for concurrent users.&lt;/p&gt; &lt;p&gt;Biggest challenge isn't model quality - it's preventing resource contention when multiple users hit the system simultaneously. Use semaphores to limit concurrent model calls and proper queue management.&lt;/p&gt; &lt;h1&gt;Key lessons that actually matter&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Document quality detection first:&lt;/strong&gt; You cannot process all enterprise docs the same way. Build quality assessment before anything else.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Metadata &amp;gt; embeddings:&lt;/strong&gt; Poor metadata means poor retrieval regardless of how good your vectors are. Spend the time on domain-specific schemas.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Hybrid retrieval is mandatory:&lt;/strong&gt; Pure semantic search fails too often in specialized domains. Need rule-based fallbacks and document relationship mapping.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Tables are critical:&lt;/strong&gt; If you can't handle tabular data properly, you're missing huge chunks of enterprise value.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Infrastructure determines success:&lt;/strong&gt; Clients care more about reliability than fancy features. Resource management and uptime matter more than model sophistication.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The real talk&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Enterprise RAG is way more engineering than ML. Most failures aren't from bad models - they're from underestimating the document processing challenges, metadata complexity, and production infrastructure needs.&lt;/p&gt; &lt;p&gt;The demand is honestly crazy right now. Every company with substantial document repositories needs these systems, but most have no idea how complex it gets with real-world documents.&lt;/p&gt; &lt;p&gt;Anyway, this stuff is way harder than tutorials make it seem. The edge cases with enterprise documents will make you want to throw your laptop out the window. But when it works, the ROI is pretty impressive - seen teams cut document search from hours to minutes.&lt;/p&gt; &lt;p&gt;Posted this in LLMDevs a few days ago and many people found the technical breakdown helpful, so wanted to share here too for the broader AI community!&lt;/p&gt; &lt;p&gt;Happy to answer questions if anyone's hitting similar walls with their implementations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Low_Acanthisitta7686"&gt; /u/Low_Acanthisitta7686 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ned2ai/building_rag_systems_at_enterprise_scale_20k_docs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ned2ai/building_rag_systems_at_enterprise_scale_20k_docs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ned2ai/building_rag_systems_at_enterprise_scale_20k_docs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T16:16:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1neey2c</id>
    <title>Qwen3-next “technical” blog is up</title>
    <updated>2025-09-11T17:27:40+00:00</updated>
    <author>
      <name>/u/Alarming-Ad8154</name>
      <uri>https://old.reddit.com/user/Alarming-Ad8154</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here: &lt;a href="https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;amp;from=research.latest-advancements-list"&gt;https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;amp;from=research.latest-advancements-list&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarming-Ad8154"&gt; /u/Alarming-Ad8154 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neey2c/qwen3next_technical_blog_is_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neey2c/qwen3next_technical_blog_is_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1neey2c/qwen3next_technical_blog_is_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T17:27:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne7y69</id>
    <title>Qwen3-Next-80B-A3B-Thinking soon</title>
    <updated>2025-09-11T12:49:27+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne7y69/qwen3next80ba3bthinking_soon/"&gt; &lt;img alt="Qwen3-Next-80B-A3B-Thinking soon" src="https://preview.redd.it/bo8hhc558jof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=547def56180e3c7f03468272c1979619111e065e" title="Qwen3-Next-80B-A3B-Thinking soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bo8hhc558jof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne7y69/qwen3next80ba3bthinking_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne7y69/qwen3next80ba3bthinking_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T12:49:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nejluw</id>
    <title>Qwen Next Is A Preview Of Qwen3.5👀</title>
    <updated>2025-09-11T20:26:30+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nejluw/qwen_next_is_a_preview_of_qwen35/"&gt; &lt;img alt="Qwen Next Is A Preview Of Qwen3.5👀" src="https://preview.redd.it/hddap3b9hlof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80ea94871e4f36ff1a26b9ef506a7c93ef43d580" title="Qwen Next Is A Preview Of Qwen3.5👀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After experimenting with Qwen3 Next, it's a very impressive model. It does have problems with sycophancy and coherence- but it's fast, smart and it's long context performance is solid. Awesome stuff from the Tongyi Lab!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hddap3b9hlof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nejluw/qwen_next_is_a_preview_of_qwen35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nejluw/qwen_next_is_a_preview_of_qwen35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T20:26:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1neba8b</id>
    <title>Qwen</title>
    <updated>2025-09-11T15:06:37+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neba8b/qwen/"&gt; &lt;img alt="Qwen" src="https://preview.redd.it/p5fbgn0owjof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94561db32b1fca11c0250280863739d22d76e841" title="Qwen" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p5fbgn0owjof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neba8b/qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1neba8b/qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T15:06:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nefmzr</id>
    <title>Qwen released Qwen3-Next-80B-A3B — the FUTURE of efficient LLMs is here!</title>
    <updated>2025-09-11T17:53:48+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nefmzr/qwen_released_qwen3next80ba3b_the_future_of/"&gt; &lt;img alt="Qwen released Qwen3-Next-80B-A3B — the FUTURE of efficient LLMs is here!" src="https://a.thumbs.redditmedia.com/WcRnBmHLixgTcJRWqYT5sRJfRzk65bSO8Y3sUSxIA28.jpg" title="Qwen released Qwen3-Next-80B-A3B — the FUTURE of efficient LLMs is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 Introducing Qwen3-Next-80B-A3B — the FUTURE of efficient LLMs is here!&lt;/p&gt; &lt;p&gt;🔹 80B params, but only 3B activated per token → 10x cheaper training, 10x faster inference than Qwen3-32B.(esp. @ 32K+ context!) 🔹Hybrid Architecture: Gated DeltaNet + Gated Attention → best of speed &amp;amp; recall 🔹 Ultra-sparse MoE: 512 experts, 10 routed + 1 shared 🔹 Multi-Token Prediction → turbo-charged speculative decoding 🔹 Beats Qwen3-32B in perf, rivals Qwen3-235B in reasoning &amp;amp; long-context&lt;/p&gt; &lt;p&gt;🧠 Qwen3-Next-80B-A3B-Instruct approaches our 235B flagship. 🧠 Qwen3-Next-80B-A3B-Thinking outperforms Gemini-2.5-Flash-Thinking.&lt;/p&gt; &lt;p&gt;Try it now: chat.qwen.ai&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;amp;from=research.latest-advancements-list"&gt;https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;amp;from=research.latest-advancements-list&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Huggingface: &lt;a href="https://huggingface.co/collections/Qwen/qwen3-next-68c25fd6838e585db8eeea9d"&gt;https://huggingface.co/collections/Qwen/qwen3-next-68c25fd6838e585db8eeea9d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nefmzr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nefmzr/qwen_released_qwen3next80ba3b_the_future_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nefmzr/qwen_released_qwen3next80ba3b_the_future_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T17:53:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nedq3i</id>
    <title>We just released the world's first 70B intermediate checkpoints. Yes, Apache 2.0. Yes, we're still broke.</title>
    <updated>2025-09-11T16:42:00+00:00</updated>
    <author>
      <name>/u/jshin49</name>
      <uri>https://old.reddit.com/user/jshin49</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Remember when y'all roasted us about the license? We listened.&lt;/p&gt; &lt;p&gt;Just dropped what we think is a world first: &lt;strong&gt;70B model intermediate checkpoints&lt;/strong&gt;. Not just the final model - the entire training journey. Previous releases (SmolLM-3, OLMo-2) maxed out at &amp;lt;14B.&lt;/p&gt; &lt;p&gt;Everything is Apache 2.0 now (no gated access):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;70B, 7B, 1.9B, 0.5B models + all their intermediate checkpoints and base models&lt;/li&gt; &lt;li&gt;First Korean 70B ever (but secretly optimized for English lol)&lt;/li&gt; &lt;li&gt;Actually open-source, not just open-weights BS&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/trillionlabs/Tri-70B-Intermediate-Checkpoints"&gt;https://huggingface.co/trillionlabs/Tri-70B-Intermediate-Checkpoints&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're a 1-year-old startup with pocket change competing against companies with infinite money glitch. Not the best model, but probably the most transparent 70B training ever shared.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jshin49"&gt; /u/jshin49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nedq3i/we_just_released_the_worlds_first_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nedq3i/we_just_released_the_worlds_first_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nedq3i/we_just_released_the_worlds_first_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T16:42:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nct0z8</id>
    <title>Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)</title>
    <updated>2025-09-09T19:47:12+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt; &lt;img alt="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" src="https://preview.redd.it/7vh8enuu07of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0f499252613d5bcf478fb1b75c594bb50f43436" title="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7vh8enuu07of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T19:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndjxdt</id>
    <title>AMA with the Unsloth team</title>
    <updated>2025-09-10T17:02:18+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;, I'm Daniel from &lt;a href="https://docs.unsloth.ai/"&gt;Unsloth&lt;/a&gt;! You might know us from our RL &amp;amp; fine-tuning open-source framework, our GGUFs, kernels or bug fixes. We’re super excited to answer all your questions!! 🦥 Our GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To celebrate the AMA, we’re releasing Aider Polyglot benchmarks comparing our DeepSeek-V3.1 Dynamic GGUFs to other models and quants. We also made a Localllama post here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our participants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Daniel, u/danielhanchen&lt;/li&gt; &lt;li&gt;Michael, u/yoracale&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 10AM – 1PM PST, with the Unsloth team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thanks so much!🥰&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T17:02:18+00:00</published>
  </entry>
</feed>
