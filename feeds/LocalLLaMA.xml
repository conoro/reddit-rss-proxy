<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-05T08:11:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q3t7go</id>
    <title>LLM memory systems</title>
    <updated>2026-01-04T15:48:40+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is good in LLM memory systems these days?&lt;/p&gt; &lt;p&gt;I don‚Äôt mean RAG&lt;/p&gt; &lt;p&gt;I mean like memory storage that an LLM can read or write to, or long-term memory that persists across generations &lt;/p&gt; &lt;p&gt;Has anyone seen any interesting design patterns or github repos?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3t7go/llm_memory_systems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3t7go/llm_memory_systems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3t7go/llm_memory_systems/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T15:48:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3nvcn</id>
    <title>Can you connect a GPU with 12V rail coming from a second PSU?</title>
    <updated>2026-01-04T11:38:04+00:00</updated>
    <author>
      <name>/u/Rock_and_Rolf</name>
      <uri>https://old.reddit.com/user/Rock_and_Rolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3nvcn/can_you_connect_a_gpu_with_12v_rail_coming_from_a/"&gt; &lt;img alt="Can you connect a GPU with 12V rail coming from a second PSU?" src="https://preview.redd.it/vyb6lxutibbg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88ed9009e4b7eaa72d7fc916ae20f8ff60475ec3" title="Can you connect a GPU with 12V rail coming from a second PSU?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;; Can you connect a GPU with the 12V rail coming from a second PSU? &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update1:&lt;/strong&gt; I have already made a connector to connect both GND's, i forgot to mention this.&lt;br /&gt; &lt;strong&gt;Update2:&lt;/strong&gt; I have found another way to test this without breaking needed hardware. Somebody on a local marketplace sells a GTX770 for ‚Ç¨20 that appears to have a 6 + 8 pin power connector, i can pick this up in a few hours. If this doesn't work i'll look in to splitting 12V or bifurcation. Thanks for your replies!!&lt;br /&gt; &lt;strong&gt;Update3:&lt;/strong&gt; I nearly have my scrap test setup ready to test, but I have other thing to do now and will continue tomorrow, i'll keep you all posted. Thanks for all the replies, much appreciated!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full story&lt;/strong&gt;; I currently have a Dell T7910 with two AMD Radeon VII's (GFX906, Pmax set=190W) to play with LLMs/Roo Code. Last week, i managed to buy two more of these GPU's for an absurdly low price. I knew i had enough PCI-E slots, but i would need to use PCI-E extender cables to actually connect them (i already bought a pair). But i hadn't fully thought about the power supply, because despite the 1300W PSU, it doesn't have enough 8 or 6-pin 12V connectors. Now i have a second 950W PSU from a deceased Dell T5820 that i could use to power these extra GPUs.&lt;/p&gt; &lt;p&gt;As i am an electrical engineer myself, i had an idea of how this should work, but i also see a problem. Switching on synchronized works fine and i split the on/off button to both PSU breakout boards via a relay. However, since the PCI-E slot it self also supplies 12V to the GPU (25 or 75W depending on the slot), this is likely to cause problems with balancing the difference in 12V voltages on the GPU or motherboard, since these currents are huge and these are quite low resistance paths, even 100 to 200mV difference can cause huge balancing currents in places that are not meant for this.&lt;/p&gt; &lt;p&gt;On the other hand, other PSU's commonly have different 12V rails that can cause similar problems. So since i didn't measure a direct contact i got the feeling the solution/isolation to my problem is already designed in for these kind of PSU's.&lt;/p&gt; &lt;p&gt;Since i am surely not the first person to encounter this problem, i started looking for information about it. Most of the time, you end up on forums about crypto mining, and they often use a PCI-E extender via USB, which makes their situation completely different. I have read in several places that the PCI-E slot power is not directly connected to the 6 and/or 8-pin connectors and that this should be possible. I also verified this by measuring resistance between the 6/8 pins to the PCI-E connector, these are not directly connected. However, i think this is a huge risk and i would like to know from you, whether my information/assumptions are correct and how others have solved similar problems.&lt;/p&gt; &lt;p&gt;Since the PSU in this PC is &lt;strong&gt;not a standard ATX PSU&lt;/strong&gt;, replacing it with a high-power version with enough power/connections is not possible. Otherwise, i would have done so, because i don't want to risk my system to save a (tiny) bit of money. Also the standard multi PSU turn on cables are not compatible because the architecture is somewhat different, because this machine need so much (peak) power, they feed everything with 12V and convert down to the low voltages locally, to reduce the impedance/loses of the path. So most of the plugs from the PSU &amp;lt;&amp;gt; Motherboard are different.&lt;/p&gt; &lt;p&gt;I'm also thinking about using my old workstation (Dell T5600) and an old GPU as a first test. But my old GPU (Nvidia 1060) i need to drive my old dual DVI 2k monitor on my bench PC, so it would be shame to lose that system as well. Another option would be to remove the 12V pins on the PCI-E extender, but if that fails i've ruined another ‚Ç¨100. If this test setup works i can check with a sensitive thermal camera (Flir E8) if no new hotspots appear.&lt;/p&gt; &lt;p&gt;Does anyone have information or experience with this? or have good ideas on how to test it more safely, i have all the measurement tools i might ever need so exotic suggestions/solutions/tests are also welcome. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rock_and_Rolf"&gt; /u/Rock_and_Rolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vyb6lxutibbg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3nvcn/can_you_connect_a_gpu_with_12v_rail_coming_from_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3nvcn/can_you_connect_a_gpu_with_12v_rail_coming_from_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T11:38:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3oqvl</id>
    <title>MiniMax-M2.1 REAP models from 0xSero</title>
    <updated>2026-01-04T12:27:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now you can run MiniMax on everything :)&lt;/p&gt; &lt;p&gt;(waiting for GGUFs)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/0xSero/MiniMax-M2.1-REAP-50"&gt;https://huggingface.co/0xSero/MiniMax-M2.1-REAP-50&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/0xSero/MiniMax-M2.1-REAP-40"&gt;https://huggingface.co/0xSero/MiniMax-M2.1-REAP-40&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/0xSero/MiniMax-M2.1-REAP-30"&gt;https://huggingface.co/0xSero/MiniMax-M2.1-REAP-30&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/0xSero/MiniMax-M2.1-REAP-25"&gt;https://huggingface.co/0xSero/MiniMax-M2.1-REAP-25&lt;/a&gt;&lt;/p&gt; &lt;p&gt;looks like there will be more: Intellect3 25 / 30 / 40 / 50&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3oqvl/minimaxm21_reap_models_from_0xsero/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3oqvl/minimaxm21_reap_models_from_0xsero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3oqvl/minimaxm21_reap_models_from_0xsero/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T12:27:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1q414vj</id>
    <title>gsh - play with any local model directly in your shell REPL or scripts</title>
    <updated>2026-01-04T20:46:34+00:00</updated>
    <author>
      <name>/u/atinylittleshell</name>
      <uri>https://old.reddit.com/user/atinylittleshell</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q414vj/gsh_play_with_any_local_model_directly_in_your/"&gt; &lt;img alt="gsh - play with any local model directly in your shell REPL or scripts" src="https://preview.redd.it/yh1dwt9j8ebg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=383dc60edeb928b15b21feb2777015966be76471" title="gsh - play with any local model directly in your shell REPL or scripts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing a holiday side project i just built: gsh - a new shell, like bash, zsh, fish, but fully agentic. I find it really useful for playing with local models both interactively and in automation scripts. &lt;a href="https://github.com/atinylittleshell/gsh"&gt;https://github.com/atinylittleshell/gsh&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Key features:&lt;br /&gt; - It can predict the next shell command you may want to run, or help you write one when you forgot how to&lt;br /&gt; - It can act as a coding agent itself, or delegate to other agents via ACP&lt;br /&gt; - It comes with an agentic scripting language which you can use to build agentic workflows, or to customize gsh (almost the entire repl can be customized, like neovim)&lt;br /&gt; - Use whatever LLM you like - a lot can be done with local models&lt;br /&gt; - Battery included - syntax highlighting, tab completion, history, auto suggestion, starship integration all work out of the box&lt;/p&gt; &lt;p&gt;Super early of course, but i've been daily driving for a while and replaced zsh with it. If you think it's time to try a new shell or new ways to play with local models, give it a try and let me know how it goes! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atinylittleshell"&gt; /u/atinylittleshell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yh1dwt9j8ebg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q414vj/gsh_play_with_any_local_model_directly_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q414vj/gsh_play_with_any_local_model_directly_in_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T20:46:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1q48guf</id>
    <title>Using small lightweight models for AI chatbots that watch a livestream and comment on what is going on</title>
    <updated>2026-01-05T01:47:40+00:00</updated>
    <author>
      <name>/u/Powerful-Frame-44</name>
      <uri>https://old.reddit.com/user/Powerful-Frame-44</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been experimenting with lightweight ultra-fast models. They don't need to do anything too complicated, just respond to a description of what is happening on a livestream and comment on it in real-time. &lt;/p&gt; &lt;p&gt;I've found smaller models are a bit too dumb and repetitive. They also overly rely on emojis. So far, Llama 3.1 8B is the best option I've found that is not too computationally expensive and produces results that seem at least vaguely like a human chatter.&lt;/p&gt; &lt;p&gt;What model would you use for this purpose?&lt;/p&gt; &lt;p&gt;The bots watch the stream and comment on what happens in the chat and on stream. They sometimes have some interesting emergent behaviors.&lt;/p&gt; &lt;p&gt;You can check out what they're saying at &lt;a href="https://onestreamer.live"&gt;https://onestreamer.live&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Frame-44"&gt; /u/Powerful-Frame-44 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q48guf/using_small_lightweight_models_for_ai_chatbots/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q48guf/using_small_lightweight_models_for_ai_chatbots/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q48guf/using_small_lightweight_models_for_ai_chatbots/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T01:47:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4c4jb</id>
    <title>Stress-testing local LLM agents with adversarial inputs (Ollama, Qwen)</title>
    <updated>2026-01-05T04:35:23+00:00</updated>
    <author>
      <name>/u/No-Common1466</name>
      <uri>https://old.reddit.com/user/No-Common1466</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on a small open-source tool to stress-test AI agents that run on local models (Ollama, Qwen, Gemma, etc.).&lt;/p&gt; &lt;p&gt;The problem I kept running into: an agent looks fine when tested with clean prompts, but once you introduce typos, tone shifts, long context, or basic prompt injection patterns, behavior gets unpredictable very fast ‚Äî especially on smaller local models.&lt;/p&gt; &lt;p&gt;So I built Flakestorm, which takes a single ‚Äúgolden prompt‚Äù, generates adversarial mutations (paraphrases, noise, injections, encoding edge cases, etc.), and runs them against a local agent endpoint. It produces a simple robustness score + an HTML report showing what failed.&lt;/p&gt; &lt;p&gt;This is very much local-first: Uses Ollama for mutation generation Tested primarily with Qwen 2.5 (3B / 7B) and Gemma&lt;/p&gt; &lt;p&gt;No cloud required, no API keys Example failures I‚Äôve seen on local agents: Silent instruction loss after long-context mutations JSON output breaking under simple noise Injection patterns leaking system instructions Latency exploding with certain paraphrases&lt;/p&gt; &lt;p&gt;I‚Äôm early and still validating whether this is useful beyond my own workflows, so I‚Äôd genuinely love feedback from people running local agents: Is this something you already do manually? Are there failure modes you‚Äôd want to test that aren‚Äôt covered?&lt;/p&gt; &lt;p&gt;Does ‚Äúchaos testing for agents‚Äù resonate, or is this better framed differently?&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/flakestorm/flakestorm"&gt;https://github.com/flakestorm/flakestorm&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Common1466"&gt; /u/No-Common1466 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4c4jb/stresstesting_local_llm_agents_with_adversarial/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4c4jb/stresstesting_local_llm_agents_with_adversarial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4c4jb/stresstesting_local_llm_agents_with_adversarial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T04:35:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3u89f</id>
    <title>HomeGenie v2.0: 100% Local Agentic AI (Sub-5s response on CPU, No Cloud)</title>
    <updated>2026-01-04T16:28:19+00:00</updated>
    <author>
      <name>/u/genielabs</name>
      <uri>https://old.reddit.com/user/genielabs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3u89f/homegenie_v20_100_local_agentic_ai_sub5s_response/"&gt; &lt;img alt="HomeGenie v2.0: 100% Local Agentic AI (Sub-5s response on CPU, No Cloud)" src="https://external-preview.redd.it/aXVic3Z5NTEwZGJnMX-wuN5UqDYSq_G1PvG8gD6oltW7ZDgAnY8CDzv70t9I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39b052b57497a280e1ac5403e6f63479f2615fd4" title="HomeGenie v2.0: 100% Local Agentic AI (Sub-5s response on CPU, No Cloud)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I‚Äôve been working on HomeGenie 2.0, focusing on bringing &amp;quot;Agentic AI&amp;quot; to the edge.&lt;/p&gt; &lt;p&gt;Unlike standard dashboards, it integrates a local neural core (Lailama) that uses LLamaSharp to run GGUF models (Qwen 3, Llama 3.2, etc.) entirely offline.&lt;/p&gt; &lt;p&gt;Key technical bits: - &lt;strong&gt;Autonomous Reasoning:&lt;/strong&gt; It's not just a chatbot. It gets a real-time briefing of the home state (sensors, weather, energy) and decides which API commands to trigger. - &lt;strong&gt;Sub-5s Latency:&lt;/strong&gt; Optimized KV Cache management and history pruning to keep it fast on standard CPUs. - &lt;strong&gt;Programmable UI:&lt;/strong&gt; Built with zuix.js, allowing real-time widget editing directly in the browser. - &lt;strong&gt;Privacy First:&lt;/strong&gt; 100% cloud-independent.&lt;/p&gt; &lt;p&gt;I‚Äôm looking for feedback from the self-hosted community! Happy to answer any technical questions about the C# implementation or the agentic logic.&lt;/p&gt; &lt;p&gt;Project: &lt;a href="https://homegenie.it"&gt;https://homegenie.it&lt;/a&gt; Source: &lt;a href="https://github.com/genielabs/HomeGenie"&gt;https://github.com/genielabs/HomeGenie&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/genielabs"&gt; /u/genielabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/m40jjx610dbg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3u89f/homegenie_v20_100_local_agentic_ai_sub5s_response/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3u89f/homegenie_v20_100_local_agentic_ai_sub5s_response/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T16:28:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4ew2k</id>
    <title>Need architectural feedback: privacy-safe cloud memory between your data and LLM agents</title>
    <updated>2026-01-05T07:00:58+00:00</updated>
    <author>
      <name>/u/Ok_Soup6298</name>
      <uri>https://old.reddit.com/user/Ok_Soup6298</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Reddit,&lt;/p&gt; &lt;p&gt;We‚Äôre the team behind &lt;strong&gt;Aristo&lt;/strong&gt;, currently building a product called &lt;strong&gt;Membase&lt;/strong&gt;. We‚Äôre trying to solve a problem many of you probably know well: every time you switch LLMs or agent frameworks, you have to rebuild or re-inject all your context from scratch.&lt;/p&gt; &lt;p&gt;Our goal is to build a &lt;strong&gt;syncable, persistent memory layer&lt;/strong&gt; that sits between your data and whatever models/agents you want to use. Initially, we explored a fully local database design to maximize privacy, but ran into real constraints around integrations, latency, and reliability at scale, so we moved to a cloud-based architecture instead.&lt;/p&gt; &lt;p&gt;Because of that, we‚Äôre now trying to be very deliberate about &lt;strong&gt;how much control users get over their data&lt;/strong&gt;, especially for people who are (rightfully) sensitive about sending anything to the cloud. Concretely, the current design is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Our team &lt;strong&gt;cannot see or use&lt;/strong&gt; your data for training or analytics beyond what‚Äôs strictly necessary for running the service (no ‚Äúsecret fine-tuning‚Äù, no manual inspection by default).&lt;/li&gt; &lt;li&gt;When you connect an existing LLM or agent stack to Membase, you can choose &lt;strong&gt;which categories of context&lt;/strong&gt; can flow into the memory layer (e.g., Work, Personal, Hobby, etc.), instead of everything being ingested blindly.&lt;/li&gt; &lt;li&gt;When you let external agents read from Membase, you can again choose &lt;strong&gt;which categories&lt;/strong&gt; they are allowed to access, so you could, for example, expose only ‚ÄúWork‚Äù but never ‚ÄúPersonal‚Äù.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Right now, Membase is still in the early design/prototyping phase and we‚Äôre only running a waitlist, so feedback will have a real impact on what we build. This community seems to care a lot about privacy and threat models, so we‚Äôd really appreciate your thoughts on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Does this category-based, user-controlled sharing model feel meaningful from a privacy perspective, or is it just ‚Äúcheckbox security‚Äù?&lt;/li&gt; &lt;li&gt;Are there technical patterns you‚Äôd expect here (e.g., client-side encryption, zero-knowledge-style access, audit logs, local cache layers) that you‚Äôd consider ‚Äúmust have‚Äù before trusting a cloud memory layer?&lt;/li&gt; &lt;li&gt;Any examples of systems that got this right (or very wrong) that we should study?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer detailed questions about the architecture and trade-offs. Thanks in advance for any candid feedback.&lt;/p&gt; &lt;p&gt;*If you need more information or are interested, this is our landing page: &lt;a href="https://membase.so/?utm_source=reddit&amp;amp;utm_medium=LocalLLama"&gt;Membase&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Soup6298"&gt; /u/Ok_Soup6298 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4ew2k/need_architectural_feedback_privacysafe_cloud/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4ew2k/need_architectural_feedback_privacysafe_cloud/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4ew2k/need_architectural_feedback_privacysafe_cloud/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T07:00:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3ryd7</id>
    <title>Will the prices of GPUs go up even more?</title>
    <updated>2026-01-04T14:57:45+00:00</updated>
    <author>
      <name>/u/NotSoCleverAlternate</name>
      <uri>https://old.reddit.com/user/NotSoCleverAlternate</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hear discussions about this so I wanted to hear your guys take on it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NotSoCleverAlternate"&gt; /u/NotSoCleverAlternate &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ryd7/will_the_prices_of_gpus_go_up_even_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ryd7/will_the_prices_of_gpus_go_up_even_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ryd7/will_the_prices_of_gpus_go_up_even_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T14:57:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3zl67</id>
    <title>Best memory strategy for long-form NSFW/Erotic RP: Raw context vs. Summarization vs. MemGPT?</title>
    <updated>2026-01-04T19:48:18+00:00</updated>
    <author>
      <name>/u/FollowingFresh6411</name>
      <uri>https://old.reddit.com/user/FollowingFresh6411</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I‚Äôm experimenting with a dedicated LLM bot for writing long-form erotic stories and roleplay, and I‚Äôm hitting the classic context wall. I‚Äôm curious about what the community finds most effective for maintaining &amp;quot;the heat&amp;quot; and prose quality over long sessions.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Which approach yields better results in your experience?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Full Raw Context (Sliding Window): Sending the entire recent history. It keeps the vibe and prose style consistent, but obviously, I lose the beginning of the story once the token limit is reached.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. LLM-based Summarization: Using a secondary (or the same) model to summarize previous events. My concern here is that summaries often feel too &amp;quot;clinical&amp;quot; or dry, which tends to kill the tension and descriptive nuances that are crucial for erotic writing.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Persistent Memory (MemGPT / Letta / Mem0): Using a memory engine to store facts and character traits. Does this actually work for keeping the narrative &amp;quot;flow,&amp;quot; or is it better suited only for static lore facts?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I‚Äôm currently looking at SillyTavern‚Äôs hybrid approach (Lorebooks + Summarize extension), but I‚Äôm wondering if anyone has found a way to use MemGPT-style memory without making the AI sound like a robot reciting a Wikipedia entry mid-scene.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What‚Äôs your setup for keeping the story consistent without losing the stylistic &amp;quot;soul&amp;quot; of the writing?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FollowingFresh6411"&gt; /u/FollowingFresh6411 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3zl67/best_memory_strategy_for_longform_nsfwerotic_rp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3zl67/best_memory_strategy_for_longform_nsfwerotic_rp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3zl67/best_memory_strategy_for_longform_nsfwerotic_rp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T19:48:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3sfr1</id>
    <title>Propagate: Train thinking models using evolutionary strategies!</title>
    <updated>2026-01-04T15:17:42+00:00</updated>
    <author>
      <name>/u/Good-Assumption5582</name>
      <uri>https://old.reddit.com/user/Good-Assumption5582</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3sfr1/propagate_train_thinking_models_using/"&gt; &lt;img alt="Propagate: Train thinking models using evolutionary strategies!" src="https://b.thumbs.redditmedia.com/jKCq9SIWUx4OFIfHcnbQ57A60zT3kWy5DTpg1qMi3XA.jpg" title="Propagate: Train thinking models using evolutionary strategies!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, this paper released:&lt;br /&gt; &lt;a href="https://arxiv.org/abs/2509.24372"&gt;https://arxiv.org/abs/2509.24372&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And showed that with only 30 random gaussian perturbations, you can accurately approximate a gradient and outperform GRPO on RLVR tasks. They found zero overfitting, and training was significantly faster because you didn't have to perform any backward passes.&lt;/p&gt; &lt;p&gt;I thought that this was ridiculous, so I took their repo, cleaned up the codebase, and it replicates!&lt;/p&gt; &lt;p&gt;A couple weeks later, and I've implemented LoRA and pass@k training, with more features to come.&lt;/p&gt; &lt;p&gt;I hope you'll give ES a try!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Green0-0/propagate"&gt;https://github.com/Green0-0/propagate&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Good-Assumption5582"&gt; /u/Good-Assumption5582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q3sfr1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3sfr1/propagate_train_thinking_models_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3sfr1/propagate_train_thinking_models_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T15:17:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3p9oz</id>
    <title>MultiverseComputingCAI/HyperNova-60B ¬∑ Hugging Face</title>
    <updated>2026-01-04T12:55:03+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3p9oz/multiversecomputingcaihypernova60b_hugging_face/"&gt; &lt;img alt="MultiverseComputingCAI/HyperNova-60B ¬∑ Hugging Face" src="https://external-preview.redd.it/4y3rB_X0xi7PC07OhAWlbpJK6pkTGA-GxUmQGu5l2u4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d9d4e40f5211cdcc28dd6fde8fa1da920bd51a8" title="MultiverseComputingCAI/HyperNova-60B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;HyperNova 60B&lt;/strong&gt; base architecture is &lt;a href="https://huggingface.co/openai/gpt-oss-120b"&gt;&lt;code&gt;gpt-oss-120b&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;59B parameters with 4.8B active parameters&lt;/li&gt; &lt;li&gt;MXFP4 quantization&lt;/li&gt; &lt;li&gt;Configurable reasoning effort (low, medium, high)&lt;/li&gt; &lt;li&gt;GPU usage of less than 40GB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/HyperNova-60B-GGUF"&gt;https://huggingface.co/mradermacher/HyperNova-60B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/HyperNova-60B-i1-GGUF"&gt;https://huggingface.co/mradermacher/HyperNova-60B-i1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/MultiverseComputingCAI/HyperNova-60B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3p9oz/multiversecomputingcaihypernova60b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3p9oz/multiversecomputingcaihypernova60b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T12:55:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4aiko</id>
    <title>[Release] We trained an AI to understand Taiwanese memes and slang because major models couldn't. Meet Twinkle AI's gemma-3-4B-T1-it.</title>
    <updated>2026-01-05T03:19:37+00:00</updated>
    <author>
      <name>/u/piske_usagi</name>
      <uri>https://old.reddit.com/user/piske_usagi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4aiko/release_we_trained_an_ai_to_understand_taiwanese/"&gt; &lt;img alt="[Release] We trained an AI to understand Taiwanese memes and slang because major models couldn't. Meet Twinkle AI's gemma-3-4B-T1-it." src="https://b.thumbs.redditmedia.com/apQKs5SnpvjUL3x2Lbp2vdNWMFekHx7PLLC6v_u-SdQ.jpg" title="[Release] We trained an AI to understand Taiwanese memes and slang because major models couldn't. Meet Twinkle AI's gemma-3-4B-T1-it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ,&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;Twinkle AI&lt;/strong&gt;, and today we are releasing &lt;strong&gt;gemma-3-4B-T1-Instruct&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;We realized that when major LLMs generate Traditional Chinese, they often default to Mainland Chinese terminology, slang, and cultural perspectives. They translate the &lt;em&gt;words&lt;/em&gt;, but miss the &lt;em&gt;context&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;We built &lt;strong&gt;gemma-3-4B-T1-it&lt;/strong&gt;, a specialized version of Google's new Gemma 3 designed specifically for the context of &lt;strong&gt;Taiwan&lt;/strong&gt;. It knows our laws, our geography, and yes, our internet slang.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tda9w1qu7gbg1.png?width=3469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0245d5368ee8f42fe3d51fa5776017534e5754f4"&gt;True Cultural Alignment: It knows the difference between local Taiwanese slang (e.g., \&amp;quot;ÂæàÁõ§\&amp;quot; - rip-off) and generic terms. It understands local geography and memes.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's a fun experiment in how deep localization changes model behavior. It also happens to be really good at &lt;strong&gt;Function Calling&lt;/strong&gt; if you want to build agents with it.&lt;/p&gt; &lt;p&gt;We'd love to hear your &lt;a href="https://discord.gg/tnkXrNGst3"&gt;feedback&lt;/a&gt; on this approach to highly localized LLMs!&lt;/p&gt; &lt;p&gt;ü§ó &lt;a href="https://huggingface.co/twinkle-ai/gemma-3-4B-T1-it/blob/main/README_EN.md"&gt;twinkle-ai/gemma-3-4B-T1-it&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/piske_usagi"&gt; /u/piske_usagi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4aiko/release_we_trained_an_ai_to_understand_taiwanese/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4aiko/release_we_trained_an_ai_to_understand_taiwanese/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4aiko/release_we_trained_an_ai_to_understand_taiwanese/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T03:19:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1q48q2s</id>
    <title>EasyWhisperUI - Open-Source Easy UI for OpenAI‚Äôs Whisper model with cross platform GPU support (Windows/Mac)</title>
    <updated>2026-01-05T01:58:52+00:00</updated>
    <author>
      <name>/u/mehtabmahir</name>
      <uri>https://old.reddit.com/user/mehtabmahir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys, it‚Äôs been a while but I‚Äôm happy to announce a major update for &lt;strong&gt;EasyWhisperUI&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Whisper is OpenAI‚Äôs automatic speech recognition (ASR) model that converts audio into text, and it can also translate speech into English. It‚Äôs commonly used for transcribing things like meetings, lectures, podcasts, and videos with strong accuracy across many languages.&lt;/p&gt; &lt;p&gt;If you‚Äôve seen my earlier posts, EasyWhisperUI originally used a &lt;strong&gt;Qt-based UI&lt;/strong&gt;. After a lot of iteration, I‚Äôve now migrated the app to an &lt;strong&gt;Electron architecture (React + Electron + IPC)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The whole point of EasyWhisperUI is simple: &lt;strong&gt;make the entire Whisper/whisper.cpp process extremely beginner friendly&lt;/strong&gt;. No digging through CLI flags, no ‚Äúfigure out models yourself,‚Äù no piecing together FFmpeg, no confusing setup steps. You download the app, pick a model, drop in your files, and it just runs.&lt;/p&gt; &lt;p&gt;It‚Äôs also built around &lt;strong&gt;cross platform GPU acceleration&lt;/strong&gt;, because I didn‚Äôt want this to be NVIDIA-only. On Windows it uses &lt;strong&gt;Vulkan&lt;/strong&gt; (so it works across &lt;strong&gt;Intel + AMD + NVIDIA&lt;/strong&gt; GPUs, including integrated graphics), and on macOS it uses &lt;strong&gt;Metal&lt;/strong&gt; on Apple Silicon. &lt;strong&gt;Linux is coming very soon.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After &lt;strong&gt;countless hours of work&lt;/strong&gt;, the app has been migrated to Electron to deliver a &lt;strong&gt;consistent cross-platform UI experience&lt;/strong&gt; across &lt;strong&gt;Windows + macOS (and Linux very soon)&lt;/strong&gt; and make updates/features ship much faster.&lt;/p&gt; &lt;p&gt;The new build has also been &lt;strong&gt;tested on a fresh Windows system several times&lt;/strong&gt; to verify clean installs, dependency setup, and end-to-end transcription.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/mehtabmahir/easy-whisper-ui"&gt;https://github.com/mehtabmahir/easy-whisper-ui&lt;/a&gt;&lt;br /&gt; Releases: &lt;a href="https://github.com/mehtabmahir/easy-whisper-ui/releases"&gt;https://github.com/mehtabmahir/easy-whisper-ui/releases&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What EasyWhisperUI does (beginner-friendly on purpose)&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Local transcription powered by whisper.cpp&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cross platform GPU acceleration&lt;/strong&gt; Vulkan on Windows (Intel/AMD/NVIDIA) Metal on macOS (Apple Silicon)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Batch processing&lt;/strong&gt; with a queue (drag in multiple files and let it run)&lt;/li&gt; &lt;li&gt;Export to &lt;code&gt;.txt&lt;/code&gt; or &lt;code&gt;.srt&lt;/code&gt; (timestamps)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live transcription&lt;/strong&gt; (beta)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Automatic model downloads&lt;/strong&gt; (pick a model and it downloads if missing)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Automatic media conversion&lt;/strong&gt; via FFmpeg when needed&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Support for 100+ languages and more!&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;What‚Äôs new in this Electron update&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;First-launch Loader / Setup Wizard&lt;/strong&gt; Full-screen setup flow with real-time progress and logs shown directly in the UI.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Improved automatic dependency setup (Windows)&lt;/strong&gt; More hands-off setup that installs/validates what‚Äôs needed and then builds/stages Whisper automatically.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Per-user workspace (clean + predictable)&lt;/strong&gt; Binaries, models, toolchain, and downloads are managed under your user profile so updates and cleanup stay painless.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cross-platform UI consistency&lt;/strong&gt; Same UI behavior and feature set across Windows + macOS (and Linux very soon).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Way fewer Windows Defender headaches&lt;/strong&gt; This should be noticeably smoother now.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Quick Windows note for GPU acceleration&lt;/h1&gt; &lt;p&gt;For Vulkan GPU acceleration on Windows, make sure you‚Äôre using the latest drivers directly from Intel/AMD/NVIDIA (not OEM drivers).&lt;br /&gt; Example: on my &lt;strong&gt;ASUS Zenbook S16&lt;/strong&gt;, the OEM graphics drivers did &lt;strong&gt;not&lt;/strong&gt; include Vulkan support.&lt;/p&gt; &lt;p&gt;Please try it out and let me know your results! Consider supporting my work if it helps you out :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehtabmahir"&gt; /u/mehtabmahir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q48q2s/easywhisperui_opensource_easy_ui_for_openais/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q48q2s/easywhisperui_opensource_easy_ui_for_openais/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q48q2s/easywhisperui_opensource_easy_ui_for_openais/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T01:58:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1q401ka</id>
    <title>Ratios of Active Parameters to Total Parameters on major MoE models</title>
    <updated>2026-01-04T20:05:05+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Total Params&lt;/th&gt; &lt;th align="left"&gt;Active Params&lt;/th&gt; &lt;th align="left"&gt;% Active&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5 Air&lt;/td&gt; &lt;td align="left"&gt;106&lt;/td&gt; &lt;td align="left"&gt;12&lt;/td&gt; &lt;td align="left"&gt;11.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.6 and 4.7&lt;/td&gt; &lt;td align="left"&gt;355&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;9%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT OSS 20B&lt;/td&gt; &lt;td align="left"&gt;21&lt;/td&gt; &lt;td align="left"&gt;3.6&lt;/td&gt; &lt;td align="left"&gt;17.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT OSS 120B&lt;/td&gt; &lt;td align="left"&gt;117&lt;/td&gt; &lt;td align="left"&gt;5.1&lt;/td&gt; &lt;td align="left"&gt;4.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B A3B&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;10%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 Next 80B A3B&lt;/td&gt; &lt;td align="left"&gt;80&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;3.8%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B A22B&lt;/td&gt; &lt;td align="left"&gt;235&lt;/td&gt; &lt;td align="left"&gt;22&lt;/td&gt; &lt;td align="left"&gt;9.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Deepseek 3.2&lt;/td&gt; &lt;td align="left"&gt;685&lt;/td&gt; &lt;td align="left"&gt;37&lt;/td&gt; &lt;td align="left"&gt;5.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MiniMax M2.1&lt;/td&gt; &lt;td align="left"&gt;230&lt;/td&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;4.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi K2&lt;/td&gt; &lt;td align="left"&gt;1000&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;3.2%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;And for fun, some oldies:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Total Params&lt;/th&gt; &lt;th align="left"&gt;Active Params&lt;/th&gt; &lt;th align="left"&gt;% Active&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Mixtral 8x7B&lt;/td&gt; &lt;td align="left"&gt;47&lt;/td&gt; &lt;td align="left"&gt;13&lt;/td&gt; &lt;td align="left"&gt;27.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mixtral 8x22B&lt;/td&gt; &lt;td align="left"&gt;141&lt;/td&gt; &lt;td align="left"&gt;39&lt;/td&gt; &lt;td align="left"&gt;27.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Deepseek V2&lt;/td&gt; &lt;td align="left"&gt;236&lt;/td&gt; &lt;td align="left"&gt;21&lt;/td&gt; &lt;td align="left"&gt;8.9%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Grok 2&lt;/td&gt; &lt;td align="left"&gt;270&lt;/td&gt; &lt;td align="left"&gt;115&lt;/td&gt; &lt;td align="left"&gt;42.6% (record highest?)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;(Disclaimer: I'm just a casual user, and I know very little about the science of LLMs. My opinion is entirely based on osmosis and vibes.)&lt;/p&gt; &lt;p&gt;Total Parameters tends to represent the variety of knowledge available to the LLM, while Active Parameters is the intelligence. We've been trending towards lower percentage of Active params, probably because of the focus on benchmarks. Models have to know all sorts of trivia to pass all those multiple-choice tests, and know various programming languages to pass coding benchmarks.&lt;/p&gt; &lt;p&gt;I personally prefer high Active (sometimes preferring dense models for this reason), because I mainly use local LLMs for creative writing or one-off local tasks where I want it to read between the lines instead of me having to be extremely clear.&lt;/p&gt; &lt;p&gt;Fun thought: how would some popular models have changed with a different parameter count? What if GLM-4.5-Air was 5B active and GPT-OSS-120B was 12B? What if Qwen3 80B was 10B active?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q401ka/ratios_of_active_parameters_to_total_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q401ka/ratios_of_active_parameters_to_total_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q401ka/ratios_of_active_parameters_to_total_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T20:05:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1q44ujj</id>
    <title>Orla: use lightweight, open-source, local agents as UNIX tools.</title>
    <updated>2026-01-04T23:15:07+00:00</updated>
    <author>
      <name>/u/Available_Pressure47</name>
      <uri>https://old.reddit.com/user/Available_Pressure47</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q44ujj/orla_use_lightweight_opensource_local_agents_as/"&gt; &lt;img alt="Orla: use lightweight, open-source, local agents as UNIX tools." src="https://a.thumbs.redditmedia.com/5H4mcMhdhUXkPtWCzCv4kT2VIIIvzq4oOo4MRjSXc34.jpg" title="Orla: use lightweight, open-source, local agents as UNIX tools." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/dorcha-inc/orla"&gt;https://github.com/dorcha-inc/orla&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The current ecosystem around agents feels like a collection of bloated SaaS with expensive subscriptions and privacy concerns. Orla brings large language models to your terminal with a dead-simple, Unix-friendly interface. Everything runs 100% locally. You don't need any API keys or subscriptions, and your data never leaves your machine. Use it like any other command-line tool:&lt;/p&gt; &lt;p&gt;$ orla agent &amp;quot;summarize this code&amp;quot; &amp;lt; main.go&lt;/p&gt; &lt;p&gt;$ git status | orla agent &amp;quot;Draft a commit message for these changes.&amp;quot;&lt;/p&gt; &lt;p&gt;$ cat data.json | orla agent &amp;quot;extract all email addresses&amp;quot; | sort -u&lt;/p&gt; &lt;p&gt;It's built on the Unix philosophy and is pipe-friendly and easily extensible.&lt;/p&gt; &lt;p&gt;The README in the repo contains a quick demo.&lt;/p&gt; &lt;p&gt;Installation is a single command. The script installs Orla, sets up Ollama for local inference, and pulls a lightweight model to get you started.&lt;/p&gt; &lt;p&gt;You can use homebrew (on Mac OS or Linux)&lt;/p&gt; &lt;p&gt;$ brew install --cask dorcha-inc/orla/orla&lt;/p&gt; &lt;p&gt;Or use the shell installer:&lt;/p&gt; &lt;p&gt;$ curl -fsSL &lt;a href="https://raw.githubusercontent.com/dorcha-inc/orla/main/scrip"&gt;https://raw.githubusercontent.com/dorcha-inc/orla/main/scrip&lt;/a&gt;... | sh&lt;/p&gt; &lt;p&gt;Orla is written in Go and is completely free software (MIT licensed) built on other free software. We'd love your feedback.&lt;/p&gt; &lt;p&gt;Thank you! :-)&lt;/p&gt; &lt;p&gt;Side note: contributions to Orla are very welcome. Please see (&lt;a href="https://github.com/dorcha-inc/orla/blob/main/CONTRIBUTING.md"&gt;https://github.com/dorcha-inc/orla/blob/main/CONTRIBUTING.md&lt;/a&gt;) for a guide on how to contribute.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available_Pressure47"&gt; /u/Available_Pressure47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q44ujj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q44ujj/orla_use_lightweight_opensource_local_agents_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q44ujj/orla_use_lightweight_opensource_local_agents_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T23:15:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3yug4</id>
    <title>FLUX.2-dev-Turbo is surprisingly good at image editing</title>
    <updated>2026-01-04T19:20:47+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3yug4/flux2devturbo_is_surprisingly_good_at_image/"&gt; &lt;img alt="FLUX.2-dev-Turbo is surprisingly good at image editing" src="https://external-preview.redd.it/dmN0aGFnMHN1ZGJnMWhQRIGbuygHvibzarhf8EVUxtFTiMplyPAlWNfH6-Zg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e2f3950c64b4732685a054c1ec22a163f1d83f4e" title="FLUX.2-dev-Turbo is surprisingly good at image editing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Getting excellent results, FAL did a great job with this FLUX.2 [dev] LoRA: &lt;a href="https://huggingface.co/fal/FLUX.2-dev-Turbo"&gt;https://huggingface.co/fal/FLUX.2-dev-Turbo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The speed and cost (&lt;strong&gt;only 8 inference steps!&lt;/strong&gt;) of it makes it very competitive with closed models. Perfect for daily creative workflow and local use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/os8k650sudbg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3yug4/flux2devturbo_is_surprisingly_good_at_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3yug4/flux2devturbo_is_surprisingly_good_at_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T19:20:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4f0tm</id>
    <title>I built a visual AI workflow tool that runs entirely in your browser - Ollama, LM Studio, llama.cpp and Most cloud API's all work out of the box. Agents/Websearch/TTS/Etc.</title>
    <updated>2026-01-05T07:08:30+00:00</updated>
    <author>
      <name>/u/l33t-Mt</name>
      <uri>https://old.reddit.com/user/l33t-Mt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4f0tm/i_built_a_visual_ai_workflow_tool_that_runs/"&gt; &lt;img alt="I built a visual AI workflow tool that runs entirely in your browser - Ollama, LM Studio, llama.cpp and Most cloud API's all work out of the box. Agents/Websearch/TTS/Etc." src="https://external-preview.redd.it/aGJ3cmdlMXMyaGJnMfKIu2bgp1pENmKjPeusz-I2kkXf7vs8dV2V756jCzVD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67c688a8e1b37aaeac76f077a39bd2c01ad85859" title="I built a visual AI workflow tool that runs entirely in your browser - Ollama, LM Studio, llama.cpp and Most cloud API's all work out of the box. Agents/Websearch/TTS/Etc." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You might remember me from LlamaCards a previous program ive built or maybe you've seen some of my agentic computer use posts with Moondream/Minicpm navigation creating reddit posts.&lt;/p&gt; &lt;p&gt;Ive had my head down and I've finally gotten something I wanted to show you all.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EmergentFlow&lt;/strong&gt; - a visual node-based editor for creating AI workflows and agents. The whole execution engine runs in your browser. Its a great sandbox for developing AI workflows.&lt;/p&gt; &lt;p&gt;You just open it and go. No Docker, no Python venv, no dependencies. Connect your Ollama(or other local) instance, paste your API keys for whatever providers you use, and start building. Everything runs client-side - your keys stay in your browser, your prompts go directly to the providers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Supported:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ollama (just works - point it at localhost:11434, auto-fetches models)&lt;/li&gt; &lt;li&gt;LM Studio + llama.cpp (works once CORS is configured)&lt;/li&gt; &lt;li&gt;OpenAI, Anthropic, Groq, Gemini, DeepSeek, xAI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For edge cases where you hit CORS issues, there's an optional desktop runner that acts as a local proxy. It's open source: &lt;a href="http://github.com/l33tkr3w/EmergentFlow-runner"&gt;github.com/l33tkr3w/EmergentFlow-runner&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But honestly most stuff works straight from the browser.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The deal:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It's free. Like, actually free - not &amp;quot;free trial&amp;quot; free. &lt;/p&gt; &lt;p&gt;You get a full sandbox with unlimited use of your own API keys. The only thing that costs credits is if you use my server-paid models (Gemini) because Google charges me for those.&lt;/p&gt; &lt;p&gt;Free tier gets 25 daily credits for server models(Gemini through my API key). &lt;/p&gt; &lt;p&gt;Running Ollama/LMStudio/llama.cpp or BYOK? &lt;strong&gt;Unlimited. Forever. No catch.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I do have a Pro tier ($19/mo) for power users who want more server credits and team collaboration, node/flow gallery - because I'm a solo dev with a kid trying to make this sustainable. But honestly most people here running local models won't need it. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; &lt;a href="https://emergentflow.io/try"&gt;emergentflow.io/try&lt;/a&gt; - no signup, no credit card, just start dragging nodes.&lt;/p&gt; &lt;p&gt;If you run into issues (there will be some), please submit a bug report. Happy to answer questions about how stuff works under the hood.&lt;/p&gt; &lt;p&gt;Support a fellow LocalLlama enthusiast! Updoot?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/l33t-Mt"&gt; /u/l33t-Mt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ps5d841s2hbg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4f0tm/i_built_a_visual_ai_workflow_tool_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4f0tm/i_built_a_visual_ai_workflow_tool_that_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T07:08:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4bhtm</id>
    <title>vLLM reaches 2000 contributors!</title>
    <updated>2026-01-05T04:04:52+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/vllm-project/vllm/graphs/contributors"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4bhtm/vllm_reaches_2000_contributors/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4bhtm/vllm_reaches_2000_contributors/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T04:04:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4atlx</id>
    <title>[R] We built a framework to make Agents "self-evolve" using LoongFlow. Paper + Code released</title>
    <updated>2026-01-05T03:33:57+00:00</updated>
    <author>
      <name>/u/FreshmanDD</name>
      <uri>https://old.reddit.com/user/FreshmanDD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Reddit,&lt;/p&gt; &lt;p&gt;We are the team behind &lt;strong&gt;LoongFlow&lt;/strong&gt;. We've been researching how to solve the &amp;quot;static agent&amp;quot; problem‚Äîwhere agents fail to adapt to complex tasks or get stuck in loops.&lt;/p&gt; &lt;p&gt;Instead of manual prompt engineering, we applied &lt;strong&gt;Evolutionary Algorithms&lt;/strong&gt; (Selection, Mutation, Crossover) to the agent workflow. Treat prompts and logic as &amp;quot;DNA&amp;quot; that can evolve over generations to find the optimal solution.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üß¨ &lt;strong&gt;General-Evolve:&lt;/strong&gt; Automatically optimizes prompts and code logic.&lt;/li&gt; &lt;li&gt;üìà &lt;strong&gt;Proven Results:&lt;/strong&gt; In our benchmarks (detailed in the paper), we saw significant accuracy improvements compared to standard ReAct agents.&lt;/li&gt; &lt;li&gt;üîß &lt;strong&gt;Extensible:&lt;/strong&gt; Built for developers to create custom evolutionary pipelines.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We just released the paper on arXiv and the code is fully open-source.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;üìÑ Paper:&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2512.24077"&gt;https://arxiv.org/abs/2512.24077&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;üíª GitHub:&lt;/strong&gt;&lt;a href="https://github.com/baidu-baige/LoongFlow"&gt;https://github.com/baidu-baige/LoongFlow&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are looking for feedback on the architecture! Would love to hear your thoughts on combining EA with LLMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FreshmanDD"&gt; /u/FreshmanDD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4atlx/r_we_built_a_framework_to_make_agents_selfevolve/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4atlx/r_we_built_a_framework_to_make_agents_selfevolve/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4atlx/r_we_built_a_framework_to_make_agents_selfevolve/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T03:33:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1q42wtt</id>
    <title>Introducing Adaptive-P: A New Sampler for Creative Text Generation (llama.cpp PR)</title>
    <updated>2026-01-04T21:58:39+00:00</updated>
    <author>
      <name>/u/DragPretend7554</name>
      <uri>https://old.reddit.com/user/DragPretend7554</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share a sampling method we've been working on called Adaptive-P. Before I get into it, I should mention that due to a visual impairment, I used AI assistance in writing both the documentation and this post. I want to be upfront about that. The algorithm itself and the underlying idea are human created, however.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is it?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Adaptive-P is a different approach to token sampling that tries to address models getting stuck in predictable patterns. When generating creative content, models often fall back on the same phrasing, sentence structures, and narrative beats. The model has more interesting options available, but standard sampling methods don't give you a way to encourage it toward those alternatives.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How does it work?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of uniformly scaling probabilities like temperature does, or making binary keep/discard decisions like truncation methods, Adaptive-P lets you specify a probability range you want to target. It applies a transformation that creates a preference curve centered on your target probability‚Äîtokens near the target get boosted, tokens far from it get suppressed.&lt;/p&gt; &lt;p&gt;The transformation uses unbounded negative logits for distant tokens rather than a floor value. This prevents probability from accumulating in the tail of the distribution, which is a problem that affects some other approaches to forced alternative selection.&lt;/p&gt; &lt;p&gt;The sampler maintains an exponential moving average of the original probabilities of selected tokens. It uses this history to compute an adjusted target at each step. If recent selections have been running above your configured target, the sampler compensates by aiming lower on the next step, and vice versa. This feedback loop keeps the average selection probability tracking toward your target over time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Chain breaking&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The adaptive mechanism is what breaks repetitive high-confidence chains. When the model keeps selecting dominant tokens, the history shifts upward, which pushes the calculated target downward, which makes alternatives more attractive. The sampler naturally resists getting stuck in a rut without requiring external repetition penalties.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's it good for?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is designed for creative work‚Äîfiction, roleplay, brainstorming. It's not meant for tasks where accuracy matters more than variety.&lt;/p&gt; &lt;p&gt;It pairs well with Min-P, which handles removing genuinely bad options while Adaptive-P handles selection among the remaining quality candidates. Adaptive-P needs to be the final sampler in the chain since it performs the actual token selection.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Documentation: &lt;a href="https://github.com/MrJackSpade/adaptive-p-docs/blob/main/Documentation.md"&gt;https://github.com/MrJackSpade/adaptive-p-docs/blob/main/Documentation.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;llama.cpp PR: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17927"&gt;https://github.com/ggml-org/llama.cpp/pull/17927&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discord discussion: &lt;a href="https://discord.com/channels/1238219753324281886/1447392417769721926"&gt;https://discord.com/channels/1238219753324281886/1447392417769721926&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Any and all questions will likely be answered by the documentation, or the discord server.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DragPretend7554"&gt; /u/DragPretend7554 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q42wtt/introducing_adaptivep_a_new_sampler_for_creative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q42wtt/introducing_adaptivep_a_new_sampler_for_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q42wtt/introducing_adaptivep_a_new_sampler_for_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T21:58:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4ahw1</id>
    <title>Llama 3.3 8B, abliterated to &lt;0.05 KL</title>
    <updated>2026-01-05T03:18:45+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is an abliterated version of the allegedly leaked Llama 3.3 8B 128k model that tries to minimize intelligence loss while optimizing for compliance.&lt;/p&gt; &lt;p&gt;Link (BF16 weights):&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Llama-3.3-8B-Instruct-128K_Abliterated"&gt;https://huggingface.co/SicariusSicariiStuff/Llama-3.3-8B-Instruct-128K_Abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Credits: Fizzarolli, p-e-w, some employee @ meta for another successful failure.&lt;/p&gt; &lt;p&gt;Enjoy :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4ahw1/llama_33_8b_abliterated_to_005_kl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4ahw1/llama_33_8b_abliterated_to_005_kl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4ahw1/llama_33_8b_abliterated_to_005_kl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T03:18:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q41bw1</id>
    <title>GLM-Image model from Z.ai is coming</title>
    <updated>2026-01-04T20:54:04+00:00</updated>
    <author>
      <name>/u/Ravencloud007</name>
      <uri>https://old.reddit.com/user/Ravencloud007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/"&gt; &lt;img alt="GLM-Image model from Z.ai is coming" src="https://preview.redd.it/sm31vizebebg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ae576450ba7112c06760ff8cddee6f5bdd7b672" title="GLM-Image model from Z.ai is coming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/pull/43100/files"&gt;https://github.com/huggingface/transformers/pull/43100/files&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ravencloud007"&gt; /u/Ravencloud007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sm31vizebebg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T20:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
