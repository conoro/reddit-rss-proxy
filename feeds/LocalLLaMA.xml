<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-16T14:08:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pnnkxc</id>
    <title>Qwen3 next 80B w/ 250k tok context fits fully on one 7900 XTX (24 GB) and runs at 41 tok/s</title>
    <updated>2025-12-16T00:16:44+00:00</updated>
    <author>
      <name>/u/1ncehost</name>
      <uri>https://old.reddit.com/user/1ncehost</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Late to the party, but better late than never. Using IQ2_XSS quant, Q4_0 KV quants, &amp;amp; FA enabled.&lt;/p&gt; &lt;p&gt;I feel like this is a major milestone in general for single card LLM usage. It seems very usable for programming at this quant level.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1ncehost"&gt; /u/1ncehost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnnkxc/qwen3_next_80b_w_250k_tok_context_fits_fully_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnnkxc/qwen3_next_80b_w_250k_tok_context_fits_fully_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnnkxc/qwen3_next_80b_w_250k_tok_context_fits_fully_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T00:16:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnyowo</id>
    <title>How Embeddings Enable Modern Search - Visualizing The Latent Space [Clip]</title>
    <updated>2025-12-16T10:31:38+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnyowo/how_embeddings_enable_modern_search_visualizing/"&gt; &lt;img alt="How Embeddings Enable Modern Search - Visualizing The Latent Space [Clip]" src="https://external-preview.redd.it/OGh2NnB5OTBuajdnMXvlMSuOk6Yj3OVLPT5Vhd2Psp3lO7t6XDInPZ-YNWwd.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8ae3d077589ecc5e1b395e680005225ae157ead" title="How Embeddings Enable Modern Search - Visualizing The Latent Space [Clip]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/g6kejpa0nj7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnyowo/how_embeddings_enable_modern_search_visualizing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnyowo/how_embeddings_enable_modern_search_visualizing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T10:31:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnzt9y</id>
    <title>2025 Open Models Year in Review</title>
    <updated>2025-12-16T11:39:20+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnzt9y/2025_open_models_year_in_review/"&gt; &lt;img alt="2025 Open Models Year in Review" src="https://a.thumbs.redditmedia.com/bAksHsJN0-iuMgHdsfyM9GRrMDkxdq7w432ACMBAY84.jpg" title="2025 Open Models Year in Review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/x9r0l9rcyj7g1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=04829370c50c43b71249d0b687d517beaa024d53"&gt;https://preview.redd.it/x9r0l9rcyj7g1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=04829370c50c43b71249d0b687d517beaa024d53&lt;/a&gt;&lt;/p&gt; &lt;p&gt;AI research organization &lt;a href="https://www.interconnects.ai/p/2025-open-models-year-in-review"&gt;Interconnects&lt;/a&gt; released the 2025 Annual Review Report on Open-Source Models, stating that 2025 is a milestone year for the development of open-source models. The report shows that open-source models have achieved performance comparable to closed-source models in most key benchmarks, with DeepSeek R1 and Qwen 3 being recognized as the most influential models of the year.&lt;/p&gt; &lt;h1&gt;Mapping the open ecosystem&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/obgfnkd2zj7g1.png?width=1946&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e1c376a28402c3d1b0912f2b66c2d0ee2ebbc28"&gt;https://preview.redd.it/obgfnkd2zj7g1.png?width=1946&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e1c376a28402c3d1b0912f2b66c2d0ee2ebbc28&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The organizations are as follows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Frontier&lt;/strong&gt;: DeepSeek, Qwen, Moonshot AI (Kimi)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Close competitors&lt;/strong&gt;: Zhipu (Z.Ai), Minimax&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Noteworthy&lt;/strong&gt;: StepFun, InclusionAI / Ant Ling, Meituan Longcat, Tencent, IBM, NVIDIA, Google, Mistral&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Specialists&lt;/strong&gt;: OpenAI, Ai2, Moondream, Arcee, RedNote, HuggingFace, LiquidAI, Microsoft, Xiaomi, Mohamed bin Zayed University of Artificial Intelligence&lt;/p&gt; &lt;p&gt;&lt;strong&gt;On the rise&lt;/strong&gt;: ByteDance Seed, Apertus, OpenBMB, Motif, Baidu, Marin Community, InternLM, OpenGVLab, ServiceNow, Skywork&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Honorable mentions&lt;/strong&gt;: TNG Group, Meta, Cohere, Beijing Academy of Artificial Intelligence, Multimodal Art Projection, Huawei&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnzt9y/2025_open_models_year_in_review/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnzt9y/2025_open_models_year_in_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnzt9y/2025_open_models_year_in_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T11:39:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnc045</id>
    <title>status of Nemotron 3 Nano support in llama.cpp</title>
    <updated>2025-12-15T16:38:13+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnc045/status_of_nemotron_3_nano_support_in_llamacpp/"&gt; &lt;img alt="status of Nemotron 3 Nano support in llama.cpp" src="https://preview.redd.it/glwccqikbe7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71433581f884eabbfd427838b07ab54bbdbbd438" title="status of Nemotron 3 Nano support in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18058"&gt;https://github.com/ggml-org/llama.cpp/pull/18058&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/glwccqikbe7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnc045/status_of_nemotron_3_nano_support_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnc045/status_of_nemotron_3_nano_support_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T16:38:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn8h5h</id>
    <title>NVIDIA Nemotron 3 Nano 30B A3B released</title>
    <updated>2025-12-15T14:18:28+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16"&gt;https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-Base-BF16"&gt;https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-Base-BF16&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unsloth GGUF quants: &lt;a href="https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF/tree/main"&gt;https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nvidia blog post: &lt;a href="https://developer.nvidia.com/blog/inside-nvidia-nemotron-3-techniques-tools-and-data-that-make-it-efficient-and-accurate/"&gt;https://developer.nvidia.com/blog/inside-nvidia-nemotron-3-techniques-tools-and-data-that-make-it-efficient-and-accurate/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF blog post: &lt;a href="https://huggingface.co/blog/nvidia/nemotron-3-nano-efficient-open-intelligent-models"&gt;https://huggingface.co/blog/nvidia/nemotron-3-nano-efficient-open-intelligent-models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights (copy-pasta from HF blog):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hybrid Mamba-Transformer MoE architecture:&lt;/strong&gt; Mamba‚Äë2 for long-context, low-latency inference combined with transformer attention for high-accuracy, fine-grained reasoning&lt;/li&gt; &lt;li&gt;&lt;strong&gt;31.6B total parameters, ~3.6B active per token:&lt;/strong&gt; Designed for high throughput and low latency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Exceptional inference efficiency:&lt;/strong&gt; Up to 4x faster than Nemotron Nano 2 and up to 3.3x faster than leading models in its size category&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best-in-class reasoning accuracy:&lt;/strong&gt; Across reasoning, coding, tools, and multi-step agentic tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning controls:&lt;/strong&gt; Reasoning ON/OFF modes plus a configurable thinking budget to cap ‚Äúthinking‚Äù tokens and keep inference cost predictable&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1M-token context window:&lt;/strong&gt; Ideal for long-horizon workflows, retrieval-augmented tasks, and persistent memory&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully open:&lt;/strong&gt; Open Weights, datasets, training recipes, and framework&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A full open data stack&lt;/strong&gt;: 3T new high-quality pre-training tokens, 13M cross-disciplinary post-training samples, 10+ RL environments with datasets covering more than 900k tasks in math, coding, reasoning, and tool-use, and ~11k agent-safety traces&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy deployment:&lt;/strong&gt; Seamless serving with vLLM and SGLang, and integration via OpenRouter, popular inference service providers, and &lt;a href="http://build.nvidia.com"&gt;build.nvidia.com&lt;/a&gt; endpoints&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; Released under the &lt;a href="https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/"&gt;nvidia-open-model-license&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;PS. Nemotron 3 Super (~4x bigger than Nano) and Ultra (~16x bigger than Nano) to follow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8h5h/nvidia_nemotron_3_nano_30b_a3b_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8h5h/nvidia_nemotron_3_nano_30b_a3b_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8h5h/nvidia_nemotron_3_nano_30b_a3b_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T14:18:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnusq8</id>
    <title>Open Source Alternative to Perplexity</title>
    <updated>2025-12-16T06:16:11+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RBAC (Role Based Access for Teams)&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Agentic chat&lt;/li&gt; &lt;li&gt;Note Management (Like Notion)&lt;/li&gt; &lt;li&gt;Multi Collaborative Chats.&lt;/li&gt; &lt;li&gt;Multi Collaborative Documents.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Installation (Self-Host)&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Linux/macOS:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 \ -v surfsense-data:/data \ --name surfsense \ --restart unless-stopped \ ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Windows (PowerShell):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 ` -v surfsense-data:/data ` --name surfsense ` --restart unless-stopped ` ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusq8/open_source_alternative_to_perplexity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusq8/open_source_alternative_to_perplexity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusq8/open_source_alternative_to_perplexity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T06:16:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn37mw</id>
    <title>New Google model incoming!!!</title>
    <updated>2025-12-15T09:26:05+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn37mw/new_google_model_incoming/"&gt; &lt;img alt="New Google model incoming!!!" src="https://preview.redd.it/ho8nhiae6c7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53bdb437b0e3d6b162a1f97be9b2f4ae540eda69" title="New Google model incoming!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/osanseviero/status/2000493503860892049?s=20"&gt;https://x.com/osanseviero/status/2000493503860892049?s=20&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google"&gt;https://huggingface.co/google&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ho8nhiae6c7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn37mw/new_google_model_incoming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn37mw/new_google_model_incoming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T09:26:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnul23</id>
    <title>Sometimes it‚Äôs stupid even if it works</title>
    <updated>2025-12-16T06:04:18+00:00</updated>
    <author>
      <name>/u/Stunning_Mast2001</name>
      <uri>https://old.reddit.com/user/Stunning_Mast2001</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnul23/sometimes_its_stupid_even_if_it_works/"&gt; &lt;img alt="Sometimes it‚Äôs stupid even if it works" src="https://preview.redd.it/xvgt5nx9bi7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52c4bfd3eb7be97e9faed9c2c4c560c81a0efa06" title="Sometimes it‚Äôs stupid even if it works" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Someone gave me a quadro but I have a 1080ti already so no internal space‚Ä¶ just strapped it to the outside with the riser cables looping out the back‚Ä¶ works fine&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stunning_Mast2001"&gt; /u/Stunning_Mast2001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xvgt5nx9bi7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnul23/sometimes_its_stupid_even_if_it_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnul23/sometimes_its_stupid_even_if_it_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T06:04:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnd5uf</id>
    <title>They're finally here (Radeon 9700)</title>
    <updated>2025-12-15T17:20:23+00:00</updated>
    <author>
      <name>/u/Zeikos</name>
      <uri>https://old.reddit.com/user/Zeikos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnd5uf/theyre_finally_here_radeon_9700/"&gt; &lt;img alt="They're finally here (Radeon 9700)" src="https://b.thumbs.redditmedia.com/LlhzLUprDuJWJk6b4cZsmRPc06FSQCX3yS5XKj_YEOk.jpg" title="They're finally here (Radeon 9700)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zeikos"&gt; /u/Zeikos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pnd5uf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnd5uf/theyre_finally_here_radeon_9700/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnd5uf/theyre_finally_here_radeon_9700/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T17:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnllux</id>
    <title>New budget local AI rig</title>
    <updated>2025-12-15T22:51:32+00:00</updated>
    <author>
      <name>/u/vucamille</name>
      <uri>https://old.reddit.com/user/vucamille</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnllux/new_budget_local_ai_rig/"&gt; &lt;img alt="New budget local AI rig" src="https://preview.redd.it/6aavy1486g7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=505d5c7891c288215bdfa28b4ea82e8ed8df45bc" title="New budget local AI rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to buy 32GB Mi50s but decided against it because of their recent inflated prices. However, the 16GB versions are still affordable! I might buy another one in the future, or wait until the 32GB gets cheaper again.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qiyida X99 mobo with 32GB RAM and Xeon E5 2680 V4: 90 USD (AliExpress)&lt;/li&gt; &lt;li&gt;2x MI50 16GB with dual fan mod: 108 USD each plus 32 USD shipping (Alibaba)&lt;/li&gt; &lt;li&gt;1200W PSU bought in my country: 160 USD - lol the most expensive component in the PC&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In total, I spent about 650 USD. ROCm 7.0.2 works, and I have done some basic inference tests with llama.cpp and the two MI50, everything works well. Initially I tried with the latest ROCm release but multi GPU was not working for me.&lt;/p&gt; &lt;p&gt;I still need to buy brackets to prevent the bottom MI50 from sagging and maybe some decorations and LEDs, but so far super happy! And as a bonus, this thing can game!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vucamille"&gt; /u/vucamille &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6aavy1486g7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnllux/new_budget_local_ai_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnllux/new_budget_local_ai_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T22:51:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnxxnq</id>
    <title>Nemotron-Cascade 8B/14B from NVIDIA (Qwen3 finetunes)</title>
    <updated>2025-12-16T09:42:03+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxxnq/nemotroncascade_8b14b_from_nvidia_qwen3_finetunes/"&gt; &lt;img alt="Nemotron-Cascade 8B/14B from NVIDIA (Qwen3 finetunes)" src="https://external-preview.redd.it/2QEDEkegLrJTJtx6HLSiu0oL0Rwu2mfV0busJyR6xa4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=931815c93c1175064ff4b44e489163b0700b3b19" title="Nemotron-Cascade 8B/14B from NVIDIA (Qwen3 finetunes)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;powerful general-purpose model trained through sequential and domain-wise reinforcement learning&amp;quot;&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-Cascade-8B#results"&gt;&lt;/a&gt;Results&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;We evaluate our model against competitive reasoning models on a diverse set of benchmarks, covering general-knowledge reasoning, alignment and instruction following, mathematical reasoning, competitive programming, software engineering, and tool-use proficiency.&lt;/li&gt; &lt;li&gt;For Nemotron-Cascade models, we use a maximum generation length of 64K tokens and set the temperature to 0.6 and top-p to 0.95 for reasoning tasks.&lt;/li&gt; &lt;li&gt;Our Nemotron-Cascade models achieve best-in-class performance across almost all benchmarks. Remarkably, Nemotron-Cascade-8B and Nemotron-Cascade-8B-Thinking achieve comparable LiveCodeBench (LCB) and LCB Pro scores to DeepSeek-R1-0528 (671B).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-Cascade-14B-Thinking"&gt;https://huggingface.co/nvidia/Nemotron-Cascade-14B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gj477a45gj7g1.png?width=3686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44c7b8f4bdfa6c79c9fa4c4b27e7d5cb59a5d845"&gt;https://preview.redd.it/gj477a45gj7g1.png?width=3686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44c7b8f4bdfa6c79c9fa4c4b27e7d5cb59a5d845&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-Cascade-8B-Thinking"&gt;https://huggingface.co/nvidia/Nemotron-Cascade-8B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4bhwmvzbgj7g1.png?width=3654&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd06d124d8130f3e608bd39d3392fe9ec908b4eb"&gt;https://preview.redd.it/4bhwmvzbgj7g1.png?width=3654&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bd06d124d8130f3e608bd39d3392fe9ec908b4eb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-Cascade-8B"&gt;https://huggingface.co/nvidia/Nemotron-Cascade-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pto8szxfgj7g1.png?width=3664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2525653a81ac3fd712ed5f92052fbbdf3674880"&gt;https://preview.redd.it/pto8szxfgj7g1.png?width=3664&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2525653a81ac3fd712ed5f92052fbbdf3674880&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxxnq/nemotroncascade_8b14b_from_nvidia_qwen3_finetunes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxxnq/nemotroncascade_8b14b_from_nvidia_qwen3_finetunes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxxnq/nemotroncascade_8b14b_from_nvidia_qwen3_finetunes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T09:42:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnslcb</id>
    <title>My Local coding agent worked 2 hours unsupervised and here is my setup</title>
    <updated>2025-12-16T04:14:38+00:00</updated>
    <author>
      <name>/u/Express_Quail_1493</name>
      <uri>https://old.reddit.com/user/Express_Quail_1493</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Setup&lt;/p&gt; &lt;p&gt;--- Model&lt;br /&gt; devstral-small-2 from bartowski IQ3_xxs version.&lt;br /&gt; Run with lm studio &amp;amp; intentionally limit the context at 40960 which should't take more than (14gb ram even when context is full)&lt;/p&gt; &lt;p&gt;---Tool&lt;br /&gt; kilo code (set file limit to 500 lines) it will read in chunks&lt;br /&gt; 40960 ctx limit is actually a strength not weakness (more ctx = easier confusion)&lt;br /&gt; Paired with qdrant in the kilo code UI.&lt;br /&gt; Setup the indexing with qdrant (the little database icon) use model &lt;a href="https://ollama.com/toshk0/nomic-embed-text-v2-moe"&gt;https://ollama.com/toshk0/nomic-embed-text-v2-moe&lt;/a&gt; in ollama (i choose ollama to keep indexing and seperate from Lm studio to allow lm studio to focus on the heavy lifting)&lt;/p&gt; &lt;p&gt;--Result&lt;br /&gt; minimal drift on tasks&lt;br /&gt; slight errors on tool call but the model quickly realign itself. A oneshot prompt implimentation of a new feature in my codebase in architect mode resulted in 2 hours of coding unsupervised kilo code auto switches to code mode to impliment after planning in architect mode which is amazing. Thats been my lived experience&lt;/p&gt; &lt;p&gt;EDIT: ministral 3 3b also works okayISH if you are desprate on hardware resources (3.5gb laptop GPU) but it will want to frequently pause and ask you some questions at the slightest hint of anythings it might be unclear on&lt;/p&gt; &lt;p&gt;Feel free to also share your fully localhost setup that also solved long running tasks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express_Quail_1493"&gt; /u/Express_Quail_1493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnslcb/my_local_coding_agent_worked_2_hours_unsupervised/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnslcb/my_local_coding_agent_worked_2_hours_unsupervised/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnslcb/my_local_coding_agent_worked_2_hours_unsupervised/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T04:14:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pn8upp</id>
    <title>NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!</title>
    <updated>2025-12-15T14:34:28+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/"&gt; &lt;img alt="NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!" src="https://preview.redd.it/sic85bvhpd7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d01067e3a3899e680b913799c37c8ef9b609ff4c" title="NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unsloth GGUF: &lt;a href="https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF"&gt;https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nemotron 3 has a 1M context window and the best in class performance for SWE-Bench, reasoning and chat.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sic85bvhpd7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T14:34:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnb824</id>
    <title>Chatterbox Turbo, new open-source voice AI model, just released on Hugging Face</title>
    <updated>2025-12-15T16:08:45+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnb824/chatterbox_turbo_new_opensource_voice_ai_model/"&gt; &lt;img alt="Chatterbox Turbo, new open-source voice AI model, just released on Hugging Face" src="https://external-preview.redd.it/Mno1ZHBiNTg0ZTdnMetROQBwb-dMzbNK88p-4KlSnzkAfcO7Jy5xOmtEL7Fy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea250dde0c1f1556ba5b404754e09373d2c88623" title="Chatterbox Turbo, new open-source voice AI model, just released on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Links:&lt;br /&gt; - Model (PyTorch): &lt;a href="https://huggingface.co/ResembleAI/chatterbox-turbo"&gt;https://huggingface.co/ResembleAI/chatterbox-turbo&lt;/a&gt;&lt;br /&gt; - Model (ONNX): &lt;a href="https://huggingface.co/ResembleAI/chatterbox-turbo-ONNX"&gt;https://huggingface.co/ResembleAI/chatterbox-turbo-ONNX&lt;/a&gt;&lt;br /&gt; - GitHub: &lt;a href="https://github.com/resemble-ai/chatterbox"&gt;https://github.com/resemble-ai/chatterbox&lt;/a&gt;&lt;br /&gt; - Demo: &lt;a href="https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo"&gt;https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6v5yql484e7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnb824/chatterbox_turbo_new_opensource_voice_ai_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnb824/chatterbox_turbo_new_opensource_voice_ai_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T16:08:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnz1je</id>
    <title>support for GLM4V vision encoder has been merged into llama.cpp</title>
    <updated>2025-12-16T10:53:34+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz1je/support_for_glm4v_vision_encoder_has_been_merged/"&gt; &lt;img alt="support for GLM4V vision encoder has been merged into llama.cpp" src="https://external-preview.redd.it/i0ktGuORgovwZVXClbj98qHky3ndOw6pJOFp0qnTifE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d399641787a216379ff3d6b42189093d66157b6" title="support for GLM4V vision encoder has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18042"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz1je/support_for_glm4v_vision_encoder_has_been_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz1je/support_for_glm4v_vision_encoder_has_been_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T10:53:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pny30h</id>
    <title>The Attention Hybrid MoE Architecture is the Future. Now, AI Labs Should Dedicate Resources to Improve Long Context Recall Capabilities.</title>
    <updated>2025-12-16T09:52:11+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using Qwen3-Next-80B-A30 since it was fully supported in Llama.cpp, and I found it to be the best open-weight model I've ever ran locally ((Unsloth)_Qwen3-Next-80B-A3B-Instruct-GGUF-Q6_K_XL). It's also the first model I could run at full context size (256K) on a single RTX3090 (forcing model expert weights onto CPU, obviously) at around 12t/s.&lt;/p&gt; &lt;p&gt;Before, you say &amp;quot;oh, that's so slow&amp;quot;, let me clarify that a 12t/s speed is twice as fast as I can ever read. Also, just last year, people were happy to run llama3-70B at an average speed of 5t/s, and 2 years ago, people were happy to run llama2-7B (8K context size ü§¶‚Äç‚ôÄÔ∏è) at 12t/s.&lt;/p&gt; &lt;p&gt;Today, I tried (Unsloth)_Nemotron-3-Nano-30B-A3B-GGUF-Q8_K_XL at full context size (1M ü§Ø), and the speed is around 12.5t/s (again, forcing model expert weights onto CPU, obviously). The full context uses 12.6GB of VRAM, leaving me with about 11GB of free VRAM üåãü§Ø. I tested it's recall capability up to 80K, and the model is solid, with almost no context degradation that I can tell.&lt;/p&gt; &lt;p&gt;So, if it's not obvious to some already, this Mamba2-Transformer Hybrid MoE architecture is here so stay. AI Labs must now improve models recall capabilities to truly benefit from in-context learning. I am no expert in the field, and please feel free to interject and correct me if I am wrong, but I think if a smaller model is well trained to fully utilize long context to draw conclusions or discover knowledge it was not trained on, if will allow for the shipping of smaller yet capable models.&lt;/p&gt; &lt;p&gt;My point is, we don't need a model that holds all the human knowledge in its weights, but one that is trained to derive or rediscover unseen knowledge and build upon that to solve novel problems. In other words, I think if a model can reason about novel data, it would reuse the same parameters for many domains, dramatically reducing the size of the training corpus needed to reach a given capability ceiling.&lt;/p&gt; &lt;p&gt;I think if this is achieved, we can expect a decrease in training costs and an increase in model intelligence. We might even see a better model generalization very soon.&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pny30h/the_attention_hybrid_moe_architecture_is_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pny30h/the_attention_hybrid_moe_architecture_is_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pny30h/the_attention_hybrid_moe_architecture_is_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T09:52:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnxkvw</id>
    <title>llama.cpp support for Nemotron 3 Nano merged!</title>
    <updated>2025-12-16T09:17:48+00:00</updated>
    <author>
      <name>/u/QuackerEnte</name>
      <uri>https://old.reddit.com/user/QuackerEnte</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7418"&gt;https://github.com/ggml-org/llama.cpp/releases/tag/b7418&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Details&lt;/p&gt; &lt;p&gt;llama : add support for NVIDIA Nemotron 3 Nano (#18058)&lt;/p&gt; &lt;p&gt;llama : add support for NVIDIA Nemotron Nano 3 This commit adds support for the NVIDIA Nemotron Nano 3 model, enabling the conversion and running of this model.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuackerEnte"&gt; /u/QuackerEnte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxkvw/llamacpp_support_for_nemotron_3_nano_merged/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxkvw/llamacpp_support_for_nemotron_3_nano_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxkvw/llamacpp_support_for_nemotron_3_nano_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T09:17:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnz80z</id>
    <title>I may have over-quantized this little guy.</title>
    <updated>2025-12-16T11:04:26+00:00</updated>
    <author>
      <name>/u/AllergicToTeeth</name>
      <uri>https://old.reddit.com/user/AllergicToTeeth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz80z/i_may_have_overquantized_this_little_guy/"&gt; &lt;img alt="I may have over-quantized this little guy." src="https://preview.redd.it/35p9o4zosj7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f20379a29c30a291fbfb4ccd8cb2c67757d7a55" title="I may have over-quantized this little guy." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AllergicToTeeth"&gt; /u/AllergicToTeeth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/35p9o4zosj7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz80z/i_may_have_overquantized_this_little_guy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz80z/i_may_have_overquantized_this_little_guy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T11:04:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1po18y9</id>
    <title>GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)</title>
    <updated>2025-12-16T12:56:27+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/"&gt; &lt;img alt="GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)" src="https://external-preview.redd.it/bLrsVXDvN3_NMKaZkcGBPVdeuTpEZp7rVIyw-KAF9KY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc4cc8f4e345c1545572514e6454ad7fa760089d" title="GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;you need this&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pnz1je/support_for_glm4v_vision_encoder_has_been_merged/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1pnz1je/support_for_glm4v_vision_encoder_has_been_merged/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ggml-org/glm-4v"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T12:56:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnusp9</id>
    <title>Alibaba Open-Sources CosyVoice 3, a New TTS Model</title>
    <updated>2025-12-16T06:16:09+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Key Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Language Coverage&lt;/strong&gt;: Covers 9 common languages (Chinese, English, Japanese, Korean, German, Spanish, French, Italian, Russian), 18+ Chinese dialects/accents and meanwhile supports both multi-lingual/cross-lingual zero-shot voice cloning.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Content Consistency &amp;amp; Naturalness&lt;/strong&gt;: Achieves state-of-the-art performance in content consistency, speaker similarity, and prosody naturalness.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pronunciation Inpainting&lt;/strong&gt;: Supports pronunciation inpainting of Chinese Pinyin and English CMU phonemes, providing more controllability and thus suitable for production use.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text Normalization&lt;/strong&gt;: Supports reading of numbers, special symbols and various text formats without a traditional frontend module.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bi-Streaming&lt;/strong&gt;: Support both text-in streaming and audio-out streaming, and achieves latency as low as 150ms while maintaining high-quality audio output.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Instruct Support&lt;/strong&gt;: Supports various instructions such as languages, dialects, emotions, speed, volume, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Weight: &lt;a href="https://huggingface.co/FunAudioLLM/Fun-CosyVoice3-0.5B-2512"&gt;https://huggingface.co/FunAudioLLM/Fun-CosyVoice3-0.5B-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2505.17589"&gt;https://arxiv.org/abs/2505.17589&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T06:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnfaqo</id>
    <title>I'm strong enough to admit that this bugs the hell out of me</title>
    <updated>2025-12-15T18:40:57+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/"&gt; &lt;img alt="I'm strong enough to admit that this bugs the hell out of me" src="https://preview.redd.it/9xkz6sfcxe7g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55149229a153c6c87d61ae1aa53e61a1b3a65df8" title="I'm strong enough to admit that this bugs the hell out of me" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9xkz6sfcxe7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T18:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnz9xu</id>
    <title>Qwen3 Next speed optimization has been merged into llama.cpp</title>
    <updated>2025-12-16T11:07:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/"&gt; &lt;img alt="Qwen3 Next speed optimization has been merged into llama.cpp" src="https://external-preview.redd.it/DvlPrtOQd3Cfpjgulr94g-6gX7cbuY0-dqBY_cGanOg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51f4d927a593a1d76b03526eda2d2fe2ba251bc9" title="Qwen3 Next speed optimization has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17996"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T11:07:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnxekt</id>
    <title>It was Ilya who "closed" OpenAI</title>
    <updated>2025-12-16T09:05:33+00:00</updated>
    <author>
      <name>/u/licuphand</name>
      <uri>https://old.reddit.com/user/licuphand</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/"&gt; &lt;img alt="It was Ilya who &amp;quot;closed&amp;quot; OpenAI" src="https://preview.redd.it/rn6rsl7p7j7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9fd882c08aa9fff702ae363b643c6636cc846267" title="It was Ilya who &amp;quot;closed&amp;quot; OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/licuphand"&gt; /u/licuphand &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rn6rsl7p7j7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T09:05:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pniwfj</id>
    <title>Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams.</title>
    <updated>2025-12-15T21:02:55+00:00</updated>
    <author>
      <name>/u/ai2_official</name>
      <uri>https://old.reddit.com/user/ai2_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt; &lt;img alt="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." src="https://b.thumbs.redditmedia.com/bsv34WIHZXC49Az9mFES5lSIAtaQ2CuLZJ4dCaLxsEY.jpg" title="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tuesday, Dec 16 from 1-2pm PST, join us for an AMA with researchers and engineers from Ai2, the nonprofit AI lab behind the fully open Olmo &amp;amp; Molmo models. &lt;/p&gt; &lt;p&gt;Please feel free to ask your questions now! Our team will begin answering them as soon as the AMA begins. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8"&gt;https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai2_official"&gt; /u/ai2_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T21:02:55+00:00</published>
  </entry>
</feed>
