<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-09T18:08:57+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oskjx9</id>
    <title>"AI Done Right" - in YaCy</title>
    <updated>2025-11-09T14:20:20+00:00</updated>
    <author>
      <name>/u/Sure-Technology6660</name>
      <uri>https://old.reddit.com/user/Sure-Technology6660</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sure-Technology6660"&gt; /u/Sure-Technology6660 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/orbiterlab/status/1987517545595716051"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oskjx9/ai_done_right_in_yacy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oskjx9/ai_done_right_in_yacy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T14:20:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1oslucq</id>
    <title>What's the lowest GPT2 pre-training loss achievable with a 50k vocab on a shoestring budget, say USD250?</title>
    <updated>2025-11-09T15:15:18+00:00</updated>
    <author>
      <name>/u/mistr3ated</name>
      <uri>https://old.reddit.com/user/mistr3ated</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oslucq/whats_the_lowest_gpt2_pretraining_loss_achievable/"&gt; &lt;img alt="What's the lowest GPT2 pre-training loss achievable with a 50k vocab on a shoestring budget, say USD250?" src="https://b.thumbs.redditmedia.com/oveakNSBwZhQIVrjCIaocEYzNAgXPR8eMz-NN0GMgAA.jpg" title="What's the lowest GPT2 pre-training loss achievable with a 50k vocab on a shoestring budget, say USD250?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This describes my first time building a small GPT2 style LLM: &lt;a href="https://psychometrics.ai/llm-training"&gt;https://psychometrics.ai/llm-training&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The compute on the final run was only about $75 but $250 covers all the computing time for the failed runs on AWS. &lt;/p&gt; &lt;p&gt;The 50M par model (8 layers, 8 heads, 512-dim embeddings) on 10GB of OpenWebText plateaued at loss of 4.64 (perplexity 103) after 2 epochs. &lt;/p&gt; &lt;p&gt;The loss is too high for anything other than learning, which is why I call it Seedling. The completions are grammatically ok but incoherent:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The best career advice i ever received is: to make sure you're not going anywhere. This is to provide you with the necessary tools to show off your skills and get more training, as well as less awareness about the game.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;I’m gearing up for another run and would love input on where to focus improvements. Possible changes:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Adjusting vocab size to nearest multiple of 64 for tensor alignment&lt;/li&gt; &lt;li&gt;Going deeper/wider (but how many layers and what side?)&lt;/li&gt; &lt;li&gt;Streaming a larger dataset (e.g., 20 GB instead of epochs)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What would you prioritize, and what’s the lowest loss you’d expect possible for about $250 of compute?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sr0mp14zt80g1.png?width=904&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ce8dd163edcd76a3db798129313f751c1b37d91"&gt;Seedling LLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mistr3ated"&gt; /u/mistr3ated &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oslucq/whats_the_lowest_gpt2_pretraining_loss_achievable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oslucq/whats_the_lowest_gpt2_pretraining_loss_achievable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oslucq/whats_the_lowest_gpt2_pretraining_loss_achievable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T15:15:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1os5uur</id>
    <title>Locally running LLMs on DGX Spark as an attorney?</title>
    <updated>2025-11-09T01:01:07+00:00</updated>
    <author>
      <name>/u/Viaprato</name>
      <uri>https://old.reddit.com/user/Viaprato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm an attorney and under our applicable professional rules (non US), I'm not allowed to upload client data to LLM servers to maintain absolute confidentiality.&lt;/p&gt; &lt;p&gt;Is it a good idea to get the Lenovo DGX Spark and run Llama 3.1 70B or Qwen 2.5 72B on it for example to review large amount of documents (e.g. 1000 contracts) for specific clauses or to summarize e.g. purchase prices mentioned in these documents?&lt;/p&gt; &lt;p&gt;Context windows on the device are small (~130,000 tokens which are about 200 pages), but with &amp;quot;RAG&amp;quot; using Open WebUI it seems to still be possible to analyze much larger amounts of data.&lt;/p&gt; &lt;p&gt;I am a heavy user of AI consumer models, but have never used linux, I can't code and don't have much time to set things up.&lt;/p&gt; &lt;p&gt;Also I am concerned with performance since GPT has become much better with GPT-5 and in particular perplexity, seemingly using claude sonnet 4.5, is mostly superior over gpt-5. i can't use these newest models but would have to use llama 3.1 or qwen 3.2. &lt;/p&gt; &lt;p&gt;What do you think, will this work well?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Viaprato"&gt; /u/Viaprato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os5uur/locally_running_llms_on_dgx_spark_as_an_attorney/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os5uur/locally_running_llms_on_dgx_spark_as_an_attorney/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1os5uur/locally_running_llms_on_dgx_spark_as_an_attorney/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T01:01:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1orwvvj</id>
    <title>Another day, another model - But does it really matter to everyday users?</title>
    <updated>2025-11-08T18:36:18+00:00</updated>
    <author>
      <name>/u/PumpkinNarrow6339</name>
      <uri>https://old.reddit.com/user/PumpkinNarrow6339</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orwvvj/another_day_another_model_but_does_it_really/"&gt; &lt;img alt="Another day, another model - But does it really matter to everyday users?" src="https://preview.redd.it/8tf3l0pyu20g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3a59b3b2fd820136d7af927e2e8fb8f5f6c1611" title="Another day, another model - But does it really matter to everyday users?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We see new models dropping almost every week now, each claiming to beat the previous ones on benchmarks. Kimi 2 (the new thinking model from Chinese company Moonshot AI) just posted these impressive numbers on Humanity's Last Exam:&lt;/p&gt; &lt;p&gt;Agentic Reasoning Benchmark: - Kimi 2: 44.9&lt;/p&gt; &lt;p&gt;Here's what I've been thinking: For most regular users, benchmarks don't matter anymore.&lt;/p&gt; &lt;p&gt;When I use an AI model, I don't care if it scored 44.9 or 41.7 on some test. I care about one thing: Did it solve MY problem correctly?&lt;/p&gt; &lt;p&gt;The answer quality matters, not which model delivered it.&lt;/p&gt; &lt;p&gt;Sure, developers and researchers obsess over these numbers - and I totally get why. Benchmarks help them understand capabilities, limitations, and progress. That's their job.&lt;/p&gt; &lt;p&gt;But for us? The everyday users who are actually the end consumers of these models? We just want: - Accurate answers - Fast responses&lt;br /&gt; - Solutions that work for our specific use case&lt;/p&gt; &lt;p&gt;Maybe I'm missing something here, but it feels like we're in a weird phase where companies are in a benchmark arms race, while actual users are just vibing with whichever model gets their work done.&lt;/p&gt; &lt;p&gt;What do you think? Am I oversimplifying this, or do benchmarks really not matter much for regular users anymore?&lt;/p&gt; &lt;p&gt;Source: Moonshot AI's Kimi 2 thinking model benchmark results&lt;/p&gt; &lt;p&gt;TL;DR: New models keep topping benchmarks, but users don't care about scores just whether it solves their problem. Benchmarks are for devs; users just want results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PumpkinNarrow6339"&gt; /u/PumpkinNarrow6339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8tf3l0pyu20g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orwvvj/another_day_another_model_but_does_it_really/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orwvvj/another_day_another_model_but_does_it_really/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T18:36:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1osi8m7</id>
    <title>Trying to break into open-source LLMs in 2 months — need roadmap + hardware advice</title>
    <updated>2025-11-09T12:34:15+00:00</updated>
    <author>
      <name>/u/Expert-Highlight-538</name>
      <uri>https://old.reddit.com/user/Expert-Highlight-538</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I’ve been working as a full-stack dev and mostly using closed-source LLMs (OpenAI, Anthropic etc) just RAG and prompting nothing deep. Lately I’ve been super interested in the open-source side (Llama, Mistral, Ollama, vLLM etc) and want to actually learn how to do fine-tuning, serving, optimizing and all that.&lt;/p&gt; &lt;p&gt;Found The Smol Training Playbook from Hugging Face (that ~220-page guide to training world-class LLMs) it looks awesome but also a bit over my head right now. Trying to figure out what I should learn first before diving into it.&lt;/p&gt; &lt;p&gt;My setup: • Ryzen 7 5700X3D • RTX 2060 Super (8GB VRAM) • 32 GB DDR4 RAM I’m thinking about grabbing a used 3090 to play around with local models.&lt;/p&gt; &lt;p&gt;So I’d love your thoughts on:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;A rough 2-month roadmap to get from “just prompting” → “actually building and fine-tuning open models.”&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What technical skills matter most for employability in this space right now.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Any hardware or setup tips for local LLM experimentation.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;And what prereqs I should hit before tackling the Smol Playbook.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Appreciate any pointers, resources or personal tips as I'm trying to go all in for the next two months.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expert-Highlight-538"&gt; /u/Expert-Highlight-538 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osi8m7/trying_to_break_into_opensource_llms_in_2_months/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osi8m7/trying_to_break_into_opensource_llms_in_2_months/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osi8m7/trying_to_break_into_opensource_llms_in_2_months/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T12:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1osdbxz</id>
    <title>Full Stack Local Deep Research Agent</title>
    <updated>2025-11-09T07:38:12+00:00</updated>
    <author>
      <name>/u/Fun-Wolf-2007</name>
      <uri>https://old.reddit.com/user/Fun-Wolf-2007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/anilsharmay/full-stack-local-deep-research-agent"&gt;https://github.com/anilsharmay/full-stack-local-deep-research-agent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Wolf-2007"&gt; /u/Fun-Wolf-2007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osdbxz/full_stack_local_deep_research_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osdbxz/full_stack_local_deep_research_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osdbxz/full_stack_local_deep_research_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T07:38:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1osns7l</id>
    <title>Mixing 3090s and mi60 on same machine in containers?</title>
    <updated>2025-11-09T16:33:31+00:00</updated>
    <author>
      <name>/u/Salt_Armadillo8884</name>
      <uri>https://old.reddit.com/user/Salt_Armadillo8884</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have two 3090s and considering a third. However thinking about dual mi60s for the same price as a third and using a container to run rocm models. Whilst I cannot combine the ram I could run two separate models. &lt;/p&gt; &lt;p&gt;Was a post a while back about having these in the same machine, but thought this would be cleaner?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salt_Armadillo8884"&gt; /u/Salt_Armadillo8884 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osns7l/mixing_3090s_and_mi60_on_same_machine_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osns7l/mixing_3090s_and_mi60_on_same_machine_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osns7l/mixing_3090s_and_mi60_on_same_machine_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T16:33:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1osnv17</id>
    <title>We made a multi-agent framework . Here’s the demo. Break it harder.</title>
    <updated>2025-11-09T16:36:34+00:00</updated>
    <author>
      <name>/u/wikkid_lizard</name>
      <uri>https://old.reddit.com/user/wikkid_lizard</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osnv17/we_made_a_multiagent_framework_heres_the_demo/"&gt; &lt;img alt="We made a multi-agent framework . Here’s the demo. Break it harder." src="https://external-preview.redd.it/4BEVVDWWk0p8l-rmKtxNg3NJqXG3x5Xaq8LtHSZqFmg.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e3a4d662253c1885f12d0b7f2ea6294a2977e1e3" title="We made a multi-agent framework . Here’s the demo. Break it harder." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since we dropped Laddr about a week ago, a bunch of people on our last post said “cool idea, but show it actually working.”&lt;br /&gt; So we put together a short demo of how to get started with Laddr.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo video:&lt;/strong&gt; &lt;a href="https://www.youtube.com/watch?v=ISeaVNfH4aM"&gt;https://www.youtube.com/watch?v=ISeaVNfH4aM&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/AgnetLabs/laddr"&gt;https://github.com/AgnetLabs/laddr&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Docs:&lt;/strong&gt; &lt;a href="https://laddr.agnetlabs.com"&gt;https://laddr.agnetlabs.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to try weird workflows, force edge cases, or just totally break the orchestration logic.&lt;br /&gt; We’re actively improving based on what hurts.&lt;/p&gt; &lt;p&gt;Also, tell us what you want to see Laddr do next.&lt;br /&gt; Browser agent? research assistant? something chaotic?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wikkid_lizard"&gt; /u/wikkid_lizard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=ISeaVNfH4aM"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osnv17/we_made_a_multiagent_framework_heres_the_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osnv17/we_made_a_multiagent_framework_heres_the_demo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T16:36:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1os8fmn</id>
    <title>Any news about DeepSeek R2?</title>
    <updated>2025-11-09T03:07:01+00:00</updated>
    <author>
      <name>/u/Sorry_Ad191</name>
      <uri>https://old.reddit.com/user/Sorry_Ad191</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os8fmn/any_news_about_deepseek_r2/"&gt; &lt;img alt="Any news about DeepSeek R2?" src="https://b.thumbs.redditmedia.com/PlhiSr0p8aXYM8RnTcq4VE2UHze85d2iBHWp0VLED0M.jpg" title="Any news about DeepSeek R2?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/xbf0p2vod50g1.png?width=450&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a63ccc2bd5ae577ce29167d3995c88a6fd898f22"&gt;Holiday wish: 300B release for community pls :)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Oh my can't even imagine the joy and enthusiasm when/if released!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sorry_Ad191"&gt; /u/Sorry_Ad191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os8fmn/any_news_about_deepseek_r2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os8fmn/any_news_about_deepseek_r2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1os8fmn/any_news_about_deepseek_r2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T03:07:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1orusbs</id>
    <title>Here comes another bubble (AI edition)</title>
    <updated>2025-11-08T17:12:45+00:00</updated>
    <author>
      <name>/u/Parking-Recipe-9003</name>
      <uri>https://old.reddit.com/user/Parking-Recipe-9003</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orusbs/here_comes_another_bubble_ai_edition/"&gt; &lt;img alt="Here comes another bubble (AI edition)" src="https://external-preview.redd.it/NDBoMGRhcjBnMjBnMeOwxEKY_BwUmvv0yJlvuSQnrkHkZJuTTKSVmRt4UrhV.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50d9634803859b13dbbfac1614c7af5d9f6931e2" title="Here comes another bubble (AI edition)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Parking-Recipe-9003"&gt; /u/Parking-Recipe-9003 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bnjv3qq0g20g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orusbs/here_comes_another_bubble_ai_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orusbs/here_comes_another_bubble_ai_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T17:12:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ormxoq</id>
    <title>Kimi K2 Thinking was trained with only $4.6 million</title>
    <updated>2025-11-08T11:16:59+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"&gt; &lt;img alt="Kimi K2 Thinking was trained with only $4.6 million" src="https://a.thumbs.redditmedia.com/L1ScXXMSwvWG0gMuGlVV23jh1shgAs9PlFDsIKJOn94.jpg" title="Kimi K2 Thinking was trained with only $4.6 million" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI: &amp;quot;We need government support to cover $1.4 trillion in chips and data centers.&amp;quot;&lt;/p&gt; &lt;p&gt;Kimi:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/03swwvpfo00g1.png?width=1199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f97e99e5a5aac5bb23a62ff58df048f5db678e09"&gt;https://preview.redd.it/03swwvpfo00g1.png?width=1199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f97e99e5a5aac5bb23a62ff58df048f5db678e09&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T11:16:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oskxx9</id>
    <title>Any decent TTS that runs for AMD that runs on llama.cpp?</title>
    <updated>2025-11-09T14:36:57+00:00</updated>
    <author>
      <name>/u/simracerman</name>
      <uri>https://old.reddit.com/user/simracerman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The search for Kokoro like quality and speed for a TTS that runs on AMD and llama.cpp has proven quite difficult.&lt;/p&gt; &lt;p&gt;Currently, only Kokoro on CPU offers the quality and runs decently enough on CPU. If they supported AMD GPUs or even the AMD NPU, I’d be grateful. There just seems no way to do that now.&lt;/p&gt; &lt;p&gt;What are you using?&lt;/p&gt; &lt;p&gt;EDIT: I’m on Windows, running Docker with WSL2. I can run Linux but prefer to keep my Windows setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simracerman"&gt; /u/simracerman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oskxx9/any_decent_tts_that_runs_for_amd_that_runs_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oskxx9/any_decent_tts_that_runs_for_amd_that_runs_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oskxx9/any_decent_tts_that_runs_for_amd_that_runs_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T14:36:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1osl5o0</id>
    <title>Help Pick the Funniest LLM at Funny Arena</title>
    <updated>2025-11-09T14:46:21+00:00</updated>
    <author>
      <name>/u/demegir</name>
      <uri>https://old.reddit.com/user/demegir</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osl5o0/help_pick_the_funniest_llm_at_funny_arena/"&gt; &lt;img alt="Help Pick the Funniest LLM at Funny Arena" src="https://b.thumbs.redditmedia.com/nsK7MKtq6rc9Ozw7ASglzA6mi7PoppBIC6C33_P5W2g.jpg" title="Help Pick the Funniest LLM at Funny Arena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created this joke arena to determine the least unfunny LLM. Yes, they regurgitate jokes on the internet but some are funnier than others and the jokes gives a peek into their 'personality'. Right now we have grok-4-fast at #1.&lt;/p&gt; &lt;p&gt;Vote at &lt;a href="https://demegire.com/funny-arena/"&gt;https://demegire.com/funny-arena/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can view the code for generating the jokes and the website at &lt;a href="https://github.com/demegire/funny-arena"&gt;https://github.com/demegire/funny-arena&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/demegir"&gt; /u/demegir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1osl5o0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osl5o0/help_pick_the_funniest_llm_at_funny_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osl5o0/help_pick_the_funniest_llm_at_funny_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T14:46:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1osmemg</id>
    <title>Hobby level workstation: build advice</title>
    <updated>2025-11-09T15:39:01+00:00</updated>
    <author>
      <name>/u/LDM-88</name>
      <uri>https://old.reddit.com/user/LDM-88</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m looking for some advice on building a small workstation that sits separately to my main PC.&lt;/p&gt; &lt;p&gt;Its primary use-case would be to serve LLMs locally and perform some hobby-grade fine-tuning. Its secondary use case would be as a means of storage and if possible, a very simple home-server for a handful of devices.&lt;/p&gt; &lt;p&gt;I’ve upgraded my main PC recently and subsequently have a few spare parts I could utilise:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ryzen 5 3600 6-core CPU&lt;/li&gt; &lt;li&gt;16GB DDR4 2933Mhz RAM&lt;/li&gt; &lt;li&gt;B450+ AM4 Motherboard&lt;/li&gt; &lt;li&gt;550W PSU&lt;/li&gt; &lt;li&gt;8GB Radeon RX590 GPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My question is – outside of the GPU, are any of these parts good enough for such a hobby-grade workstation? I’m aware the GPU would need updating, so any advice on which cards to look at here would be much appreciated too! Given that hobbying is mostly about experimentation, i'll probably dive into the used market for additional hardware. &lt;/p&gt; &lt;p&gt;Also – my understanding is that NVIDIA are still light years ahead of AMD in terms of AI support through CUDA using frameworks such as PyTorch, HF, Unsloth, etc. Is that still the case, or is it worth exploring AMD cards too&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LDM-88"&gt; /u/LDM-88 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osmemg/hobby_level_workstation_build_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osmemg/hobby_level_workstation_build_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osmemg/hobby_level_workstation_build_advice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T15:39:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1osmnlp</id>
    <title>CodeWiki: Research-Grade Repository Documentation at Scale [Open Source]</title>
    <updated>2025-11-09T15:49:05+00:00</updated>
    <author>
      <name>/u/Prize_Cost_7706</name>
      <uri>https://old.reddit.com/user/Prize_Cost_7706</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osmnlp/codewiki_researchgrade_repository_documentation/"&gt; &lt;img alt="CodeWiki: Research-Grade Repository Documentation at Scale [Open Source]" src="https://external-preview.redd.it/OWYxbGQ4MHg1OTBnMQhj1jeEZTm3kJ-2tjF6W5cKTD-ayqoaa3bxlqaqIxQQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9da75c2f2ce7867337a918970b57e5d6235e9ee4" title="CodeWiki: Research-Grade Repository Documentation at Scale [Open Source]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; communities! I'm excited to share &lt;strong&gt;CodeWiki&lt;/strong&gt;, our newly published research project from FSoft-AI4Code that tackles automated repository-level documentation generation. After seeing DeepWiki and its open-source implementations, we thought the community might appreciate a different approach backed by academic research.&lt;/p&gt; &lt;h1&gt;What is CodeWiki?&lt;/h1&gt; &lt;p&gt;CodeWiki is the first &lt;strong&gt;semi-agentic framework&lt;/strong&gt; specifically designed for comprehensive, repository-level documentation across 7 programming languages (Python, Java, JavaScript, TypeScript, C, C++, C#). Currently submitted to ACL ARR 2025. &lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href="https://github.com/FSoft-AI4Code/CodeWiki"&gt;FSoft-AI4Code/CodeWiki&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;How is CodeWiki Different from DeepWiki?&lt;/h1&gt; &lt;p&gt;I've researched both &lt;a href="https://github.com/AsyncFuncAI/deepwiki-open"&gt;AsyncFuncAI/deepwiki-open&lt;/a&gt; and &lt;a href="https://github.com/AIDotNet/OpenDeepWiki"&gt;AIDotNet/OpenDeepWiki&lt;/a&gt;, and here's an honest comparison:&lt;/p&gt; &lt;h1&gt;CodeWiki's Unique Approach:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Hierarchical Decomposition with Dependency Analysis&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Uses static analysis + AST parsing (Tree-Sitter) to build dependency graphs&lt;/li&gt; &lt;li&gt;Identifies architectural entry points and recursively partitions modules&lt;/li&gt; &lt;li&gt;Maintains architectural coherence while scaling to repositories of any size&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Recursive Agentic Processing with Dynamic Delegation&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Agents can dynamically delegate complex sub-modules to specialized sub-agents- Bounded complexity handling through recursive bottom-up processing&lt;/li&gt; &lt;li&gt;Cross-module coherence via intelligent reference management&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Research-Backed Evaluation (CodeWikiBench)&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;First benchmark specifically for repository-level documentation&lt;/li&gt; &lt;li&gt;Hierarchical rubric generation from official docs- Multi-model agentic assessment with reliability metrics&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Outperforms closed-source DeepWiki by 4.73% on average&lt;/strong&gt; (68.79% vs 64.06%)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Differences:&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;CodeWiki&lt;/th&gt; &lt;th align="left"&gt;DeepWiki (Open Source)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Core Focus&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Architectural understanding &amp;amp; scalability&lt;/td&gt; &lt;td align="left"&gt;Quick documentation generation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Dependency-driven hierarchical decomposition&lt;/td&gt; &lt;td align="left"&gt;Direct code analysis&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Agent System&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Recursive delegation with specialized sub-agents&lt;/td&gt; &lt;td align="left"&gt;Single-pass generation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Academic benchmark (CodeWikiBench)&lt;/td&gt; &lt;td align="left"&gt;User-facing features&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Performance Highlights&lt;/h1&gt; &lt;p&gt;On 21 diverse repositories (86K to 1.4M LOC):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;TypeScript&lt;/strong&gt;: +18.54% over DeepWiki&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: +9.41% over DeepWiki&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scripting languages avg&lt;/strong&gt;: 79.14% (vs DeepWiki's 68.67%)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Consistent cross-language&lt;/strong&gt; generalization&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What's Next?&lt;/h1&gt; &lt;p&gt;We are actively working on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Enhanced systems language support&lt;/li&gt; &lt;li&gt;Multi-version documentation tracking&lt;/li&gt; &lt;li&gt;Downstream SE task integration (code migration, bug localization, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear your thoughts, especially from folks who've tried the DeepWiki implementations! What features matter most for automated documentation in your workflows?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prize_Cost_7706"&gt; /u/Prize_Cost_7706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/98rdo80x590g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osmnlp/codewiki_researchgrade_repository_documentation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osmnlp/codewiki_researchgrade_repository_documentation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T15:49:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1orw0fz</id>
    <title>I've been trying to make a real production service that uses LLM and it turned into a pure agony. Here are some of my "experiences".</title>
    <updated>2025-11-08T18:02:08+00:00</updated>
    <author>
      <name>/u/DaniyarQQQ</name>
      <uri>https://old.reddit.com/user/DaniyarQQQ</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone. I hope this won't be an off topic, but I want to share my experience in creating real production service. Like a real deal, that will earn money. &lt;/p&gt; &lt;p&gt;For this service I've been using &lt;strong&gt;ChatGPT-5&lt;/strong&gt; and &lt;strong&gt;Claude Haiku 4.5&lt;/strong&gt; but I think this could be suitable for other LLMs too. &lt;/p&gt; &lt;p&gt;The idea was as simple as rock. Make an assistant bot that will communicate with people and make a scheduled appointments to the doctor. &lt;/p&gt; &lt;p&gt;Well in a short time I've implemented everything. The vector database that will inject doctor specific knowledge to the conversation at the right time. Multiple tools that will work with doctors data, and couple other integrations. I've extensively made very detailed system prompt, and each tool call returns instructive results. Each tools' parameters' descriptions were written in very detailed way. After testing for a week we finally deployed on production and started to receive conversations from real people.&lt;/p&gt; &lt;p&gt;And then real life had showed a lot of annoying and downright frustrating caveats of these LLMs.&lt;/p&gt; &lt;p&gt;The first frustrating thing is that &lt;strong&gt;LLMs makes an assumptions&lt;/strong&gt; &lt;strong&gt;without calling required tool, which deceives people.&lt;/strong&gt; It happened like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: Please give me an address where this doctor will be on tomorrow. LLM: Tomorrow is sunday, which is weekend, doctor is unavalable. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There is a tool that explicitly returns that address, and doctor actually works on Sundays. It did not call that tool and made up a bullshit excuse. Then I have emulated this question again by myself:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Me: Give me address where this doctor will be on tomorrow. LLM: &amp;lt;DID NOT CALL THE TOOL&amp;gt;. Tomorrow is sunday, which is weekend, doctor is unavalable. Me: Are you sure about that? LLM: &amp;lt;Finally starts calling the tool which returns address for tomorrow and outputs this address.&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This happens always. No matter what kind of prompts you write, telling it not make any assumptions without any tool calls it still made ups bullshit, which deceives people. Even if you explicitly inject the constraints, it fights them and keeps its own bullshit. &lt;/p&gt; &lt;p&gt;Another problem is close to the first one. &lt;strong&gt;LLMs always agrees with requests without calling tools which confuses people.&lt;/strong&gt; Which looks something like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: I want an appointment for tomorrow. Is it possible. LLM: Of course you can make an appointment. I've prepared everything. What time? User: At 18:00 LLM: &amp;lt;Makes a tool call which returns negative result that next day is unavailable&amp;gt;. I'm sorry tomorrow is unavailable. User: WTF? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Instead of asking proper question before agreeing, it agrees and then shits itself, confusing the user. Especially ChatGPT-5 has this problem, Claude is more rarer but still could shit itself.&lt;/p&gt; &lt;p&gt;And another problem is that &lt;strong&gt;LLMs output text which is complete opposite of it's tool results.&lt;/strong&gt; I've seen this only single time, but I'm now getting paranoid that this could be happening for a long time. It looks something like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: I want an appointment for tomorrow at 18:00. LLM: &amp;lt;Makes a call to the tool that returns that it is impossible for this user to make an appointment, because user has another pending appointment&amp;gt; LLM: Good. I've scheduled your appointment for tomorrow at 18:00. Don't forget about that &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That was an epic failure, LLM completely lied it's own results. I don't even know what to say about that.&lt;/p&gt; &lt;p&gt;And finally the funny one. &lt;strong&gt;Looks like ChatGPT does not like that tools can return negative results, and it keeps calling until it fully overloads all context and finally shits itself.&lt;/strong&gt; It looks something like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: I want an appointment for next friday at 18:00 LLM: &amp;lt;Calls a tool for available window for next friday. No available window&amp;gt; LLM: &amp;lt;Calls again this tool but for the next next friday. No available window&amp;gt; LLM: &amp;lt;Cals AGAIN this tool but for the next next next friday. No available window&amp;gt; ------- And so on and so on | By the way, this doctor does not work on fridays, it was explicitly given in a system prompt, but ChatGPT wants to persevere. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These problems are fixable. You can make even more detailed prompts. Make tools return better and understandable results. You can tune some of LLM parameters. However it is game of whack-a-mole, frustrating one. You fix one thing, another thing comes out. I think some of these models, at least ChatGPT and Claude, were so overly trained on positivity, that they generate deceiving or downright wrong results.&lt;/p&gt; &lt;p&gt;Currently It seems to be that these LLMs can at mostly do their jobs correctly, but these fails, even if they happen rarely, are completely negating all of their reliability. It is not a wonderful magic thing that can solve everything. It is very finnicky (and sometimes very frustrating) tool, that maybe can do what you want. You think you have prepared it for everything, but users can make it shit itself just with a single sentence. &lt;/p&gt; &lt;p&gt;At least I've learned a lot, from these models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DaniyarQQQ"&gt; /u/DaniyarQQQ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orw0fz/ive_been_trying_to_make_a_real_production_service/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orw0fz/ive_been_trying_to_make_a_real_production_service/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orw0fz/ive_been_trying_to_make_a_real_production_service/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T18:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1oskcfd</id>
    <title>Does repurposing this older PC make any sense?</title>
    <updated>2025-11-09T14:11:02+00:00</updated>
    <author>
      <name>/u/Valuable-Question706</name>
      <uri>https://old.reddit.com/user/Valuable-Question706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My goal is to run models locally for coding (only for some tasks that require privacy, not all). &lt;/p&gt; &lt;p&gt;&lt;em&gt;So far, I’m happy with Qwen3-Coder-30b-A3B level of results.&lt;/em&gt; It runs on my current machine (32RAM+8VRAM) at ~4-6 tokens/s. But it takes the larger part of my RAM - this is what I’m not happy with.&lt;/p&gt; &lt;p&gt;I also have a ~10yr old PC with PCIe 3.0 motherboard, 48GB DDR4 RAM, 5th gen i7 CPU and 9xx-series GPU with 4GB RAM.&lt;/p&gt; &lt;p&gt;I’m thinking of upgrading it with a modern 16GB GPU and setting it up as a dedicated inference server. Also, maybe maxing up RAM to 64 that this system supports. &lt;/p&gt; &lt;p&gt;First, does it make any sense model-wise? &lt;strong&gt;Are there any models with much better output in this RAM+VRAM range?&lt;/strong&gt; Or you need to go much higher (120+) for something &lt;em&gt;not marginally&lt;/em&gt; better?&lt;/p&gt; &lt;p&gt;Second, does a modern GPU make any sense for such a machine?&lt;/p&gt; &lt;p&gt;&lt;em&gt;Where I live, only reasonable 16GB options available are newer PCIe 5.0 GPUs, like 5060 Ti, and higher. Nobody’s selling their older 8-16GB GPUs here yet.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Valuable-Question706"&gt; /u/Valuable-Question706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oskcfd/does_repurposing_this_older_pc_make_any_sense/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oskcfd/does_repurposing_this_older_pc_make_any_sense/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oskcfd/does_repurposing_this_older_pc_make_any_sense/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T14:11:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1osd8ie</id>
    <title>PSA Kimi K2 Thinking seems to currently be broken for most agents because of tool calling within it's thinking tags</title>
    <updated>2025-11-09T07:32:05+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yeah, just what the title says. If any of you are having issues with coding using K2 thinking it's because of this. Only Kimi CLI really supports it atm. Minimax m2 had a similar issue I think and glm 4.6 too, but this could be worked around by disabling tool_calling in thinking, however this can't be done for K2 thinking, hence all the issues people are having with this model for coding. Hopefully most agents will have this fixed soon. I think this is called interleaved thinking, or is something similar to that? Feel free to shed some light on this in the comments if you're more familiar with what's going on.&lt;/p&gt; &lt;p&gt;EDIT - I found the issue: &lt;a href="https://github.com/MoonshotAI/Kimi-K2/issues/89"&gt;https://github.com/MoonshotAI/Kimi-K2/issues/89&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's better explained there. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osd8ie/psa_kimi_k2_thinking_seems_to_currently_be_broken/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osd8ie/psa_kimi_k2_thinking_seems_to_currently_be_broken/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osd8ie/psa_kimi_k2_thinking_seems_to_currently_be_broken/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T07:32:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1osh24c</id>
    <title>Best coding agent for GLM-4.6 that's not CC</title>
    <updated>2025-11-09T11:28:22+00:00</updated>
    <author>
      <name>/u/Illustrious-Many-782</name>
      <uri>https://old.reddit.com/user/Illustrious-Many-782</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I already use GLM with Opencode, Claude Code, and Codex CLI, but since I have the one-year z.ai mini plan, I want to use GLM more than I am right now, Is there a better option than OpenCode (that's not Claude Code, because it's being used by Claude)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Many-782"&gt; /u/Illustrious-Many-782 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osh24c/best_coding_agent_for_glm46_thats_not_cc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osh24c/best_coding_agent_for_glm46_thats_not_cc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osh24c/best_coding_agent_for_glm46_thats_not_cc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T11:28:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ortopy</id>
    <title>Kimi K2 Thinking 1-bit Unsloth Dynamic GGUFs</title>
    <updated>2025-11-08T16:28:21+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ortopy/kimi_k2_thinking_1bit_unsloth_dynamic_ggufs/"&gt; &lt;img alt="Kimi K2 Thinking 1-bit Unsloth Dynamic GGUFs" src="https://preview.redd.it/s190tdo2720g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a07ae8f05f136602ecbf12323e286c3cca29d84" title="Kimi K2 Thinking 1-bit Unsloth Dynamic GGUFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! You can now run Kimi K2 Thinking locally with our Unsloth Dynamic 1bit GGUFs. We also collaborated with the Kimi team on a &lt;strong&gt;fix for K2&lt;/strong&gt; &lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Thinking/discussions/12"&gt;&lt;strong&gt;Thinking's chat template&lt;/strong&gt;&lt;/a&gt; not prepending the default system prompt of &lt;code&gt;You are Kimi, an AI assistant created by Moonshot AI.&lt;/code&gt; on the 1st turn.&lt;/p&gt; &lt;p&gt;We also we &lt;strong&gt;fixed llama.cpp custom jinja separators&lt;/strong&gt; for tool calling - Kimi does &lt;code&gt;{&amp;quot;a&amp;quot;:&amp;quot;1&amp;quot;,&amp;quot;b&amp;quot;:&amp;quot;2&amp;quot;}&lt;/code&gt; and not with extra spaces like &lt;code&gt;{&amp;quot;a&amp;quot;: &amp;quot;1&amp;quot;, &amp;quot;b&amp;quot;: &amp;quot;2&amp;quot;}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The 1-bit GGUF will run on 247GB RAM. We shrank the 1T model to 245GB (-62%) &amp;amp; the accuracy recovery is comparable to our third-party &lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot"&gt;DeepSeek-V3.1 Aider Polyglot benchmarks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All 1bit, 2bit and other bit width GGUFs are at &lt;a href="https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF"&gt;https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The suggested temp is &lt;code&gt;temperature = 1.0&lt;/code&gt;. We also suggest a &lt;code&gt;min_p = 0.01&lt;/code&gt;. If you do not see &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt;, use &lt;code&gt;--special&lt;/code&gt;. The code for llama-cli is below which offloads MoE layers to CPU RAM, and leaves the rest of the model on GPU VRAM:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export LLAMA_CACHE=&amp;quot;unsloth/Kimi-K2-Thinking-GGUF&amp;quot; ./llama.cpp/llama-cli \ -hf unsloth/Kimi-K2-Thinking-GGUF:UD-TQ1_0 \ --n-gpu-layers 99 \ --temp 1.0 \ --min-p 0.01 \ --ctx-size 16384 \ --seed 3407 \ -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Step-by-step Guide + fix details: &lt;a href="https://docs.unsloth.ai/models/kimi-k2-thinking-how-to-run-locally"&gt;https://docs.unsloth.ai/models/kimi-k2-thinking-how-to-run-locally&lt;/a&gt; and GGUFs are &lt;a href="https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Let us know if you have any questions and hope you have a great weekend!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s190tdo2720g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ortopy/kimi_k2_thinking_1bit_unsloth_dynamic_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ortopy/kimi_k2_thinking_1bit_unsloth_dynamic_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T16:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1osiog7</id>
    <title>Qwen3-VL works really good with Zoom-in Tool</title>
    <updated>2025-11-09T12:56:03+00:00</updated>
    <author>
      <name>/u/indigos661</name>
      <uri>https://old.reddit.com/user/indigos661</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osiog7/qwen3vl_works_really_good_with_zoomin_tool/"&gt; &lt;img alt="Qwen3-VL works really good with Zoom-in Tool" src="https://a.thumbs.redditmedia.com/GlsFc4qF0jvYKaVjwuK5hhN3xFcCV40Gl5A3-CDlYE4.jpg" title="Qwen3-VL works really good with Zoom-in Tool" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While Qwen3-VL-30B-A3B(Q6_ud) performs better than previous open-source models in general image recognition, it still has issues with hallucinations and inaccurate recognition. &lt;/p&gt; &lt;p&gt;However, with the zoom_in tool the situation is completely different. On my own frontend implementation with zoom_in, Qwen3-VL can zoom in on the image, significantly improving the accuracy of content recognition. For those who haven't tried it, qwen team has released a reference implementation: &lt;a href="https://github.com/QwenLM/Qwen-Agent/blob/main/examples/cookbook_think_with_images.ipynb"&gt;https://github.com/QwenLM/Qwen-Agent/blob/main/examples/cookbook_think_with_images.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n6zw8zpfa80g1.png?width=1513&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7bffdedb3be6155771d24adcaf1df358854043e2"&gt;https://preview.redd.it/n6zw8zpfa80g1.png?width=1513&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7bffdedb3be6155771d24adcaf1df358854043e2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you are using Qwen3-VL, I strongly recommend using it with this tool.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/indigos661"&gt; /u/indigos661 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osiog7/qwen3vl_works_really_good_with_zoomin_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osiog7/qwen3vl_works_really_good_with_zoomin_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osiog7/qwen3vl_works_really_good_with_zoomin_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T12:56:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1osj4mb</id>
    <title>Worth the switch from Claude to GLM 4.6 for my coding side hustle?</title>
    <updated>2025-11-09T13:16:58+00:00</updated>
    <author>
      <name>/u/Ok_Investigator_5036</name>
      <uri>https://old.reddit.com/user/Ok_Investigator_5036</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been freelancing web development projects for about 8 months now, mostly custom dashboards, client portals, and admin panels. The economics are tough because clients always want &amp;quot;simple&amp;quot; projects that turn into months of iteration hell. (Never trust anything to be &amp;quot;simple&amp;quot;)&lt;/p&gt; &lt;p&gt;I started using Claude API for rapid prototyping and client demos. Problem is my margins were getting narrow, especially when a client would request their fifth redesign of a data visualization component or want to &amp;quot;just tweak&amp;quot; the entire authentication flow.&lt;/p&gt; &lt;p&gt;Someone in a dev Discord mentioned using GLM-4.6 with Claude Code. They were getting 55% off first year, so GLM Coding Pro works out to $13.5/month vs Claude Pro at $20+, with 3x usage quota.&lt;/p&gt; &lt;p&gt;I've tested GLM-4.6's coding output. It seems on par with Claude for most tasks, but with 3x the usage quota. We're talking 600 prompts every 5 hours vs Claude Max's ~200.&lt;/p&gt; &lt;p&gt;My typical project flow:&lt;/p&gt; &lt;p&gt;- Client consultation and mockups&lt;/p&gt; &lt;p&gt;- Use AI to scaffold React components and API routes&lt;/p&gt; &lt;p&gt;- Rapid iteration on UI/UX (this is where the 3x quota matters)&lt;/p&gt; &lt;p&gt;- Testing, refactoring, deployment&lt;/p&gt; &lt;p&gt;Last month I landed three projects: a SaaS dashboard with Stripe integration and two smaller automation tools. But some months it's just one or two projects with endless revision rounds.&lt;/p&gt; &lt;p&gt;Right now my prompt usage is manageable, but I've had months where client iterations alone hit thousands of prompts, especially when they're A/B testing different UI approaches or want real-time previews of changes.&lt;/p&gt; &lt;p&gt;For me, the limiting factor isn't base capability (GLM-4.6 ≈ Claude quality), but having the quota to iterate without stressing about costs.&lt;/p&gt; &lt;p&gt;Wondering how you guys optimizing your AI coding setup costs? With all the client demands and iteration cycles, seems smart to go for affordable with high limits.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Investigator_5036"&gt; /u/Ok_Investigator_5036 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osj4mb/worth_the_switch_from_claude_to_glm_46_for_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osj4mb/worth_the_switch_from_claude_to_glm_46_for_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osj4mb/worth_the_switch_from_claude_to_glm_46_for_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T13:16:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1osmr6i</id>
    <title>Is the RTX 5090 that good of a deal?</title>
    <updated>2025-11-09T15:53:07+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osmr6i/is_the_rtx_5090_that_good_of_a_deal/"&gt; &lt;img alt="Is the RTX 5090 that good of a deal?" src="https://preview.redd.it/v9cx89gr690g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c96c392f6cfc52c3e86aaa74147a35d574967307" title="Is the RTX 5090 that good of a deal?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to find a model agnostic approach to estimate which cards to pick &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v9cx89gr690g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osmr6i/is_the_rtx_5090_that_good_of_a_deal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osmr6i/is_the_rtx_5090_that_good_of_a_deal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T15:53:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1osglws</id>
    <title>Kimi K2 Thinking scores lower than Gemini 2.5 Flash on Livebench</title>
    <updated>2025-11-09T11:01:13+00:00</updated>
    <author>
      <name>/u/ihexx</name>
      <uri>https://old.reddit.com/user/ihexx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osglws/kimi_k2_thinking_scores_lower_than_gemini_25/"&gt; &lt;img alt="Kimi K2 Thinking scores lower than Gemini 2.5 Flash on Livebench" src="https://preview.redd.it/mqize91iq70g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=717d19b768a2a88e4b9a2a6690bbde033a817303" title="Kimi K2 Thinking scores lower than Gemini 2.5 Flash on Livebench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihexx"&gt; /u/ihexx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mqize91iq70g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osglws/kimi_k2_thinking_scores_lower_than_gemini_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osglws/kimi_k2_thinking_scores_lower_than_gemini_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T11:01:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1osnnfn</id>
    <title>How to build an AI computer (version 2.0)</title>
    <updated>2025-11-09T16:28:27+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osnnfn/how_to_build_an_ai_computer_version_20/"&gt; &lt;img alt="How to build an AI computer (version 2.0)" src="https://preview.redd.it/03t3yj51d90g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d467717022f60865bdfcbb8d96bb265e2bdb541" title="How to build an AI computer (version 2.0)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/03t3yj51d90g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osnnfn/how_to_build_an_ai_computer_version_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osnnfn/how_to_build_an_ai_computer_version_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T16:28:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqy1k7</id>
    <title>AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)</title>
    <updated>2025-11-07T15:53:33+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt; &lt;img alt="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" src="https://preview.redd.it/8v2luf5owuzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9bc34ec8dddd94422397eaa91e0310250da5ba3" title="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8v2luf5owuzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:53:33+00:00</published>
  </entry>
</feed>
