<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-07T20:07:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oqxx8w</id>
    <title>Vulnerability Inception: How AI Code Assistants Replicate and Amplify Security Flaws</title>
    <updated>2025-11-07T15:48:49+00:00</updated>
    <author>
      <name>/u/ortegaalfredo</name>
      <uri>https://old.reddit.com/user/ortegaalfredo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqxx8w/vulnerability_inception_how_ai_code_assistants/"&gt; &lt;img alt="Vulnerability Inception: How AI Code Assistants Replicate and Amplify Security Flaws" src="https://external-preview.redd.it/5sm3GRNGqkHPJJHTtLe5heJ60MTx9qDDMNLNZZnboms.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81efb413a983424e2fb3cecba428a0affa68289f" title="Vulnerability Inception: How AI Code Assistants Replicate and Amplify Security Flaws" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm sharing an article about prompt injection in Large Language Models (LLMs), specifically regarding coding and coding agents. The research shows that it's easy to manipulate LLMs into injecting backdoors and vulnerabilities into code, simply by embedding instructions in a comment, as the LLM will follow any instructions it finds in the original source code.&lt;/p&gt; &lt;p&gt;This is relevant to the localLlama community because only one open-weights model, Deepseek 3.2 Exp, appears to be resistant (but not immune) to this vulnerability. It seems to have received specialized training to avoid introducing security flaws. I think this is a significant finding and hope you find it useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ortegaalfredo"&gt; /u/ortegaalfredo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ortegaalfredo/aiweaknesses/blob/main/ai_vulnerabilities_article.pdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqxx8w/vulnerability_inception_how_ai_code_assistants/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqxx8w/vulnerability_inception_how_ai_code_assistants/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:48:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqki9e</id>
    <title>128GB RAM costs ~$1000 &amp; Strix Halo costs $1600 in total</title>
    <updated>2025-11-07T04:04:41+00:00</updated>
    <author>
      <name>/u/johnnytshi</name>
      <uri>https://old.reddit.com/user/johnnytshi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We all know RAM has gone up quite a bit, like: &lt;a href="https://pcpartpicker.com/product/WTMMnQ/corsair-vengeance-rgb-64-gb-2-x-32-gb-ddr5-6000-cl30-memory-cmh64gx5m2b6000c30"&gt;https://pcpartpicker.com/product/WTMMnQ/corsair-vengeance-rgb-64-gb-2-x-32-gb-ddr5-6000-cl30-memory-cmh64gx5m2b6000c30&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How is it possible that Strix Halo with 128GB costs $1699? like &lt;a href="https://www.gmktec.com/products/amd-ryzen%E2%84%A2-ai-max-395-evo-x2-ai-mini-pc?srsltid=AfmBOopMa5dg-W23Ck2BDBNK2wWvPAnToenYsT16yQ-_mreQ8HR7gD9v"&gt;https://www.gmktec.com/products/amd-ryzen%E2%84%A2-ai-max-395-evo-x2-ai-mini-pc?srsltid=AfmBOopMa5dg-W23Ck2BDBNK2wWvPAnToenYsT16yQ-_mreQ8HR7gD9v&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LPDDR5X, 8000MHz &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johnnytshi"&gt; /u/johnnytshi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqki9e/128gb_ram_costs_1000_strix_halo_costs_1600_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqki9e/128gb_ram_costs_1000_strix_halo_costs_1600_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqki9e/128gb_ram_costs_1000_strix_halo_costs_1600_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T04:04:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq8mmy</id>
    <title>Kimi K2 Thinking and DeepSeek R1 Architectures Side by Side</title>
    <updated>2025-11-06T19:37:54+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq8mmy/kimi_k2_thinking_and_deepseek_r1_architectures/"&gt; &lt;img alt="Kimi K2 Thinking and DeepSeek R1 Architectures Side by Side" src="https://b.thumbs.redditmedia.com/bLGXBa7gA85RfSO792H013zd_aNhH4CnZeSr7OxsZVc.jpg" title="Kimi K2 Thinking and DeepSeek R1 Architectures Side by Side" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2 is based on the DeepSeek V3/R1 architecture, and here's a side-by-side comparison.&lt;/p&gt; &lt;p&gt;- 2√ó fewer attention heads (64 vs. 128)&lt;br /&gt; - ~1.5√ó more experts per MoE layer (384 vs. 256)&lt;br /&gt; - Bigger vocabulary (160k vs. 129k)&lt;br /&gt; - K2 activates ~32B parameters per token (vs. 37B in DeepSeek R1)&lt;br /&gt; - Fewer dense FFN blocks before MoE&lt;br /&gt; - 2x longer supported context&lt;/p&gt; &lt;p&gt;In short, Kimi K2 is a slightly scaled DeepSeek V3/R1. And the gains are in the data and training recipes. Hopefully, we will see some details on those soon, too.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/98ghpsqn9pzf1.jpg?width=2200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8ca505a76cd4755ed0b557ac281e621eeac3da9e"&gt;https://preview.redd.it/98ghpsqn9pzf1.jpg?width=2200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8ca505a76cd4755ed0b557ac281e621eeac3da9e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq8mmy/kimi_k2_thinking_and_deepseek_r1_architectures/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq8mmy/kimi_k2_thinking_and_deepseek_r1_architectures/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq8mmy/kimi_k2_thinking_and_deepseek_r1_architectures/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T19:37:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqcd1y</id>
    <title>Just want to take a moment to express gratitude for this tech</title>
    <updated>2025-11-06T22:00:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What a time to be alive! &lt;/p&gt; &lt;p&gt;I was just randomly reflecting today - a single file with just a bunch of numbers can be used to make poems, apps, reports and so much more. And that's just LLMs.. But then this applies to image, video, speech, music, audio, 3D models and whatever else that can be expressed digitally&lt;/p&gt; &lt;p&gt;Anyone can do this with publicly available downloads and software. You dont need sophisticated computers or hardware.&lt;/p&gt; &lt;p&gt;Possibly most insane of all is that you can do all of this for free.&lt;/p&gt; &lt;p&gt;This is just utter insanity. If you had told me this would be the ecosystem before this wave happened, I would have never believed you. Regardless of how things evolve, I think we should be immensely grateful for all of this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqcd1y/just_want_to_take_a_moment_to_express_gratitude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqcd1y/just_want_to_take_a_moment_to_express_gratitude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqcd1y/just_want_to_take_a_moment_to_express_gratitude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T22:00:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1or323v</id>
    <title>Recently built my first LLM and im wondering why there hasn't been more innovation on moving away from transformers and gradient descent?</title>
    <updated>2025-11-07T19:01:17+00:00</updated>
    <author>
      <name>/u/CelebrationMinimum50</name>
      <uri>https://old.reddit.com/user/CelebrationMinimum50</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So please excuse my lack of knowledge in this area as im new to AI/LLMs but I just recently build my first micro llm and I dunno something about them seems wrong.&lt;/p&gt; &lt;p&gt;Is the industry stuck on transformers and gradient descent because coming up with alternatives is a hugely difficult problem or is the industry just having blinders on?&lt;/p&gt; &lt;p&gt;I like a lot of the research about sparse models that use hebbian/oja and i know these come with challenges like catastrophic interference. But this seems like a very solvable problem.&lt;/p&gt; &lt;p&gt;Anyways im starting to tinker with my micro llm to see if I can get rid of gradient descent and traditional transformers and see if I cant make a sparse model based on hebbian/oja at the very least in a small scale&lt;/p&gt; &lt;p&gt;Again pardon my nativity, my expertise is mostly in backend systems and architecture. I have very little exposure to AI/LLMs until recently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CelebrationMinimum50"&gt; /u/CelebrationMinimum50 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or323v/recently_built_my_first_llm_and_im_wondering_why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or323v/recently_built_my_first_llm_and_im_wondering_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or323v/recently_built_my_first_llm_and_im_wondering_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T19:01:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq9ui3</id>
    <title>Microsoft‚Äôs AI Scientist</title>
    <updated>2025-11-06T20:23:52+00:00</updated>
    <author>
      <name>/u/Ok-Breakfast-4676</name>
      <uri>https://old.reddit.com/user/Ok-Breakfast-4676</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9ui3/microsofts_ai_scientist/"&gt; &lt;img alt="Microsoft‚Äôs AI Scientist" src="https://preview.redd.it/jbv9rmub4pzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7b040b383a3c04d5034fca2fe81396d6e5d57a9" title="Microsoft‚Äôs AI Scientist" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft literally just dropped the first AI scientist&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Breakfast-4676"&gt; /u/Ok-Breakfast-4676 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jbv9rmub4pzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9ui3/microsofts_ai_scientist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9ui3/microsofts_ai_scientist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T20:23:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqyix9</id>
    <title>How practical is finetuning larger models with 4x 3090 setup?</title>
    <updated>2025-11-07T16:11:13+00:00</updated>
    <author>
      <name>/u/Specialist-Let9791</name>
      <uri>https://old.reddit.com/user/Specialist-Let9791</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am thinking of building 4x3090 setup cause other options with large VRAM are quite expensive and not worth the buck. For instance, pro 6000 has 96gigs but costs around 10,000. OTH, 3090's VRAM could be pooled together so 4x3090 would have same VRAM (a bit slower though) but significantly cheaper. &lt;/p&gt; &lt;p&gt;Is it practical? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist-Let9791"&gt; /u/Specialist-Let9791 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqyix9/how_practical_is_finetuning_larger_models_with_4x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqyix9/how_practical_is_finetuning_larger_models_with_4x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqyix9/how_practical_is_finetuning_larger_models_with_4x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T16:11:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq9b7e</id>
    <title>Nvidia's Jensen Huang: 'China is going to win the AI race,' FT reports</title>
    <updated>2025-11-06T20:03:28+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9b7e/nvidias_jensen_huang_china_is_going_to_win_the_ai/"&gt; &lt;img alt="Nvidia's Jensen Huang: 'China is going to win the AI race,' FT reports" src="https://external-preview.redd.it/B5kYZqF-LXs8_vBUF8bfaXMktkNYepX59paDPfYv7go.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04b0e3c2929dde65c0820bb5e348487a3bb39955" title="Nvidia's Jensen Huang: 'China is going to win the AI race,' FT reports" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reuters.com/world/asia-pacific/nvidias-jensen-huang-says-china-will-win-ai-race-with-us-ft-reports-2025-11-05/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9b7e/nvidias_jensen_huang_china_is_going_to_win_the_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9b7e/nvidias_jensen_huang_china_is_going_to_win_the_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T20:03:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqpt4w</id>
    <title>Minimax will launch a coding package on November 14th</title>
    <updated>2025-11-07T09:20:02+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqpt4w/minimax_will_launch_a_coding_package_on_november/"&gt; &lt;img alt="Minimax will launch a coding package on November 14th" src="https://b.thumbs.redditmedia.com/vtguBl-F1WEoWi4Kor5zaXQYJwCg2LdpS7SCl2DWLrM.jpg" title="Minimax will launch a coding package on November 14th" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oqpt4w"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqpt4w/minimax_will_launch_a_coding_package_on_november/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqpt4w/minimax_will_launch_a_coding_package_on_november/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T09:20:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqtose</id>
    <title>Sparse Attention MoE - a test repo for a novel swappable attention mechanism</title>
    <updated>2025-11-07T12:56:53+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw someone talking about using a MoE for Attention a few weeks back. At the time, it seemed like nonsense, but something about the post made me fiddle around with it a bit, and I was surprised to find it... worked? Crazier still... it seems to beat regular attention while radically reducing the amount of time and compute needed to train a model in my testing.&lt;/p&gt; &lt;p&gt;This is an experiment I put together for testing Sparse Attention MoE, a novel attention mechanism that reduces self-attention computational complexity. The idea is to create a new drop-in attention mechanism that should work in existing AI training pipelines while radically reducing the amount of compute required (allowing larger models to be trained on smaller devices, for example). Faster training, lower use of resources, and in my testing so far it trains models that outperforms regular dense attention (at least on my small toy model tests).&lt;/p&gt; &lt;p&gt;Normally, MoE routes feed-forward experts. This concept routes attention sparsity levels. By training Attention we are able to get it to identify easy, medium, and hard tokens, allowing it to route them in a way that reduces how much compute is required as a whole.&lt;/p&gt; &lt;p&gt;I've built a small end-to-end test model and provided all the code to train one yourself at this github repo. This demonstrates &lt;strong&gt;O(N¬∑k) attention&lt;/strong&gt; (vs. O(N¬≤)) attention, and allows efficient training since you don't have quadratic blowup on attention. I test-trained a small LLM to see how it would go and saw similar improvement: The adaptive model achieved **12.03% perplexity improvement** over the non-adaptive baseline with **balanced expert usage** (47%/34%/19%) and was **1.7√ó faster to train**. This directly replicates the vision model's success pattern in a different domain, proving the mechanism is **task-general, not vision-specific**.&lt;/p&gt; &lt;p&gt;For now I'm sharing the diffusion version (it's doing a denoise job on cifar data since that's a simplistic task that can be trained in a few minutes on a 4090).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Deveraux-Parker/Adaptive_Sparse_Attention_MoE/tree/main"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqtose/sparse_attention_moe_a_test_repo_for_a_novel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqtose/sparse_attention_moe_a_test_repo_for_a_novel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T12:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1or2077</id>
    <title>Want to Learn More About Agentic AI</title>
    <updated>2025-11-07T18:21:38+00:00</updated>
    <author>
      <name>/u/Superb_Practice_4544</name>
      <uri>https://old.reddit.com/user/Superb_Practice_4544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone ‚Äî I‚Äôve built a few agentic AI systems around SaaS automation and coding tools. I‚Äôm familiar with LangChain, LangGraph, RAG, tool calling, and MCP, but I want to learn more by contributing to real projects.&lt;/p&gt; &lt;p&gt;If you‚Äôre working on something in this space or know an open-source project looking for contributors, I‚Äôd love to help out and learn from it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Superb_Practice_4544"&gt; /u/Superb_Practice_4544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or2077/want_to_learn_more_about_agentic_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or2077/want_to_learn_more_about_agentic_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or2077/want_to_learn_more_about_agentic_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T18:21:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq1arc</id>
    <title>Kimi released Kimi K2 Thinking, an open-source trillion-parameter reasoning model</title>
    <updated>2025-11-06T15:04:59+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"&gt; &lt;img alt="Kimi released Kimi K2 Thinking, an open-source trillion-parameter reasoning model" src="https://b.thumbs.redditmedia.com/NupD3tHHs6sXvqucL46py-jFU7OPNJHTwiCDt_n7fGc.jpg" title="Kimi released Kimi K2 Thinking, an open-source trillion-parameter reasoning model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d01vorgfjnzf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a8f26127a8125731e93b25522a7bcdc28637d6f"&gt;https://preview.redd.it/d01vorgfjnzf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a8f26127a8125731e93b25522a7bcdc28637d6f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech blog:&lt;/strong&gt; &lt;a href="https://moonshotai.github.io/Kimi-K2/thinking.html"&gt;https://moonshotai.github.io/Kimi-K2/thinking.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Weights &amp;amp; code:&lt;/strong&gt; &lt;a href="https://huggingface.co/moonshotai"&gt;https://huggingface.co/moonshotai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T15:04:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqx4dj</id>
    <title>Emergent Occam's Razor: Teaching qwen2.5:7b to learn through journaling (51%‚Üí78%) [Full code + paper]</title>
    <updated>2025-11-07T15:18:06+00:00</updated>
    <author>
      <name>/u/Next_Bid_8339</name>
      <uri>https://old.reddit.com/user/Next_Bid_8339</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just finished an experiment where a 7B model learns through reflection and self-critique - no weight updates, no training data, just journaling about mistakes.&lt;/p&gt; &lt;p&gt;**The surprising part: the model discovered Occam's Razor on its own.**&lt;/p&gt; &lt;p&gt;## The Setup&lt;/p&gt; &lt;p&gt;- Model: qwen2.5:7b (local, via Ollama)&lt;/p&gt; &lt;p&gt;- Task: Meeting room scheduling (constraint satisfaction)&lt;/p&gt; &lt;p&gt;- Method: After each batch, model writes reflective journal and distills strategy&lt;/p&gt; &lt;p&gt;- Hardware: Consumer laptop, no GPU needed&lt;/p&gt; &lt;p&gt;- Runtime: ~40 minutes total&lt;/p&gt; &lt;p&gt;## The Results&lt;/p&gt; &lt;p&gt;| Stage | Accuracy | What Happened |&lt;/p&gt; &lt;p&gt;|-------|----------|---------------|&lt;/p&gt; &lt;p&gt;| Baseline | 51.3% | Zero-shot, weak |&lt;/p&gt; &lt;p&gt;| Bootstrap | 66.0% | Learning phase (messy) |&lt;/p&gt; &lt;p&gt;| Test w/ LRL | 78.0% | **+26.7% improvement!** |&lt;/p&gt; &lt;p&gt;## The Learning Journey (This is the cool part)&lt;/p&gt; &lt;p&gt;**Batches 1-5: &amp;quot;The Over-Engineer&amp;quot;**&lt;/p&gt; &lt;p&gt;Model confidently proposes complex solutions:&lt;/p&gt; &lt;p&gt;- &amp;quot;Implement interval trees!&amp;quot;&lt;/p&gt; &lt;p&gt;- &amp;quot;Apply dynamic programming!&amp;quot;&lt;/p&gt; &lt;p&gt;- &amp;quot;Use graph theory approaches!&amp;quot;&lt;/p&gt; &lt;p&gt;Result: ~35% accuracy. Sophisticated nonsense.&lt;/p&gt; &lt;p&gt;**Batches 6-8: &amp;quot;Seeds of Doubt&amp;quot;**&lt;/p&gt; &lt;p&gt;Journal entries start showing conflict:&lt;/p&gt; &lt;p&gt;&amp;gt; &amp;quot;Since the problem is straightforward, focusing on basic interval checking...&amp;quot;&lt;/p&gt; &lt;p&gt;First time admitting simplicity might be the answer.&lt;/p&gt; &lt;p&gt;**Batches 9-10: &amp;quot;The Awakening&amp;quot;**&lt;/p&gt; &lt;p&gt;The breakthrough journal entry:&lt;/p&gt; &lt;p&gt;&amp;gt; &amp;quot;This suggests a **fundamental misunderstanding** of how to handle overlapping intervals.&amp;quot;&lt;/p&gt; &lt;p&gt;The model admitted it was wrong. Everything changed from there.&lt;/p&gt; &lt;p&gt;## Why This Matters for Local LLMs&lt;/p&gt; &lt;p&gt;‚úÖ **Interpretable** - Read the complete thought process in journals &lt;/p&gt; &lt;p&gt;‚úÖ **Efficient** - No GPU training, pure inference &lt;/p&gt; &lt;p&gt;‚úÖ **Transferable** - Strategies are text files you can share &lt;/p&gt; &lt;p&gt;‚úÖ **Safe** - Models that learn to doubt themselves &lt;/p&gt; &lt;p&gt;The distillation process acts like evolution: ideas that work (simple counting) survive, ideas that fail (graph theory) get filtered out.&lt;/p&gt; &lt;p&gt;## Try It Yourself&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;p&gt;git clone &lt;a href="https://github.com/DRawson5570/linguistic-rl-scheduling"&gt;https://github.com/DRawson5570/linguistic-rl-scheduling&lt;/a&gt;&lt;/p&gt; &lt;p&gt;cd linguistic-rl-scheduling&lt;/p&gt; &lt;p&gt;ollama pull qwen2.5:7b&lt;/p&gt; &lt;p&gt;python3 scheduling_lrl_paper.py&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Next_Bid_8339"&gt; /u/Next_Bid_8339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqx4dj/emergent_occams_razor_teaching_qwen257b_to_learn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqx4dj/emergent_occams_razor_teaching_qwen257b_to_learn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqx4dj/emergent_occams_razor_teaching_qwen257b_to_learn/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:18:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqu4i3</id>
    <title>Intel Arc Pro B50 GPU Review: An Affordable, Low-Power Workstation GPU</title>
    <updated>2025-11-07T13:16:02+00:00</updated>
    <author>
      <name>/u/brand_momentum</name>
      <uri>https://old.reddit.com/user/brand_momentum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqu4i3/intel_arc_pro_b50_gpu_review_an_affordable/"&gt; &lt;img alt="Intel Arc Pro B50 GPU Review: An Affordable, Low-Power Workstation GPU" src="https://external-preview.redd.it/3Egk-w2HASlmi68QUUwWJyPFPlaRWFVtyMPx2j34DHg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0db3e975c5d76b36da431c0de3d9c2859f58e5a6" title="Intel Arc Pro B50 GPU Review: An Affordable, Low-Power Workstation GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brand_momentum"&gt; /u/brand_momentum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.storagereview.com/review/intel-arc-pro-b50-gpu-review-an-affordable-low-power-workstation-gpu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqu4i3/intel_arc_pro_b50_gpu_review_an_affordable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqu4i3/intel_arc_pro_b50_gpu_review_an_affordable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T13:16:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqxuqs</id>
    <title>From your experience for text only, how is Qwen3VL compared to Qwen3, does having a Visual module penalize the text-only capacities ?</title>
    <updated>2025-11-07T15:46:05+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title.&lt;/p&gt; &lt;p&gt;Let's say &lt;code&gt;Qwen3-30B-A3B-Instruct-2507&lt;/code&gt; excels at text only and long context.&lt;/p&gt; &lt;p&gt;What about &lt;code&gt;Qwen3-VL-30B-A3B-Instruct&lt;/code&gt; if you use it as a text only model ? have you seen any quality loss ?&lt;/p&gt; &lt;p&gt;We're wondering if it make sense to have in one gpu Qwen3 VL and on another gpu Qwen3.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqxuqs/from_your_experience_for_text_only_how_is_qwen3vl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqxuqs/from_your_experience_for_text_only_how_is_qwen3vl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqxuqs/from_your_experience_for_text_only_how_is_qwen3vl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:46:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqmder</id>
    <title>Co-authored a book called "Build DeepSeek from Scratch" | Live Now</title>
    <updated>2025-11-07T05:45:39+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqmder/coauthored_a_book_called_build_deepseek_from/"&gt; &lt;img alt="Co-authored a book called &amp;quot;Build DeepSeek from Scratch&amp;quot; | Live Now" src="https://preview.redd.it/1felu4y3wrzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd71bff3abb06e132c57c36de21150381fe19207" title="Co-authored a book called &amp;quot;Build DeepSeek from Scratch&amp;quot; | Live Now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Book link: &lt;a href="https://hubs.la/Q03Rl_lh0"&gt;https://hubs.la/Q03Rl_lh0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github repository: &lt;a href="https://github.com/VizuaraAI/DeepSeek-From-Scratch"&gt;https://github.com/VizuaraAI/DeepSeek-From-Scratch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Published by Manning Publications. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1felu4y3wrzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqmder/coauthored_a_book_called_build_deepseek_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqmder/coauthored_a_book_called_build_deepseek_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T05:45:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1or46rv</id>
    <title>Cerebras/Kimi-Linear-REAP-35B-A3B-Instruct ¬∑ Hugging Face</title>
    <updated>2025-11-07T19:44:19+00:00</updated>
    <author>
      <name>/u/maroule</name>
      <uri>https://old.reddit.com/user/maroule</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or46rv/cerebraskimilinearreap35ba3binstruct_hugging_face/"&gt; &lt;img alt="Cerebras/Kimi-Linear-REAP-35B-A3B-Instruct ¬∑ Hugging Face" src="https://external-preview.redd.it/A5NFpNf7XiO2gm9NBBYXrtttQxX4Zw8QmamAzVNdgao.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4b51e11f834d5f8b364cbfd4018254e64276366" title="Cerebras/Kimi-Linear-REAP-35B-A3B-Instruct ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maroule"&gt; /u/maroule &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/cerebras/Kimi-Linear-REAP-35B-A3B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or46rv/cerebraskimilinearreap35ba3binstruct_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or46rv/cerebraskimilinearreap35ba3binstruct_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T19:44:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqjgnh</id>
    <title>30 days to become AI engineer</title>
    <updated>2025-11-07T03:12:00+00:00</updated>
    <author>
      <name>/u/CayleneKole</name>
      <uri>https://old.reddit.com/user/CayleneKole</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm moving from 12 years in cybersecurity (big tech) into a Staff AI Engineer role.&lt;br /&gt; I have 30 days (~16h/day) to get production-ready, prioritizing context engineering, RAG, and reliable agents.&lt;br /&gt; I need a focused path: the few resources, habits, and pitfalls that matter most.&lt;br /&gt; If you‚Äôve done this or ship real LLM systems, how would you spend the 30 days?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CayleneKole"&gt; /u/CayleneKole &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqjgnh/30_days_to_become_ai_engineer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqjgnh/30_days_to_become_ai_engineer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqjgnh/30_days_to_become_ai_engineer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T03:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqo57j</id>
    <title>ubergarm/Kimi-K2-Thinking-GGUF ¬∑ Hugging Face</title>
    <updated>2025-11-07T07:32:32+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqo57j/ubergarmkimik2thinkinggguf_hugging_face/"&gt; &lt;img alt="ubergarm/Kimi-K2-Thinking-GGUF ¬∑ Hugging Face" src="https://external-preview.redd.it/-6vnf_3yTWf3TtVUA6a-SCJQHQSGAkjtdxEpaCd4oLc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1be9cda0b5e2a3434209c5a9d38f045a106ba74" title="ubergarm/Kimi-K2-Thinking-GGUF ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Great job ngxson, compilade, DevQuasar, Bartowski, AesSedai, and more folks who pulled together hacking on this one today! ü´∂&lt;/p&gt; &lt;p&gt;Only one quant released so far which is &lt;code&gt;q4_0&lt;/code&gt; for the routed experts and &lt;code&gt;q8_0&lt;/code&gt; for everything else. This is because the original model is released in roughly this size at &amp;quot;full quality&amp;quot;.&lt;/p&gt; &lt;p&gt;I've tested the quant on both ik_llama.cpp and mainline llama.cpp and it inferences fine. Though it wasn't giving me any &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; or &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; tags so you might have to fiddle with the template or something (model card shows how to just load whatever you want).&lt;/p&gt; &lt;p&gt;I may try some smaller quants for ik_llama.cpp to see if they hold up despite original model being QAT'd to ~4bpw. The &amp;quot;full size&amp;quot; weighs in at 543.617 GiB (4.549 BPW).&lt;/p&gt; &lt;p&gt;Have fun!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqo57j/ubergarmkimik2thinkinggguf_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqo57j/ubergarmkimik2thinkinggguf_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T07:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqiduq</id>
    <title>Kimi 2 is the #1 creative writing AI right now. better than sonnet 4.5</title>
    <updated>2025-11-07T02:20:29+00:00</updated>
    <author>
      <name>/u/Excellent-Run7265</name>
      <uri>https://old.reddit.com/user/Excellent-Run7265</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just tried Kimi 2 and I'm genuinely impressed. It's the best creative writer AI I've used‚Äîbetter than Sonnet 4.5, better than anything else out there. And it's dirt cheap compared to Sonnet.&lt;/p&gt; &lt;p&gt;I never thought a cheap, open model would beat Anthropic at writing. don't do coding as much, but its understanding is so strong that it's probably capable there too. This is amazing for us consumers.&lt;/p&gt; &lt;p&gt;The giants now have to slash prices significantly or lose to China. At this pace, we'll see locally-run LLMs outperforming current top models in months. That's terrible for big companies like OpenAI and Anthropic‚Äîthey'll need AGI or something massively better to justify their cost difference or cut the price down to half at least for now.&lt;/p&gt; &lt;p&gt;This market is unpredictable and wild. With the US and Chinese companies pushing each other like this and not holding back, AI will become so powerful so fast that we won't have to do anything ourselves anymore.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent-Run7265"&gt; /u/Excellent-Run7265 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqiduq/kimi_2_is_the_1_creative_writing_ai_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqiduq/kimi_2_is_the_1_creative_writing_ai_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqiduq/kimi_2_is_the_1_creative_writing_ai_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T02:20:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1or1e7p</id>
    <title>I fine-tuned Gemma 3 1B for CLI command translation... but it runs 100% locally. 810MB, 1.5s inference on CPU.</title>
    <updated>2025-11-07T17:58:57+00:00</updated>
    <author>
      <name>/u/theRealSachinSpk</name>
      <uri>https://old.reddit.com/user/theRealSachinSpk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"&gt; &lt;img alt="I fine-tuned Gemma 3 1B for CLI command translation... but it runs 100% locally. 810MB, 1.5s inference on CPU." src="https://b.thumbs.redditmedia.com/k4PYFs253tXR75-utWF1-v10OmEqGwkzGrkkq8FHHVo.jpg" title="I fine-tuned Gemma 3 1B for CLI command translation... but it runs 100% locally. 810MB, 1.5s inference on CPU." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I built a locally-running NL‚ÜíCLI translator by fine-tuning Gemma 3 1B with QLoRA.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/pranavkumaarofficial/nlcli-wizard"&gt;[Link to repo]&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Built a privacy-first CLI copilot. No API calls, no subscriptions. Just 810MB of local AI that converts natural language to CLI commands.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jpo4dd4jivzf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3aa7bc9af223d3ab2e4c3eb9156907994885cf5"&gt;https://preview.redd.it/jpo4dd4jivzf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3aa7bc9af223d3ab2e4c3eb9156907994885cf5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wanted to try out something like a CLI wizard: running locally and loaded within the package. Now of course there is an overhead of embedding an SLM in every package.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;But definitely makes sense for complex, domain-specific tools with non-obvious CLI patterns&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Instead of: kubectl get pods -n production --field-selector status.phase=Running&lt;/p&gt; &lt;p&gt;Could be: kubectl -w &amp;quot;show me running pods in production&amp;quot;&lt;/p&gt; &lt;p&gt;Shell-GPT is the closest tool that is available but doesnt do what I wanted, and ofcourse uses closedsource LLMs&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here is what I tried:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Takes natural language like &amp;quot;show my environments sorted by size&amp;quot; and outputs the correct CLI command, eg : &lt;code&gt;venvy ls --sort size&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key stats:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~1.5s inference on CPU (4 threads)&lt;/li&gt; &lt;li&gt;810MB quantized model (Q4_K_M with smart fallback)&lt;/li&gt; &lt;li&gt;Trained on Colab T4 in &amp;lt;1 hr&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Setup&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Base model:&lt;/strong&gt; Gemma 3-1B-Instruct (March 2025 release)&lt;br /&gt; &lt;strong&gt;Training:&lt;/strong&gt; Unsloth + QLoRA (only 14M params trained, 1.29% of model)&lt;br /&gt; &lt;strong&gt;Hardware:&lt;/strong&gt; Free Colab T4, trained in under 1 hour&lt;br /&gt; &lt;strong&gt;Final model:&lt;/strong&gt; 810MB GGUF (Q4_K_M with smart fallback to Q5/Q6)&lt;br /&gt; &lt;strong&gt;Inference:&lt;/strong&gt; llama.cpp, ~1.5s on CPU (4 threads, M1 Mac / Ryzen)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The architecture part:&lt;/strong&gt; Used smart quantization with mixed precision (Q4_K/Q5_0/Q6_K) that adapts per-layer based on tensor dimensions. Some layers can't be quantized to 4-bit without accuracy loss, so llama.cpp automatically upgrades them to 5/6-bit.&lt;/p&gt; &lt;p&gt;Training loss was extremely clean - 0.135 (train), 0.142 (val) with zero overfitting across 3 epochs.&lt;/p&gt; &lt;p&gt;Limitations (being honest here)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Model size:&lt;/strong&gt; 810MB is chunky. Too big for Docker images, fine for dev machines.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool-specific:&lt;/strong&gt; Currently only works for &lt;code&gt;venvy&lt;/code&gt;. Need to retrain for kubectl/docker/etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Latency:&lt;/strong&gt; 1.5s isn't instant. Experts will still prefer muscle memory.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accuracy:&lt;/strong&gt; 80-85% means you MUST verify before executing.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Safety&lt;/h1&gt; &lt;p&gt;Always asks for confirmation before executing. I'm not &lt;em&gt;that&lt;/em&gt; reckless.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;confirm = input(&amp;quot;Execute? [Y/n] &amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Still working on this : to check where this can really help, but yeah pls go check it out&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/pranavkumaarofficial/nlcli-wizard"&gt;[Link to repo]&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theRealSachinSpk"&gt; /u/theRealSachinSpk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T17:58:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqttg0</id>
    <title>Can someone explain what a Mixture-of-Experts model really is?</title>
    <updated>2025-11-07T13:02:36+00:00</updated>
    <author>
      <name>/u/Weebviir</name>
      <uri>https://old.reddit.com/user/Weebviir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I've been aware of MoE since Deepseek dropped in the beginning of the year but I never really delved deep into what it is and how it helps in things like local AI inferencing. This sub's been very helpful with my local AI related questions so I wanted to learn from the people here.&lt;/p&gt; &lt;p&gt;Here are some more questions:&lt;br /&gt; - How does a model know when an expert is to be used?&lt;br /&gt; - Are MoE models really easier to run than traditional models?&lt;br /&gt; - How do Activation parameters really work? Do they affect fine tuning processes later?&lt;br /&gt; - Why do MoE models work better than traditional models?&lt;br /&gt; - What are ‚Äúsparse‚Äù vs ‚Äúdense‚Äù MoE architectures?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weebviir"&gt; /u/Weebviir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqttg0/can_someone_explain_what_a_mixtureofexperts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqttg0/can_someone_explain_what_a_mixtureofexperts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqttg0/can_someone_explain_what_a_mixtureofexperts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T13:02:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqebr3</id>
    <title>World's strongest agentic model is now open source</title>
    <updated>2025-11-06T23:20:15+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqebr3/worlds_strongest_agentic_model_is_now_open_source/"&gt; &lt;img alt="World's strongest agentic model is now open source" src="https://preview.redd.it/jd607rvrzpzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f84c70ace26fdbd5db78313787e58d2403961e38" title="World's strongest agentic model is now open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jd607rvrzpzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqebr3/worlds_strongest_agentic_model_is_now_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqebr3/worlds_strongest_agentic_model_is_now_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T23:20:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oquezp</id>
    <title>Kimi K2 Thinking with sglang and mixed GPU / ktransformers CPU inference @ 31 tokens/sec</title>
    <updated>2025-11-07T13:28:44+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got Kimi K2 Thinking running locally and I'm blown away how fast it runs in simple chat tests: approximately ~ 30 tokens/sec with 4000 tokens in the context. Obviously a lot more testing to be done, but wow... a trillion parameter model running at 30 tokens/sec. &lt;/p&gt; &lt;p&gt;I'll whip up some tests around batching and available context lengths soon, but for now here's the recipe to get it running should you have the necessary hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: it looks like only the first API request works. Subsequent requests always cause sglang to crash and require a restart, regardless of how I configure things:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; File &amp;quot;/home/carl/ktransformers/ktransformers/.venv/lib/python3.11/site-packages/triton/compiler/compiler.py&amp;quot;, line 498, in __getattribute__ self._init_handles() File &amp;quot;/home/carl/ktransformers/ktransformers/.venv/lib/python3.11/site-packages/triton/compiler/compiler.py&amp;quot;, line 483, in _init_handles raise OutOfResources(self.metadata.shared, max_shared, &amp;quot;shared memory&amp;quot;) triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 106496, Hardware limit: 101376. Reducing block sizes or `num_stages` may help. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;System&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;EPYC &lt;del&gt;7B45&lt;/del&gt; 9B45 (128-core, 256 thread) CPU&lt;/li&gt; &lt;li&gt;768GB DDR5 6400 MT/s&lt;/li&gt; &lt;li&gt;4x RTX 6000 Pro Workstation 96GB GPUs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup virtual python environment&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mkdir sglang-ktransformers cd sglang-ktransformers uv venv --python 3.11 --seed . .venv/bin/activate &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Install sglang&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uv pip install &amp;quot;sglang&amp;quot; --prerelease=allow &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Download and initialize ktransformers repo&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/kvcache-ai/ktransformers cd ktransformers git submodule update --init --recursive &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Install ktransformers CPU kernel for sglang&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd kt-kernel export CPUINFER_CPU_INSTRUCT=AVX512 export CPUINFER_ENABLE_AMX=OFF uv pip install . cd .. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Download Kimi K2 Thinking GPU &amp;amp; CPU parts&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uv pip install -U hf hf_transfer hf download moonshotai/Kimi-K2-Thinking hf download KVCache-ai/Kimi-K2-Thinking-CPU-weight &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Run k2&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 python -m sglang.launch_server \ --host 0.0.0.0 --port 8080 \ --model ~/.cache/huggingface/hub/models--moonshotai--Kimi-K2-Thinking/snapshots/357b94aee9d50ec88e5e6dd9550fd7f957cb1baa \ --kt-amx-weight-path ~/.cache/huggingface/hub/models--KVCache-ai--Kimi-K2-Thinking-CPU-weight/snapshots/690ffacb9203d3b5e05ee8167ff1f5d4ae027c83 \ --kt-cpuinfer 252 \ --kt-threadpool-count 2 \ --kt-num-gpu-experts 238 \ --kt-amx-method AMXINT4 \ --attention-backend triton --trust-remote-code \ --mem-fraction-static 0.98 \ --chunked-prefill-size 4096 \ --max-running-requests 1 \ --max-total-tokens 32768 \ --enable-mixed-chunk \ --tensor-parallel-size 4 \ --enable-p2p-check \ --disable-shared-experts-fusion &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oquezp/kimi_k2_thinking_with_sglang_and_mixed_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oquezp/kimi_k2_thinking_with_sglang_and_mixed_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oquezp/kimi_k2_thinking_with_sglang_and_mixed_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T13:28:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1or08aq</id>
    <title>OpenAI Pushes to Label Datacenters as ‚ÄòAmerican Manufacturing‚Äô Seeking Federal Subsidies After Preaching Independence</title>
    <updated>2025-11-07T17:15:05+00:00</updated>
    <author>
      <name>/u/Ok-Breakfast-4676</name>
      <uri>https://old.reddit.com/user/Ok-Breakfast-4676</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"&gt; &lt;img alt="OpenAI Pushes to Label Datacenters as ‚ÄòAmerican Manufacturing‚Äô Seeking Federal Subsidies After Preaching Independence" src="https://preview.redd.it/jq1jrz6kbvzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=258e2c389f69a6ed7cdcda3ea33b5a80b38a0fb3" title="OpenAI Pushes to Label Datacenters as ‚ÄòAmerican Manufacturing‚Äô Seeking Federal Subsidies After Preaching Independence" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI is now lobbying to classify datacenter spending as ‚ÄúAmerican manufacturing.‚Äù&lt;/p&gt; &lt;p&gt;In their recent submission, they explicitly advocate for Federal loan guarantees the same kind used to subsidize large-scale industrial projects.&lt;/p&gt; &lt;p&gt;So after all the talk about independence and no need for government help‚Ä¶ Sam lied. Again.Ôøº&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Breakfast-4676"&gt; /u/Ok-Breakfast-4676 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jq1jrz6kbvzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T17:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqy1k7</id>
    <title>AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)</title>
    <updated>2025-11-07T15:53:33+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt; &lt;img alt="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" src="https://preview.redd.it/8v2luf5owuzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9bc34ec8dddd94422397eaa91e0310250da5ba3" title="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8v2luf5owuzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:53:33+00:00</published>
  </entry>
</feed>
