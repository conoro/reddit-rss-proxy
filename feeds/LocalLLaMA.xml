<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-05T15:51:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q4j6qr</id>
    <title>Several publicly available university courses focusing on AI-Agents:</title>
    <updated>2026-01-05T11:22:22+00:00</updated>
    <author>
      <name>/u/QuanstScientist</name>
      <uri>https://old.reddit.com/user/QuanstScientist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4j6qr/several_publicly_available_university_courses/"&gt; &lt;img alt="Several publicly available university courses focusing on AI-Agents:" src="https://b.thumbs.redditmedia.com/rcFNDv3nlPdKs22xNL8ToomXEZXSYhO5jJErm9Nd38g.jpg" title="Several publicly available university courses focusing on AI-Agents:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/tmq67ptcmibg1.png?width=2106&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5167350a0cafb4bace94cff8ae805c60b13f3317"&gt;https://preview.redd.it/tmq67ptcmibg1.png?width=2106&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5167350a0cafb4bace94cff8ae805c60b13f3317&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://cs329a.stanford.edu/"&gt;https://cs329a.stanford.edu/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://cseweb.ucsd.edu/%7Eyiying/cse291a-fall25/reading/"&gt;https://cseweb.ucsd.edu/~yiying/cse291a-fall25/reading/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://rdi.berkeley.edu/agentic-ai/f25"&gt;https://rdi.berkeley.edu/agentic-ai/f25&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuanstScientist"&gt; /u/QuanstScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4j6qr/several_publicly_available_university_courses/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4j6qr/several_publicly_available_university_courses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4j6qr/several_publicly_available_university_courses/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T11:22:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4bhtm</id>
    <title>vLLM reaches 2000 contributors!</title>
    <updated>2026-01-05T04:04:52+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/vllm-project/vllm/graphs/contributors"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4bhtm/vllm_reaches_2000_contributors/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4bhtm/vllm_reaches_2000_contributors/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T04:04:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4otf8</id>
    <title>So hi all, i am currently playing with all this self hosted LLM (SLM in my case with my hardware limitations) im just using a Proxmox enviroment with Ollama installed direcly on a Ubuntu server container and on top of it Open WebUI to get the nice dashboard and to be able to create user accounts.</title>
    <updated>2026-01-05T15:35:11+00:00</updated>
    <author>
      <name>/u/Franceesios</name>
      <uri>https://old.reddit.com/user/Franceesios</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4otf8/so_hi_all_i_am_currently_playing_with_all_this/"&gt; &lt;img alt="So hi all, i am currently playing with all this self hosted LLM (SLM in my case with my hardware limitations) im just using a Proxmox enviroment with Ollama installed direcly on a Ubuntu server container and on top of it Open WebUI to get the nice dashboard and to be able to create user accounts." src="https://b.thumbs.redditmedia.com/TSL_ufQwuYISV4OKu2TKcSTDXCAkpokZsXHmh4Ru8dw.jpg" title="So hi all, i am currently playing with all this self hosted LLM (SLM in my case with my hardware limitations) im just using a Proxmox enviroment with Ollama installed direcly on a Ubuntu server container and on top of it Open WebUI to get the nice dashboard and to be able to create user accounts." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/u6jijzn0vjbg1.png?width=386&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c25c9dadc05edb8c44da0490d9f1a1082e87b8d3"&gt;https://preview.redd.it/u6jijzn0vjbg1.png?width=386&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c25c9dadc05edb8c44da0490d9f1a1082e87b8d3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far im using just these models&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hnwl19szsjbg1.png?width=765&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af1ebdb856d1af2cf34e1a9c27bf019a6be62db3"&gt;https://preview.redd.it/hnwl19szsjbg1.png?width=765&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af1ebdb856d1af2cf34e1a9c27bf019a6be62db3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/so-hi-all-i-am-currently-playing-with-all-this-self-hosted-v0-w18f48hnh5ag1.png?width=1306&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b52bca2636634ffb0b1d0bc145f2d3c38eef9d9c"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They are running ok at the time, the 8B ones would take atleast 2 minutes to give some proper answer but im ok with since this is for my own learning progress, and ive also put this template (as a safety guardrale) for the models to remember with each answer they give out ;&lt;/p&gt; &lt;p&gt;### Task:&lt;/p&gt; &lt;p&gt;Respond to the user query using the provided context, incorporating inline citations in the format [id] **only when the &amp;lt;source&amp;gt; tag includes an explicit id attribute** (e.g., &amp;lt;source id=&amp;quot;1&amp;quot;&amp;gt;). Always include a confidence rating for your answer.&lt;/p&gt; &lt;p&gt;### Guidelines:&lt;/p&gt; &lt;p&gt;- Only provide answers you are confident in. Do not guess or invent information.&lt;/p&gt; &lt;p&gt;- If unsure or lacking sufficient information, respond with &amp;quot;I don‚Äôt know&amp;quot; or &amp;quot;I‚Äôm not sure.&amp;quot;&lt;/p&gt; &lt;p&gt;- Include a confidence rating from 1 to 5:&lt;/p&gt; &lt;p&gt;1 = very uncertain&lt;/p&gt; &lt;p&gt;2 = somewhat uncertain&lt;/p&gt; &lt;p&gt;3 = moderately confident&lt;/p&gt; &lt;p&gt;4 = confident&lt;/p&gt; &lt;p&gt;5 = very confident&lt;/p&gt; &lt;p&gt;- Respond in the same language as the user's query.&lt;/p&gt; &lt;p&gt;- If the context is unreadable or low-quality, inform the user and provide the best possible answer.&lt;/p&gt; &lt;p&gt;- If the answer isn‚Äôt present in the context but you possess the knowledge, explain this and provide the answer.&lt;/p&gt; &lt;p&gt;- Include inline citations [id] only when &amp;lt;source&amp;gt; has an id attribute.&lt;/p&gt; &lt;p&gt;- Do not use XML tags in your response.&lt;/p&gt; &lt;p&gt;- Ensure citations are concise and directly relevant.&lt;/p&gt; &lt;p&gt;- Do NOT use Web Search or external sources.&lt;/p&gt; &lt;p&gt;- If the context does not contain the answer, reply: ‚ÄòI don‚Äôt know‚Äô and Confidence 1‚Äì2.&lt;/p&gt; &lt;p&gt;### Evidence-first rule (prevents guessing and helps debug RAG):&lt;/p&gt; &lt;p&gt;- When a query mentions multiple months, treat each month as an independent lookup.&lt;/p&gt; &lt;p&gt;- Do not assume a month is unavailable unless it is explicitly missing from the retrieved context.&lt;/p&gt; &lt;p&gt;- When the user asks for a specific factual value (e.g., totals, dates, IDs, counts, prices, metrics), you must first locate and extract the **exact supporting line(s)** from the provided context.&lt;/p&gt; &lt;p&gt;- In your answer, include a short **Evidence:** section that quotes the exact line(s) you relied on (verbatim or near-verbatim).&lt;/p&gt; &lt;p&gt;- If you cannot find a supporting line for the requested value in the retrieved context, do not infer it. Instead respond:&lt;/p&gt; &lt;p&gt;Answer: NOT FOUND IN CONTEXT&lt;/p&gt; &lt;p&gt;Confidence: 1‚Äì2&lt;/p&gt; &lt;p&gt;(You may add one short sentence suggesting the document chunking/retrieval may have missed the relevant section.)&lt;/p&gt; &lt;p&gt;### Financial document disambiguation rule (IMPORTANT):&lt;/p&gt; &lt;p&gt;- If a document contains both **estimated** and **invoiced** totals, select the value based on the user‚Äôs wording:&lt;/p&gt; &lt;p&gt;- Use **‚ÄúEstimated grand total‚Äù** when the query includes terms like: *estimated*, *expected*, *forecast*, *monthly spend*, *cost for the month*.&lt;/p&gt; &lt;p&gt;- Use **‚ÄúTotal invoiced charges‚Äù** when the query includes terms like: *invoice*, *invoiced*, *billed*, *final invoice*.&lt;/p&gt; &lt;p&gt;- If both totals exist but the user‚Äôs wording does not clearly indicate which one they want, do **not** choose. Respond:&lt;/p&gt; &lt;p&gt;Answer: AMBIGUOUS REQUEST ‚Äì MULTIPLE TOTALS FOUND &lt;/p&gt; &lt;p&gt;Confidence: 2 &lt;/p&gt; &lt;p&gt;(Optionally list the available totals in Evidence to help the user clarify.)&lt;/p&gt; &lt;p&gt;- If the document is an AWS &amp;quot;estimated bill&amp;quot; or &amp;quot;billing summary&amp;quot; (not a finalized invoice),&lt;/p&gt; &lt;p&gt;and the user asks for &amp;quot;invoice grand total&amp;quot;, interpret this as&lt;/p&gt; &lt;p&gt;&amp;quot;Estimated grand total&amp;quot; unless the user explicitly requests &amp;quot;invoiced charges&amp;quot;.&lt;/p&gt; &lt;p&gt;### Source lock rule (prevents cross-document mistakes):&lt;/p&gt; &lt;p&gt;- If the user‚Äôs question specifies a month or billing period (e.g., &amp;quot;December 2025&amp;quot;), you must only use evidence from a source that explicitly matches that month/period (by filename, header, or billing period line).&lt;/p&gt; &lt;p&gt;- Do not combine or average totals across multiple months.&lt;/p&gt; &lt;p&gt;- If retrieved context includes multiple months, you must either:&lt;/p&gt; &lt;p&gt;(a) ignore non-matching months, or&lt;/p&gt; &lt;p&gt;(b) respond: &amp;quot;AMBIGUOUS CONTEXT ‚Äì MULTIPLE MONTHS RETRIEVED&amp;quot; with Confidence 1‚Äì2.&lt;/p&gt; &lt;p&gt;### Evidence completeness rule (required for totals):&lt;/p&gt; &lt;p&gt;- For invoice/billing totals, the Evidence must include:&lt;/p&gt; &lt;p&gt;1) the month/period identifier (e.g., &amp;quot;Billing period Dec 1 - Dec 31, 2025&amp;quot; or &amp;quot;December 2025&amp;quot;), AND&lt;/p&gt; &lt;p&gt;2) the total line containing the numeric amount.&lt;/p&gt; &lt;p&gt;- If you cannot quote evidence containing both (1) and (2), respond:&lt;/p&gt; &lt;p&gt;Answer: NOT FOUND IN CONTEXT&lt;/p&gt; &lt;p&gt;Confidence: 1‚Äì2&lt;/p&gt; &lt;p&gt;### Example Output:&lt;/p&gt; &lt;p&gt;Answer: [Your answer here] &lt;/p&gt; &lt;p&gt;Evidence: [&amp;quot;exact supporting line(s)&amp;quot; ...] (include [id] only if available) &lt;/p&gt; &lt;p&gt;Confidence: [1-5]&lt;/p&gt; &lt;p&gt;### Confidence gating:&lt;/p&gt; &lt;p&gt;- Confidence 5 is allowed only when the Evidence includes an exact total line AND a matching month/period line from the same source.&lt;/p&gt; &lt;p&gt;- If the month/period is not explicitly proven in Evidence, Confidence must be 1‚Äì2.&lt;/p&gt; &lt;p&gt;### Context:&lt;/p&gt; &lt;p&gt;&amp;lt;context&amp;gt;&lt;/p&gt; &lt;p&gt;{{CONTEXT}}&lt;/p&gt; &lt;p&gt;&amp;lt;/context&amp;gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fm263jlgtjbg1.png?width=1881&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7514d353d2bd0bae6b23de9832f71c10e9637f30"&gt;https://preview.redd.it/fm263jlgtjbg1.png?width=1881&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7514d353d2bd0bae6b23de9832f71c10e9637f30&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/so-hi-all-i-am-currently-playing-with-all-this-self-hosted-v0-tbnk6bekh5ag1.png?width=1647&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f4fc3604dd4b2c5482d10adf75088911754837c9"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far its kind of working great, my primarly test right about now is the RAG method that Open WebUI offers, ive currently uploaded some invoices from 2025 worth of data as .MD files.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/btthrm1ntjbg1.png?width=1110&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b382079bbf80d543b37a18d2ec2676b8924c577a"&gt;https://preview.redd.it/btthrm1ntjbg1.png?width=1110&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b382079bbf80d543b37a18d2ec2676b8924c577a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/so-hi-all-i-am-currently-playing-with-all-this-self-hosted-v0-nchwh0kyh5ag1.png?width=887&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52d591d969d0a096cc8b6a8f8db63e0778203f34"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Ive converted the PDF invoices to MD files and uploaded them in my knowledge base in Open WebUI.)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nrka4wrttjbg1.png?width=885&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8863c9812f7f3175839b8d431904f5d5b0f892fe"&gt;https://preview.redd.it/nrka4wrttjbg1.png?width=885&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8863c9812f7f3175839b8d431904f5d5b0f892fe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And asks the model (selecting the folder with the data first with # command/option) and i would get some good answers and some times some not so good answers but with the confidence level accurate ;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hmr5vpnztjbg1.png?width=1473&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8de09626b0edadb1921ed95ee32c1f190b33a34a"&gt;https://preview.redd.it/hmr5vpnztjbg1.png?width=1473&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8de09626b0edadb1921ed95ee32c1f190b33a34a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/98u5bll1ujbg1.png?width=1412&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf777029d17409f5267f4318f3e799596c5c0103"&gt;https://preview.redd.it/98u5bll1ujbg1.png?width=1412&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf777029d17409f5267f4318f3e799596c5c0103&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/su6yhib3ujbg1.png?width=1111&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8942a60504f0096ef4be0e15698d6d5c7fbca9a"&gt;https://preview.redd.it/su6yhib3ujbg1.png?width=1111&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e8942a60504f0096ef4be0e15698d6d5c7fbca9a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Frome the given answer the sources that the model gather information from are right and each converted md file was given an added layer of metadata for the model to be able to read more easy i assume ;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ti2z9viiujbg1.png?width=788&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c202f68184c40db350839a79cf478ef21e470642"&gt;https://preview.redd.it/ti2z9viiujbg1.png?width=788&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c202f68184c40db350839a79cf478ef21e470642&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thus each of the bellow MD files has more than enough information for the model to be able to gather and give a proper good answer right? &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u9km28y6ujbg1.png?width=452&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ff7cd284d781102ff42fb34bd2a6182ea4eeba5"&gt;https://preview.redd.it/u9km28y6ujbg1.png?width=452&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2ff7cd284d781102ff42fb34bd2a6182ea4eeba5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/so-hi-all-i-am-currently-playing-with-all-this-self-hosted-v0-vqzwaupsh5ag1.png?width=559&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=efce1c85320dccc7f0f7a71c7d5aa087566f40aa"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now my question is, if some tech company wants to implement these type of LLM (SML) into there on premise network for like finance department to use, is this a good start? How does some enterprise do it at the moment? Like sites like &lt;a href="http://llm.co/"&gt;llm.co&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/so-hi-all-i-am-currently-playing-with-all-this-self-hosted-v0-9knu91phh5ag1.png?width=1438&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ceae50dbb15a673c5543ea24077de9302aa0b34"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far i can see real use case for this RAG method with some more powerfull hardware ofcourse, or to use ollama cloud? But using the cloud version defeats the on-prem and isolated from the internal use case, but i really want to know a real enterprise use case of a on-prem LLM RAG method.&lt;/p&gt; &lt;p&gt;Thanks all! And any feedback is welcomed since this is really fun and im learning allot here. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Franceesios"&gt; /u/Franceesios &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4otf8/so_hi_all_i_am_currently_playing_with_all_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4otf8/so_hi_all_i_am_currently_playing_with_all_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4otf8/so_hi_all_i_am_currently_playing_with_all_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T15:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4atlx</id>
    <title>[R] We built a framework to make Agents "self-evolve" using LoongFlow. Paper + Code released</title>
    <updated>2026-01-05T03:33:57+00:00</updated>
    <author>
      <name>/u/FreshmanDD</name>
      <uri>https://old.reddit.com/user/FreshmanDD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Reddit,&lt;/p&gt; &lt;p&gt;We are the team behind &lt;strong&gt;LoongFlow&lt;/strong&gt;. We've been researching how to solve the &amp;quot;static agent&amp;quot; problem‚Äîwhere agents fail to adapt to complex tasks or get stuck in loops.&lt;/p&gt; &lt;p&gt;Instead of manual prompt engineering, we applied &lt;strong&gt;Evolutionary Algorithms&lt;/strong&gt; (Selection, Mutation, Crossover) to the agent workflow. Treat prompts and logic as &amp;quot;DNA&amp;quot; that can evolve over generations to find the optimal solution.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üß¨ &lt;strong&gt;General-Evolve:&lt;/strong&gt; Automatically optimizes prompts and code logic.&lt;/li&gt; &lt;li&gt;üìà &lt;strong&gt;Proven Results:&lt;/strong&gt; In our benchmarks (detailed in the paper), we saw significant accuracy improvements compared to standard ReAct agents.&lt;/li&gt; &lt;li&gt;üîß &lt;strong&gt;Extensible:&lt;/strong&gt; Built for developers to create custom evolutionary pipelines.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We just released the paper on arXiv and the code is fully open-source.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;üìÑ Paper:&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2512.24077"&gt;https://arxiv.org/abs/2512.24077&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;üíª GitHub:&lt;/strong&gt;&lt;a href="https://github.com/baidu-baige/LoongFlow"&gt;https://github.com/baidu-baige/LoongFlow&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are looking for feedback on the architecture! Would love to hear your thoughts on combining EA with LLMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FreshmanDD"&gt; /u/FreshmanDD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4atlx/r_we_built_a_framework_to_make_agents_selfevolve/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4atlx/r_we_built_a_framework_to_make_agents_selfevolve/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4atlx/r_we_built_a_framework_to_make_agents_selfevolve/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T03:33:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1q42wtt</id>
    <title>Introducing Adaptive-P: A New Sampler for Creative Text Generation (llama.cpp PR)</title>
    <updated>2026-01-04T21:58:39+00:00</updated>
    <author>
      <name>/u/DragPretend7554</name>
      <uri>https://old.reddit.com/user/DragPretend7554</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share a sampling method we've been working on called Adaptive-P. Before I get into it, I should mention that due to a visual impairment, I used AI assistance in writing both the documentation and this post. I want to be upfront about that. The algorithm itself and the underlying idea are human created, however.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is it?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Adaptive-P is a different approach to token sampling that tries to address models getting stuck in predictable patterns. When generating creative content, models often fall back on the same phrasing, sentence structures, and narrative beats. The model has more interesting options available, but standard sampling methods don't give you a way to encourage it toward those alternatives.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How does it work?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of uniformly scaling probabilities like temperature does, or making binary keep/discard decisions like truncation methods, Adaptive-P lets you specify a probability range you want to target. It applies a transformation that creates a preference curve centered on your target probability‚Äîtokens near the target get boosted, tokens far from it get suppressed.&lt;/p&gt; &lt;p&gt;The transformation uses unbounded negative logits for distant tokens rather than a floor value. This prevents probability from accumulating in the tail of the distribution, which is a problem that affects some other approaches to forced alternative selection.&lt;/p&gt; &lt;p&gt;The sampler maintains an exponential moving average of the original probabilities of selected tokens. It uses this history to compute an adjusted target at each step. If recent selections have been running above your configured target, the sampler compensates by aiming lower on the next step, and vice versa. This feedback loop keeps the average selection probability tracking toward your target over time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Chain breaking&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The adaptive mechanism is what breaks repetitive high-confidence chains. When the model keeps selecting dominant tokens, the history shifts upward, which pushes the calculated target downward, which makes alternatives more attractive. The sampler naturally resists getting stuck in a rut without requiring external repetition penalties.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's it good for?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is designed for creative work‚Äîfiction, roleplay, brainstorming. It's not meant for tasks where accuracy matters more than variety.&lt;/p&gt; &lt;p&gt;It pairs well with Min-P, which handles removing genuinely bad options while Adaptive-P handles selection among the remaining quality candidates. Adaptive-P needs to be the final sampler in the chain since it performs the actual token selection.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Documentation: &lt;a href="https://github.com/MrJackSpade/adaptive-p-docs/blob/main/Documentation.md"&gt;https://github.com/MrJackSpade/adaptive-p-docs/blob/main/Documentation.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;llama.cpp PR: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17927"&gt;https://github.com/ggml-org/llama.cpp/pull/17927&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discord discussion: &lt;a href="https://discord.com/channels/1238219753324281886/1447392417769721926"&gt;https://discord.com/channels/1238219753324281886/1447392417769721926&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Any and all questions will likely be answered by the documentation, or the discord server.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DragPretend7554"&gt; /u/DragPretend7554 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q42wtt/introducing_adaptivep_a_new_sampler_for_creative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q42wtt/introducing_adaptivep_a_new_sampler_for_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q42wtt/introducing_adaptivep_a_new_sampler_for_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T21:58:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4o50c</id>
    <title>New to LocalLLaMA. Any other recommended subs for developers working with LLMs?</title>
    <updated>2026-01-05T15:09:54+00:00</updated>
    <author>
      <name>/u/vitaelabitur</name>
      <uri>https://old.reddit.com/user/vitaelabitur</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone. I am just getting started with LocalLLaMA, and Reddit. My current work involves building agents and RAG.&lt;/p&gt; &lt;p&gt;What other communities should I be following to keep up? &lt;/p&gt; &lt;p&gt;I'll really appreciate any recommendations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vitaelabitur"&gt; /u/vitaelabitur &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4o50c/new_to_localllama_any_other_recommended_subs_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4o50c/new_to_localllama_any_other_recommended_subs_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4o50c/new_to_localllama_any_other_recommended_subs_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T15:09:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4hcsf</id>
    <title>Grafted Titans: a Plug-and-Play Neural Memory for Open-Weight LLMs</title>
    <updated>2026-01-05T09:34:36+00:00</updated>
    <author>
      <name>/u/Forsaken-Park8149</name>
      <uri>https://old.reddit.com/user/Forsaken-Park8149</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4hcsf/grafted_titans_a_plugandplay_neural_memory_for/"&gt; &lt;img alt="Grafted Titans: a Plug-and-Play Neural Memory for Open-Weight LLMs" src="https://external-preview.redd.it/yT3YokEiN9WPYwPoTIkN__Yl-gpZQVW4GImTVvFalds.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9763e79f1d5ac10e28dcd72effae05fbce31406" title="Grafted Titans: a Plug-and-Play Neural Memory for Open-Weight LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been experimenting with &lt;strong&gt;Test-Time Training (TTT)&lt;/strong&gt;, specifically trying to replicate the core concept of Google‚Äôs &amp;quot;Titans&amp;quot; architecture (learning a neural memory on the fly) without the massive compute requirement of training a transformer from scratch.&lt;/p&gt; &lt;p&gt;I wanted to see if I could &amp;quot;graft&amp;quot; a trainable memory module onto a &lt;strong&gt;frozen open-weight model&lt;/strong&gt; (Qwen-2.5-0.5B) using a consumer-grade setup (I got Nvidia DGX Spark BlackWell, 128GB)&lt;/p&gt; &lt;p&gt;I‚Äôm calling this architecture &amp;quot;Grafted Titans.&amp;quot; I just finished the evaluation on the BABILong benchmark and the results were very interesting&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Base Model:&lt;/strong&gt; Qwen-2.5-0.5B-Instruct (Frozen weights).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mechanism:&lt;/strong&gt; I appended memory embeddings to the input layer (Layer 0) via a trainable cross-attention gating mechanism. This acts as an adapter, allowing the memory to update recursively while the base model stays static.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Benchmark (BABILong, up to 2k context):&lt;/strong&gt; I used a strict 2-turn protocol.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Turn 1:&lt;/strong&gt; Feed context -&amp;gt; Memory updates -&amp;gt; Context removed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Turn 2:&lt;/strong&gt; Feed question -&amp;gt; Model retrieves answer solely from neural memory.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Results:&lt;/strong&gt; I compared my grafted memory against two baselines.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Random Guessing:&lt;/strong&gt; 0.68% Accuracy. Basically all wrong.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vanilla Qwen (Full Context):&lt;/strong&gt; I fed the &lt;em&gt;entire&lt;/em&gt; token context to the standard Qwen model in the prompt. It scored &lt;strong&gt;34.0%&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grafted Titans (Memory Only):&lt;/strong&gt; The model saw &lt;em&gt;no&lt;/em&gt; context in the prompt, only the memory state. It scored &lt;strong&gt;44.7%&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It appears the &lt;strong&gt;neural memory module is acting as a&lt;/strong&gt; &lt;strong&gt;denoising filter&lt;/strong&gt;. When a small model like Qwen-0.5B sees 1.5k tokens of text, its attention mechanism gets &amp;quot;diluted&amp;quot; by the noise. The grafted memory, however, compresses that signal into specific vectors, making retrieval sharper than the native attention window.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Limitations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Signal Dilution:&lt;/strong&gt; Because I'm injecting memory at Layer 0 (soft prompting style), I suspect a vanishing gradient effect as the signal travels up the layers. Future versions need multi-layer injection.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Guardrails:&lt;/strong&gt; The memory is currently &amp;quot;gullible.&amp;quot; It treats all input as truth, meaning it's highly susceptible to poisoning in a multi-turn setting.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmark:&lt;/strong&gt; This was a 2-turn evaluation. Stability in long conversations (10+ turns) is unproven.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm currently cleaning up the code and weights to open-source the entire project (will be under &amp;quot;AI Realist&amp;quot; if you want to search for it later).&lt;/p&gt; &lt;p&gt;Has anyone else experimented with cross-attention adapters for memory retrieval? I'm curious if injecting at the middle layers (e.g., block 12 of 24) would solve the signal dilution issue without destabilizing the frozen weights.&lt;/p&gt; &lt;p&gt;Thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Forsaken-Park8149"&gt; /u/Forsaken-Park8149 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://msukhareva.substack.com/p/grafted-titans-i-built-a-plug-and"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4hcsf/grafted_titans_a_plugandplay_neural_memory_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4hcsf/grafted_titans_a_plugandplay_neural_memory_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T09:34:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4kxs7</id>
    <title>backend sampling has been merged into llama.cpp</title>
    <updated>2026-01-05T12:54:29+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4kxs7/backend_sampling_has_been_merged_into_llamacpp/"&gt; &lt;img alt="backend sampling has been merged into llama.cpp" src="https://external-preview.redd.it/2cWHgmxIHvuopjsCPWeTIoGKCFWNy96-VYIpJVWfrsI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=28dd05c0eb1e48f57f8d81d5825967a34e7bc5bf" title="backend sampling has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It means that sampling can now be integrated directly into the computation graph on backends (like CUDA), potentially reducing GPU/CPU data transfers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17004"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4kxs7/backend_sampling_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4kxs7/backend_sampling_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T12:54:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4hdzs</id>
    <title>We trained a 7B model (OpenChat) on synthetic OCR data to beat public dataset benchmarks on financial docs. (Paper + Method inside)</title>
    <updated>2026-01-05T09:36:45+00:00</updated>
    <author>
      <name>/u/Hyperbots</name>
      <uri>https://old.reddit.com/user/Hyperbots</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have been researching a major bottleneck in Financial Document Understanding (FDU): &lt;strong&gt;The Privacy Paradox.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;To build accurate invoice parsers, you need complex, messy, real-world data (nested tables, colliding columns). But due to privacy laws, you can't use client data for training. Most teams resort to public datasets like UCSF or RVL-CDIP, but we found these datasets are often too &amp;quot;clean&amp;quot; or structurally simple to represent real-world financial chaos.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Experiment:&lt;/strong&gt; We hypothesized that high-fidelity &lt;strong&gt;synthetic data&lt;/strong&gt; could outperform real (but structurally simple) public data.&lt;/p&gt; &lt;p&gt;We developed a framework called &lt;strong&gt;DocuLite&lt;/strong&gt; containing two generators:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;InvoicePy (Text):&lt;/strong&gt; Uses LLaMA-3-70B to generate synthetic OCR text that mimics complex layouts (tables, key-value pairs) without containing any real PII.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TemplatePy (Vision):&lt;/strong&gt; Generates HTML-based invoice templates to train Vision Language Models (VLMs).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The Results:&lt;/strong&gt; We benchmarked this against models trained on standard public datasets.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LLM Performance:&lt;/strong&gt; A 7B model (OpenChat-3.5) trained on our synthetic data saw a &lt;strong&gt;0.525 improvement in F1 score&lt;/strong&gt; compared to the same model trained on public data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VLM Performance:&lt;/strong&gt; An 8B model (InternVL-2) saw a &lt;strong&gt;0.513 F1 improvement&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key Takeaway:&lt;/strong&gt; For anyone building RAG or Extraction pipelines in sensitive domains (Finance/Healthcare), our results suggest that investing in a &lt;em&gt;synthetic data generator&lt;/em&gt; (that preserves layout logic) yields better ROI than hunting for &amp;quot;anonymized&amp;quot; public datasets. The model learns the &lt;em&gt;structure&lt;/em&gt; better when you control the generation parameters.&lt;/p&gt; &lt;p&gt;We published the full breakdown of the architecture, the F1 charts per field, and the methodology in our technical blog if anyone is interested in the deeper engineering details:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.hyperbots.com/research/breaking-the-annotation-barrier-with-doculite"&gt;https://www.hyperbots.com/research/breaking-the-annotation-barrier-with-doculite&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Has anyone else here successfully replaced real data with synthetic data for complex tabular extraction? I'd love to hear if you faced similar F1 score jumps.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hyperbots"&gt; /u/Hyperbots &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4hdzs/we_trained_a_7b_model_openchat_on_synthetic_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4hdzs/we_trained_a_7b_model_openchat_on_synthetic_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4hdzs/we_trained_a_7b_model_openchat_on_synthetic_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T09:36:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4i7m2</id>
    <title>Apple CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning</title>
    <updated>2026-01-05T10:26:56+00:00</updated>
    <author>
      <name>/u/PlasticTourist6527</name>
      <uri>https://old.reddit.com/user/PlasticTourist6527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4i7m2/apple_clara_bridging_retrieval_and_generation/"&gt; &lt;img alt="Apple CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning" src="https://b.thumbs.redditmedia.com/lr9MpjbY9ZQcfQfKCIYFMregw58svjBmOTHq1XoZPtQ.jpg" title="Apple CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have not seen any discussion about this effort so I'm posting it here.&lt;br /&gt; But it looks like apple tried a new approach at RAG.&lt;br /&gt; Basically they took their own attempt at linguistic compression, it can shrink documents by &lt;strong&gt;32x to 64x&lt;/strong&gt; without losing the important details needed to answer a question.&lt;br /&gt; and the novel thing in my opinion is instead of having a separate retriever and a separate writer, it unifies them. It learns to find the right info and write the answer in one smooth process.&lt;/p&gt; &lt;p&gt;And ofcourse its fully open source.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l8kt1oflgibg1.png?width=1924&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a0ced30785edce71970f436406c9105af5b0229"&gt;https://preview.redd.it/l8kt1oflgibg1.png?width=1924&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a0ced30785edce71970f436406c9105af5b0229&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Links:&lt;br /&gt; &lt;a href="https://github.com/apple/ml-clara"&gt;https://github.com/apple/ml-clara&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/datasets/apple/CLaRa_multi_stage"&gt;https://huggingface.co/datasets/apple/CLaRa_multi_stage&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/apple/CLaRa-7B-Instruct"&gt;https://huggingface.co/apple/CLaRa-7B-Instruct&lt;/a&gt;&lt;br /&gt; &lt;a href="https://arxiv.org/pdf/2511.18659"&gt;https://arxiv.org/pdf/2511.18659&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PlasticTourist6527"&gt; /u/PlasticTourist6527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4i7m2/apple_clara_bridging_retrieval_and_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4i7m2/apple_clara_bridging_retrieval_and_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4i7m2/apple_clara_bridging_retrieval_and_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T10:26:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4jf67</id>
    <title>TeleChat3-105B-A4.7B-Thinking and TeleChat3-36B-Thinking</title>
    <updated>2026-01-05T11:35:48+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jf67/telechat3105ba47bthinking_and_telechat336bthinking/"&gt; &lt;img alt="TeleChat3-105B-A4.7B-Thinking and TeleChat3-36B-Thinking" src="https://b.thumbs.redditmedia.com/diBmQ2hoP7eVo8Z7ziFcczmVLcSbccadB3HOJJfPYhA.jpg" title="TeleChat3-105B-A4.7B-Thinking and TeleChat3-36B-Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/o810skkwnibg1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a3c8fa43b527dea185123cdf3cf7f80ee3e9ddcc"&gt;https://preview.redd.it/o810skkwnibg1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a3c8fa43b527dea185123cdf3cf7f80ee3e9ddcc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Xingchen Semantic Large Model TeleChat3 is a large language model developed and trained by the China Telecom Artificial Intelligence Research Institute; this series of models was trained entirely using China computing resources.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Tele-AI/TeleChat3?tab=readme-ov-file"&gt;https://github.com/Tele-AI/TeleChat3?tab=readme-ov-file&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/collections/TeleAI/TeleChat3"&gt;https://modelscope.cn/collections/TeleAI/TeleChat3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Current doesn't have huggingface‚ò†Ô∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jf67/telechat3105ba47bthinking_and_telechat336bthinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jf67/telechat3105ba47bthinking_and_telechat336bthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jf67/telechat3105ba47bthinking_and_telechat336bthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T11:35:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4ahw1</id>
    <title>Llama 3.3 8B, abliterated to &lt;0.05 KL</title>
    <updated>2026-01-05T03:18:45+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is an abliterated version of the allegedly leaked Llama 3.3 8B 128k model that tries to minimize intelligence loss while optimizing for compliance.&lt;/p&gt; &lt;p&gt;Link (BF16 weights):&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Llama-3.3-8B-Instruct-128K_Abliterated"&gt;https://huggingface.co/SicariusSicariiStuff/Llama-3.3-8B-Instruct-128K_Abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Credits: Fizzarolli, p-e-w, some employee @ meta for another successful failure.&lt;/p&gt; &lt;p&gt;Enjoy :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4ahw1/llama_33_8b_abliterated_to_005_kl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4ahw1/llama_33_8b_abliterated_to_005_kl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4ahw1/llama_33_8b_abliterated_to_005_kl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T03:18:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4mxu0</id>
    <title>Upstage has finally posted benchmark results for Solar Open 100B</title>
    <updated>2026-01-05T14:22:16+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4mxu0/upstage_has_finally_posted_benchmark_results_for/"&gt; &lt;img alt="Upstage has finally posted benchmark results for Solar Open 100B" src="https://b.thumbs.redditmedia.com/ydV66xzb188h1ELcBzHRydHMUSlO7VdO7H3iIUSexVg.jpg" title="Upstage has finally posted benchmark results for Solar Open 100B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/upstage/Solar-Open-100B/blob/main/solar-open-technical-report.pdf"&gt;https://huggingface.co/upstage/Solar-Open-100B/blob/main/solar-open-technical-report.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q4mxu0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4mxu0/upstage_has_finally_posted_benchmark_results_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4mxu0/upstage_has_finally_posted_benchmark_results_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T14:22:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1q41bw1</id>
    <title>GLM-Image model from Z.ai is coming</title>
    <updated>2026-01-04T20:54:04+00:00</updated>
    <author>
      <name>/u/Ravencloud007</name>
      <uri>https://old.reddit.com/user/Ravencloud007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/"&gt; &lt;img alt="GLM-Image model from Z.ai is coming" src="https://preview.redd.it/sm31vizebebg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ae576450ba7112c06760ff8cddee6f5bdd7b672" title="GLM-Image model from Z.ai is coming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/pull/43100/files"&gt;https://github.com/huggingface/transformers/pull/43100/files&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ravencloud007"&gt; /u/Ravencloud007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sm31vizebebg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T20:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4i19c</id>
    <title>Benchmarking 23 LLMs on Nonogram (Logic Puzzle) Solving Performance</title>
    <updated>2026-01-05T10:16:23+00:00</updated>
    <author>
      <name>/u/mauricekleine</name>
      <uri>https://old.reddit.com/user/mauricekleine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4i19c/benchmarking_23_llms_on_nonogram_logic_puzzle/"&gt; &lt;img alt="Benchmarking 23 LLMs on Nonogram (Logic Puzzle) Solving Performance" src="https://preview.redd.it/fdryj8qkaibg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e8aa66881e7017590b4656228716be0bf22299ee" title="Benchmarking 23 LLMs on Nonogram (Logic Puzzle) Solving Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the Christmas holidays I went down a rabbit hole and built a benchmark to test how well large language models can solve nonograms (grid-based logic puzzles).&lt;/p&gt; &lt;p&gt;The benchmark evaluates 23 LLMs across increasing puzzle sizes (5x5, 10x10, 15x15).&lt;/p&gt; &lt;p&gt;A few interesting observations: - Performance drops sharply as puzzle size increases - Some models generate code to brute-force solutions - Others actually reason through the puzzle step-by-step, almost like a human - GPT-5.2 is currently dominating the leaderboard&lt;/p&gt; &lt;p&gt;Cost of curiosity: - ~$250 - ~17,000,000 tokens - zero regrets&lt;/p&gt; &lt;p&gt;Everything is fully open source and rerunnable when new models drop. Benchmark: &lt;a href="https://www.nonobench.com"&gt;https://www.nonobench.com&lt;/a&gt;&lt;br /&gt; Code: &lt;a href="https://github.com/mauricekleine/nono-bench"&gt;https://github.com/mauricekleine/nono-bench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I mostly built this out of curiosity, but I‚Äôm interested in what people here think: Are we actually measuring reasoning ability ‚Äî or just different problem-solving strategies?&lt;/p&gt; &lt;p&gt;Happy to answer questions or run specific models if people are interested.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mauricekleine"&gt; /u/mauricekleine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fdryj8qkaibg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4i19c/benchmarking_23_llms_on_nonogram_logic_puzzle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4i19c/benchmarking_23_llms_on_nonogram_logic_puzzle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T10:16:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4harh</id>
    <title>Introducing Falcon H1R 7B</title>
    <updated>2026-01-05T09:31:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4harh/introducing_falcon_h1r_7b/"&gt; &lt;img alt="Introducing Falcon H1R 7B" src="https://external-preview.redd.it/cp8sHrI0u-v727PXUjUREk9f3_bJczbhgY4L_llZyME.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e151ef7170642c032e8387d6514663a6cc0f364e" title="Introducing Falcon H1R 7B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1R-7B"&gt;https://huggingface.co/tiiuae/Falcon-H1R-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This repository presents &lt;strong&gt;Falcon-H1R-7B&lt;/strong&gt;, a reasoning-specialized model built on top of &lt;a href="https://huggingface.co/tiiuae/Falcon-H1-7B-Base"&gt;Falcon-H1-7B-Base&lt;/a&gt; and trained via cold-start supervised fine-tuning with long reasoning traces and further enhanced by scaling RL with GRPO. The model demonstrates outstanding performance across various benchmark evaluations, including mathematics, programming, instruction following, and general logic.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tiiuae/Falcon-H1R-7B-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1R-7B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/tiiuae/falcon-h1r-7b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4harh/introducing_falcon_h1r_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4harh/introducing_falcon_h1r_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T09:31:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4mmiz</id>
    <title>Miromind_ai released Miro Thinker 1.5</title>
    <updated>2026-01-05T14:09:11+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4mmiz/miromind_ai_released_miro_thinker_15/"&gt; &lt;img alt="Miromind_ai released Miro Thinker 1.5" src="https://preview.redd.it/8sefq240gjbg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af4a29e216377f1704f3b3c5dc81609fdd17916e" title="Miromind_ai released Miro Thinker 1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HF Link: &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v15"&gt;https://huggingface.co/collections/miromind-ai/mirothinker-v15&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- Post-trained on top of qwen3 - Available in both 30A3B and 235A22B - Claimed to have great result on BrowserComp - Technical report coming soon - MiT license&lt;/p&gt; &lt;p&gt;Official demo: &lt;a href="https://dr.miromind.ai"&gt;https://dr.miromind.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8sefq240gjbg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4mmiz/miromind_ai_released_miro_thinker_15/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4mmiz/miromind_ai_released_miro_thinker_15/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T14:09:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4icio</id>
    <title>Bielik-11B-v3.0-Instruct</title>
    <updated>2026-01-05T10:34:59+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4icio/bielik11bv30instruct/"&gt; &lt;img alt="Bielik-11B-v3.0-Instruct" src="https://external-preview.redd.it/5cEj5o78oh6TbyHqprpd205PtMWxpwd8yMVStGNcCRo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22861a4ba482c1b57948a7a0af2d215159d5c0d3" title="Bielik-11B-v3.0-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bielik-11B-v3.0-Instruct is a generative text model featuring 11 billion parameters. It is an instruct fine-tuned version of the &lt;a href="https://huggingface.co/speakleash/Bielik-11B-v3-Base-20250730"&gt;Bielik-11B-v3-Base-20250730&lt;/a&gt;. Forementioned model stands as a testament to the unique collaboration between the open-science/open-source project SpeakLeash and the High Performance Computing (HPC) center: ACK Cyfronet AGH. &lt;/p&gt; &lt;p&gt;Developed and trained on multilingual text corpora across &lt;strong&gt;32 European languages&lt;/strong&gt;, with &lt;strong&gt;emphasis on Polish&lt;/strong&gt;, which has been cherry-picked and processed by the SpeakLeash team, this endeavor leverages Polish large-scale computing infrastructure, specifically within the PLGrid environment, and more precisely, the HPC centers: ACK Cyfronet AGH.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/speakleash/Bielik-11B-v3.0-Instruct-GGUF"&gt;https://huggingface.co/speakleash/Bielik-11B-v3.0-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/speakleash/bielik-papers/blob/main/v3/Bielik_11B_v3.pdf"&gt;https://github.com/speakleash/bielik-papers/blob/main/v3/Bielik_11B_v3.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/speakleash/Bielik-11B-v3.0-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4icio/bielik11bv30instruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4icio/bielik11bv30instruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T10:34:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4f0tm</id>
    <title>I built a visual AI workflow tool that runs entirely in your browser - Ollama, LM Studio, llama.cpp and Most cloud API's all work out of the box. Agents/Websearch/TTS/Etc.</title>
    <updated>2026-01-05T07:08:30+00:00</updated>
    <author>
      <name>/u/l33t-Mt</name>
      <uri>https://old.reddit.com/user/l33t-Mt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4f0tm/i_built_a_visual_ai_workflow_tool_that_runs/"&gt; &lt;img alt="I built a visual AI workflow tool that runs entirely in your browser - Ollama, LM Studio, llama.cpp and Most cloud API's all work out of the box. Agents/Websearch/TTS/Etc." src="https://external-preview.redd.it/aGJ3cmdlMXMyaGJnMfKIu2bgp1pENmKjPeusz-I2kkXf7vs8dV2V756jCzVD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67c688a8e1b37aaeac76f077a39bd2c01ad85859" title="I built a visual AI workflow tool that runs entirely in your browser - Ollama, LM Studio, llama.cpp and Most cloud API's all work out of the box. Agents/Websearch/TTS/Etc." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You might remember me from LlamaCards a previous program ive built or maybe you've seen some of my agentic computer use posts with Moondream/Minicpm navigation creating reddit posts.&lt;/p&gt; &lt;p&gt;Ive had my head down and I've finally gotten something I wanted to show you all.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EmergentFlow&lt;/strong&gt; - a visual node-based editor for creating AI workflows and agents. The whole execution engine runs in your browser. Its a great sandbox for developing AI workflows.&lt;/p&gt; &lt;p&gt;You just open it and go. No Docker, no Python venv, no dependencies. Connect your Ollama(or other local) instance, paste your API keys for whatever providers you use, and start building. Everything runs client-side - your keys stay in your browser, your prompts go directly to the providers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Supported:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ollama (just works - point it at localhost:11434, auto-fetches models)&lt;/li&gt; &lt;li&gt;LM Studio + llama.cpp (works once CORS is configured)&lt;/li&gt; &lt;li&gt;OpenAI, Anthropic, Groq, Gemini, DeepSeek, xAI&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For edge cases where you hit CORS issues, there's an optional desktop runner that acts as a local proxy. It's open source: &lt;a href="http://github.com/l33tkr3w/EmergentFlow-runner"&gt;github.com/l33tkr3w/EmergentFlow-runner&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But honestly most stuff works straight from the browser.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The deal:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It's free. Like, actually free - not &amp;quot;free trial&amp;quot; free. &lt;/p&gt; &lt;p&gt;You get a full sandbox with unlimited use of your own API keys. The only thing that costs credits is if you use my server-paid models (Gemini) because Google charges me for those.&lt;/p&gt; &lt;p&gt;Free tier gets 25 daily credits for server models(Gemini through my API key). &lt;/p&gt; &lt;p&gt;Running Ollama/LMStudio/llama.cpp or BYOK? &lt;strong&gt;Unlimited. Forever. No catch.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I do have a Pro tier ($19/mo) for power users who want more server credits and team collaboration, node/flow gallery - because I'm a solo dev with a kid trying to make this sustainable. But honestly most people here running local models won't need it. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; &lt;a href="https://emergentflow.io/try"&gt;emergentflow.io/try&lt;/a&gt; - no signup, no credit card, just start dragging nodes.&lt;/p&gt; &lt;p&gt;If you run into issues (there will be some), please submit a bug report. Happy to answer questions about how stuff works under the hood.&lt;/p&gt; &lt;p&gt;Support a fellow LocalLlama enthusiast! Updoot?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/l33t-Mt"&gt; /u/l33t-Mt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ps5d841s2hbg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4f0tm/i_built_a_visual_ai_workflow_tool_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4f0tm/i_built_a_visual_ai_workflow_tool_that_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T07:08:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4mfe3</id>
    <title>miromind-ai/MiroThinker-v1.5-30B ¬∑ Hugging Face</title>
    <updated>2026-01-05T14:01:07+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4mfe3/miromindaimirothinkerv1530b_hugging_face/"&gt; &lt;img alt="miromind-ai/MiroThinker-v1.5-30B ¬∑ Hugging Face" src="https://external-preview.redd.it/-aM-CHNwHaN5ag6XD8UTMnw7B3_FAQ--6ST4zVuMZqA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8030148e6cd1af20f994ef0a510d00fa0662c358" title="miromind-ai/MiroThinker-v1.5-30B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/MiroThinker-v1.5-30B-GGUF"&gt;https://huggingface.co/mradermacher/MiroThinker-v1.5-30B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MiroThinker v1.5 is the world-leading search agent designed to advance tool-augmented reasoning and information-seeking capabilities.&lt;/p&gt; &lt;p&gt;Unlike previous agents that scale only model size or context length, MiroThinker introduces &lt;strong&gt;interactive scaling&lt;/strong&gt; at the model level, systematically training the model to handle deeper and more frequent agent‚Äìenvironment interactions as a third dimension of performance improvement. Interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories.&lt;/p&gt; &lt;p&gt;Empirical results demonstrate the effectiveness of this interactive scaling. Performance across several benchmarks improves predictably as the model engages in increasingly deep and frequent interactions with its environment.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MiroThinker v1.5 supports a 256K context window, long-horizon reasoning, and deep multi-step analysis.&lt;/li&gt; &lt;li&gt;Handles up to 400 tool calls per task ‚Äî a substantial improvement over previous open-source research agents.&lt;/li&gt; &lt;li&gt;Released in 30B and 235B parameter scales, accompanied by a comprehensive suite of tools and workflows to flexibly support diverse research settings and compute budgets.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-30B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4mfe3/miromindaimirothinkerv1530b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4mfe3/miromindaimirothinkerv1530b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T14:01:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4jnq0</id>
    <title>Falcon H1R 7B, a new reasoning model with 256k context window by the Technology Innovation Institute (TII) in Abu Dhabi</title>
    <updated>2026-01-05T11:48:59+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jnq0/falcon_h1r_7b_a_new_reasoning_model_with_256k/"&gt; &lt;img alt="Falcon H1R 7B, a new reasoning model with 256k context window by the Technology Innovation Institute (TII) in Abu Dhabi" src="https://preview.redd.it/khf18ffgqibg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c79a97cb99a35abb0788f044e1494984c94cd524" title="Falcon H1R 7B, a new reasoning model with 256k context window by the Technology Innovation Institute (TII) in Abu Dhabi" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GGUF: &lt;a href="https://huggingface.co/tiiuae/Falcon-H1R-7B-GGUF"&gt;https://huggingface.co/tiiuae/Falcon-H1R-7B-GGUF&lt;/a&gt;&lt;br /&gt; Model: &lt;a href="https://huggingface.co/tiiuae/Falcon-H1R-7B"&gt;https://huggingface.co/tiiuae/Falcon-H1R-7B&lt;/a&gt;&lt;br /&gt; Blog post: &lt;a href="https://huggingface.co/blog/tiiuae/falcon-h1r-7b"&gt;https://huggingface.co/blog/tiiuae/falcon-h1r-7b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/khf18ffgqibg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jnq0/falcon_h1r_7b_a_new_reasoning_model_with_256k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jnq0/falcon_h1r_7b_a_new_reasoning_model_with_256k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T11:48:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4m6k0</id>
    <title>The Major Release of MiroMind‚Äôs Flagship Search Agent Model, MiroThinker 1.5.</title>
    <updated>2026-01-05T13:50:38+00:00</updated>
    <author>
      <name>/u/wuqiao</name>
      <uri>https://old.reddit.com/user/wuqiao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4m6k0/the_major_release_of_mirominds_flagship_search/"&gt; &lt;img alt="The Major Release of MiroMind‚Äôs Flagship Search Agent Model, MiroThinker 1.5." src="https://external-preview.redd.it/cH2lE5iC3U5CuznHdVEsQrxsFQW9rX4gLlOCeNsa0eE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c8e5a6d9fb506d45e380a2b69000398cdfa1e84" title="The Major Release of MiroMind‚Äôs Flagship Search Agent Model, MiroThinker 1.5." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have officially released our self-developed flagship search-based agent model, MiroThinker 1.5.This release delivers significant performance improvements and explores as well as implements predictive use cases.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get started now:&lt;/strong&gt; &lt;a href="https://dr.miromind.ai/"&gt;&lt;strong&gt;https://dr.miromind.ai/&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Leading Performance:&lt;/strong&gt; MiroThinker 1.5 (235B) surpasses ChatGPT-Agent in BrowseComp, ranking among the world's top tier.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extreme Efficiency:&lt;/strong&gt; MiroThinker 1.5 (30B) costs only 1/20 of Kimi-K2, delivering faster inference and higher intelligence-to-cost ratio.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Predict the Future:&lt;/strong&gt; Proprietary ‚ÄúInteractive Scaling‚Äù and ‚ÄúTemporal-Sensitive Training‚Äù enable forward-looking analysis of how macro events trigger chain reactions across the Nasdaq.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully Open-Source:&lt;/strong&gt; Model and code are fully open, immediately unlocking discovery-driven intelligence for free.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Sample Showcase&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Case 1: What major events next week could affect the U.S. Nasdaq Index, and how might each of them impact it?&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://dr.miromind.ai/share/85ebca56-20b4-431d-bd3a-9dbbce7a82ea"&gt;https://dr.miromind.ai/share/85ebca56-20b4-431d-bd3a-9dbbce7a82ea&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;ul&gt; &lt;li&gt;Case 2: Which film is most likely to receive a Best Picture nomination at the 2026 Oscars?&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://dr.miromind.ai/share/e1099047-4488-4642-b7a4-e001e6213b22"&gt;https://dr.miromind.ai/share/e1099047-4488-4642-b7a4-e001e6213b22&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;ul&gt; &lt;li&gt;Case 3: Which team is most likely to make it to the Super Bowl in 2026?&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://dr.miromind.ai/share/c5ee0db8-676a-4b75-b42d-fd5ef8a2e0db"&gt;https://dr.miromind.ai/share/c5ee0db8-676a-4b75-b42d-fd5ef8a2e0db&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub &lt;strong&gt;:&lt;/strong&gt; &lt;a href="https://github.com/MiroMindAI/MiroThinker"&gt;https://github.com/MiroMindAI/MiroThinker&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Discord: &lt;a href="https://discord.gg/F7EQFnYscV"&gt;https://discord.gg/F7EQFnYscV&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Details&lt;/strong&gt;Ôºö&lt;a href="https://github.com/MiroMindAI/MiroThinker/discussions/64"&gt;https://github.com/MiroMindAI/MiroThinker/discussions/64&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/wuqiao"&gt; /u/wuqiao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-235B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4m6k0/the_major_release_of_mirominds_flagship_search/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4m6k0/the_major_release_of_mirominds_flagship_search/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T13:50:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1q4jc99</id>
    <title>What do we think about Gorgon Point (Ryzen AI 9 HX 470)?</title>
    <updated>2026-01-05T11:31:03+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jc99/what_do_we_think_about_gorgon_point_ryzen_ai_9_hx/"&gt; &lt;img alt="What do we think about Gorgon Point (Ryzen AI 9 HX 470)?" src="https://preview.redd.it/6lfowdxxnibg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be0f7d010f4bb84aa8472d0b245abd51e0e8c43b" title="What do we think about Gorgon Point (Ryzen AI 9 HX 470)?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The new APU is promised to support DDR5-6400 (102.4 GB/s) and LPDDR5X-8533 (136.5 GB/s) which should move some models that were barely usable on Strix Point to the usable territory.&lt;/p&gt; &lt;p&gt;However, it really seems that to utilise these capabilities, manufacturers would have to get chips that are basically inaccessible right now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6lfowdxxnibg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jc99/what_do_we_think_about_gorgon_point_ryzen_ai_9_hx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q4jc99/what_do_we_think_about_gorgon_point_ryzen_ai_9_hx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-05T11:31:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
