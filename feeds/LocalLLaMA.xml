<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-16T18:26:02+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o89fnx</id>
    <title>Thoughts on M5 MacBook Pro to run models locally?</title>
    <updated>2025-10-16T15:36:13+00:00</updated>
    <author>
      <name>/u/A4_Ts</name>
      <uri>https://old.reddit.com/user/A4_Ts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It’s a huge boost but unfortunately with such little RAM(16gb) my thinking was might as well stay with MacBook Air M4 than shelling out at least 2.5x the amount and instead use Cloud services for $40/month&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A4_Ts"&gt; /u/A4_Ts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o89fnx/thoughts_on_m5_macbook_pro_to_run_models_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o89fnx/thoughts_on_m5_macbook_pro_to_run_models_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o89fnx/thoughts_on_m5_macbook_pro_to_run_models_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T15:36:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1o75kkb</id>
    <title>AI has replaced programmers… totally.</title>
    <updated>2025-10-15T08:37:54+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"&gt; &lt;img alt="AI has replaced programmers… totally." src="https://preview.redd.it/bnnb2fb9m8vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1a55140b6915df726dfa4932943df64e43e7d94" title="AI has replaced programmers… totally." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bnnb2fb9m8vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o75kkb/ai_has_replaced_programmers_totally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T08:37:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8c3hu</id>
    <title>Vulkan with Strix halo igpu and external 3090s not possible?</title>
    <updated>2025-10-16T17:13:27+00:00</updated>
    <author>
      <name>/u/Goldkoron</name>
      <uri>https://old.reddit.com/user/Goldkoron</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I bought an AI Max 395 mini pc with 128gb with the hope that I could connect 3090 egpus and run larger models like GLM-4.6. However, I get memory errors and crashes when trying to load a model with llama-cpp with the igpu plus any other gpu.&lt;/p&gt; &lt;p&gt;Before I bought the strix halo pc I confirmed with the radeon 780m igpu on my old pc that vulkan could run igpus and nvidia gpus together. But it's not working at all with Strix Halo. Am I screwed and this will never work?&lt;/p&gt; &lt;p&gt;I cant even use rocm with my 395, AMD's support for their own &amp;quot;AI Max&amp;quot; series seems abyssmal.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Goldkoron"&gt; /u/Goldkoron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8c3hu/vulkan_with_strix_halo_igpu_and_external_3090s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8c3hu/vulkan_with_strix_halo_igpu_and_external_3090s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8c3hu/vulkan_with_strix_halo_igpu_and_external_3090s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T17:13:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1o87u2f</id>
    <title>I know the DGX Spark isn’t what a lot people hoped it would be, but what if……</title>
    <updated>2025-10-16T14:35:31+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o87u2f/i_know_the_dgx_spark_isnt_what_a_lot_people_hoped/"&gt; &lt;img alt="I know the DGX Spark isn’t what a lot people hoped it would be, but what if……" src="https://preview.redd.it/42fhtkk1jhvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63509ea9a99a07f56b3e3f3db9a472885786f097" title="I know the DGX Spark isn’t what a lot people hoped it would be, but what if……" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What if you bought a ConnectX-7 NIC PCI card and connected the Spark’s Connect-X-7 port to an existing AI rig that had a couple 3090s in it? Would you be able to offload some layers to your 3090s and use the DGX Spark unified memory for the other layers or whatever? Is this a thing? Or is it not worth even trying? Just curious. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/42fhtkk1jhvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o87u2f/i_know_the_dgx_spark_isnt_what_a_lot_people_hoped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o87u2f/i_know_the_dgx_spark_isnt_what_a_lot_people_hoped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T14:35:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7b5i4</id>
    <title>Apple unveils M5</title>
    <updated>2025-10-15T13:34:26+00:00</updated>
    <author>
      <name>/u/Agreeable-Rest9162</name>
      <uri>https://old.reddit.com/user/Agreeable-Rest9162</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b5i4/apple_unveils_m5/"&gt; &lt;img alt="Apple unveils M5" src="https://preview.redd.it/5ehnojlm2avf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bbc46c6e19f88c18588d2f5384d7fb2dd4717f50" title="Apple unveils M5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following the iPhone 17 AI accelerators, most of us were expecting the same tech to be added to M5. Here it is! Lets see what M5 Pro &amp;amp; Max will add. The speedup from M4 to M5 seems to be around 3.5x for prompt processing. &lt;/p&gt; &lt;p&gt;Faster SSDs &amp;amp; RAM:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Additionally, with up to 2x faster SSD performance than the prior generation, the new 14-inch MacBook Pro lets users load a local LLM faster, and they can now choose up to 4TB of storage. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;150GB/s of unified memory bandwidth&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agreeable-Rest9162"&gt; /u/Agreeable-Rest9162 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5ehnojlm2avf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b5i4/apple_unveils_m5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7b5i4/apple_unveils_m5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T13:34:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1o891o4</id>
    <title>Tensor Logic: The Language of AI</title>
    <updated>2025-10-16T15:21:52+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Progress in AI is hindered by the lack of a programming language with all the requisite features. Libraries like PyTorch and TensorFlow provide automatic differentiation and efficient GPU implementation, but are additions to Python, which was never intended for AI. Their lack of support for automated reasoning and knowledge acquisition has led to a long and costly series of hacky attempts to tack them on. On the other hand, AI languages like LISP an Prolog lack scalability and support for learning. This paper proposes tensor logic, a language that solves these problems by unifying neural and symbolic AI at a fundamental level. The sole construct in tensor logic is the tensor equation, based on the observation that logical rules and Einstein summation are essentially the same operation, and all else can be reduced to them. I show how to elegantly implement key forms of neural, symbolic and statistical AI in tensor logic, including transformers, formal reasoning, kernel machines and graphical models. Most importantly, tensor logic makes new directions possible, such as sound reasoning in embedding space. This combines the scalability and learnability of neural networks with the reliability and transparency of symbolic reasoning, and is potentially a basis for the wider adoption of AI.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2510.12269"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o891o4/tensor_logic_the_language_of_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o891o4/tensor_logic_the_language_of_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T15:21:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1o88042</id>
    <title>but can someone correct me, I'm curious how an LLM can generate new hypotheses if it is based only on the prediction of the next token, isn't gemma a simple LLM trained on medical data ?</title>
    <updated>2025-10-16T14:41:54+00:00</updated>
    <author>
      <name>/u/panspective</name>
      <uri>https://old.reddit.com/user/panspective</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o88042/but_can_someone_correct_me_im_curious_how_an_llm/"&gt; &lt;img alt="but can someone correct me, I'm curious how an LLM can generate new hypotheses if it is based only on the prediction of the next token, isn't gemma a simple LLM trained on medical data ?" src="https://preview.redd.it/nwhvk3i6khvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7b9b73c809acb0d60aaa1c286a9e4054969b4c4" title="but can someone correct me, I'm curious how an LLM can generate new hypotheses if it is based only on the prediction of the next token, isn't gemma a simple LLM trained on medical data ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panspective"&gt; /u/panspective &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nwhvk3i6khvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o88042/but_can_someone_correct_me_im_curious_how_an_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o88042/but_can_someone_correct_me_im_curious_how_an_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T14:41:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o808av</id>
    <title>I fine-tuned Qwen3-VL (4B &amp; 8B) on a free Colab instance using TRL (SFT and GRPO)!</title>
    <updated>2025-10-16T07:58:52+00:00</updated>
    <author>
      <name>/u/External-Rub5414</name>
      <uri>https://old.reddit.com/user/External-Rub5414</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've created a couple of notebook that work for free on Colab (T4 GPU) to fine-tune the new Qwen3-VL small and dense vision-language models (4B and 8B). Both the Instruct and Thinking variants are supported.&lt;/p&gt; &lt;p&gt;They use &lt;strong&gt;TRL&lt;/strong&gt;, which handles most of the training complexity so you can focus entirely on the specific task you want to fine-tune for.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;SFT&lt;/strong&gt; notebook: fine-tunes with a dataset to refine the model's response style: &lt;a href="https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_qwen_vl.ipynb"&gt;https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_qwen_vl.ipynb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GRPO&lt;/strong&gt; notebook: includes two reward functions to make the non-reasoning model learn to reason (&lt;a href="https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/grpo_qwen3_vl.ipynb"&gt;https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/grpo_qwen3_vl.ipynb&lt;/a&gt;): &lt;ol&gt; &lt;li&gt;A tag-based reward that checks for &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;answer&amp;gt;&lt;/code&gt; sections.&lt;/li&gt; &lt;li&gt;A length-based reward that discourages overthinking and checks correctness.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both notebooks can be run on a free Colab instance, but can also be scaled up for more advanced setups. The notebooks can also be accessed here: &lt;a href="https://github.com/huggingface/trl/tree/main/examples/notebooks"&gt;https://github.com/huggingface/trl/tree/main/examples/notebooks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feedback and experiments are welcome!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External-Rub5414"&gt; /u/External-Rub5414 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o808av/i_finetuned_qwen3vl_4b_8b_on_a_free_colab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o808av/i_finetuned_qwen3vl_4b_8b_on_a_free_colab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o808av/i_finetuned_qwen3vl_4b_8b_on_a_free_colab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T07:58:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1o89znk</id>
    <title>A new, super simple LLM benchmark for testing changes across models, quants, parameters, samplers, engines, etc</title>
    <updated>2025-10-16T15:57:09+00:00</updated>
    <author>
      <name>/u/Eisenstein</name>
      <uri>https://old.reddit.com/user/Eisenstein</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o89znk/a_new_super_simple_llm_benchmark_for_testing/"&gt; &lt;img alt="A new, super simple LLM benchmark for testing changes across models, quants, parameters, samplers, engines, etc" src="https://external-preview.redd.it/0joRre5VS2PgnS7WJD0-RjSuntbOAn34cwNgbAHQMrk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92e5b40ebc550a11ecbbf21117405dc916931b9e" title="A new, super simple LLM benchmark for testing changes across models, quants, parameters, samplers, engines, etc" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eisenstein"&gt; /u/Eisenstein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jabberjabberjabber/Context-Tester/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o89znk/a_new_super_simple_llm_benchmark_for_testing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o89znk/a_new_super_simple_llm_benchmark_for_testing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T15:57:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7x7ss</id>
    <title>GLM 4.5 Air AWQ 4bit on RTX Pro 6000 with vllm</title>
    <updated>2025-10-16T04:47:49+00:00</updated>
    <author>
      <name>/u/notaDestroyer</name>
      <uri>https://old.reddit.com/user/notaDestroyer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x7ss/glm_45_air_awq_4bit_on_rtx_pro_6000_with_vllm/"&gt; &lt;img alt="GLM 4.5 Air AWQ 4bit on RTX Pro 6000 with vllm" src="https://b.thumbs.redditmedia.com/vckUDC6hgDdCUoZhjQ0jn7hdsNydHHYam8wvadpbBSo.jpg" title="GLM 4.5 Air AWQ 4bit on RTX Pro 6000 with vllm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/g45oegzplevf1.png?width=5379&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44f1bf58f80336a1524c4fd128d5e07a6034f517"&gt;https://preview.redd.it/g45oegzplevf1.png?width=5379&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=44f1bf58f80336a1524c4fd128d5e07a6034f517&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ran benchmark of cpatonn/GLM-4.5-Air-AWQ-4bit on a single Pro 6000 with vllm. Nvidia Driver Version: 580.95.05 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notaDestroyer"&gt; /u/notaDestroyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x7ss/glm_45_air_awq_4bit_on_rtx_pro_6000_with_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x7ss/glm_45_air_awq_4bit_on_rtx_pro_6000_with_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7x7ss/glm_45_air_awq_4bit_on_rtx_pro_6000_with_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T04:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o86h5j</id>
    <title>What MoE model sizes and capabilities are currently missing in the open weight ecosystem?</title>
    <updated>2025-10-16T13:41:40+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As someone who trains models, I’d love to know if you have specific requests for model size or capabilities you’d like to see in a (fully) open MoE model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o86h5j/what_moe_model_sizes_and_capabilities_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o86h5j/what_moe_model_sizes_and_capabilities_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o86h5j/what_moe_model_sizes_and_capabilities_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T13:41:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7gpr8</id>
    <title>Got the DGX Spark - ask me anything</title>
    <updated>2025-10-15T17:02:50+00:00</updated>
    <author>
      <name>/u/sotech117</name>
      <uri>https://old.reddit.com/user/sotech117</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"&gt; &lt;img alt="Got the DGX Spark - ask me anything" src="https://preview.redd.it/9mr835ne4bvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42dc8e85dcff8b55d4174e98495bb8d2d144fd7d" title="Got the DGX Spark - ask me anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If there’s anything you want me to benchmark (or want to see in general), let me know, and I’ll try to reply to your comment. I will be playing with this all night trying a ton of different models I’ve always wanted to run. &lt;/p&gt; &lt;p&gt;(&amp;amp; shoutout to microcenter my goats!)&lt;/p&gt; &lt;p&gt;updates: Hit it hard with Wan2.2 via ComfyUI, base template but upped the resolution to &lt;a href="mailto:720p@24fps"&gt;720p@24fps&lt;/a&gt;. Extremely easy to setup. NVIDIA-SMI queries are trolling, giving lots of N/A.&lt;/p&gt; &lt;p&gt;Max-acpi-temp: 91.8 C (&lt;a href="https://drive.mfoi.dev/s/pDZm9F3axRnoGca"&gt;https://drive.mfoi.dev/s/pDZm9F3axRnoGca&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Max-gpu-tdp: 101 W (&lt;a href="https://drive.mfoi.dev/s/LdwLdzQddjiQBKe"&gt;https://drive.mfoi.dev/s/LdwLdzQddjiQBKe&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Max-watt-consumption (from-wall): 195.5 W (&lt;a href="https://drive.mfoi.dev/s/643GLEgsN5sBiiS"&gt;https://drive.mfoi.dev/s/643GLEgsN5sBiiS&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;final-output: &lt;a href="https://drive.mfoi.dev/s/rWe9yxReqHxB9Py"&gt;https://drive.mfoi.dev/s/rWe9yxReqHxB9Py&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Physical observations: Under heavy load, it gets uncomfortably hot to the touch, and the fan noise is prevalent and almost makes a grinding sound (?). Unfortunately, mine has some coil whine during computation (, which is may more noticeable than the fan noise). It's really not a &amp;quot;on your desk machine&amp;quot; - makes more sense in a server rack using ssh and/or webtools.&lt;/p&gt; &lt;p&gt;coil-whine: &lt;a href="https://drive.mfoi.dev/s/eGcxiMXZL3NXQYT"&gt;https://drive.mfoi.dev/s/eGcxiMXZL3NXQYT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Still and setting up a ton of requested models (mostly LLMs) currently. Should have some llm and image/video gen numbers tonight. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sotech117"&gt; /u/sotech117 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9mr835ne4bvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7gpr8/got_the_dgx_spark_ask_me_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T17:02:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o83b2i</id>
    <title>Qwen3 Next 80b FP8 with vllm on Pro 6000 Blackwell</title>
    <updated>2025-10-16T11:11:58+00:00</updated>
    <author>
      <name>/u/notaDestroyer</name>
      <uri>https://old.reddit.com/user/notaDestroyer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o83b2i/qwen3_next_80b_fp8_with_vllm_on_pro_6000_blackwell/"&gt; &lt;img alt="Qwen3 Next 80b FP8 with vllm on Pro 6000 Blackwell" src="https://b.thumbs.redditmedia.com/YJxZs33mPxgOa7fkc4zIPMKaCT3QAzLt14Sr270bH7A.jpg" title="Qwen3 Next 80b FP8 with vllm on Pro 6000 Blackwell" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPU: NVIDIA RTX Pro 6000 Blackwell Edition (96GB VRAM)&lt;/p&gt; &lt;p&gt;- Driver: 580.95.05&lt;/p&gt; &lt;p&gt;- CUDA: 13.0&lt;/p&gt; &lt;p&gt;- Compute Capability: 9.0 (Blackwell)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tf7qkz8ligvf1.png?width=7569&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=48977ab9548affc46738478260262bbf19184782"&gt;https://preview.redd.it/tf7qkz8ligvf1.png?width=7569&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=48977ab9548affc46738478260262bbf19184782&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Software:&lt;/p&gt; &lt;p&gt;- vLLM: v0.11.1rc2.dev72+gf7d318de2 (nightly)&lt;/p&gt; &lt;p&gt;- Attention Backend: **FlashInfer** (with JIT autotuning)&lt;/p&gt; &lt;p&gt;- Quantization: FP8 W8A8&lt;/p&gt; &lt;p&gt;- Python: 3.12.12&lt;/p&gt; &lt;p&gt;- PyTorch with CUDA 12.4 backend (forward compatible with CUDA 13.0 driver)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notaDestroyer"&gt; /u/notaDestroyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o83b2i/qwen3_next_80b_fp8_with_vllm_on_pro_6000_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o83b2i/qwen3_next_80b_fp8_with_vllm_on_pro_6000_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o83b2i/qwen3_next_80b_fp8_with_vllm_on_pro_6000_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T11:11:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8clx0</id>
    <title>mtmd : support home-cooked Mistral Small Omni by ngxson · Pull Request #14928 · ggml-org/llama.cpp</title>
    <updated>2025-10-16T17:32:29+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8clx0/mtmd_support_homecooked_mistral_small_omni_by/"&gt; &lt;img alt="mtmd : support home-cooked Mistral Small Omni by ngxson · Pull Request #14928 · ggml-org/llama.cpp" src="https://external-preview.redd.it/LDnY4VtL8nEMhrRnSn7xRje529JikFQ9xlCA17hsv2k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43014e497d8695214852c7b34ef392748272ec68" title="mtmd : support home-cooked Mistral Small Omni by ngxson · Pull Request #14928 · ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Support a home-cooked version of Mistral Small which can take &lt;strong&gt;both audio and image&lt;/strong&gt; as input&lt;/p&gt; &lt;p&gt;Link to GGUF: &lt;a href="https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF"&gt;https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(This is a multimodal model created by merging Mistral Small 2506 (with vision capabilities) and Voxtral 2507 (with audio capabilities) using a modified version of the &lt;code&gt;mergekit&lt;/code&gt; tool.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14928"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8clx0/mtmd_support_homecooked_mistral_small_omni_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8clx0/mtmd_support_homecooked_mistral_small_omni_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T17:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o88j9i</id>
    <title>HuggingChat Omni: new chat app by Hugging Face</title>
    <updated>2025-10-16T15:02:18+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HuggingChat is back! the main new feature is auto-routing to the best open source model for your query. Making it competitive and often better than base chatgpt. &lt;/p&gt; &lt;p&gt;more info about it: &lt;a href="https://x.com/victormustar/status/1978817795312808065?s=46"&gt;https://x.com/victormustar/status/1978817795312808065?s=46&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o88j9i/huggingchat_omni_new_chat_app_by_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o88j9i/huggingchat_omni_new_chat_app_by_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T15:02:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o88w9c</id>
    <title>ARM Partners with Meta</title>
    <updated>2025-10-16T15:16:10+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o88w9c/arm_partners_with_meta/"&gt; &lt;img alt="ARM Partners with Meta" src="https://preview.redd.it/wpf7pikaqhvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ab9b69847afb2486fa0aa6d707cbb6b7c878439" title="ARM Partners with Meta" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ARM Partners with Meta for Data Center and Next Generation Software, Collaboration May Be Interesting Info : &lt;a href="https://x.com/Arm/status/1978494349966025044?t=9tw4dYon0ecqebNQfE5rsQ&amp;amp;s=19"&gt;https://x.com/Arm/status/1978494349966025044?t=9tw4dYon0ecqebNQfE5rsQ&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wpf7pikaqhvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o88w9c/arm_partners_with_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o88w9c/arm_partners_with_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T15:16:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1o7pe1u</id>
    <title>gigaResearch</title>
    <updated>2025-10-15T22:34:35+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"&gt; &lt;img alt="gigaResearch" src="https://preview.redd.it/nb2hmgqircvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71c101f2683e8df117cbc2a9abd685bcac5cbce0" title="gigaResearch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nb2hmgqircvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o7pe1u/gigaresearch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T22:34:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1o82kta</id>
    <title>NVIDIA DGX Spark – A Non-Sponsored Review (Strix Halo Comparison, Pros &amp; Cons)</title>
    <updated>2025-10-16T10:30:22+00:00</updated>
    <author>
      <name>/u/Corylus-Core</name>
      <uri>https://old.reddit.com/user/Corylus-Core</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;NVIDIA DGX Spark – A Non-Sponsored Review (Strix Halo Comparison, Pros &amp;amp; Cons)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=Pww8rIzr1pg"&gt;https://www.youtube.com/watch?v=Pww8rIzr1pg&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Corylus-Core"&gt; /u/Corylus-Core &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o82kta/nvidia_dgx_spark_a_nonsponsored_review_strix_halo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o82kta/nvidia_dgx_spark_a_nonsponsored_review_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o82kta/nvidia_dgx_spark_a_nonsponsored_review_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T10:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8d3gt</id>
    <title>Internship with local LLMs at AMD!</title>
    <updated>2025-10-16T17:50:36+00:00</updated>
    <author>
      <name>/u/dholanda_amd</name>
      <uri>https://old.reddit.com/user/dholanda_amd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks! &lt;/p&gt; &lt;p&gt;My team and I at AMD have been having a lot of fun developing agents, building next-gen apps for local LLMs, fine-tuning models, and posting a lot of that here on &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;) . We’re now looking for a (ideally grad) student who loves hands-on local AI for an internship on our team.&lt;/p&gt; &lt;p&gt;Our team really tries to contribute quite a bit to the open source community. One of our key projects is &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;Lemonade &lt;/a&gt;(Ollama-like local app with a really cool Discord community).&lt;/p&gt; &lt;p&gt;Here is the rough description of what we envision for this position:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Develop an agentic LLM framework, designed to operate effectively on client devices&lt;/li&gt; &lt;li&gt;Build and refine the framework by developing a focused application (from computer use to database reasoning - your choice!)&lt;/li&gt; &lt;li&gt;Experiment with fine-tuning, LoRAs, RAG, and agent architectures&lt;/li&gt; &lt;li&gt;Work side-by-side with the Lemonade team =D&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Experience with some of the above (e.g., fine-tuning) is a huge bonus. We also love people who are active on open-source GitHub projects, Hugging Face, and of course &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ;)&lt;/p&gt; &lt;p&gt;If you’re excited about this opportunity with local AI, let’s chat! Please apply using the link below. Please also feel free to ask questions here or DM me on Discord (look for Daniel H).&lt;/p&gt; &lt;p&gt;Excited to hear from this community!&lt;/p&gt; &lt;p&gt;Details here: careers (dot) amd (dot) com/careers-home/jobs/70208&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dholanda_amd"&gt; /u/dholanda_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8d3gt/internship_with_local_llms_at_amd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8d3gt/internship_with_local_llms_at_amd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8d3gt/internship_with_local_llms_at_amd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T17:50:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8c9ta</id>
    <title>new 1B LLM by meta</title>
    <updated>2025-10-16T17:20:06+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8c9ta/new_1b_llm_by_meta/"&gt; &lt;img alt="new 1B LLM by meta" src="https://external-preview.redd.it/jpWPa_miOIzMa-yE2f-cKYYuEKEP1Bglx0QJxru_lGs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e2417b6932d5a1fcaf49ac7d2561558b6a9873e" title="new 1B LLM by meta" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/5adjo04ecivf1.png?width=825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c09b47aee7f7597d580d6dbe1eec7e6429504d41"&gt;https://preview.redd.it/5adjo04ecivf1.png?width=825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c09b47aee7f7597d580d6dbe1eec7e6429504d41&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/facebook/MobileLLM-Pro"&gt;facebook/MobileLLM-Pro · Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8c9ta/new_1b_llm_by_meta/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8c9ta/new_1b_llm_by_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8c9ta/new_1b_llm_by_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T17:20:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o84b36</id>
    <title>Qwen3-30B-A3B FP8 on RTX Pro 6000 blackwell with vllm</title>
    <updated>2025-10-16T12:04:11+00:00</updated>
    <author>
      <name>/u/notaDestroyer</name>
      <uri>https://old.reddit.com/user/notaDestroyer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o84b36/qwen330ba3b_fp8_on_rtx_pro_6000_blackwell_with/"&gt; &lt;img alt="Qwen3-30B-A3B FP8 on RTX Pro 6000 blackwell with vllm" src="https://b.thumbs.redditmedia.com/tevD9Ijs-RE6cQcddGazJZY--0Lzdu42Yn4uKrDPRjI.jpg" title="Qwen3-30B-A3B FP8 on RTX Pro 6000 blackwell with vllm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Power limit set to 450w &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Short Context (1K tokens):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single user: 88.4 tok/s&lt;/li&gt; &lt;li&gt;10 concurrent users: &lt;strong&gt;652 tok/s&lt;/strong&gt; throughput&lt;/li&gt; &lt;li&gt;Latency: 5.65s → 7.65s (1→10 users)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Long Context (256K tokens):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single user: 22.0 tok/s&lt;/li&gt; &lt;li&gt;10 concurrent users: &lt;strong&gt;115.5 tok/s&lt;/strong&gt; throughput&lt;/li&gt; &lt;li&gt;Latency: 22.7s → 43.2s (1→10 users)&lt;/li&gt; &lt;li&gt;Still able to handle 10 concurrent requests!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Sweet Spot (32K-64K context):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;64K @ 10 users: 311 tok/s total, 31 tok/s per user&lt;/li&gt; &lt;li&gt;32K @ 10 users: 413 tok/s total, 41 tok/s per user&lt;/li&gt; &lt;li&gt;Best balance of context length and throughput&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;FP8 quantization really shines here - getting 115 tok/s aggregate at 256K context with 10 users is wild, even with the power constraint. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x9t4ttsvrgvf1.png?width=7590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c86bf3cc42032a595ee4d02b2c78986da150836"&gt;https://preview.redd.it/x9t4ttsvrgvf1.png?width=7590&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c86bf3cc42032a595ee4d02b2c78986da150836&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notaDestroyer"&gt; /u/notaDestroyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o84b36/qwen330ba3b_fp8_on_rtx_pro_6000_blackwell_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o84b36/qwen330ba3b_fp8_on_rtx_pro_6000_blackwell_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o84b36/qwen330ba3b_fp8_on_rtx_pro_6000_blackwell_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T12:04:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8b00e</id>
    <title>GLM 4.6 is hilarious, I wish I could run this on my own PC lol</title>
    <updated>2025-10-16T16:34:48+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I have a fun task for you! Search the web to find the most (infamously) popular annoying LLM clichés (aka AI slop) in roleplay and creative writing and generate a sentence using only those phrases. Make the sentence as long as possible to cover them all.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;GLM 4.6 response:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Based on the search results, I've compiled the most notoriously annoying LLM clichés in roleplay and creative writing. Here's a sentence that combines as many of these overused phrases as possible:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;In a world where the tapestry of human emotions unfolds like a delicate symphony, I'm sorry, but as an AI language model, I cannot delve into unlocking your full potential without first embarking on a journey to harness the vast landscape of creativity, though I'm just an AI and don't have feelings, but I'm here to help you explore the intricate dance of storytelling where every thread weaves a new chapter in the grand narrative of imagination.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8b00e/glm_46_is_hilarious_i_wish_i_could_run_this_on_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8b00e/glm_46_is_hilarious_i_wish_i_could_run_this_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8b00e/glm_46_is_hilarious_i_wish_i_could_run_this_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T16:34:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o81rvs</id>
    <title>Google C2S-Scale 27B (based on Gemma) built with Yale generated a novel hypothesis about cancer cellular behavior - Model + resources are now on Hugging Face and GitHub</title>
    <updated>2025-10-16T09:41:04+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o81rvs/google_c2sscale_27b_based_on_gemma_built_with/"&gt; &lt;img alt="Google C2S-Scale 27B (based on Gemma) built with Yale generated a novel hypothesis about cancer cellular behavior - Model + resources are now on Hugging Face and GitHub" src="https://b.thumbs.redditmedia.com/h0BE1gNO8S-6xv6b1X5IIoHb8CSHWZoS7YxS0LFbuxA.jpg" title="Google C2S-Scale 27B (based on Gemma) built with Yale generated a novel hypothesis about cancer cellular behavior - Model + resources are now on Hugging Face and GitHub" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog post: How a Gemma model helped discover a new potential cancer therapy pathway - We’re launching a new 27 billion parameter foundation model for single-cell analysis built on the Gemma family of open models.: &lt;a href="https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/"&gt;https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/&lt;/a&gt;&lt;br /&gt; Hugging Face: &lt;a href="https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B"&gt;https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B&lt;/a&gt;&lt;br /&gt; Scientific preprint on bioRxiv: &lt;a href="https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2"&gt;https://www.biorxiv.org/content/10.1101/2025.04.14.648850v2&lt;/a&gt;&lt;br /&gt; Code on GitHub: &lt;a href="https://github.com/vandijklab/cell2sentence"&gt;https://github.com/vandijklab/cell2sentence&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o81rvs"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o81rvs/google_c2sscale_27b_based_on_gemma_built_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o81rvs/google_c2sscale_27b_based_on_gemma_built_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T09:41:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o866vl</id>
    <title>PaddleOCR-VL, is better than private models</title>
    <updated>2025-10-16T13:29:48+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o866vl/paddleocrvl_is_better_than_private_models/"&gt; &lt;img alt="PaddleOCR-VL, is better than private models" src="https://b.thumbs.redditmedia.com/X4U2z8D2mUefEIEuBe11hVIGCJrPT-oF1EOiA8c0dyw.jpg" title="PaddleOCR-VL, is better than private models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/PaddlePaddle/status/1978809999263781290?t=mcHYAF7osq3MmicjMLi0IQ&amp;amp;s=19"&gt;https://x.com/PaddlePaddle/status/1978809999263781290?t=mcHYAF7osq3MmicjMLi0IQ&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o866vl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o866vl/paddleocrvl_is_better_than_private_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o866vl/paddleocrvl_is_better_than_private_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T13:29:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8ay34</id>
    <title>GLM 4.6 air when?</title>
    <updated>2025-10-16T16:32:48+00:00</updated>
    <author>
      <name>/u/gamblingapocalypse</name>
      <uri>https://old.reddit.com/user/gamblingapocalypse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8ay34/glm_46_air_when/"&gt; &lt;img alt="GLM 4.6 air when?" src="https://preview.redd.it/1bhgri4w3ivf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=660b0d931d85bfb508e26c3e8647b3b8a75f8708" title="GLM 4.6 air when?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gamblingapocalypse"&gt; /u/gamblingapocalypse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1bhgri4w3ivf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8ay34/glm_46_air_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8ay34/glm_46_air_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T16:32:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
