<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-14T18:53:32+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ovxksu</id>
    <title>Jan-v2-VL: 8B model for long-horizon tasks, improving Qwen3-VL-8B‚Äôs agentic capabilities almost 10x</title>
    <updated>2025-11-13T10:22:48+00:00</updated>
    <author>
      <name>/u/Delicious_Focus3465</name>
      <uri>https://old.reddit.com/user/Delicious_Focus3465</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxksu/janv2vl_8b_model_for_longhorizon_tasks_improving/"&gt; &lt;img alt="Jan-v2-VL: 8B model for long-horizon tasks, improving Qwen3-VL-8B‚Äôs agentic capabilities almost 10x" src="https://external-preview.redd.it/bmthZnk4cjV4ejBnMYgdXr3Xr8K8l3LMKEIqfiXLStzaSkNnB6704_pmF3PX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0626efa53bd219b2126a6e5fa2884ec700c482b3" title="Jan-v2-VL: 8B model for long-horizon tasks, improving Qwen3-VL-8B‚Äôs agentic capabilities almost 10x" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, this is Bach from the Jan team. We‚Äôre releasing Jan-v2-VL, an 8B vision‚Äìlanguage model aimed at long-horizon, multi-step tasks starting from browser use.&lt;/p&gt; &lt;p&gt;Jan-v2-VL-high executes 49 steps without failure on the Long-Horizon Execution benchmark, while the base model (Qwen3-VL-8B-Thinking) stops at 5 and other similar-scale VLMs stop between 1 and 2.&lt;/p&gt; &lt;p&gt;Across text and multimodal benchmarks, it matches or slightly improves on the base model, so you get higher long-horizon stability without giving up reasoning or vision quality.&lt;/p&gt; &lt;p&gt;We're releasing 3 variants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan-v2-VL-low (efficiency-oriented)&lt;/li&gt; &lt;li&gt;Jan-v2-VL-med (balanced)&lt;/li&gt; &lt;li&gt;Jan-v2-VL-high (deeper reasoning and longer execution)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How to run the model&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Download Jan-v2-VL from the Model Hub in Jan&lt;/li&gt; &lt;li&gt;Open the model‚Äôs settings and enable Tools and Vision&lt;/li&gt; &lt;li&gt;Enable BrowserUse MCP (or your preferred MCP setup for browser control)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can also run the model with vLLM or llama.cpp.&lt;/p&gt; &lt;p&gt;Recommended parameters&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;temperature: 1.0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;top_p: 0.95&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;top_k: 20&lt;/code&gt;&lt;/li&gt; &lt;li&gt;repetition_penalty&lt;code&gt;: 1.0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;presence_penalty&lt;code&gt;: 1.5&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/collections/janhq/jan-v2-vl"&gt;https://huggingface.co/collections/janhq/jan-v2-vl&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Jan app: &lt;a href="https://github.com/janhq/jan"&gt;https://github.com/janhq/jan&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're also working on a browser extension to make model-driven browser automation faster and more reliable on top of this.&lt;/p&gt; &lt;p&gt;Credit to the Qwen team for the Qwen3-VL-8B-Thinking base model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious_Focus3465"&gt; /u/Delicious_Focus3465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/go4j38r5xz0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxksu/janv2vl_8b_model_for_longhorizon_tasks_improving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxksu/janv2vl_8b_model_for_longhorizon_tasks_improving/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T10:22:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow3anq</id>
    <title>Rejected for not using LangChain/LangGraph?</title>
    <updated>2025-11-13T15:00:14+00:00</updated>
    <author>
      <name>/u/dougeeai</name>
      <uri>https://old.reddit.com/user/dougeeai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today I got rejected after a job interview for not being &amp;quot;technical enough&amp;quot; because I use PyTorch/CUDA/GGUF directly with FastAPI microservices for multi-agent systems instead of LangChain/LangGraph in production.&lt;/p&gt; &lt;p&gt;They asked about 'efficient data movement in LangGraph' - I explained I work at a lower level with bare metal for better performance and control. Later it was revealed they mostly just use APIs to Claude/OpenAI/Bedrock.&lt;/p&gt; &lt;p&gt;I am legitimately asking - not venting - Am I missing something by not using LangChain? Is it becoming a required framework for AI engineering roles, or is this just framework bias?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Should I be adopting it even though I haven't seen performance benefits for my use cases?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dougeeai"&gt; /u/dougeeai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3anq/rejected_for_not_using_langchainlanggraph/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3anq/rejected_for_not_using_langchainlanggraph/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3anq/rejected_for_not_using_langchainlanggraph/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T15:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow9pdh</id>
    <title>new ops required by Qwen3 Next and Kimi Linear have been merged into llama.cpp</title>
    <updated>2025-11-13T19:00:34+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow9pdh/new_ops_required_by_qwen3_next_and_kimi_linear/"&gt; &lt;img alt="new ops required by Qwen3 Next and Kimi Linear have been merged into llama.cpp" src="https://external-preview.redd.it/5ziszOa8NRon-ATgGFg5Bv3PXC9P_Gr-hIwXsD0snnU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61860c512fcc2dad6ebe8431387929cdf3acb61d" title="new ops required by Qwen3 Next and Kimi Linear have been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 Next is still in progress &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095"&gt;https://github.com/ggml-org/llama.cpp/pull/16095&lt;/a&gt;&lt;/p&gt; &lt;p&gt;but this merge was needed to unblock it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17063"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow9pdh/new_ops_required_by_qwen3_next_and_kimi_linear/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow9pdh/new_ops_required_by_qwen3_next_and_kimi_linear/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T19:00:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow3kj3</id>
    <title>Qwen model coming soon üëÄ</title>
    <updated>2025-11-13T15:10:39+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3kj3/qwen_model_coming_soon/"&gt; &lt;img alt="Qwen model coming soon üëÄ" src="https://preview.redd.it/ibsrtr3ri11g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91dd786919de8cd49f495890d2b241fd22bf2f83" title="Qwen model coming soon üëÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ibsrtr3ri11g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3kj3/qwen_model_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3kj3/qwen_model_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T15:10:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1owpxdd</id>
    <title>Anyone trying out Motif 2 13B?</title>
    <updated>2025-11-14T07:15:12+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just saw that a S Korean group released this model: &lt;a href="https://huggingface.co/collections/Motif-Technologies/motif-2-127b"&gt;Motif 2 12.7 B&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The benchmarks appear impressive for the size (whatever they are worth).&lt;/p&gt; &lt;p&gt;Has anyone tried this model yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owpxdd/anyone_trying_out_motif_2_13b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owpxdd/anyone_trying_out_motif_2_13b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owpxdd/anyone_trying_out_motif_2_13b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T07:15:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow8j6d</id>
    <title>The return of the modded 4090 48GB</title>
    <updated>2025-11-13T18:17:12+00:00</updated>
    <author>
      <name>/u/king_priam_of_Troy</name>
      <uri>https://old.reddit.com/user/king_priam_of_Troy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow8j6d/the_return_of_the_modded_4090_48gb/"&gt; &lt;img alt="The return of the modded 4090 48GB" src="https://b.thumbs.redditmedia.com/LgVvEhALpx4xDsp8AmNOoxzbQcsL_pEGUDe4iwXtJ-M.jpg" title="The return of the modded 4090 48GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last month I bought a 4090 48GB in ShenZhen. I had to put this project on hold for a while but it's back.&lt;/p&gt; &lt;p&gt;The card is really fast even with my poor Gen3 4x PCIe connector. I can't put it inside as I can't find any compatible power cable.&lt;/p&gt; &lt;p&gt;I'm running at 150 tokens/second with GPT-OSS 20B from my first tests.&lt;/p&gt; &lt;p&gt;(This is a follow up of &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nifajh/i%5C_bought%5C_a%5C_modded%5C_4090%5C_48gb%5C_in%5C_shenzhen%5C_this%5C_is/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1nifajh/i\_bought\_a\_modded\_4090\_48gb\_in\_shenzhen\_this\_is/&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/king_priam_of_Troy"&gt; /u/king_priam_of_Troy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ow8j6d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow8j6d/the_return_of_the_modded_4090_48gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow8j6d/the_return_of_the_modded_4090_48gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T18:17:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ownirj</id>
    <title>MCP is great in theory, but it‚Äôs not always a blanket yes</title>
    <updated>2025-11-14T04:56:53+00:00</updated>
    <author>
      <name>/u/Miserable_Agent_9006</name>
      <uri>https://old.reddit.com/user/Miserable_Agent_9006</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been building agentic workflows in production lately and spent some time exploring MCP. It‚Äôs clean, standardized, and clearly the direction things are headed.&lt;/p&gt; &lt;p&gt;But I think when you're trying to move fast, it‚Äôs a bit heavy.&lt;/p&gt; &lt;p&gt;- another server to run and maintain&lt;/p&gt; &lt;p&gt;- extra network hops&lt;/p&gt; &lt;p&gt;- schema wrapping + versioning overhead&lt;/p&gt; &lt;p&gt;The lightweight ‚Äúhandshake‚Äù between agents and APIs works well enough for now. MCP makes sense when you‚Äôve got scale, multiple services, or teams to align.&lt;/p&gt; &lt;p&gt;I‚Äôm sure we‚Äôll adopt it eventually, but for now my team and I decided to skip it.&lt;/p&gt; &lt;p&gt;Anyone else taking a similar approach?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Miserable_Agent_9006"&gt; /u/Miserable_Agent_9006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ownirj/mcp_is_great_in_theory_but_its_not_always_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ownirj/mcp_is_great_in_theory_but_its_not_always_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ownirj/mcp_is_great_in_theory_but_its_not_always_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T04:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox3n0t</id>
    <title>Fixed KV cache bug in ByteDance Ouro-1.4B - 1.7x speedup</title>
    <updated>2025-11-14T17:59:29+00:00</updated>
    <author>
      <name>/u/Livid_Fisherman_9884</name>
      <uri>https://old.reddit.com/user/Livid_Fisherman_9884</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I encountered a KV-cache bug in ByteDance's &lt;strong&gt;Ouro-1.4B&lt;/strong&gt; that caused out-of-bounds errors and slow inference. I created a fix that's now available on PyPI.&lt;/p&gt; &lt;h3&gt;üîç Problem&lt;/h3&gt; &lt;p&gt;The Universal Transformer architecture needs &lt;strong&gt;96‚Äì128 cache indices&lt;/strong&gt;, but &lt;code&gt;DynamicCache&lt;/code&gt; only provides ~30, leading to crashes and degraded performance.&lt;/p&gt; &lt;h3&gt;üõ† Solution&lt;/h3&gt; &lt;p&gt;&lt;code&gt;UniversalTransformerCache&lt;/code&gt; pre-allocates cache indices for all UT steps, eliminating out-of-bounds issues.&lt;/p&gt; &lt;h3&gt;üìà Results&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;1.3√ó‚Äì1.7√ó faster inference&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No more KV cache errors&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;üì¶ Install&lt;/h3&gt; &lt;p&gt;pip install ouro-cache-fix&lt;/p&gt; &lt;h3&gt;üîó Links&lt;/h3&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/Antizana/ouro-cache-fix"&gt;https://github.com/Antizana/ouro-cache-fix&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PyPI: &lt;a href="https://pypi.org/project/ouro-cache-fix/"&gt;https://pypi.org/project/ouro-cache-fix/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking for testers and feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Livid_Fisherman_9884"&gt; /u/Livid_Fisherman_9884 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox3n0t/fixed_kv_cache_bug_in_bytedance_ouro14b_17x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox3n0t/fixed_kv_cache_bug_in_bytedance_ouro14b_17x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ox3n0t/fixed_kv_cache_bug_in_bytedance_ouro14b_17x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T17:59:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox1icc</id>
    <title>LLMs from emacs</title>
    <updated>2025-11-14T16:40:47+00:00</updated>
    <author>
      <name>/u/GregariousWolf</name>
      <uri>https://old.reddit.com/user/GregariousWolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox1icc/llms_from_emacs/"&gt; &lt;img alt="LLMs from emacs" src="https://b.thumbs.redditmedia.com/SNSWRjSxujhbT9r7PCLy1iybEubGT5fYirJKDo0GAjY.jpg" title="LLMs from emacs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/b3l97npl391g1.jpg?width=2560&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4d005ea34a7167bbfc909c90188f299159d792cd"&gt;https://preview.redd.it/b3l97npl391g1.jpg?width=2560&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4d005ea34a7167bbfc909c90188f299159d792cd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been working on the home lab doing Linux stuff and testing out my LLM orchestration tool. It's not really meant to be used like this. What you see is a utility view to see all the buffers that are open. What it really looks like is emacs because you're editing and compiling and debugging. It started as a convenient way to get a buffer to and fro. Here I can connect them with a pipe, broadcast to multiple models at once, send two outputs to a third for comparison.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GregariousWolf"&gt; /u/GregariousWolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox1icc/llms_from_emacs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox1icc/llms_from_emacs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ox1icc/llms_from_emacs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T16:40:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1owulxd</id>
    <title>Kimi k2 thinking + kilo code really not bad</title>
    <updated>2025-11-14T12:00:43+00:00</updated>
    <author>
      <name>/u/Federal_Spend2412</name>
      <uri>https://old.reddit.com/user/Federal_Spend2412</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm genuinely impressed. Once your AGENTS.md and rules.md are clear enough, kimi k2 thinking + kilo code really seems to be just as capable as Claude 4.0 sonnet, especially when it comes to programming and debugging. It‚Äôs a surprisingly powerful combination.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal_Spend2412"&gt; /u/Federal_Spend2412 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owulxd/kimi_k2_thinking_kilo_code_really_not_bad/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owulxd/kimi_k2_thinking_kilo_code_really_not_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owulxd/kimi_k2_thinking_kilo_code_really_not_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T12:00:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1owu2nn</id>
    <title>We built a framework for generating custom RAG evaluation datasets and released a D&amp;D-based one (open-source)</title>
    <updated>2025-11-14T11:31:58+00:00</updated>
    <author>
      <name>/u/mario_candela</name>
      <uri>https://old.reddit.com/user/mario_candela</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owu2nn/we_built_a_framework_for_generating_custom_rag/"&gt; &lt;img alt="We built a framework for generating custom RAG evaluation datasets and released a D&amp;amp;D-based one (open-source)" src="https://external-preview.redd.it/WM9coASZAPosxQnRrgryGmXh5OYtno3uUsf9XZYu3E8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=40dfbae3947d96d45263d794ae5426b261117e4f" title="We built a framework for generating custom RAG evaluation datasets and released a D&amp;amp;D-based one (open-source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üîó &lt;a href="https://datapizza.tech/it/blog/aij4r/"&gt;Blog post&lt;/a&gt;&lt;br /&gt; üîó &lt;a href="https://github.com/datapizza-labs/rag-dataset-builder"&gt;GitHub repo&lt;/a&gt;&lt;br /&gt; üîó &lt;a href="https://huggingface.co/datasets/datapizza-ai-lab/dnd5e-srd-qa"&gt;Dataset on Hugging Face&lt;/a&gt;&lt;br /&gt; Would love to hear your thoughts, feedback, or ideas on how to improve this! ‚ù§Ô∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mario_candela"&gt; /u/mario_candela &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="http://datapizza.tech/it/blog/aij4r/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owu2nn/we_built_a_framework_for_generating_custom_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owu2nn/we_built_a_framework_for_generating_custom_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T11:31:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow6a9i</id>
    <title>IBM's AI Researchers Patented a 200 yr old Math Technique by Rebranding as AI Interpretability</title>
    <updated>2025-11-13T16:54:38+00:00</updated>
    <author>
      <name>/u/DataBaeBee</name>
      <uri>https://old.reddit.com/user/DataBaeBee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow6a9i/ibms_ai_researchers_patented_a_200_yr_old_math/"&gt; &lt;img alt="IBM's AI Researchers Patented a 200 yr old Math Technique by Rebranding as AI Interpretability" src="https://external-preview.redd.it/bnA5cnNld3owMjFnMV58D9bda3Jb0zpLqYjHalvpbPpYKPrlCJRkL-iXGaPt.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc24dc8d853c78013954d7d9f658801ad9c1a4e7" title="IBM's AI Researchers Patented a 200 yr old Math Technique by Rebranding as AI Interpretability" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IBM AI researchers implemented a Continued Fraction class as linear layers in Pytorch and was awarded a patent for calling backward() on the computation graph. It's pretty bizarre.&lt;/p&gt; &lt;p&gt;Anyone who uses derivatives/power series to work with continued fractions is affected.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Mechanical engineers, Robotics and Industrialists - you can't use Pytorch to find the best number of teeth for your desired gear ratios lest you interfere with IBM's patent.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Pure Mathematicians and Math Educators - I learnt about the patent while investigating Continued Fractions and their relation to elliptic curves. I needed to find an approximate relationship and while I was writing in Torch I stumbled upon the patent.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Numerical programmers - continued fractions and their derivatives are used to approximate errors in algorithm design.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here's the &lt;a href="https://leetarxiv.substack.com/p/ibm-patented-eulers-fractions"&gt;complete writeup with patent links&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataBaeBee"&gt; /u/DataBaeBee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nddv4ewz021g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow6a9i/ibms_ai_researchers_patented_a_200_yr_old_math/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow6a9i/ibms_ai_researchers_patented_a_200_yr_old_math/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T16:54:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1owxob9</id>
    <title>Why aren't there cheap NVLink adapters for RTX 3090s?</title>
    <updated>2025-11-14T14:17:43+00:00</updated>
    <author>
      <name>/u/alex_bit_</name>
      <uri>https://old.reddit.com/user/alex_bit_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is the NVLink only a wire jumper linking both cards together?&lt;/p&gt; &lt;p&gt;Can I make my own homemade connections?&lt;/p&gt; &lt;p&gt;Or are there some chips or other things inside the bridge?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alex_bit_"&gt; /u/alex_bit_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owxob9/why_arent_there_cheap_nvlink_adapters_for_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owxob9/why_arent_there_cheap_nvlink_adapters_for_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owxob9/why_arent_there_cheap_nvlink_adapters_for_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T14:17:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1owyk52</id>
    <title>distil-localdoc.py - SLM assistant for writing Python documentation</title>
    <updated>2025-11-14T14:52:32+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owyk52/distillocaldocpy_slm_assistant_for_writing_python/"&gt; &lt;img alt="distil-localdoc.py - SLM assistant for writing Python documentation" src="https://preview.redd.it/phzq8yxnj81g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07e950f1db9905480e03d620bbe8e37210ea60dc" title="distil-localdoc.py - SLM assistant for writing Python documentation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built an SLM assistant for automatic Python documentation - a Qwen3 0.6B parameter model that generates complete, properly formatted docstrings for your code in Google style. Run it locally, keeping your proprietary code secure! Find it at &lt;a href="https://github.com/distil-labs/distil-localdoc.py"&gt;https://github.com/distil-labs/distil-localdoc.py&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Usage&lt;/h2&gt; &lt;p&gt;We load the model and your Python file. By default we load the downloaded Qwen3 0.6B model and generate Google-style docstrings.&lt;/p&gt; &lt;p&gt;```bash python localdoc.py --file your_script.py&lt;/p&gt; &lt;h1&gt;optionally, specify model and docstring style&lt;/h1&gt; &lt;p&gt;python localdoc.py --file your_script.py --model localdoc_qwen3 --style google ```&lt;/p&gt; &lt;p&gt;The tool will generate an updated file with &lt;code&gt;_documented&lt;/code&gt; suffix (e.g., &lt;code&gt;your_script_documented.py&lt;/code&gt;).&lt;/p&gt; &lt;h2&gt;Features&lt;/h2&gt; &lt;p&gt;The assistant can generate docstrings for: - &lt;strong&gt;Functions&lt;/strong&gt;: Complete parameter descriptions, return values, and raised exceptions - &lt;strong&gt;Methods&lt;/strong&gt;: Instance and class method documentation with proper formatting. The tool skips double underscore (dunder: __xxx) methods.&lt;/p&gt; &lt;h2&gt;Examples&lt;/h2&gt; &lt;p&gt;Feel free to run them yourself using the files in [examples](examples)&lt;/p&gt; &lt;h3&gt;Before:&lt;/h3&gt; &lt;p&gt;&lt;code&gt;python def calculate_total(items, tax_rate=0.08, discount=None): subtotal = sum(item['price'] * item['quantity'] for item in items) if discount: subtotal *= (1 - discount) return subtotal * (1 + tax_rate) &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;After (Google style):&lt;/h3&gt; &lt;p&gt;```python def calculate_total(items, tax_rate=0.08, discount=None): &amp;quot;&amp;quot;&amp;quot; Calculate the total cost of items, applying a tax rate and optionally a discount.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Args: items: List of item objects with price and quantity tax_rate: Tax rate expressed as a decimal (default 0.08) discount: Discount rate expressed as a decimal; if provided, the subtotal is multiplied by (1 - discount) Returns: Total amount after applying the tax Example: &amp;gt;&amp;gt;&amp;gt; items = [{'price': 10, 'quantity': 2}, {'price': 5, 'quantity': 1}] &amp;gt;&amp;gt;&amp;gt; calculate_total(items, tax_rate=0.1, discount=0.05) 22.5 &amp;quot;&amp;quot;&amp;quot; subtotal = sum(item['price'] * item['quantity'] for item in items) if discount: subtotal *= (1 - discount) return subtotal * (1 + tax_rate) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;h2&gt;FAQ&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Q: Why don't we just use GPT-4/Claude API for this?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Because your proprietary code shouldn't leave your infrastructure. Cloud APIs create security risks, compliance issues, and ongoing costs. Our models run locally with comparable quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Can I document existing docstrings or update them?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Currently, the tool only adds missing docstrings. Updating existing documentation is planned for future releases. For now, you can manually remove docstrings you want regenerated.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Which docstring style can I use?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Google&lt;/strong&gt;: Most readable, great for general Python projects&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Q: The model does not work as expected&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A: The tool calling on our platform is in active development! &lt;a href="https://www.linkedin.com/company/distil-labs/"&gt;Follow us on LinkedIn&lt;/a&gt; for updates, or &lt;a href="https://join.slack.com/t/distil-labs-community/shared_invite/zt-36zqj87le-i3quWUn2bjErRq22xoE58g"&gt;join our community&lt;/a&gt;. You can also manually refine any generated docstrings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Can you train a model for my company's documentation standards?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A: Visit our &lt;a href="https://www.distillabs.ai"&gt;website&lt;/a&gt; and reach out to us, we offer custom solutions tailored to your coding standards and domain-specific requirements.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Q: Does this support type hints or other Python documentation tools?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A: Type hints are parsed and incorporated into docstrings. Integration with tools like pydoc, Sphinx, and MkDocs is on our roadmap.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/phzq8yxnj81g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owyk52/distillocaldocpy_slm_assistant_for_writing_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owyk52/distillocaldocpy_slm_assistant_for_writing_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T14:52:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1owszii</id>
    <title>Hits different now</title>
    <updated>2025-11-14T10:30:03+00:00</updated>
    <author>
      <name>/u/lfiction</name>
      <uri>https://old.reddit.com/user/lfiction</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owszii/hits_different_now/"&gt; &lt;img alt="Hits different now" src="https://preview.redd.it/t0yq5cko971g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7faccd0b39cc19dfb7a62aecffe57e0c87dc7d3" title="Hits different now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hadn‚Äôt seen this in ages.. I don‚Äôt have opinions on AGI either way at this point, but this scene sure hits a lot harder now than it did back then! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lfiction"&gt; /u/lfiction &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t0yq5cko971g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owszii/hits_different_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owszii/hits_different_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T10:30:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ows6z3</id>
    <title>Kimi k2 thinking vs Claude Sonnet</title>
    <updated>2025-11-14T09:41:28+00:00</updated>
    <author>
      <name>/u/sebastianmicu24</name>
      <uri>https://old.reddit.com/user/sebastianmicu24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I will add my personal experience with kimi k2 thinking for my usecase since I saw contrasting opinions. &lt;/p&gt; &lt;p&gt;I needed to cluster some cells from a csv file to see if it would be achievable with my data to do some unsupervised classification of tumor cell/healthy cell. &lt;/p&gt; &lt;p&gt;I tried with claude sonnet 4 and after 2$ in api calls and a bunch of prompts i got no result, it was clustering 99.9% of cells into one group and 0.1% into the other. It was also having difficulties into rendering the cells from the x y positions in the csv. &lt;/p&gt; &lt;p&gt;Kimi k2 thinking achieved a proper clustering in 2 prompts (one for preprocessing of csv data, and one for clustering, maybe it could have done the same in 1 prompt). Total cost 0.17$&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sebastianmicu24"&gt; /u/sebastianmicu24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ows6z3/kimi_k2_thinking_vs_claude_sonnet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ows6z3/kimi_k2_thinking_vs_claude_sonnet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ows6z3/kimi_k2_thinking_vs_claude_sonnet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T09:41:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox1x54</id>
    <title>Risk of LLM Judges in Paper Review: Scores Could Mask Poor Quality</title>
    <updated>2025-11-14T16:55:58+00:00</updated>
    <author>
      <name>/u/pier4r</name>
      <uri>https://old.reddit.com/user/pier4r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See this twitter thread: &lt;a href="https://nitter.net/micahgoldblum/status/1989088547777966512"&gt;https://nitter.net/micahgoldblum/status/1989088547777966512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A couple of quotes&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;An LLM-generated paper is in the top 17% of ICLR submissions in terms of average reviewer score, having received two 8's. The paper has tons of BS jargon and hallucinated references. Fortunately, one reviewer actually looked at the paper and gave it a zero. &lt;/p&gt; &lt;p&gt;Do you think the other 2 reviewers who gave it 8 just used LLMs to review as well?&lt;/p&gt; &lt;p&gt;Likely&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;There are other discussions that also mentions: peer reviews are free (one can submit a ton of those). What if people simply produce a ton of paperslop to review and humans peer reviewers get fatigued, use LLMs as judges and those don't know better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pier4r"&gt; /u/pier4r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox1x54/risk_of_llm_judges_in_paper_review_scores_could/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox1x54/risk_of_llm_judges_in_paper_review_scores_could/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ox1x54/risk_of_llm_judges_in_paper_review_scores_could/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T16:55:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1owmtkt</id>
    <title>I brought CUDA back to macOS. Not because it was useful ‚Äî because nobody else could.</title>
    <updated>2025-11-14T04:20:16+00:00</updated>
    <author>
      <name>/u/Adept_Tip8375</name>
      <uri>https://old.reddit.com/user/Adept_Tip8375</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owmtkt/i_brought_cuda_back_to_macos_not_because_it_was/"&gt; &lt;img alt="I brought CUDA back to macOS. Not because it was useful ‚Äî because nobody else could." src="https://external-preview.redd.it/YHp6xAwqBe8oZ_OrMdwTyJjRYCv9-wbk4V-lSqlUI3I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e28774fdaaba15ba19d667058a3967b4695ebc8" title="I brought CUDA back to macOS. Not because it was useful ‚Äî because nobody else could." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just resurrected CUDA on High Sierra in 2025&lt;br /&gt; Apple killed it 2018, NVIDIA killed drivers 2021&lt;br /&gt; now my 1080 Ti is doing 11 TFLOPs under PyTorch again&lt;br /&gt; ‚Äúimpossible‚Äù they said&lt;br /&gt; &lt;a href="https://github.com/careunix/PyTorch-HighSierra-CUDA-Revival"&gt;https://github.com/careunix/PyTorch-HighSierra-CUDA-Revival&lt;/a&gt;&lt;br /&gt; who still runs 10.13 in 2025 üòÇ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adept_Tip8375"&gt; /u/Adept_Tip8375 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owmtkt/i_brought_cuda_back_to_macos_not_because_it_was/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owmtkt/i_brought_cuda_back_to_macos_not_because_it_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owmtkt/i_brought_cuda_back_to_macos_not_because_it_was/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T04:20:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1owyp8q</id>
    <title>The Big LLM Architecture Comparison: From DeepSeek-V3 to Kimi K2 Thinking</title>
    <updated>2025-11-14T14:58:02+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owyp8q/the_big_llm_architecture_comparison_from/"&gt; &lt;img alt="The Big LLM Architecture Comparison: From DeepSeek-V3 to Kimi K2 Thinking" src="https://external-preview.redd.it/5N8z_mXiAneWfY6B3hrkRbiDD5IkgsvFJWMT1AAURS8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c88626d25ad555fca14567f2825f2de4449a35ff" title="The Big LLM Architecture Comparison: From DeepSeek-V3 to Kimi K2 Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owyp8q/the_big_llm_architecture_comparison_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owyp8q/the_big_llm_architecture_comparison_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T14:58:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1owx1nh</id>
    <title>The company gmktec made a comparison of the EVO-X2 that has a Ryzen AI Max+ 395 processor vs NVIDIA DGX SPARK</title>
    <updated>2025-11-14T13:52:15+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owx1nh/the_company_gmktec_made_a_comparison_of_the_evox2/"&gt; &lt;img alt="The company gmktec made a comparison of the EVO-X2 that has a Ryzen AI Max+ 395 processor vs NVIDIA DGX SPARK" src="https://preview.redd.it/pl1lqj8r981g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1855485e8cb6f5d5b69639209615733627982830" title="The company gmktec made a comparison of the EVO-X2 that has a Ryzen AI Max+ 395 processor vs NVIDIA DGX SPARK" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My point is that they should make comparisons with small models that have come out lately because they are enough for most people and because the inference is also faster&lt;/p&gt; &lt;p&gt;Info :&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.gmktec.com/blog/evo-x2-vs-nvidia-dgx-spark-redefining-local-ai-performance"&gt;https://www.gmktec.com/blog/evo-x2-vs-nvidia-dgx-spark-redefining-local-ai-performance&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pl1lqj8r981g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owx1nh/the_company_gmktec_made_a_comparison_of_the_evox2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owx1nh/the_company_gmktec_made_a_comparison_of_the_evox2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T13:52:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox3e1f</id>
    <title>Drummer's Precog 24B and 123B v1 - AI that writes a short draft before responding</title>
    <updated>2025-11-14T17:50:02+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox3e1f/drummers_precog_24b_and_123b_v1_ai_that_writes_a/"&gt; &lt;img alt="Drummer's Precog 24B and 123B v1 - AI that writes a short draft before responding" src="https://external-preview.redd.it/vl2ei1-FehJR-7jZHQXuFZ_Y0kemf2CP216W8qh6VxE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2798025eaf994dc8b7c090c13aa6bdefb4507a02" title="Drummer's Precog 24B and 123B v1 - AI that writes a short draft before responding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! &lt;/p&gt; &lt;p&gt;I wanted to explore a different way of thinking where the AI uses the &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; block to plan ahead and create a short draft so that its &lt;em&gt;actual&lt;/em&gt; response has &lt;strong&gt;basis&lt;/strong&gt;. It seems like a good way to have the AI pan out its start, middle, and end before writing the entire thing. Kind of like a synopsis or abstract. &lt;/p&gt; &lt;p&gt;I'm hoping it could strengthen consistency and flow since the AI doesn't have to &lt;em&gt;wing it&lt;/em&gt; and write a thousand tokens from the get-go. It's a cheaper, more effective alternative to reasoning, especially when it comes to story / RP. You can also make adjustments to the draft to steer it a certain way. Testers have been happy with it.&lt;/p&gt; &lt;p&gt;24B: &lt;a href="https://huggingface.co/TheDrummer/Precog-24B-v1"&gt;https://huggingface.co/TheDrummer/Precog-24B-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;123B: &lt;a href="https://huggingface.co/TheDrummer/Precog-123B-v1"&gt;https://huggingface.co/TheDrummer/Precog-123B-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Examples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1li2viecf91g1.png?width=2264&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af225606b23751beaf3076b1a58140b1c77b1a4f"&gt;https://preview.redd.it/1li2viecf91g1.png?width=2264&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af225606b23751beaf3076b1a58140b1c77b1a4f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7iu4m7zcf91g1.png?width=887&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4de7655654340ec91216d8a61c93c474571b1dc0"&gt;https://preview.redd.it/7iu4m7zcf91g1.png?width=887&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4de7655654340ec91216d8a61c93c474571b1dc0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3qo833ndf91g1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0cac98a5e93dd87baa885bda58574385b8e73c11"&gt;https://preview.redd.it/3qo833ndf91g1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0cac98a5e93dd87baa885bda58574385b8e73c11&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox3e1f/drummers_precog_24b_and_123b_v1_ai_that_writes_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox3e1f/drummers_precog_24b_and_123b_v1_ai_that_writes_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ox3e1f/drummers_precog_24b_and_123b_v1_ai_that_writes_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T17:50:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1owskm6</id>
    <title>Windows llama.cpp is 20% faster</title>
    <updated>2025-11-14T10:05:21+00:00</updated>
    <author>
      <name>/u/johannes_bertens</name>
      <uri>https://old.reddit.com/user/johannes_bertens</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owskm6/windows_llamacpp_is_20_faster/"&gt; &lt;img alt="Windows llama.cpp is 20% faster" src="https://preview.redd.it/tfdcbkf6571g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f97a0d3f3c6a2519462ab5e159f2045396e9409" title="Windows llama.cpp is 20% faster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;But why?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Windows: 1000+ PP&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;llama-bench -m C:\Users\johan\.lmstudio\models\unsloth\Qwen3-VL-30B-A3B-Instruct-GGUF\Qwen3-VL-30B-A3B-Instruct-UD-Q8_K_XL.gguf -p 512,1024,2048,4096 -n 0 -fa 0 --mmap 0&lt;br /&gt; load_backend: loaded RPC backend from C:\Users\johan\Downloads\llama-b7032-bin-win-vulkan-x64\ggml-rpc.dll&lt;br /&gt; ggml_vulkan: Found 1 Vulkan devices:&lt;br /&gt; ggml_vulkan: 0 = AMD Radeon(TM) 8060S Graphics (AMD proprietary driver) | uma: 1 | fp16: 1 | bf16: 1 | warp size: 64 | shared memory: 32768 | int dot: 1 | matrix cores: KHR_coopmat&lt;br /&gt; load_backend: loaded Vulkan backend from C:\Users\johan\Downloads\llama-b7032-bin-win-vulkan-x64\ggml-vulkan.dll&lt;br /&gt; load_backend: loaded CPU backend from C:\Users\johan\Downloads\llama-b7032-bin-win-vulkan-x64\ggml-cpu-icelake.dll &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model &lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend &lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;mmap&lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp512&lt;/td&gt; &lt;td align="right"&gt; 1079.12 ¬± 4.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp1024&lt;/td&gt; &lt;td align="right"&gt; 975.04 ¬± 4.46&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 892.94 ¬± 2.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp4096&lt;/td&gt; &lt;td align="right"&gt; 806.84 ¬± 2.89&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Linux: 880 PP&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; [johannes@toolbx ~]$ llama-bench -m models/Qwen3-VL-30B-A3B-Instruct-UD-Q8_K_XL.gguf -p 512,1024,2048,4096 -n 0 -fa 0 --mmap 0&lt;br /&gt; ggml_vulkan: Found 1 Vulkan devices:&lt;br /&gt; ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model &lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend &lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;mmap&lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp512&lt;/td&gt; &lt;td align="right"&gt; 876.79 ¬± 4.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp1024&lt;/td&gt; &lt;td align="right"&gt; 797.87 ¬± 1.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 757.55 ¬± 2.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;qwen3vlmoe 30B.A3B Q8_0 &lt;/td&gt; &lt;td align="right"&gt; 33.51 GiB&lt;/td&gt; &lt;td align="right"&gt; 30.53 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; 99&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp4096&lt;/td&gt; &lt;td align="right"&gt; 686.61 ¬± 0.89&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Obviously it's not 20% over the board, but still a very big difference. Is the &amp;quot;AMD proprietary driver&amp;quot; such a big deal?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johannes_bertens"&gt; /u/johannes_bertens &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tfdcbkf6571g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owskm6/windows_llamacpp_is_20_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owskm6/windows_llamacpp_is_20_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T10:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1owocd2</id>
    <title>Is it normal to hear weird noises when running an LLM on 4√ó Pro 6000 Max-Q cards?</title>
    <updated>2025-11-14T05:41:14+00:00</updated>
    <author>
      <name>/u/PlusProfession9245</name>
      <uri>https://old.reddit.com/user/PlusProfession9245</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owocd2/is_it_normal_to_hear_weird_noises_when_running_an/"&gt; &lt;img alt="Is it normal to hear weird noises when running an LLM on 4√ó Pro 6000 Max-Q cards?" src="https://external-preview.redd.it/MnFzdzJ0b3l0NTFnMbphl7ifhldDVQJssqSE3uLNJKqrQJ4o9dG0SGtQf767.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b2e8e94666721a90be11f2cea3b9f593dc28f21" title="Is it normal to hear weird noises when running an LLM on 4√ó Pro 6000 Max-Q cards?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It doesn‚Äôt sound like normal coil whine.&lt;br /&gt; In a Docker environment, when I run gpt-oss-120b across 4 GPUs, I hear a strange noise.&lt;br /&gt; The sound is also different depending on the model.&lt;br /&gt; Is this normal??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PlusProfession9245"&gt; /u/PlusProfession9245 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9eez1soyt51g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1owocd2/is_it_normal_to_hear_weird_noises_when_running_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1owocd2/is_it_normal_to_hear_weird_noises_when_running_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T05:41:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oth5pw</id>
    <title>AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model</title>
    <updated>2025-11-10T15:44:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt; &lt;img alt="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" src="https://b.thumbs.redditmedia.com/yz9_FpdLcHNiCkaH5fLEIoXS2f5u5twNBr7SQ9Go3AI.jpg" title="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Moonshot AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/ComfortableAsk4494"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/zxytim"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ppwwyyxx"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87"&gt;https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:44:10+00:00</published>
  </entry>
</feed>
