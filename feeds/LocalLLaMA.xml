<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-18T08:50:32+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nk1504</id>
    <title>Any resources on implementing “memory” like ChatGPT</title>
    <updated>2025-09-18T06:44:50+00:00</updated>
    <author>
      <name>/u/DataScientia</name>
      <uri>https://old.reddit.com/user/DataScientia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m trying to understand how systems like ChatGPT handle their “memory” feature. I don’t mean RAG , where documents are chunked and queried, but more of a lightweight, vague memory that stores facts and surfaces them only when relevant in later conversations.&lt;/p&gt; &lt;p&gt;Is there any blog, paper, or open-source implementation that explains how to design and implement something like this? &lt;/p&gt; &lt;p&gt;Basically: • How to decide what to store vs ignore • How to retrieve only when it’s contextually useful • How to keep it lightweight instead of doing full-blown vector DB lookups for everything&lt;/p&gt; &lt;p&gt;Would love to dive deeper if anyone has resources, papers, or even experimental repos!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataScientia"&gt; /u/DataScientia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk1504/any_resources_on_implementing_memory_like_chatgpt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk1504/any_resources_on_implementing_memory_like_chatgpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nk1504/any_resources_on_implementing_memory_like_chatgpt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T06:44:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1njptb5</id>
    <title>DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning</title>
    <updated>2025-09-17T21:27:08+00:00</updated>
    <author>
      <name>/u/Suitable-Economy-346</name>
      <uri>https://old.reddit.com/user/Suitable-Economy-346</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njptb5/deepseekr1_incentivizes_reasoning_in_llms_through/"&gt; &lt;img alt="DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning" src="https://external-preview.redd.it/15DG2R3Q_ewn709WrKa0dvZ06SwwVBGTDcg2W0ay3Is.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=842b4c95d2aa8f1de9ddc7ffa7efbbed35663107" title="DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suitable-Economy-346"&gt; /u/Suitable-Economy-346 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.nature.com/articles/s41586-025-09422-z"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njptb5/deepseekr1_incentivizes_reasoning_in_llms_through/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njptb5/deepseekr1_incentivizes_reasoning_in_llms_through/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T21:27:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1njlnad</id>
    <title>LACT "indirect undervolt &amp; OC" method beats `nvidia-smi -pl 400` on 3090TI FE.</title>
    <updated>2025-09-17T18:45:23+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njlnad/lact_indirect_undervolt_oc_method_beats_nvidiasmi/"&gt; &lt;img alt="LACT &amp;quot;indirect undervolt &amp;amp; OC&amp;quot; method beats `nvidia-smi -pl 400` on 3090TI FE." src="https://preview.redd.it/h4082k0frrpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b52b3a347341b760c6e962eb14813fd99117eb4" title="LACT &amp;quot;indirect undervolt &amp;amp; OC&amp;quot; method beats `nvidia-smi -pl 400` on 3090TI FE." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There have been some recent posts about using the new &amp;quot;indirect undervolt and overclock&amp;quot; method with LACT under Linux instead of simply naieve power capping your GPU(s) with &lt;code&gt;nvidia-smi -pl 300&lt;/code&gt; for example.&lt;/p&gt; &lt;p&gt;I wasn't sure if it was really any better or not, so vibe coded a small script to integrate 1Hz power measurements from my 3090TI FE 24GB GPU and run two benchmarks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Baseline &lt;code&gt;nvidia -pl 400&lt;/code&gt; naieve 400W power cap&lt;/li&gt; &lt;li&gt;LACT overclock profile with same 400W power cap&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I then ran the same ik_llama.cpp llama-sweep-bench test and sure enough the LACT overclock profile performs better/faster with less overall energy usage within the same power envelope.&lt;/p&gt; &lt;p&gt;LACT has worked on a variety of Intel/AMD/NVIDIA GPUs for a while now, but the &amp;quot;new&amp;quot; discovery to me was this &amp;quot;indirect undervolt and overclock&amp;quot; method specific to NVIDIA GPUs.&lt;/p&gt; &lt;p&gt;I have some anecdotal measurements with ComfyUI Wan2.2 i2v workflows suggesting it is faster for a given power cap as well. However, when I increased the overclocks too far it would output all dark/black videos or have occasional grey/dark square tile patches appear in the output video. I had to undo the aggressive overclock, reboot, and then it was all fine again. The values listed in the legend here seem to be working fine for now.&lt;/p&gt; &lt;p&gt;Curious what overclock profiles other folks are using for various GPU make/models. It does work headless as well and some have reported using it to reduce idle power psure. Also has anyone compared this against using nvidia-smi to set frequency cap instead of power cap or other strategies?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h4082k0frrpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njlnad/lact_indirect_undervolt_oc_method_beats_nvidiasmi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njlnad/lact_indirect_undervolt_oc_method_beats_nvidiasmi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T18:45:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nixynv</id>
    <title>The Qwen of Pain.</title>
    <updated>2025-09-16T23:58:16+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nixynv/the_qwen_of_pain/"&gt; &lt;img alt="The Qwen of Pain." src="https://preview.redd.it/0px1banw6mpf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8edc833e57220e0c00a8b11ba32c881974742ef1" title="The Qwen of Pain." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0px1banw6mpf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nixynv/the_qwen_of_pain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nixynv/the_qwen_of_pain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T23:58:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj7mbu</id>
    <title>Big AI pushes the "we need to beat China" narrative cuz they want fat government contracts and zero democratic oversight. It's an old trick. Fear sells.</title>
    <updated>2025-09-17T08:36:32+00:00</updated>
    <author>
      <name>/u/katxwoods</name>
      <uri>https://old.reddit.com/user/katxwoods</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Throughout the Cold War, the military-industrial complex spent a fortune pushing the false narrative that the Soviet military was far more advanced than they actually were.&lt;/p&gt; &lt;p&gt;Why? To ensure the money from Congress kept flowing.&lt;/p&gt; &lt;p&gt;They lied… and lied… and lied again to get bigger and bigger defense contracts.&lt;/p&gt; &lt;p&gt;Now, obviously, there is &lt;em&gt;some&lt;/em&gt; amount of competition between the US and China, but &lt;strong&gt;Big Tech is stoking the flames beyond what is reasonable to terrify Congress into giving them whatever they want.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;What they want is fat government contracts and zero democratic oversight. Day after day we hear about another big AI company announcing a giant contract with the Department of Defense.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/katxwoods"&gt; /u/katxwoods &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7mbu/big_ai_pushes_the_we_need_to_beat_china_narrative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7mbu/big_ai_pushes_the_we_need_to_beat_china_narrative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7mbu/big_ai_pushes_the_we_need_to_beat_china_narrative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T08:36:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1njgb5x</id>
    <title>Qwen3 Coder Plus</title>
    <updated>2025-09-17T15:30:00+00:00</updated>
    <author>
      <name>/u/Dependent_Factor_204</name>
      <uri>https://old.reddit.com/user/Dependent_Factor_204</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just noticed &lt;a href="https://openrouter.ai/qwen/qwen3-coder-plus"&gt;https://openrouter.ai/qwen/qwen3-coder-plus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Not open though!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dependent_Factor_204"&gt; /u/Dependent_Factor_204 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgb5x/qwen3_coder_plus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgb5x/qwen3_coder_plus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njgb5x/qwen3_coder_plus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T15:30:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1njws3n</id>
    <title>What’s the training cost for models like Qwen3 coder 30b and is the code for training it is open source or close source?</title>
    <updated>2025-09-18T02:40:19+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it also possible to grab qwen3 coder 4b and train it again on more and new data? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njws3n/whats_the_training_cost_for_models_like_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njws3n/whats_the_training_cost_for_models_like_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njws3n/whats_the_training_cost_for_models_like_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T02:40:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1njgj9s</id>
    <title>Drummer's Cydonia ReduX 22B and Behemoth ReduX 123B - Throwback tunes of the good old days, now with updated tuning! Happy birthday, Cydonia v1!</title>
    <updated>2025-09-17T15:38:21+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgj9s/drummers_cydonia_redux_22b_and_behemoth_redux/"&gt; &lt;img alt="Drummer's Cydonia ReduX 22B and Behemoth ReduX 123B - Throwback tunes of the good old days, now with updated tuning! Happy birthday, Cydonia v1!" src="https://external-preview.redd.it/ZtYL1Go2LfQzSyi5MifYgd-epIPOSy-LGxJA6DFzU3k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=18777ba97955acd7508bb2f53af88f4c18be1858" title="Drummer's Cydonia ReduX 22B and Behemoth ReduX 123B - Throwback tunes of the good old days, now with updated tuning! Happy birthday, Cydonia v1!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Behemoth ReduX 123B: &lt;a href="https://huggingface.co/TheDrummer/Behemoth-ReduX-123B-v1"&gt;https://huggingface.co/TheDrummer/Behemoth-ReduX-123B-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They're updated finetunes of the old Mistral 22B and Mistral 123B 2407. &lt;/p&gt; &lt;p&gt;Both bases were arguably peak Mistral (aside from Nemo and &lt;span class="md-spoiler-text"&gt;Miqu&lt;/span&gt;). I decided to finetune them since the writing/creativity is just... different from what we've got today. They hold up stronger than ever, but they're still old bases so intelligence and context length isn't up there with the newer base models. Still, they both prove that these smarter, stronger models are missing out on &lt;em&gt;something&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;I figured I'd release it on Cydonia v1's one year anniversary. Can't believe it's been a year and a half since I started this journey with you all. Hope you enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-ReduX-22B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgj9s/drummers_cydonia_redux_22b_and_behemoth_redux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njgj9s/drummers_cydonia_redux_22b_and_behemoth_redux/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T15:38:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1njkgkf</id>
    <title>SvelteKit-based WebUI by allozaur · Pull Request #14839 · ggml-org/llama.cpp</title>
    <updated>2025-09-17T18:01:20+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njkgkf/sveltekitbased_webui_by_allozaur_pull_request/"&gt; &lt;img alt="SvelteKit-based WebUI by allozaur · Pull Request #14839 · ggml-org/llama.cpp" src="https://external-preview.redd.it/lfHyUwYZ8aPaE0KlMcSCbv60pDFTFEz7jL_zRZTwdcw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94339bd1349da02ea7d59583c915f4eb21952ba8" title="SvelteKit-based WebUI by allozaur · Pull Request #14839 · ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;This PR introduces a complete rewrite of the llama.cpp web interface, migrating from a React-based implementation to a modern SvelteKit architecture. The new implementation provides significant improvements in user experience, developer tooling, and feature capabilities while maintaining full compatibility with the llama.cpp server API.&amp;quot;&lt;/p&gt; &lt;p&gt;✨ Feature Enhancements&lt;/p&gt; &lt;h1&gt;File Handling&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Dropdown Upload Menu&lt;/strong&gt;: Type-specific file selection (Images/Text/PDFs)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Universal Preview System&lt;/strong&gt;: Full-featured preview dialogs for all supported file types&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PDF Dual View&lt;/strong&gt;: Text extraction + page-by-page image rendering&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced Support&lt;/strong&gt;: SVG/WEBP→PNG conversion, binary detection, syntax highlighting&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vision Model Awareness&lt;/strong&gt;: Smart UI adaptation based on model capabilities&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Graceful Failure&lt;/strong&gt;: Proper error handling and user feedback for unsupported file types&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Advanced Chat Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Reasoning Content&lt;/strong&gt;: Dedicated thinking blocks with streaming support&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Conversation Branching&lt;/strong&gt;: Full tree structure with parent-child relationships&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Message Actions&lt;/strong&gt;: Edit, regenerate, delete with intelligent branch management&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Keyboard Shortcuts&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;code&gt;Ctrl+Shift+N&lt;/code&gt;: Start new conversation&lt;/li&gt; &lt;li&gt;&lt;code&gt;Ctrl+Shift+D&lt;/code&gt;: Delete current conversation&lt;/li&gt; &lt;li&gt;&lt;code&gt;Ctrl+K&lt;/code&gt;: Focus search conversations&lt;/li&gt; &lt;li&gt;&lt;code&gt;Ctrl+V&lt;/code&gt;: Paste files and content to conversation&lt;/li&gt; &lt;li&gt;&lt;code&gt;Ctrl+B&lt;/code&gt;: Toggle sidebar&lt;/li&gt; &lt;li&gt;&lt;code&gt;Enter&lt;/code&gt;: Send message&lt;/li&gt; &lt;li&gt;&lt;code&gt;Shift+Enter&lt;/code&gt;: New line in message&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Paste&lt;/strong&gt;: Auto-conversion of long text to files with customizable threshold (default 2000 characters)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Server Integration&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Slots Monitoring&lt;/strong&gt;: Real-time server resource tracking during generation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Management&lt;/strong&gt;: Advanced context error handling and recovery&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Server Status&lt;/strong&gt;: Comprehensive server state monitoring&lt;/li&gt; &lt;li&gt;&lt;strong&gt;API Integration&lt;/strong&gt;: Full &lt;code&gt;reasoning_content&lt;/code&gt; and slots endpoint support&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🎨 User Experience Improvements&lt;/h1&gt; &lt;h1&gt;Interface Design&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Modern UI Components&lt;/strong&gt;: Consistent design system with ShadCN components&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Responsive Layout&lt;/strong&gt;: Adaptive sidebar and mobile-friendly design&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Theme System&lt;/strong&gt;: Seamless auto/light/dark mode switching&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual Hierarchy&lt;/strong&gt;: Clear information architecture and content organization&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Interaction Patterns&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Keyboard Navigation&lt;/strong&gt;: Complete keyboard accessibility with shortcuts&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Drag &amp;amp; Drop&lt;/strong&gt;: Intuitive file upload with visual feedback&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Defaults&lt;/strong&gt;: Context-aware UI behavior and intelligent defaults (sidebar auto-management, conversation naming)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Progressive Disclosure&lt;/strong&gt;: Advanced features available without cluttering basic interface&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Feedback &amp;amp; Communication&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Loading States&lt;/strong&gt;: Clear progress indicators during operations&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Error Handling&lt;/strong&gt;: User-friendly error messages with recovery suggestions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Status Indicators&lt;/strong&gt;: Real-time server status and resource monitoring&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Confirmation Dialogs&lt;/strong&gt;: Prevent accidental data loss with confirmation prompts&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14839"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njkgkf/sveltekitbased_webui_by_allozaur_pull_request/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njkgkf/sveltekitbased_webui_by_allozaur_pull_request/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T18:01:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk2an9</id>
    <title>SIngle VS double GPU: Why was it worst ?</title>
    <updated>2025-09-18T07:58:58+00:00</updated>
    <author>
      <name>/u/Manoelnb</name>
      <uri>https://old.reddit.com/user/Manoelnb</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk2an9/single_vs_double_gpu_why_was_it_worst/"&gt; &lt;img alt="SIngle VS double GPU: Why was it worst ?" src="https://b.thumbs.redditmedia.com/QvfUGQBxSinnkyp0UEaPjXVgf2I_s3s2xPLuXlIwwLY.jpg" title="SIngle VS double GPU: Why was it worst ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey! I was playing around with AI in LM Studio. My wife has the same GPU as me, so I tried adding both to my PC. Here’s how it went in LM Studio (Hope posting this here is fine).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l8mtn1nipvpf1.png?width=1082&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a079bc9c7afb66bb67f8f855e05f649125e6eca9"&gt;https://preview.redd.it/l8mtn1nipvpf1.png?width=1082&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a079bc9c7afb66bb67f8f855e05f649125e6eca9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And I tried the ‘new’ GPT-OSS 20B model with the default settings.&lt;/p&gt; &lt;p&gt;On double GPU enabled: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zkvlql1opvpf1.png?width=423&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d1fb2638cdcc4c38ec5e70dea9bbe821a63735fc"&gt;https://preview.redd.it/zkvlql1opvpf1.png?width=423&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d1fb2638cdcc4c38ec5e70dea9bbe821a63735fc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On single GPU:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/em8ldmnppvpf1.png?width=450&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4e996c69e6be9e906ee4ae0429f424601ad4da7"&gt;https://preview.redd.it/em8ldmnppvpf1.png?width=450&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4e996c69e6be9e906ee4ae0429f424601ad4da7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;for the same prompt. &lt;/p&gt; &lt;p&gt;I think it’s normal not to get the same results with the same prompt. But +1.5s for the first token and +15 tok/sec seems like a lot to me. (I did a bit more testing, but got the same results.) This still feels a bit off. &lt;/p&gt; &lt;p&gt;Any ideas to help explain or understand why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Manoelnb"&gt; /u/Manoelnb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk2an9/single_vs_double_gpu_why_was_it_worst/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk2an9/single_vs_double_gpu_why_was_it_worst/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nk2an9/single_vs_double_gpu_why_was_it_worst/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T07:58:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1njm4w0</id>
    <title>How to make a small LLM from scratch?</title>
    <updated>2025-09-17T19:03:25+00:00</updated>
    <author>
      <name>/u/Charming_Barber_3317</name>
      <uri>https://old.reddit.com/user/Charming_Barber_3317</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to build an llm 0.1B to 0.6B params on a less popular language. How much data will i require of that particular language? and what are the exact steps i should follow? is this a good project for my final year? I have access to rtx3090 on which i can run 20B to 40B models easily at q4_k_m.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charming_Barber_3317"&gt; /u/Charming_Barber_3317 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njm4w0/how_to_make_a_small_llm_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njm4w0/how_to_make_a_small_llm_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njm4w0/how_to_make_a_small_llm_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T19:03:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj9601</id>
    <title>Ling Flash 2.0 released</title>
    <updated>2025-09-17T10:14:07+00:00</updated>
    <author>
      <name>/u/abskvrm</name>
      <uri>https://old.reddit.com/user/abskvrm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj9601/ling_flash_20_released/"&gt; &lt;img alt="Ling Flash 2.0 released" src="https://b.thumbs.redditmedia.com/Mje87GDqOP-_eCjHaD9MMo5kTGeRXnZhVYDnfTltcjY.jpg" title="Ling Flash 2.0 released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ling Flash-2.0, from InclusionAI, a language model with 100B total parameters and 6.1B activated parameters (4.8B non-embedding).&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-flash-2.0"&gt;https://huggingface.co/inclusionAI/Ling-flash-2.0&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abskvrm"&gt; /u/abskvrm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nj9601"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj9601/ling_flash_20_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj9601/ling_flash_20_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T10:14:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1njkqdm</id>
    <title>Arcee going Apache 2.0!!!</title>
    <updated>2025-09-17T18:11:16+00:00</updated>
    <author>
      <name>/u/Lowgooo</name>
      <uri>https://old.reddit.com/user/Lowgooo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;CTO of Arcee just announced that their AFM-4.5B model - &lt;a href="https://huggingface.co/arcee-ai/AFM-4.5B"&gt;https://huggingface.co/arcee-ai/AFM-4.5B&lt;/a&gt;&lt;br /&gt; as well as upcoming models will all be fully open source!&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/LucasAtkins7/status/1968371293184741876"&gt;https://x.com/LucasAtkins7/status/1968371293184741876&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lowgooo"&gt; /u/Lowgooo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njkqdm/arcee_going_apache_20/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njkqdm/arcee_going_apache_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njkqdm/arcee_going_apache_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T18:11:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1njwjmd</id>
    <title>Every SOTA on its own data</title>
    <updated>2025-09-18T02:28:54+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Feels like every new RAG paper shows &lt;em&gt;huge gains&lt;/em&gt;… but always on their own curated dataset.&lt;br /&gt; Once you swap in messy PDFs, private notes, or latency-sensitive use cases, the story changes fast.&lt;/p&gt; &lt;p&gt;Anyone here actually compared different RAG flavors side by side? (multi-hop vs. rerankers, retrieval-aug agents vs. lightweight hybrids, etc.)&lt;br /&gt; What did you find in practice — stability, speed, or truthfulness?&lt;/p&gt; &lt;p&gt;Would love to hear war stories from real deployments, not just benchmark tables.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njwjmd/every_sota_on_its_own_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njwjmd/every_sota_on_its_own_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njwjmd/every_sota_on_its_own_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T02:28:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1njet2z</id>
    <title>IBM just released Granite Docling</title>
    <updated>2025-09-17T14:33:34+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njet2z/ibm_just_released_granite_docling/"&gt; &lt;img alt="IBM just released Granite Docling" src="https://external-preview.redd.it/9VrSOe38oy5d5NsTP4RWGmhv_WIFkf4SUZ5rRkZXUAc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c208e780cd41278802ac7163a907c2de39c8d987" title="IBM just released Granite Docling" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;granite-docling-258M with Apache 2.0 license for document analysis &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-docling-682b8c766a565487bcb3ca00"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njet2z/ibm_just_released_granite_docling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njet2z/ibm_just_released_granite_docling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T14:33:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk1tz2</id>
    <title>A first stab at packaging llama.cpp in a performance-optimized manner</title>
    <updated>2025-09-18T07:28:24+00:00</updated>
    <author>
      <name>/u/jikkii</name>
      <uri>https://old.reddit.com/user/jikkii</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk1tz2/a_first_stab_at_packaging_llamacpp_in_a/"&gt; &lt;img alt="A first stab at packaging llama.cpp in a performance-optimized manner" src="https://preview.redd.it/xxno8cwokvpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c56cc3e0047565fcd6f49be7615fe04e6c509cb0" title="A first stab at packaging llama.cpp in a performance-optimized manner" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama.cpp has been a real enabler to get access to LLMs locally. However, one feedback that has come up regularly is that the package isn't easy to install, and, especially so if trying to do so in a performance-optimized manner taking advantage of one's hardware.&lt;/p&gt; &lt;p&gt;There's a very active discussion on the topic over on llama.cpp's GitHub (&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/15313"&gt;#15313&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;We've taken a first stab at implementing a performance-optimized packaging solution, so that it's easily installable and takes advantage of the feature flags your hardware provides (see attached pic).&lt;/p&gt; &lt;p&gt;While still a WIP, it's working on Linux (cpu/cuda) now, we'll follow-up with Metal, and finally Windows. The idea is to build the basis of a system that is easy to be iterated upon by the community. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jikkii"&gt; /u/jikkii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xxno8cwokvpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk1tz2/a_first_stab_at_packaging_llamacpp_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nk1tz2/a_first_stab_at_packaging_llamacpp_in_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T07:28:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1njjn2a</id>
    <title>Kimi-K2 0905, DeepSeek V3.1, Qwen3-Next-80B-A3B, Grok 4, and others on fresh SWE-bench–style tasks collected in August 2025</title>
    <updated>2025-09-17T17:32:11+00:00</updated>
    <author>
      <name>/u/CuriousPlatypus1881</name>
      <uri>https://old.reddit.com/user/CuriousPlatypus1881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjn2a/kimik2_0905_deepseek_v31_qwen3next80ba3b_grok_4/"&gt; &lt;img alt="Kimi-K2 0905, DeepSeek V3.1, Qwen3-Next-80B-A3B, Grok 4, and others on fresh SWE-bench–style tasks collected in August 2025" src="https://b.thumbs.redditmedia.com/ZgnOCtlnsYSchww7mxS8EvEaABQb-to_49dQyMZXKqg.jpg" title="Kimi-K2 0905, DeepSeek V3.1, Qwen3-Next-80B-A3B, Grok 4, and others on fresh SWE-bench–style tasks collected in August 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/54d12kuq8rpf1.png?width=5684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6967f375cf67e45ff0fde346d6d6bb73abc997e"&gt;https://preview.redd.it/54d12kuq8rpf1.png?width=5684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6967f375cf67e45ff0fde346d6d6bb73abc997e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi all, I'm Anton from Nebius.&lt;/p&gt; &lt;p&gt;We’ve updated the &lt;a href="https://swe-rebench.com"&gt;SWE-rebench leaderboard&lt;/a&gt; with model evaluations of Grok 4, Kimi K2 Instruct 0905, DeepSeek-V3.1, and Qwen3-Next-80B-A3B-Instruct on 52 fresh tasks.&lt;/p&gt; &lt;p&gt;Key takeaways from this update:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Kimi-K2 0915&lt;/strong&gt; has grown significantly (34.6% -&amp;gt; 42.3% increase in resolved rate) and is now in the top 3 open-source models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1&lt;/strong&gt; also improved, though less dramatically. What’s interesting is how many more tokens it now produces.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Next-80B-A3B-Instruct&lt;/strong&gt;, despite not being trained directly for coding, performs on par with the 30B-Coder. To reflect models speed, we’re also thinking about how best to report efficiency metrics such as tokens/sec on the leaderboard.&lt;/li&gt; &lt;li&gt;Finally, &lt;strong&gt;Grok 4&lt;/strong&gt;: the frontier model from xAI has now entered the leaderboard and is among the top performers. It’ll be fascinating to watch how it develops.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All &lt;strong&gt;52 new tasks collected in August&lt;/strong&gt; are available on the site — you can explore every problem in detail.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CuriousPlatypus1881"&gt; /u/CuriousPlatypus1881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjn2a/kimik2_0905_deepseek_v31_qwen3next80ba3b_grok_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjn2a/kimik2_0905_deepseek_v31_qwen3next80ba3b_grok_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njjn2a/kimik2_0905_deepseek_v31_qwen3next80ba3b_grok_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T17:32:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk1jbc</id>
    <title>I just made VRAM approximation tool for LLM</title>
    <updated>2025-09-18T07:09:12+00:00</updated>
    <author>
      <name>/u/SmilingGen</name>
      <uri>https://old.reddit.com/user/SmilingGen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a simple tool to estimate how much memory is needed to run GGUF models locally, based on your desired maximum context size.&lt;/p&gt; &lt;p&gt;You just paste the direct download URL of a GGUF model (for example, from Hugging Face), enter the context length you plan to use, and it will give you an approximate memory requirement.&lt;/p&gt; &lt;p&gt;It’s especially useful if you're trying to figure out whether a model will fit in your available VRAM or RAM, or when comparing different quantization levels like Q4_K_M vs Q8_0.&lt;/p&gt; &lt;p&gt;The tool is completely free and open-source. You can try it here: &lt;a href="https://www.kolosal.ai/memory-calculator"&gt;https://www.kolosal.ai/memory-calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And check out the code on GitHub: &lt;a href="https://github.com/KolosalAI/model-memory-calculator"&gt;https://github.com/KolosalAI/model-memory-calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd really appreciate any feedback, suggestions, or bug reports if you decide to give it a try.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SmilingGen"&gt; /u/SmilingGen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk1jbc/i_just_made_vram_approximation_tool_for_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk1jbc/i_just_made_vram_approximation_tool_for_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nk1jbc/i_just_made_vram_approximation_tool_for_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T07:09:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1njzxmx</id>
    <title>VoxCPM 0.5B : Tokenizer-Free TTS and Voice Cloning</title>
    <updated>2025-09-18T05:31:23+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It runs on MiniCPM-4 (0.5B params) and actually sounds expressive: prosody flows naturally, and it can clone a voice from just a short sample. It’s also practical: real-time streaming with RTF ~0.17 on a consumer GPU (RTX 4090). Trained on 1.8M hours of English + Chinese data, and the best part: fully open-sourced under Apache-2.0.&lt;/p&gt; &lt;p&gt;HuggingFace : &lt;a href="https://huggingface.co/openbmb/VoxCPM-0.5B"&gt;https://huggingface.co/openbmb/VoxCPM-0.5B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video : &lt;a href="https://youtu.be/HO3tuuEuhTw?si=2iFA5ApaCPD6yUWj"&gt;https://youtu.be/HO3tuuEuhTw?si=2iFA5ApaCPD6yUWj&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njzxmx/voxcpm_05b_tokenizerfree_tts_and_voice_cloning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njzxmx/voxcpm_05b_tokenizerfree_tts_and_voice_cloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njzxmx/voxcpm_05b_tokenizerfree_tts_and_voice_cloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T05:31:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1njwmtl</id>
    <title>Google's paper, SLED, seems to improve factuality with (all? Most?) LLMs at only a 4% speed penalty</title>
    <updated>2025-09-18T02:33:14+00:00</updated>
    <author>
      <name>/u/laser_man6</name>
      <uri>https://old.reddit.com/user/laser_man6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://research.google/blog/making-llms-more-accurate-by-using-all-of-their-layers/"&gt;https://research.google/blog/making-llms-more-accurate-by-using-all-of-their-layers/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This paper put out a year or so ago, and referenced by today's blog post, shows a method for decoding using the weighted average of every layer's logits. It improves factuality over DoLa (which itself improves over just standard sampling?) by anywhere from 2-16%with only a 4% hit to speed! I'm surprised I haven't seen this here since it seems like it shouldn't be too bad to implement into something like VLLM or llama.cpp, and it seems to work for many different models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/laser_man6"&gt; /u/laser_man6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njwmtl/googles_paper_sled_seems_to_improve_factuality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njwmtl/googles_paper_sled_seems_to_improve_factuality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njwmtl/googles_paper_sled_seems_to_improve_factuality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T02:33:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1njvw9b</id>
    <title>When AI admits it doesn't know: the surprising behavior of Magistral-small-2509 in a soccer quiz contest</title>
    <updated>2025-09-18T01:57:33+00:00</updated>
    <author>
      <name>/u/Different_File6723</name>
      <uri>https://old.reddit.com/user/Different_File6723</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was surprised by something about magistral-small-2509. Let's say I have a little personal test that consists of a kind of alphabetical wheel with soccer questions. They tend to be historical questions and a bit obscure. Normally, I don't get 100% correct on many LLMs, although on some I come close. The thing is, magistral came very close to 100%, but what really surprised me is that in the chain of reasoning, it couldn't find the right answer and decided to go for “pass,” which is the wild card you can use if you don't know the answer. In other tests with other LLMs, I don't usually see this; they are very reluctant to admit that they don't know the answer. However, I know I need to do more testing, but I thought it was interesting to share. Here is the piece of the thought chain&lt;/p&gt; &lt;p&gt;(Note: Originally, the post was in Spanish, as it is my native language. I was going to upload it in English, but I made a mistake and posted it in Spanish. However, I have already edited it.).&lt;/p&gt; &lt;p&gt;&lt;code&gt;K: Former German soccer player with dual Panamanian and Brazilian nationality who played as a forward for clubs such as Stuttgart and Schalke 04.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;I think of German players with dual nationality. Klodins? No, I don't think so. Kalle? No. Klaassen? He's not German. Klose? He doesn't have dual Panamanian and Brazilian nationality. Kroos? No. Kahn? No. Khedira? No. Kross? I don't think so.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Maybe it's Klose, but he doesn't have dual nationality. Kluivert? He's not German. Kun? Kun Agüero is Argentine, not German. Kolasinac? He's not German. Kimmich? He doesn't have dual nationality.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Could it be Kaka? But Kaka is Brazilian, not German. Klarke? I don't recognize that name. Keller? I don't think he has dual Panamanian and Brazilian nationality.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Maybe Kevin Kurányi. He's German, but I'm not sure about dual nationality. Kevin Prince Boateng? He's German with Ghanaian nationality, not Panamanian and Brazilian. Klose? Doesn't fit.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Could it be Klaus? I don't recognize the name. Kopila? I don't recognize the name. Kramaric? Croatian, not German.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;If I can't think of it, I can say “Pass.”&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;But before I give up, I remember there's a player named Klose, but he doesn't fit. Another name: Kluivert is Dutch. Kroos? No.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Wait, could it be Kevin Dyer? I don't recognize that name. Kevin Kampl? He's Slovenian.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;I don't think I know. Pass for K.&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_File6723"&gt; /u/Different_File6723 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njvw9b/when_ai_admits_it_doesnt_know_the_surprising/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njvw9b/when_ai_admits_it_doesnt_know_the_surprising/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njvw9b/when_ai_admits_it_doesnt_know_the_surprising/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T01:57:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1njt6ut</id>
    <title>DeepSeek-R1 on Nature: How Pure Reinforcement Learning Unlocks LLM Reasoning</title>
    <updated>2025-09-17T23:50:20+00:00</updated>
    <author>
      <name>/u/First_Ground_9849</name>
      <uri>https://old.reddit.com/user/First_Ground_9849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, Big news in the AI world today—&lt;strong&gt;DeepSeek-R1&lt;/strong&gt; is featured on the cover of &lt;em&gt;Nature&lt;/em&gt;! This is a significant milestone for reinforcement learning and reasoning in large language models. Here’s what makes this groundbreaking:&lt;/p&gt; &lt;h3&gt;🧠 Pure Reinforcement Learning Breakthrough&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;DeepSeek-R1 is the &lt;strong&gt;first model&lt;/strong&gt; to achieve state-of-the-art reasoning &lt;strong&gt;without any supervised fine-tuning (SFT)&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;It uses &lt;strong&gt;Group Relative Policy Optimization (GRPO)&lt;/strong&gt;, a novel RL method that reduces computational cost while maintaining high performance.&lt;/li&gt; &lt;li&gt;The model &lt;strong&gt;autonomously developed&lt;/strong&gt; advanced reasoning strategies like self-reflection, verification, and dynamic adaptation—all through RL, &lt;strong&gt;without human demonstrations&lt;/strong&gt;. ### 🏆 Top-Tier Performance&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AIME 2024&lt;/strong&gt;:&lt;/li&gt; &lt;li&gt;&lt;code&gt;pass@1&lt;/code&gt;: &lt;strong&gt;77.9%&lt;/strong&gt; → with self-consistency: &lt;strong&gt;86.7%&lt;/strong&gt; (surpassing human average)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MATH-500&lt;/strong&gt;: &lt;strong&gt;97.3%&lt;/strong&gt; (pass@1)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Codeforces Rating&lt;/strong&gt;: &lt;strong&gt;2029&lt;/strong&gt; (Top 5% globally)&lt;/li&gt; &lt;li&gt;Also excels in biology, physics, chemistry, and broader benchmarks like MMLU-Pro (&lt;strong&gt;84.0%&lt;/strong&gt;), AlpacaEval 2.0 (&lt;strong&gt;87.6%&lt;/strong&gt;), and Arena-Hard (&lt;strong&gt;92.3%&lt;/strong&gt;) ### 🔍 Emergent Reasoning Behaviors During training, the model showed:&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Self-correction&lt;/strong&gt;: “Aha moments” where it reevaluated its reasoning (e.g., sudden increase in the word “wait”)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-chain reasoning&lt;/strong&gt;: Generating hundreds to thousands of tokens to solve complex problems&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Adaptive token usage&lt;/strong&gt;: Using more tokens for hard problems, fewer for easy ones ### 🌍 Open Research &amp;amp; Model Release DeepSeek has released:&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek-R1-Zero&lt;/strong&gt; (pure RL version)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek-R1&lt;/strong&gt; (multistage RL + SFT for alignment)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Distilled smaller models&lt;/strong&gt; for broader accessibility&lt;/li&gt; &lt;li&gt;All &lt;strong&gt;code, weights, and data&lt;/strong&gt; under MIT license ### 📌 Limitations &amp;amp; Future Work The model still has room for improvement in:&lt;/li&gt; &lt;li&gt;Tool use (e.g., calculators, search)&lt;/li&gt; &lt;li&gt;Token efficiency (sometimes overthinks)&lt;/li&gt; &lt;li&gt;Language mixing (optimized for EN/ZH only)&lt;/li&gt; &lt;li&gt;Prompt sensitivity (works best zero-shot) But the work proves that &lt;strong&gt;pure RL can unlock reasoning&lt;/strong&gt; without human data—paving the way for more autonomous, self-improving AI. &lt;strong&gt;Paper &amp;amp; Resources:&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.nature.com/articles/s41586-025-09422-z"&gt;Nature Article&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-R1"&gt;GitHub Repo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/DeepSeek-ai"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What do you think? Is pure RL the future of LLM training?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/First_Ground_9849"&gt; /u/First_Ground_9849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njt6ut/deepseekr1_on_nature_how_pure_reinforcement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njt6ut/deepseekr1_on_nature_how_pure_reinforcement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njt6ut/deepseekr1_on_nature_how_pure_reinforcement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T23:50:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1njgovj</id>
    <title>Magistral Small 2509 has been released</title>
    <updated>2025-09-17T15:44:12+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgovj/magistral_small_2509_has_been_released/"&gt; &lt;img alt="Magistral Small 2509 has been released" src="https://external-preview.redd.it/lya4BYVSdGKwEDIK4epA43BL60WtN4IIIDfpTgnEljc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=14e03904bf3f936ad1691d3e1bcf8b07536b90d5" title="Magistral Small 2509 has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Magistral-Small-2509-GGUF"&gt;https://huggingface.co/mistralai/Magistral-Small-2509-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Magistral-Small-2509"&gt;https://huggingface.co/mistralai/Magistral-Small-2509&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Magistral Small 1.2&lt;/h1&gt; &lt;p&gt;Building upon &lt;a href="https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506"&gt;Mistral Small 3.2 (2506)&lt;/a&gt;, &lt;strong&gt;with added reasoning capabilities&lt;/strong&gt;, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters.&lt;/p&gt; &lt;p&gt;Magistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized.&lt;/p&gt; &lt;p&gt;Learn more about Magistral in our &lt;a href="https://mistral.ai/news/magistral/"&gt;blog post&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The model was presented in the paper &lt;a href="https://huggingface.co/papers/2506.10910"&gt;Magistral&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Updates compared with &lt;a href="https://huggingface.co/mistralai/Magistral-Small-2507"&gt;Magistral Small 1.1&lt;/a&gt;&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multimodality&lt;/strong&gt;: The model now has a vision encoder and can take multimodal inputs, extending its reasoning capabilities to vision.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance upgrade&lt;/strong&gt;: Magistral Small 1.2 should give you significatively better performance than Magistral Small 1.1 as seen in the &lt;a href="https://huggingface.co/mistralai/Magistral-Small-2509#benchmark-results"&gt;benchmark results&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Better tone and persona&lt;/strong&gt;: You should experiment better LaTeX and Markdown formatting, and shorter answers on easy general prompts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Finite generation&lt;/strong&gt;: The model is less likely to enter infinite generation loops.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Special think tokens&lt;/strong&gt;: [THINK] and [/THINK] special tokens encapsulate the reasoning content in a thinking chunk. This makes it easier to parse the reasoning trace and prevents confusion when the '[THINK]' token is given as a string in the prompt.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning prompt&lt;/strong&gt;: The reasoning prompt is given in the system prompt.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Reasoning:&lt;/strong&gt; Capable of long chains of reasoning traces before providing an answer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vision&lt;/strong&gt;: Vision capabilities enable the model to analyze images and reason based on visual content in addition to text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apache 2.0 License:&lt;/strong&gt; Open license allowing usage and modification for both commercial and non-commercial purposes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Window:&lt;/strong&gt; A 128k context window. Performance &lt;em&gt;might&lt;/em&gt; degrade past &lt;strong&gt;40k&lt;/strong&gt; but Magistral should still give good results. Hence we recommend to leave the maximum model length to 128k and only lower if you encounter low performance.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d0vo5ev3xqpf1.png?width=1342&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f81d6fa64a262e991112d1c8011e18d1d75b2774"&gt;https://preview.redd.it/d0vo5ev3xqpf1.png?width=1342&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f81d6fa64a262e991112d1c8011e18d1d75b2774&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgovj/magistral_small_2509_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgovj/magistral_small_2509_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njgovj/magistral_small_2509_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T15:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1njgicz</id>
    <title>China bans its biggest tech companies from acquiring Nvidia chips, says report — Beijing claims its homegrown AI processors now match H20 and RTX Pro 6000D</title>
    <updated>2025-09-17T15:37:22+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgicz/china_bans_its_biggest_tech_companies_from/"&gt; &lt;img alt="China bans its biggest tech companies from acquiring Nvidia chips, says report — Beijing claims its homegrown AI processors now match H20 and RTX Pro 6000D" src="https://external-preview.redd.it/8TEqL7hV1nddGcswRwl5w0myECFYu5Ll4SYjP8Vx1jY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a29ca54687a9865b636eb76fe44ba0da1943af79" title="China bans its biggest tech companies from acquiring Nvidia chips, says report — Beijing claims its homegrown AI processors now match H20 and RTX Pro 6000D" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/china-bans-its-biggest-tech-companies-from-acquiring-nvidia-chips-says-report-beijing-claims-its-homegrown-ai-processors-now-match-h20-and-rtx-pro-6000d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgicz/china_bans_its_biggest_tech_companies_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njgicz/china_bans_its_biggest_tech_companies_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T15:37:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1njqt5s</id>
    <title>once China is able to produce its own GPU for datacenters (which they are forced to due to both import and export bans by both China and USA), there will be less reason to release their models open weight?</title>
    <updated>2025-09-17T22:07:52+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njqt5s/once_china_is_able_to_produce_its_own_gpu_for/"&gt; &lt;img alt="once China is able to produce its own GPU for datacenters (which they are forced to due to both import and export bans by both China and USA), there will be less reason to release their models open weight?" src="https://preview.redd.it/s4cols18tspf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ff443b6756ab190d26356690b7694db6efda4f6" title="once China is able to produce its own GPU for datacenters (which they are forced to due to both import and export bans by both China and USA), there will be less reason to release their models open weight?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s4cols18tspf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njqt5s/once_china_is_able_to_produce_its_own_gpu_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njqt5s/once_china_is_able_to_produce_its_own_gpu_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T22:07:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1njjz7j</id>
    <title>Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)</title>
    <updated>2025-09-17T17:44:17+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt; &lt;img alt="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" src="https://preview.redd.it/4xt9enbairpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cc1ac16a9a2b96134934fcb2f81a9f3d4916b31" title="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4xt9enbairpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T17:44:17+00:00</published>
  </entry>
</feed>
