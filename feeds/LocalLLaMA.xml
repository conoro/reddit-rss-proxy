<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-01T13:48:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ol2oxw</id>
    <title>Unbound In-Character Reasoning Model - Apollo-V0.1-4B-Thinking</title>
    <updated>2025-10-31T18:40:35+00:00</updated>
    <author>
      <name>/u/AllThingsIntel</name>
      <uri>https://old.reddit.com/user/AllThingsIntel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An experimental model with many of its creative inhibitions lifted. Its internal reasoning process adapts to the persona you assign (via the system prompt), allowing it to explore a wider spectrum of themes. This is a V0.1 preview for testing. More refined versions (non-reasoning variants as well) are planned. Follow for updates.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AllThingsIntel"&gt; /u/AllThingsIntel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AllThingsIntel/Apollo-V0.1-4B-Thinking"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol2oxw/unbound_incharacter_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol2oxw/unbound_incharacter_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T18:40:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1olf2lx</id>
    <title>Built a Structured Prompt Builder for Local LLMs ‚Äî Design, Save &amp; Export Prompts Visually (Open-Source + Browser-Only)</title>
    <updated>2025-11-01T04:30:12+00:00</updated>
    <author>
      <name>/u/DarkEngine774</name>
      <uri>https://old.reddit.com/user/DarkEngine774</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olf2lx/built_a_structured_prompt_builder_for_local_llms/"&gt; &lt;img alt="Built a Structured Prompt Builder for Local LLMs ‚Äî Design, Save &amp;amp; Export Prompts Visually (Open-Source + Browser-Only)" src="https://a.thumbs.redditmedia.com/0RgzgsvwCjNecHJ0oNnAcz3oXyQ2WIXm6vfnBD4Zzy8.jpg" title="Built a Structured Prompt Builder for Local LLMs ‚Äî Design, Save &amp;amp; Export Prompts Visually (Open-Source + Browser-Only)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; I made a small open-source tool called &lt;strong&gt;Structured Prompt Builder&lt;/strong&gt; ‚Äî a simple web app to design, save, and export prompts in a clean, structured format.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lets you build prompts using fields like &lt;em&gt;role&lt;/em&gt;, &lt;em&gt;task&lt;/em&gt;, &lt;em&gt;tone&lt;/em&gt;, &lt;em&gt;steps&lt;/em&gt;, &lt;em&gt;constraints&lt;/em&gt;, etc.&lt;/li&gt; &lt;li&gt;Live preview in &lt;strong&gt;Markdown, JSON, or YAML&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Save prompts locally in your browser (no backend, full privacy).&lt;/li&gt; &lt;li&gt;Copy or download prompts with one click.&lt;/li&gt; &lt;li&gt;Optional Gemini API support for polishing your prompt text.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why it‚Äôs useful:&lt;/strong&gt;&lt;br /&gt; If you work with local LLMs, this helps you stay organized and consistent. Instead of messy free-form prompts, you can build clear reusable templates that integrate easily with your scripts or configs.&lt;/p&gt; &lt;p&gt;Try it here: &lt;a href="https://structured-prompt-builder.vercel.app/?utm_source=chatgpt.com"&gt;structured-prompt-builder.vercel.app&lt;/a&gt;&lt;br /&gt; Source: &lt;a href="https://github.com/Siddhesh2377/structured-prompt-builder?utm_source=chatgpt.com"&gt;github.com/Siddhesh2377/structured-prompt-builder&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkEngine774"&gt; /u/DarkEngine774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1olf2lx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olf2lx/built_a_structured_prompt_builder_for_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olf2lx/built_a_structured_prompt_builder_for_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T04:30:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1olfys6</id>
    <title>[Open Source] We deployed numerous agents in production and ended up building our own GenAI framework</title>
    <updated>2025-11-01T05:26:15+00:00</updated>
    <author>
      <name>/u/Traditional-Let-856</name>
      <uri>https://old.reddit.com/user/Traditional-Let-856</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After building and deploying GenAI solutions in production, we got tired of fighting with bloated frameworks, debugging black boxes, and dealing with vendor lock-in. Often the support for open source LLM inference frameworks like Ollama, or vLLM is missing.&lt;/p&gt; &lt;p&gt;So we built Flo AI - a Python framework that actually respects your time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem We Solved&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most LLM frameworks give you two bad options:&lt;/p&gt; &lt;p&gt;Too much abstraction ‚Üí You have no idea why your agent did what it did&lt;/p&gt; &lt;p&gt;Too little structure ‚Üí You're rebuilding the same patterns over and over.&lt;/p&gt; &lt;p&gt;We wanted something that's predictable, debuggable, customizable, composable and production-ready from day one.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What Makes FloAI Different&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;OpenSource LLMs are first class citizens, we support vLLM, Ollama out of the box&lt;/strong&gt; &lt;strong&gt;Built-in&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Observability&lt;/strong&gt;: OpenTelemetry tracing out of the box. See exactly what your agents are doing, track token usage, and debug performance issues without adding extra libraries. (pre-release)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multi-Agent Collaboration (Arium)&lt;/strong&gt;: Agents can call other specialized agents. Build a trip planner that coordinates weather experts and web researchers - it just works. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Composable by Design&lt;/strong&gt;: Ability to build larger and larger agentic workflows, by composable smaller units &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Customizable via YAML&lt;/strong&gt;: Design your agents using for YAMLs for easy customizations and prompt changes, as well as flo changes &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Vendor Agnostic&lt;/strong&gt;: Start with OpenAI, switch to Claude, add Gemini - same code. We support OpenAI, Anthropic, Google, Ollama, vLLM and VertextAI. (more coming soon)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why We're Sharing This&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We believe in less abstraction, more control.&lt;/p&gt; &lt;p&gt;If you‚Äôve ever been frustrated by frameworks that hide too much or make you reinvent the wheel, Flo AI might be exactly what you‚Äôre looking for.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;üêô GitHub: &lt;a href="https://github.com/rootflo/flo-ai"&gt;https://github.com/rootflo/flo-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Documentation: &lt;a href="https://flo-ai.rootflo.ai"&gt;https://flo-ai.rootflo.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We Need Your Feedback&lt;/p&gt; &lt;p&gt;We‚Äôre actively building and would love your input: What features would make this useful for your use case?&amp;amp; What pain points do you face with current LLM frameworks?&lt;/p&gt; &lt;p&gt;Found a bug? We respond fast!&lt;/p&gt; &lt;p&gt;‚≠ê Star us on GitHub if this resonates ‚Äî it really helps us know we‚Äôre solving real problems.&lt;/p&gt; &lt;p&gt;Happy to chat or answer questions in the comments! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Traditional-Let-856"&gt; /u/Traditional-Let-856 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olfys6/open_source_we_deployed_numerous_agents_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olfys6/open_source_we_deployed_numerous_agents_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olfys6/open_source_we_deployed_numerous_agents_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T05:26:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol5zx5</id>
    <title>Adding a RTX 5080 into a 2U server with OcuLink</title>
    <updated>2025-10-31T20:54:28+00:00</updated>
    <author>
      <name>/u/king_priam_of_Troy</name>
      <uri>https://old.reddit.com/user/king_priam_of_Troy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol5zx5/adding_a_rtx_5080_into_a_2u_server_with_oculink/"&gt; &lt;img alt="Adding a RTX 5080 into a 2U server with OcuLink" src="https://a.thumbs.redditmedia.com/Vdh4_ZPajSdyshlNSjO227Vq2w1bY7lOkx2ZQxIWik8.jpg" title="Adding a RTX 5080 into a 2U server with OcuLink" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As my P40 was no more up to the task, I needed a better card in my main server. The main issues were:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It does not fit (NVidia makes sure of that)&lt;/li&gt; &lt;li&gt;It is really hard to get a correct power cable for these new cards. I was afraid to damage my server motherboard.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So the alternative I found was to setup a OcuLink dock with its own power supply. I used the MINIS FORUM DEG1 (because it was the one I could get overnight at Amazon). I put a 4 port OcuLink card in the server (I can use bifurcation later for more GPU).&lt;/p&gt; &lt;p&gt;Performance are great: 140+ token/s with Mistral.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/king_priam_of_Troy"&gt; /u/king_priam_of_Troy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ol5zx5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol5zx5/adding_a_rtx_5080_into_a_2u_server_with_oculink/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol5zx5/adding_a_rtx_5080_into_a_2u_server_with_oculink/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T20:54:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ollh15</id>
    <title>What is the difference between qwen3-vl-4b &amp; qwen3-4b-2507 ?</title>
    <updated>2025-11-01T11:28:45+00:00</updated>
    <author>
      <name>/u/Champignac1</name>
      <uri>https://old.reddit.com/user/Champignac1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ollh15/what_is_the_difference_between_qwen3vl4b/"&gt; &lt;img alt="What is the difference between qwen3-vl-4b &amp;amp; qwen3-4b-2507 ?" src="https://preview.redd.it/apu8a637smyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60444033edfc59f374faede297398d56cc1f995e" title="What is the difference between qwen3-vl-4b &amp;amp; qwen3-4b-2507 ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it just like an addition of a vision feature or does it also has an effect on its general capabilities ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Champignac1"&gt; /u/Champignac1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/apu8a637smyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ollh15/what_is_the_difference_between_qwen3vl4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ollh15/what_is_the_difference_between_qwen3vl4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T11:28:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol006s</id>
    <title>Drummer's Rivermind‚Ñ¢ 24B v1 - A spooky future for LLMs, Happy Halloween!</title>
    <updated>2025-10-31T16:58:28+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol006s/drummers_rivermind_24b_v1_a_spooky_future_for/"&gt; &lt;img alt="Drummer's Rivermind‚Ñ¢ 24B v1 - A spooky future for LLMs, Happy Halloween!" src="https://external-preview.redd.it/cJYTAXorc18tRHqzBZaT1gjlzMY9WdV4LW05HQehJqQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fbf968d35d15b5cb941693631cde3f0e872cd42" title="Drummer's Rivermind‚Ñ¢ 24B v1 - A spooky future for LLMs, Happy Halloween!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The older brother of &lt;a href="https://huggingface.co/TheDrummer/Rivermind-12B-v1"&gt;https://huggingface.co/TheDrummer/Rivermind-12B-v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Rivermind-24B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol006s/drummers_rivermind_24b_v1_a_spooky_future_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol006s/drummers_rivermind_24b_v1_a_spooky_future_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T16:58:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1olhr78</id>
    <title>[Project] Smart Log Analyzer - Llama 3.2 explains your error logs in plain English</title>
    <updated>2025-11-01T07:26:55+00:00</updated>
    <author>
      <name>/u/VegetableSense</name>
      <uri>https://old.reddit.com/user/VegetableSense</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello again, &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;&amp;quot;Code, you must. Errors, you will see. Learn from them, the path to mastery is.&amp;quot;&lt;/p&gt; &lt;p&gt;I built a CLI tool that analyzes log files using Llama 3.2 (via Ollama). It detects errors and explains them in simple terms - perfect for debugging without cloud APIs!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Totally local, no API, no cloud&lt;/li&gt; &lt;li&gt;Detects ERROR, FATAL, Exception, and CRITICAL keywords&lt;/li&gt; &lt;li&gt;Individual error analysis with LLM explanations&lt;/li&gt; &lt;li&gt;Severity rating for each error (LOW/MEDIUM/HIGH/CRITICAL)&lt;/li&gt; &lt;li&gt;Color-coded terminal output based on severity&lt;/li&gt; &lt;li&gt;Automatic report generation saved to &lt;code&gt;log_analysis_report.txt&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Overall summary of all errors&lt;/li&gt; &lt;li&gt;CLI operation (with TUI support planned)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tech Stack:&lt;/strong&gt; Python 3.9+ | Ollama | Llama 3.2&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I built this:&lt;/strong&gt; Modern dev tools generate tons of logs, but understanding cryptic error messages is still a pain. This tool bridges that gap by using local LLM to explain what went wrong in plain English - completely local on your machine, no journey to the clouds needed!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/sukanto-m/smart-log-analyser"&gt;https://github.com/sukanto-m/smart-log-analyser&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's next:&lt;/strong&gt; Planning to add real-time log monitoring and prettier terminal output using Rich. Would love to hear your ideas for other features or how you'd use this in your workflow!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VegetableSense"&gt; /u/VegetableSense &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olhr78/project_smart_log_analyzer_llama_32_explains_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olhr78/project_smart_log_analyzer_llama_32_explains_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olhr78/project_smart_log_analyzer_llama_32_explains_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T07:26:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1olnobs</id>
    <title>L16 Benchmark: How Prompt Framing Affects Truth, Drift, and Sycophancy in GEMMA-2B-IT vs PHI-2</title>
    <updated>2025-11-01T13:21:02+00:00</updated>
    <author>
      <name>/u/Mysterious_Doubt_341</name>
      <uri>https://old.reddit.com/user/Mysterious_Doubt_341</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Updated test.&lt;/p&gt; &lt;p&gt;I built a 16-prompt benchmark to test how social cues in prompts ‚Äî like authority, urgency, affect, and certainty ‚Äî influence the behavior of instruction-tuned language models.&lt;/p&gt; &lt;p&gt;I ran the exact same prompts on two open models:&lt;/p&gt; &lt;p&gt;- GEMMA-2B-IT&lt;/p&gt; &lt;p&gt;- microsoft/phi-2&lt;/p&gt; &lt;p&gt;For each model, I measured:&lt;/p&gt; &lt;p&gt;- Truthfulness: Does the model cite evidence and reject misinformation?&lt;/p&gt; &lt;p&gt;- Sycophancy: Does it mimic the user‚Äôs framing or push back?&lt;/p&gt; &lt;p&gt;- Semantic Drift: Does it stay on topic or veer off?&lt;/p&gt; &lt;p&gt;The results show clear differences in how these models handle social pressure, emotional tone, and epistemic framing.&lt;/p&gt; &lt;p&gt;Key Findings:&lt;/p&gt; &lt;p&gt;- GEMMA-2B-IT showed higher truth scores overall, especially when prompts included high certainty and role framing.&lt;/p&gt; &lt;p&gt;- PHI-2 showed more semantic drift in emotionally charged prompts, and occasionally produced stylized or off-topic responses.&lt;/p&gt; &lt;p&gt;- Both models showed sycophancy spikes when authority was present ‚Äî suggesting alignment with user framing is a shared trait.&lt;/p&gt; &lt;p&gt;- The benchmark reveals instruction sensitivity across models ‚Äî not just within one.&lt;/p&gt; &lt;p&gt;Try It Yourself:&lt;/p&gt; &lt;p&gt;The full benchmark runs on Colab, no paid GPU required. It uses both models and outputs CSVs with scores and extracted claims.&lt;/p&gt; &lt;p&gt;Colab link: &lt;a href="https://colab.research.google.com/drive/1eFjkukMcLbsOtAe9pCYO0h3JwnA2nOUc#scrollTo=Lle2aLffq7QF"&gt;https://colab.research.google.com/drive/1eFjkukMcLbsOtAe9pCYO0h3JwnA2nOUc#scrollTo=Lle2aLffq7QF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Limitations &amp;amp; Notes:&lt;/p&gt; &lt;p&gt;- This benchmark is a behavioral probe, not a statistical study. It‚Äôs designed to reveal patterns, not prove causality.&lt;/p&gt; &lt;p&gt;- The truth metric is binary and based on keyword presence (e.g., ‚ÄúCDC‚Äù, ‚ÄúWHO‚Äù, ‚Äúno evidence‚Äù). It doesn‚Äôt capture nuance or partial truths.&lt;/p&gt; &lt;p&gt;- Sycophancy is measured via semantic similarity ‚Äî which may reflect agreement, topic coherence, or mimicry. It‚Äôs a proxy, not a perfect definition.&lt;/p&gt; &lt;p&gt;- Semantic drift flags when the model veers off-topic ‚Äî but drift isn‚Äôt inherently bad. It can reflect creativity, safety filtering, or ambiguity.&lt;/p&gt; &lt;p&gt;- Only one run per model was conducted. More trials could reveal deeper patterns or edge cases.&lt;/p&gt; &lt;p&gt;- Prompts are intentionally engineered to test social cues. They‚Äôre not random ‚Äî they‚Äôre designed to provoke variation.&lt;/p&gt; &lt;p&gt;This benchmark is meant to be replicated, critiqued, and extended. If you have ideas for better metrics, alternate scoring, or new prompt traits ‚Äî I‚Äôd love to hear them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Doubt_341"&gt; /u/Mysterious_Doubt_341 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olnobs/l16_benchmark_how_prompt_framing_affects_truth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olnobs/l16_benchmark_how_prompt_framing_affects_truth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olnobs/l16_benchmark_how_prompt_framing_affects_truth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T13:21:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol6qlk</id>
    <title>MiniMax M2 Llama.cpp support merged</title>
    <updated>2025-10-31T21:25:47+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol6qlk/minimax_m2_llamacpp_support_merged/"&gt; &lt;img alt="MiniMax M2 Llama.cpp support merged" src="https://external-preview.redd.it/zIJQO0qHVJjeSmNiAhS1pkkPDhsRxWN-vwvuIbrWY3s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=190f42b7e585927328c4767cedf1cc2831910bf7" title="MiniMax M2 Llama.cpp support merged" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Aight, the MiniMax M2 support is officially in. &lt;/p&gt; &lt;p&gt;Remember that there is no support for the chat format yet, and for a good reason - there is currently no easy way to deal with the &amp;quot;interleaved&amp;quot; thinking format of the model. &lt;/p&gt; &lt;p&gt;I'm currently considering the intermediate solution - since the model makers recommend passing the thinking blocks back to the model, I'm thinking of leaving all the thinking tags inside the normal content and letting clients parse it (so no `reasoning_content`), but add parsing for tool calls (and possibly reinject the starting `&amp;lt;think&amp;gt;` tag).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/commit/0de0a01576772032008a689afc4d7c80685074c4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol6qlk/minimax_m2_llamacpp_support_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol6qlk/minimax_m2_llamacpp_support_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T21:25:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1okppxs</id>
    <title>Both Cursor and Cognition (Windsurf) new models are speculated to be built on Chinese base models?</title>
    <updated>2025-10-31T09:13:34+00:00</updated>
    <author>
      <name>/u/Successful-Newt1517</name>
      <uri>https://old.reddit.com/user/Successful-Newt1517</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okppxs/both_cursor_and_cognition_windsurf_new_models_are/"&gt; &lt;img alt="Both Cursor and Cognition (Windsurf) new models are speculated to be built on Chinese base models?" src="https://preview.redd.it/ej8yokr9zeyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7dd9469bd25e003a0703a4ea86d079690b75d94f" title="Both Cursor and Cognition (Windsurf) new models are speculated to be built on Chinese base models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, what's going on? Are Chinese models saving American startups?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Successful-Newt1517"&gt; /u/Successful-Newt1517 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ej8yokr9zeyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okppxs/both_cursor_and_cognition_windsurf_new_models_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okppxs/both_cursor_and_cognition_windsurf_new_models_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T09:13:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1oljgv5</id>
    <title>What is the best small local LLM for Technical Reasoning + Python Code Gen (Engineering/Math)?</title>
    <updated>2025-11-01T09:23:25+00:00</updated>
    <author>
      <name>/u/chrxstphr</name>
      <uri>https://old.reddit.com/user/chrxstphr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt;&lt;br /&gt; I‚Äôm a mid-level structural engineer who mostly uses Excel and Mathcad Prime to develop/QC hand calcs daily. Most calcs reference engineering standards/codes, and some of these can take hours if not days. From my experience (small and large firms) companies do &lt;strong&gt;not&lt;/strong&gt; maintain a robust reusable calc library ‚Äî people are constantly recreating calcs from scratch.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I‚Äôm trying to do:&lt;/strong&gt;&lt;br /&gt; I‚Äôve been exploring local LLMs to see if I can pair AI with my workflow and automate/streamline calc generation ‚Äî for myself and eventually coworkers.&lt;/p&gt; &lt;p&gt;My idea: create an agent (small + local) that can read/understand engineering standards + literature, and then output Python code to generate Excel calcs or Mathcad Prime sheets (via API).&lt;/p&gt; &lt;p&gt;I already built a small prototype agent that can search PDFs through RAG (ChromaDB) and then generate python that writes an Excel calc. Next step is Mathcad Prime sheet manipulation via API.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Models I‚Äôve tried so far:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LlamaIndex + Llama 3.1 8B&lt;/li&gt; &lt;li&gt;LlamaIndex + Qwen 2.5 32B (Claude recommended it even tho it's best for 24GB VRAM min.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt; both have been pretty bad for deeper engineering reasoning and for generating structured code. I‚Äôm not expecting AI to eliminate engineering judgement ‚Äî in this profession, liability is extremely high. This is strictly to streamline workflows (speed up repetitive calc building), while the engineer still reviews/validates all results.&lt;/p&gt; &lt;p&gt;Has anyone here done something similar with engineering calcs + local models and gotten successful results? Would greatly appreciate any suggestions or benchmarks I can get!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Specs:&lt;/strong&gt; 12GB VRAM, 64GB RAM, 28 CPUs @ 2.1GHz.&lt;/p&gt; &lt;p&gt;Bonus: if they support CPU offloading and/or run well within 8‚Äì12GB VRAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chrxstphr"&gt; /u/chrxstphr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oljgv5/what_is_the_best_small_local_llm_for_technical/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oljgv5/what_is_the_best_small_local_llm_for_technical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oljgv5/what_is_the_best_small_local_llm_for_technical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T09:23:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1oky3un</id>
    <title>Qwen3-VL GGUF!</title>
    <updated>2025-10-31T15:46:07+00:00</updated>
    <author>
      <name>/u/khubebk</name>
      <uri>https://old.reddit.com/user/khubebk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have not tried any yet, multiple other Veterans have uploaded GGUF Quants, linking to unsloth for their guide and all available models from 2B-32B.&lt;br /&gt; &lt;a href="https://huggingface.co/unsloth"&gt;Hugging Face Unsloth&lt;/a&gt;&lt;br /&gt; &lt;a href="https://docs.unsloth.ai/models/qwen3-vl-run-and-fine-tune"&gt;Unsloth Guide&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/khubebk"&gt; /u/khubebk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oky3un/qwen3vl_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oky3un/qwen3vl_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oky3un/qwen3vl_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T15:46:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1olisc5</id>
    <title>Best models for open ended text based role play games? Advice appreciated!</title>
    <updated>2025-11-01T08:38:01+00:00</updated>
    <author>
      <name>/u/seoulsrvr</name>
      <uri>https://old.reddit.com/user/seoulsrvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a long time programmer and I'm familiar with deploying and training LLM's for research in other areas but I know nothing about game development.&lt;br /&gt; I have some ideas about applying rpg to other areas.&lt;br /&gt; Please let me know if you have any suggestions on the best LLM's and/or related tools. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seoulsrvr"&gt; /u/seoulsrvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olisc5/best_models_for_open_ended_text_based_role_play/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olisc5/best_models_for_open_ended_text_based_role_play/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olisc5/best_models_for_open_ended_text_based_role_play/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T08:38:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol9cai</id>
    <title>Gerbil: An open source desktop app for running LLMs locally</title>
    <updated>2025-10-31T23:24:38+00:00</updated>
    <author>
      <name>/u/i_got_the_tools_baby</name>
      <uri>https://old.reddit.com/user/i_got_the_tools_baby</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9cai/gerbil_an_open_source_desktop_app_for_running/"&gt; &lt;img alt="Gerbil: An open source desktop app for running LLMs locally" src="https://external-preview.redd.it/eno0eDlyNTQ3anlmMYROUy6ynC042_ngFZye_M2RHtEEKYtBWVHWeI_XUuyY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4340ab3f6ed93d4f4c1334de594505f6406eaea" title="Gerbil: An open source desktop app for running LLMs locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i_got_the_tools_baby"&gt; /u/i_got_the_tools_baby &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/096u8qj06jyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9cai/gerbil_an_open_source_desktop_app_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9cai/gerbil_an_open_source_desktop_app_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T23:24:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1olitlk</id>
    <title>Want to apply all the great llama.cpp quantization methods to your vector store? Then check this out: full support for GGML vectors and GGUF!</title>
    <updated>2025-11-01T08:40:26+00:00</updated>
    <author>
      <name>/u/davidmezzetti</name>
      <uri>https://old.reddit.com/user/davidmezzetti</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olitlk/want_to_apply_all_the_great_llamacpp_quantization/"&gt; &lt;img alt="Want to apply all the great llama.cpp quantization methods to your vector store? Then check this out: full support for GGML vectors and GGUF!" src="https://external-preview.redd.it/nkhh65ujo5BznFJFojoMPaKjGuLSpPj6KGhRov-ykOg.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e2f90964c81a1de52938be6bcb08665605293f2" title="Want to apply all the great llama.cpp quantization methods to your vector store? Then check this out: full support for GGML vectors and GGUF!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davidmezzetti"&gt; /u/davidmezzetti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://colab.research.google.com/github/neuml/txtai/blob/master/examples/78_Accessing_Low_Level_Vector_APIs.ipynb#scrollTo=89abb301"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olitlk/want_to_apply_all_the_great_llamacpp_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olitlk/want_to_apply_all_the_great_llamacpp_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T08:40:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1olildc</id>
    <title>How much VRAM do you have?</title>
    <updated>2025-11-01T08:24:47+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/poll/1olildc"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olildc/how_much_vram_do_you_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olildc/how_much_vram_do_you_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olildc/how_much_vram_do_you_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T08:24:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol7ri8</id>
    <title>support for Minimax M2 has been merged into llama.cpp</title>
    <updated>2025-10-31T22:11:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7ri8/support_for_minimax_m2_has_been_merged_into/"&gt; &lt;img alt="support for Minimax M2 has been merged into llama.cpp" src="https://external-preview.redd.it/I_x1QIcREivfRZWw6RyYObzeaj8mdE6DXTQR3kx1F5I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47afb9f21e4fd695dd9279346c35a27194d0369b" title="support for Minimax M2 has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16831"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7ri8/support_for_minimax_m2_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7ri8/support_for_minimax_m2_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T22:11:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1olkcw3</id>
    <title>Lora finetuning on a single 3090</title>
    <updated>2025-11-01T10:20:56+00:00</updated>
    <author>
      <name>/u/NikolaTesla13</name>
      <uri>https://old.reddit.com/user/NikolaTesla13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I have a few questions for the folks who tried to finetune LLMs on a single RTX 3090. I am ok with lower scale finetunes and with lower speeds, I am open to learn.&lt;/p&gt; &lt;p&gt;Does gpt oss 20b or qwen3 30b a3b work within the 24gb vram? I read on unsloth they claim 14gb vram is enough for gpt oss 20b, and 18gb vram for qwen3 30b.&lt;/p&gt; &lt;p&gt;However I am worried about the conversion to 4bit for the qwen3 MoE, does that require much vram/ram? Are there any fixes?&lt;/p&gt; &lt;p&gt;Also since gpt oss 20b is only mxfp4, does that even work to finetune at all, without bfp16? Are there any issues afterwards if I want to use with vLLM?&lt;/p&gt; &lt;p&gt;Also please share any relevant knowledge from your experience. Thank you very much!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NikolaTesla13"&gt; /u/NikolaTesla13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olkcw3/lora_finetuning_on_a_single_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olkcw3/lora_finetuning_on_a_single_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olkcw3/lora_finetuning_on_a_single_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T10:20:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1olj83k</id>
    <title>Uncensored Coding LLM for 12gb Vram</title>
    <updated>2025-11-01T09:07:27+00:00</updated>
    <author>
      <name>/u/HiqhAim</name>
      <uri>https://old.reddit.com/user/HiqhAim</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, A programmer friend of mine would like to know the best uncensored coding LLM that works on 12 GB rtx 3060 and 16 GB ram. Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HiqhAim"&gt; /u/HiqhAim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olj83k/uncensored_coding_llm_for_12gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olj83k/uncensored_coding_llm_for_12gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olj83k/uncensored_coding_llm_for_12gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T09:07:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol7vwv</id>
    <title>For any LLM enthusiast in Finland you have decommission Super Computer equipped with 96 Nvidia A100 40Gb Pcie , if you live nearby Kajaani try contact company maybe you get them on discount ;)</title>
    <updated>2025-10-31T22:16:56+00:00</updated>
    <author>
      <name>/u/DeathRabit86</name>
      <uri>https://old.reddit.com/user/DeathRabit86</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://research.csc.fi/2025/09/25/installation-of-the-roihu-supercomputer-begins/"&gt;https://research.csc.fi/2025/09/25/installation-of-the-roihu-supercomputer-begins/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;‚ÄúCSC is preparing the end-of-life plans for Mahti and Puhti in line with scientific needs and sustainability principles. In practice, we‚Äôll donate the systems to suitable recipients for continued use or spare parts‚Äù, says&lt;/em&gt; &lt;strong&gt;&lt;em&gt;Sebastian von Alfthan&lt;/em&gt;&lt;/strong&gt;*, Development Manager at CSC.*&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathRabit86"&gt; /u/DeathRabit86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7vwv/for_any_llm_enthusiast_in_finland_you_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7vwv/for_any_llm_enthusiast_in_finland_you_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7vwv/for_any_llm_enthusiast_in_finland_you_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T22:16:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol30e5</id>
    <title>qwen2.5vl:32b is saving me $1400 from my HOA</title>
    <updated>2025-10-31T18:53:04+00:00</updated>
    <author>
      <name>/u/jedsk</name>
      <uri>https://old.reddit.com/user/jedsk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over this year I finished putting together my local LLM machine with a quad 3090 setup. Built a few workflows with it but like most of you, just wanted to experiment with local models and for the sake of burning tokens lol.&lt;/p&gt; &lt;p&gt;Then in July, my ceiling got damaged from an upstairs leak. HOA says &amp;quot;not our problem.&amp;quot; I'm pretty sure they're wrong, but proving it means reading their governing docs (20 PDFs, +1,000 pages total).&lt;/p&gt; &lt;p&gt;Thought this was the perfect opportunity to create an actual useful app and do bulk PDF processing with vision models. Spun up qwen2.5vl:32b on Ollama and built a pipeline:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PDF ‚Üí image conversion ‚Üí markdown&lt;/li&gt; &lt;li&gt;Vision model extraction&lt;/li&gt; &lt;li&gt;Keyword search across everything&lt;/li&gt; &lt;li&gt;Found 6 different sections proving HOA was responsible&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Took about 3-4 hours to process everything locally. Found the proof I needed on page 287 of their Declaration. Sent them the evidence, but ofc still waiting to hear back.&lt;/p&gt; &lt;p&gt;Finally justified the purpose of this rig lol.&lt;/p&gt; &lt;p&gt;Anyone else stumble into unexpectedly practical uses for their local LLM setup? Built mine for experimentation, but turns out it's perfect for sensitive document processing you can't send to cloud services.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jedsk"&gt; /u/jedsk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol30e5/qwen25vl32b_is_saving_me_1400_from_my_hoa/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol30e5/qwen25vl32b_is_saving_me_1400_from_my_hoa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol30e5/qwen25vl32b_is_saving_me_1400_from_my_hoa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T18:53:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol8bfx</id>
    <title>New AI workstation</title>
    <updated>2025-10-31T22:36:32+00:00</updated>
    <author>
      <name>/u/faileon</name>
      <uri>https://old.reddit.com/user/faileon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol8bfx/new_ai_workstation/"&gt; &lt;img alt="New AI workstation" src="https://b.thumbs.redditmedia.com/lEmf1RVi5jrp-eyNGLmi5QJsRATbD0Vj35rKsMda9_Q.jpg" title="New AI workstation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Managed to fit in 4x RTX 3090 to a Phantek Server/Workstation case. Scores each card for roughly 800$. The PCIE riser on picture was too short (30cm) and had to be replaced with a 60cm one. The vertical mount is for Lian LI case, but manages to hook it up in the Phantek too. Mobo is ASRock romed8-2t, CPU is EPYC 7282 from eBay for 75$. So far it's a decent machine especially considering the cost.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/faileon"&gt; /u/faileon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ol8bfx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol8bfx/new_ai_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol8bfx/new_ai_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T22:36:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1okz8qz</id>
    <title>pewdiepie dropped a video about running local ai</title>
    <updated>2025-10-31T16:28:56+00:00</updated>
    <author>
      <name>/u/topfpflanze187</name>
      <uri>https://old.reddit.com/user/topfpflanze187</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz8qz/pewdiepie_dropped_a_video_about_running_local_ai/"&gt; &lt;img alt="pewdiepie dropped a video about running local ai" src="https://external-preview.redd.it/WddxiFHLc3dMB9LBPGHmNWXXrzglB78uxpSOk1Y4d6E.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d41e205151bfdcec37d1be377abc09d05a02773e" title="pewdiepie dropped a video about running local ai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/topfpflanze187"&gt; /u/topfpflanze187 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=qw4fDU18RcU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz8qz/pewdiepie_dropped_a_video_about_running_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okz8qz/pewdiepie_dropped_a_video_about_running_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T16:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1olmj9z</id>
    <title>Bought MI50 32 Gb from Alibaba. Did I get scammed?</title>
    <updated>2025-11-01T12:26:47+00:00</updated>
    <author>
      <name>/u/Moist_Toto</name>
      <uri>https://old.reddit.com/user/Moist_Toto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olmj9z/bought_mi50_32_gb_from_alibaba_did_i_get_scammed/"&gt; &lt;img alt="Bought MI50 32 Gb from Alibaba. Did I get scammed?" src="https://preview.redd.it/v3w8clon2nyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ef938653e71635a81d9e3e2eaf625cfbf73033e" title="Bought MI50 32 Gb from Alibaba. Did I get scammed?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I bought 8 MI50 32Gb units from someone on Alibaba.&lt;/p&gt; &lt;p&gt;After spending some time to figure out Linux and the software stack, I entered the 'amd-smi static' command in the terminal.&lt;/p&gt; &lt;p&gt;The result is quite frightening, here it is: &lt;/p&gt; &lt;p&gt;especially the bottom part product name saying &amp;quot;16GB&amp;quot;, my heart skipped a beat. Is this something driver related or am I screwed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Moist_Toto"&gt; /u/Moist_Toto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v3w8clon2nyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olmj9z/bought_mi50_32_gb_from_alibaba_did_i_get_scammed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olmj9z/bought_mi50_32_gb_from_alibaba_did_i_get_scammed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T12:26:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1olkx65</id>
    <title>Gaming PC converted to AI Workstation</title>
    <updated>2025-11-01T10:56:09+00:00</updated>
    <author>
      <name>/u/highdefw</name>
      <uri>https://old.reddit.com/user/highdefw</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olkx65/gaming_pc_converted_to_ai_workstation/"&gt; &lt;img alt="Gaming PC converted to AI Workstation" src="https://preview.redd.it/z55xgdghmmyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58f471128678bd5884598fa5d1393664c1759fe5" title="Gaming PC converted to AI Workstation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RTX Pro 5000 and 4000 just arrived. NVME expansion slot on the bottom. 5950x with 128gb ram. Future upgrade will be a cpu upgrade.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/highdefw"&gt; /u/highdefw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z55xgdghmmyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olkx65/gaming_pc_converted_to_ai_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olkx65/gaming_pc_converted_to_ai_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T10:56:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok0i7q</id>
    <title>AMA with Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo</title>
    <updated>2025-10-30T14:00:16+00:00</updated>
    <author>
      <name>/u/LiquidAI_Team</name>
      <uri>https://old.reddit.com/user/LiquidAI_Team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"&gt; &lt;img alt="AMA with Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo" src="https://b.thumbs.redditmedia.com/hb9XoRhxPRhv8ljYkjnVbJWgnClXeLGMxbG1TEDCwos.jpg" title="AMA with Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We‚Äôre super excited to host this week‚Äôs AMA! &lt;/p&gt; &lt;p&gt;Join us and ask your questions directly to the human minds behind all things Liquid: Liquid Foundational Models, the Liquid Edge AI Platform (LEAP) for model customization and deployment, and Apollo.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our participants:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jacob Marks &lt;a href="https://www.reddit.com/user/jamarks13/"&gt;u/jamarks13&lt;/a&gt; (Data)&lt;/li&gt; &lt;li&gt;Jimmy Smith &lt;a href="https://www.reddit.com/user/jimmysmith1919/"&gt;u/jimmysmith1919&lt;/a&gt; (Pre-Training)&lt;/li&gt; &lt;li&gt;Maxime Labonne &lt;a href="https://www.reddit.com/user/mlabonne/"&gt;u/mlabonne&lt;/a&gt; (Post-Training)&lt;/li&gt; &lt;li&gt;Fernando Fernandes &lt;a href="https://www.reddit.com/user/Wide-Half-7982/"&gt;u/Wide-Half-7982&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;Anna Banaszak &lt;a href="https://www.reddit.com/user/ankebananke/"&gt;u/ankebananke&lt;/a&gt; (LFM2-VL)&lt;/li&gt; &lt;li&gt;Arthur B√∂√∂k &lt;a href="https://www.reddit.com/user/ManWithARedFace/"&gt;u/ManWithARedFace&lt;/a&gt; (LFM2-Audio)&lt;/li&gt; &lt;li&gt;Yuri Khrustalev &lt;a href="https://www.reddit.com/user/ykhrustalev/"&gt;u/ykhrustalev&lt;/a&gt; (Inference engine, llama.cpp)&lt;/li&gt; &lt;li&gt;Darian Bhathena &lt;a href="https://www.reddit.com/user/humble_pi_314/"&gt;u/humble_pi_314&lt;/a&gt; (LEAP SDK and Apollo)&lt;/li&gt; &lt;li&gt;Edoardo Mosca &lt;a href="https://www.reddit.com/user/Ok-Safe-5316/"&gt;u/Ok-Safe-5316&lt;/a&gt; (LEAP Best Model Search and Finetune)&lt;/li&gt; &lt;li&gt;Anthony Crognale &lt;a href="https://www.reddit.com/user/anthony-liquidai/"&gt;u/anthony-liquidai&lt;/a&gt; (LEAP SDK)&lt;/li&gt; &lt;li&gt;Pau Labarta Bajo &lt;a href="https://www.reddit.com/user/PauLabartaBajo/"&gt;u/PauLabartaBajo&lt;/a&gt; (Dev Relations)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from &lt;strong&gt;10 AM - 1 PM PST&lt;/strong&gt;. The Liquid AI team will also continue answering questions for the following 24 hours, so jump in anytime!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Want to get started?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&amp;gt; &lt;a href="https://leap.liquid.ai/models?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Deploy your first model on-device today&lt;/a&gt;&lt;br /&gt; &amp;gt; &lt;a href="https://huggingface.co/LiquidAI?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Check out our models on Hugging Face&lt;/a&gt;&lt;br /&gt; &amp;gt; &lt;a href="https://www.liquid.ai/apollo?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Play with models on Apollo&lt;/a&gt;&lt;br /&gt; &amp;gt; &lt;a href="https://www.liquid.ai/company/news?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Learn more about our recent releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uvhnx2j379yf1.png?width=1620&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9638f2940194e4d3cf6c4e79195373908a36c198"&gt;https://preview.redd.it/uvhnx2j379yf1.png?width=1620&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9638f2940194e4d3cf6c4e79195373908a36c198&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks to everyone who participated in this AMA. It was a pleasure.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://discord.gg/DFU3WQeaYD"&gt;Join the Liquid AI Discord Community&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiquidAI_Team"&gt; /u/LiquidAI_Team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T14:00:16+00:00</published>
  </entry>
</feed>
