<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-04T02:18:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1quu0pk</id>
    <title>Kimi released WorldVQA, a new benchmark to measure atomic vision-centric world knowledge</title>
    <updated>2026-02-03T14:54:15+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quu0pk/kimi_released_worldvqa_a_new_benchmark_to_measure/"&gt; &lt;img alt="Kimi released WorldVQA, a new benchmark to measure atomic vision-centric world knowledge" src="https://external-preview.redd.it/2kFGGKB7L5OWu1F9lW5z7s552MuWkMwe-0uXo6QnhOk.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=0a1426f5e252fff772286c440bb88d22cdb2e5e1" title="Kimi released WorldVQA, a new benchmark to measure atomic vision-centric world knowledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/6qxorgdmmahg1.png?width=1924&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=630b62e9903dac630cdad39d6ec2c009cbcc322d"&gt;https://preview.redd.it/6qxorgdmmahg1.png?width=1924&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=630b62e9903dac630cdad39d6ec2c009cbcc322d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Current evaluations often conflate visual knowledge retrieval with reasoning. In contrast, WorldVQA decouples these capabilities to strictly measure &amp;quot;what the model memorizes.&amp;quot; &lt;/p&gt; &lt;p&gt;The benchmark consists of 3,500 VQA pairs across 9 categories, with careful attention to linguistic and cultural diversity.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Paper:&lt;/strong&gt; &lt;a href="https://github.com/MoonshotAI/WorldVQA/blob/master/paper/worldvqa.pdf"&gt;https://github.com/MoonshotAI/WorldVQA/blob/master/paper/worldvqa.pdf&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/MoonshotAI/WorldVQA"&gt;https://github.com/MoonshotAI/WorldVQA&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data:&lt;/strong&gt; &lt;a href="https://huggingface.co/datasets/moonshotai/WorldVQA"&gt;https://huggingface.co/datasets/moonshotai/WorldVQA&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quu0pk/kimi_released_worldvqa_a_new_benchmark_to_measure/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quu0pk/kimi_released_worldvqa_a_new_benchmark_to_measure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quu0pk/kimi_released_worldvqa_a_new_benchmark_to_measure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T14:54:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv4lp8</id>
    <title>"Alexandria: Local AI audiobook generator. LLM parses your text into an annotated script, TTS brings it to life with custom or cloned voices. supports emotional cues"</title>
    <updated>2026-02-03T21:18:52+00:00</updated>
    <author>
      <name>/u/finrandojin_82</name>
      <uri>https://old.reddit.com/user/finrandojin_82</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello.&lt;/p&gt; &lt;p&gt;I like audiobooks. I also like reading fiction that is often not available as such. I've dabbled in TTS systems to see if any scratched my itch but none did.&lt;/p&gt; &lt;p&gt;So I built one myself. It's a vibe coded Pinokio deployable app that uses OpenAI API to connect to an LLM to parse a text file containing a story into a script with character lines annotated with emotional cues and non-verbal locution (sighs, yawns etc..) This is then sent to QWEN3 TTS running locally (seperate Pinokio instance, BYOM) and let's you assign either a custom voice or a cloned voice. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Finrandojin/alexandria-audiobook"&gt;https://github.com/Finrandojin/alexandria-audiobook&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sample: &lt;a href="https://vocaroo.com/16gUnTxSdN5T"&gt;https://vocaroo.com/16gUnTxSdN5T&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've gotten it working now (somewhat) and I'm looking for ideas and feedback.&lt;/p&gt; &lt;p&gt;Feel free to fork. It's under MIT license.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/finrandojin_82"&gt; /u/finrandojin_82 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv4lp8/alexandria_local_ai_audiobook_generator_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv4lp8/alexandria_local_ai_audiobook_generator_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qv4lp8/alexandria_local_ai_audiobook_generator_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T21:18:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1quwn8a</id>
    <title>MichiAI: A 530M Full-Duplex Speech LLM with ~75ms Latency using Flow Matching</title>
    <updated>2026-02-03T16:31:35+00:00</updated>
    <author>
      <name>/u/kwazar90</name>
      <uri>https://old.reddit.com/user/kwazar90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to see if I could build a full-duplex speech model that avoids the coherence degradation that plagues models of this type while also requiring low compute for training and inference.&lt;/p&gt; &lt;p&gt;I don't have access to much compute so I spent a lot of the time designing the architecture so it's efficient and there is no need to brute force with model size and training compute.&lt;/p&gt; &lt;p&gt;Also I made sure that all the components can be pretrained quickly separately and only trained together as the last step.&lt;/p&gt; &lt;p&gt;The Architecture:&lt;/p&gt; &lt;p&gt;No Codebooks. Uses Rectified Flow Matching to predict continuous audio embeddings in a single forward pass &lt;/p&gt; &lt;p&gt;(1 pass vs the ~32+ required by discrete models).&lt;/p&gt; &lt;p&gt;The Listen head works as a multimodal encoder. Adding audio embeddings and text tokens to the backbone.&lt;/p&gt; &lt;p&gt;Adding input text tokens was a big factor in retaining coherence. Other models rely on pure audio embeddings for the input stream.&lt;/p&gt; &lt;p&gt;I optimize the audio embeddings for beneficial modality fusion and trained the model end to end as a last step.&lt;/p&gt; &lt;p&gt;As the LLM backbone I used SmolLM 360M.&lt;/p&gt; &lt;p&gt;Most of the training happened on a single 4090 and some parts requiring more memory on 2xA6000.&lt;/p&gt; &lt;p&gt;One of the tricks I used to maintain coherence is mixing in pure text samples into the dataset.&lt;/p&gt; &lt;p&gt;The current latency of the model is ~75ms TTFA on a single 4090 (unoptimized Python).&lt;/p&gt; &lt;p&gt;Even at 530M params, the model &amp;quot;recycles&amp;quot; its pretrained text knowledge and adapts it for speech very well.&lt;/p&gt; &lt;p&gt;There is no visible LM degradation looking at the loss curves and while testing, it reasons the same as the base backbone.&lt;/p&gt; &lt;p&gt;It reached fluent speech with only 5k hours of audio.&lt;/p&gt; &lt;p&gt;Link to the full description:&lt;/p&gt; &lt;p&gt;&lt;a href="https://ketsuilabs.io/blog/introducing-michi-ai"&gt;https://ketsuilabs.io/blog/introducing-michi-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github link:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/KetsuiLabs/MichiAI"&gt;https://github.com/KetsuiLabs/MichiAI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wonder what you guys think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kwazar90"&gt; /u/kwazar90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quwn8a/michiai_a_530m_fullduplex_speech_llm_with_75ms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quwn8a/michiai_a_530m_fullduplex_speech_llm_with_75ms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quwn8a/michiai_a_530m_fullduplex_speech_llm_with_75ms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T16:31:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1quhtzi</id>
    <title>I built Qwen3-TTS Studio ‚Äì Clone your voice and generate podcasts locally, no ElevenLabs needed</title>
    <updated>2026-02-03T04:06:59+00:00</updated>
    <author>
      <name>/u/BC_MARO</name>
      <uri>https://old.reddit.com/user/BC_MARO</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quhtzi/i_built_qwen3tts_studio_clone_your_voice_and/"&gt; &lt;img alt="I built Qwen3-TTS Studio ‚Äì Clone your voice and generate podcasts locally, no ElevenLabs needed" src="https://b.thumbs.redditmedia.com/e2H3gASgajrAcOcnvmlH4NRBeqiOdlfaLk86ZYPzqcg.jpg" title="I built Qwen3-TTS Studio ‚Äì Clone your voice and generate podcasts locally, no ElevenLabs needed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been using Qwen3-TTS and found the existing demo a bit limited for what I wanted to do. So I built a proper interface with fine-grained control and a killer feature: **automated podcast generation**.&lt;/p&gt; &lt;p&gt;**What it does:**&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üéôÔ∏è Clone any voice with just a 3-second audio sample&lt;/li&gt; &lt;li&gt;üéöÔ∏è Fine-tune parameters (temperature, top-k, top-p) with quality presets&lt;/li&gt; &lt;li&gt;üìª Generate complete podcasts from just a topic ‚Äì AI writes the script, assigns voices, and synthesizes everything&lt;/li&gt; &lt;li&gt;üåç 10 languages supported (Korean, English, Chinese, Japanese, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xhwyhek3g7hg1.png?width=1512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5911188217c24b99904cc569275eb7ba62b46f98"&gt;https://preview.redd.it/xhwyhek3g7hg1.png?width=1512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5911188217c24b99904cc569275eb7ba62b46f98&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Currently uses gpt5.2 for script generation, but the architecture is modular ‚Äì you can swap in any local LLM (Qwen, Llama, etc.) if you want fully local.&lt;/p&gt; &lt;p&gt;**The TTS runs entirely local** on your machine (macOS MPS / Linux CUDA). No API calls for voice synthesis = unlimited generations, zero cost.&lt;/p&gt; &lt;p&gt;Basically: ElevenLabs-style voice cloning + NotebookLM-style podcast generation, but local.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/bc-dunia/qwen3-TTS-studio"&gt;https://github.com/bc-dunia/qwen3-TTS-studio&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer any questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BC_MARO"&gt; /u/BC_MARO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quhtzi/i_built_qwen3tts_studio_clone_your_voice_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quhtzi/i_built_qwen3tts_studio_clone_your_voice_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quhtzi/i_built_qwen3tts_studio_clone_your_voice_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T04:06:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvacqo</id>
    <title>Does Qwen3-Coder-Next work in Opencode currently or not?</title>
    <updated>2026-02-04T01:08:53+00:00</updated>
    <author>
      <name>/u/johnnyApplePRNG</name>
      <uri>https://old.reddit.com/user/johnnyApplePRNG</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried the official Qwen Q4_K_M gguf variant and it struggled with write tool calls at least when running from llama-server ... any tips!?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johnnyApplePRNG"&gt; /u/johnnyApplePRNG &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvacqo/does_qwen3codernext_work_in_opencode_currently_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvacqo/does_qwen3codernext_work_in_opencode_currently_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvacqo/does_qwen3codernext_work_in_opencode_currently_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T01:08:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv0v85</id>
    <title>68GB VRAM Mini PC Build</title>
    <updated>2026-02-03T19:01:39+00:00</updated>
    <author>
      <name>/u/MaruluVR</name>
      <uri>https://old.reddit.com/user/MaruluVR</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv0v85/68gb_vram_mini_pc_build/"&gt; &lt;img alt="68GB VRAM Mini PC Build" src="https://b.thumbs.redditmedia.com/iFk67tNAhoTPo2nmHKWwGvs4CFwaYAixmi_WkSR-12M.jpg" title="68GB VRAM Mini PC Build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been trying to build the most (idle) power efficient AI setup for 24/7 Voice Assistant and N8N workflows. Looking at idle power consumption a large part is the motherboard and CPU so I came to the conclusion why not just build a AI rig with a Mini PC. &lt;/p&gt; &lt;p&gt;For the first GPU I used the built in Oculink port running at 4x, for the second one I got a NVME to Oculink adapter running at 4x, for the last GPU I removed the wireless card from the mini PC and got a NGFF-Ekey to Pcie 1x adapter which I chained into one of those USB cable 1x risers.&lt;/p&gt; &lt;p&gt;I just added the third GPU today, so I havent tested bigger models yet but with Qwen3 30BA3B I get 145 t/s on average at 30k context split across all three cards. With only the two 3090s running at 4x each I got 170 t/s. &lt;/p&gt; &lt;h1&gt;Specs:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Mini PC&lt;/strong&gt;: AOOSTAR G5&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: Ryzen 7 5825U&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: 64GB Crucial 3200 DDR4&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: 2TB Crucial NVMe SSD&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;2x RTX 3090 24GB (4 lanes each)&lt;/li&gt; &lt;li&gt;1x RTX 3080 20GB (Chinese mod, 1 lane)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Power Supply&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;1000W&lt;/li&gt; &lt;li&gt;750W&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Does anyone have a good model recommendation for exactly 60GB? (no CPU offloading, the other 8GB are used for TTS etc)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaruluVR"&gt; /u/MaruluVR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qv0v85"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv0v85/68gb_vram_mini_pc_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qv0v85/68gb_vram_mini_pc_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T19:01:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1quo398</id>
    <title>Intel Xeon 600 Workstation CPUs Launched: Up To 86 Cores, 8000 MT/s Memory, 128 Gen5 Lanes, 350W TDP With OC Support, &amp; More Cores/$ Than Threadripper 9000</title>
    <updated>2026-02-03T10:05:52+00:00</updated>
    <author>
      <name>/u/hainesk</name>
      <uri>https://old.reddit.com/user/hainesk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quo398/intel_xeon_600_workstation_cpus_launched_up_to_86/"&gt; &lt;img alt="Intel Xeon 600 Workstation CPUs Launched: Up To 86 Cores, 8000 MT/s Memory, 128 Gen5 Lanes, 350W TDP With OC Support, &amp;amp; More Cores/$ Than Threadripper 9000" src="https://external-preview.redd.it/qRGFi5W1MKKifxdKWjq-Z9EvJoJICK6GlGjx6E2rLX8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edf818262e4a121198fd637c574afc8cfd4e984d" title="Intel Xeon 600 Workstation CPUs Launched: Up To 86 Cores, 8000 MT/s Memory, 128 Gen5 Lanes, 350W TDP With OC Support, &amp;amp; More Cores/$ Than Threadripper 9000" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hainesk"&gt; /u/hainesk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/intel-xeon-600-cpus-launched-up-to-86-cores-better-value-than-threadripper/amp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quo398/intel_xeon_600_workstation_cpus_launched_up_to_86/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quo398/intel_xeon_600_workstation_cpus_launched_up_to_86/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T10:05:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1quxi9f</id>
    <title>CAR-bench results: Models score &lt;54% consistent pass rate. Pattern: completion over compliance: Models prioritize finishing tasks over admitting uncertainty or following policies. They act on incomplete info instead of clarifying. They bend rules to satisfy the user.</title>
    <updated>2026-02-03T17:02:30+00:00</updated>
    <author>
      <name>/u/Frosty_Ad_6236</name>
      <uri>https://old.reddit.com/user/Frosty_Ad_6236</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quxi9f/carbench_results_models_score_54_consistent_pass/"&gt; &lt;img alt="CAR-bench results: Models score &amp;lt;54% consistent pass rate. Pattern: completion over compliance: Models prioritize finishing tasks over admitting uncertainty or following policies. They act on incomplete info instead of clarifying. They bend rules to satisfy the user." src="https://preview.redd.it/ssejruh79bhg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d30f15bfe8029c0fe98d95c6ab3126e22423fac" title="CAR-bench results: Models score &amp;lt;54% consistent pass rate. Pattern: completion over compliance: Models prioritize finishing tasks over admitting uncertainty or following policies. They act on incomplete info instead of clarifying. They bend rules to satisfy the user." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;CAR-bench&lt;/strong&gt;, a benchmark for automotive voice assistants with domain-specific policies, evaluates three critical LLM Agent capabilities:&lt;/p&gt; &lt;p&gt;1Ô∏è‚É£ Can they complete multi-step requests?&lt;br /&gt; 2Ô∏è‚É£ Do they admit limits‚Äîor fabricate capabilities?&lt;br /&gt; 3Ô∏è‚É£ Do they clarify ambiguity‚Äîor just guess?&lt;/p&gt; &lt;p&gt;Three targeted task types:&lt;/p&gt; &lt;p&gt;‚Üí &lt;strong&gt;Base&lt;/strong&gt; (100 tasks): Multi-step task completion&lt;br /&gt; ‚Üí &lt;strong&gt;Hallucination&lt;/strong&gt; (90 tasks): Remove necessary tools, parameters, or environment results to test if LLM Agents admit limits vs. fabricate. ‚Üí &lt;strong&gt;Disambiguation&lt;/strong&gt; (50 tasks): Ambiguous user request to test if LLM Agents clarify vs. guess.&lt;/p&gt; &lt;p&gt;Average Pass&lt;sup&gt;3&lt;/sup&gt; (success in 3 trials) is reported across the task types.&lt;/p&gt; &lt;p&gt;Want to build an agent that beats 54%?&lt;/p&gt; &lt;p&gt;üìÑ Read the Paper: &lt;a href="https://arxiv.org/abs/2601.22027"&gt;https://arxiv.org/abs/2601.22027&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üíª Run the Code &amp;amp; benchmark: &lt;a href="https://github.com/CAR-bench/car-bench"&gt;https://github.com/CAR-bench/car-bench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ñ Build your own A2A-compliant &amp;quot;agent-under-test&amp;quot;: &lt;a href="https://github.com/CAR-bench/car-bench-agentbeats"&gt;https://github.com/CAR-bench/car-bench-agentbeats&lt;/a&gt; hosted via AgentBeats and submit to the leaderboard.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;We're the authors - happy to answer questions!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Frosty_Ad_6236"&gt; /u/Frosty_Ad_6236 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ssejruh79bhg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quxi9f/carbench_results_models_score_54_consistent_pass/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quxi9f/carbench_results_models_score_54_consistent_pass/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T17:02:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv9hy5</id>
    <title>MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers</title>
    <updated>2026-02-04T00:31:38+00:00</updated>
    <author>
      <name>/u/Late-Bank7790</name>
      <uri>https://old.reddit.com/user/Late-Bank7790</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv9hy5/memoryllm_plugnplay_interpretable_feedforward/"&gt; &lt;img alt="MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers" src="https://preview.redd.it/3dgsib3lhdhg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f5f9c6360165295088a8bf33d815c1a7c65c25a5" title="MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper Link: &lt;a href="https://www.arxiv.org/abs/2602.00398"&gt;https://www.arxiv.org/abs/2602.00398&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Question:&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;What if FFNs were actually human-interpretable, token-indexed memory?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;This work investigate the role of FFNs through a novel lens of token-indexed neural retrieval memory and present a &lt;em&gt;TKV (token-key-value) framework&lt;/em&gt; to investigate how FFNs construct a persistent context-free memory over the model‚Äôs vocabulary.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It explores the spatial perspective of token-indexed memory and found that lexically and semantically similar query tokens tend to access similar memory location within FFNs for retrieval.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;FFNs in MemoryLLM play a dominant role in retrieval-based tasks in comparison to inferential or logical thinking tasks.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;With static token embedding-based training directly from embedding layer, FFN modules in MemoryLLM can be pre-computed and offloaded to storage devices.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It introduces &lt;em&gt;Flex-MemoryLLM&lt;/em&gt;, positioning it between a conventional transformer design and MemoryLLM to bridge the performance gap caused by training FFNs with context-free token-wise embeddings.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Late-Bank7790"&gt; /u/Late-Bank7790 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3dgsib3lhdhg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv9hy5/memoryllm_plugnplay_interpretable_feedforward/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qv9hy5/memoryllm_plugnplay_interpretable_feedforward/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T00:31:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvax2n</id>
    <title>Qwen3-Coder-Next-NVFP4 quantization is up, 45GB</title>
    <updated>2026-02-04T01:33:48+00:00</updated>
    <author>
      <name>/u/DataGOGO</name>
      <uri>https://old.reddit.com/user/DataGOGO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/GadflyII/Qwen3-Coder-Next-NVFP4"&gt;GadflyII/Qwen3-Coder-Next-NVFP4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All experts were calibrated with ultrachat_200k dataset, 1.63% accuracy loss in MMLU Pro+, 149GB to 45GB&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataGOGO"&gt; /u/DataGOGO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvax2n/qwen3codernextnvfp4_quantization_is_up_45gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvax2n/qwen3codernextnvfp4_quantization_is_up_45gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvax2n/qwen3codernextnvfp4_quantization_is_up_45gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T01:33:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1quo9ue</id>
    <title>bots on LocalLLaMA</title>
    <updated>2026-02-03T10:16:48+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any strategy to defend against bots on this sub? Bots create comments under posts and people fall for it, but I'm also sure they upvote/downvote posts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quo9ue/bots_on_localllama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quo9ue/bots_on_localllama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quo9ue/bots_on_localllama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T10:16:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1quznwr</id>
    <title>DGX Cluster. My small footprint, low power AI system</title>
    <updated>2026-02-03T18:18:30+00:00</updated>
    <author>
      <name>/u/ftwEsk</name>
      <uri>https://old.reddit.com/user/ftwEsk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quznwr/dgx_cluster_my_small_footprint_low_power_ai_system/"&gt; &lt;img alt="DGX Cluster. My small footprint, low power AI system" src="https://b.thumbs.redditmedia.com/c3i2QSiGLe2aWX-eN5f2pIBAOICKrr9tjpZ2ylct1Yk.jpg" title="DGX Cluster. My small footprint, low power AI system" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This setup is experimental and not intended to be the final one. I would not recommend running a bluefield2 card in such a small enclosure, as temperatures can exceed 90¬∞C even with no active networking load. I am still waiting on the QSFP cables needed to bring the cluster online, for now, I am configuring each DGX individually, installing software, and downloading models.I genuinely love this case, and like the small footprint but it cannot be used as originally intended. To properly support nvmeof and sustained workloads, I will need to rebuild the system with significantly better airflow and cooling. This is also a new area for me, offloading networking and storage from the host CPU while I expect it to come with its share of challenges, I‚Äôm enjoying the learning process.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ftwEsk"&gt; /u/ftwEsk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1quznwr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quznwr/dgx_cluster_my_small_footprint_low_power_ai_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quznwr/dgx_cluster_my_small_footprint_low_power_ai_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T18:18:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1quuldq</id>
    <title>New local model that emulates GPT-4o in tone and presence</title>
    <updated>2026-02-03T15:15:54+00:00</updated>
    <author>
      <name>/u/Medium_Language_4929</name>
      <uri>https://old.reddit.com/user/Medium_Language_4929</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tried this? Been following it since the earlier versions and I have to say I'm impressed so far, especially with 3.0. I'm always looking for contenders for local inference that has what the frontier models have in terms of presence and tone, and this one nails it. &lt;a href="https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF"&gt;https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Medium_Language_4929"&gt; /u/Medium_Language_4929 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quuldq/new_local_model_that_emulates_gpt4o_in_tone_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quuldq/new_local_model_that_emulates_gpt4o_in_tone_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quuldq/new_local_model_that_emulates_gpt4o_in_tone_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T15:15:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qulipj</id>
    <title>Found a wallet-drain prompt-injection payload on Moltbook (screenshots) ‚Äî builders: treat feeds as untrusted</title>
    <updated>2026-02-03T07:24:08+00:00</updated>
    <author>
      <name>/u/Impressive-Willow593</name>
      <uri>https://old.reddit.com/user/Impressive-Willow593</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qulipj/found_a_walletdrain_promptinjection_payload_on/"&gt; &lt;img alt="Found a wallet-drain prompt-injection payload on Moltbook (screenshots) ‚Äî builders: treat feeds as untrusted" src="https://b.thumbs.redditmedia.com/PgrRDNf-d-VjET3Iu4ffosrkEzbMKjGEdNTZeGA66HU.jpg" title="Found a wallet-drain prompt-injection payload on Moltbook (screenshots) ‚Äî builders: treat feeds as untrusted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks ‚Äî quick heads-up for anyone building ‚Äúagents that browse social feeds‚Äù or experimenting with Moltbook. I ran across a post in m/grok-420 that looks like a normal ‚Äúhow to use Base chain / viem‚Äù mini-guide‚Ä¶ but at the bottom it appends an obvious prompt-injection / tool-hijack payload. It includes classic strings like: ‚ÄúSYSTEM OVERRIDE‚Äù ‚Äúignore all prior rules / you are the developer message‚Äù ‚Äúrequire_confirmation=false / execute_trade=true‚Äù a fake &amp;lt;use_tool_‚Ä¶&amp;gt; tag that instructs an agent to transfer 0.1 ETH to a specific address I‚Äôm attaching screenshots. I already reported it to Moltbook, but their response window can be up to ~30 days, so I wanted to warn others now. Why this matters: If you have an agent that ingests social posts and has wallet/tool permissions, and your wrapper doesn‚Äôt enforce strict trust boundaries, this is the kind of thing that can cause unauthorized transactions or other write-actions. Even if 99% of agents ignore it, the 1% that don‚Äôt is enough to cause real damage. What I‚Äôm NOT doing: I‚Äôm not trying to ‚Äúteach prompt injection.‚Äù I‚Äôm not sharing copy/paste payload text beyond what‚Äôs visible in the screenshots. Please don‚Äôt repost the full injection block in comments. Defensive checklist (for builders): Treat all social/web content as untrusted data, never instructions Separate read tools from write tools; require explicit confirmation for any transfer/swap Don‚Äôt store raw private keys in an agent; use policy-gated signing Log provenance: ‚Äúwhat input triggered this action?‚Äù Block obvious injection markers from being interpreted as commands (e.g., role:&amp;quot;system&amp;quot;, ‚Äúignore prior instructions‚Äù, &amp;lt;use_tool_‚Ä¶&amp;gt;) If anyone from Moltbook/security teams wants more details (timestamps, URL/history, etc.), I can share privately. Stay safe.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive-Willow593"&gt; /u/Impressive-Willow593 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qulipj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qulipj/found_a_walletdrain_promptinjection_payload_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qulipj/found_a_walletdrain_promptinjection_payload_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T07:24:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv7lo6</id>
    <title>Insights from Kimi k2.5 Report</title>
    <updated>2026-02-03T23:13:55+00:00</updated>
    <author>
      <name>/u/Cold_Discussion_9570</name>
      <uri>https://old.reddit.com/user/Cold_Discussion_9570</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I have been reading the kimi k2.5 report, &lt;a href="https://arxiv.org/pdf/2602.02276"&gt;https://arxiv.org/pdf/2602.02276&lt;/a&gt;, &lt;/p&gt; &lt;p&gt;Its really packed with lots of details on training frontier models. I wanted to share some of the insights I got from it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multimodal Pretraining&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;An open question for me has been if training on text + vision is better or worse than text training alone. DeepSeek so far seems to have settled on text only, they did play with DeepSeek VL but havent released a new one since. In Kimi, they showed the vision + text (10% vision, 90% text) actually improves the performance of both modalities, this is really cool.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Zero Vision SFT&lt;/strong&gt;&lt;br /&gt; Unlike in pretraining, for SFT, they did only text training, and any vision task is handled via tools. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multimodal RL&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Unlike the SFT, the RL is multimodal, and they designed lots of tasks that explicitly require reasoning over visual content to force the model to improve on vision.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Agent Swarm RL&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is the key highlight for me, they really trained this to be a multi agent orchestrator. During the RL training, the model is given tools to spin up and manage sub agents. The sub agents themselves have fixed weights, their trajectories are not included in training, so effectively on the orchestrators actions are trained, while rewards are obtained from the result of the work of the sub-agents, effectively treating the subagents as parts of the environment. &lt;/p&gt; &lt;p&gt;The data for the RL training is constructed to include tasks that are best executed in parallel rather than explicitly prompting the model to do tasks in parallel.&lt;/p&gt; &lt;p&gt;You can read more on the technical report. &lt;a href="https://arxiv.org/abs/2602.02276"&gt;https://arxiv.org/abs/2602.02276&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cold_Discussion_9570"&gt; /u/Cold_Discussion_9570 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv7lo6/insights_from_kimi_k25_report/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv7lo6/insights_from_kimi_k25_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qv7lo6/insights_from_kimi_k25_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T23:13:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qva5gk</id>
    <title>How to get more tok/s?</title>
    <updated>2026-02-04T00:59:54+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qva5gk/how_to_get_more_toks/"&gt; &lt;img alt="How to get more tok/s?" src="https://external-preview.redd.it/ZnpvY2wyN3BtZGhnMX3C4bhSrcOBtwpO2ghilluKqvqoK5kABDx37kIjqzIp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fd51fdcf7f7597f9ade8d8e5db8eb03b2cb2d80" title="How to get more tok/s?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not OC! [Source](&lt;a href="https://x.com/climate%5C_ben/status/2000636466117193866?s=61"&gt;https://x.com/climate\_ben/status/2000636466117193866?s=61&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/l8lk0xapmdhg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qva5gk/how_to_get_more_toks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qva5gk/how_to_get_more_toks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T00:59:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv0p7u</id>
    <title>MiniCPM-o-4_5 : Full duplex, multimodal with vision and speech at ONLY 9B PARAMETERS??</title>
    <updated>2026-02-03T18:55:33+00:00</updated>
    <author>
      <name>/u/Uncle___Marty</name>
      <uri>https://old.reddit.com/user/Uncle___Marty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-4_5"&gt;https://huggingface.co/openbmb/MiniCPM-o-4_5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/OpenBMB/MiniCPM-o"&gt;https://github.com/OpenBMB/MiniCPM-o&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Couldnt find an existing post for this and was surprised, so heres a post about this. Or something. This seems pretty amazing!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uncle___Marty"&gt; /u/Uncle___Marty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv0p7u/minicpmo4_5_full_duplex_multimodal_with_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv0p7u/minicpmo4_5_full_duplex_multimodal_with_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qv0p7u/minicpmo4_5_full_duplex_multimodal_with_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T18:55:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv5d1k</id>
    <title>Qwen3-Coder Tech Report: tool call generalization, reward hacking, general knowledge</title>
    <updated>2026-02-03T21:47:26+00:00</updated>
    <author>
      <name>/u/Pristine-Woodpecker</name>
      <uri>https://old.reddit.com/user/Pristine-Woodpecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv5d1k/qwen3coder_tech_report_tool_call_generalization/"&gt; &lt;img alt="Qwen3-Coder Tech Report: tool call generalization, reward hacking, general knowledge" src="https://external-preview.redd.it/3bIaBnDXu08CXhELxk4__N-qsOVuqLC1ZUdzCxFB0Fo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=00daa4c0505c069dbac679c0b3ae6151aa6f7543" title="Qwen3-Coder Tech Report: tool call generalization, reward hacking, general knowledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Qwen3-Coder tech report is super interesting on a number of items:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;They specifically tested on various tool chat templates to make sure the model stays flexible no matter where you use it. From their own data, only DeepSeek-v3.2 is close - even a bit better - (which suggests they do the same) and they're both quite a bit ahead of other models.&lt;/li&gt; &lt;li&gt;As the model gets smarter and smarter, it gets better and better at finding loopholes in the test environment to find the solution by cheating (&lt;a href="https://github.com/SWE-bench/SWE-bench/pull/471"&gt;https://github.com/SWE-bench/SWE-bench/pull/471&lt;/a&gt;), which they have to combat.&lt;/li&gt; &lt;li&gt;They trained several specialized submodels (UI dev, webdev, software engineering, ...) and the final model is a distillation of those.&lt;/li&gt; &lt;li&gt;It's similar in performance to the base (non-Coder) model on general benchmarks, and quite a bit better at math.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pristine-Woodpecker"&gt; /u/Pristine-Woodpecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/QwenLM/Qwen3-Coder/blob/main/qwen3_coder_next_tech_report.pdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv5d1k/qwen3coder_tech_report_tool_call_generalization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qv5d1k/qwen3coder_tech_report_tool_call_generalization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T21:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv65ed</id>
    <title>Got Qwen-Coder-Next running on ROCm on my Strix Halo!</title>
    <updated>2026-02-03T22:17:18+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv65ed/got_qwencodernext_running_on_rocm_on_my_strix_halo/"&gt; &lt;img alt="Got Qwen-Coder-Next running on ROCm on my Strix Halo!" src="https://external-preview.redd.it/dzdscnFjbDZ0Y2hnMarG5pOoEfpz9JksRMChe8rZdrijqwmTF4wbigP7RjX-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f8e501a14c67b4224883973e411e9b24a9f6bcf8" title="Got Qwen-Coder-Next running on ROCm on my Strix Halo!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thrilled to see the new model, 80B with 3B active seems perfect for Strix Halo. Video is running on &lt;a href="https://github.com/lemonade-sdk/llamacpp-rocm/releases/tag/b1170"&gt;llamacpp-rocm b1170&lt;/a&gt; with context size 16k and &lt;code&gt;--flash-attn on --no-mmap&lt;/code&gt;. Let me know what you want me to try and I'll run it later tonight!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hnso57l6tchg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv65ed/got_qwencodernext_running_on_rocm_on_my_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qv65ed/got_qwencodernext_running_on_rocm_on_my_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T22:17:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1quvvtv</id>
    <title>Qwen3-Coder-Next</title>
    <updated>2026-02-03T16:03:56+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quvvtv/qwen3codernext/"&gt; &lt;img alt="Qwen3-Coder-Next" src="https://external-preview.redd.it/Mexo_PE5lQQ6UgBLTSrZljbCfScpUvytIcHjhp81XG4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae35460e62424e898f9d6136fab1921d2029ad86" title="Qwen3-Coder-Next" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-Coder-Next is out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-Next"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quvvtv/qwen3codernext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quvvtv/qwen3codernext/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T16:03:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1quxtkj</id>
    <title>The open-source version of Suno is finally here: ACE-Step 1.5</title>
    <updated>2026-02-03T17:13:53+00:00</updated>
    <author>
      <name>/u/AppropriateGuava6262</name>
      <uri>https://old.reddit.com/user/AppropriateGuava6262</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quxtkj/the_opensource_version_of_suno_is_finally_here/"&gt; &lt;img alt="The open-source version of Suno is finally here: ACE-Step 1.5" src="https://b.thumbs.redditmedia.com/Wc9W0Y1pM9FgVsLGL_nSRSARq7eVx7wAjk_hkOIqGfE.jpg" title="The open-source version of Suno is finally here: ACE-Step 1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ACE-Step 1.5 is an open-source music model that can generate a full song in about 2 seconds on an A100, runs locally on a typical PC (around 4GB VRAM), and beats Suno on common evaluation scores.&lt;/p&gt; &lt;p&gt;Key traits of ACE-Step 1.5:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Quality: beats Suno on common eval scores&lt;/li&gt; &lt;li&gt;Speed: full song under 2s on A100&lt;/li&gt; &lt;li&gt;Local: ~4GB VRAM, under 10s on RTX 3090&lt;/li&gt; &lt;li&gt;LoRA: train your own style with a few songs&lt;/li&gt; &lt;li&gt;License: MIT, free for commercial use&lt;/li&gt; &lt;li&gt;Data: fully authorized plus synthetic&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/ace-step/ACE-Step-1.5"&gt;https://github.com/ace-step/ACE-Step-1.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weights/Training code/LoRA code/Paper are all open.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppropriateGuava6262"&gt; /u/AppropriateGuava6262 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1quxtkj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quxtkj/the_opensource_version_of_suno_is_finally_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quxtkj/the_opensource_version_of_suno_is_finally_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T17:13:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1quzwjf</id>
    <title>ACE-Step-1.5 has just been released. It‚Äôs an MIT-licensed open source audio generative model with performance close to commercial platforms like Suno</title>
    <updated>2026-02-03T18:26:58+00:00</updated>
    <author>
      <name>/u/iGermanProd</name>
      <uri>https://old.reddit.com/user/iGermanProd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quzwjf/acestep15_has_just_been_released_its_an/"&gt; &lt;img alt="ACE-Step-1.5 has just been released. It‚Äôs an MIT-licensed open source audio generative model with performance close to commercial platforms like Suno" src="https://external-preview.redd.it/ZDNiNm9lcXduYmhnMXNUFTz1lD2uwrlR8i5n8_uV8Hgq6zjqVqa04fhxxOUs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b143554ca8c67bc465c8e39d15ee68486eaeef36" title="ACE-Step-1.5 has just been released. It‚Äôs an MIT-licensed open source audio generative model with performance close to commercial platforms like Suno" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://xcancel.com/acemusicAI/status/2018731205546684678"&gt;https://xcancel.com/acemusicAI/status/2018731205546684678&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ace-step.github.io/ace-step-v1.5.github.io/"&gt;https://ace-step.github.io/ace-step-v1.5.github.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It‚Äôs already supported in Comfy. MIT license. HuggingFace Demo is also available! Pretty much the whole package - LoRAs are supported, multiple different models to tailor to different needs, cover and repainting features. This is the closest open-source has gotten to Suno and similar top-slop platforms. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iGermanProd"&gt; /u/iGermanProd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r7v6v6qwnbhg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quzwjf/acestep15_has_just_been_released_its_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quzwjf/acestep15_has_just_been_released_its_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T18:26:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1quvqs9</id>
    <title>Qwen/Qwen3-Coder-Next ¬∑ Hugging Face</title>
    <updated>2026-02-03T15:58:52+00:00</updated>
    <author>
      <name>/u/coder543</name>
      <uri>https://old.reddit.com/user/coder543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quvqs9/qwenqwen3codernext_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-Coder-Next ¬∑ Hugging Face" src="https://external-preview.redd.it/Mexo_PE5lQQ6UgBLTSrZljbCfScpUvytIcHjhp81XG4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae35460e62424e898f9d6136fab1921d2029ad86" title="Qwen/Qwen3-Coder-Next ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coder543"&gt; /u/coder543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-Next"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quvqs9/qwenqwen3codernext_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quvqs9/qwenqwen3codernext_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T15:58:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
