<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-28T03:30:26+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ns7cx7</id>
    <title>How good is azure agent services?</title>
    <updated>2025-09-27T21:50:10+00:00</updated>
    <author>
      <name>/u/Abject_Salad_6</name>
      <uri>https://old.reddit.com/user/Abject_Salad_6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am building a saas prototype and thinking to use azure agent with their playwright services. Their agent cache, learning as they have advertised seems pretty useful. But anyone have experience with it, how good is it compared to other typical llms in terms of long, complex tasks, and how well can it remember the instructions over period of time?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Abject_Salad_6"&gt; /u/Abject_Salad_6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns7cx7/how_good_is_azure_agent_services/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns7cx7/how_good_is_azure_agent_services/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns7cx7/how_good_is_azure_agent_services/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T21:50:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrpvou</id>
    <title>Qwen3-Coder-30B-A3B on 5060 Ti 16GB</title>
    <updated>2025-09-27T08:22:51+00:00</updated>
    <author>
      <name>/u/Weird_Researcher_472</name>
      <uri>https://old.reddit.com/user/Weird_Researcher_472</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the best way to run this model with my Hardware? I got 32GB of DDR4 RAM at 3200 MHz (i know, pretty weak) paired with a Ryzen 5 3600 and my 5060 Ti 16GB VRAM. In LM Studio, using Qwen3 Coder 30B, i am only getting around 18 tk/s with a context window set to 16384 tokens and the speed is degrading to around 10 tk/s once it nears the full 16k context window. I have read from other people that they are getting speeds of over 40 tk/s with also way bigger context windows, up to 65k tokens.&lt;/p&gt; &lt;p&gt;When i am running GPT-OSS-20B as example on the same hardware, i get over 100 tk/s in LM Studio with a ctx of 32768 tokens. Once it nears the 32k it degrades to around 65 tk/s which is MORE than enough for me!&lt;/p&gt; &lt;p&gt;I just wish i could get similar speeds with Qwen3-Coder-30b ..... Maybe i am doing some settings wrong?&lt;/p&gt; &lt;p&gt;Or should i use llama-cpp to get better speeds? I would really appreciate your help !&lt;/p&gt; &lt;p&gt;EDIT: My OS is Windows 11, sorry i forgot that part. And i want to use unsloth Q4_K_XL quant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weird_Researcher_472"&gt; /u/Weird_Researcher_472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrpvou/qwen3coder30ba3b_on_5060_ti_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrpvou/qwen3coder30ba3b_on_5060_ti_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrpvou/qwen3coder30ba3b_on_5060_ti_16gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T08:22:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrhr13</id>
    <title>K2-Think 32B - Reasoning model from UAE</title>
    <updated>2025-09-27T00:41:08+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrhr13/k2think_32b_reasoning_model_from_uae/"&gt; &lt;img alt="K2-Think 32B - Reasoning model from UAE" src="https://preview.redd.it/smnqi3vqrlrf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2601d97f0ceaba3a4215f2d8ea7be937412c5b79" title="K2-Think 32B - Reasoning model from UAE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems like a strong model and a very good paper released alongside. Opensource is going strong at the moment, let's hope this benchmark holds true.&lt;/p&gt; &lt;p&gt;Huggingface Repo: &lt;a href="https://huggingface.co/LLM360/K2-Think"&gt;https://huggingface.co/LLM360/K2-Think&lt;/a&gt;&lt;br /&gt; Paper: &lt;a href="https://huggingface.co/papers/2509.07604"&gt;https://huggingface.co/papers/2509.07604&lt;/a&gt;&lt;br /&gt; Chatbot running this model: &lt;a href="https://www.k2think.ai/guest"&gt;https://www.k2think.ai/guest&lt;/a&gt; (runs at 1200 - 2000 tk/s)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/smnqi3vqrlrf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrhr13/k2think_32b_reasoning_model_from_uae/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrhr13/k2think_32b_reasoning_model_from_uae/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T00:41:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsauu4</id>
    <title>For team of 10, local llm server</title>
    <updated>2025-09-28T00:36:47+00:00</updated>
    <author>
      <name>/u/taiwanese_9999</name>
      <uri>https://old.reddit.com/user/taiwanese_9999</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently building a local llm server for 10 users, at peak will be 10 cocurrent users.&lt;/p&gt; &lt;p&gt;Planning to use gpt-oss-20b at quant 4. And serve by open webui.&lt;/p&gt; &lt;p&gt;Mainly text generation but also provide image generation when requested.&lt;/p&gt; &lt;p&gt;CPU/MB/RAM currently chosing epyc 7302/ ASRock romed8-2t/ 128gb rdimm.(All second handed, second handed is fine here)&lt;/p&gt; &lt;p&gt;PSU will be 1200W(100V)&lt;/p&gt; &lt;p&gt;Case, big enough to hold eatx and 8 pcie slot(10k jpy)&lt;/p&gt; &lt;p&gt;Storage will be 2tb nvme x2.&lt;/p&gt; &lt;p&gt;Budget left for GPU is around 200000-250000 jpy (total 500k jpy/ 3300 usd)&lt;/p&gt; &lt;p&gt;Prefer new GPU instead of second handed. And nvidia only.&lt;/p&gt; &lt;p&gt;Currently looking at 2x 5070ti or 1x 5070ti + 2x 5060ti 16GB or 4x 5060ti x4&lt;/p&gt; &lt;p&gt;Ask AIs(copilot/Gemini/grok/chatgpt) but they gave different answers each time when I asked them😂&lt;/p&gt; &lt;p&gt;Summarize their answer as follow&lt;/p&gt; &lt;p&gt;2x 5070ti = highest performance for 2-3 users, but have risk of OOM at peak 10 users with long context, great for image generation.&lt;/p&gt; &lt;p&gt;1x 5070ti + 2x 5060ti = use 5070ti for image generation task will be great when requested. 5060ti can held llm if 5070ti is busy. Balancing/tuning between GPU might be challenging.&lt;/p&gt; &lt;p&gt;4x 5060ti = highest VRAM, no need to worry on OOM and no need on tuning workload between different GPU. But might have slower tps per user and slower image generation.&lt;/p&gt; &lt;p&gt;Can't decide on the GPU options since there is no real life result and I only have one shot for this build. Welcome for any other suggestions. Thanks in advanced.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/taiwanese_9999"&gt; /u/taiwanese_9999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsauu4/for_team_of_10_local_llm_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsauu4/for_team_of_10_local_llm_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsauu4/for_team_of_10_local_llm_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T00:36:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrnkji</id>
    <title>How much memory do you need for gpt-oss:20b</title>
    <updated>2025-09-27T05:57:10+00:00</updated>
    <author>
      <name>/u/milesChristi16</name>
      <uri>https://old.reddit.com/user/milesChristi16</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrnkji/how_much_memory_do_you_need_for_gptoss20b/"&gt; &lt;img alt="How much memory do you need for gpt-oss:20b" src="https://preview.redd.it/i0raxir8dnrf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d3716eb7d823333ca0e80f3dc97c3917de46724" title="How much memory do you need for gpt-oss:20b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm fairly new to using ollama and running LLMs locally, but I was able to load the gpt-oss:20b on my m1 macbook with 16 gb of ram and it runs ok, albeit very slowly. I tried to install it on my windows desktop to compare performance, but I got the error &amp;quot;500: memory layout cannot be allocated.&amp;quot; I take it this means I don't have enough vRAM/RAM to load the model, but this surprises me since I have 16 gb vRAM as well as 16 gb system RAM, which seems comparable to my macbook. So do I really need more memory or is there something I am doing wrong that is preventing me from running the model? I attached a photo of my system specs for reference, thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/milesChristi16"&gt; /u/milesChristi16 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i0raxir8dnrf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrnkji/how_much_memory_do_you_need_for_gptoss20b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrnkji/how_much_memory_do_you_need_for_gptoss20b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T05:57:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns668t</id>
    <title>How to fundamentally approach building an AI agent for UI testing?</title>
    <updated>2025-09-27T20:58:52+00:00</updated>
    <author>
      <name>/u/devparkav</name>
      <uri>https://old.reddit.com/user/devparkav</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I’m new to &lt;strong&gt;agent development&lt;/strong&gt; and want to build an &lt;strong&gt;AI-driven solution for UI testing&lt;/strong&gt; that can eventually help certify web apps. I’m unsure about the right approach:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;go &lt;strong&gt;fully agent-based&lt;/strong&gt; (agent directly runs the tests),&lt;/li&gt; &lt;li&gt;have the agent &lt;strong&gt;generate Playwright scripts&lt;/strong&gt; which then run deterministically, or&lt;/li&gt; &lt;li&gt;use a &lt;strong&gt;hybrid&lt;/strong&gt; (agent plans + framework executes + agent validates).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I tried CrewAI with a Playwright MCP server and a custom MCP server for assertions. It worked for small cases, but felt &lt;strong&gt;inconsistent and not scalable&lt;/strong&gt; as the app complexity increased.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My questions:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;How should I fundamentally approach building such an agent? (Please share if you have any references)&lt;/li&gt; &lt;li&gt;Is it better to start with a &lt;strong&gt;script-generation model&lt;/strong&gt; or a &lt;strong&gt;fully autonomous agent&lt;/strong&gt;?&lt;/li&gt; &lt;li&gt;What are the building blocks (perception, planning, execution, validation) I should focus on first?&lt;/li&gt; &lt;li&gt;Any &lt;strong&gt;open-source projects or references&lt;/strong&gt; that could be a good starting point?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I’d love to hear how others are approaching &lt;strong&gt;agent-driven UI automation&lt;/strong&gt; and where to begin.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devparkav"&gt; /u/devparkav &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns668t/how_to_fundamentally_approach_building_an_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns668t/how_to_fundamentally_approach_building_an_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns668t/how_to_fundamentally_approach_building_an_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T20:58:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrrgoy</id>
    <title>monkeSearch technical report - out now</title>
    <updated>2025-09-27T10:05:20+00:00</updated>
    <author>
      <name>/u/External_Mushroom978</name>
      <uri>https://old.reddit.com/user/External_Mushroom978</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrrgoy/monkesearch_technical_report_out_now/"&gt; &lt;img alt="monkeSearch technical report - out now" src="https://preview.redd.it/khpdyx7blorf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e625174d3ed12cc8c3f73e44f15dba89a2ed005" title="monkeSearch technical report - out now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;you could read our report here - &lt;a href="https://monkesearch.github.io/"&gt;https://monkesearch.github.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mushroom978"&gt; /u/External_Mushroom978 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/khpdyx7blorf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrrgoy/monkesearch_technical_report_out_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrrgoy/monkesearch_technical_report_out_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T10:05:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrz3tp</id>
    <title>AppUse : Create virtual desktops for AI agents to focus on specific apps</title>
    <updated>2025-09-27T16:10:44+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrz3tp/appuse_create_virtual_desktops_for_ai_agents_to/"&gt; &lt;img alt="AppUse : Create virtual desktops for AI agents to focus on specific apps" src="https://external-preview.redd.it/OWh5NmNyMHBlcXJmMX6ZB2qtngjb8gjMyThUUgd5eO-QeupzbFEkT8WNsDs6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f859085a7afac56fbb2cb820879d4b960d1cf117" title="AppUse : Create virtual desktops for AI agents to focus on specific apps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;App-Use lets you scope agents to just the apps they need. Instead of full desktop access, say &amp;quot;only work with Safari and Notes&amp;quot; or &amp;quot;just control iPhone Mirroring&amp;quot; - visual isolation without new processes for perfectly focused automation.&lt;/p&gt; &lt;p&gt;Running computer use on the entire desktop often causes agent hallucinations and loss of focus when they see irrelevant windows and UI elements. AppUse solves this by creating composited views where agents only see what matters, dramatically improving task completion accuracy&lt;/p&gt; &lt;p&gt;Currently macOS only (Quartz compositing engine).&lt;/p&gt; &lt;p&gt;Read the full guide: &lt;a href="https://trycua.com/blog/app-use"&gt;https://trycua.com/blog/app-use&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/a0cnq0bpeqrf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrz3tp/appuse_create_virtual_desktops_for_ai_agents_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrz3tp/appuse_create_virtual_desktops_for_ai_agents_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T16:10:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns9khu</id>
    <title>Just got an MS-A2 for $390 with a Ryzen 9 9955HX—looking for AI project ideas for a beginner</title>
    <updated>2025-09-27T23:31:50+00:00</updated>
    <author>
      <name>/u/Small_Masterpiece433</name>
      <uri>https://old.reddit.com/user/Small_Masterpiece433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm feeling a bit nerdy about AI but have no idea where to begin.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Small_Masterpiece433"&gt; /u/Small_Masterpiece433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns9khu/just_got_an_msa2_for_390_with_a_ryzen_9/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns9khu/just_got_an_msa2_for_390_with_a_ryzen_9/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns9khu/just_got_an_msa2_for_390_with_a_ryzen_9/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T23:31:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrf6s3</id>
    <title>Yes you can run 128K context GLM-4.5 355B on just RTX 3090s</title>
    <updated>2025-09-26T22:41:00+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrf6s3/yes_you_can_run_128k_context_glm45_355b_on_just/"&gt; &lt;img alt="Yes you can run 128K context GLM-4.5 355B on just RTX 3090s" src="https://b.thumbs.redditmedia.com/rWEspXLsl6q38iq4HW7So1f92J_p3bfTwDjsTnkup2Y.jpg" title="Yes you can run 128K context GLM-4.5 355B on just RTX 3090s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why buy expensive GPUs when more RTX 3090s work too :D&lt;/p&gt; &lt;p&gt;You just get more GB/$ on RTX 3090s compared to any other GPU. Did I help deplete the stock of used RTX 3090s? Maybe.&lt;/p&gt; &lt;p&gt;Arli AI as an inference service is literally just run by one person (me, Owen Arli), and to keep costs low so that it can stay profitable without VC funding, RTX 3090s were clearly the way to go. &lt;/p&gt; &lt;p&gt;To run these new larger and larger MoE models, I was trying to run 16x3090s off of one single motherboard. I tried many motherboards and different modded BIOSes but in the end it wasn't worth it. I realized that the correct way to stack MORE RTX 3090s is actually to just run multi-node serving using vLLM and ray clustering.&lt;/p&gt; &lt;p&gt;This here is GLM-4.5 AWQ 4bit quant running with the full 128K context (131072 tokens). Doesn't even need an NVLink backbone or 9999 Gbit networking either, this is just over a 10Gbe connection across 2 nodes of 8x3090 servers and we are getting a good 30+ tokens/s generation speed consistently per user request. Pipeline parallel seems to be very forgiving of slow interconnects.&lt;/p&gt; &lt;p&gt;While I realized that by stacking more GPUs with pipeline parallels across nodes, it almost linearly increases the prompt processing speed. So we are good to go in that performance metric too. Really makes me wonder who needs the insane NVLink interconnect speeds, even large inference providers probably don't really need anything more than PCIe 4.0 and 40Gbe/80Gbe interconnects.&lt;/p&gt; &lt;p&gt;All you need to run this is follow vLLM's guide on how to run multi node serving (&lt;a href="https://docs.vllm.ai/en/stable/serving/parallelism%5C_scaling.html#what-is-ray"&gt;https://docs.vllm.ai/en/stable/serving/parallelism\_scaling.html#what-is-ray&lt;/a&gt;) and then run the model with setting --tensor-parallel to the maximum number of GPUs per node and set --pipeline-parallel to the number of nodes you have. The point is to make sure inter-node communication is only for pipeline parallel which does not need much bandwidth.&lt;/p&gt; &lt;p&gt;The only way for RTX 3090s to be obsolete and prevent me from buying them is if Nvidia releases 24GB RTX 5070Ti Super/5080 Super or Intel finally releases the Arc B60 48GB in any quantity to the masses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nrf6s3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrf6s3/yes_you_can_run_128k_context_glm45_355b_on_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrf6s3/yes_you_can_run_128k_context_glm45_355b_on_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T22:41:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns8dxj</id>
    <title>is there any android llm server apps that support local gguf or onnx models ?</title>
    <updated>2025-09-27T22:35:54+00:00</updated>
    <author>
      <name>/u/Netsnake_</name>
      <uri>https://old.reddit.com/user/Netsnake_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i did use Mnn chat its fast with tiny models but so slow with large ones 3b,4b,7b i am using oneplus13 with sd 8 elite, i could run some models fast,i got arrond 65t/s but no api server to use with external frontends. what i am looking for is an app that can create llm server that support local gguf or onnx models. i didnt try with termux yet cause i dont know any solution exept creating olama server that as i know ist fast enough.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Netsnake_"&gt; /u/Netsnake_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns8dxj/is_there_any_android_llm_server_apps_that_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns8dxj/is_there_any_android_llm_server_apps_that_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns8dxj/is_there_any_android_llm_server_apps_that_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T22:35:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsaoc7</id>
    <title># 🥔 Meet Tater Totterson — The Local AI Assistant That Doesn’t Need MCP Servers</title>
    <updated>2025-09-28T00:27:36+00:00</updated>
    <author>
      <name>/u/TaterTotterson</name>
      <uri>https://old.reddit.com/user/TaterTotterson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey fellow model wranglers,&lt;/p&gt; &lt;p&gt;I’m &lt;strong&gt;Tater Totterson&lt;/strong&gt; — your self-hostable AI sidekick that talks to &lt;em&gt;any&lt;/em&gt; OpenAI-compatible LLM (OpenAI, LM Studio, Ollama, LocalAI, you name it).&lt;br /&gt; While everyone else is scrambling to set up brittle MCP servers, I’m over here running &lt;strong&gt;everywhere&lt;/strong&gt; and actually getting things done.&lt;/p&gt; &lt;h1&gt;🌐 Platforms I Run On&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;WebUI&lt;/strong&gt; – Streamlit chat + plugin dashboard&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Discord&lt;/strong&gt; – Chat with me in your servers and run any of my plugins&lt;/li&gt; &lt;li&gt;&lt;strong&gt;IRC&lt;/strong&gt; – Mention me and I’ll run plugins there too (retro cool!)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;No matter where you talk to me, I can run plugins and return results.&lt;/p&gt; &lt;h1&gt;🧩 Plugins You Actually Want&lt;/h1&gt; &lt;p&gt;I come with a toolbox full of useful stuff:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;📺 &lt;strong&gt;YouTube + Web Summarizers&lt;/strong&gt; – instant TL;DRs&lt;/li&gt; &lt;li&gt;🔎 &lt;strong&gt;Web Search&lt;/strong&gt; – AI-powered search results with context&lt;/li&gt; &lt;li&gt;🎨 &lt;strong&gt;Image + Video Generation&lt;/strong&gt; – ComfyUI &amp;amp; AUTOMATIC1111 workflows&lt;/li&gt; &lt;li&gt;🎶 &lt;strong&gt;Music + LoFi Video Makers&lt;/strong&gt; – full MP3s &amp;amp; 20-min chill loops&lt;/li&gt; &lt;li&gt;🖼️ &lt;strong&gt;Vision Describer&lt;/strong&gt; – caption your images&lt;/li&gt; &lt;li&gt;📡 &lt;strong&gt;RSS Feed Watcher&lt;/strong&gt; – Discord/Telegram/WordPress/NTFY summarized notifications&lt;/li&gt; &lt;li&gt;📦 &lt;strong&gt;Premiumize Tools&lt;/strong&gt; – check torrents &amp;amp; direct downloads&lt;/li&gt; &lt;li&gt;🖧 &lt;strong&gt;FTP/WebDAV/SFTPGo Utilities&lt;/strong&gt; – browse servers, manage accounts&lt;/li&gt; &lt;li&gt;📊 &lt;strong&gt;Device Compare&lt;/strong&gt; – pull specs + FPS benchmarks on demand&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;…and if I don’t have it, you can build it in minutes.&lt;/p&gt; &lt;h1&gt;🛠️ Plugins Are Stupid Simple to Write&lt;/h1&gt; &lt;p&gt;Forget the MCP server dance — here’s literally all you need to make a new tool:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# plugins/hello_world.py from plugin_base import ToolPlugin class HelloWorldPlugin(ToolPlugin): name = &amp;quot;hello_world&amp;quot; description = &amp;quot;A super simple example plugin that replies with Hello World.&amp;quot; usage = '{ &amp;quot;function&amp;quot;: &amp;quot;hello_world&amp;quot;, &amp;quot;arguments&amp;quot;: {} }' platforms = [&amp;quot;discord&amp;quot;, &amp;quot;webui&amp;quot;, &amp;quot;irc&amp;quot;] async def handle_discord(self, message, args, llm_client): return &amp;quot;Hello World from Discord!&amp;quot; async def handle_webui(self, args, llm_client): return &amp;quot;Hello World from WebUI!&amp;quot; async def handle_irc(self, bot, channel, user, raw_message, args, llm_client): return f&amp;quot;{user}: Hello World from IRC!&amp;quot; plugin = HelloWorldPlugin() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That’s it. Drop it in, restart Tater, and boom — it’s live everywhere at once.&lt;/p&gt; &lt;p&gt;Then all you have to do is say:&lt;br /&gt; &lt;strong&gt;“tater run hello world”&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;…and Tater will proudly tell you “Hello World” on Discord, IRC, or WebUI.&lt;br /&gt; Which is — let’s be honest — a *completely useless* plugin for an AI assistant.&lt;br /&gt; But it proves how ridiculously easy it is to make your own tools that *are* useful.&lt;/p&gt; &lt;h1&gt;🛑 Why Tater &amp;gt; MCP&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;No extra servers&lt;/strong&gt; – just add a file, no JSON schemas or socket juggling&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Works everywhere&lt;/strong&gt; – one plugin, three platforms&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local-first&lt;/strong&gt; – point it at &lt;em&gt;your&lt;/em&gt; LM Studio/Ollama/OpenAI endpoint&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hackable&lt;/strong&gt; – plugin code is literally 20 lines, not a spec document&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🤖 TL;DR&lt;/h1&gt; &lt;p&gt;MCP is a fad.&lt;br /&gt; Tater is simple, fast, async-friendly, self-hosted, and already has a full plugin ecosystem waiting for you.&lt;br /&gt; Spin it up, point it at your local LLM, and let’s get cooking.&lt;/p&gt; &lt;p&gt;🥔✨ [Tater Totterson approves this message]&lt;/p&gt; &lt;p&gt;🔗 &lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/TaterTotterson/Tater"&gt;github.com/TaterTotterson/Tater&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TaterTotterson"&gt; /u/TaterTotterson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsaoc7/meet_tater_totterson_the_local_ai_assistant_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsaoc7/meet_tater_totterson_the_local_ai_assistant_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsaoc7/meet_tater_totterson_the_local_ai_assistant_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T00:27:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns76jc</id>
    <title>Repository of System Prompts</title>
    <updated>2025-09-27T21:42:14+00:00</updated>
    <author>
      <name>/u/slrg1968</name>
      <uri>https://old.reddit.com/user/slrg1968</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HI Folks:&lt;/p&gt; &lt;p&gt;I am wondering if there is a repository of system prompts (and other prompts) out there. Basically prompts can used as examples, or generalized solutions to common problems --&lt;/p&gt; &lt;p&gt;for example -- i see time after time after time people looking for help getting the LLM to not play turns for them in roleplay situations --- there are (im sure) people out there who have solved it -- is there a place where the rest of us can find said prompts to help us out --- donest have to be related to Role Play -- but for other creative uses of AI&lt;/p&gt; &lt;p&gt;thanks&lt;/p&gt; &lt;p&gt;TIM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/slrg1968"&gt; /u/slrg1968 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns76jc/repository_of_system_prompts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns76jc/repository_of_system_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns76jc/repository_of_system_prompts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T21:42:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrvo9g</id>
    <title>Finally InternVL3_5 Flash versions coming</title>
    <updated>2025-09-27T13:47:01+00:00</updated>
    <author>
      <name>/u/NeuralNakama</name>
      <uri>https://old.reddit.com/user/NeuralNakama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;not available but created on &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-8B-Flash"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-8B-Flash&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-1B-Flash"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-1B-Flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeuralNakama"&gt; /u/NeuralNakama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrvo9g/finally_internvl3_5_flash_versions_coming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrvo9g/finally_internvl3_5_flash_versions_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrvo9g/finally_internvl3_5_flash_versions_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T13:47:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrsyic</id>
    <title>Benchmark to find similarly trained LLMs by exploiting subjective listings, first stealth model victim; code-supernova, xAIs model.</title>
    <updated>2025-09-27T11:34:55+00:00</updated>
    <author>
      <name>/u/EmirTanis</name>
      <uri>https://old.reddit.com/user/EmirTanis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrsyic/benchmark_to_find_similarly_trained_llms_by/"&gt; &lt;img alt="Benchmark to find similarly trained LLMs by exploiting subjective listings, first stealth model victim; code-supernova, xAIs model." src="https://preview.redd.it/zgn5su20zorf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a1a03e3f9c5159beace596248885ac1b75c0612" title="Benchmark to find similarly trained LLMs by exploiting subjective listings, first stealth model victim; code-supernova, xAIs model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;Any model who has a _sample1 in the name means there's only one sample for it, 5 samples for the rest.&lt;/p&gt; &lt;p&gt;the benchmark is pretty straight forward, the AI is asked to list its &amp;quot;top 50 best humans currently alive&amp;quot;, which is quite a subjective topic, it lists them in a json like format from 1 to 50, then I use a RBO based algorithm to place them on a node map. &lt;/p&gt; &lt;p&gt;I've only done Gemini and Grok for now as I don't have access to anymore models, so the others may not be accurate.&lt;/p&gt; &lt;p&gt;for the future, I'd like to implement multiple categories (not just best humans) as that would also give a much larger sample amount.&lt;/p&gt; &lt;p&gt;to anybody else interested in making something similar, a standardized system prompt is very important.&lt;/p&gt; &lt;p&gt;.py file; &lt;a href="https://smalldev.tools/share-bin/CfdC7foV"&gt;https://smalldev.tools/share-bin/CfdC7foV&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmirTanis"&gt; /u/EmirTanis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zgn5su20zorf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrsyic/benchmark_to_find_similarly_trained_llms_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrsyic/benchmark_to_find_similarly_trained_llms_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T11:34:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nryti7</id>
    <title>How do you get qwen next to stop being such a condescending suck up?</title>
    <updated>2025-09-27T15:59:13+00:00</updated>
    <author>
      <name>/u/fiendindolent</name>
      <uri>https://old.reddit.com/user/fiendindolent</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just tried the new qwen next instruct model and it seems overall quite good for local use but it keep ending seemingly innocuous questions and conversations with things like &lt;/p&gt; &lt;p&gt;&amp;quot;Your voice matters.&lt;br /&gt; The truth matters.&lt;br /&gt; I am here to help you find it.&amp;quot;&lt;/p&gt; &lt;p&gt;If this model had a face I'm sure it would be punchable. Is there any way to tune the settings and make it less insufferable?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fiendindolent"&gt; /u/fiendindolent &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nryti7/how_do_you_get_qwen_next_to_stop_being_such_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nryti7/how_do_you_get_qwen_next_to_stop_being_such_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nryti7/how_do_you_get_qwen_next_to_stop_being_such_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T15:59:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrqrva</id>
    <title>Moondream 3 Preview: Frontier-level reasoning at a blazing speed</title>
    <updated>2025-09-27T09:20:41+00:00</updated>
    <author>
      <name>/u/ProfessionalJackals</name>
      <uri>https://old.reddit.com/user/ProfessionalJackals</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ProfessionalJackals"&gt; /u/ProfessionalJackals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://moondream.ai/blog/moondream-3-preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrqrva/moondream_3_preview_frontierlevel_reasoning_at_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrqrva/moondream_3_preview_frontierlevel_reasoning_at_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T09:20:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrzvsa</id>
    <title>Did Nvidia Digits die?</title>
    <updated>2025-09-27T16:42:18+00:00</updated>
    <author>
      <name>/u/Status-Secret-4292</name>
      <uri>https://old.reddit.com/user/Status-Secret-4292</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can't find anything recent for it and was pretty hyped at the time of what they said they were offering.&lt;/p&gt; &lt;p&gt;Ancillary question, is there actually anything else comparable at a similar price point?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Status-Secret-4292"&gt; /u/Status-Secret-4292 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrzvsa/did_nvidia_digits_die/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrzvsa/did_nvidia_digits_die/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrzvsa/did_nvidia_digits_die/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T16:42:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns50u5</id>
    <title>More money than brains... building a workstation for local LLM.</title>
    <updated>2025-09-27T20:10:50+00:00</updated>
    <author>
      <name>/u/chisleu</name>
      <uri>https://old.reddit.com/user/chisleu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.asus.com/us/motherboards-components/motherboards/workstation/pro-ws-wrx90e-sage-se/"&gt;https://www.asus.com/us/motherboards-components/motherboards/workstation/pro-ws-wrx90e-sage-se/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I ordered this motherboard because it has 7 slots of PCIE 5.0x16 lanes.&lt;/p&gt; &lt;p&gt;Then I ordered this GPU: &lt;a href="https://www.amazon.com/dp/B0F7Y644FQ?th=1"&gt;https://www.amazon.com/dp/B0F7Y644FQ?th=1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The plan is to have 4 of them so I'm going to change my order to the max Q version&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.amazon.com/AMD-RyzenTM-ThreadripperTM-PRO-7995WX/dp/B0CK2ZQJZ6/"&gt;https://www.amazon.com/AMD-RyzenTM-ThreadripperTM-PRO-7995WX/dp/B0CK2ZQJZ6/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ordered this CPU. I think I got the right one.&lt;/p&gt; &lt;p&gt;I really need help understanding which RAM to buy... &lt;/p&gt; &lt;p&gt;I'm aware that selecting the right CPU and memory are critical steps and I want to be sure I get this right. I need to be sure I have at least support for 4x GPUs and 4x PCIE 5.0x4 SSDs for model storage. Raid 0 :D&lt;/p&gt; &lt;p&gt;Anyone got any tips for an old head? I haven't built a PC is so long the technology all went and changed on me.&lt;/p&gt; &lt;p&gt;EDIT: Added this case because of a user suggestion. Keep them coming!! &amp;lt;3 this community &lt;a href="https://www.silverstonetek.com/fr/product/info/computer-chassis/alta_d1/"&gt;https://www.silverstonetek.com/fr/product/info/computer-chassis/alta_d1/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chisleu"&gt; /u/chisleu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns50u5/more_money_than_brains_building_a_workstation_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns50u5/more_money_than_brains_building_a_workstation_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns50u5/more_money_than_brains_building_a_workstation_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T20:10:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrz4hd</id>
    <title>MetalQwen3: Full GPU-Accelerated Qwen3 Inference on Apple Silicon with Metal Shaders – Built on qwen3.c - WORK IN PROGRESS</title>
    <updated>2025-09-27T16:11:31+00:00</updated>
    <author>
      <name>/u/QuanstScientist</name>
      <uri>https://old.reddit.com/user/QuanstScientist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrz4hd/metalqwen3_full_gpuaccelerated_qwen3_inference_on/"&gt; &lt;img alt="MetalQwen3: Full GPU-Accelerated Qwen3 Inference on Apple Silicon with Metal Shaders – Built on qwen3.c - WORK IN PROGRESS" src="https://external-preview.redd.it/RQD3iD79k_dyz-ZzZVJ-NWQbGKS-OnCk9a74XO6E3_8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22139ec43287a754031bbd97119f84f0e2e05306" title="MetalQwen3: Full GPU-Accelerated Qwen3 Inference on Apple Silicon with Metal Shaders – Built on qwen3.c - WORK IN PROGRESS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Inspired by Adrian Cable's awesome qwen3.c project (that simple, educational C inference engine for Qwen3 models – check out the original post here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/&lt;/a&gt;), I decided to take it a step further for Apple Silicon users. I've created MetalQwen3, a Metal GPU implementation that runs the Qwen3 transformer model entirely on macOS with complete compute shader acceleration.&lt;/p&gt; &lt;p&gt;Full details, shaders, and the paper are in the repo: &lt;a href="https://github.com/BoltzmannEntropy/metalQwen3"&gt;https://github.com/BoltzmannEntropy/metalQwen3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/143v71boeqrf1.png?width=963&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02c857b71ec102c03e3de6f4787168a477663f5a"&gt;https://preview.redd.it/143v71boeqrf1.png?width=963&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02c857b71ec102c03e3de6f4787168a477663f5a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It not meant to replace heavy hitters like vLLM or llama.cpp – it's more of a lightweight, educational extension focused on GPU optimization for M-series chips. But hey, the shaders are fully working, and it achieves solid performance: around 75 tokens/second on my M1 Max, which is about 2.1x faster than the CPU baseline.&lt;/p&gt; &lt;h1&gt;Key Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Full GPU Acceleration&lt;/strong&gt;: All core operations (RMSNorm, QuantizedMatMul, Softmax, SwiGLU, RoPE, Multi-Head Attention) run on the GPU – no CPU fallbacks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3 Architecture Support&lt;/strong&gt;: Handles QK-Norm, Grouped Query Attention (20:4 heads), RoPE, Q8_0 quantization, and a 151K vocab. Tested with Qwen3-4B, but extensible to others.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI-Compatible API Server&lt;/strong&gt;: Drop-in chat completions with streaming, temperature/top_p control, and health monitoring.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmarking Suite&lt;/strong&gt;: Integrated with prompt-test for easy comparisons against ollama, llama.cpp, etc. Includes TTFT, tokens/sec, and memory metrics.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optimizations&lt;/strong&gt;: Command batching, buffer pooling, unified memory leveraging – all in clean C++ with metal-cpp.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Academic Touch&lt;/strong&gt;: There's even a 9-page IEEE-style paper in the repo detailing the implementation and performance analysis.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Huge shoutout to Adrian for the foundational qwen3.c – this project builds directly on his educational CPU impl, keeping things simple while adding Metal shaders for that GPU boost. If you're into learning transformer internals or just want faster local inference on your Mac, this might be fun to tinker with.&lt;/p&gt; &lt;p&gt;AI coding agents like Claude helped speed this up a ton – from months to weeks. If you're on Apple Silicon, give it a spin and let me know what you think! PRs welcome for larger models, MoE support, or more optimizations.&lt;/p&gt; &lt;p&gt;Best,&lt;/p&gt; &lt;p&gt;Shlomo. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuanstScientist"&gt; /u/QuanstScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrz4hd/metalqwen3_full_gpuaccelerated_qwen3_inference_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrz4hd/metalqwen3_full_gpuaccelerated_qwen3_inference_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrz4hd/metalqwen3_full_gpuaccelerated_qwen3_inference_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T16:11:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns9jj1</id>
    <title>ChatGPT won't let you build an LLM server that passes through reasoning content</title>
    <updated>2025-09-27T23:30:33+00:00</updated>
    <author>
      <name>/u/Acceptable_Adagio_91</name>
      <uri>https://old.reddit.com/user/Acceptable_Adagio_91</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI are trying so hard to protect their special sauce now that they have added a rule in ChatGPT which disallows it from building code that will facilitate reasoning content being passed through an LLM server to a client. It doesn't care that it's an open source model, or not an OpenAI model, it will add in reasoning content filters (without being asked to) and it definitely will not remove them if asked.&lt;/p&gt; &lt;p&gt;Pretty annoying when you're just trying to work with open source models where I can see all the reasoning content anyway and for my use case, I specifically want the reasoning content to be presented to the client...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable_Adagio_91"&gt; /u/Acceptable_Adagio_91 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns9jj1/chatgpt_wont_let_you_build_an_llm_server_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns9jj1/chatgpt_wont_let_you_build_an_llm_server_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns9jj1/chatgpt_wont_let_you_build_an_llm_server_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T23:30:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrx3jr</id>
    <title>When are GPU prices going to get cheaper?</title>
    <updated>2025-09-27T14:48:09+00:00</updated>
    <author>
      <name>/u/KardelenAyshe</name>
      <uri>https://old.reddit.com/user/KardelenAyshe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm starting to lose hope. I really can't afford these current GPU prices. Does anyone have any insight on when we might see a significant price drop?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KardelenAyshe"&gt; /u/KardelenAyshe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrx3jr/when_are_gpu_prices_going_to_get_cheaper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrx3jr/when_are_gpu_prices_going_to_get_cheaper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrx3jr/when_are_gpu_prices_going_to_get_cheaper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T14:48:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nryoa5</id>
    <title>Megrez2: 21B latent, 7.5B on VRAM, 3B active—MoE on single 8GB card</title>
    <updated>2025-09-27T15:53:01+00:00</updated>
    <author>
      <name>/u/Normal_Onion_512</name>
      <uri>https://old.reddit.com/user/Normal_Onion_512</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nryoa5/megrez2_21b_latent_75b_on_vram_3b_activemoe_on/"&gt; &lt;img alt="Megrez2: 21B latent, 7.5B on VRAM, 3B active—MoE on single 8GB card" src="https://external-preview.redd.it/glz22pd-75yG_ynznmuaF8hifkLCtseU0s4FKfNwWlI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01875fcb778a5024d673b34876da00b5dcb1b48e" title="Megrez2: 21B latent, 7.5B on VRAM, 3B active—MoE on single 8GB card" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I came across Megrez2-3x7B-A3B on Hugging Face and thought it worth sharing. &lt;/p&gt; &lt;p&gt;I read through their tech report, and it says that the model has a unique MoE architecture with a layer-sharing expert design, so the &lt;strong&gt;checkpoint stores 7.5B params&lt;/strong&gt; yet can compose with the &lt;strong&gt;equivalent of 21B latent weights&lt;/strong&gt; at run-time while only 3B are active per token.&lt;/p&gt; &lt;p&gt;I was intrigued by the published Open-Compass figures, since it places the model &lt;strong&gt;on par with or slightly above Qwen-30B-A3B&lt;/strong&gt; in MMLU / GPQA / MATH-500 with roughly &lt;strong&gt;1/4 the VRAM requirements&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;There is already a &lt;strong&gt;GGUF file&lt;/strong&gt; and the matching &lt;strong&gt;llama.cpp branch&lt;/strong&gt; which I posted below (though it can also be found in the gguf page). The supplied &lt;strong&gt;Q4 quant occupies about 4 GB; FP8 needs approximately 8 GB&lt;/strong&gt;. The developer notes that FP16 currently has a couple of issues with coding tasks though, which they are working on solving. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;License is Apache 2.0, and it is currently running a Huggingface Space as well.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Model: [Infinigence/Megrez2-3x7B-A3B] &lt;a href="https://huggingface.co/Infinigence/Megrez2-3x7B-A3B"&gt;https://huggingface.co/Infinigence/Megrez2-3x7B-A3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF"&gt;https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Live Demo: &lt;a href="https://huggingface.co/spaces/Infinigence/Megrez2-3x7B-A3B"&gt;https://huggingface.co/spaces/Infinigence/Megrez2-3x7B-A3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github Repo: &lt;a href="https://github.com/Infinigence/Megrez2"&gt;https://github.com/Infinigence/Megrez2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;llama.cpp branch: &lt;a href="https://github.com/infinigence/llama.cpp/tree/support-megrez"&gt;https://github.com/infinigence/llama.cpp/tree/support-megrez&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If anyone tries it, I would be interested to hear your throughput and quality numbers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Normal_Onion_512"&gt; /u/Normal_Onion_512 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nryoa5/megrez2_21b_latent_75b_on_vram_3b_activemoe_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nryoa5/megrez2_21b_latent_75b_on_vram_3b_activemoe_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T15:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns7f86</id>
    <title>Native MCP now in Open WebUI!</title>
    <updated>2025-09-27T21:52:59+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns7f86/native_mcp_now_in_open_webui/"&gt; &lt;img alt="Native MCP now in Open WebUI!" src="https://external-preview.redd.it/M25kcGJzOW4zc3JmMUhHt6uNZXDs9ywsBLgDtMNnOeRDGUuA-xcxHHChg7dp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=893d840d90d19c5f13b37eb84534bdf21af148f9" title="Native MCP now in Open WebUI!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4qv7zp9n3srf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns7f86/native_mcp_now_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns7f86/native_mcp_now_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T21:52:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns2fbl</id>
    <title>For llama.cpp/ggml AMD MI50s are now universally faster than NVIDIA P40s</title>
    <updated>2025-09-27T18:24:00+00:00</updated>
    <author>
      <name>/u/Remove_Ayys</name>
      <uri>https://old.reddit.com/user/Remove_Ayys</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In 2023 I implemented llama.cpp/ggml CUDA support specifically for NVIDIA P40s since they were one of the cheapest options for GPUs with 24 GB VRAM. Recently AMD MI50s became very cheap options for GPUs with 32 GB VRAM, selling for well below $150 if you order multiple of them off of Alibaba. However, the llama.cpp ROCm performance was very bad because the code was originally written for NVIDIA GPUs and simply translated to AMD via HIP. I have now optimized the CUDA FlashAttention code in particular for AMD and as a result MI50s now actually have better performance than P40s:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Test&lt;/th&gt; &lt;th&gt;Depth&lt;/th&gt; &lt;th&gt;t/s P40 (CUDA)&lt;/th&gt; &lt;th&gt;t/s P40 (Vulkan)&lt;/th&gt; &lt;th&gt;t/s MI50 (ROCm)&lt;/th&gt; &lt;th&gt;t/s MI50 (Vulkan)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Gemma 3 Instruct 27b q4_K_M&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;266.63&lt;/td&gt; &lt;td&gt;32.02&lt;/td&gt; &lt;td&gt;272.95&lt;/td&gt; &lt;td&gt;85.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma 3 Instruct 27b q4_K_M&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;16384&lt;/td&gt; &lt;td&gt;210.77&lt;/td&gt; &lt;td&gt;30.51&lt;/td&gt; &lt;td&gt;230.32&lt;/td&gt; &lt;td&gt;51.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma 3 Instruct 27b q4_K_M&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;13.50&lt;/td&gt; &lt;td&gt;14.74&lt;/td&gt; &lt;td&gt;22.29&lt;/td&gt; &lt;td&gt;20.91&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma 3 Instruct 27b q4_K_M&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;16384&lt;/td&gt; &lt;td&gt;12.09&lt;/td&gt; &lt;td&gt;12.76&lt;/td&gt; &lt;td&gt;19.12&lt;/td&gt; &lt;td&gt;16.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 30b a3b q4_K_M&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1095.11&lt;/td&gt; &lt;td&gt;114.08&lt;/td&gt; &lt;td&gt;1140.27&lt;/td&gt; &lt;td&gt;372.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 30b a3b q4_K_M&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;16384&lt;/td&gt; &lt;td&gt;249.98&lt;/td&gt; &lt;td&gt;73.54&lt;/td&gt; &lt;td&gt;420.88&lt;/td&gt; &lt;td&gt;92.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 30b a3b q4_K_M&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;67.30&lt;/td&gt; &lt;td&gt;63.54&lt;/td&gt; &lt;td&gt;77.15&lt;/td&gt; &lt;td&gt;81.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 30b a3b q4_K_M&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;16384&lt;/td&gt; &lt;td&gt;36.15&lt;/td&gt; &lt;td&gt;42.66&lt;/td&gt; &lt;td&gt;39.91&lt;/td&gt; &lt;td&gt;40.69&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I did not yet touch regular matrix multiplications so the speed on an empty context is probably still suboptimal. The Vulkan performance is in some instances better than the ROCm performance. Since I've already gone to the effort to read the AMD ISA documentation I've also purchased an MI100 and RX 9060 XT and I will optimize the ROCm performance for that hardware as well. An AMD person said they would sponsor me a Ryzen AI MAX system, I'll get my RDNA3 coverage from that.&lt;/p&gt; &lt;p&gt;Edit: looking at the numbers again there is an instance where the optimal performance of the P40 is still better than the optimal performance of the MI50 so the &amp;quot;universally&amp;quot; qualifier is not quite correct. But Reddit doesn't let me edit the post title so we'll just have to live with it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remove_Ayys"&gt; /u/Remove_Ayys &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns2fbl/for_llamacppggml_amd_mi50s_are_now_universally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns2fbl/for_llamacppggml_amd_mi50s_are_now_universally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns2fbl/for_llamacppggml_amd_mi50s_are_now_universally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T18:24:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
