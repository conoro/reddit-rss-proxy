<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-15T20:06:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oxtw3p</id>
    <title>Local model for creative writing with MCP.</title>
    <updated>2025-11-15T15:03:39+00:00</updated>
    <author>
      <name>/u/Elsuvio</name>
      <uri>https://old.reddit.com/user/Elsuvio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I use LLM models (mainly proprietary Claude) for many things, but recently I started using it to brainstorm ideas for my DnD campaign. I usually come up with ideas that I would like to develop and discuss them with LLM. Usually, the model refines or supplements my idea, I make some changes to it, and when I'm satisfied, I ask it to save the idea in Obsidian in a specific note. This works quite well - I have a custom MCP configuration that allows Claude to access my Obsidian notes, but the problem is that it uses up my daily/weekly limits quite quickly, even though I try to limit the context I give it. I was wondering if there is anything in terms of open source models that I could self-host on my RTX 5080 with 16 GB VRAM (+32 GB RAM, if that matters) that could leverage my simple MCP and I wouldn't have to worry so much about limits anymore? &lt;/p&gt; &lt;p&gt;I would appreciate any information if there are models that would fit my use case or a place where I could find them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Elsuvio"&gt; /u/Elsuvio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxtw3p/local_model_for_creative_writing_with_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxtw3p/local_model_for_creative_writing_with_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxtw3p/local_model_for_creative_writing_with_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T15:03:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy1b26</id>
    <title>A RAG Boilerplate with Extensive Documentation</title>
    <updated>2025-11-15T19:59:00+00:00</updated>
    <author>
      <name>/u/mburaksayici</name>
      <uri>https://old.reddit.com/user/mburaksayici</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1b26/a_rag_boilerplate_with_extensive_documentation/"&gt; &lt;img alt="A RAG Boilerplate with Extensive Documentation" src="https://preview.redd.it/zo99x15z7h1g1.gif?width=640&amp;amp;crop=smart&amp;amp;s=aa347c69b94931a0116e9198614b94746d283ddc" title="A RAG Boilerplate with Extensive Documentation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I open-sourced the RAG boilerplate I’ve been using for my own experiments with extensive docs on system design.&lt;/p&gt; &lt;p&gt;It's mostly for educational purposes, but why not make it bigger later on?&lt;br /&gt; Repo: &lt;a href="https://github.com/mburaksayici/RAG-Boilerplate"&gt;https://github.com/mburaksayici/RAG-Boilerplate&lt;/a&gt;&lt;br /&gt; - Includes propositional + semantic and recursive overlap chunking, hybrid search on Qdrant (BM25 + dense), and optional LLM reranking.&lt;br /&gt; - Uses E5 embeddings as the default model for vector representations.&lt;br /&gt; - Has a query-enhancer agent built with CrewAI and a Celery-based ingestion flow for document processing.&lt;br /&gt; - Uses Redis (hot) + MongoDB (cold) for session handling and restoration.&lt;br /&gt; - Runs on FastAPI with a small Gradio UI to test retrieval and chat with the data.&lt;br /&gt; - Stack: FastAPI, Qdrant, Redis, MongoDB, Celery, CrewAI, Gradio, HuggingFace models, OpenAI.&lt;br /&gt; Blog : &lt;a href="https://mburaksayici.com/blog/2025/11/13/a-rag-boilerplate.html"&gt;https://mburaksayici.com/blog/2025/11/13/a-rag-boilerplate.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mburaksayici"&gt; /u/mburaksayici &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zo99x15z7h1g1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1b26/a_rag_boilerplate_with_extensive_documentation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1b26/a_rag_boilerplate_with_extensive_documentation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T19:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy1bv3</id>
    <title>Looking for feedback - I built Socratic, a knowledge-base builder where YOU stay in control</title>
    <updated>2025-11-15T19:59:54+00:00</updated>
    <author>
      <name>/u/Unable-Living-3506</name>
      <uri>https://old.reddit.com/user/Unable-Living-3506</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’ve been working on an open-source project and would love your feedback. Not selling anything - just trying to see whether it solves a real problem.&lt;/p&gt; &lt;p&gt;Most agent knowledge base tools today are &amp;quot;document dumps&amp;quot;: throw everything into RAG and hope the agent picks the right info. If the agent gets confused or misinterprets sth? Too bad ¯\_(ツ)_/¯ you’re at the mercy of retrieval.&lt;/p&gt; &lt;p&gt;Socratic flips this: the expert should stay in control of the knowledge, not the vector index.&lt;/p&gt; &lt;p&gt;To do this, you collaborate with the Socratic agent to construct your knowledge base, like teaching a junior person how your system works. The result is a curated, explicit knowledge base you actually trust.&lt;/p&gt; &lt;p&gt;If you have a few minutes, I'm genuine wondering: is this a real problem for you? If so, does the solution sound useful?&lt;/p&gt; &lt;p&gt;I’m genuinely curious what others building agents think about the problem and direction. Any feedback is appreciated!&lt;/p&gt; &lt;p&gt;3-min demo: &lt;a href="https://www.youtube.com/watch?v=R4YpbqQZlpU"&gt;https://www.youtube.com/watch?v=R4YpbqQZlpU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/kevins981/Socratic"&gt;https://github.com/kevins981/Socratic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unable-Living-3506"&gt; /u/Unable-Living-3506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1bv3/looking_for_feedback_i_built_socratic_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1bv3/looking_for_feedback_i_built_socratic_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1bv3/looking_for_feedback_i_built_socratic_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T19:59:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxqmhc</id>
    <title>Understanding vLLM internals</title>
    <updated>2025-11-15T12:35:07+00:00</updated>
    <author>
      <name>/u/Majestic_Two_8940</name>
      <uri>https://old.reddit.com/user/Majestic_Two_8940</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I want to understand how vLLM works so that I can create plugins. What are some of the good resources to learn VLLM under the hood? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Majestic_Two_8940"&gt; /u/Majestic_Two_8940 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxqmhc/understanding_vllm_internals/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxqmhc/understanding_vllm_internals/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxqmhc/understanding_vllm_internals/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T12:35:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ox3e1f</id>
    <title>Drummer's Precog 24B and 123B v1 - AI that writes a short draft before responding</title>
    <updated>2025-11-14T17:50:02+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox3e1f/drummers_precog_24b_and_123b_v1_ai_that_writes_a/"&gt; &lt;img alt="Drummer's Precog 24B and 123B v1 - AI that writes a short draft before responding" src="https://external-preview.redd.it/vl2ei1-FehJR-7jZHQXuFZ_Y0kemf2CP216W8qh6VxE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2798025eaf994dc8b7c090c13aa6bdefb4507a02" title="Drummer's Precog 24B and 123B v1 - AI that writes a short draft before responding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! &lt;/p&gt; &lt;p&gt;I wanted to explore a different way of thinking where the AI uses the &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; block to plan ahead and create a short draft so that its &lt;em&gt;actual&lt;/em&gt; response has &lt;strong&gt;basis&lt;/strong&gt;. It seems like a good way to have the AI pan out its start, middle, and end before writing the entire thing. Kind of like a synopsis or abstract. &lt;/p&gt; &lt;p&gt;I'm hoping it could strengthen consistency and flow since the AI doesn't have to &lt;em&gt;wing it&lt;/em&gt; and write a thousand tokens from the get-go. It's a cheaper, more effective alternative to reasoning, especially when it comes to story / RP. You can also make adjustments to the draft to steer it a certain way. Testers have been happy with it.&lt;/p&gt; &lt;p&gt;24B: &lt;a href="https://huggingface.co/TheDrummer/Precog-24B-v1"&gt;https://huggingface.co/TheDrummer/Precog-24B-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;123B: &lt;a href="https://huggingface.co/TheDrummer/Precog-123B-v1"&gt;https://huggingface.co/TheDrummer/Precog-123B-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Examples:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1li2viecf91g1.png?width=2264&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af225606b23751beaf3076b1a58140b1c77b1a4f"&gt;https://preview.redd.it/1li2viecf91g1.png?width=2264&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af225606b23751beaf3076b1a58140b1c77b1a4f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7iu4m7zcf91g1.png?width=887&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4de7655654340ec91216d8a61c93c474571b1dc0"&gt;https://preview.redd.it/7iu4m7zcf91g1.png?width=887&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4de7655654340ec91216d8a61c93c474571b1dc0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3qo833ndf91g1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0cac98a5e93dd87baa885bda58574385b8e73c11"&gt;https://preview.redd.it/3qo833ndf91g1.png?width=1010&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0cac98a5e93dd87baa885bda58574385b8e73c11&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox3e1f/drummers_precog_24b_and_123b_v1_ai_that_writes_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ox3e1f/drummers_precog_24b_and_123b_v1_ai_that_writes_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ox3e1f/drummers_precog_24b_and_123b_v1_ai_that_writes_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T17:50:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxsz7e</id>
    <title>What do you think about Cerebras REAP models?</title>
    <updated>2025-11-15T14:24:20+00:00</updated>
    <author>
      <name>/u/Swimming-Ratio4879</name>
      <uri>https://old.reddit.com/user/Swimming-Ratio4879</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cerebras launched a few REAP models on huggingface,what do you think about them ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Swimming-Ratio4879"&gt; /u/Swimming-Ratio4879 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxsz7e/what_do_you_think_about_cerebras_reap_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxsz7e/what_do_you_think_about_cerebras_reap_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxsz7e/what_do_you_think_about_cerebras_reap_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T14:24:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxeif6</id>
    <title>Premise: MoE models have exploitable locality in expert activation patterns, and LRU caching with profiling could cut VRAM requirements in half.</title>
    <updated>2025-11-15T01:16:49+00:00</updated>
    <author>
      <name>/u/CodeSlave9000</name>
      <uri>https://old.reddit.com/user/CodeSlave9000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently doing some brainstorming - and a few back-of-the-page calculations, and came up with this. The premise is that with some profiling based on actual user workload, we should be able to determine expert activation patterns and locality for caching. TLDR; A &amp;quot;smart&amp;quot; MOE caching size could reduce VRAM needs by up to half. I'm sure I'm not the first to think about this, and I'm sure I've got a screw loose, but maybe someone can set me straight.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MoE models have exploitable locality in expert activation patterns, and LRU caching with profiling could cut VRAM requirements in half&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Meaning, that:&lt;/p&gt; &lt;p&gt;Total VRAM budget: X&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Expert size: &lt;strong&gt;E&lt;/strong&gt; (some fraction of total model Y)&lt;/li&gt; &lt;li&gt;Can fit in cache: &lt;strong&gt;C = X / E&lt;/strong&gt; experts&lt;/li&gt; &lt;li&gt;Experts activated per token across all layers: &lt;strong&gt;A&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;LRU cache hit rate: &lt;strong&gt;H&lt;/strong&gt; (empirically ~70-80% with temporal locality)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Cost Model&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Without swapping&lt;/strong&gt;: Need all experts in VRAM = can't run the model if total experts &amp;gt; X&lt;/p&gt; &lt;p&gt;&lt;strong&gt;With swapping&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cache hits: free (already in VRAM)&lt;/li&gt; &lt;li&gt;Cache misses: pay PCIe transfer cost&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Per-token cost&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Expert activations needed: A&lt;/li&gt; &lt;li&gt;Cache hits: A × H (free)&lt;/li&gt; &lt;li&gt;Cache misses: A × (1 - H) × transfer_cost&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Transfer cost&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PCIe bandwidth: ~25 GB/s practical&lt;/li&gt; &lt;li&gt;Expert size: E&lt;/li&gt; &lt;li&gt;Transfer time: E / 25 GB/s&lt;/li&gt; &lt;li&gt;Token generation time target: ~10-50ms (20-100 tokens/sec)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Break-even -&lt;/p&gt; &lt;p&gt;You want: &lt;code&gt;cache_miss_overhead &amp;lt; token_generation_time_savings&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Simple threshold&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;If C ≥ A / (1 - target_miss_rate) then swapping is likely worth it&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Per layer&lt;/strong&gt; (assuming 8 experts per layer):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If C_layer = 2: you can only fit exactly what's needed, 0% cache benefit&lt;/li&gt; &lt;li&gt;If C_layer = 4: ~50-60% hit rate&lt;/li&gt; &lt;li&gt;If C_layer = 6: ~75-85% hit rate&lt;/li&gt; &lt;li&gt;If C_layer = 8: 100% hit rate (all experts cached)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Break-even point&lt;/strong&gt;: When &lt;code&gt;(1 - H) × E / 25GB/s &amp;lt; token_budget&lt;/code&gt;&lt;/p&gt; &lt;p&gt;If E = 1GB, token_budget = 20ms:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;With H = 75%: 0.25 × 1GB / 25GB/s = 10ms ✓ Worth it&lt;/li&gt; &lt;li&gt;With H = 50%: 0.50 × 1GB / 25GB/s = 20ms ≈ Break-even&lt;/li&gt; &lt;li&gt;With H = 25%: 0.75 × 1GB / 25GB/s = 30ms ✗ Too slow&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you can fit at least &lt;strong&gt;half the experts&lt;/strong&gt; in VRAM, LRU swapping is likely a win because temporal locality gives you 70-80% hit rates.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Not worth it when&lt;/strong&gt;: C &amp;lt; 0.25 × total_experts - you're thrashing too much&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Sweet spot&lt;/strong&gt;: Models where you can fit 50-75% of experts - you get most of the benefit of the full model at a fraction of the VRAM cost.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CodeSlave9000"&gt; /u/CodeSlave9000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxeif6/premise_moe_models_have_exploitable_locality_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxeif6/premise_moe_models_have_exploitable_locality_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxeif6/premise_moe_models_have_exploitable_locality_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T01:16:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy0x1j</id>
    <title>Mi50 Prices Nov 2025</title>
    <updated>2025-11-15T19:43:01+00:00</updated>
    <author>
      <name>/u/Success-Dependent</name>
      <uri>https://old.reddit.com/user/Success-Dependent</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The best prices on alibaba for small order quantities I'm seeing is $106 for the 16gb (with turbo fan) and $320 for the 32gb.&lt;/p&gt; &lt;p&gt;The 32gb are mostly sold out.&lt;/p&gt; &lt;p&gt;What prices are you paying?&lt;/p&gt; &lt;p&gt;Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Success-Dependent"&gt; /u/Success-Dependent &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy0x1j/mi50_prices_nov_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy0x1j/mi50_prices_nov_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy0x1j/mi50_prices_nov_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T19:43:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxj2mq</id>
    <title>I just realized 20 tokens per second is a decent speed in token generation.</title>
    <updated>2025-11-15T05:04:06+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I can ever afford a mac studio with 512 unified memory, I will happily take it. I just want inference and even 20 tokens per second is not bad. At least I’ll be able to locally run models on it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxj2mq/i_just_realized_20_tokens_per_second_is_a_decent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxj2mq/i_just_realized_20_tokens_per_second_is_a_decent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxj2mq/i_just_realized_20_tokens_per_second_is_a_decent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T05:04:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxzmai</id>
    <title>Extract structured data from long Pdf/excel docs with no standards.</title>
    <updated>2025-11-15T18:51:30+00:00</updated>
    <author>
      <name>/u/LakeRadiant446</name>
      <uri>https://old.reddit.com/user/LakeRadiant446</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have documents(excel, pdf) with lots of pages, mostly things like bills, items, quantities etc. There are divisions, categories and items within it. And Excels can have multiple sheets. And things can span multi pages. I have a structured pydantic schema I want as output. I need to identify each item and the category/division it belong to, along with some additional fields. But there are no unified standards of these layouts and content its entirely dependent on the client. Even for a Division, some contain division keyword some may just some bold header. Some fields in it also in different places depend on the client so we need look at multiple places to find it depending on context. &lt;/p&gt; &lt;p&gt;What's the best workflow for this? Currently I am experimenting with first convert Document -&amp;gt; Markdown. Then feed it in fixed character count based chunks with some overlap( Sheets are merged).. Then finally merge them. This is not working well for me. Can anyone guide me in right direction? &lt;/p&gt; &lt;p&gt;Thank you! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LakeRadiant446"&gt; /u/LakeRadiant446 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxzmai/extract_structured_data_from_long_pdfexcel_docs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxzmai/extract_structured_data_from_long_pdfexcel_docs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxzmai/extract_structured_data_from_long_pdfexcel_docs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T18:51:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxsnbv</id>
    <title>I have a friend who as 21 3060Tis from his mining times. Can this be, in any way be used for inference?</title>
    <updated>2025-11-15T14:10:03+00:00</updated>
    <author>
      <name>/u/puru991</name>
      <uri>https://old.reddit.com/user/puru991</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just the title. Is there any way to put that Vram to anything usable? He is open to adding ram, cpu and other things that might help the setup be usable. Any directions or advice appreciated.&lt;/p&gt; &lt;p&gt;Edit: so it seems the answer is - it is a bad idea. Sell&amp;gt;buy fewer more vram cards&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/puru991"&gt; /u/puru991 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxsnbv/i_have_a_friend_who_as_21_3060tis_from_his_mining/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxsnbv/i_have_a_friend_who_as_21_3060tis_from_his_mining/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxsnbv/i_have_a_friend_who_as_21_3060tis_from_his_mining/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T14:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxq9yq</id>
    <title>A Deep Dive into Self-Attention and Multi-Head Attention in Transformers</title>
    <updated>2025-11-15T12:17:00+00:00</updated>
    <author>
      <name>/u/Creative_Leader_7339</name>
      <uri>https://old.reddit.com/user/Creative_Leader_7339</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Understanding &lt;strong&gt;Self-Attention&lt;/strong&gt; and &lt;strong&gt;Multi-Head Attention&lt;/strong&gt; is key to understanding how modern LLMs like GPT work. These mechanisms let Transformers process text efficiently, capture long-range relationships, and understand meaning across an entire sequence all without recurrence or convolution.&lt;/p&gt; &lt;p&gt;In this Medium article, I take a deep dive into the attention system, breaking it down step-by-step from the basics all the way to the full Transformer implementation.&lt;br /&gt; &lt;a href="https://medium.com/@habteshbeki/inside-gpt-a-deep-dive-into-self-attention-and-multi-head-attention-6f2749fa2e03"&gt;https://medium.com/@habteshbeki/inside-gpt-a-deep-dive-into-self-attention-and-multi-head-attention-6f2749fa2e03&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creative_Leader_7339"&gt; /u/Creative_Leader_7339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxq9yq/a_deep_dive_into_selfattention_and_multihead/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxq9yq/a_deep_dive_into_selfattention_and_multihead/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxq9yq/a_deep_dive_into_selfattention_and_multihead/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T12:17:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxze50</id>
    <title>Voices to clone</title>
    <updated>2025-11-15T18:42:36+00:00</updated>
    <author>
      <name>/u/EfficientCourage588</name>
      <uri>https://old.reddit.com/user/EfficientCourage588</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically, I need people who would allow me to clone their voice on a local LLM for audiobooks and sell them. Do you know any free-to-use or paid voice datasets for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EfficientCourage588"&gt; /u/EfficientCourage588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxze50/voices_to_clone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxze50/voices_to_clone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxze50/voices_to_clone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T18:42:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxo07k</id>
    <title>Is getting a $350 modded 22GB RTX 2080TI from Alibaba as a low budget inference/gaming card a really stupid idea?</title>
    <updated>2025-11-15T10:03:20+00:00</updated>
    <author>
      <name>/u/SarcasticBaka</name>
      <uri>https://old.reddit.com/user/SarcasticBaka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello lads, I'm a newbie to the whole LLM scene and I've been experimenting for the last couple of months with various small models using my Ryzen 7 7840u laptop which is cool but very limiting for obvious reasons.&lt;/p&gt; &lt;p&gt;I figured I could get access to better models by upgrading my desktop PC which currently has an AMD RX580 to a better GPU with CUDA and more VRAM, which would also let me play modern games at decent framerates so that's pretty cool. Being a student in a 3rd world country and having a very limited budget tho I cant really afford to spend more than 300$ or so on a gpu, so my best options at this price point I have as far as I can tell are either this Frankenstein monster of a card or something like the the RTX 3060 12GB.&lt;/p&gt; &lt;p&gt;So does anyone have experience with these cards? are they too good to be true and do they have any glaring issues I should be aware of? Are they a considerable upgrade over my Radeon 780m APU or should I not even bother.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SarcasticBaka"&gt; /u/SarcasticBaka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxo07k/is_getting_a_350_modded_22gb_rtx_2080ti_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxo07k/is_getting_a_350_modded_22gb_rtx_2080ti_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxo07k/is_getting_a_350_modded_22gb_rtx_2080ti_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T10:03:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxumd9</id>
    <title>The Silicon Leash: Why ASI Takeoff has a Hard Physical Bottleneck for 10-20 Years</title>
    <updated>2025-11-15T15:34:01+00:00</updated>
    <author>
      <name>/u/Reddactor</name>
      <uri>https://old.reddit.com/user/Reddactor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR / Short Version:&lt;/strong&gt;&lt;br /&gt; We often think of ASI takeoff as a purely computational event. But a nascent ASI will be critically dependent on the human-run semiconductor supply chain for at least a decade. This chain is incredibly fragile (ASML's EUV monopoly, $40B fabs, geopolitical chokepoints) and relies on &amp;quot;tacit knowledge&amp;quot; that can't be digitally copied. The paradox is that the AI leading to ASI will cause a massive economic collapse by automating knowledge work, which in turn defunds and breaks the very supply chain the ASI needs to scale its own intelligence. This physical dependency is a hard leash on the speed of takeoff.&lt;/p&gt; &lt;p&gt;Hey LocalLlama,&lt;/p&gt; &lt;p&gt;I've been working on my &lt;a href="https://github.com/dnhkng/GlaDOS"&gt;GLaDOS Project&lt;/a&gt; which was really popular here, and have built a pretty nice new server for her. At the same time as I work full-time in AI, and also in my private time, I have pondered a lot on the future. I have spent some time collecting and organising these thoughts, especially about the physical constraints on the intelligence explosion, moving beyond pure software and compute scaling. I wrote a deep dive on this, and the core idea is something I call &amp;quot;The Silicon Leash.&amp;quot;&lt;/p&gt; &lt;p&gt;We're all familiar with exponential growth curves, but an ASI doesn't emerge in a vacuum. It emerges inside the most complex and fragile supply chain humans have ever built. Consider the dependencies:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;EUV Lithography:&lt;/strong&gt; The entire world's supply of sub-7nm chips depends on EUV machines. Only one company, ASML, can make them. They cost ~$200M each and are miracles of physics.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fab Construction:&lt;/strong&gt; A single leading-edge fab (like TSMC's 2nm) costs $20-40 billion and takes 3-5 years to build, requiring ultrapure water, stable power grids, and thousands of suppliers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Tacit Knowledge Problem:&lt;/strong&gt; This is the most interesting part. Even with the same EUV machines, TSMC's yields at 3nm are reportedly ~90% while Samsung's are closer to 50%. Why? Decades of accumulated, unwritten process knowledge held in the heads of human engineers. You can't just copy the blueprints; you need the experienced team. An ASI can't easily extract this knowledge by force.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Here's the feedback loop that creates the leash:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;AI Automates Knowledge Work:&lt;/strong&gt; GPT-5/6 level models will automate millions of office jobs (law, finance, admin) far faster than physical jobs (plumbers, electricians).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Economic Demand Collapses:&lt;/strong&gt; This mass unemployment craters consumer, corporate, and government spending. The economy that buys iPhones, funds R&amp;amp;D, and invests in new fabs disappears.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Supply Chain Breaks:&lt;/strong&gt; Without demand, there's no money or incentive to build the next generation of fabs. Utilization drops below 60% and existing fabs shut down. The semiconductor industry stalls.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;An ASI emerging in, say, 2033, finds itself in a trap. It's superintelligent, but it can't conjure a 1nm fab into existence. It needs the existing human infrastructure to continue functioning while it builds its own, but its very emergence is what causes that infrastructure to collapse.&lt;/p&gt; &lt;p&gt;This creates a mandatory 10-20 year window of physical dependency—a leash. It doesn't solve alignment, but it fundamentally changes the game theory of the initial takeoff period from one of immediate dominance to one of forced coordination.&lt;/p&gt; &lt;p&gt;Curious to hear your thoughts on this as a physical constraint on the classic intelligence explosion models.&lt;/p&gt; &lt;p&gt;(Disclaimer: This is a summary of Part 1 of my own four-part series on the topic. Happy to discuss and debate!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reddactor"&gt; /u/Reddactor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://dnhkng.github.io/posts/silicon-leash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxumd9/the_silicon_leash_why_asi_takeoff_has_a_hard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxumd9/the_silicon_leash_why_asi_takeoff_has_a_hard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T15:34:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxb9zp</id>
    <title>Local models handle tools way better when you give them a code sandbox instead of individual tools</title>
    <updated>2025-11-14T22:54:49+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxb9zp/local_models_handle_tools_way_better_when_you/"&gt; &lt;img alt="Local models handle tools way better when you give them a code sandbox instead of individual tools" src="https://preview.redd.it/83hx5w1txa1g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e2fbc834e05621ee050a05b0ee016fd280ff683" title="Local models handle tools way better when you give them a code sandbox instead of individual tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/83hx5w1txa1g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxb9zp/local_models_handle_tools_way_better_when_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxb9zp/local_models_handle_tools_way_better_when_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T22:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxl3ju</id>
    <title>What makes closed source models good? Data, Architecture, Size?</title>
    <updated>2025-11-15T07:00:06+00:00</updated>
    <author>
      <name>/u/Bitter-College8786</name>
      <uri>https://old.reddit.com/user/Bitter-College8786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know Kimi K2, Minimax M2 and Deepseek R1 are strong, but I asked myself: what makes the closed source models like Sonnet 4.5 or GPT-5 so strong? Do they have better training data? Or are their models even bigger, e.g. 2T, or do their models have some really good secret architecture (what I assume for Gemini 2.5 with its 1M context)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bitter-College8786"&gt; /u/Bitter-College8786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxl3ju/what_makes_closed_source_models_good_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxl3ju/what_makes_closed_source_models_good_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxl3ju/what_makes_closed_source_models_good_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T07:00:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy053m</id>
    <title>Why do (some) people hate Open WebUI?</title>
    <updated>2025-11-15T19:12:11+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m new to local hosted LLMs. I’ve setup mine using LM Studio + Open WebUI (for external access). I couldn’t help but notice every video/post/tutorial has some people in the comments saying how you shouldn’t use Open WebUi. But not really clear as to “why?”&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy053m/why_do_some_people_hate_open_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy053m/why_do_some_people_hate_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy053m/why_do_some_people_hate_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T19:12:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxywsc</id>
    <title>New Sherlock Alpha Stealth Models on OpenRouter might be Grok 4.20</title>
    <updated>2025-11-15T18:23:15+00:00</updated>
    <author>
      <name>/u/According-Zombie-337</name>
      <uri>https://old.reddit.com/user/According-Zombie-337</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxywsc/new_sherlock_alpha_stealth_models_on_openrouter/"&gt; &lt;img alt="New Sherlock Alpha Stealth Models on OpenRouter might be Grok 4.20" src="https://preview.redd.it/j373g4gxqg1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86cd8ced7274dd0c65bcbb2cc3d09a41e635028c" title="New Sherlock Alpha Stealth Models on OpenRouter might be Grok 4.20" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Sherlock models are from xAI, probably Grok 4.20.&lt;/p&gt; &lt;p&gt;For context, two new stealth models just appeared on OpenRouter:&lt;/p&gt; &lt;p&gt;Sherlock Alpha and Sherlock Think Alpha.&lt;/p&gt; &lt;p&gt;From the testing I've done so far, capabilities aren't anything super new, but better than Grok 4 and Grok 4 Fast.&lt;/p&gt; &lt;p&gt;If this doesn't come out before Gemini 3 (which it looks like it won't since Gemini 3 is coming next week), then this will not be a Frontier model release. But the benchmarks might say differently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/According-Zombie-337"&gt; /u/According-Zombie-337 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j373g4gxqg1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxywsc/new_sherlock_alpha_stealth_models_on_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxywsc/new_sherlock_alpha_stealth_models_on_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T18:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxw1rf</id>
    <title>“We don’t need corp AI, we have AI at home.. “</title>
    <updated>2025-11-15T16:30:40+00:00</updated>
    <author>
      <name>/u/Birchi</name>
      <uri>https://old.reddit.com/user/Birchi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxw1rf/we_dont_need_corp_ai_we_have_ai_at_home/"&gt; &lt;img alt="“We don’t need corp AI, we have AI at home.. “" src="https://a.thumbs.redditmedia.com/qc9sVTXit2tBCYT5qsuH3I3XzrNQcOniJHSAWtLWmA4.jpg" title="“We don’t need corp AI, we have AI at home.. “" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;.. the AI at home. I figured you guys would appreciate this more than my irl peeps :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Birchi"&gt; /u/Birchi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oxw1rf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxw1rf/we_dont_need_corp_ai_we_have_ai_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxw1rf/we_dont_need_corp_ai_we_have_ai_at_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T16:30:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxtc5y</id>
    <title>US Cloud Giants to Spend ~8.16× What China Does in 2025–27 — $1.7 Trillion vs $210 Billion, Will it translate to stronger US AI dominance?</title>
    <updated>2025-11-15T14:40:20+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxtc5y/us_cloud_giants_to_spend_816_what_china_does_in/"&gt; &lt;img alt="US Cloud Giants to Spend ~8.16× What China Does in 2025–27 — $1.7 Trillion vs $210 Billion, Will it translate to stronger US AI dominance?" src="https://preview.redd.it/sjklvwo8nf1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be22252ccd8c2abcae9179c2f3c14bcfe022e978" title="US Cloud Giants to Spend ~8.16× What China Does in 2025–27 — $1.7 Trillion vs $210 Billion, Will it translate to stronger US AI dominance?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sjklvwo8nf1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxtc5y/us_cloud_giants_to_spend_816_what_china_does_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxtc5y/us_cloud_giants_to_spend_816_what_china_does_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T14:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxxrhc</id>
    <title>Kimi K2 is the best clock AI</title>
    <updated>2025-11-15T17:38:21+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every minute, a new clock is displayed that has been generated by nine different AI models.&lt;/p&gt; &lt;p&gt;Each model is allowed 2000 tokens to generate its clock. Here is its prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Create HTML/CSS of an analog clock showing ${time}. Include numbers (or numerals) if you wish, and have a CSS animated second hand. Make it responsive and use a white background. Return ONLY the HTML/CSS code with no markdown formatting.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I have observed for a long time that the Kimi K2 is the only model that can maintain 12 digits in the correct clock positions, even with the second hand perfectly aligned with the actual time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxxrhc/kimi_k2_is_the_best_clock_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxxrhc/kimi_k2_is_the_best_clock_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxxrhc/kimi_k2_is_the_best_clock_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T17:38:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oximzj</id>
    <title>Anthropic pushing again for regulation of open source models?</title>
    <updated>2025-11-15T04:40:56+00:00</updated>
    <author>
      <name>/u/MasterDragon_</name>
      <uri>https://old.reddit.com/user/MasterDragon_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oximzj/anthropic_pushing_again_for_regulation_of_open/"&gt; &lt;img alt="Anthropic pushing again for regulation of open source models?" src="https://preview.redd.it/623qojxaoc1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd955c46ca05077bed949b46643bd7061e16d04c" title="Anthropic pushing again for regulation of open source models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MasterDragon_"&gt; /u/MasterDragon_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/623qojxaoc1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oximzj/anthropic_pushing_again_for_regulation_of_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oximzj/anthropic_pushing_again_for_regulation_of_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T04:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oth5pw</id>
    <title>AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model</title>
    <updated>2025-11-10T15:44:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt; &lt;img alt="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" src="https://b.thumbs.redditmedia.com/yz9_FpdLcHNiCkaH5fLEIoXS2f5u5twNBr7SQ9Go3AI.jpg" title="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Moonshot AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/ComfortableAsk4494"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/zxytim"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ppwwyyxx"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM – 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87"&gt;https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:44:10+00:00</published>
  </entry>
</feed>
