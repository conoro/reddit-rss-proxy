<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-07T23:34:35+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pgeda8</id>
    <title>RTX6000Pro stability issues (system spontaneous power cycling)</title>
    <updated>2025-12-07T09:32:28+00:00</updated>
    <author>
      <name>/u/Elv13</name>
      <uri>https://old.reddit.com/user/Elv13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I just upgraded from 4xP40 to 1x RTX6000Pro (NVIDIA RTX PRO 6000 Blackwell Workstation Edition Graphic Card - 96 GB GDDR7 ECC - PCIe 5.0 x16 - 512-Bit - 2x Slot - XHFL - Active - 600 W- 900-5G144-2200-000). I bought a 1200W corsair RM1200 along with it.&lt;/p&gt; &lt;p&gt;At 600W, the machine just reboots at soon as llama.cpp or ComfyUI starts. At 200w (&lt;code&gt;sudo nvidia-smi -pl 200&lt;/code&gt;), it starts, but reboot at some point. I just can't get it to finish anything. My old 800w PSU does no better when I power limit it to 150w.&lt;/p&gt; &lt;p&gt;VBios:&lt;/p&gt; &lt;p&gt;&lt;code&gt; nvidia-smi -q | grep &amp;quot;VBIOS Version&amp;quot; VBIOS Version : 98.02.81.00.07 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;(machine is a threadriper pro 3000 series with 16 core and 128Gb ram, OS is Ubuntu 24.04). All 4 power connectors are attached to different PSU 12v lanes. Even then, power limited at 200w, this is equivalent to a single P40 and I was running 4 of them.&lt;/p&gt; &lt;p&gt;Is that card a lemon or am I doing it wrong? Has anyone experienced this kind of instability. Do I need a 3rd PSU to test?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Elv13"&gt; /u/Elv13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgeda8/rtx6000pro_stability_issues_system_spontaneous/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgeda8/rtx6000pro_stability_issues_system_spontaneous/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgeda8/rtx6000pro_stability_issues_system_spontaneous/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T09:32:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgjqm3</id>
    <title>Is it possible to run two seperate llama-server.exe processes that share the same layers and weights stored in DRAM?</title>
    <updated>2025-12-07T14:32:54+00:00</updated>
    <author>
      <name>/u/PairOfRussels</name>
      <uri>https://old.reddit.com/user/PairOfRussels</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think what happens currently is if I'm running two llama-server.exe processes with the same MOE LLM model (qwen3-next-80b) on two GPUs, and if I have any layers offloaded to CPU or MOE expert weightings on CPU, then it will have TWO independent sets of that data in DRAM.&lt;/p&gt; &lt;p&gt;I was wondering if anyone thinks it's possible to have both processes use the same data to save on ram usage.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PairOfRussels"&gt; /u/PairOfRussels &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgjqm3/is_it_possible_to_run_two_seperate_llamaserverexe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgjqm3/is_it_possible_to_run_two_seperate_llamaserverexe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgjqm3/is_it_possible_to_run_two_seperate_llamaserverexe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T14:32:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgwyo9</id>
    <title>Gemma 3n E4B Question.</title>
    <updated>2025-12-07T23:27:46+00:00</updated>
    <author>
      <name>/u/Automatic-Hall-1685</name>
      <uri>https://old.reddit.com/user/Automatic-Hall-1685</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to finetune the gemma-3n-E4B model using Unsloth on Google Colab. I'm on the free tier, and everything goes well until it's time to convert the model into GGUF. Google Colab just shuts down during this process. It generates all the tensor files, but the conversion does not seem to work. Does anyone know how to proceed? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Automatic-Hall-1685"&gt; /u/Automatic-Hall-1685 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgwyo9/gemma_3n_e4b_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgwyo9/gemma_3n_e4b_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgwyo9/gemma_3n_e4b_question/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T23:27:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgwznn</id>
    <title>Non agentic uses of LLMs for coding</title>
    <updated>2025-12-07T23:29:01+00:00</updated>
    <author>
      <name>/u/WasteTechnology</name>
      <uri>https://old.reddit.com/user/WasteTechnology</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to answers to this post: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pg76jo/why_local_coding_models_are_less_popular_than/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1pg76jo/why_local_coding_models_are_less_popular_than/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;It seems that most people believe that local LLMs for coding are far behind hosted models, at least for agentic coding.&lt;/p&gt; &lt;p&gt;However, there's a question, is there any other case? Do you use them for tab completion, next edit prediction, code review, asking questions about code? Which among these use cases are good enough for local LLMs to be usable? Which tooling do you use for them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WasteTechnology"&gt; /u/WasteTechnology &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgwznn/non_agentic_uses_of_llms_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgwznn/non_agentic_uses_of_llms_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgwznn/non_agentic_uses_of_llms_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T23:29:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg7f00</id>
    <title>Zen CPU Performance Uplift (Epyc &amp; Strix Halo) w/ ZenDNN Backend Integration for llama.cpp</title>
    <updated>2025-12-07T02:50:50+00:00</updated>
    <author>
      <name>/u/Noble00_</name>
      <uri>https://old.reddit.com/user/Noble00_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg7f00/zen_cpu_performance_uplift_epyc_strix_halo_w/"&gt; &lt;img alt="Zen CPU Performance Uplift (Epyc &amp;amp; Strix Halo) w/ ZenDNN Backend Integration for llama.cpp" src="https://external-preview.redd.it/NDJTzKU3ltYG49f6LU-R2hFmqhxjjyJK3XNi_UF7GlA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d8c9b4d0929a1c550bf37e26c240c96d38c9d9c" title="Zen CPU Performance Uplift (Epyc &amp;amp; Strix Halo) w/ ZenDNN Backend Integration for llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just happened to cross this and thought this seemed interesting. Here are some benchmarks:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Test Configuration&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hardware&lt;/strong&gt;: AMD EPYC 9004 Series (Zen 4)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Threads&lt;/strong&gt;: 96&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Batch Size&lt;/strong&gt;: 4096&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool&lt;/strong&gt;: llama-bench&lt;/li&gt; &lt;li&gt;&lt;strong&gt;llama.cpp version&lt;/strong&gt;: 7134&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ZenDNN version&lt;/strong&gt;: 1.0.0&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt;: &lt;code&gt;ZENDNNL_MATMUL_ALGO=2&lt;/code&gt; (Blocked AOCL BLIS)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;LLaMA 3.1 8B (BF16)&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;CPU t/s&lt;/th&gt; &lt;th align="left"&gt;ZenDNN t/s&lt;/th&gt; &lt;th align="left"&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp128&lt;/td&gt; &lt;td align="left"&gt;341.50&lt;/td&gt; &lt;td align="left"&gt;395.58&lt;/td&gt; &lt;td align="left"&gt;1.16x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp256&lt;/td&gt; &lt;td align="left"&gt;382.52&lt;/td&gt; &lt;td align="left"&gt;561.94&lt;/td&gt; &lt;td align="left"&gt;1.47x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;423.40&lt;/td&gt; &lt;td align="left"&gt;624.61&lt;/td&gt; &lt;td align="left"&gt;1.48x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;414.12&lt;/td&gt; &lt;td align="left"&gt;637.97&lt;/td&gt; &lt;td align="left"&gt;1.54x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;338.50&lt;/td&gt; &lt;td align="left"&gt;622.08&lt;/td&gt; &lt;td align="left"&gt;1.84x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp4096&lt;/td&gt; &lt;td align="left"&gt;308.53&lt;/td&gt; &lt;td align="left"&gt;534.76&lt;/td&gt; &lt;td align="left"&gt;1.73x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.28&lt;/td&gt; &lt;td align="left"&gt;10.53&lt;/td&gt; &lt;td align="left"&gt;1.45x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;LLaMA 3.1 8B (F32)&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;CPU t/s&lt;/th&gt; &lt;th align="left"&gt;ZenDNN t/s&lt;/th&gt; &lt;th align="left"&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp128&lt;/td&gt; &lt;td align="left"&gt;184.44&lt;/td&gt; &lt;td align="left"&gt;293.39&lt;/td&gt; &lt;td align="left"&gt;1.59x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp256&lt;/td&gt; &lt;td align="left"&gt;189.69&lt;/td&gt; &lt;td align="left"&gt;384.71&lt;/td&gt; &lt;td align="left"&gt;2.03x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;234.74&lt;/td&gt; &lt;td align="left"&gt;431.21&lt;/td&gt; &lt;td align="left"&gt;1.84x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp1024&lt;/td&gt; &lt;td align="left"&gt;231.49&lt;/td&gt; &lt;td align="left"&gt;451.51&lt;/td&gt; &lt;td align="left"&gt;1.95x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;220.05&lt;/td&gt; &lt;td align="left"&gt;425.65&lt;/td&gt; &lt;td align="left"&gt;1.93x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;pp4096&lt;/td&gt; &lt;td align="left"&gt;189.75&lt;/td&gt; &lt;td align="left"&gt;396.73&lt;/td&gt; &lt;td align="left"&gt;2.09x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;2.69&lt;/td&gt; &lt;td align="left"&gt;7.34&lt;/td&gt; &lt;td align="left"&gt;2.73x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Merged: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17690"&gt;https://github.com/ggml-org/llama.cpp/pull/17690&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, &lt;del&gt;while disappointingly for Epyc and STX-H only it seems&lt;/del&gt;, it has been able to work on the Ryzen 7940HS, perhaps uplifts can be seen on consumer desktop.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Noble00_"&gt; /u/Noble00_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/17684"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg7f00/zen_cpu_performance_uplift_epyc_strix_halo_w/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg7f00/zen_cpu_performance_uplift_epyc_strix_halo_w/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T02:50:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg8jtk</id>
    <title>SGLang Diffusion + Cache-DiT = 20-165% Faster Local Image/Video Generation</title>
    <updated>2025-12-07T03:48:50+00:00</updated>
    <author>
      <name>/u/Expert-Pineapple-740</name>
      <uri>https://old.reddit.com/user/Expert-Pineapple-740</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quick heads up: SGLang Diffusion now supports Cache-DiT integration, delivering 20-165% speedup for diffusion models with basically zero effort.&lt;/p&gt; &lt;p&gt;Just add some env variables and you're getting 46%+ faster inference on models like FLUX, Qwen-Image, HunyuanVideo, etc.&lt;/p&gt; &lt;p&gt;Works with torch.compile, quantization, and all the usual optimizations. Supports pretty much every major open-source DiT model.&lt;/p&gt; &lt;p&gt;Install: &lt;code&gt;uv pip install 'sglang[diffusion]' --prerelease=allow&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Docs: &lt;a href="https://github.com/sgl-project/sglang/blob/main/python/sglang/multimodal_gen/docs/cache_dit.md"&gt;https://github.com/sgl-project/sglang/blob/main/python/sglang/multimodal_gen/docs/cache_dit.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expert-Pineapple-740"&gt; /u/Expert-Pineapple-740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8jtk/sglang_diffusion_cachedit_20165_faster_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8jtk/sglang_diffusion_cachedit_20165_faster_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8jtk/sglang_diffusion_cachedit_20165_faster_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T03:48:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgm8pw</id>
    <title>Most AI websites are almost unsearchable</title>
    <updated>2025-12-07T16:16:10+00:00</updated>
    <author>
      <name>/u/good-parameter</name>
      <uri>https://old.reddit.com/user/good-parameter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been looking for some models and I CAN'T EVEN FIND THE OFFICIAL WEBSITE,the results are flooded with fake websites that's named after the model,they share the same logo,and they show similar content,I asked an AI model to do a deep search for me and find the official website and it couldn't sadly (the model told me of 3 websites so it doesn't know the original) and I don't want to visit random websits,is there any way that directly connect me to the official website of the model? And how are those websites still reachable after that long time? (I looked up some of them on VirusTotal,most are 2-5+ month online).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/good-parameter"&gt; /u/good-parameter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgm8pw/most_ai_websites_are_almost_unsearchable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgm8pw/most_ai_websites_are_almost_unsearchable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgm8pw/most_ai_websites_are_almost_unsearchable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T16:16:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg76jo</id>
    <title>Why local coding models are less popular than hosted coding models?</title>
    <updated>2025-12-07T02:39:17+00:00</updated>
    <author>
      <name>/u/WasteTechnology</name>
      <uri>https://old.reddit.com/user/WasteTechnology</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In theory, local coding models sound very good. You don't send your most valuable assets to another company, keep everything local and under control. However, the leading AI coding startups work with hosted models (correct me if I'm wrong). Why do you think it is so?&lt;/p&gt; &lt;p&gt;If you use one, please share your setup. Which model, which engine, which coding tool do you use?, What is your experience? Do you get productive enough with them compared to hosted options?&lt;/p&gt; &lt;p&gt;UPD: Some of folks downvoted some of my comments to minus a lot. I don't understand why. A bit to share why I am asking. I use some of hosted LLMs. I use codex pretty often, but not for writing code, but for asking questions about the codebase, i.e. to understand how something works. I also used other models from time to time in the last 6 months. However, I don't feel that any of them will replace me writing manual code as I do it now. They are improving, but I prefer what I write myself, and use them as an additional tool, not the thing which writes my code.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WasteTechnology"&gt; /u/WasteTechnology &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg76jo/why_local_coding_models_are_less_popular_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg76jo/why_local_coding_models_are_less_popular_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg76jo/why_local_coding_models_are_less_popular_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T02:39:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgqyw5</id>
    <title>5070ti (16gb) or GMKTec Evo X2?</title>
    <updated>2025-12-07T19:20:23+00:00</updated>
    <author>
      <name>/u/FrozenBuffalo25</name>
      <uri>https://old.reddit.com/user/FrozenBuffalo25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why I’d consider the 5070ti: 16gb vram, $1000 cheaper than a new MiniPC, cuda for stable diffusion&lt;/p&gt; &lt;p&gt;Why I’d consider strix halo miniPC: much larger MoE models, small form factor, low power consumption&lt;/p&gt; &lt;p&gt;Where would you lean for a future-proof box with some flexibility, capable of performing a wide variety of tasks (not just hosting a single model using 100% ram and nothing else.)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrozenBuffalo25"&gt; /u/FrozenBuffalo25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgqyw5/5070ti_16gb_or_gmktec_evo_x2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgqyw5/5070ti_16gb_or_gmktec_evo_x2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgqyw5/5070ti_16gb_or_gmktec_evo_x2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T19:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgkzwq</id>
    <title>Anyone here need temporary A10 compute for LLM finetuning (QLoRA etc.)?</title>
    <updated>2025-12-07T15:25:54+00:00</updated>
    <author>
      <name>/u/Perfect-Analysis5015</name>
      <uri>https://old.reddit.com/user/Perfect-Analysis5015</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm setting up some A10 compute for my own experiments and have spare capacity.&lt;/p&gt; &lt;p&gt;If anyone working on Llama/Qwen/Mistral finetuning needs short-term access, I can share some of the compute to help cover the server costs.&lt;/p&gt; &lt;p&gt;Specs:&lt;/p&gt; &lt;p&gt;• 2× NVIDIA A10 (24GB each)&lt;/p&gt; &lt;p&gt;• 30 vCPUs, 480GB RAM&lt;/p&gt; &lt;p&gt;• CUDA 12.2, PyTorch/Transformers/bitsandbytes preinstalled&lt;/p&gt; &lt;p&gt;• Clean environment for each user&lt;/p&gt; &lt;p&gt;Useful for:&lt;/p&gt; &lt;p&gt;• QLoRA finetuning&lt;/p&gt; &lt;p&gt;• Embedding generation&lt;/p&gt; &lt;p&gt;• Model evaluation&lt;/p&gt; &lt;p&gt;• Research projects&lt;/p&gt; &lt;p&gt;If interested, DM me and I can spin up a fresh VM. &lt;/p&gt; &lt;p&gt;(crypto/PayPal just to cover costs)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Perfect-Analysis5015"&gt; /u/Perfect-Analysis5015 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgkzwq/anyone_here_need_temporary_a10_compute_for_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgkzwq/anyone_here_need_temporary_a10_compute_for_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgkzwq/anyone_here_need_temporary_a10_compute_for_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T15:25:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgdyxr</id>
    <title>RnJ-1-Instruct FP8 Quantization</title>
    <updated>2025-12-07T09:07:06+00:00</updated>
    <author>
      <name>/u/doradus_novae</name>
      <uri>https://old.reddit.com/user/doradus_novae</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdyxr/rnj1instruct_fp8_quantization/"&gt; &lt;img alt="RnJ-1-Instruct FP8 Quantization" src="https://external-preview.redd.it/f3hfIybvzLxoPbJzEByWUBJrtbOq2n1Nf0l2zrxAhSw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d280df0cf1ec0f321434e30aa1d6a29aebed5432" title="RnJ-1-Instruct FP8 Quantization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;FP8 quantized version of RnJ1-Instruct-8B BF16 instruction model.&lt;/p&gt; &lt;p&gt;VRAM: 16GB → 8GB (50% reduction)&lt;/p&gt; &lt;p&gt;Benchmarks:&lt;/p&gt; &lt;p&gt;- GSM8K: 87.2%&lt;/p&gt; &lt;p&gt;- MMLU-Pro: 44.5%&lt;/p&gt; &lt;p&gt;- IFEval: 55.3%&lt;/p&gt; &lt;p&gt;Runs on RTX 3060 12GB. One-liner to try:&lt;/p&gt; &lt;p&gt;docker run --gpus '&amp;quot;device=0&amp;quot;' -p 8000:8000 vllm/vllm-openai:v0.12.0 \&lt;/p&gt; &lt;p&gt;--model Doradus/Rn&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/doradus_novae"&gt; /u/doradus_novae &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Doradus/RnJ-1-Instruct-FP8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdyxr/rnj1instruct_fp8_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdyxr/rnj1instruct_fp8_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T09:07:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgs5ff</id>
    <title>We Got Claude to Fine-Tune an Open Source LLM</title>
    <updated>2025-12-07T20:06:46+00:00</updated>
    <author>
      <name>/u/Fun-Wolf-2007</name>
      <uri>https://old.reddit.com/user/Fun-Wolf-2007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgs5ff/we_got_claude_to_finetune_an_open_source_llm/"&gt; &lt;img alt="We Got Claude to Fine-Tune an Open Source LLM" src="https://external-preview.redd.it/l06xIWkTYOUJkcqguhRvZ9P7N3hhRtMIAo-7AUAScmM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=801d854c5c870a9aa86a874f96638f28a87fd5e2" title="We Got Claude to Fine-Tune an Open Source LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is interesting , I am glad to see the progress. Searching for the datasets will be useful for different use cases &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Wolf-2007"&gt; /u/Fun-Wolf-2007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/hf-skills-training"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgs5ff/we_got_claude_to_finetune_an_open_source_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgs5ff/we_got_claude_to_finetune_an_open_source_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T20:06:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgs1kh</id>
    <title>Dev on mac, seems promising but what will I miss?</title>
    <updated>2025-12-07T20:02:33+00:00</updated>
    <author>
      <name>/u/No_Afternoon_4260</name>
      <uri>https://old.reddit.com/user/No_Afternoon_4260</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been a linux boy since 10 years, before I used to have macs. I've great memories of mac, the fact that's based on unix will be helping I'm sure.&lt;/p&gt; &lt;p&gt;I've been really happy with cuda, linux+cuda makes me really feel I can do anything.&lt;/p&gt; &lt;p&gt;I know on mac I'll be relying on mlx implementation and whatnot. From my understanding llm inference is acquired. Training is a no go (probably not a software support problem? Nvidia cards just have the proper accelerator?).&lt;br /&gt; From my really quick research I found a macos desktop version for comfyui so I guess they can run diffusion models.&lt;br /&gt; I found that transformers from huggingface should run fine, it relies on pytorch which I'm sure is correctly implemented on macos. Am I wrong?&lt;/p&gt; &lt;p&gt;What would I be missing? Have you found any library that's badly supported or things that are just impossible because it rely on a x86 implementation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Afternoon_4260"&gt; /u/No_Afternoon_4260 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgs1kh/dev_on_mac_seems_promising_but_what_will_i_miss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgs1kh/dev_on_mac_seems_promising_but_what_will_i_miss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgs1kh/dev_on_mac_seems_promising_but_what_will_i_miss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T20:02:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pglclf</id>
    <title>Thoughts on decentralized training with Psyche?</title>
    <updated>2025-12-07T15:40:24+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was bored browsing this sub, and found a barely-upvoted thread about Hermes 4.3 36B. I don't care about the model (I never bother with finetunes + I can't run a dense 36B anyway), but buried in there was a very interesting piece of information: this model was trained entirely in a decentralized way on consumer hardware. Supposedly the largest model ever trained in a decentralized manner.&lt;/p&gt; &lt;p&gt;TLDR: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;They created a tool called Psyche (open-source) to split training across multiple remote GPUs. GPUs can join and leave the swarm in the middle of a training run. Training can be paused/resumed. One of its design goals was to maximize savings by letting you train on rented GPUs during offhours. They also use some sort of blockchain bullshit, I think it's to make sure a rented GPU can't poison their training by submitting fake results.&lt;/li&gt; &lt;li&gt;They also trained a 2nd copy of the model the classic way, on a single cluster of GPUs, and got comparable or better result on the version trained decentralized. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Their blog post where they discuss Psyche vs Centralized release: &lt;a href="https://nousresearch.com/introducing-hermes-4-3/"&gt;https://nousresearch.com/introducing-hermes-4-3/&lt;/a&gt; You can see the status web UI of Psyche here: &lt;a href="https://psyche.network/runs"&gt;https://psyche.network/runs&lt;/a&gt; &lt;/p&gt; &lt;p&gt;There's a few questionable things that tempered my excitement:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This may be hard to answer given the heterogenous nature of Psyche training, but there's no estimates of how much &amp;quot;efficiency&amp;quot; may be lost training the same model in Psyche vs centralized. No mention of how many rejections they had to do. It's likely they didn't record those things.&lt;/li&gt; &lt;li&gt;The big one: why would the Psyche version of 4.3 get better benchmarks than Centralized 4.3? They just mention it like it's an exciting news and don't address it again, but a normal reader would expect both models to have similar benchmark results, and therefore any significant difference is sus.&lt;/li&gt; &lt;li&gt;I wanted to ask the above questions on their Discord before posting here, but it has a buggy verification bot that asks you to enter numbers that are not there on the test image. It almost made me not want to submit this post, because if their Discord bot is this shitty, that reflects badly on their other tools.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Anyway, I'd love to hear what people who do training think of Psyche. Is it a huge deal?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pglclf/thoughts_on_decentralized_training_with_psyche/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pglclf/thoughts_on_decentralized_training_with_psyche/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pglclf/thoughts_on_decentralized_training_with_psyche/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T15:40:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pguel4</id>
    <title>Deepseek R1 671b Q4_K_M</title>
    <updated>2025-12-07T21:39:03+00:00</updated>
    <author>
      <name>/u/I_like_fragrances</name>
      <uri>https://old.reddit.com/user/I_like_fragrances</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pguel4/deepseek_r1_671b_q4_k_m/"&gt; &lt;img alt="Deepseek R1 671b Q4_K_M" src="https://b.thumbs.redditmedia.com/olNH80zXa9V59_KTFmt1b8Awrp30ktlgeOVQAJqqN8w.jpg" title="Deepseek R1 671b Q4_K_M" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was able to run Deepseek R1 671b locally with 384gb of VRAM. Get between 10-15 tok/s. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/i1pbettypu5g1.png?width=880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a21fb31c437ea1368541dae4cbb18becb314dc62"&gt;https://preview.redd.it/i1pbettypu5g1.png?width=880&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a21fb31c437ea1368541dae4cbb18becb314dc62&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I_like_fragrances"&gt; /u/I_like_fragrances &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pguel4/deepseek_r1_671b_q4_k_m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pguel4/deepseek_r1_671b_q4_k_m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pguel4/deepseek_r1_671b_q4_k_m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T21:39:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgoezb</id>
    <title>What are the cons of MXFP4?</title>
    <updated>2025-12-07T17:43:34+00:00</updated>
    <author>
      <name>/u/good-parameter</name>
      <uri>https://old.reddit.com/user/good-parameter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Considering that we can make the model FP16 and fine-tune it and then quantize to MXFP4 again,and the model will be robust because it was trained with QAT,what would be the cons? MXFP4 is (almost) virtually lossless,not FP16 but near-lossless,and it cuts training cost into the half compared to FP16? (FP8 won't be exactly the half because some layers will be kept in FP16 or FP32,so usually like 30% less) while MXFP4 still uses layers that are in higher precision the MoE layers are almost always in 4-bit and that's where the bulk of the computation go,so why it's not the new route? Especially it's standardized so it's verified to be in production and we have seen that with GPT-OSS,I found that MXFP4 gets much less loss even when they get upscaled to FP16 and then quantized to something like INT4 (which has wide compatibility with all types of hardware) compared to model that are trained in FP16.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/good-parameter"&gt; /u/good-parameter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgoezb/what_are_the_cons_of_mxfp4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgoezb/what_are_the_cons_of_mxfp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgoezb/what_are_the_cons_of_mxfp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T17:43:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pggss8</id>
    <title>I'm tired of claude limits, what's the best alternative? (cloud based or local llm)</title>
    <updated>2025-12-07T12:04:59+00:00</updated>
    <author>
      <name>/u/Dry_Explanation_7774</name>
      <uri>https://old.reddit.com/user/Dry_Explanation_7774</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone I hope y'all having a great day.&lt;/p&gt; &lt;p&gt;I've been using Claude Code since they released but I'm tired of the usage limits they have even when paying subscription.&lt;/p&gt; &lt;p&gt;I'm asking here since most of you have a great knowledge on what's the best and efficient way to run AI be it online with API or running a local LLM.&lt;/p&gt; &lt;p&gt;I'm asking, what's the best way to actually run Claude at cheap rates and at the same time getting the best of it without that ridiculous usage limits? &lt;/p&gt; &lt;p&gt;Or is there any other model that gives super similar or higher results for &amp;quot;coding&amp;quot; related activities but at the same time super cheap?&lt;/p&gt; &lt;p&gt;Or any of you recommend running my own local llm? which are your recommendations about this?&lt;/p&gt; &lt;p&gt;I currently have a GTX 1650 SUPER and 16GB RAM, i know it's super funny lol, but just lyk my current specs, so u can recommend me to buy something local or just deploy a local ai into a &amp;quot;custom ai hosting&amp;quot; and use the API?&lt;/p&gt; &lt;p&gt;I know there are a lot of questions, but I think you get my idea. I wanna get started to use the &amp;quot;&amp;quot;&amp;quot;tricks&amp;quot;&amp;quot;&amp;quot; that some of you use in order to use AI with the highest performace and at lowest rate.&lt;/p&gt; &lt;p&gt;Looking forward to hear ideas, recommendations or guidance!&lt;/p&gt; &lt;p&gt;Thanks a lot in advance, and I wish y'all a wonderful day :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Explanation_7774"&gt; /u/Dry_Explanation_7774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pggss8/im_tired_of_claude_limits_whats_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pggss8/im_tired_of_claude_limits_whats_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pggss8/im_tired_of_claude_limits_whats_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T12:04:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgrevr</id>
    <title>Pro tip for Local LLM usage on the phone</title>
    <updated>2025-12-07T19:37:46+00:00</updated>
    <author>
      <name>/u/Seglem</name>
      <uri>https://old.reddit.com/user/Seglem</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgrevr/pro_tip_for_local_llm_usage_on_the_phone/"&gt; &lt;img alt="Pro tip for Local LLM usage on the phone" src="https://a.thumbs.redditmedia.com/AJCKYWs4WfxH5ecM1yeAO1JYZeQEd6VYjDEAyblFIB0.jpg" title="Pro tip for Local LLM usage on the phone" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have it plugged in a charger and chat/work away. By classifying your LLM app of choice as a game, you can access the pause charging when &amp;quot;playing&amp;quot; in order to not heat up and throttle performance. But they use the power from the charger directly, instead of going through the battery, saving heat, battery cycles/wear and keeping the performance fast and the phone cooler. &lt;/p&gt; &lt;p&gt;I've also got a BodyGuardz Paradigm Pro case for my s25ultra, with better cooling than 99% of cases while protecting. And I sometimes use Baseus MagPro II. It has a fan so the charging and phone is cool &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Seglem"&gt; /u/Seglem &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pgrevr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgrevr/pro_tip_for_local_llm_usage_on_the_phone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgrevr/pro_tip_for_local_llm_usage_on_the_phone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T19:37:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgdh8q</id>
    <title>[ Removed by Reddit ]</title>
    <updated>2025-12-07T08:35:02+00:00</updated>
    <author>
      <name>/u/NandaVegg</name>
      <uri>https://old.reddit.com/user/NandaVegg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;[ Removed by Reddit on account of violating the &lt;a href="/help/contentpolicy"&gt;content policy&lt;/a&gt;. ]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NandaVegg"&gt; /u/NandaVegg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdh8q/removed_by_reddit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdh8q/removed_by_reddit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgdh8q/removed_by_reddit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T08:35:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgvhal</id>
    <title>mbzuai ifm releases Open 70b model - beats qwen-2.5</title>
    <updated>2025-12-07T22:23:17+00:00</updated>
    <author>
      <name>/u/Powerful-Sail-8826</name>
      <uri>https://old.reddit.com/user/Powerful-Sail-8826</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/LLM360/K2-V2-Instruct"&gt;https://huggingface.co/LLM360/K2-V2-Instruct&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Sail-8826"&gt; /u/Powerful-Sail-8826 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgvhal/mbzuai_ifm_releases_open_70b_model_beats_qwen25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgvhal/mbzuai_ifm_releases_open_70b_model_beats_qwen25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgvhal/mbzuai_ifm_releases_open_70b_model_beats_qwen25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T22:23:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pg8ix9</id>
    <title>My little decentralized Locallama setup, 216gb VRAM</title>
    <updated>2025-12-07T03:47:31+00:00</updated>
    <author>
      <name>/u/Goldkoron</name>
      <uri>https://old.reddit.com/user/Goldkoron</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8ix9/my_little_decentralized_locallama_setup_216gb_vram/"&gt; &lt;img alt="My little decentralized Locallama setup, 216gb VRAM" src="https://preview.redd.it/o1o7ekxycp5g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66f62e54e1c923ea71f1a1d46415562ffdcbc1ba" title="My little decentralized Locallama setup, 216gb VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Goldkoron"&gt; /u/Goldkoron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o1o7ekxycp5g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8ix9/my_little_decentralized_locallama_setup_216gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pg8ix9/my_little_decentralized_locallama_setup_216gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T03:47:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgnj1q</id>
    <title>Aquif 3.5 Max 1205 (42B-A3B)</title>
    <updated>2025-12-07T17:08:15+00:00</updated>
    <author>
      <name>/u/Holiday_Purpose_3166</name>
      <uri>https://old.reddit.com/user/Holiday_Purpose_3166</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgnj1q/aquif_35_max_1205_42ba3b/"&gt; &lt;img alt="Aquif 3.5 Max 1205 (42B-A3B)" src="https://b.thumbs.redditmedia.com/NEG8P3hqBPVNg2ugj-hAZZUETF03fHVZR0b3FQKZD7g.jpg" title="Aquif 3.5 Max 1205 (42B-A3B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Aquif 3.5 Max 1205 is out and seems much better than the previous one on some work.&lt;/p&gt; &lt;p&gt;No tool call problems so far (Aider or Kilocode) but as usual, early to tell. &lt;/p&gt; &lt;p&gt;It did fix some FE issues I had in a single-shot where Qwen3-Coder-30B or Aquif 3.5 Plus needed a couple turns - Devstral 2507 still managed but slower.&lt;/p&gt; &lt;p&gt;Nice one to Aquif and thanks Noctrex for the GGUF.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/reqwqu4cdt5g1.png?width=1403&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35eac71c387b9ebda5e9e2f99e4baa70ac874ab2"&gt;https://preview.redd.it/reqwqu4cdt5g1.png?width=1403&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35eac71c387b9ebda5e9e2f99e4baa70ac874ab2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Original: &lt;a href="https://huggingface.co/aquif-ai/aquif-3.5-Max-1205"&gt;https://huggingface.co/aquif-ai/aquif-3.5-Max-1205&lt;/a&gt;&lt;br /&gt; MXFP4: &lt;a href="https://huggingface.co/noctrex/aquif-3.5-Max-1205-MXFP4_MOE-GGUF"&gt;https://huggingface.co/noctrex/aquif-3.5-Max-1205-MXFP4_MOE-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit 1:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;As much I appreciate OSS development, The model seems to be allegedly copied from DavidAU with no credits - as seen on the other Aquif Plus and Max. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The reasoning tokens don't seem to work, unless you use [ironically] DavidAU system prompt, or use the correct chat template. I've done slight adjustment to close the tags properly and ensure all other blocks are also closed. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt; REASONING Enable deep thinking subroutine. You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside &amp;lt;thinking&amp;gt;thinking&amp;lt;/thinking&amp;gt; tags, and then provide your solution or response to the problem. Enclose code blocks with triple back ticks. &lt;/code&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The model seems to be doing ok in small refactors in NextJS, where GPT-OSS-120B and Devstral Small 1.1 2507 succeeds in a single shot in one of my repos. However, it loops reading in larger jobs - will wait tomorrow to check if thinking tags help that or is flop. &lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Purpose_3166"&gt; /u/Holiday_Purpose_3166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgnj1q/aquif_35_max_1205_42ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgnj1q/aquif_35_max_1205_42ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgnj1q/aquif_35_max_1205_42ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T17:08:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgv2fi</id>
    <title>Unimpressed with Mistral Large 3 675B</title>
    <updated>2025-12-07T22:06:04+00:00</updated>
    <author>
      <name>/u/notdba</name>
      <uri>https://old.reddit.com/user/notdba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From initial testing (coding related), this seems to be the new llama4.&lt;/p&gt; &lt;p&gt;The accusation from an ex-employee few months ago looks legit now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/suchenzang/status/1954973424486608928"&gt;https://x.com/suchenzang/status/1954973424486608928&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://36kr.com/p/3428277839465857"&gt;https://36kr.com/p/3428277839465857&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;No idea whether the new Mistral Large 3 675B was indeed trained from scratch, or &amp;quot;shell-wrapped&amp;quot; on top of DSV3 (i.e. like Pangu: &lt;a href="https://github.com/HW-whistleblower/True-Story-of-Pangu"&gt;https://github.com/HW-whistleblower/True-Story-of-Pangu&lt;/a&gt; ). Probably from scratch as it is much worse than DSV3.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notdba"&gt; /u/notdba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgv2fi/unimpressed_with_mistral_large_3_675b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgv2fi/unimpressed_with_mistral_large_3_675b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgv2fi/unimpressed_with_mistral_large_3_675b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T22:06:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pgsodd</id>
    <title>ServiceNow-AI/Apriel-1.6-15b-Thinker · Hugging Face</title>
    <updated>2025-12-07T20:28:17+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgsodd/servicenowaiapriel1615bthinker_hugging_face/"&gt; &lt;img alt="ServiceNow-AI/Apriel-1.6-15b-Thinker · Hugging Face" src="https://external-preview.redd.it/KDS1GGF2jYTqD2RRTZIBI42Bz7Kwl8ZrRXizgMq0fZU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0d2bc4dce3f56a2b8d2f8f355fdfb5b4072551a" title="ServiceNow-AI/Apriel-1.6-15b-Thinker · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Apriel-1.6-15B-Thinker&lt;/strong&gt; is an updated multimodal reasoning model in ServiceNow’s Apriel SLM series, building on &lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-1.5-15b-Thinker"&gt;&lt;strong&gt;Apriel-1.5-15B-Thinker&lt;/strong&gt;&lt;/a&gt;. With significantly improved text and image reasoning capabilities, Apriel-1.6 achieves competitive performance against models up to 10x its size. Like its predecessor, it benefits from extensive continual pretraining across both text and image domains. We further perform post-training, focusing on Supervised Finetuning (SFT) and Reinforcement Learning (RL). Apriel-1.6 obtains frontier performance without sacrificing reasoning token efficiency. The model improves or maintains task performance in comparison with Apriel-1.5-15B-Thinker, while &lt;em&gt;reducing reasoning token usage by more than 30%&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Achieves a score of &lt;strong&gt;57&lt;/strong&gt; on the Artificial Analysis index outperforming models like Gemini 2.5 Flash, Claude Haiku 4.5 and GPT OSS 20b. It obtains a score on par with Qwen3 235B A22B, while being signficantly more efficient.&lt;/li&gt; &lt;li&gt;Scores &lt;strong&gt;69&lt;/strong&gt; on Tau2 Bench Telecom and &lt;strong&gt;69&lt;/strong&gt; on IFBench, which are key benchmarks for the enterprise domain.&lt;/li&gt; &lt;li&gt;At 15B parameters, the model fits on a single GPU, making it highly memory-efficient.&lt;/li&gt; &lt;li&gt;Based on community feedback on Apriel-1.5-15b-Thinker, we simplified the chat template by removing redundant tags and introduced four special tokens to the tokenizer (&lt;code&gt;&amp;lt;tool_calls&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;/tool_calls&amp;gt;&lt;/code&gt;, &lt;code&gt;[BEGIN FINAL RESPONSE]&lt;/code&gt;, &lt;code&gt;&amp;lt;|end|&amp;gt;&lt;/code&gt;) for easier output parsing.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-1.6-15b-Thinker"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pgsodd/servicenowaiapriel1615bthinker_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pgsodd/servicenowaiapriel1615bthinker_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-07T20:28:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
