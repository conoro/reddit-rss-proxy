<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-25T14:50:28+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pvdbd8</id>
    <title>I built an open-source tool to "lint" your RAG dataset before indexing (Dedup, PII, Coverage Gaps)</title>
    <updated>2025-12-25T13:23:43+00:00</updated>
    <author>
      <name>/u/Federal_Floor7900</name>
      <uri>https://old.reddit.com/user/Federal_Floor7900</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvdbd8/i_built_an_opensource_tool_to_lint_your_rag/"&gt; &lt;img alt="I built an open-source tool to &amp;quot;lint&amp;quot; your RAG dataset before indexing (Dedup, PII, Coverage Gaps)" src="https://b.thumbs.redditmedia.com/g5l36hkwDI7qqXLXJqrkovItWBYBSd4cMCmx5rOvhNQ.jpg" title="I built an open-source tool to &amp;quot;lint&amp;quot; your RAG dataset before indexing (Dedup, PII, Coverage Gaps)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Like many of you, I‚Äôve spent the last few months debugging RAG pipelines. I realized that 90% of the time when my model hallucinated, it wasn't the LLM's fault, it was the retrieval. My vector database was full of duplicate policies, &amp;quot;Page 1 of 5&amp;quot; headers, and sometimes accidental PII.&lt;/p&gt; &lt;p&gt;I wanted something like &lt;code&gt;pandas-profiling&lt;/code&gt; but for unstructured RAG datasets. I couldn't find one that ran locally and handled security, so I built &lt;strong&gt;rag-corpus-profiler&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It‚Äôs a CLI tool that audits your documents (JSON, DOCX, TXT) &lt;em&gt;before&lt;/em&gt; you embed them.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it actually does:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Semantic Deduplication:&lt;/strong&gt; It uses &lt;code&gt;all-MiniLM-L6-v2&lt;/code&gt; locally to identify chunks that &lt;em&gt;mean&lt;/em&gt; the same thing, even if the wording is different. I found this reduced my token usage/cost by ~20% in testing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PII Gatekeeping:&lt;/strong&gt; It runs a regex scan for Emails, Phone Numbers, and High-Entropy Secrets (AWS/OpenAI keys) to prevent data leaks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Coverage Gap Analysis:&lt;/strong&gt; You can feed it a list of user queries (e.g., &lt;code&gt;queries.txt&lt;/code&gt;), and it calculates a &amp;quot;Blind Spot&amp;quot; report; telling you which user intents your current dataset &lt;em&gt;cannot&lt;/em&gt; answer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CI/CD Mode:&lt;/strong&gt; Added a &lt;code&gt;--strict&lt;/code&gt; flag that returns exit code 1 if PII is found. You can drop this into a GitHub Action to block bad data from reaching production.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Embeddings:&lt;/strong&gt; &lt;code&gt;sentence-transformers&lt;/code&gt; (runs on CPU or MPS/CUDA).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parsing:&lt;/strong&gt; &lt;code&gt;python-docx&lt;/code&gt; for Word docs, standard JSON/Text loaders.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reporting:&lt;/strong&gt; Generates a standalone HTML dashboard (no server needed).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It‚Äôs fully open-source (MIT). I‚Äôd love to hear if this fits into your ingestion pipelines or what other &amp;quot;sanity checks&amp;quot; you usually run on your corpus. &lt;/p&gt; &lt;p&gt;A github Star is appreciated &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/aashirpersonal/rag-corpus-profiler"&gt;https://github.com/aashirpersonal/rag-corpus-profiler&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nfep1gcxpc9g1.png?width=3048&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=13b0ccd02e4205105ce97044001d4d3de6b91c31"&gt;sample report&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal_Floor7900"&gt; /u/Federal_Floor7900 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvdbd8/i_built_an_opensource_tool_to_lint_your_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvdbd8/i_built_an_opensource_tool_to_lint_your_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvdbd8/i_built_an_opensource_tool_to_lint_your_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T13:23:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pullo0</id>
    <title>Hmm all reference to open-sourcing has been removed for Minimax M2.1...</title>
    <updated>2025-12-24T11:48:37+00:00</updated>
    <author>
      <name>/u/Responsible_Fig_1271</name>
      <uri>https://old.reddit.com/user/Responsible_Fig_1271</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Funny how yesterday this page &lt;a href="https://www.minimax.io/news/minimax-m21"&gt;https://www.minimax.io/news/minimax-m21&lt;/a&gt; had a statement that weights would be open-sourced on Huggingface and even a discussion of how to run locally on vLLM and SGLang. There was even a (broken but soon to be functional) HF link for the repo...&lt;/p&gt; &lt;p&gt;Today that's all gone.&lt;/p&gt; &lt;p&gt;Has MiniMax decided to go API only? Seems like they've backtracked on open-sourcing this one. Maybe they realized it's so good that it's time to make some $$$ :( Would be sad news for this community and a black mark against MiniMax.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible_Fig_1271"&gt; /u/Responsible_Fig_1271 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pullo0/hmm_all_reference_to_opensourcing_has_been/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pullo0/hmm_all_reference_to_opensourcing_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pullo0/hmm_all_reference_to_opensourcing_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T11:48:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv4aug</id>
    <title>Speed vs. Substance: Is Sparse Attention Making LLMs "Dumber"?</title>
    <updated>2025-12-25T03:31:53+00:00</updated>
    <author>
      <name>/u/madSaiyanUltra_9789</name>
      <uri>https://old.reddit.com/user/madSaiyanUltra_9789</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv4aug/speed_vs_substance_is_sparse_attention_making/"&gt; &lt;img alt="Speed vs. Substance: Is Sparse Attention Making LLMs &amp;quot;Dumber&amp;quot;?" src="https://b.thumbs.redditmedia.com/2EN6Q9apTQyZGGIIsBFAbRJELNDiHTqjNCnesTawAWs.jpg" title="Speed vs. Substance: Is Sparse Attention Making LLMs &amp;quot;Dumber&amp;quot;?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, my first post!!&lt;/p&gt; &lt;p&gt;I've been digging into the latest advancements in attention mechanisms, and it's fascinating how the field is evolving. We're seeing a clear trend towards efficiency: methods like DeepSeek's DSA (&lt;a href="https://arxiv.org/pdf/2512.02556"&gt;DeepSeek Sparse Attention&lt;/a&gt;) and &lt;a href="https://arxiv.org/pdf/2505.06708"&gt;Qwen's Gated Attention&lt;/a&gt; are revolutionizing inference speed by selectively focusing on &amp;quot;important&amp;quot; tokens.&lt;/p&gt; &lt;p&gt;The core idea is brilliant: instead of processing every single token in a sequence, these models use a &amp;quot;lightning indexer&amp;quot; (DeepSeek) or a gating mechanism (Qwen) to filter out less relevant information. This drastically reduces computational complexity, allowing for faster responses and better handling of long contexts.&lt;/p&gt; &lt;p&gt;However, this efficiency comes with a question that's been nagging me: are we potentially sacrificing some of the model's ability to grasp the full nuance of a prompt?&lt;/p&gt; &lt;p&gt;The Qwen paper, for instance, introduces &amp;quot;Gated Attention&amp;quot; which introduces input-dependent sparsity. While this mitigates the &amp;quot;attention sink&amp;quot; problem and improves training stability, it inherently means the model is not considering all tokens equally. Similarly, DeepSeek's DSA uses a top-k selection mechanism, effectively creating a &amp;quot;sparse&amp;quot; view of the input.&lt;/p&gt; &lt;p&gt;I find myself wondering: when a model is trained to ignore a significant portion of the input by design, does it lose some of the subtle connections or contextual understanding that a fully dense attention mechanism might capture? The papers show clear benefits in speed and stability, but I'm curious about the qualitative impact.&lt;/p&gt; &lt;p&gt;Has anyone else noticed a difference in how these newer, sparse-attention models &amp;quot;understand&amp;quot; complex prompts compared to their dense-attention predecessors? I'm not saying it's a definitive loss, but it feels like there might be a subtle trade-off happening here.&lt;/p&gt; &lt;p&gt;What are your thoughts? Am I overthinking this, or is there a genuine shift in how these models process information?&lt;/p&gt; &lt;p&gt;Cheers,&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m5ir80osr99g1.png?width=1255&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e9955658e9431c22f2b613339444bca8e572a2d"&gt;https://preview.redd.it/m5ir80osr99g1.png?width=1255&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e9955658e9431c22f2b613339444bca8e572a2d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madSaiyanUltra_9789"&gt; /u/madSaiyanUltra_9789 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv4aug/speed_vs_substance_is_sparse_attention_making/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv4aug/speed_vs_substance_is_sparse_attention_making/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pv4aug/speed_vs_substance_is_sparse_attention_making/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T03:31:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv7a2d</id>
    <title>Fine-tuning gpt-oss-20B on a Ryzen 5950X because ROCm wouldn‚Äôt cooperate with bf16.</title>
    <updated>2025-12-25T06:39:08+00:00</updated>
    <author>
      <name>/u/Double-Primary-2871</name>
      <uri>https://old.reddit.com/user/Double-Primary-2871</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv7a2d/finetuning_gptoss20b_on_a_ryzen_5950x_because/"&gt; &lt;img alt="Fine-tuning gpt-oss-20B on a Ryzen 5950X because ROCm wouldn‚Äôt cooperate with bf16." src="https://preview.redd.it/48vgazvupa9g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=255ddc4bddb8526e72813f4ff865967cdbcb37ae" title="Fine-tuning gpt-oss-20B on a Ryzen 5950X because ROCm wouldn‚Äôt cooperate with bf16." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;at 1am.&lt;/p&gt; &lt;p&gt;I am fine-tuning my personal AI, into a gpt-oss-20b model, via LoRA, on a Ryzen 5950x CPU.&lt;/p&gt; &lt;p&gt;I had to painstakingly deal with massive axolotl errors, venv and python version hell, yaml misconfigs, even fought with my other ai assistant, whom literally told me this couldn‚Äôt be done on my system‚Ä¶. for hours and hours, for over a week.&lt;/p&gt; &lt;p&gt;Can‚Äôt fine-tune with my radeon 7900XT because of bf16 kernel issues with ROCm on axolotl. I literally even tried to rent an h100 to help, and ran into serious roadblocks.&lt;/p&gt; &lt;p&gt;So the solution was for me to convert the mxfp4 (bf16 format) weights back to fp32 and tell axolotl to stop downcasting back fp16. Sure this will take days to compute all three of the shards, but after days of banging my head against the nearest convenient wall and keyboard, I finally got this s-o-b to work.&lt;/p&gt; &lt;p&gt;üòÅ also hi, new here. just wanted to share my story.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Double-Primary-2871"&gt; /u/Double-Primary-2871 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/48vgazvupa9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv7a2d/finetuning_gptoss20b_on_a_ryzen_5950x_because/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pv7a2d/finetuning_gptoss20b_on_a_ryzen_5950x_because/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T06:39:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv1ibv</id>
    <title>Planning to upgrade from 3060 to 5070 Ti for Local AI. Thoughts?</title>
    <updated>2025-12-25T00:45:02+00:00</updated>
    <author>
      <name>/u/shoonee_balavolka</name>
      <uri>https://old.reddit.com/user/shoonee_balavolka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv1ibv/planning_to_upgrade_from_3060_to_5070_ti_for/"&gt; &lt;img alt="Planning to upgrade from 3060 to 5070 Ti for Local AI. Thoughts?" src="https://preview.redd.it/q5nnyofoy89g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=079138e819f58946f12885281101f247e3a30aa9" title="Planning to upgrade from 3060 to 5070 Ti for Local AI. Thoughts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RAM prices have been crazy lately, right? I have a feeling other PC parts are going to skyrocket next year too, so I want to upgrade before that happens. ‚ÄãI run local AI models like Stable Diffusion, Gemma 3, and Qwen at home. I use them for fun, but also to assist with my hobby game development. ‚ÄãCurrently, I'm rocking an RTX 3060 12GB. Honestly, I'd love to go straight for the 5090, but I fund my PC upgrades purely through ad revenue from my games... and the budget just isn't there yet. ‚ÄãSo I'm eyeing the 5070 Ti. It seems like the best bang for the buck right now. I'm expecting a slight VRAM bump and maybe a 3-4x speed increase thanks to the higher core count. ‚ÄãDo you guys think the 5070 Ti is the right move in this situation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shoonee_balavolka"&gt; /u/shoonee_balavolka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q5nnyofoy89g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv1ibv/planning_to_upgrade_from_3060_to_5070_ti_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pv1ibv/planning_to_upgrade_from_3060_to_5070_ti_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T00:45:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1puzin1</id>
    <title>Llama.cpp multiple model presets appreciation post</title>
    <updated>2025-12-24T22:55:44+00:00</updated>
    <author>
      <name>/u/robiinn</name>
      <uri>https://old.reddit.com/user/robiinn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently Llama.cpp &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17859"&gt;added support&lt;/a&gt; for &lt;a href="https://github.com/ggml-org/llama.cpp/tree/master/tools/server#model-presets"&gt;model presets&lt;/a&gt;, which is a awsome feature that allow model loading and switching, and I have not seen much talk about. I would like to show my appreciation to the developers that are working on Llama.cpp and also share that the &lt;a href="https://github.com/ggml-org/llama.cpp/tree/master/tools/server#model-presets"&gt;model preset feature&lt;/a&gt; exists to switch models. &lt;/p&gt; &lt;p&gt;A short guide of how to use it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Get your hands on a recent version of &lt;code&gt;llama-server&lt;/code&gt; from Llama.cpp.&lt;/li&gt; &lt;li&gt;Create a &lt;code&gt;.ini&lt;/code&gt; file. I named my file &lt;code&gt;models.ini&lt;/code&gt;. &lt;/li&gt; &lt;li&gt;Add the content of the models to your &lt;code&gt;.ini&lt;/code&gt; file. See either the &lt;a href="https://github.com/ggml-org/llama.cpp/tree/master/tools/server#model-presets"&gt;README&lt;/a&gt; or my example below. The values in the &lt;code&gt;[*]&lt;/code&gt; section is shared between each model, and &lt;code&gt;[Devstral2:Q5_K_XL]&lt;/code&gt; declares a new model.&lt;/li&gt; &lt;li&gt;Run &lt;code&gt;llama-server --models-preset &amp;lt;path to your.ini&amp;gt;/models.ini&lt;/code&gt; to start the server.&lt;/li&gt; &lt;li&gt;Optional: Try out the webui on &lt;a href="http://localhost:8080"&gt;&lt;code&gt;http://localhost:8080&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here is my &lt;code&gt;models.ini&lt;/code&gt; file as an example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;version = 1 [*] flash-attn = on n-gpu-layers = 99 c = 32768 jinja = true t = -1 b = 2048 ub = 2048 [Devstral2:Q5_K_XL] temp = 0.15 min-p = 0.01 model = /home/&amp;lt;name&amp;gt;/gguf/Devstral-Small-2-24B-Instruct-2512-UD-Q5_K_XL.gguf cache-type-v = q8_0 [Nemotron-3-nano:Q4_K_M] model = /home/&amp;lt;name&amp;gt;/gguf/Nemotron-3-Nano-30B-A3B-Q4_K_M.gguf c = 1048576 temp = 0.6 top-p = 0.95 chat-template-kwargs = {&amp;quot;enable_thinking&amp;quot;:true} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Thanks for me, I just wanted to share this with you all and I hope it helps someone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robiinn"&gt; /u/robiinn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puzin1/llamacpp_multiple_model_presets_appreciation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puzin1/llamacpp_multiple_model_presets_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puzin1/llamacpp_multiple_model_presets_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T22:55:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv04uy</id>
    <title>model: support MiMo-V2-Flash by ngxson ¬∑ Pull Request #18328 ¬∑ ggml-org/llama.cpp</title>
    <updated>2025-12-24T23:28:46+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv04uy/model_support_mimov2flash_by_ngxson_pull_request/"&gt; &lt;img alt="model: support MiMo-V2-Flash by ngxson ¬∑ Pull Request #18328 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/ljL76ES0ycKhBmmY2ipEg2qGxnKYFbw_FzMzU74PR0Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=951b663470ca3d64de2becd5786b3d589c6d0ac7" title="model: support MiMo-V2-Flash by ngxson ¬∑ Pull Request #18328 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18328"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv04uy/model_support_mimov2flash_by_ngxson_pull_request/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pv04uy/model_support_mimov2flash_by_ngxson_pull_request/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T23:28:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1puwi5o</id>
    <title>Deepseek will release a larger model next year</title>
    <updated>2025-12-24T20:25:01+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;THis is old news but, I forgot to mention this before.&lt;/p&gt; &lt;p&gt;This is from section 5, &lt;a href="https://arxiv.org/html/2512.02556v1#S5"&gt;https://arxiv.org/html/2512.02556v1#S5&lt;/a&gt; -&amp;quot; First, due to fewer total training FLOPs, the breadth of world knowledge in DeepSeek-V3.2 still lags behind that of leading proprietary models. We plan to address this knowledge gap in future iterations by scaling up the pre-training compute.&amp;quot;&lt;/p&gt; &lt;p&gt;I speculate it will be bigger than 1.6T params(maybe 1.7-2.5T) and have 95B-111B active params and at least trained 2.5-3x more tokens than now... Hopefully they will releases the weights for this. I also hope for a smaller version(maybe it won't happen)..&lt;/p&gt; &lt;p&gt;&amp;quot; Second, token efficiency remains a challenge; DeepSeek-V3.2 typically requires longer generation trajectories (i.e., more tokens) to match the output quality of models like Gemini-3.0-Pro. Future work will focus on optimizing the intelligence density of the model‚Äôs reasoning chains to improve efficiency. Third, solving complex tasks is still inferior to frontier models, motivating us to further refine our foundation model and post-training recipe.&amp;quot;&lt;/p&gt; &lt;p&gt;- They will increase the efficiency of its reasoning ie it will use less thinking tokens than before for the same task .&lt;/p&gt; &lt;p&gt;Also they will improve its abilities solving complex task, this probably means better reasoning and agentic tooling&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puwi5o/deepseek_will_release_a_larger_model_next_year/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puwi5o/deepseek_will_release_a_larger_model_next_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puwi5o/deepseek_will_release_a_larger_model_next_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T20:25:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv022d</id>
    <title>What is llama.cpp equivalent for image &amp; video gen?</title>
    <updated>2025-12-24T23:24:39+00:00</updated>
    <author>
      <name>/u/ClimateBoss</name>
      <uri>https://old.reddit.com/user/ClimateBoss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use &lt;strong&gt;llama.cpp&lt;/strong&gt; to generate text from GGUF models on a server offline. I can scp GGUF and run it and even build llama.cpp from source.&lt;/p&gt; &lt;p&gt;Most examples I found are setting up Gradio, using python scripts, and installing python pip packages or even running MacOS app (I use arch btw!)&lt;/p&gt; &lt;p&gt;What's a local cli for image &amp;amp; video gen? Text 2 Image and Image 2 Video if you dont want a UI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ClimateBoss"&gt; /u/ClimateBoss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv022d/what_is_llamacpp_equivalent_for_image_video_gen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv022d/what_is_llamacpp_equivalent_for_image_video_gen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pv022d/what_is_llamacpp_equivalent_for_image_video_gen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T23:24:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1puxg7h</id>
    <title>MiniMax M2.1 scores 43.4% on SWE-rebench (November)</title>
    <updated>2025-12-24T21:10:50+00:00</updated>
    <author>
      <name>/u/Fabulous_Pollution10</name>
      <uri>https://old.reddit.com/user/Fabulous_Pollution10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puxg7h/minimax_m21_scores_434_on_swerebench_november/"&gt; &lt;img alt="MiniMax M2.1 scores 43.4% on SWE-rebench (November)" src="https://preview.redd.it/s0vbt46vt79g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a1fcc806bfccb9370c298a67d419e024ce322b3" title="MiniMax M2.1 scores 43.4% on SWE-rebench (November)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;br /&gt; We added MiniMax M2.1 results to the December SWE-rebench update.&lt;/p&gt; &lt;p&gt;Please check the leaderboard: &lt;a href="https://swe-rebench.com/"&gt;https://swe-rebench.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We‚Äôll add GLM-4.7 and Gemini Flash 3 in the next release.&lt;br /&gt; By the way, we just released a large dataset of agentic trajectories and two checkpoints trained on it, based on Qwen models.&lt;br /&gt; Here‚Äôs the post:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1puxedb/we_release_67074_qwen3coder_openhands/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1puxedb/we_release_67074_qwen3coder_openhands/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Pollution10"&gt; /u/Fabulous_Pollution10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s0vbt46vt79g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puxg7h/minimax_m21_scores_434_on_swerebench_november/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puxg7h/minimax_m21_scores_434_on_swerebench_november/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T21:10:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvdwca</id>
    <title>Should I be switching to DoRA instead of LoRA?</title>
    <updated>2025-12-25T13:56:46+00:00</updated>
    <author>
      <name>/u/CartographerFun4221</name>
      <uri>https://old.reddit.com/user/CartographerFun4221</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(also posted to &lt;a href="/r/unsloth"&gt;/r/unsloth&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Should I switch to using DoRA instead of LoRA?&lt;/p&gt; &lt;p&gt;I've been training a small LLM on the medical field and have been doing CPT using full parameters. Due to this I've been limited to models around 3B in size (GPU poor, AWS creds almost ran out). I know LoRA won't be ideal for me, I have about 200M high quality tokens to do CPT with and I feel like LoRA will just not instill as much as I want. If I used DoRA, will I get as much benefit as full parameter fine-tuning? I'm okay with eating the slower processing costs because at least they'll be instances I can afford. &lt;/p&gt; &lt;p&gt;Additionally, should I be using DoRA for SFT too? Does each model need bespoke support upon release or is it more of a case of it being so new that the unsloth implementation could be improved? If the only downside right now is slower processing + maybe slightly more VRAM usage compared to LoRA, but gives similar performance to full parameter tuning then that's a win IMO. thoughts? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CartographerFun4221"&gt; /u/CartographerFun4221 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvdwca/should_i_be_switching_to_dora_instead_of_lora/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvdwca/should_i_be_switching_to_dora_instead_of_lora/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvdwca/should_i_be_switching_to_dora_instead_of_lora/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T13:56:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv8mqt</id>
    <title>I was waiting for Minimax and MiMo-V2-Flash arrived!!!</title>
    <updated>2025-12-25T08:10:38+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv8mqt/i_was_waiting_for_minimax_and_mimov2flash_arrived/"&gt; &lt;img alt="I was waiting for Minimax and MiMo-V2-Flash arrived!!!" src="https://b.thumbs.redditmedia.com/TuP_ZT9ePVWlrWIsTH0q4fp1KIrKls9IEOi6bqrgNPU.jpg" title="I was waiting for Minimax and MiMo-V2-Flash arrived!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/m8gg48gh5b9g1.png?width=1854&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ded00e01296c618dece05a1eb812bd4abacb8236"&gt;MiMo-V2-Flash llama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nice Christmas present guys! &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pv04uy/model_support_mimov2flash_by_ngxson_pull_request/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1pv04uy/model_support_mimov2flash_by_ngxson_pull_request/&lt;/a&gt; now merged!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/XiaomiMiMo/MiMo-V2-Flash"&gt;https://huggingface.co/XiaomiMiMo/MiMo-V2-Flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Merged!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv8mqt/i_was_waiting_for_minimax_and_mimov2flash_arrived/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv8mqt/i_was_waiting_for_minimax_and_mimov2flash_arrived/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pv8mqt/i_was_waiting_for_minimax_and_mimov2flash_arrived/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T08:10:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1puzo82</id>
    <title>Merry Christmas! üéÑ üéÅ</title>
    <updated>2025-12-24T23:03:48+00:00</updated>
    <author>
      <name>/u/Rare_Carry9799</name>
      <uri>https://old.reddit.com/user/Rare_Carry9799</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Merry Christmas! ü•≥&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rare_Carry9799"&gt; /u/Rare_Carry9799 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puzo82/merry_christmas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puzo82/merry_christmas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puzo82/merry_christmas/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T23:03:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pveluj</id>
    <title>Honestly, has anyone actually tried GLM 4.7 yet? (Not just benchmarks)</title>
    <updated>2025-12-25T14:35:15+00:00</updated>
    <author>
      <name>/u/Empty_Break_8792</name>
      <uri>https://old.reddit.com/user/Empty_Break_8792</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm seeing all these charts claiming GLM 4.7 is officially the ‚ÄúSonnet 4.5 and GPT-5.2 killer‚Äù for coding and math. The benchmarks look insane (84.8 on LiveCodeBench?!), but we all know how easy it is to game those for a release day hype cycle.&lt;/p&gt; &lt;p&gt;I‚Äôm specifically curious about using it as a daily driver for complex web development. Most of my work involves managing complex TypeScript code and refactoring legacy React code.&lt;/p&gt; &lt;p&gt;For those of you who have actually hooked the API into an agent like &lt;strong&gt;Kilo Code&lt;/strong&gt; or &lt;strong&gt;OpenCode&lt;/strong&gt; (or even just &lt;strong&gt;Cline&lt;/strong&gt; / &lt;strong&gt;Roo Code&lt;/strong&gt;), how is your experience with it? Please be honest i don't just believe the benchmarks. Tell me if you really use it, and with which agent?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Empty_Break_8792"&gt; /u/Empty_Break_8792 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pveluj/honestly_has_anyone_actually_tried_glm_47_yet_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pveluj/honestly_has_anyone_actually_tried_glm_47_yet_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pveluj/honestly_has_anyone_actually_tried_glm_47_yet_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T14:35:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv8qiv</id>
    <title>CVE-2025-51471 ‚Äì Ollama auth tokens can be stolen via malicious model URLs</title>
    <updated>2025-12-25T08:17:55+00:00</updated>
    <author>
      <name>/u/DueFaithlessness4550</name>
      <uri>https://old.reddit.com/user/DueFaithlessness4550</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you use Ollama with private or organization models, this is worth being aware&lt;/p&gt; &lt;p&gt;of.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CVE-2025-51471&lt;/strong&gt; allows an attacker-controlled model registry to capture&lt;/p&gt; &lt;p&gt;authentication tokens by abusing the registry authentication flow.&lt;/p&gt; &lt;p&gt;This happens during a normal &lt;code&gt;ollama pull&lt;/code&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No malware.&lt;/li&gt; &lt;li&gt;No exploit chain.&lt;/li&gt; &lt;li&gt;Just a trust boundary issue.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;I reproduced this on the latest version&lt;/strong&gt; and recorded the video showing&lt;/p&gt; &lt;p&gt;the token capture and attack flow.&lt;/p&gt; &lt;p&gt;Original discovery credit goes to FuzzingLabs:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huntr.com/bounties/94eea285-fd65-4e01-a035-f533575ebdc2"&gt;https://huntr.com/bounties/94eea285-fd65-4e01-a035-f533575ebdc2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PoC repo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ajtazer/CVE-2025-51471-PoC"&gt;https://github.com/ajtazer/CVE-2025-51471-PoC&lt;/a&gt;&lt;/p&gt; &lt;p&gt;YT Video:&lt;br /&gt; &lt;a href="https://youtu.be/kC80FSrWbNk"&gt;https://youtu.be/kC80FSrWbNk&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Fix PR (still open):&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama/pull/10750"&gt;https://github.com/ollama/ollama/pull/10750&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DueFaithlessness4550"&gt; /u/DueFaithlessness4550 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv8qiv/cve202551471_ollama_auth_tokens_can_be_stolen_via/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv8qiv/cve202551471_ollama_auth_tokens_can_be_stolen_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pv8qiv/cve202551471_ollama_auth_tokens_can_be_stolen_via/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T08:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvacv8</id>
    <title>Thoughts on picking up dual RTX 3090s at this point?</title>
    <updated>2025-12-25T10:10:53+00:00</updated>
    <author>
      <name>/u/Affectionate-Bid-650</name>
      <uri>https://old.reddit.com/user/Affectionate-Bid-650</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know, you guys probably get this question a lot, but could use some help like always.&lt;/p&gt; &lt;p&gt;I'm currently running an RTX 4080 and have been playing around with Qwen 3 14B and similar LLaMA models. But now I really want to try running larger models, specifically in the 70B range.&lt;/p&gt; &lt;p&gt;I'm a native Korean speaker, and honestly, the Korean performance on 14B models is pretty lackluster. I've seen benchmarks suggesting that 30B+ models are decent, but my 4080 can't even touch those due to VRAM limits.&lt;/p&gt; &lt;p&gt;I know the argument for &amp;quot;just paying for an API&amp;quot; makes total sense, and that's actually why I'm hesitating so much.&lt;/p&gt; &lt;p&gt;Anyway, here is the main question: If I invest around $800 (swapping my 4080 for two used 3090s), will I be able to run this setup for a long time?&lt;/p&gt; &lt;p&gt;It looks like things are shifting towards the unified memory era recently, and I really don't want my dual 3090 setup to become obsolete overnight.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Affectionate-Bid-650"&gt; /u/Affectionate-Bid-650 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvacv8/thoughts_on_picking_up_dual_rtx_3090s_at_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvacv8/thoughts_on_picking_up_dual_rtx_3090s_at_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvacv8/thoughts_on_picking_up_dual_rtx_3090s_at_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T10:10:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvaqp0</id>
    <title>Strix Halo First Impressions</title>
    <updated>2025-12-25T10:37:25+00:00</updated>
    <author>
      <name>/u/Fit-Produce420</name>
      <uri>https://old.reddit.com/user/Fit-Produce420</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's awesome for LLMs.&lt;/p&gt; &lt;p&gt;It's not fast for dense models, but it's decent with moe models.&lt;/p&gt; &lt;p&gt;I run devstral 2 123b (iq4_xs) in kilo code (dense model) and dang it's smart, makes me think the free tier of api are about the same quant/context (I have 128k locally). (3 t/s, haven't optimized anything just up and running)&lt;/p&gt; &lt;p&gt;But, gpt-oss 120b is where this really flies. It's native mxfp4, MoE and it's both capable and very fast. I hope more models are designed with native mxfp4, I think maybe mac already supported it and some other cards? (50+ t/s)&lt;/p&gt; &lt;p&gt;Anyway, it took a literal day of fucking around to get everything working but I have working local vs code, devstral2 or gptoss120bat 128k context. I have Wan 2.2 video generation up and running. Qwen image and qwen edit up and running.&lt;/p&gt; &lt;p&gt;Next I'm looking into Lora training.&lt;/p&gt; &lt;p&gt;All in all if you are a patient person and like getting fucked in the ass by rocm or Vulcan at every turn then how else do you get 112Gb of usable VRAM for the price? Software stack sucks.&lt;/p&gt; &lt;p&gt;I did install steam and it games just fine, 1080P ran better than steam deck for recent major titles.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fit-Produce420"&gt; /u/Fit-Produce420 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvaqp0/strix_halo_first_impressions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvaqp0/strix_halo_first_impressions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvaqp0/strix_halo_first_impressions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T10:37:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv2wwm</id>
    <title>FYI GLM 4.7 is way more censored than 4.6.</title>
    <updated>2025-12-25T02:08:11+00:00</updated>
    <author>
      <name>/u/bigman11</name>
      <uri>https://old.reddit.com/user/bigman11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;4.6 was excellent at adult writing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigman11"&gt; /u/bigman11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv2wwm/fyi_glm_47_is_way_more_censored_than_46/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv2wwm/fyi_glm_47_is_way_more_censored_than_46/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pv2wwm/fyi_glm_47_is_way_more_censored_than_46/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T02:08:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv2cnz</id>
    <title>All of the major open weight labs have shifted to large params general models instead of smaller, more focused models. By this time next year, there won‚Äôt be much ‚Äúlocal‚Äù about this sub unless the paradigm shifts to smaller models good at specific domains.</title>
    <updated>2025-12-25T01:34:36+00:00</updated>
    <author>
      <name>/u/LocoMod</name>
      <uri>https://old.reddit.com/user/LocoMod</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It‚Äôs happening very openly but very subtly. The champions of open weight models are slowly increasing their sizes to the point a very small portion of this sub can run them locally. An even smaller portion can run them as benchmarked (no quants). Many are now having to resort to Q3 and below, which will have a significant impact compared to what is marketed. Now, without any other recourse, those that cannot access or afford the more capable closed models are paying pennies for open weight models hosted by the labs themselves. This is the plan of course.&lt;/p&gt; &lt;p&gt;Given the cost of memory and other components many of us can no longer afford even a mid tier upgrade using modern components. The second hand market isn‚Äôt fairing much better.&lt;/p&gt; &lt;p&gt;The only viable way forward for local tinkerers are models that can fit between 16 to 32GB of vram.&lt;/p&gt; &lt;p&gt;The only way most of us will be able to run models locally will be to fine tune, crowd fund, or ‚Ä¶ ? smaller more focused models that can still remain competitive in specific domains vs general frontier models.&lt;/p&gt; &lt;p&gt;A capable coding model. A capable creative writing model. A capable math model. Etc.&lt;/p&gt; &lt;p&gt;We‚Äôre not going to get competitive local models from ‚Äúwell funded‚Äù labs backed by Big Co. A distinction will soon become clear that ‚Äúopen weights‚Äù does not equal ‚Äúlocal‚Äù.&lt;/p&gt; &lt;p&gt;Remember the early days? Dolphin, Hermes, etc.&lt;/p&gt; &lt;p&gt;We need to go back to that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LocoMod"&gt; /u/LocoMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv2cnz/all_of_the_major_open_weight_labs_have_shifted_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv2cnz/all_of_the_major_open_weight_labs_have_shifted_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pv2cnz/all_of_the_major_open_weight_labs_have_shifted_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T01:34:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv5shc</id>
    <title>Thoughts ?</title>
    <updated>2025-12-25T05:03:01+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv5shc/thoughts/"&gt; &lt;img alt="Thoughts ?" src="https://preview.redd.it/nlxk873n8a9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5952ca06d4f42460947176ff3841749598be91f3" title="Thoughts ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nlxk873n8a9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv5shc/thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pv5shc/thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T05:03:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pux0yc</id>
    <title>We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found.</title>
    <updated>2025-12-24T20:50:16+00:00</updated>
    <author>
      <name>/u/vox-deorum</name>
      <uri>https://old.reddit.com/user/vox-deorum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/"&gt; &lt;img alt="We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found." src="https://b.thumbs.redditmedia.com/4t8jQ0I5zf5Jo9d6oRUo6_gfae4uPfqghHDLniBqMTU.jpg" title="We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/zaib4up4s79g1.gif"&gt;GLM-4.6 Playing Civilization V + Vox Populi (Replay)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We had GPT-OSS-120B and GLM-4.6 playing 1,408 full Civilization V games (with Vox Populi/Community Patch activated). In a nutshell: LLMs set strategies for Civilization V's algorithmic AI to execute. Here is what we found:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/shjvvfpbq79g1.png?width=3187&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0175d5203c471ef332d54c2fe2b17d2369813e24"&gt;An overview of our system and results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; It is now possible to get open-source LLMs to play end-to-end Civilization V games (the m. They are not beating algorithm-based AI on a very simple prompt, but they do play quite differently. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The boring result:&lt;/strong&gt; With a simple prompt and little memory, both LLMs did slightly better in the best score they could achieve within each game (+1-2%), but slightly worse in win rates (-1~3%). Despite the large number of games run (2,207 in total, with 919 baseline games), neither metric is significant.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The surprising part:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Pure-LLM or pure-RL approaches &lt;a href="https://arxiv.org/abs/2401.10568"&gt;[1]&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2502.20807"&gt;[2]&lt;/a&gt; couldn't get an AI to play and survive full Civilization games. With our hybrid approach, LLMs can survive as long as the game goes (~97.5% LLMs, vs. ~97.3% the in-game AI). The model can be as small as OSS-20B in our internal test.&lt;/p&gt; &lt;p&gt;Moreover, the two models developed &lt;strong&gt;completely different playstyles&lt;/strong&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OSS-120B went full warmonger: +31.5% more Domination victories, -23% fewer Cultural victories compared to baseline&lt;/li&gt; &lt;li&gt;GLM-4.6 played more balanced, leaning into both Domination and Cultural strategies&lt;/li&gt; &lt;li&gt;Both models preferred &lt;strong&gt;Order&lt;/strong&gt; (&lt;strong&gt;communist-like&lt;/strong&gt;, ~24% more likely) ideology over &lt;strong&gt;Freedom&lt;/strong&gt; (democratic-like)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Cost/latency (OSS-120B):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~53,000 input / 1,500 output tokens per turn&lt;/li&gt; &lt;li&gt;&lt;strong&gt;~$0.86/game&lt;/strong&gt; (OpenRouter pricing as of 12/2025)&lt;/li&gt; &lt;li&gt;Input tokens scale linearly as the game state grows.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output stays flat: models don't automatically &amp;quot;think harder&amp;quot; in the late game.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Watch more:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Paper link: &lt;a href="https://arxiv.org/abs/2512.18564"&gt;https://arxiv.org/abs/2512.18564&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://civitas-john.github.io/vox-deorum-replay/?file=https://civitas-john.github.io/vox-deorum-replay/examples/1.Civ5Replay"&gt;Example save 1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://civitas-john.github.io/vox-deorum-replay/?file=https://civitas-john.github.io/vox-deorum-replay/examples/2.Civ5Replay"&gt;Example save 2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://civitas-john.github.io/vox-deorum-replay/?file=https://civitas-john.github.io/vox-deorum-replay/examples/3.Civ5Replay"&gt;Example save 3&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Try it yourself:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Vox Deorum system is 100% open-sourced and currently in beta testing&lt;/li&gt; &lt;li&gt;GitHub Repo: &lt;a href="https://github.com/CIVITAS-John/vox-deorum"&gt;https://github.com/CIVITAS-John/vox-deorum&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub Release: &lt;a href="https://github.com/CIVITAS-John/vox-deorum/releases"&gt;https://github.com/CIVITAS-John/vox-deorum/releases&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Works with any &lt;strong&gt;OpenAI-compatible local providers&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tccdt44oq79g1.png?width=2291&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0b8a4fe5871db4d2bf00f417acd13de3e688037f"&gt;We exposed the game as a MCP server, so your agents can play the game with you&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Your thoughts are greatly appreciated:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What's a good way to express the game state more efficiently? Consider a late-game turn where you have 20+ cities and 100+ units. Easily 50k+ tokens. Could multimodal help?&lt;/li&gt; &lt;li&gt;How can we get LLMs to play better? I have considered RAG, but there is really little data to &amp;quot;retrieve&amp;quot; here. Possibly self-play + self-reflection + long-term memory?&lt;/li&gt; &lt;li&gt;How are we going to design strategy games if LLMs are to play with you? I have put an LLM spokesperson for civilizations as an example, but there is surely more to do?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Join us:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I am hiring a PhD student for Fall '26, and we are expanding our game-related work rapidly. Shoot me a DM if you are interested!&lt;/li&gt; &lt;li&gt;I am happy to collaborate with anyone interested in furthering this line of work.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vox-deorum"&gt; /u/vox-deorum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T20:50:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1puyq9r</id>
    <title>Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record</title>
    <updated>2025-12-24T22:14:48+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/"&gt; &lt;img alt="Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record" src="https://external-preview.redd.it/jvYBeKyc_OM28gIhUnO6GEg0WpsjQzWHJmf00gHtJBw.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d32d447dab802e0a1aec9574d5282648b995cf17" title="Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/12/24/nvidia-buying-ai-chip-startup-groq-for-about-20-billion-biggest-deal.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T22:14:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv8dbb</id>
    <title>GLM 4.7 has now taken #2 on Website Arena</title>
    <updated>2025-12-25T07:52:46+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv8dbb/glm_47_has_now_taken_2_on_website_arena/"&gt; &lt;img alt="GLM 4.7 has now taken #2 on Website Arena" src="https://preview.redd.it/el2uxr8y2b9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=448cd56dc1a8abbef104c6aa6d319f88783428fb" title="GLM 4.7 has now taken #2 on Website Arena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is #1 overall amongst all open weight models and ranks just behind Gemini 3 Pro Preview, a 15-place jump from GLM 4.6&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/el2uxr8y2b9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv8dbb/glm_47_has_now_taken_2_on_website_arena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pv8dbb/glm_47_has_now_taken_2_on_website_arena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T07:52:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt50mt</id>
    <title>AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)</title>
    <updated>2025-12-22T17:12:21+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt; &lt;img alt="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" src="https://preview.redd.it/r06ch4zyfs8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0a10789f393f350618520fcb81174f3a3dae1c7" title="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r06ch4zyfs8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T17:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
</feed>
