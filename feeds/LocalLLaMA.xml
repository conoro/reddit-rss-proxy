<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-12T18:08:05+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ned2ai</id>
    <title>Building RAG systems at enterprise scale (20K+ docs): lessons from 10+ enterprise implementations</title>
    <updated>2025-09-11T16:16:40+00:00</updated>
    <author>
      <name>/u/Low_Acanthisitta7686</name>
      <uri>https://old.reddit.com/user/Low_Acanthisitta7686</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been building RAG systems for mid-size enterprise companies in the regulated space (100-1000 employees) for the past year and to be honest, this stuff is way harder than any tutorial makes it seem. Worked with around 10+ clients now - pharma companies, banks, law firms, consulting shops. Thought I'd share what actually matters vs all the basic info you read online.&lt;/p&gt; &lt;p&gt;Quick context: most of these companies had 10K-50K+ documents sitting in SharePoint hell or document management systems from 2005. Not clean datasets, not curated knowledge bases - just decades of business documents that somehow need to become searchable.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Document quality detection: the thing nobody talks about&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This was honestly the biggest revelation for me. Most tutorials assume your PDFs are perfect. Reality check: enterprise documents are absolute garbage.&lt;/p&gt; &lt;p&gt;I had one pharma client with research papers from 1995 that were scanned copies of typewritten pages. OCR barely worked. Mixed in with modern clinical trial reports that are 500+ pages with embedded tables and charts. Try applying the same chunking strategy to both and watch your system return complete nonsense.&lt;/p&gt; &lt;p&gt;Spent weeks debugging why certain documents returned terrible results while others worked fine. Finally realized I needed to score document quality before processing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clean PDFs (text extraction works perfectly): full hierarchical processing&lt;/li&gt; &lt;li&gt;Decent docs (some OCR artifacts): basic chunking with cleanup&lt;/li&gt; &lt;li&gt;Garbage docs (scanned handwritten notes): simple fixed chunks + manual review flags&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Built a simple scoring system looking at text extraction quality, OCR artifacts, formatting consistency. Routes documents to different processing pipelines based on score. This single change fixed more retrieval issues than any embedding model upgrade.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why fixed-size chunking is mostly wrong&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Every tutorial: &amp;quot;just chunk everything into 512 tokens with overlap!&amp;quot;&lt;/p&gt; &lt;p&gt;Reality: documents have structure. A research paper's methodology section is different from its conclusion. Financial reports have executive summaries vs detailed tables. When you ignore structure, you get chunks that cut off mid-sentence or combine unrelated concepts.&lt;/p&gt; &lt;p&gt;Had to build hierarchical chunking that preserves document structure:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Document level (title, authors, date, type)&lt;/li&gt; &lt;li&gt;Section level (Abstract, Methods, Results)&lt;/li&gt; &lt;li&gt;Paragraph level (200-400 tokens)&lt;/li&gt; &lt;li&gt;Sentence level for precision queries&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The key insight: query complexity should determine retrieval level. Broad questions stay at paragraph level. Precise stuff like &amp;quot;what was the exact dosage in Table 3?&amp;quot; needs sentence-level precision.&lt;/p&gt; &lt;p&gt;I use simple keyword detection - words like &amp;quot;exact&amp;quot;, &amp;quot;specific&amp;quot;, &amp;quot;table&amp;quot; trigger precision mode. If confidence is low, system automatically drills down to more precise chunks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Metadata architecture matters more than your embedding model&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is where I spent 40% of my development time and it had the highest ROI of anything I built.&lt;/p&gt; &lt;p&gt;Most people treat metadata as an afterthought. But enterprise queries are crazy contextual. A pharma researcher asking about &amp;quot;pediatric studies&amp;quot; needs completely different documents than someone asking about &amp;quot;adult populations.&amp;quot;&lt;/p&gt; &lt;p&gt;Built domain-specific metadata schemas:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For pharma docs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Document type (research paper, regulatory doc, clinical trial)&lt;/li&gt; &lt;li&gt;Drug classifications&lt;/li&gt; &lt;li&gt;Patient demographics (pediatric, adult, geriatric)&lt;/li&gt; &lt;li&gt;Regulatory categories (FDA, EMA)&lt;/li&gt; &lt;li&gt;Therapeutic areas (cardiology, oncology)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;For financial docs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Time periods (Q1 2023, FY 2022)&lt;/li&gt; &lt;li&gt;Financial metrics (revenue, EBITDA)&lt;/li&gt; &lt;li&gt;Business segments&lt;/li&gt; &lt;li&gt;Geographic regions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Avoid using LLMs for metadata extraction - they're inconsistent as hell. Simple keyword matching works way better. Query contains &amp;quot;FDA&amp;quot;? Filter for regulatory_category: &amp;quot;FDA&amp;quot;. Mentions &amp;quot;pediatric&amp;quot;? Apply patient population filters.&lt;/p&gt; &lt;p&gt;Start with 100-200 core terms per domain, expand based on queries that don't match well. Domain experts are usually happy to help build these lists.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;When semantic search fails (spoiler: a lot)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Pure semantic search fails way more than people admit. In specialized domains like pharma and legal, I see 15-20% failure rates, not the 5% everyone assumes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Main failure modes that drove me crazy:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Acronym confusion:&lt;/strong&gt; &amp;quot;CAR&amp;quot; means &amp;quot;Chimeric Antigen Receptor&amp;quot; in oncology but &amp;quot;Computer Aided Radiology&amp;quot; in imaging papers. Same embedding, completely different meanings. This was a constant headache.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Precise technical queries:&lt;/strong&gt; Someone asks &amp;quot;What was the exact dosage in Table 3?&amp;quot; Semantic search finds conceptually similar content but misses the specific table reference.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Cross-reference chains:&lt;/strong&gt; Documents reference other documents constantly. Drug A study references Drug B interaction data. Semantic search misses these relationship networks completely.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; Built hybrid approaches. Graph layer tracks document relationships during processing. After semantic search, system checks if retrieved docs have related documents with better answers.&lt;/p&gt; &lt;p&gt;For acronyms, I do context-aware expansion using domain-specific acronym databases. For precise queries, keyword triggers switch to rule-based retrieval for specific data points.&lt;/p&gt; &lt;h1&gt;Why I went with open source models (Qwen specifically)&lt;/h1&gt; &lt;p&gt;Most people assume GPT-4o or o3-mini are always better. But enterprise clients have weird constraints:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cost:&lt;/strong&gt; API costs explode with 50K+ documents and thousands of daily queries&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data sovereignty:&lt;/strong&gt; Pharma and finance can't send sensitive data to external APIs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Domain terminology:&lt;/strong&gt; General models hallucinate on specialized terms they weren't trained on&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Qwen QWQ-32B ended up working surprisingly well after domain-specific fine-tuning:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;85% cheaper than GPT-4o for high-volume processing&lt;/li&gt; &lt;li&gt;Everything stays on client infrastructure&lt;/li&gt; &lt;li&gt;Could fine-tune on medical/financial terminology&lt;/li&gt; &lt;li&gt;Consistent response times without API rate limits&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Fine-tuning approach was straightforward - supervised training with domain Q&amp;amp;A pairs. Created datasets like &amp;quot;What are contraindications for Drug X?&amp;quot; paired with actual FDA guideline answers. Basic supervised fine-tuning worked better than complex stuff like RAFT. Key was having clean training data.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Table processing: the hidden nightmare&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Enterprise docs are full of complex tables - financial models, clinical trial data, compliance matrices. Standard RAG either ignores tables or extracts them as unstructured text, losing all the relationships.&lt;/p&gt; &lt;p&gt;Tables contain some of the most critical information. Financial analysts need exact numbers from specific quarters. Researchers need dosage info from clinical tables. If you can't handle tabular data, you're missing half the value.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My approach:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Treat tables as separate entities with their own processing pipeline&lt;/li&gt; &lt;li&gt;Use heuristics for table detection (spacing patterns, grid structures)&lt;/li&gt; &lt;li&gt;For simple tables: convert to CSV. For complex tables: preserve hierarchical relationships in metadata&lt;/li&gt; &lt;li&gt;Dual embedding strategy: embed both structured data AND semantic description&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For the bank project, financial tables were everywhere. Had to track relationships between summary tables and detailed breakdowns too.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Production infrastructure reality check&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Tutorials assume unlimited resources and perfect uptime. Production means concurrent users, GPU memory management, consistent response times, uptime guarantees.&lt;/p&gt; &lt;p&gt;Most enterprise clients already had GPU infrastructure sitting around - unused compute or other data science workloads. Made on-premise deployment easier than expected.&lt;/p&gt; &lt;p&gt;Typically deploy 2-3 models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Main generation model (Qwen 32B) for complex queries&lt;/li&gt; &lt;li&gt;Lightweight model for metadata extraction&lt;/li&gt; &lt;li&gt;Specialized embedding model&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Used quantized versions when possible. Qwen QWQ-32B quantized to 4-bit only needed 24GB VRAM but maintained quality. Could run on single RTX 4090, though A100s better for concurrent users.&lt;/p&gt; &lt;p&gt;Biggest challenge isn't model quality - it's preventing resource contention when multiple users hit the system simultaneously. Use semaphores to limit concurrent model calls and proper queue management.&lt;/p&gt; &lt;h1&gt;Key lessons that actually matter&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Document quality detection first:&lt;/strong&gt; You cannot process all enterprise docs the same way. Build quality assessment before anything else.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Metadata &amp;gt; embeddings:&lt;/strong&gt; Poor metadata means poor retrieval regardless of how good your vectors are. Spend the time on domain-specific schemas.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Hybrid retrieval is mandatory:&lt;/strong&gt; Pure semantic search fails too often in specialized domains. Need rule-based fallbacks and document relationship mapping.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Tables are critical:&lt;/strong&gt; If you can't handle tabular data properly, you're missing huge chunks of enterprise value.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Infrastructure determines success:&lt;/strong&gt; Clients care more about reliability than fancy features. Resource management and uptime matter more than model sophistication.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The real talk&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Enterprise RAG is way more engineering than ML. Most failures aren't from bad models - they're from underestimating the document processing challenges, metadata complexity, and production infrastructure needs.&lt;/p&gt; &lt;p&gt;The demand is honestly crazy right now. Every company with substantial document repositories needs these systems, but most have no idea how complex it gets with real-world documents.&lt;/p&gt; &lt;p&gt;Anyway, this stuff is way harder than tutorials make it seem. The edge cases with enterprise documents will make you want to throw your laptop out the window. But when it works, the ROI is pretty impressive - seen teams cut document search from hours to minutes.&lt;/p&gt; &lt;p&gt;Posted this in LLMDevs a few days ago and many people found the technical breakdown helpful, so wanted to share here too for the broader AI community!&lt;/p&gt; &lt;p&gt;Happy to answer questions if anyone's hitting similar walls with their implementations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Low_Acanthisitta7686"&gt; /u/Low_Acanthisitta7686 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ned2ai/building_rag_systems_at_enterprise_scale_20k_docs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ned2ai/building_rag_systems_at_enterprise_scale_20k_docs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ned2ai/building_rag_systems_at_enterprise_scale_20k_docs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T16:16:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf6nnf</id>
    <title>Daily reminder that your local LLM is just a stupid stochastic parrot that can't reason, or diminishing returns from reinforcement learning + proofs</title>
    <updated>2025-09-12T15:44:00+00:00</updated>
    <author>
      <name>/u/Massive-Shift6641</name>
      <uri>https://old.reddit.com/user/Massive-Shift6641</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf6nnf/daily_reminder_that_your_local_llm_is_just_a/"&gt; &lt;img alt="Daily reminder that your local LLM is just a stupid stochastic parrot that can't reason, or diminishing returns from reinforcement learning + proofs" src="https://b.thumbs.redditmedia.com/XHoDP1fX8p8HdXv9C7a1U1rr8Q0UeRs6UBZ1yeE_rJI.jpg" title="Daily reminder that your local LLM is just a stupid stochastic parrot that can't reason, or diminishing returns from reinforcement learning + proofs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alright, seems like everyone liked my &lt;a href="http://www.reddit.com/r/LocalLLaMA/comments/1netdjp"&gt;music theory benchmark&lt;/a&gt; (or the fact that Qwen3-Next is so good (or both)), so here's something more interesting.&lt;/p&gt; &lt;p&gt;When testing new Qwen, I rephrased the problem and transposed the key a couple of semitones up and down to see if it will impact its performance. Sadly, Qwen performed a bit worse... and I thought that it could've overfit on the first version of the problem, but decided to test it against GPT-5 to have a &amp;quot;control group&amp;quot;. To my surprise, GPT-5 was performing &lt;em&gt;comparably worse&lt;/em&gt; to Qwen - that is, with the same problem with minor tweaks, it became worse too.&lt;/p&gt; &lt;p&gt;The realization stroke my mind this exact moment. I went to &lt;a href="http://hooktheory.com"&gt;hooktheory.com&lt;/a&gt;, a website that curates a database of music keys, chords and their progressions, sorted by popularity, and checked it out:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6gg4zn07hqof1.png?width=1065&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d89f71f7c1ba5d71666d3c181e08d60e13c75130"&gt;https://preview.redd.it/6gg4zn07hqof1.png?width=1065&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d89f71f7c1ba5d71666d3c181e08d60e13c75130&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m12inovchqof1.png?width=1079&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f488ef4c582f5743e028520011fcbde47bdc40b6"&gt;https://preview.redd.it/m12inovchqof1.png?width=1079&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f488ef4c582f5743e028520011fcbde47bdc40b6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can see that Locrian keys are indeed rarely used in music, and most models struggle to identify them consistently - only GPT 5 and Grok 4 were able to correctly label my song as C Locrian. However, it turns out that even these titans of the AI industry can be stumped.&lt;/p&gt; &lt;p&gt;Here is a reminder - that's how GPT 5 performs with the same harmony transposed to B Locrian - second most popular Locrian mode according to Hooktheory:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8vn148iojqof1.png?width=1297&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1bb23959178fe8cdb44087b85057baff498e789a"&gt;https://preview.redd.it/8vn148iojqof1.png?width=1297&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1bb23959178fe8cdb44087b85057baff498e789a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Correct. Most of the time, it does not miss. Occasionally, it will say F Lydian or C Major, but even so it correctly identifies the pitch collection as all these modes use the exact same notes.&lt;/p&gt; &lt;p&gt;Sure it will handle G# Locrian, the least popular key of Locrian and the least popular key in music ever, right?&lt;/p&gt; &lt;p&gt;RIGHT????&lt;/p&gt; &lt;h1&gt;GPT 5&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z2a3lwwwkqof1.png?width=1297&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=80af379b43d4cb330d48b78acf316fa1c681d241"&gt;https://preview.redd.it/z2a3lwwwkqof1.png?width=1297&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=80af379b43d4cb330d48b78acf316fa1c681d241&lt;/a&gt;&lt;/p&gt; &lt;p&gt;...&lt;/p&gt; &lt;p&gt;Okay there, maybe it just brain farted. Let's try again...&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b4p7vhy6lqof1.png?width=1297&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f317de1224e200804303709bbc78878cc513c13"&gt;https://preview.redd.it/b4p7vhy6lqof1.png?width=1297&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f317de1224e200804303709bbc78878cc513c13&lt;/a&gt;&lt;/p&gt; &lt;p&gt;...E Mixolydian. Even worse. Okay there, I can see this &amp;quot;tense, ritual/choral, slightly gothic&amp;quot;, it's correct. But can you, please, realize that &amp;quot;tense&amp;quot; is the signature sound of Locrian? Here it is, the diminished chord right into your face - EVERYTHING screams Locrian here! Why won't you just say Locrian?!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hwo258uplqof1.png?width=1297&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0850bdf87eadf44bec48d039e3684b414ceb95b4"&gt;https://preview.redd.it/hwo258uplqof1.png?width=1297&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0850bdf87eadf44bec48d039e3684b414ceb95b4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;WTF??? Bright, floating, slightly suspenseful??? &lt;strong&gt;Slightly?????&lt;/strong&gt; FYI, here is the full track:&lt;/p&gt; &lt;p&gt;&lt;a href="https://voca.ro/195AH9rN3Zh5"&gt;https://voca.ro/195AH9rN3Zh5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If anyone can hear this &lt;strong&gt;slight&lt;/strong&gt; suspense over there, I strongly urge you to visit your local otolaryngologist (or psychiatrist (or both)). It's not just slight suspense - it's literally the creepiest diatonic mode ever. How GPT 5 can call it &amp;quot;floating slight suspense&amp;quot; is a mystery to me.&lt;/p&gt; &lt;p&gt;Okay, GPT 5 is dumb. Let's try Grok 4 - the LLM that can solve math questions that are not found in textbooks, according to its founder Elon.&lt;/p&gt; &lt;h1&gt;Grok 4&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wm2enz5ioqof1.png?width=785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57fa1243ce98a63ba11066d361df90fa3c4c4f9b"&gt;https://preview.redd.it/wm2enz5ioqof1.png?width=785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57fa1243ce98a63ba11066d361df90fa3c4c4f9b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;...I have no words for this anymore.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2wgh7dl2pqof1.png?width=785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7c88d71c2aaebf1058d3bc5e680abd5344f0f280"&gt;https://preview.redd.it/2wgh7dl2pqof1.png?width=785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7c88d71c2aaebf1058d3bc5e680abd5344f0f280&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It even hallucinated G# minor once. Close, but not there anyway.&lt;/p&gt; &lt;p&gt;Luckily, sometimes it gets it - 4 times out of 10 this time:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uvnurrkqpqof1.png?width=785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e031d20a56037e219bfb25d884ac9a77dd04000a"&gt;https://preview.redd.it/uvnurrkqpqof1.png?width=785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e031d20a56037e219bfb25d884ac9a77dd04000a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But for a LLM that does so good at ARC-AGI and Humanity's last exam, Grok's performance is sure disappointing. Same about GPT 5.&lt;/p&gt; &lt;p&gt;Once again: I did &lt;strong&gt;not&lt;/strong&gt; make any changes to the melody or harmony. I did &lt;strong&gt;not&lt;/strong&gt; change any notes. I did &lt;strong&gt;not&lt;/strong&gt; change the scale. I only transposed the score just a couple of semitones up. It is literally the very same piece, playing just a bit higher (or lower) than its previous version. Any human would recognize that it is the very same song.&lt;/p&gt; &lt;p&gt;But LLMs are not humans. They cannot find anything resembling G# Locrian in their semantic space, so they immediately shit bricks and resort to the safe space of the Major scale. Not even Minor or Phrygian that are most similar to Locrian - because Major is the most common mode ever, and when unsure, they always rationalize their analysis to fit Major with some tweaks.&lt;/p&gt; &lt;h1&gt;What I think about it&lt;/h1&gt; &lt;p&gt;Even with reinforcement learning, models are still stupid stochastic parrots when they have a chance to be. On problems that approach the frontiers of their training data, they'd rather say something safe than take the risk to be right.&lt;/p&gt; &lt;p&gt;With each new iteration of reinforcement learning, the returns seem to be more and more diminishing. Grok 4 is barely able to do whatever is trivial for any human who can hear and read music. It's just insane to think that it is running in a datacenter full of hundreds of thousands GPUs.&lt;/p&gt; &lt;p&gt;The amount of money that is being spent on reinforcement learning is absolutely nuts. I do not think that the current trend of RL scaling is even sustainable. It takes billions of dollars to fail at out-of-training-distribution tasks that are trivial for any barely competent human. Sure, Google's internal model won a gold medal on IMO and invented new matrix multiplication algorithms, but they inevitably fail tasks that are too semantically different from their training data.&lt;/p&gt; &lt;p&gt;Given all of the above, I do not believe that the next breakthrough will come from scaling alone. We need some sort of magic that would enable AI (yes, AI, not just LLMs) to generalize more effectively, with improved data pipelines or architectural innovations or both. In the end, LLMs are optimized to process &lt;em&gt;natural languages&lt;/em&gt;, and they became so good at it that they easily fool us into believing that they are sentient beings, but there is much more to actual intelligence than just comprehension of natural languages - much more than LLMs don't have yet.&lt;/p&gt; &lt;p&gt;What do you think the next big AI thing is going to be?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Massive-Shift6641"&gt; /u/Massive-Shift6641 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf6nnf/daily_reminder_that_your_local_llm_is_just_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf6nnf/daily_reminder_that_your_local_llm_is_just_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf6nnf/daily_reminder_that_your_local_llm_is_just_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T15:44:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf3ur7</id>
    <title>Help me uderstand MoE models.</title>
    <updated>2025-09-12T13:53:10+00:00</updated>
    <author>
      <name>/u/kaisurniwurer</name>
      <uri>https://old.reddit.com/user/kaisurniwurer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My main question is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Why does the 30B A3B model can give better results than 3B model?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If the fact that all 30B are used at some point makes any difference, then wouldn't decreasing number of known tokens do the same?&lt;/p&gt; &lt;p&gt;Is is purely because of the shared layer? How does that make any sense, if it's still just 3B parameters?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaisurniwurer"&gt; /u/kaisurniwurer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf3ur7/help_me_uderstand_moe_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf3ur7/help_me_uderstand_moe_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf3ur7/help_me_uderstand_moe_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T13:53:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nfa11x</id>
    <title>I built a local AI agent that turns my messy computer into a private, searchable memory</title>
    <updated>2025-09-12T17:55:01+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfa11x/i_built_a_local_ai_agent_that_turns_my_messy/"&gt; &lt;img alt="I built a local AI agent that turns my messy computer into a private, searchable memory" src="https://external-preview.redd.it/_g7MxTDjiIWeWKTKuKZXglQRW6EdboZ0ViXwGjT4zqE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d3a35d2cb24eeffc8ac9b7477450487d55ab5efc" title="I built a local AI agent that turns my messy computer into a private, searchable memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My own computer is a mess: Obsidian markdowns, a chaotic downloads folder, random meeting notes, endless PDFs. I‚Äôve spent hours digging for one info I &lt;em&gt;know&lt;/em&gt; is in there somewhere ‚Äî and I‚Äôm sure plenty of valuable insights are still buried.&lt;/p&gt; &lt;p&gt;So I built &lt;strong&gt;Hyperlink&lt;/strong&gt; ‚Äî an on-device AI agent that searches your local files, powered by local AI models. 100% private. Works offline. Free and unlimited.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1nfa11x/video/fyfbgmuivrof1/player"&gt;https://reddit.com/link/1nfa11x/video/fyfbgmuivrof1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How I use it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Connect my entire desktop, download folders, and Obsidian vault (1000+ files) and have them scanned in seconds. I no longer need to upload updated files to a chatbot again!&lt;/li&gt; &lt;li&gt;Ask your PC like ChatGPT and get the answers from files in seconds -&amp;gt; with inline citations to the exact file.&lt;/li&gt; &lt;li&gt;Target a specific folder (@research_notes) and have it ‚Äúread‚Äù only that set like chatGPT project. So I can keep my &amp;quot;context&amp;quot; (files) organized on PC and use it directly with AI (no longer to reupload/organize again)&lt;/li&gt; &lt;li&gt;The AI agent also understands texts from images (screenshots, scanned docs, etc.)&lt;/li&gt; &lt;li&gt;I can also pick any Hugging Face model (GGUF + MLX supported) for different tasks. I particularly like OpenAI's GPT-OSS. It feels like using ChatGPT‚Äôs brain on my PC, but with unlimited free usage and full privacy.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Download and give it a try: &lt;a href="http://hyperlink.nexa.ai"&gt;hyperlink.nexa.ai&lt;/a&gt;&lt;br /&gt; Works today on Mac + Windows, ARM build coming soon. It‚Äôs completely free and private to use, and I‚Äôm looking to expand features‚Äîsuggestions and feedback welcome! Would also love to hear: what kind of use cases would you want a local AI agent like this to solve?&lt;/p&gt; &lt;p&gt;Hyperlink uses Nexa SDK (&lt;a href="https://github.com/NexaAI/nexa-sdk"&gt;https://github.com/NexaAI/nexa-sdk&lt;/a&gt;), which is a open-sourced local AI inference engine.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfa11x/i_built_a_local_ai_agent_that_turns_my_messy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nfa11x/i_built_a_local_ai_agent_that_turns_my_messy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nfa11x/i_built_a_local_ai_agent_that_turns_my_messy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T17:55:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf9k71</id>
    <title>PyTorch nostalgia, anyone?</title>
    <updated>2025-09-12T17:36:38+00:00</updated>
    <author>
      <name>/u/dmpiergiacomo</name>
      <uri>https://old.reddit.com/user/dmpiergiacomo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ML researcher &amp;amp; PyTorch contributor here. I'm genuinely curious: in the past year, how many of you shifted from building in PyTorch to mostly managing prompts for LLaMA and other models? Do you miss the old PyTorch workflow ‚Äî datasets, metrics, training loops ‚Äî compared to the constant &amp;quot;prompt -&amp;gt; test -&amp;gt; rewrite&amp;quot; cycle?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dmpiergiacomo"&gt; /u/dmpiergiacomo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf9k71/pytorch_nostalgia_anyone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf9k71/pytorch_nostalgia_anyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf9k71/pytorch_nostalgia_anyone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T17:36:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1neotp4</id>
    <title>How to think about GPUs</title>
    <updated>2025-09-12T00:12:23+00:00</updated>
    <author>
      <name>/u/kaggleqrdl</name>
      <uri>https://old.reddit.com/user/kaggleqrdl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neotp4/how_to_think_about_gpus/"&gt; &lt;img alt="How to think about GPUs" src="https://preview.redd.it/guijqeuxlmof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f009d812a34a8b621f1bb4fde4d159a864888777" title="How to think about GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://jax-ml.github.io/scaling-book/gpus/"&gt;https://jax-ml.github.io/scaling-book/gpus/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaggleqrdl"&gt; /u/kaggleqrdl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/guijqeuxlmof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neotp4/how_to_think_about_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1neotp4/how_to_think_about_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T00:12:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nerets</id>
    <title>RAG papers are dropping like crazy this month ‚Äî how do we even keep up?</title>
    <updated>2025-09-12T02:19:41+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My reading list is starting to look like a RAG graveyard. Just in the past few weeks we got:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;ToG¬≤ (MSR)&lt;/strong&gt; ‚Äì retriever as a teacher for generators&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RARE (Tsinghua)&lt;/strong&gt; ‚Äì multi-hop reasoning steps&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Meta-RAG (Meta)&lt;/strong&gt; ‚Äì adaptive memory + retriever&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OminiThink (DeepSeek)&lt;/strong&gt; ‚Äì retrieval + chain-of-thought&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CO-STORM&lt;/strong&gt; ‚Äì multi-agent context voting&lt;/li&gt; &lt;li&gt;&lt;strong&gt;FRAG&lt;/strong&gt; ‚Äì fine-grained doc segmentation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All sound great in papers‚Ä¶ but which ones actually work on &lt;em&gt;private data&lt;/em&gt; ‚Äî the messy PDFs, internal knowledge bases, and APIs that real teams rely on?&lt;/p&gt; &lt;p&gt;Is anyone tracking these variants in one place ‚Äî like a scoreboard for RAG? Feels impossible to keep up otherwise.&lt;/p&gt; &lt;p&gt;How are &lt;em&gt;you&lt;/em&gt; picking which setups to actually trust?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nerets/rag_papers_are_dropping_like_crazy_this_month_how/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nerets/rag_papers_are_dropping_like_crazy_this_month_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nerets/rag_papers_are_dropping_like_crazy_this_month_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T02:19:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf60zl</id>
    <title>Wasmind: A modular framework for building massively parallel agentic systems</title>
    <updated>2025-09-12T15:19:27+00:00</updated>
    <author>
      <name>/u/smarvin2</name>
      <uri>https://old.reddit.com/user/smarvin2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf60zl/wasmind_a_modular_framework_for_building/"&gt; &lt;img alt="Wasmind: A modular framework for building massively parallel agentic systems" src="https://external-preview.redd.it/sqhu8owTMdis9GBADh7-HfW-lF2_NdUtJrLbxy4XLW4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=95efc34de408c33f5da4efe0da676e967763c8df" title="Wasmind: A modular framework for building massively parallel agentic systems" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Claude code for the last few months, and after seeing its popularity and use as well as other coding CLI's use skyrocket I set out to create my own open-source version and this is what it became. &lt;/p&gt; &lt;p&gt;Wasmind is a modular framework for building massively parallel agentic systems.&lt;/p&gt; &lt;p&gt;It can be used to build systems like Claude Code or really anything multi-agent you can dream of (examples included).&lt;/p&gt; &lt;p&gt;In my mind it solves a few problems:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Modular plug and play&lt;/li&gt; &lt;li&gt;User-centered easy configuration&lt;/li&gt; &lt;li&gt;User-defined and guaranteed enforceable safety and agent restrictions (coming soon)&lt;/li&gt; &lt;li&gt;Allows easily composing any number of agents&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It's an actor based system where each actor is a wasm module. Actor's are composed together to create Agents and you can have 1-1000s of agents running at once.&lt;/p&gt; &lt;p&gt;You can configure it to use any LLM local or remote. I haven't tried qwen3-next but qwen3-coder especially served by providers like Cerebras has been incredibly fun to play with.&lt;/p&gt; &lt;p&gt;I hope this is useful to the community here either as creative inspiration or a building block for something awesome. Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smarvin2"&gt; /u/smarvin2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/SilasMarvin/wasmind"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf60zl/wasmind_a_modular_framework_for_building/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf60zl/wasmind_a_modular_framework_for_building/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T15:19:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1neba8b</id>
    <title>Qwen</title>
    <updated>2025-09-11T15:06:37+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neba8b/qwen/"&gt; &lt;img alt="Qwen" src="https://preview.redd.it/p5fbgn0owjof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94561db32b1fca11c0250280863739d22d76e841" title="Qwen" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p5fbgn0owjof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neba8b/qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1neba8b/qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T15:06:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf9x9m</id>
    <title>Apple stumbled into succes with MLX</title>
    <updated>2025-09-12T17:50:47+00:00</updated>
    <author>
      <name>/u/Alarming-Ad8154</name>
      <uri>https://old.reddit.com/user/Alarming-Ad8154</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-next 80b-a3b is out in mlx on hugging face, MLX already supports it. Open source contributors got this done within 24 hrs. Doing things apple itself couldn‚Äôt ever do quickly, simply because the call to support, or not support, specific Chinese AI companies, who‚Äôs parent company may or may not be under specific US sanctions would take months if it had the apple brand anywhere near it If apple hadn‚Äôt let MLX sort of evolve in its research arm while they tried, and failed, to manage ‚Äúapple intelligence‚Äù, and pulled it into the company, closed it, centralized it, they would be nowhere now. It‚Äôs really quite a story arc and I feel with their new M5 chip design having matmul cores (faster prompt processing) they‚Äôre actually leaning into it! Apple is never the choice for sort of ‚Äúgo at it on your own‚Äù tinkerers, but now it actually is‚Ä¶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Alarming-Ad8154"&gt; /u/Alarming-Ad8154 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf9x9m/apple_stumbled_into_succes_with_mlx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf9x9m/apple_stumbled_into_succes_with_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf9x9m/apple_stumbled_into_succes_with_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T17:50:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nezoaj</id>
    <title>Best uncensored model rn?</title>
    <updated>2025-09-12T10:33:58+00:00</updated>
    <author>
      <name>/u/toolhouseai</name>
      <uri>https://old.reddit.com/user/toolhouseai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Howdy folks, what uncensored model y'all using these days? Need something that doesn‚Äôt filter cussing/adult language and be creative at it. Never messed around with uncensored before, curious where to start in my project. Appreciate youe help/tips!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/toolhouseai"&gt; /u/toolhouseai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nezoaj/best_uncensored_model_rn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nezoaj/best_uncensored_model_rn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nezoaj/best_uncensored_model_rn/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T10:33:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf8ff4</id>
    <title>Qwen3 Next (Instruct) coding benchmark results</title>
    <updated>2025-09-12T16:52:33+00:00</updated>
    <author>
      <name>/u/mr_riptano</name>
      <uri>https://old.reddit.com/user/mr_riptano</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf8ff4/qwen3_next_instruct_coding_benchmark_results/"&gt; &lt;img alt="Qwen3 Next (Instruct) coding benchmark results" src="https://external-preview.redd.it/XmbB_Ggpaw13Ih4SiltMb7pnW0SotFk3Ey3eZ2fkjxY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=00fc58d8590fb833a39e70961d2fba37eab593c1" title="Qwen3 Next (Instruct) coding benchmark results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why I've chosen to compare with the alternatives you see at the link:&lt;/p&gt; &lt;p&gt;In terms of model size and &amp;quot;is this reasonable to run locally&amp;quot; it makes the most sense to compare Qwen3 Next with GPT-OSS-20b. I've also thrown in GPT5-nano as &amp;quot;probably around the same size as OSS-20b, and at the same price point from hosted vendors&amp;quot;, and all 3 have similar scores. &lt;/p&gt; &lt;p&gt;However, 3rd party inference vendors are currently pricing Qwen3 Next at 3x GPT-OSS-20b, while Alibaba has it at almost 10x more (lol). So I've also included gpt5-mini and flash 2.5 as &amp;quot;in the same price category that Alibaba wants to play in,&amp;quot; and also Alibaba specifically calls out &amp;quot;outperforms flash 2.5&amp;quot; in their release post (lol again).&lt;/p&gt; &lt;p&gt;So: if you're running on discrete GPUs, keep using GPT-OSS-20b. If you're running on a Mac or the new Ryzen AI unified memory chips, Qwen3 Next should be a lot faster for similar performance. And if you're outsourcing your inference then you can either get the same performance for much cheaper, or a much smarter model for the same price.&lt;/p&gt; &lt;p&gt;Note: I tried to benchmark against only Alibaba but the rate limits are too low, so I added DeepInfra as a provider as well. If DeepInfra has things misconfigured these results will be tainted. I've used DeepInfra's pricing for the Cost Efficiency graph at the link.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_riptano"&gt; /u/mr_riptano &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://brokk.ai/power-ranking?version=openround-2025-08-20&amp;amp;score=average&amp;amp;models=flash-2.5%2Cgpt-oss-20b%2Cgpt5-mini%2Cgpt5-nano%2Cq3next"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf8ff4/qwen3_next_instruct_coding_benchmark_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf8ff4/qwen3_next_instruct_coding_benchmark_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T16:52:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nejluw</id>
    <title>Qwen Next Is A Preview Of Qwen3.5üëÄ</title>
    <updated>2025-09-11T20:26:30+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nejluw/qwen_next_is_a_preview_of_qwen35/"&gt; &lt;img alt="Qwen Next Is A Preview Of Qwen3.5üëÄ" src="https://preview.redd.it/hddap3b9hlof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80ea94871e4f36ff1a26b9ef506a7c93ef43d580" title="Qwen Next Is A Preview Of Qwen3.5üëÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After experimenting with Qwen3 Next, it's a very impressive model. It does have problems with sycophancy and coherence- but it's fast, smart and it's long context performance is solid. Awesome stuff from the Tongyi Lab!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hddap3b9hlof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nejluw/qwen_next_is_a_preview_of_qwen35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nejluw/qwen_next_is_a_preview_of_qwen35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T20:26:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nefmzr</id>
    <title>Qwen released Qwen3-Next-80B-A3B ‚Äî the FUTURE of efficient LLMs is here!</title>
    <updated>2025-09-11T17:53:48+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nefmzr/qwen_released_qwen3next80ba3b_the_future_of/"&gt; &lt;img alt="Qwen released Qwen3-Next-80B-A3B ‚Äî the FUTURE of efficient LLMs is here!" src="https://a.thumbs.redditmedia.com/WcRnBmHLixgTcJRWqYT5sRJfRzk65bSO8Y3sUSxIA28.jpg" title="Qwen released Qwen3-Next-80B-A3B ‚Äî the FUTURE of efficient LLMs is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ Introducing Qwen3-Next-80B-A3B ‚Äî the FUTURE of efficient LLMs is here!&lt;/p&gt; &lt;p&gt;üîπ 80B params, but only 3B activated per token ‚Üí 10x cheaper training, 10x faster inference than Qwen3-32B.(esp. @ 32K+ context!) üîπHybrid Architecture: Gated DeltaNet + Gated Attention ‚Üí best of speed &amp;amp; recall üîπ Ultra-sparse MoE: 512 experts, 10 routed + 1 shared üîπ Multi-Token Prediction ‚Üí turbo-charged speculative decoding üîπ Beats Qwen3-32B in perf, rivals Qwen3-235B in reasoning &amp;amp; long-context&lt;/p&gt; &lt;p&gt;üß† Qwen3-Next-80B-A3B-Instruct approaches our 235B flagship. üß† Qwen3-Next-80B-A3B-Thinking outperforms Gemini-2.5-Flash-Thinking.&lt;/p&gt; &lt;p&gt;Try it now: chat.qwen.ai&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;amp;from=research.latest-advancements-list"&gt;https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;amp;from=research.latest-advancements-list&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Huggingface: &lt;a href="https://huggingface.co/collections/Qwen/qwen3-next-68c25fd6838e585db8eeea9d"&gt;https://huggingface.co/collections/Qwen/qwen3-next-68c25fd6838e585db8eeea9d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nefmzr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nefmzr/qwen_released_qwen3next80ba3b_the_future_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nefmzr/qwen_released_qwen3next80ba3b_the_future_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T17:53:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nesqlt</id>
    <title>Maxsun Intel B60s!</title>
    <updated>2025-09-12T03:27:02+00:00</updated>
    <author>
      <name>/u/Jaack18</name>
      <uri>https://old.reddit.com/user/Jaack18</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nesqlt/maxsun_intel_b60s/"&gt; &lt;img alt="Maxsun Intel B60s!" src="https://b.thumbs.redditmedia.com/1sF4sUvMgqQW0FksiFGJVQCOQg9CZu4vzbdHbvd7KoU.jpg" title="Maxsun Intel B60s!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In case anyone was wondering‚Ä¶.they do exist. I‚Äôll be listing extras on &lt;a href="/r/homelabsales"&gt;r/homelabsales&lt;/a&gt; tomorrow morning. I was only able to snag 10 due to low stock unfortunately. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jaack18"&gt; /u/Jaack18 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nesqlt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nesqlt/maxsun_intel_b60s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nesqlt/maxsun_intel_b60s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T03:27:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1nedq3i</id>
    <title>We just released the world's first 70B intermediate checkpoints. Yes, Apache 2.0. Yes, we're still broke.</title>
    <updated>2025-09-11T16:42:00+00:00</updated>
    <author>
      <name>/u/jshin49</name>
      <uri>https://old.reddit.com/user/jshin49</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Remember when y'all roasted us about the license? We listened.&lt;/p&gt; &lt;p&gt;Just dropped what we think is a world first: &lt;strong&gt;70B model intermediate checkpoints&lt;/strong&gt;. Not just the final model - the entire training journey. Previous releases (SmolLM-3, OLMo-2) maxed out at &amp;lt;14B.&lt;/p&gt; &lt;p&gt;Everything is Apache 2.0 now (no gated access):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;70B, 7B, 1.9B, 0.5B models + all their intermediate checkpoints and base models&lt;/li&gt; &lt;li&gt;First Korean 70B ever (but secretly optimized for English lol)&lt;/li&gt; &lt;li&gt;Actually open-source, not just open-weights BS&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/trillionlabs/Tri-70B-Intermediate-Checkpoints"&gt;https://huggingface.co/trillionlabs/Tri-70B-Intermediate-Checkpoints&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're a 1-year-old startup with pocket change competing against companies with infinite money glitch. Not the best model, but probably the most transparent 70B training ever shared.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jshin49"&gt; /u/jshin49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nedq3i/we_just_released_the_worlds_first_70b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nedq3i/we_just_released_the_worlds_first_70b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nedq3i/we_just_released_the_worlds_first_70b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T16:42:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf7vry</id>
    <title>Llama-OS - 0.2.1-beta + Code</title>
    <updated>2025-09-12T16:31:13+00:00</updated>
    <author>
      <name>/u/fredconex</name>
      <uri>https://old.reddit.com/user/fredconex</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf7vry/llamaos_021beta_code/"&gt; &lt;img alt="Llama-OS - 0.2.1-beta + Code" src="https://preview.redd.it/m8h48krffrof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a872d2a01db281e882ddef5b4721e6a01d375469" title="Llama-OS - 0.2.1-beta + Code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello Guys,&lt;/p&gt; &lt;p&gt;I've published the code for my app&lt;br /&gt; &lt;a href="https://github.com/fredconex/Llama-OS"&gt;https://github.com/fredconex/Llama-OS&lt;/a&gt; &lt;/p&gt; &lt;p&gt;For anyone interested into seeing it in action there's this another post&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nau0qe/llamaos_im_developing_an_app_to_make_llamacpp/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1nau0qe/llamaos_im_developing_an_app_to_make_llamacpp/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fredconex"&gt; /u/fredconex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m8h48krffrof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf7vry/llamaos_021beta_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf7vry/llamaos_021beta_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T16:31:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf2mxj</id>
    <title>Debunking the Claims of K2-Think</title>
    <updated>2025-09-12T13:01:56+00:00</updated>
    <author>
      <name>/u/nielstron</name>
      <uri>https://old.reddit.com/user/nielstron</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;K2-Think was sold as the next era in open reasoning models. However, upon closer inspection, it does not perform better than comparable competitors, even though they managed to contaminate it on the test data.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nielstron"&gt; /u/nielstron &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.sri.inf.ethz.ch/blog/k2think"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf2mxj/debunking_the_claims_of_k2think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf2mxj/debunking_the_claims_of_k2think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T13:01:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf6bgp</id>
    <title>Qwen3-Next-80B-A3B: any news on gguf?</title>
    <updated>2025-09-12T15:30:42+00:00</updated>
    <author>
      <name>/u/Herr_Drosselmeyer</name>
      <uri>https://old.reddit.com/user/Herr_Drosselmeyer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been looking on HF, but none seem to be available, which seems odd. Usually, with a high profile release, you'd see some within a day. &lt;/p&gt; &lt;p&gt;So, is there some issue with the model that prevents this for now? Anybody working on it? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Herr_Drosselmeyer"&gt; /u/Herr_Drosselmeyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf6bgp/qwen3next80ba3b_any_news_on_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf6bgp/qwen3next80ba3b_any_news_on_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf6bgp/qwen3next80ba3b_any_news_on_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T15:30:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf10ye</id>
    <title>30 Days Testing Parakeet v3 vs Whisper</title>
    <updated>2025-09-12T11:47:30+00:00</updated>
    <author>
      <name>/u/samuelroy_</name>
      <uri>https://old.reddit.com/user/samuelroy_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;MacOS dev here who just went through integration with Parakeet v3, also known as parakeet-tdt-0.6b-v3 for dictation and meeting recordings purposes, including speaker identification. I was not alone, it was a team work.&lt;/p&gt; &lt;h1&gt;Foreword&lt;/h1&gt; &lt;p&gt;Parakeet v3 supported languages are:&lt;/p&gt; &lt;p&gt;Bulgarian (&lt;strong&gt;bg&lt;/strong&gt;), Croatian (&lt;strong&gt;hr&lt;/strong&gt;), Czech (&lt;strong&gt;cs&lt;/strong&gt;), Danish (&lt;strong&gt;da&lt;/strong&gt;), Dutch (&lt;strong&gt;nl&lt;/strong&gt;), English (&lt;strong&gt;en&lt;/strong&gt;), Estonian (&lt;strong&gt;et&lt;/strong&gt;), Finnish (&lt;strong&gt;fi&lt;/strong&gt;), French (&lt;strong&gt;fr&lt;/strong&gt;), German (&lt;strong&gt;de&lt;/strong&gt;), Greek (&lt;strong&gt;el&lt;/strong&gt;), Hungarian (&lt;strong&gt;hu&lt;/strong&gt;), Italian (&lt;strong&gt;it&lt;/strong&gt;), Latvian (&lt;strong&gt;lv&lt;/strong&gt;), Lithuanian (&lt;strong&gt;lt&lt;/strong&gt;), Maltese (&lt;strong&gt;mt&lt;/strong&gt;), Polish (&lt;strong&gt;pl&lt;/strong&gt;), Portuguese (&lt;strong&gt;pt&lt;/strong&gt;), Romanian (&lt;strong&gt;ro&lt;/strong&gt;), Slovak (&lt;strong&gt;sk&lt;/strong&gt;), Slovenian (&lt;strong&gt;sl&lt;/strong&gt;), Spanish (&lt;strong&gt;es&lt;/strong&gt;), Swedish (&lt;strong&gt;sv&lt;/strong&gt;), Russian (&lt;strong&gt;ru&lt;/strong&gt;), Ukrainian (&lt;strong&gt;uk&lt;/strong&gt;)&lt;/p&gt; &lt;p&gt;Long story short: very europe / latin-based languages focus so if you are looking for Chinese, Japanese, Korean, Arabic, Hindi, etc, you are out of luck sorry.&lt;/p&gt; &lt;p&gt;(&lt;a href="https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3"&gt;More details on HF&lt;/a&gt;)&lt;/p&gt; &lt;h1&gt;The Speed Thing Everyone's Talking About&lt;/h1&gt; &lt;p&gt;Holy s***, this thing is fast. &lt;/p&gt; &lt;p&gt;We're talking an average of 10x faster than Whisper. Rule of thumb: 30 seconds per hour of audio to transcribe, allowing for real-time transcription and processing of hours-long files.&lt;/p&gt; &lt;h1&gt;What Actually Works Well&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;A bit less accurate than Whisper but so fast&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;English and French (our main languages) work great&lt;/li&gt; &lt;li&gt;Matches big Whisper models for general discussion in term of accuracy&lt;/li&gt; &lt;li&gt;Perfect for meeting notes, podcast transcripts, that kind of stuff&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Play well with Pyannote for diarization&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Actually tells people apart in most scenarios&lt;/li&gt; &lt;li&gt;Close to Deepgram Nova (our TTS cloud provider) in terms of accuracy&lt;/li&gt; &lt;li&gt;Most of our work went here to get accuracy and speed at this level&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Where It Falls Apart&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;No custom dictionary support&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This one's a killer for specialized content&lt;/li&gt; &lt;li&gt;Struggles with acronyms, company names, technical terms, french accents ;). The best example here is trying to dictate &amp;quot;Parakeet,&amp;quot; which it usually writes down as &amp;quot;Parakit.&amp;quot;&lt;/li&gt; &lt;li&gt;Can't teach it your domain-specific vocabulary&lt;/li&gt; &lt;li&gt;-&amp;gt; You need some LLM post-processing to clean up or improve it here.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Language support is... optimistic&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Claims 25 languages, but quality is all over the map&lt;/li&gt; &lt;li&gt;Tested Dutch with a colleague - results were pretty rough&lt;/li&gt; &lt;li&gt;Feels like they trained some languages way better than others&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Speaker detection is hard&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gets close to perfect with PYAnnote but...&lt;/li&gt; &lt;li&gt;You'll have a very hard time with overlapping speakers and the number of speakers detected.&lt;/li&gt; &lt;li&gt;Plus, fusing timings/segments to get a proper transcript, but overall results are better with Parakeet than Whisper.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Speech-to-text tech is now good enough on local&lt;/h1&gt; &lt;p&gt;Speech-to-text for normal use cases is solved now. Whether you use Parakeet or big Whisper models, you can get totally usable results in real-time with speaker ID.&lt;/p&gt; &lt;p&gt;But we've also hit this plateau where having 95% accuracy feels impossible.&lt;/p&gt; &lt;p&gt;This is especially true for having exact timecodes associated with speakers and clean diarization when two or more people speak at the same time.&lt;/p&gt; &lt;p&gt;The good news: it will only get better, as shown with the new Precision-2 model from PYAnnote.&lt;/p&gt; &lt;h1&gt;Our learnings so far:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;If you need &amp;quot;good enough&amp;quot; transcripts&lt;/strong&gt; (meetings, content creation, pulling topics): Parakeet v3 is fantastic. Fast, local, gets the job done.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you are processing long audio files and/or in batches&lt;/strong&gt;: Parakeet is really great too and as fast as cloud.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you need every single word perfect&lt;/strong&gt; (legal, medical, compliance): You're probably still stuck with slower, more careful approaches using Whisper or closed cloud models. The plateau is real.&lt;/p&gt; &lt;p&gt;For dictation, especially long text, you still need a LLM post process to clean out the content and do clean formatting&lt;/p&gt; &lt;h1&gt;So Parakeet or Whisper? Actually both.&lt;/h1&gt; &lt;p&gt;Whisper's the Swiss Army knife: slower but handles edge cases (with dictionnary) and supports more languages.&lt;/p&gt; &lt;p&gt;Parakeet is the race car: stupid fast when the conditions are right. (and you want to transcribe an european language)&lt;/p&gt; &lt;p&gt;Most of us probably need both depending on the job.&lt;/p&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;If you're building something where the transcript is just the starting point (topic extraction, summarization, content creation), Parakeet v3 is killer.&lt;/p&gt; &lt;p&gt;If you're in a &amp;quot;every word matters&amp;quot; situation, you might be waiting a bit longer for the tech to catch up.&lt;/p&gt; &lt;p&gt;Anyone else playing with that stack? What's your experience? Also if you want to get more technical, feel free to ask any questions in the comments.&lt;/p&gt; &lt;h1&gt;Implementation Notes&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;We used Argmax's WhisperKit, both open-source and proprietary versions: &lt;a href="https://github.com/argmaxinc/WhisperKit"&gt;https://github.com/argmaxinc/WhisperKit&lt;/a&gt; They have an optimized version of the models, both in size and battery impact, and SpeakerKit, their diarization engine is fast.&lt;/li&gt; &lt;li&gt;New kid on the block worth checking out: &lt;a href="https://github.com/FluidInference/FluidAudio"&gt;https://github.com/FluidInference/FluidAudio&lt;/a&gt;&lt;/li&gt; &lt;li&gt;This also looks promising: &lt;a href="https://github.com/Blaizzy/mlx-audio"&gt;https://github.com/Blaizzy/mlx-audio&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Benchmarks&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;OpenASR Leaderboard (with multilingual benchmarks): &lt;a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard"&gt;https://huggingface.co/spaces/hf-audio/open_asr_leaderboard&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Argmax Whisper models benchmarks on various Apple machines: &lt;a href="https://huggingface.co/spaces/argmaxinc/whisperkit-benchmarks"&gt;https://huggingface.co/spaces/argmaxinc/whisperkit-benchmarks&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Fluid Parakeet V3 benchmark: &lt;a href="https://github.com/FluidInference/FluidAudio/blob/main/Documentation/Benchmarks.md"&gt;https://github.com/FluidInference/FluidAudio/blob/main/Documentation/Benchmarks.md&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samuelroy_"&gt; /u/samuelroy_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf10ye/30_days_testing_parakeet_v3_vs_whisper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf10ye/30_days_testing_parakeet_v3_vs_whisper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf10ye/30_days_testing_parakeet_v3_vs_whisper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T11:47:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf5j8f</id>
    <title>Long context tested for Qwen3-next-80b-a3b-thinking. Performs very similarly to qwen3-30b-a3b-thinking-2507 and far behind qwen3-235b-a22b-thinking</title>
    <updated>2025-09-12T15:00:15+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf5j8f/long_context_tested_for_qwen3next80ba3bthinking/"&gt; &lt;img alt="Long context tested for Qwen3-next-80b-a3b-thinking. Performs very similarly to qwen3-30b-a3b-thinking-2507 and far behind qwen3-235b-a22b-thinking" src="https://preview.redd.it/9df1cyk90rof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b32809725e64ae3278a2a9f48134b0ee4513eb4e" title="Long context tested for Qwen3-next-80b-a3b-thinking. Performs very similarly to qwen3-30b-a3b-thinking-2507 and far behind qwen3-235b-a22b-thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9df1cyk90rof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf5j8f/long_context_tested_for_qwen3next80ba3bthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf5j8f/long_context_tested_for_qwen3next80ba3bthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T15:00:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1netdjp</id>
    <title>Qwen3-Next-80B-A3B - a big step up may be the best open source reasoning model so far</title>
    <updated>2025-09-12T04:00:48+00:00</updated>
    <author>
      <name>/u/Massive-Shift6641</name>
      <uri>https://old.reddit.com/user/Massive-Shift6641</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1netdjp/qwen3next80ba3b_a_big_step_up_may_be_the_best/"&gt; &lt;img alt="Qwen3-Next-80B-A3B - a big step up may be the best open source reasoning model so far" src="https://b.thumbs.redditmedia.com/y5Pqz_JmI2YT4BHTfapll4B_qxuvcEPNdJDCghbe6Uw.jpg" title="Qwen3-Next-80B-A3B - a big step up may be the best open source reasoning model so far" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently I presented another music theory problem and explained why it may be a great way to test LLMs' ability: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ndjoek"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ndjoek&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I love torturing models with music theory problems. I see a good reason why it may be a good proxy for the models' general ability, if not among the best measurements ever - it tests mostly the LLMs' reasoning ability rather than just knowledge.&lt;br /&gt; &lt;strong&gt;Music theory is not a big subject&lt;/strong&gt; - there is an infinite number of songs that can be written, but the entire music theory is quite compact. It makes it easy to fit it into a LLM and write evals that test their reasoning and comprehension skills rather than just knowledge.&lt;br /&gt; &lt;strong&gt;Most music theory knowledge online is never explored in-depth&lt;/strong&gt; - even most musicians' don't know anything besides basic major and minor chords and their progressions. Since most pretraining data is not particularly high quality, LLMs have to reason to analyze music that is more complex than popular.&lt;br /&gt; &lt;strong&gt;Music theory evals can easily be rewritten and updated if benchmaxxxed and overfit&lt;/strong&gt; - it may take days to even create a programming or math problem that is enough challenging for modern LLMs, but only a few hours to create a song that is beyond most models' ability to understand. (I'm not totally sure about this one)&lt;/p&gt; &lt;p&gt;So I wrote the following:&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dmvsy194gnof1.png?width=1727&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f977df8c20d9229dc1be929e12cfc1cba7ba97b"&gt;https://preview.redd.it/dmvsy194gnof1.png?width=1727&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f977df8c20d9229dc1be929e12cfc1cba7ba97b&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;This piece is special because it is written in Locrian. It is rarely used in popular music because of its inherent tension and lack of resolution (look up John Kirkpatrick's Dust to Dust), and since it is so rare, it makes it a perfect candidate to test the LLMs reasoning ability.&lt;/p&gt; &lt;p&gt;In this track, the signature Locrian sound is created with:&lt;/p&gt; &lt;p&gt;a dissonant diminished triad is outlined with the C-Eb-Gb ostinato at the organ 2 line;&lt;/p&gt; &lt;p&gt;The Gb bassline - a point of relative stability that gives an illusion of a tonal center.&lt;/p&gt; &lt;p&gt;Basically, it is Locrian with a twist - while the actual tonal center is on C, the Gb bass drone sounds more stable than C (where it occasionally plays), so it is easy to misinterpret Gb as tonic simply because it is the most stable note here.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Back then, I was surprised with the performance of all major LLMs on this task - the only two models that consistently identified the correct key and mode (C Locrian) were GPT-5 High and Grok 4. Now I am surprised with the performance of Qwen3-Next.&lt;/p&gt; &lt;h1&gt;Qwen3-next's performance on this task&lt;/h1&gt; &lt;p&gt;I fed the problem to Qwen3-Next in reasoning mode. It has really impressed me with three big improvements over its big brother 235B-A22B-2507:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;It identified the correct C Locrian mode in &lt;strong&gt;half&lt;/strong&gt; of my 10 attempts. 235B-A22B-2507 was not able to identify it more than once, and even so it hallucinated a lot during the process.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Even when it mistakenly identified another mode, it was &lt;strong&gt;always&lt;/strong&gt; a relative mode of C Locrian - that is, a scale that uses &lt;strong&gt;the same notes&lt;/strong&gt; arranged in a different order. Unlike 235B-A22B-2507, Qwen3-Next now always knows the correct notes even if it can't determine their function.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;It stopped hallucinating this much.&lt;/strong&gt; At least far less than 235B-A22B-2507. Previous Qwen was making up a ton of stuff and its delusions made its reasoning look like absolutely random shotgun debugging. Now it is no longer a problem because Qwen3-Next simply never hallucinates notes that do not exist in the scale.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;To make sure the model wasn't overfit on this exact problem since I published it, I also tested it with the same piece transposed into D and F Locrian, and while it struggled to identify F Locrian because it is far less common scale than C and D Locrian, it was able to identify correct note collection most of the time.&lt;/p&gt; &lt;p&gt;Some typical responses from Qwen3-Next:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zly615mtinof1.png?width=752&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f1388415401d7e308da78d8fe3f5a5649603656d"&gt;https://preview.redd.it/zly615mtinof1.png?width=752&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f1388415401d7e308da78d8fe3f5a5649603656d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/al056pd5jnof1.png?width=914&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f27b62ff6032d0114efe183e64a9e2ac3ce011a3"&gt;https://preview.redd.it/al056pd5jnof1.png?width=914&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f27b62ff6032d0114efe183e64a9e2ac3ce011a3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ov8skbejjnof1.png?width=914&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2915e1d2a8edae6f08cd213874b318c112ed2628"&gt;https://preview.redd.it/ov8skbejjnof1.png?width=914&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2915e1d2a8edae6f08cd213874b318c112ed2628&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So did they make Qwen better? &lt;strong&gt;Yes!&lt;/strong&gt; In fact, it is the first open source model that did this well on this problem.&lt;/p&gt; &lt;p&gt;Now since Qwen became this good, I can only wonder what wonders await us with DeepSeek R2.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Massive-Shift6641"&gt; /u/Massive-Shift6641 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1netdjp/qwen3next80ba3b_a_big_step_up_may_be_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1netdjp/qwen3next80ba3b_a_big_step_up_may_be_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1netdjp/qwen3next80ba3b_a_big_step_up_may_be_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T04:00:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf6s0w</id>
    <title>Qwen3 Next and DeepSeek V3.1 share an identical Artificial Analysis Intelligence Index Score for both their reasoning and non-reasoning modes.</title>
    <updated>2025-09-12T15:48:49+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf6s0w/qwen3_next_and_deepseek_v31_share_an_identical/"&gt; &lt;img alt="Qwen3 Next and DeepSeek V3.1 share an identical Artificial Analysis Intelligence Index Score for both their reasoning and non-reasoning modes." src="https://preview.redd.it/hb62e80c7rof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45fbd3055204b4282742bcaf6567d07ade494ed5" title="Qwen3 Next and DeepSeek V3.1 share an identical Artificial Analysis Intelligence Index Score for both their reasoning and non-reasoning modes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hb62e80c7rof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf6s0w/qwen3_next_and_deepseek_v31_share_an_identical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf6s0w/qwen3_next_and_deepseek_v31_share_an_identical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T15:48:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1neyaph</id>
    <title>A list of models released or udpated last week on this sub, in case you missed any - (12 Sep)</title>
    <updated>2025-09-12T09:08:55+00:00</updated>
    <author>
      <name>/u/aifeed-fyi</name>
      <uri>https://old.reddit.com/user/aifeed-fyi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A quick list of models updates and new releases mentioned in several posts during the week on LocalLLama.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Next-80B-A3B:&lt;/strong&gt; 80B params, only 3B activated per token (10x faster inference, 32K+ context) | ( &lt;a href="https://huggingface.co/collections/Qwen/qwen3-next-68c25fd6838e585db8eeea9d"&gt;HuggingFace&lt;/a&gt; - &lt;a href="https://www.reddit.com/gallery/1nefmzr"&gt;Release&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Jan-v1-2509:&lt;/strong&gt; A new update, improved performance in reasoning and creativity evals | (&lt;a href="https://www.reddit.com/gallery/1ncdobh"&gt;Release&lt;/a&gt; - &lt;a href="https://huggingface.co/janhq/Jan-v1-2509"&gt;HuggingFace&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MiniCPM4.1-8B:&lt;/strong&gt; 8B hybrid reasoning model (/think vs /no_think) with long context | (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nbly7o/minicpm418b/"&gt;Release&lt;/a&gt; - &lt;a href="https://huggingface.co/openbmb/MiniCPM4.1-8B"&gt;HuggingFace&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PyDevMini-1 (4B):&lt;/strong&gt; Matches/outperforms GPT-4 on Python &amp;amp; Web Dev at 1/400th the size | (&lt;a href="https://v.redd.it/nh9fq7qbn2of1"&gt;Release&lt;/a&gt; - &lt;a href="https://huggingface.co/bralynn/pydevmini1"&gt;HuggingFace&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-ASR:&lt;/strong&gt; All-in-one multilingual speech recognition (EN/CN + 9 languages) | (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nbqa1p/qwen_released_api_only_qwen3asr_the_allinone/"&gt;Release&lt;/a&gt; - &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-ASR-Demo"&gt;Demo&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;IndexTTS-2.0:&lt;/strong&gt; Emotionally expressive, duration-controlled zero-shot TTS | (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nbkxnm/introducing_indextts20_a_breakthrough_in/"&gt;Release&lt;/a&gt; - &lt;a href="https://github.com/index-tts/index-tts"&gt;Demo&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Aquif-3 Series:&lt;/strong&gt; New reasoning-focused MoE releases | (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ncccri/aquif358bthink_is_the_proof_that_reasoning_and/"&gt;Aquif-3.5-8B-Think&lt;/a&gt; - &lt;a href="https://www.reddit.com/gallery/1nb3b8l"&gt;Aquif-3-moe 17B&lt;/a&gt; - &lt;a href="https://huggingface.co/mradermacher/aquif-3-moe-17b-a2.8b-GGUF"&gt;HuggingFace&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ROMA:&lt;/strong&gt; Open-source deep research repo that beats closed-source platforms (ChatGPT, Perplexity, Gemini, etc.) on Seal-0 &amp;amp; FRAMES | (&lt;a href="https://i.redd.it/sxii7uog37of1.jpeg"&gt;Discussion&lt;/a&gt; - &lt;a href="https://github.com/sentient-agi/ROMA"&gt;GitHub&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ernie X1.1 (Baidu):&lt;/strong&gt; A Chinese model released by Baidu approaching the frontier - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ndjoek/new_ernie_x11_what_may_be_the_best_chinese_model/"&gt;Post&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Datasets&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;FinePDFs (3T tokens):&lt;/strong&gt; Largest PDF dataset ever (0.5B+ docs) | (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1namz1q/hf_releases_3t_tokens_dataset_sourced_entirely/"&gt;Release&lt;/a&gt; - &lt;a href="https://huggingface.co/datasets/HuggingFaceFW/finepdfs"&gt;HuggingFace&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LongPage:&lt;/strong&gt; 300 full novels with reasoning traces for training writing LLMs | (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1n99gpq/longpage_300_full_novels_with_reasoning_traces/"&gt;Release&lt;/a&gt; - &lt;a href="https://huggingface.co/datasets/Pageshift-Entertainment/LongPage"&gt;HuggingFace&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If I missed any, please update in the comments ..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aifeed-fyi"&gt; /u/aifeed-fyi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neyaph/a_list_of_models_released_or_udpated_last_week_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neyaph/a_list_of_models_released_or_udpated_last_week_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1neyaph/a_list_of_models_released_or_udpated_last_week_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T09:08:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nf7zhq</id>
    <title>Meta released MobileLLM-R1 on Hugging Face</title>
    <updated>2025-09-12T16:35:23+00:00</updated>
    <author>
      <name>/u/Illustrious_Row_9971</name>
      <uri>https://old.reddit.com/user/Illustrious_Row_9971</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf7zhq/meta_released_mobilellmr1_on_hugging_face/"&gt; &lt;img alt="Meta released MobileLLM-R1 on Hugging Face" src="https://preview.redd.it/huchm6bahrof1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f091dea3c1b3bd8cc946d3ae61d24b3e9a2e3a3b" title="Meta released MobileLLM-R1 on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;model: &lt;a href="https://huggingface.co/facebook/MobileLLM-R1-950M"&gt;https://huggingface.co/facebook/MobileLLM-R1-950M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;app (vibe coded): &lt;a href="https://huggingface.co/spaces/akhaliq/MobileLLM-R1-950M"&gt;https://huggingface.co/spaces/akhaliq/MobileLLM-R1-950M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;app was made in: &lt;a href="https://huggingface.co/spaces/akhaliq/anycoder"&gt;https://huggingface.co/spaces/akhaliq/anycoder&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious_Row_9971"&gt; /u/Illustrious_Row_9971 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/huchm6bahrof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nf7zhq/meta_released_mobilellmr1_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nf7zhq/meta_released_mobilellmr1_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-12T16:35:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nct0z8</id>
    <title>Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)</title>
    <updated>2025-09-09T19:47:12+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt; &lt;img alt="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" src="https://preview.redd.it/7vh8enuu07of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0f499252613d5bcf478fb1b75c594bb50f43436" title="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7vh8enuu07of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T19:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndjxdt</id>
    <title>AMA with the Unsloth team</title>
    <updated>2025-09-10T17:02:18+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;, I'm Daniel from &lt;a href="https://docs.unsloth.ai/"&gt;Unsloth&lt;/a&gt;! You might know us from our RL &amp;amp; fine-tuning open-source framework, our GGUFs, kernels or bug fixes. We‚Äôre super excited to answer all your questions!! ü¶• Our GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To celebrate the AMA, we‚Äôre releasing Aider Polyglot benchmarks comparing our DeepSeek-V3.1 Dynamic GGUFs to other models and quants. We also made a Localllama post here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our participants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Daniel, u/danielhanchen&lt;/li&gt; &lt;li&gt;Michael, u/yoracale&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 10AM ‚Äì 1PM PST, with the Unsloth team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thanks so much!ü•∞&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T17:02:18+00:00</published>
  </entry>
</feed>
