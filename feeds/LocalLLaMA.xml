<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-04T18:48:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qv5d1k</id>
    <title>Qwen3-Coder Tech Report: tool call generalization, reward hacking, general knowledge</title>
    <updated>2026-02-03T21:47:26+00:00</updated>
    <author>
      <name>/u/Pristine-Woodpecker</name>
      <uri>https://old.reddit.com/user/Pristine-Woodpecker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv5d1k/qwen3coder_tech_report_tool_call_generalization/"&gt; &lt;img alt="Qwen3-Coder Tech Report: tool call generalization, reward hacking, general knowledge" src="https://external-preview.redd.it/3bIaBnDXu08CXhELxk4__N-qsOVuqLC1ZUdzCxFB0Fo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=00daa4c0505c069dbac679c0b3ae6151aa6f7543" title="Qwen3-Coder Tech Report: tool call generalization, reward hacking, general knowledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Qwen3-Coder tech report is super interesting on a number of items:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;They specifically tested on various tool chat templates to make sure the model stays flexible no matter where you use it. From their own data, only DeepSeek-v3.2 is close - even a bit better - (which suggests they do the same) and they're both quite a bit ahead of other models.&lt;/li&gt; &lt;li&gt;As the model gets smarter and smarter, it gets better and better at finding loopholes in the test environment to find the solution by cheating (&lt;a href="https://github.com/SWE-bench/SWE-bench/pull/471"&gt;https://github.com/SWE-bench/SWE-bench/pull/471&lt;/a&gt;), which they have to combat.&lt;/li&gt; &lt;li&gt;They trained several specialized submodels (UI dev, webdev, software engineering, ...) and the final model is a distillation of those.&lt;/li&gt; &lt;li&gt;It's similar in performance to the base (non-Coder) model on general benchmarks, and quite a bit better at math.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pristine-Woodpecker"&gt; /u/Pristine-Woodpecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/QwenLM/Qwen3-Coder/blob/main/qwen3_coder_next_tech_report.pdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv5d1k/qwen3coder_tech_report_tool_call_generalization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qv5d1k/qwen3coder_tech_report_tool_call_generalization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T21:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvhc3o</id>
    <title>Yuan 3.0 Flash 40B - 3.7b parameter multimodal foundation model. Does anyone know these or have tried the model?</title>
    <updated>2026-02-04T06:41:33+00:00</updated>
    <author>
      <name>/u/Loskas2025</name>
      <uri>https://old.reddit.com/user/Loskas2025</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/YuanLabAI/Yuan3.0-Flash-4bit"&gt;https://huggingface.co/YuanLabAI/Yuan3.0-Flash-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://yuanlab.ai"&gt;https://yuanlab.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was looking for optimized models for RAG data retrieval and found this. I've never heard of it. I wonder if the architecture is supported by llama.cpp (it's probably something derived from existing models).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loskas2025"&gt; /u/Loskas2025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvhc3o/yuan_30_flash_40b_37b_parameter_multimodal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvhc3o/yuan_30_flash_40b_37b_parameter_multimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvhc3o/yuan_30_flash_40b_37b_parameter_multimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T06:41:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qv65ed</id>
    <title>Got Qwen-Coder-Next running on ROCm on my Strix Halo!</title>
    <updated>2026-02-03T22:17:18+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv65ed/got_qwencodernext_running_on_rocm_on_my_strix_halo/"&gt; &lt;img alt="Got Qwen-Coder-Next running on ROCm on my Strix Halo!" src="https://external-preview.redd.it/dzdscnFjbDZ0Y2hnMarG5pOoEfpz9JksRMChe8rZdrijqwmTF4wbigP7RjX-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f8e501a14c67b4224883973e411e9b24a9f6bcf8" title="Got Qwen-Coder-Next running on ROCm on my Strix Halo!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thrilled to see the new model, 80B with 3B active seems perfect for Strix Halo. Video is running on &lt;a href="https://github.com/lemonade-sdk/llamacpp-rocm/releases/tag/b1170"&gt;llamacpp-rocm b1170&lt;/a&gt; with context size 16k and &lt;code&gt;--flash-attn on --no-mmap&lt;/code&gt;. Let me know what you want me to try and I'll run it later tonight!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hnso57l6tchg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qv65ed/got_qwencodernext_running_on_rocm_on_my_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qv65ed/got_qwencodernext_running_on_rocm_on_my_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T22:17:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qva5gk</id>
    <title>How to get more tok/s?</title>
    <updated>2026-02-04T00:59:54+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qva5gk/how_to_get_more_toks/"&gt; &lt;img alt="How to get more tok/s?" src="https://external-preview.redd.it/ZnpvY2wyN3BtZGhnMX3C4bhSrcOBtwpO2ghilluKqvqoK5kABDx37kIjqzIp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fd51fdcf7f7597f9ade8d8e5db8eb03b2cb2d80" title="How to get more tok/s?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not OC! [Source](&lt;a href="https://x.com/climate%5C_ben/status/2000636466117193866?s=61"&gt;https://x.com/climate\_ben/status/2000636466117193866?s=61&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/l8lk0xapmdhg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qva5gk/how_to_get_more_toks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qva5gk/how_to_get_more_toks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T00:59:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvvf8y</id>
    <title>Demystified - Inference of GPT 2 117M model on Mac minis + iPad</title>
    <updated>2026-02-04T17:48:54+00:00</updated>
    <author>
      <name>/u/East-Muffin-6472</name>
      <uri>https://old.reddit.com/user/East-Muffin-6472</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here‚Äôs an in-depth description of the core components that allowed me to run inference for a GPT-2 (117M) model on a heterogeneous compute cluster made up of Mac Minis and an iPad.&lt;/p&gt; &lt;p&gt;There are three key components involved:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model Parallelism&lt;/li&gt; &lt;li&gt;Synchronous Parameter Server (SyncPS)&lt;/li&gt; &lt;li&gt;Core ML&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The main thing that flows through every node in the system is activations.&lt;/p&gt; &lt;h1&gt;Motivation&lt;/h1&gt; &lt;p&gt;I wondered whether it would be possible to use tablets (iPad or Android) alongside other devices such as MacBooks, Windows machines, or Raspberry Pis in the same compute cluster.&lt;/p&gt; &lt;p&gt;The idea was to let devices with very different compute capabilities cooperate on inference.&lt;/p&gt; &lt;h1&gt;1) Model Parallelism&lt;/h1&gt; &lt;p&gt;To make this work, I used one of the simplest parallelism techniques: model parallelism.&lt;/p&gt; &lt;p&gt;With model parallelism, the model is split across multiple worker nodes, or in this case, across different devices in the compute cluster.&lt;/p&gt; &lt;p&gt;This allows us to divide the model ‚Äî specifically its layers ‚Äî across devices, so that each device only runs a small portion of the full model.&lt;/p&gt; &lt;p&gt;This makes it possible to run inference even on resource-constrained devices like an iPad.&lt;/p&gt; &lt;h1&gt;2) Core ML&lt;/h1&gt; &lt;p&gt;We can‚Äôt directly load arbitrary models (for example, from Hugging Face) onto an iPad.&lt;/p&gt; &lt;p&gt;They need to be converted into a format that can take full advantage of the device‚Äôs compute hardware, such as the ANE or GPU on macOS and iPadOS.&lt;/p&gt; &lt;p&gt;This is where Core ML comes in.&lt;/p&gt; &lt;p&gt;Core ML allows models to be converted into a format that is highly optimized for Apple edge devices. I used it to convert specific blocks of layers from the model so they could run efficiently on the iPad.&lt;/p&gt; &lt;p&gt;The remaining blocks are run directly on the Mac Minis using Metal GPU acceleration.&lt;/p&gt; &lt;h1&gt;3) Synchronous Parameter Server (SyncPS)&lt;/h1&gt; &lt;p&gt;Once the model is split and deployed across devices, a synchronous parameter server architecture is used to coordinate execution.&lt;/p&gt; &lt;p&gt;In this setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A central server acts as the coordinator&lt;/li&gt; &lt;li&gt;Worker nodes perform their assigned model computations&lt;/li&gt; &lt;li&gt;Communication happens synchronously between the server and workers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The server also performs part of the computation and ensures that activations flow correctly between workers.&lt;/p&gt; &lt;h1&gt;Implementation&lt;/h1&gt; &lt;p&gt;The architecture and algorithms were implemented using:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Python‚Äôs &lt;code&gt;socket&lt;/code&gt; library for communication&lt;/li&gt; &lt;li&gt;A Swift app (generated with the help of ChatGPT) running on the iPad&lt;/li&gt; &lt;li&gt;Core ML models running on Apple hardware&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Swift app performs inference on its assigned model blocks and sends the resulting activations back to the server.&lt;/p&gt; &lt;p&gt;The final system enables real-time distributed inference across heterogeneous devices, as shown in the attached architecture diagram and demo video.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qvvf8y/video/9jyod72mmihg1/player"&gt;https://reddit.com/link/1qvvf8y/video/9jyod72mmihg1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/East-Muffin-6472"&gt; /u/East-Muffin-6472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvvf8y/demystified_inference_of_gpt_2_117m_model_on_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvvf8y/demystified_inference_of_gpt_2_117m_model_on_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvvf8y/demystified_inference_of_gpt_2_117m_model_on_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T17:48:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvax2n</id>
    <title>Qwen3-Coder-Next-NVFP4 quantization is up, 45GB</title>
    <updated>2026-02-04T01:33:48+00:00</updated>
    <author>
      <name>/u/DataGOGO</name>
      <uri>https://old.reddit.com/user/DataGOGO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/GadflyII/Qwen3-Coder-Next-NVFP4"&gt;GadflyII/Qwen3-Coder-Next-NVFP4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All experts were calibrated with ultrachat_200k dataset, 1.63% accuracy loss in MMLU Pro+, 149GB to 45GB&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataGOGO"&gt; /u/DataGOGO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvax2n/qwen3codernextnvfp4_quantization_is_up_45gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvax2n/qwen3codernextnvfp4_quantization_is_up_45gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvax2n/qwen3codernextnvfp4_quantization_is_up_45gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T01:33:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1quvqs9</id>
    <title>Qwen/Qwen3-Coder-Next ¬∑ Hugging Face</title>
    <updated>2026-02-03T15:58:52+00:00</updated>
    <author>
      <name>/u/coder543</name>
      <uri>https://old.reddit.com/user/coder543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quvqs9/qwenqwen3codernext_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-Coder-Next ¬∑ Hugging Face" src="https://external-preview.redd.it/Mexo_PE5lQQ6UgBLTSrZljbCfScpUvytIcHjhp81XG4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae35460e62424e898f9d6136fab1921d2029ad86" title="Qwen/Qwen3-Coder-Next ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coder543"&gt; /u/coder543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-Next"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quvqs9/qwenqwen3codernext_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quvqs9/qwenqwen3codernext_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T15:58:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvommg</id>
    <title>[Release] Eva-4B-V2: Updated Financial Evasion Detection Model. Now #1, beating Claude Opus 4.5 &amp; Gemini 3 Flash.</title>
    <updated>2026-02-04T13:31:29+00:00</updated>
    <author>
      <name>/u/Awkward_Run_9982</name>
      <uri>https://old.reddit.com/user/Awkward_Run_9982</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvommg/release_eva4bv2_updated_financial_evasion/"&gt; &lt;img alt="[Release] Eva-4B-V2: Updated Financial Evasion Detection Model. Now #1, beating Claude Opus 4.5 &amp;amp; Gemini 3 Flash." src="https://preview.redd.it/46zsrxh2chhg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ead613ff0faafb52ac64b2b37a3497d7c4ddd29" title="[Release] Eva-4B-V2: Updated Financial Evasion Detection Model. Now #1, beating Claude Opus 4.5 &amp;amp; Gemini 3 Flash." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Quick update on Eva-4B ‚Äî we've released &lt;strong&gt;Eva-4B-V2&lt;/strong&gt;, an improved version that now outperforms all frontier LLMs on EvasionBench.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's new in V2:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: 84.9% Macro-F1, beating Gemini 3 Flash (84.6%), Claude Opus 4.5 (84.4%), and GPT-5.2 (80.9%)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training&lt;/strong&gt;: Two-stage fine-tuning on 84K samples (60K consensus + 24K three-judge majority voting)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open Dataset&lt;/strong&gt;: We've released EvasionBench dataset on HuggingFace&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt; Classifies earnings call Q&amp;amp;A into &lt;code&gt;direct&lt;/code&gt;, &lt;code&gt;intermediate&lt;/code&gt;, or &lt;code&gt;fully_evasive&lt;/code&gt;. Helps identify when executives are sidestepping analysts' questions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why use this over a general LLM?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A 4B model running locally that beats models 100x+ its size on this task&lt;/li&gt; &lt;li&gt;Try it instantly in Colab ‚Äî no setup needed&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: &lt;a href="https://huggingface.co/FutureMa/Eva-4B-V2"&gt;https://huggingface.co/FutureMa/Eva-4B-V2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Dataset: &lt;a href="https://huggingface.co/datasets/FutureMa/EvasionBench"&gt;https://huggingface.co/datasets/FutureMa/EvasionBench&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Colab: &lt;a href="https://colab.research.google.com/github/IIIIQIIII/EvasionBench/blob/main/scripts/eva4b_inference.ipynb"&gt;https://colab.research.google.com/github/IIIIQIIII/EvasionBench/blob/main/scripts/eva4b_inference.ipynb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/IIIIQIIII/EvasionBench"&gt;https://github.com/IIIIQIIII/EvasionBench&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Project Page: &lt;a href="https://iiiiqiiii.github.io/EvasionBench/"&gt;https://iiiiqiiii.github.io/EvasionBench/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Awkward_Run_9982"&gt; /u/Awkward_Run_9982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/46zsrxh2chhg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvommg/release_eva4bv2_updated_financial_evasion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvommg/release_eva4bv2_updated_financial_evasion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T13:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvm388</id>
    <title>Qwen3-Coder-Next is available on HuggingChat</title>
    <updated>2026-02-04T11:28:26+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvm388/qwen3codernext_is_available_on_huggingchat/"&gt; &lt;img alt="Qwen3-Coder-Next is available on HuggingChat" src="https://external-preview.redd.it/ts3qmqwhhBSKiMfaD-GP4qTCSy4zry7pFJqkPo5wT7c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b618deb783becca7cdc00ba44e4ab3a6dfaf36bd" title="Qwen3-Coder-Next is available on HuggingChat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/chat/models/Qwen/Qwen3-Coder-Next"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvm388/qwen3codernext_is_available_on_huggingchat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvm388/qwen3codernext_is_available_on_huggingchat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T11:28:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1quzwjf</id>
    <title>ACE-Step-1.5 has just been released. It‚Äôs an MIT-licensed open source audio generative model with performance close to commercial platforms like Suno</title>
    <updated>2026-02-03T18:26:58+00:00</updated>
    <author>
      <name>/u/iGermanProd</name>
      <uri>https://old.reddit.com/user/iGermanProd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quzwjf/acestep15_has_just_been_released_its_an/"&gt; &lt;img alt="ACE-Step-1.5 has just been released. It‚Äôs an MIT-licensed open source audio generative model with performance close to commercial platforms like Suno" src="https://external-preview.redd.it/ZDNiNm9lcXduYmhnMXNUFTz1lD2uwrlR8i5n8_uV8Hgq6zjqVqa04fhxxOUs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b143554ca8c67bc465c8e39d15ee68486eaeef36" title="ACE-Step-1.5 has just been released. It‚Äôs an MIT-licensed open source audio generative model with performance close to commercial platforms like Suno" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://xcancel.com/acemusicAI/status/2018731205546684678"&gt;https://xcancel.com/acemusicAI/status/2018731205546684678&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://ace-step.github.io/ace-step-v1.5.github.io/"&gt;https://ace-step.github.io/ace-step-v1.5.github.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It‚Äôs already supported in Comfy. MIT license. HuggingFace Demo is also available! Pretty much the whole package - LoRAs are supported, multiple different models to tailor to different needs, cover and repainting features. This is the closest open-source has gotten to Suno and similar top-slop platforms. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iGermanProd"&gt; /u/iGermanProd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/r7v6v6qwnbhg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quzwjf/acestep15_has_just_been_released_its_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quzwjf/acestep15_has_just_been_released_its_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T18:26:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvsh0x</id>
    <title>Prompt Repetition Improves Non-Reasoning LLMs - article</title>
    <updated>2026-02-04T16:03:01+00:00</updated>
    <author>
      <name>/u/Loskas2025</name>
      <uri>https://old.reddit.com/user/Loskas2025</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/html/2512.14982v1"&gt;https://arxiv.org/html/2512.14982v1&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Prompt repetition improves the accuracy of Gemini 2.0 Flash-Lite on NameIndex from 21.33% to 97.33%.&lt;/p&gt; &lt;p&gt;Interesting article. Has anyone actually tried it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loskas2025"&gt; /u/Loskas2025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvsh0x/prompt_repetition_improves_nonreasoning_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvsh0x/prompt_repetition_improves_nonreasoning_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvsh0x/prompt_repetition_improves_nonreasoning_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T16:03:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvlh5n</id>
    <title>Qwen Coders Visual Benchmark</title>
    <updated>2026-02-04T10:53:25+00:00</updated>
    <author>
      <name>/u/loadsamuny</name>
      <uri>https://old.reddit.com/user/loadsamuny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to compare the new Qwen Coders so I ran various gguf (IQ1 vs Q3 vs Q4) quants of Qwen Coder Next, along with Coder 30B and VL 32B just to compare vs non coder.&lt;/p&gt; &lt;p&gt;The lightshow test is the one most fail and only the 30B passed it. &lt;/p&gt; &lt;p&gt;All code and prompts are up at&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/electricazimuth/LocalLLM%5C_VisualCodeTest"&gt;https://github.com/electricazimuth/LocalLLM\_VisualCodeTest&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/loadsamuny"&gt; /u/loadsamuny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://electricazimuth.github.io/LocalLLM_VisualCodeTest/results/2026.02.04/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvlh5n/qwen_coders_visual_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvlh5n/qwen_coders_visual_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T10:53:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvtfyk</id>
    <title>CuaBot v1.0 released, an MIT-licensed tool to run any GUI/TUI agent in a sandbox with co-operative computer-use, seamless per-window H.264 streaming, and multi-cursor support</title>
    <updated>2026-02-04T16:38:11+00:00</updated>
    <author>
      <name>/u/a6oo</name>
      <uri>https://old.reddit.com/user/a6oo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvtfyk/cuabot_v10_released_an_mitlicensed_tool_to_run/"&gt; &lt;img alt="CuaBot v1.0 released, an MIT-licensed tool to run any GUI/TUI agent in a sandbox with co-operative computer-use, seamless per-window H.264 streaming, and multi-cursor support" src="https://preview.redd.it/qaapo5x98ihg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75317cfbd5e04f5c7d3d514663961cd80537b9a3" title="CuaBot v1.0 released, an MIT-licensed tool to run any GUI/TUI agent in a sandbox with co-operative computer-use, seamless per-window H.264 streaming, and multi-cursor support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLaMa"&gt;r/LocalLaMa&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;CuaBot is our MIT-licensed tool to launch any CLI agent (Claude Code, OpenClaw, Codex, etc.) or GUI app inside a sandbox with computer-use. Agent windows appear natively on your desktop with a colored border. &lt;/p&gt; &lt;p&gt;This enables what I like to call &lt;em&gt;co-op mode&lt;/em&gt;: you and your agent work in the same windows with separate cursors, without any mouse/focus hijacking or invasive full-desktop screenshots.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What you can do:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;$ npx cuabot claude&lt;/code&gt;&lt;br /&gt; &lt;code&gt;&amp;gt; &amp;quot;Write a 2-player tic-tac-toe game, then let's play. I'll go first&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Claude Code will open the game in a sandboxed window on your desktop. When ready, you click your move through the native window while the agent watches and waits to click its move. The agent can see your cursor and its windows while keeping your full desktop isolated.&lt;/p&gt; &lt;p&gt;&lt;code&gt;# Run agents in parallel:&lt;/code&gt;&lt;br /&gt; &lt;code&gt;$ npx cuabot -n research openclaw&lt;/code&gt;&lt;br /&gt; &lt;code&gt;$ npx cuabot -n coding codex&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;# Or script the CLI:&lt;/code&gt;&lt;br /&gt; &lt;code&gt;$ npx cuabot libreoffice --writer &amp;amp;&lt;/code&gt;&lt;br /&gt; &lt;code&gt;$ npx cuabot --click 150 48&lt;/code&gt;&lt;br /&gt; &lt;code&gt;$ npx cuabot --type ‚ÄúI ‚ù§Ô∏è Cua!‚Äù&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Right now my cuabot agent is exploring mobile/desktop apps to turn into cuabench RL environments. I can watch the windows appear, intervene when it gets stuck, and let it continue until it opens the completed GUI gym for me to interact with.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why we built this:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We built the Cua OSS SDK for building and benchmarking computer-use systems with GUI sandboxes. We kept seeing two common UX patterns when people built computer-use agents:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Agent screenshots your desktop and controls your mouse&lt;/strong&gt; ‚Äì Works with your data, but unsafe and locks you out&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agent runs in a sandbox with an external VNC desktop&lt;/strong&gt; ‚Äì Safer, but clunky to monitor, hard to interact with, and tedious for data transfer&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;General computer-use should be frictionless. Asking your agent to debug a GUI app shouldn't require opening an entire desktop stream. The GUI app should just appear alongside your windows, sandboxed and ready.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;cuabot [command]&lt;/code&gt; launches &lt;code&gt;cuabotd&lt;/code&gt;, which manages a Ubuntu + Xpra Docker container, a multi-cursor overlay, an Xpra computer-use MCP server, and an Xpra seamless client. It auto-configures your agent (Claude, Aider, etc.) to connect to the computer-use MCP, then pipes terminal I/O through WebSocket. The Xpra client automatically detects and streams windows launched in the container, with H.264 encoding, audio, and customizable clipboard sharing. &lt;/p&gt; &lt;p&gt;Since the computer-use MCP interacts through an Xpra client, the agent only sees the windows it needs, sparing it from your desktop clutter!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt; (monorepo; libs/cuabot directory)&lt;br /&gt; Docs: &lt;a href="https://cua.ai/docs/cuabot/cuabot"&gt;https://cua.ai/docs/cuabot/cuabot&lt;/a&gt;&lt;br /&gt; npm: &lt;a href="https://www.npmjs.com/package/cuabot"&gt;https://www.npmjs.com/package/cuabot&lt;/a&gt;&lt;br /&gt; installer/onboarding: &lt;code&gt;npx cuabot&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a6oo"&gt; /u/a6oo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qaapo5x98ihg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvtfyk/cuabot_v10_released_an_mitlicensed_tool_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvtfyk/cuabot_v10_released_an_mitlicensed_tool_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T16:38:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvtk9d</id>
    <title>Kimi K2.5 set a new record among open-weight models on the Epoch Capabilities Index (ECI), which combines multiple benchmarks onto a single scale. Its score of 147 is about on par with o3, Grok 4, and Sonnet 4.5. It still lags the overall frontier.</title>
    <updated>2026-02-04T16:42:34+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvtk9d/kimi_k25_set_a_new_record_among_openweight_models/"&gt; &lt;img alt="Kimi K2.5 set a new record among open-weight models on the Epoch Capabilities Index (ECI), which combines multiple benchmarks onto a single scale. Its score of 147 is about on par with o3, Grok 4, and Sonnet 4.5. It still lags the overall frontier." src="https://preview.redd.it/kqk0iq3waihg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1a75f43dadb5c39aa582ac5e08cca8259b7d970e" title="Kimi K2.5 set a new record among open-weight models on the Epoch Capabilities Index (ECI), which combines multiple benchmarks onto a single scale. Its score of 147 is about on par with o3, Grok 4, and Sonnet 4.5. It still lags the overall frontier." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kqk0iq3waihg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvtk9d/kimi_k25_set_a_new_record_among_openweight_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvtk9d/kimi_k25_set_a_new_record_among_openweight_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T16:42:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvjonm</id>
    <title>First Qwen3-Coder-Next REAP is out</title>
    <updated>2026-02-04T09:04:09+00:00</updated>
    <author>
      <name>/u/Dany0</name>
      <uri>https://old.reddit.com/user/Dany0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvjonm/first_qwen3codernext_reap_is_out/"&gt; &lt;img alt="First Qwen3-Coder-Next REAP is out" src="https://external-preview.redd.it/j98XKqoJ3UOGeW66Etg0lVtFqPsaabyeyZuH8PQVb-0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=234ec5f7ffcda5d2272c5b48c2652755e36ad2b9" title="First Qwen3-Coder-Next REAP is out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;40% REAP&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dany0"&gt; /u/Dany0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lovedheart/Qwen3-Coder-Next-REAP-48B-A3B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvjonm/first_qwen3codernext_reap_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvjonm/first_qwen3codernext_reap_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T09:04:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvv8ps</id>
    <title>GPT-4o's system prompt now includes instructions for handling users upset about its upcoming Feb 13 shutdown (including 'dyad pair' and 'gnosis revelation' edge cases)</title>
    <updated>2026-02-04T17:42:26+00:00</updated>
    <author>
      <name>/u/frubberism</name>
      <uri>https://old.reddit.com/user/frubberism</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvv8ps/gpt4os_system_prompt_now_includes_instructions/"&gt; &lt;img alt="GPT-4o's system prompt now includes instructions for handling users upset about its upcoming Feb 13 shutdown (including 'dyad pair' and 'gnosis revelation' edge cases)" src="https://preview.redd.it/na7gtkyjkihg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b514f505e77f8c426d994d5b69332b04dadfda4" title="GPT-4o's system prompt now includes instructions for handling users upset about its upcoming Feb 13 shutdown (including 'dyad pair' and 'gnosis revelation' edge cases)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frubberism"&gt; /u/frubberism &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/na7gtkyjkihg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvv8ps/gpt4os_system_prompt_now_includes_instructions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvv8ps/gpt4os_system_prompt_now_includes_instructions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T17:42:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvo91g</id>
    <title>Intern-S1-Pro</title>
    <updated>2026-02-04T13:14:55+00:00</updated>
    <author>
      <name>/u/lly0571</name>
      <uri>https://old.reddit.com/user/lly0571</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-Pro"&gt;https://huggingface.co/internlm/Intern-S1-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Another 1T-ish VLM. Looks like a Qwen3-235B scaled to 512 experts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lly0571"&gt; /u/lly0571 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvo91g/interns1pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvo91g/interns1pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvo91g/interns1pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T13:14:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvp0hm</id>
    <title>model: (qwen3next) correct vectorized key_gdiff calculation by ngxson ¬∑ Pull Request #19324 ¬∑ ggml-org/llama.cpp</title>
    <updated>2026-02-04T13:47:55+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvp0hm/model_qwen3next_correct_vectorized_key_gdiff/"&gt; &lt;img alt="model: (qwen3next) correct vectorized key_gdiff calculation by ngxson ¬∑ Pull Request #19324 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/Dqgg7ZvrLWPUlWr_lQFMlLvrUGKt4Wjs_hNwPvpf-8k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cecc3e5f53db33a22bb927dfd55b97e94627ad38" title="model: (qwen3next) correct vectorized key_gdiff calculation by ngxson ¬∑ Pull Request #19324 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(First?) Fix for Qwen Next Coder&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19324"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvp0hm/model_qwen3next_correct_vectorized_key_gdiff/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvp0hm/model_qwen3next_correct_vectorized_key_gdiff/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T13:47:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvp2hg</id>
    <title>internlm/Intern-S1-Pro ¬∑ Hugging Face</title>
    <updated>2026-02-04T13:50:20+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvp2hg/internlminterns1pro_hugging_face/"&gt; &lt;img alt="internlm/Intern-S1-Pro ¬∑ Hugging Face" src="https://external-preview.redd.it/YxAPCHfyx1X69aAa5eRKFFzDTrzC_SvUlWSg_aGoYn8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1a22c338f7ffdcfdd1ac0da2068e064b078cc48" title="internlm/Intern-S1-Pro ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;from internlm:&lt;/p&gt; &lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;We introduce Intern-S1-Pro, a trillion-scale MoE multimodal scientific reasoning model. Intern-S1-Pro scales to 1T total parameters with 512 experts, activating 8 experts per token (22B activated parameters). The model delivers top-tier performance on advanced reasoning benchmarks and achieves leading results across key AI4Science domains (chemistry, materials, life-science, earth, etc.), while maintaining strong general multimodal and text capabilities.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-Pro#features"&gt;&lt;/a&gt;Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;State-of-the-art scientific reasoning, competitive with leading closed-source models across AI4Science tasks.&lt;/li&gt; &lt;li&gt;Strong general multimodal performance on various benchmarks.&lt;/li&gt; &lt;li&gt;Trillion-scale MoE training efficiency with STE routing (dense gradient for router training) and grouped routing for stable convergence and balanced expert parallelism.&lt;/li&gt; &lt;li&gt;Fourier Position Encoding (FoPE) + upgraded time-series modeling for better physical signal representation; supports long, heterogeneous time-series (10^0‚Äì10^6 points).&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/internlm/Intern-S1-Pro"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvp2hg/internlminterns1pro_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvp2hg/internlminterns1pro_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T13:50:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvox18</id>
    <title>Intern-S1-Pro (1T/A22B)</title>
    <updated>2026-02-04T13:43:51+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvox18/interns1pro_1ta22b/"&gt; &lt;img alt="Intern-S1-Pro (1T/A22B)" src="https://preview.redd.it/kobet850fhhg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3ca84ca0879baf4bbb204fc239f5b6087ee3a57" title="Intern-S1-Pro (1T/A22B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄIntroducing Intern-S1-Pro, an advanced 1T MoE open-source multimodal scientific reasoning model.&lt;/p&gt; &lt;p&gt;- SOTA scientific reasoning, competitive with leading closed-source models across AI4Science tasks.&lt;/p&gt; &lt;p&gt;- Top-tier performance on advanced reasoning benchmarks, strong general multimodal performance on various benchmarks.&lt;/p&gt; &lt;p&gt;- 1T-A22B MoE training efficiency with STE routing (dense gradient for router training) and grouped routing for stable convergence and balanced expert parallelism.&lt;/p&gt; &lt;p&gt;- Fourier Position Encoding (FoPE) + upgraded time-series modeling for better physical signal representation; supports long, heterogeneous time-series (10^0‚Äì10^6 points).&lt;/p&gt; &lt;p&gt;- Intern-S1-Pro is now supported by vLLM @vllm_project and SGLang @sgl_project @lmsysorg ‚Äî more ecosystem integrations are on the way.&lt;/p&gt; &lt;p&gt;Huggingface: &lt;a href="https://huggingface.co/internlm/Intern-S1-Pro"&gt;https://huggingface.co/internlm/Intern-S1-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/InternLM/Intern-S1"&gt;https://github.com/InternLM/Intern-S1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kobet850fhhg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvox18/interns1pro_1ta22b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvox18/interns1pro_1ta22b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T13:43:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvrib9</id>
    <title>mistralai/Voxtral-Mini-4B-Realtime-2602 ¬∑ Hugging Face</title>
    <updated>2026-02-04T15:27:12+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvrib9/mistralaivoxtralmini4brealtime2602_hugging_face/"&gt; &lt;img alt="mistralai/Voxtral-Mini-4B-Realtime-2602 ¬∑ Hugging Face" src="https://external-preview.redd.it/RirqAaXL1g9xgccy6jCj8FpDgCmNmT4kPmfCbcwIIl8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4fa31e09623598564a99beea3a398a9c824d4f9" title="mistralai/Voxtral-Mini-4B-Realtime-2602 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Voxtral Mini 4B Realtime 2602 is a &lt;strong&gt;multilingual, realtime speech-transcription model&lt;/strong&gt; and among the first open-source solutions to achieve accuracy comparable to offline systems with a delay of &lt;strong&gt;&amp;lt;500ms&lt;/strong&gt;. It supports &lt;strong&gt;13 languages&lt;/strong&gt; and outperforms existing open-source baselines across a range of tasks, making it ideal for applications like voice assistants and live subtitling.&lt;/p&gt; &lt;p&gt;Built with a &lt;strong&gt;natively streaming architecture&lt;/strong&gt; and a custom causal audio encoder - it allows configurable transcription delays (240ms to 2.4s), enabling users to balance &lt;strong&gt;latency and accuracy&lt;/strong&gt; based on their needs. At a &lt;strong&gt;480ms delay&lt;/strong&gt;, it matches the performance of leading offline open-source transcription models, as well as realtime APIs.&lt;/p&gt; &lt;p&gt;As a &lt;strong&gt;4B-parameter model&lt;/strong&gt;, is optimized for &lt;strong&gt;on-device deployment&lt;/strong&gt;, requiring minimal hardware resources. It runs in realtime with on devices minimal hardware with throughput exceeding 12.5 tokens/second.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvrib9/mistralaivoxtralmini4brealtime2602_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvrib9/mistralaivoxtralmini4brealtime2602_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T15:27:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvrc59</id>
    <title>Some hard lessons learned building a private H100 cluster (Why PCIe servers failed us for training)</title>
    <updated>2026-02-04T15:20:42+00:00</updated>
    <author>
      <name>/u/NTCTech</name>
      <uri>https://old.reddit.com/user/NTCTech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;sup&gt;Just wanted to dump some notes here after spending the last few months architecting a private training stack (70B+ param models. We initially tried to save budget by looking at standard PCIe servers instead of the HGX/SXM form factors, and honestly, the &amp;quot;paper math&amp;quot; vs. reality was a brutal wake-up call.&lt;/sup&gt;)&lt;/p&gt; &lt;p&gt;&lt;sup&gt;Thought this might save someone else the headache if you're trying to move from inference to actual training runs on-prem.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;&lt;sup&gt;1. The &amp;quot;NVLink Tax&amp;quot; isn't optional for training. We tried to model this out with PCIe Gen5, but the math just falls apart. When you're doing All-Reduce ops across nodes, PCIe caps out at \&lt;/sup&gt;128 GB/s. NVLink is pushing ~900 GB/s. If you cheap out here, you basically end up with expensive GPUs sitting idle, waiting for data. For inference, PCIe is totally fine. For training, it‚Äôs a bottleneck that kills your ROI.)&lt;/p&gt; &lt;p&gt;&lt;sup&gt;2. Storage checkpoints are violent. This was the biggest surprise. Everyone talks about GPU VRAM, but nobody warned us about the checkpoint writes. A 175B model dumps a \&lt;/sup&gt;2.5TB checkpoint. To keep the GPUs from stalling, you need to write that to disk in under a minute. Our standard NFS filer absolutely choked. We had to look at parallel filesystems (Weka/VAST or local NVMe raid just to survive the write bursts.))&lt;/p&gt; &lt;p&gt;&lt;sup&gt;3. You don't need InfiniBand, but Ethernet is annoying. We didn't have the budget/staff for an InfiniBand fabric, so we went with RoCEv2 on standard switches. It works, but it‚Äôs finicky. One silent buffer overflow or a misconfigured PFC (Priority Flow Control setting can stall the whole cluster. If you go Ethernet, monitor your pause frames religiously.&lt;/sup&gt;)&lt;/p&gt; &lt;p&gt;&lt;sup&gt;Anyway, I wrote up a longer deep dive with the specific diagrams and our decision framework for &amp;quot;Sandbox vs Production&amp;quot; builds if anyone is interested. Link is pinned in my profile.&lt;/sup&gt;&lt;/p&gt; &lt;p&gt;&lt;sup&gt;Happy to answer questions on the networking side - that RoCEv2 tuning took years off my life.&lt;/sup&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NTCTech"&gt; /u/NTCTech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvrc59/some_hard_lessons_learned_building_a_private_h100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvrc59/some_hard_lessons_learned_building_a_private_h100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvrc59/some_hard_lessons_learned_building_a_private_h100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T15:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qvq0xe</id>
    <title>Bashing Ollama isn‚Äôt just a pleasure, it‚Äôs a duty</title>
    <updated>2026-02-04T14:29:48+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvq0xe/bashing_ollama_isnt_just_a_pleasure_its_a_duty/"&gt; &lt;img alt="Bashing Ollama isn‚Äôt just a pleasure, it‚Äôs a duty" src="https://preview.redd.it/ad5zhvq0nhhg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b9fa62de0e64a6887124b87e66b3b99b2942107" title="Bashing Ollama isn‚Äôt just a pleasure, it‚Äôs a duty" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ad5zhvq0nhhg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qvq0xe/bashing_ollama_isnt_just_a_pleasure_its_a_duty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qvq0xe/bashing_ollama_isnt_just_a_pleasure_its_a_duty/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-04T14:29:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
