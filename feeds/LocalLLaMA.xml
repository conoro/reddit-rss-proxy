<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-07T02:13:31+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1o02ci4</id>
    <title>LLM question</title>
    <updated>2025-10-07T02:04:55+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are there any models that are singularly focused on individual coding tasks? Like for example python only or flutter etc? I’m extremely lucky that I was able to build my memory system with only help from ChatGPT and Claude in VS Code. I’m not very good at coding myself. I’m good at the overall design of something. Like knowing how I want something to work, but due to having severe ADHD, and having had 4 strokes, my memory doesn’t really work all that well anymore for learning how to code something. So if anyone can direct me to a model that excels at coding in the 30B to 70B area or is explicitly for coding that would be a great help &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o02ci4/llm_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o02ci4/llm_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o02ci4/llm_question/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T02:04:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzhvoo</id>
    <title>Is agentic programming on own HW actually feasible?</title>
    <updated>2025-10-06T12:38:03+00:00</updated>
    <author>
      <name>/u/petr_bena</name>
      <uri>https://old.reddit.com/user/petr_bena</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Being a senior dev I gotta admit that latest models are really good, yes it's still not &amp;quot;job replacing&amp;quot; good, but they are surprisingly capable (I am talking mostly about Claude 4.5 and similar). I was making some simple calculations and it seems to me that these agentic tools that they are selling now are almost impossible to return any profit to them with current prices, it seems like they just pushed the prices as low as possible to onboard all possible enterprise customers and get them totally dependent on their AI services before dramatically increasing the price, so I am assuming all these are available just temporarily.&lt;/p&gt; &lt;p&gt;So yes, agentic programming on those massive GPU farms with hundreds of thousand GPUs look like it work great, because it writes a lot of output very fast (1000TPS+), but since you can't rely on this stuff being &amp;quot;almost free&amp;quot; forever, I am wondering: Is running similar models locally to get any real work done actually feasible?&lt;/p&gt; &lt;p&gt;I have a rather low-end HW for AI (16GB VRAM on RTX 4060Ti + 64 GB DDR4 on mobo) and best models I could get to run were &amp;lt; 24b models with quantization or higher parameter models using DMA to motherboard (which resulted in inference being about 10x slower, but it gave me an idea what I would be able to get with slightly more VRAM).&lt;/p&gt; &lt;p&gt;Smaller models are IMHO absolutely unusable. They just can't get any real or useful work done. For stuff similar to Claude you probably need something like deepseek or llama full with FP16, that's like 671b parameters, so what kind of VRAM you need for that? 512GB is probably minimum if you run some kind of quantization (dumbing the model down). If you want some decent context window too, that's like 1TB VRAM?&lt;/p&gt; &lt;p&gt;Then how fast is that going to be, if you get something like Mac Studio with shared RAM between CPU and GPU? What TPS you get? 5? 10? Maybe even less?&lt;/p&gt; &lt;p&gt;I think with that speed, you don't only have to spend ENORMOUS money upfront, but you end up with something that will need 2 hours to solve something you could do by yourself in 1 hour.&lt;/p&gt; &lt;p&gt;Sure you can keep it running when you are sleeping working over night, but then you still have to pay electricity right? We talk about system that could easily have 1, maybe 2kW input at that size?&lt;/p&gt; &lt;p&gt;Or maybe my math is totally off? IDK, is there anyone that actually does it and built a system that can run top models and get agentic programming work done on similar level of quality you get from Claude 4.5 or codex? How much did it cost to buy? How fast is it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/petr_bena"&gt; /u/petr_bena &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzhvoo/is_agentic_programming_on_own_hw_actually_feasible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzhvoo/is_agentic_programming_on_own_hw_actually_feasible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzhvoo/is_agentic_programming_on_own_hw_actually_feasible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T12:38:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzal91</id>
    <title>My experience coding with open models (Qwen3, GLM 4.6, Kimi K2) inside VS Code</title>
    <updated>2025-10-06T05:23:36+00:00</updated>
    <author>
      <name>/u/Arindam_200</name>
      <uri>https://old.reddit.com/user/Arindam_200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been using &lt;strong&gt;Cursor&lt;/strong&gt; for a while, mainly for its smooth AI coding experience. But recently, I decided to move my workflow back to &lt;strong&gt;VS Code&lt;/strong&gt; and test how far &lt;strong&gt;open-source coding models&lt;/strong&gt; have come.&lt;/p&gt; &lt;p&gt;The setup I’m using is simple:&lt;br /&gt; - VS Code + Hugging Face Copilot Chat extension&lt;br /&gt; - Models: Qwen 3, GLM 4.6, and Kimi K2&lt;/p&gt; &lt;p&gt;Honestly, I didn’t expect much at first, but the results have been surprisingly solid.&lt;br /&gt; Here’s what stood out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;These open models handle refactoring, commenting, and quick edits really well.&lt;/li&gt; &lt;li&gt;They’re &lt;strong&gt;way&lt;/strong&gt; cheaper than proprietary models, no token anxiety, no credit drain.&lt;/li&gt; &lt;li&gt;You can switch models on the fly, depending on task complexity.&lt;/li&gt; &lt;li&gt;No vendor lock-in, full transparency, and control inside your editor.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I still agree that Claude 4.5 or GPT-5 outperform in deep reasoning and complex tasks, but for 50–60% of everyday work, writing code, debugging, or doc generation, these open models perform just fine.&lt;/p&gt; &lt;p&gt;It feels like the first time open LLMs can actually compete with closed ones in real-world dev workflows. I also made a short tutorial showing how to set it up step-by-step if you want to try it: &lt;a href="https://youtu.be/6pcBBLXxOEc"&gt;Setup guide&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would love to hear your thoughts on these open source models!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arindam_200"&gt; /u/Arindam_200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzal91/my_experience_coding_with_open_models_qwen3_glm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzal91/my_experience_coding_with_open_models_qwen3_glm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzal91/my_experience_coding_with_open_models_qwen3_glm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T05:23:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzpjz8</id>
    <title>How did LM Studio convert IBM's Granite 4.0 models to GGUF?</title>
    <updated>2025-10-06T17:30:27+00:00</updated>
    <author>
      <name>/u/WowSkaro</name>
      <uri>https://old.reddit.com/user/WowSkaro</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had been under the impression that the GGUF format only supported the transformers architecture, and that hybrid transformers/mamba models were not able to be converted into GGUF format. But, somehow, LM Studio has GGUF files for all the IBM hybrid transformers/mamba2 Granite 4.0 LLM models: &lt;a href="https://huggingface.co/lmstudio-community/granite-4.0-h-small-GGUF"&gt;granite-4.0-h-small-GGUF&lt;/a&gt;, &lt;a href="https://huggingface.co/lmstudio-community/granite-4.0-h-tiny-GGUF"&gt;granite-4.0-h-tiny-GGUF&lt;/a&gt; and &lt;a href="https://huggingface.co/lmstudio-community/granite-4.0-micro-GGUF"&gt;granite-4.0-micro-GGUF&lt;/a&gt;. How is this possible? Did Georgi Gerganov (or some contributor) update the GGUF format to include hybrid transformers/mamba models? &lt;/p&gt; &lt;p&gt;I have been trying to get Microsoft's Phi-4-mini-flash-reasoning to run in my PC for a month already and have been stuck at trying to get vLLM to run on Windows together with all the requirements that are needed to run the Phi-4-mini-flash-reasoning model, but they seem to be speciffically made to target Linux (oh! The irony!) ((Also, as I know some people will be posting in the comments, the Phi-4-mini-flash-reasoning is not the Phi-4-mini or the Phi-4-mini-reasoning, those are standard transformer models; The Phi-4-mini-flash-reasoning is a hybrid transformers(SWA)/mamba(1) model (SambaY) that somehow has higher benchmark scores than the full transformers Phi-4-mini-reasoning model)).&lt;/p&gt; &lt;p&gt;If conversion to the GGUF format is possible for transformers/mamba hybrid models, I would like to try converting the Phi-4-mini-flash-reasoning to GGUF and Nvidia's Nemotron-Nano-9B-v2 which is a transformers/mamba2 hybrid model focused on coding (I have been using &lt;a href="https://build.nvidia.com/microsoft/phi-4-mini-flash-reasoning"&gt;https://build.nvidia.com/microsoft/phi-4-mini-flash-reasoning&lt;/a&gt; and &lt;a href="https://build.nvidia.com/nvidia/nvidia-nemotron-nano-9b-v2"&gt;https://build.nvidia.com/nvidia/nvidia-nemotron-nano-9b-v2&lt;/a&gt; to test these models, was happy with their performance, and wanted to try running them locally; Strangely, enough I thought that Nemotron-Nano-9B-v2 was some type of expansion of the Phi-4-mini-flash-reasoning since some responses of them seemed to be formated in the same way, but apparently Nemotron-Nano-9B-v2 is a hybrid of traditional transformers and mamba2, whereas Phi-4-mini-flash-reasoning is a hybrid of transformers using sliding window attention (SWA) with mamba1 which guarantees linear inference cost by input length. I suppose they may have just used the same open-source data for trainning the base model).&lt;/p&gt; &lt;p&gt;The fact that Phi-4-mini-flash-reasoning uses sliding window attention (SWA) and gated memory units (GMU), I think that sliding window attention must already be translatable to the GGUF format, since the gemma-3 models use it and are available in GGUF formats, but perhaps the gated memory units (GMU) or the fact that it uses mamba1 instead of mamba2 might be a obstacle for Phi-4-mini-flash-reasoning in particular. Although, there should be no such problem with Nvidia's Nemotron-Nano-9B-v2 since it doesn't use SWA or GMU or mamba1; which should make the model be somewhat equivalent to IBM's Granite 4.0 hybrid transformers/mamba2 LLM models, which have been converted to the GGUF format, as I already said.&lt;/p&gt; &lt;p&gt;Although Granite 4.0 and Nemotron-Nano-9B-v2 use mamba2 to decrease the computational cost of inference, since they still use full attention they must still increase quadratically their inference cost with the input length, as the attention window is a fixed size and just slides to the most recent input, Phi-4-mini-flash-reasoning should only increase linearly, although it appears that even though this might be the case asymptotically, Granite 4.0 seems to have a way lower upfront costs for small inputs (although I don't know if the gains are so big that even growing quadratically, the Granite 4.0 models would still require less compute for the maximum input length than Phi-4-mini-flash-reasoning at the same input length, that said, the fact that Phi-4-mini-flash-reasoning uses SWA should allow it to process a never ending continuously streaming input, since after a certain point, old imputs stop being in the attention context, I believe this was the original idea behind the original Samba model, that was latter refined to the SambaY model with the introduction of the gated memory units (GMU) which I think are used to improve mamba's retention of information (mamba's biggest disadvantage against transformers).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WowSkaro"&gt; /u/WowSkaro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzpjz8/how_did_lm_studio_convert_ibms_granite_40_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzpjz8/how_did_lm_studio_convert_ibms_granite_40_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzpjz8/how_did_lm_studio_convert_ibms_granite_40_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T17:30:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzz007</id>
    <title>Recommendation for a better local model with less "safety" restrictions</title>
    <updated>2025-10-06T23:31:00+00:00</updated>
    <author>
      <name>/u/Away-Lecture-3172</name>
      <uri>https://old.reddit.com/user/Away-Lecture-3172</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using GPT OSS 120b for a while and noticed that it can consult OpenAI policies up to three times during thinking. This feels rather frustrating, I was mostly asking some philosophical questions and asking analyze some text from various books. It was consistently trying to avoid any kind of opinion and hate speech (I have no idea what this even is). As a result its responses are rather disappointing, it feels handicapped when working with other peoples texts and thoughts.&lt;/p&gt; &lt;p&gt;I'm looking for a more transparent, less restricted model that can run on a single RTX PRO 6000 and is good at reading text &amp;quot;as-is&amp;quot;. Definitely less biased compared to OpenAI's creation. What would you recommend?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Away-Lecture-3172"&gt; /u/Away-Lecture-3172 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzz007/recommendation_for_a_better_local_model_with_less/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzz007/recommendation_for_a_better_local_model_with_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzz007/recommendation_for_a_better_local_model_with_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T23:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzgben</id>
    <title>[Update] FamilyBench: New models tested - Claude Sonnet 4.5 takes 2nd place, Qwen 3 Next breaks 70%, new Kimi weirdly below the old version, same for GLM 4.6</title>
    <updated>2025-10-06T11:22:33+00:00</updated>
    <author>
      <name>/u/Orolol</name>
      <uri>https://old.reddit.com/user/Orolol</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello again, I've been testing more models on FamilyBench, my benchmark that tests LLM ability to understand complex tree-like relationships in a family tree across a massive context. For those who missed the initial post: this is a Python program that generates a family tree and uses its structure to generate questions about it. You get a textual description of the tree and questions that are hard to parse for LLMs. GitHub: &lt;a href="https://github.com/Orolol/familyBench"&gt;https://github.com/Orolol/familyBench&lt;/a&gt; &lt;/p&gt; &lt;p&gt;What's new: I've added 4 new models to the leaderboard, including Claude Sonnet 4.5 which shows impressive improvements over Sonnet 4, Qwen 3 Next 80B which demonstrates massive progress in the Qwen family, and GLM 4.6 which surprisingly excels at enigma questions despite lower overall accuracy. All models are tested on the same complex tree with 400 people across 10 generations (~18k tokens). 189 questions are asked (after filtering). Tests run via OpenRouter with low reasoning effort or 8k max tokens, temperature 0.3. Example of family description: &amp;quot;Aaron (M) has white hair, gray eyes, wears a gold hat and works as a therapist. Aaron (M) has 2 children: Barry (M), Erica (F). Abigail (F) has light brown hair, amber eyes, wears a red hat and works as a teacher...&amp;quot; Example of questions: &amp;quot;Which of Paula's grandparents have salt and pepper hair?&amp;quot; &amp;quot;Who is the cousin of the daughter of Quentin with red hair?&amp;quot; &lt;/p&gt; &lt;p&gt;Current Leaderboard:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Accuracy&lt;/th&gt; &lt;th&gt;Total Tokens&lt;/th&gt; &lt;th&gt;No Response Rate&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Gemini 2.5 Pro&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;81.48%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;271,500&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Claude Sonnet 4.5&lt;/strong&gt; &lt;em&gt;(New)&lt;/em&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;77.78%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;211,249&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;DeepSeek R1&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;75.66%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;575,624&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GLM 4.6&lt;/strong&gt; (New)&lt;/td&gt; &lt;td&gt;&lt;strong&gt;74.60%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;245,113&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Gemini 2.5 Flash&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;73.54%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;258,214&lt;/td&gt; &lt;td&gt;2.65%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Qwen 3 Next 80B A3B Thinking&lt;/strong&gt; &lt;em&gt;(New)&lt;/em&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;71.43%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;1,076,302&lt;/td&gt; &lt;td&gt;3.17%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Claude Sonnet 4&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;67.20%&lt;/td&gt; &lt;td&gt;258,883&lt;/td&gt; &lt;td&gt;1.06%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;DeepSeek V3.2 Exp&lt;/strong&gt; &lt;em&gt;(New)&lt;/em&gt;&lt;/td&gt; &lt;td&gt;66.67%&lt;/td&gt; &lt;td&gt;427,396&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GLM 4.5&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;64.02%&lt;/td&gt; &lt;td&gt;216,281&lt;/td&gt; &lt;td&gt;2.12%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GLM 4.5 Air&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;57.14%&lt;/td&gt; &lt;td&gt;1,270,138&lt;/td&gt; &lt;td&gt;26.46%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GPT-OSS 120B&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;50.26%&lt;/td&gt; &lt;td&gt;167,938&lt;/td&gt; &lt;td&gt;1.06%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Qwen3-235B-A22B-Thinking-2507&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;50.26%&lt;/td&gt; &lt;td&gt;1,077,814&lt;/td&gt; &lt;td&gt;20.63%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Kimi K2&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;34.92%&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Kimi K2 0905&lt;/strong&gt; &lt;em&gt;(New)&lt;/em&gt;&lt;/td&gt; &lt;td&gt;31.75%&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Hunyuan A13B&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;30.16%&lt;/td&gt; &lt;td&gt;121,150&lt;/td&gt; &lt;td&gt;2.12%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Mistral Medium 3.1&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;29.63%&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;0.53%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Next plan : Redo all tests en a whole new seed, with harder questions and a larger tree. I have to think how I can decrease the costs first.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Orolol"&gt; /u/Orolol &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzgben/update_familybench_new_models_tested_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzgben/update_familybench_new_models_tested_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzgben/update_familybench_new_models_tested_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T11:22:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzq9vy</id>
    <title>What happened to Longcat models? Why are there no quants available?</title>
    <updated>2025-10-06T17:57:11+00:00</updated>
    <author>
      <name>/u/kaisurniwurer</name>
      <uri>https://old.reddit.com/user/kaisurniwurer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzq9vy/what_happened_to_longcat_models_why_are_there_no/"&gt; &lt;img alt="What happened to Longcat models? Why are there no quants available?" src="https://external-preview.redd.it/SJG1eRbm3SwVjlU4Orqm4a5_PTL6rFKUpPzL-4SiNJI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d1f89904849c371c282657b5befc8d11c2c3998" title="What happened to Longcat models? Why are there no quants available?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaisurniwurer"&gt; /u/kaisurniwurer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Chat"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzq9vy/what_happened_to_longcat_models_why_are_there_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzq9vy/what_happened_to_longcat_models_why_are_there_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T17:57:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o015au</id>
    <title>What is the smallest reasoning model you fine tuned and what do you use it for?</title>
    <updated>2025-10-07T01:09:35+00:00</updated>
    <author>
      <name>/u/SnooMarzipans2470</name>
      <uri>https://old.reddit.com/user/SnooMarzipans2470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wondering what this sub was able to make out of small models like qwen 0.6 b and Gemma 270. Have you been able to get it working for anything useful? What was your experience fine tuning. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooMarzipans2470"&gt; /u/SnooMarzipans2470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o015au/what_is_the_smallest_reasoning_model_you_fine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o015au/what_is_the_smallest_reasoning_model_you_fine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o015au/what_is_the_smallest_reasoning_model_you_fine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T01:09:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nze0lj</id>
    <title>What GPT-oss Leaks About OpenAI's Training Data</title>
    <updated>2025-10-06T09:03:17+00:00</updated>
    <author>
      <name>/u/AppearanceHeavy6724</name>
      <uri>https://old.reddit.com/user/AppearanceHeavy6724</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppearanceHeavy6724"&gt; /u/AppearanceHeavy6724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://fi-le.net/oss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nze0lj/what_gptoss_leaks_about_openais_training_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nze0lj/what_gptoss_leaks_about_openais_training_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T09:03:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzvhth</id>
    <title>Run Open AI GPT-OSS on a mobile phone (Demo)</title>
    <updated>2025-10-06T21:08:24+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzvhth/run_open_ai_gptoss_on_a_mobile_phone_demo/"&gt; &lt;img alt="Run Open AI GPT-OSS on a mobile phone (Demo)" src="https://external-preview.redd.it/MGVqd2I4aDAza3RmMW_X92lPZSdh6fcDHohlh8eF1OAewrbD-P9SZJPkXXH-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=374c8834cc58cb56027f6b87f5aa80d55532bd4b" title="Run Open AI GPT-OSS on a mobile phone (Demo)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sam Altman recently said: “GPT-OSS has strong real-world performance comparable to o4-mini—and you can run it locally on your phone.” Many believed running a 20B-parameter model on mobile devices was still years away. &lt;/p&gt; &lt;p&gt;I am from &lt;a href="https://github.com/NexaAI/nexa-sdk"&gt;Nexa AI&lt;/a&gt;, we’ve managed to run GPT-OSS on a mobile phone for real and want to share with you a demo and its performance&lt;/p&gt; &lt;p&gt;GPT-OSS-20B on Snapdragon Gen 5 with ASUS ROG 9 phone&lt;/p&gt; &lt;ul&gt; &lt;li&gt;17 tokens/sec decoding speed &lt;/li&gt; &lt;li&gt;&amp;lt; 3 seconds Time-to-First-Token&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We think it is super cool and would love to hear everyone's thought. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/o92q3wh03ktf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzvhth/run_open_ai_gptoss_on_a_mobile_phone_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzvhth/run_open_ai_gptoss_on_a_mobile_phone_demo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T21:08:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1o00bnb</id>
    <title>Last week in Multimodal AI - Local Edition</title>
    <updated>2025-10-07T00:31:27+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o00bnb/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last week in Multimodal AI - Local Edition" src="https://b.thumbs.redditmedia.com/vo1uUejG2VE5tG6ipAmjCB3f3SG-JplGsGuMrSb94Ug.jpg" title="Last week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly newsletter on multimodal AI, here are the local/edge highlights from today's edition:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ModernVBERT - 250M beats 2.5B models&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;7x faster CPU inference&lt;/li&gt; &lt;li&gt;Bidirectional attention beats causal by +10.6 nDCG@5&lt;/li&gt; &lt;li&gt;Runs on devices that can't load traditional models&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2510.01149"&gt;Paper&lt;/a&gt; | &lt;a href="https://huggingface.co/ModernVBERT"&gt;HuggingFace&lt;/a&gt; | &lt;a href="https://colab.research.google.com/drive/1bT5LWeO1gPL83GKUZsFeFEleHmEDEQRy"&gt;Colab&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r15po9xz3ltf1.png?width=1170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=729ce13d7c40e57130be324b03b66d1a978b31d7"&gt;https://preview.redd.it/r15po9xz3ltf1.png?width=1170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=729ce13d7c40e57130be324b03b66d1a978b31d7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-VL - GPT-5 performance at 3B active params&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Matches GPT-5-Mini and Claude4-Sonnet&lt;/li&gt; &lt;li&gt;Handles STEM, VQA, OCR, video, agents&lt;/li&gt; &lt;li&gt;FP8 quantized version available&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks"&gt;GitHub&lt;/a&gt; | &lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;DocPruner - Cut storage by 60%&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;lt;1% performance drop&lt;/li&gt; &lt;li&gt;Adaptive pruning per document&lt;/li&gt; &lt;li&gt;Makes multi-vector retrieval affordable&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2509.23883"&gt;Paper&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/05ix4vj34ltf1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d993b6e742f4fd49b72dfdb3c37e3c0fe19a21ba"&gt;The illustration of comparison between OCR-based (a) &amp;amp; LVLM-based (b) paradigms for VDR, and DocPruner (c), a novel framework to adaptively prune the patch-level embeddings for diverse document types.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fathom-DeepResearch - 4B SOTA web investigation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Two specialized 4B models&lt;/li&gt; &lt;li&gt;DuetQA dataset + RAPO optimization&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2509.24107"&gt;Paper&lt;/a&gt; | &lt;a href="https://github.com/FractalAIResearchLabs/Fathom-DeepResearch"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Claude Sonnet 4.5 codes for 30+ hours straight&lt;/li&gt; &lt;li&gt;Ovi generates synchronized audio-video&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1o00bnb/video/qfohebyw4ltf1/player"&gt;https://reddit.com/link/1o00bnb/video/qfohebyw4ltf1/player&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CU-1 achieves 67.5% GUI click accuracy&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1o00bnb/video/8syoo09y4ltf1/player"&gt;https://reddit.com/link/1o00bnb/video/8syoo09y4ltf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full newsletter(demos,papers,more): &lt;a href="https://thelivingedge.substack.com/p/multimodal-monday-27-small-models"&gt;https://thelivingedge.substack.com/p/multimodal-monday-27-small-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o00bnb/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o00bnb/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o00bnb/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T00:31:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzk46z</id>
    <title>Connected a 3090 to my Strix Halo</title>
    <updated>2025-10-06T14:10:06+00:00</updated>
    <author>
      <name>/u/itsjustmarky</name>
      <uri>https://old.reddit.com/user/itsjustmarky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzk46z/connected_a_3090_to_my_strix_halo/"&gt; &lt;img alt="Connected a 3090 to my Strix Halo" src="https://b.thumbs.redditmedia.com/xzsRs0nhefliJmAnyUNq3sxE8eXJ0rNimfEyNYVd8UU.jpg" title="Connected a 3090 to my Strix Halo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/74kng2v31itf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=21fc853640df3ea31c9b968e4f7af6dfc1da91cb"&gt;https://preview.redd.it/74kng2v31itf1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=21fc853640df3ea31c9b968e4f7af6dfc1da91cb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Testing with GPT-OSS-120B MXFP4&lt;/p&gt; &lt;p&gt;Before:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 1034.63 ms / 277 tokens ( 3.74 ms per token, 267.73 tokens per second) eval time = 2328.85 ms / 97 tokens ( 24.01 ms per token, 41.65 tokens per second) total time = 3363.48 ms / 374 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 864.31 ms / 342 tokens ( 2.53 ms per token, 395.69 tokens per second) eval time = 994.16 ms / 55 tokens ( 18.08 ms per token, 55.32 tokens per second) total time = 1858.47 ms / 397 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;llama-server \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--no-mmap \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ngl 999 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;\&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-fa on \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-b 4096 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ub 4096 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--temp 0.7 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--top-p 0.95 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--top-k 50 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--min-p 0.05 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--ctx-size 262114 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--jinja \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--chat-template-kwargs '{&amp;quot;reasoning_effort&amp;quot;:&amp;quot;high&amp;quot;}' \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--alias gpt-oss-120b \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-m &amp;quot;$MODEL_PATH&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;$DEVICE_ARGS \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;$SPLIT_ARGS&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/itsjustmarky"&gt; /u/itsjustmarky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzk46z/connected_a_3090_to_my_strix_halo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzk46z/connected_a_3090_to_my_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzk46z/connected_a_3090_to_my_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T14:10:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o00ban</id>
    <title>Open Source Alternative to Perplexity</title>
    <updated>2025-10-07T00:31:00+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here’s a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mergeable MindMaps.&lt;/li&gt; &lt;li&gt;Note Management&lt;/li&gt; &lt;li&gt;Multi Collaborative Notebooks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o00ban/open_source_alternative_to_perplexity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o00ban/open_source_alternative_to_perplexity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o00ban/open_source_alternative_to_perplexity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T00:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzrhkg</id>
    <title>Conduit 2.0 - OpenWebUI Mobile Client: Completely Redesigned, Faster, and Smoother Than Ever!</title>
    <updated>2025-10-06T18:42:07+00:00</updated>
    <author>
      <name>/u/cogwheel0</name>
      <uri>https://old.reddit.com/user/cogwheel0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzrhkg/conduit_20_openwebui_mobile_client_completely/"&gt; &lt;img alt="Conduit 2.0 - OpenWebUI Mobile Client: Completely Redesigned, Faster, and Smoother Than Ever!" src="https://external-preview.redd.it/NDR0cGwzZ3NkanRmMe3itAdHO7JxtY5YivFkYCiYZ8sXROLwyG4vlc6wIxOg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bc502b079d4c56d8fea5995ce2aefb9a0d85fd7" title="Conduit 2.0 - OpenWebUI Mobile Client: Completely Redesigned, Faster, and Smoother Than Ever!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;A few months back, &lt;a href="https://www.reddit.com/r/selfhosted/comments/1mo9w3t/built_a_native_openwebui_client_for_ios_android/"&gt;I shared my native mobile client for OpenWebUI&lt;/a&gt;. I'm thrilled to drop version 2.0 today, which is basically a full rebuild from the ground up. I've ditched the old limitations for a snappier, more customizable experience that feels right at home on iOS and Android.&lt;/p&gt; &lt;p&gt;If you're running OpenWebUI on your server, this update brings it to life in ways the PWA just can't match. Built with Flutter for cross-platform magic, it's open-source (as always) and pairs perfectly with your self-hosted setup.&lt;/p&gt; &lt;p&gt;Here's what's new in 2.0:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance Overhaul&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Switched to Riverpod 3 for state management, go_router for navigation, and Hive for local storage.&lt;/li&gt; &lt;li&gt;New efficient Markdown parser means smoother scrolling and rendering—chats load instantly, even with long threads. (Pro tip: Data migrates automatically on update. If something glitches, just clear app data and log back in.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Fresh Design &amp;amp; Personalization&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Total UI redesign: Modern, clean interfaces that are easier on the eyes and fingers.&lt;/li&gt; &lt;li&gt;Ditch the purple-only theme, pick from new accent colors.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upgraded Chat Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Share handling:&lt;/strong&gt; Share text/image/files from anywhere to start a chat. Android users also get an OS-wide 'Ask Conduit' context menu option when selecting text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Two input modes:&lt;/strong&gt; Minimal for quick chats, or extended with one-tap access to tools, image generation, and web search.&lt;/li&gt; &lt;li&gt;Slash commands! Type &amp;quot;/&amp;quot; in the input to pull up workspace prompts.&lt;/li&gt; &lt;li&gt;Follow-up suggestions to keep conversations flowing.&lt;/li&gt; &lt;li&gt;Mermaid diagrams now render beautifully in.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;AI Enhancements&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text-to-Speech (TTS) for reading responses aloud. (Live calling is being worked on for the next release!)&lt;/li&gt; &lt;li&gt;Realtime status updates for image gen, web searches, and tools, matching OpenWebUI's polished UX.&lt;/li&gt; &lt;li&gt;Sources and citations for web searches and RAG based responses.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Grab it now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;iOS&lt;/strong&gt;: &lt;a href="https://apps.apple.com/us/app/conduit-openwebui-client/id6749840287"&gt;App Store Link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Android&lt;/strong&gt;: &lt;a href="https://play.google.com/store/apps/details?id=app.cogwheel.conduit"&gt;Google Play Link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Source &amp;amp; Builds&lt;/strong&gt;: &lt;a href="https://github.com/cogwheel0/conduit"&gt;GitHub Repo&lt;/a&gt; (FOSS forever—stars and PRs welcome!)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Huge thanks to the community for the feedback on 1.x. What do you think? Any must-have features for 2.1? Post below, or open an issue on GitHub if you're running into setup quirks. Happy self-hosting!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cogwheel0"&gt; /u/cogwheel0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/s0i7luesdjtf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzrhkg/conduit_20_openwebui_mobile_client_completely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzrhkg/conduit_20_openwebui_mobile_client_completely/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T18:42:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzx061</id>
    <title>GLM 4.6 is the top new open weight model on Design Arena</title>
    <updated>2025-10-06T22:06:33+00:00</updated>
    <author>
      <name>/u/No-Tackle-5388</name>
      <uri>https://old.reddit.com/user/No-Tackle-5388</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzx061/glm_46_is_the_top_new_open_weight_model_on_design/"&gt; &lt;img alt="GLM 4.6 is the top new open weight model on Design Arena" src="https://b.thumbs.redditmedia.com/omSAbCz7n8MkX7LnPoLxwq4PBhxTLhqcszulgIHDZZI.jpg" title="GLM 4.6 is the top new open weight model on Design Arena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM 4.6 is outperforming the new Kimi models and both DeepSeek 3.2 and 3.2-exp in the seven day overall category on design arena. It's also beating every non-Anthropic SOTA model.&lt;/p&gt; &lt;p&gt;I saw a post a few days ago showing it also took the top position on lmarena (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nxbbxe/glm%5C_46%5C_new%5C_best%5C_open%5C_weight%5C_overall%5C_on%5C_lmarena/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1nxbbxe/glm\_46\_new\_best\_open\_weight\_overall\_on\_lmarena/&lt;/a&gt;) and it looks like it's doing the same for visual reasoning. This model is insane&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hzp0gpp8ektf1.png?width=1883&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a7c84277e40c130e803a7bb5c6c7d8a2674f6a1"&gt;https://preview.redd.it/hzp0gpp8ektf1.png?width=1883&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a7c84277e40c130e803a7bb5c6c7d8a2674f6a1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Tackle-5388"&gt; /u/No-Tackle-5388 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzx061/glm_46_is_the_top_new_open_weight_model_on_design/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzx061/glm_46_is_the_top_new_open_weight_model_on_design/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzx061/glm_46_is_the_top_new_open_weight_model_on_design/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T22:06:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzzurf</id>
    <title>Granite 4 (gguf) is useless if you try to use the full 128k context.</title>
    <updated>2025-10-07T00:09:43+00:00</updated>
    <author>
      <name>/u/mantafloppy</name>
      <uri>https://old.reddit.com/user/mantafloppy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt; After some research, no model is actually able to use that context size, all model maker are liar. I'm learning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; its useless with long context from my test with multiple model, and configuration. Both MLX and GUFF&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;I had a special task, required 156k token, decided to try it.&lt;/p&gt; &lt;p&gt;I have a game guide i made with AI, i know its full of error(i'm slowly correcting them as i spot them), so i gave the guide, with the full wiki of said game, and ask the model to find mistake.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The website contain wrong information. Find them by comparing the information to the official wiki. Report all of them. &amp;lt;website&amp;gt; ... &amp;lt;/website&amp;gt; &amp;lt;game wiki&amp;gt; ... &amp;lt;/game wiki&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With LmStudio, All runtime updated. M2 Max 64GB.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;I tried Granite 4.0 H Small 8Bit MLX at first (had to trim some data, MLX only support about 131k context for some reason?).&lt;/p&gt; &lt;p&gt;The response was a barely coherent new guide covering one of the subject of the game.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;granite-4.0-h-small-mlx (23.24 tok/sec, 781 tokens, 607.44s to first token, Stop reason: User Stopped) Introduction In this guide, we'll discuss the various methods for generating income in the game RimWorld. By understanding these strategies and optimizing your colony's operations, you'll be able to build a thriving settlement capable of surviving any challenge thrown its way. Farming One of the primary methods for generating income in RimWorld is through farming. Planting crops and harvesting them can provide a steady stream of resources, which in turn can be sold or processed into various products. &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;p&gt;I never had any luck with any MLX, so i gave the GUFF a try.&lt;/p&gt; &lt;p&gt;Now with : Granite 4.0 H Small GGUF Q4_K_M(Q8 with that much context wont fit my 64gb) (let me push to 164K context keeping all my data)&lt;/p&gt; &lt;p&gt;After about 5 minuet of prompt processing, it crashed :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;lmstudio/granite-4.0-h-small@q4_k_m Processing Prompt... 85% This message contains no content. The AI has nothing to say. &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;p&gt;Lets try tiny, its not like it need knowledge, i give it all it need.&lt;/p&gt; &lt;p&gt;Again, ignore, instruction, write a small guide on part of the content.&lt;/p&gt; &lt;p&gt;ibm/granite-4-h-tiny Q8&lt;/p&gt; &lt;pre&gt;&lt;code&gt;23.88 tok/sec•822 tokens•221.59s to first token•Stop reason: EOS Token Found Ideology Min-Maxing Here are some tips and suggestions for optimizing your ideoligion choices for the best results: Fluid vs Static Ideology: If you don't need specific memes immediately, choose fluid to adapt to various scenarios in a single game. Choose static if you have a clear idea of what you want from the start. &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;p&gt;Since not all Guff are created equal, lets try the latest sensation everybody toot : unsloth, ill even follow their guide : &lt;a href="https://docs.unsloth.ai/new/ibm-granite-4.0"&gt;https://docs.unsloth.ai/new/ibm-granite-4.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ill even trim my context to 114k , unsloth guide say its max 131k, even though Ibm said no limit, i guess GUFF have limitation?&lt;/p&gt; &lt;p&gt;unsloth/granite-4.0-h-tiny-GGUF(26.68 tok/sec•794 tokens•152.78s to first token•Stop reason: EOS Token Found&lt;/p&gt; &lt;pre&gt;&lt;code&gt;granite-4.0-h-tiny Food crops: Corn: Product = Silver 163.5 per hour work; Profit/day plant growth = Silver 1.16. Haygrass: Product = Silver 73.0 per hour work; Profit/day plant growth = Silver 0.83. Rice: Product = Silver 44.6 per hour work; Profit/day plant growth = Silver 1.19. Potatoes: Product = Silver 81.77 per hour work; Profit/day plant growth = Silver 1.12. Strawberries: Product = Silver 101.35 per hour work; Profit/day plant growth = Silver 1.02. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Again, its just write a small guide on part of the data...&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Lets try the unsloth small version with recommended setting, we never know, i might have screw up setting.&lt;/p&gt; &lt;p&gt;unsloth : Granite 4.0 H Small GGUF Q4_K_M&lt;/p&gt; &lt;pre&gt;&lt;code&gt;granite-4.0-h-small Processing Prompt... 81% This message contains no content. The AI has nothing to say. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Crash while processing the prompt, while under the 131k limit.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mantafloppy"&gt; /u/mantafloppy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzzurf/granite_4_gguf_is_useless_if_you_try_to_use_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzzurf/granite_4_gguf_is_useless_if_you_try_to_use_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzzurf/granite_4_gguf_is_useless_if_you_try_to_use_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T00:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzimvg</id>
    <title>October 2025 model selections, what do you use?</title>
    <updated>2025-10-06T13:10:24+00:00</updated>
    <author>
      <name>/u/getpodapp</name>
      <uri>https://old.reddit.com/user/getpodapp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzimvg/october_2025_model_selections_what_do_you_use/"&gt; &lt;img alt="October 2025 model selections, what do you use?" src="https://preview.redd.it/syzg3f8oqhtf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e686e4b8db47ba995e1c43c3c24fb0dd3547175e" title="October 2025 model selections, what do you use?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getpodapp"&gt; /u/getpodapp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/syzg3f8oqhtf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzimvg/october_2025_model_selections_what_do_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzimvg/october_2025_model_selections_what_do_you_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T13:10:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzru92</id>
    <title>AMD stock skyrockets 30% as OpenAI looks to take stake in AI chipmaker</title>
    <updated>2025-10-06T18:55:22+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzru92/amd_stock_skyrockets_30_as_openai_looks_to_take/"&gt; &lt;img alt="AMD stock skyrockets 30% as OpenAI looks to take stake in AI chipmaker" src="https://external-preview.redd.it/cRxMiHUULYNg9ilSv0igBEg5GDo6YuyoZ5csopbw4k0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd1ec0577ebabb74b53154e89231b7a124d894a1" title="AMD stock skyrockets 30% as OpenAI looks to take stake in AI chipmaker" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/10/06/openai-amd-chip-deal-ai.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzru92/amd_stock_skyrockets_30_as_openai_looks_to_take/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzru92/amd_stock_skyrockets_30_as_openai_looks_to_take/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T18:55:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzskwx</id>
    <title>Kiln RAG Builder: Now with Local &amp; Open Models</title>
    <updated>2025-10-06T19:23:09+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzskwx/kiln_rag_builder_now_with_local_open_models/"&gt; &lt;img alt="Kiln RAG Builder: Now with Local &amp;amp; Open Models" src="https://external-preview.redd.it/NzFjNWRjcHdranRmMTouR_gGQN0P-1NcenpN-hyrI6W-iM6M3YFQQOwxr0u6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=353cf1fe2f3a8b489a6a9078f45dea8e0c4b7989" title="Kiln RAG Builder: Now with Local &amp;amp; Open Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone - two weeks ago we launched our new RAG-builder &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nnso4p/new_rag_builder_create_a_sota_rag_system_in_under/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;on here&lt;/a&gt; and &lt;a href="https://github.com/kiln-ai/kiln"&gt;Github&lt;/a&gt;. It allows you to build a RAG in under 5 minutes with a simple drag and drop interface. Unsurprisingly, LocalLLaMA requested local + open model support! Well we've added a bunch of open-weight/local models in our new release:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Extraction models&lt;/strong&gt; (vision models which convert documents into text for RAG indexing): Qwen 2.5VL 3B/7B/32B/72B, Qwen 3VL and GLM 4.5V Vision&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Embedding models&lt;/strong&gt;: Qwen 3 embedding 0.6B/4B/8B, Embed Gemma 300M, Nomic Embed 1.5, ModernBert, M2 Bert, E5, BAAI/bge, and more&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can run fully local with a config like Qwen 2.5VL + Qwen 3 Embedding. We added an &amp;quot;All Local&amp;quot; RAG template, so you can get started with local RAG with 1-click.&lt;/p&gt; &lt;p&gt;Note: we’re waiting on Llama.cpp support for Qwen 3 VL (so it’s open, but not yet local). We’ll add it as soon as it’s available, for now you can use it via the cloud.&lt;/p&gt; &lt;p&gt;Progress on other asks from the community in the last thread:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Semantic chunking&lt;/strong&gt;: We have this working. It's still in a branch while we test it, but if anyone wants early access let us know on &lt;a href="https://getkiln.ai/discord"&gt;Discord&lt;/a&gt;. It should be in our next release.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Graph RAG (specifically Graphiti)&lt;/strong&gt;: We’re looking into this, but it’s a bigger project. It will take a while as we figure out the best design.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Some links to the repo and guides:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/Kiln-AI/Kiln"&gt;Kiln AI on Github: &amp;gt;4k stars&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.kiln.tech/docs/documents-and-search-rag"&gt;Documents &amp;amp; Search (RAG) Docs/Guide&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://getkiln.ai/discord"&gt;Kiln Discord&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://kiln.tech"&gt;Homepage&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm happy to answer questions if anyone wants details or has ideas! Let me know if you want support for any specific local vision models or local embedding models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lioqj7pwkjtf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzskwx/kiln_rag_builder_now_with_local_open_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzskwx/kiln_rag_builder_now_with_local_open_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T19:23:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzozpg</id>
    <title>Granite4 Small-h 32b-A9b (Q4_K_M) at FULL 1M context window is using only 73GB of VRAM - Life is good!</title>
    <updated>2025-10-06T17:09:38+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This model seems to fit nicely on a single H100 or RTX Pro 6000. it’s great for high context RAG. This is the perfect model for my use case of models that call multiple tools in the same prompt while RAGing a bunch of knowledge bases. Might be our new daily driver for RAG use cases. If they add reasoning and vision then this is probably going to be everybody’s workhorse model. Great job big blue!! &lt;/p&gt; &lt;ul&gt; &lt;li&gt;KV cache set to Q8_0&lt;/li&gt; &lt;li&gt;Output tokens set to 131,072&lt;/li&gt; &lt;li&gt;Num_ctx set to 1000000 (I know it’s supposed to be 1048576 but Ollama errors out at that value for some reason) &lt;/li&gt; &lt;li&gt;Unsloth recommended settings for everything else. &lt;/li&gt; &lt;li&gt;Seems to support and perform “native” tool calling as well as GPT-OSS. &lt;/li&gt; &lt;li&gt;70.88 response tokens/s &lt;/li&gt; &lt;li&gt;Open WebUI as my front end client and Ollama 0.12.4 rc6 for inference &lt;/li&gt; &lt;li&gt;FRIGGIN’ 1 Million context window locally is crazy to me!! &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzozpg/granite4_smallh_32ba9b_q4_k_m_at_full_1m_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzozpg/granite4_smallh_32ba9b_q4_k_m_at_full_1m_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzozpg/granite4_smallh_32ba9b_q4_k_m_at_full_1m_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T17:09:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzoyu1</id>
    <title>AI for Scientific Discovery is a Social Problem - so we made Hugging Science!</title>
    <updated>2025-10-06T17:08:39+00:00</updated>
    <author>
      <name>/u/evijit</name>
      <uri>https://old.reddit.com/user/evijit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzoyu1/ai_for_scientific_discovery_is_a_social_problem/"&gt; &lt;img alt="AI for Scientific Discovery is a Social Problem - so we made Hugging Science!" src="https://b.thumbs.redditmedia.com/aSMXvgTel5VUpnlw5aJPuDvQRry_wrp7qusyuCWqUYE.jpg" title="AI for Scientific Discovery is a Social Problem - so we made Hugging Science!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I am Avijit Ghosh from Hugging Face. I wanted to share our new initiative for Scientific Discovery using Open source AI.&lt;/p&gt; &lt;p&gt;My colleague Georgia Channing and I just published a position paper that challenges a core assumption in AI for science: that the main barriers are technical.&lt;/p&gt; &lt;p&gt;They're not. We systematically analyzed why AI tools aren't democratizing scientific discovery and found that culture, incentives, and coordination failures are the real bottlenecks:&lt;/p&gt; &lt;p&gt;🚨 The &amp;quot;AI Scientist&amp;quot; myth is counterproductive: Waiting for AGI to solve science delays advances we could achieve now. Worse, it devalues human expertise essential for discovery and obscures science's real purpose: cultivating human understanding, not just producing outputs. (For eg: a transformer model achieves high accuracy predicting planetary motion but learns completely wrong physics.)&lt;/p&gt; &lt;p&gt;📊 We're rewarding the wrong contributions: High-quality datasets often have 100x longer impact than individual models, yet data curation work is systematically undervalued in hiring and tenure. Most models are superseded within months. Good datasets underpin research for decades.&lt;/p&gt; &lt;p&gt;⚠️ Collaboration is broken: Domain scientists prioritize mechanistic understanding. ML researchers optimize for predictive performance. Without shared language or success criteria, projects fail before they start. We lack educational resources for technically mature but domain-naive ML practitioners (and vice versa).&lt;/p&gt; &lt;p&gt;🔍 Accessibility and Fragmentation Remain Major Challenges: Harmonizing just 9 cancer imaging files took 329.5 hours over 6 months. Global South researchers face 6-day iteration cycles versus 30 minutes in G7 countries. 66% of scientists rate their computing access as inadequate. Current AI architectures struggle with complex scientific data that lacks clear tokenization strategies.&lt;/p&gt; &lt;p&gt;Why this matters now: While we chase narrow domain-specific applications, upstream computational bottlenecks like efficient PDE solvers and multi-scale coupling go unsolved. These problems could accelerate discovery across physics, chemistry, biology, and materials science simultaneously.&lt;/p&gt; &lt;p&gt;We need to build infrastructure, incentives, and community practices that make AI tools actually accessible. &lt;/p&gt; &lt;p&gt;That's why we're launching Hugging Science! A global community committed to addressing these barriers through concrete action: collaborative challenges targeting upstream problems, cross-disciplinary education and exchange, recognition for data and infrastructure contributions, and community-owned, accessible infrastructure.&lt;/p&gt; &lt;p&gt;This requires coordinated effort from researchers, funders, and institutions. But the foundation starts with community. Whether you curate datasets, build infrastructure, or bridge disciplines, there's a place for you! &lt;/p&gt; &lt;p&gt;Links: &lt;/p&gt; &lt;p&gt;Position Paper: &lt;a href="https://huggingface.co/papers/2509.06580"&gt;https://huggingface.co/papers/2509.06580&lt;/a&gt; Hugging Science Org: &lt;a href="https://huggingface.co/hugging-science"&gt;https://huggingface.co/hugging-science&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to know what you think and even better if you join the community and contribute! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/evijit"&gt; /u/evijit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nzoyu1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzoyu1/ai_for_scientific_discovery_is_a_social_problem/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzoyu1/ai_for_scientific_discovery_is_a_social_problem/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T17:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzl8y5</id>
    <title>How Transformers avoids becoming a black box, even at 1M+ LOC</title>
    <updated>2025-10-06T14:53:04+00:00</updated>
    <author>
      <name>/u/El_Olbap</name>
      <uri>https://old.reddit.com/user/El_Olbap</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm Pablo from Hugging Face Open-Source team. We just wrote a software-engineering focused deep dive on how we keep the `transformers` library hackable/maintainable while it keeps growing and growing. If you're running models locally, fine-tuning on your own hardware, or just want to understand the code you're using, I recommend the read!&lt;/p&gt; &lt;p&gt;Light spoilers about what's in it:&lt;/p&gt; &lt;p&gt;- ****One Model, One File:**** You can still read a `modeling_*.py` top-to-bottom and see exactly what's happening.&lt;/p&gt; &lt;p&gt;- ****Modular Transformers:**** This is our trick to fight code bloat. Contributors can reuse code via a small `modular_*.py` file, but we auto-generate the full, readable modeling file so you never lose the &amp;quot;one file&amp;quot; experience. It cut our maintenance work by ~15x.&lt;/p&gt; &lt;p&gt;- ****Config-Driven Performance:**** Features like FlashAttention(and ofc 2,3..), tensor parallelism (`tp_plan`), and per-layer attention schedules are enabled in the config, not by changing the model code. A `Linear` layer is always just a `Linear` layer, you don't have to change it depending on how it's sliced.&lt;/p&gt; &lt;p&gt;- ****Tools for Local Use:**** This philosophy lets us build helpful tools. The post covers an attention visualizer, a model tracer for debugging ports, and faster CUDA warmups, and we also go over `transformers serve` usage. &lt;/p&gt; &lt;p&gt;Hope you enjoy the read!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/El_Olbap"&gt; /u/El_Olbap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/transformers-community/Transformers-tenets"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzl8y5/how_transformers_avoids_becoming_a_black_box_even/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzl8y5/how_transformers_avoids_becoming_a_black_box_even/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T14:53:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz722n</id>
    <title>Biggest Provider for the community for at moment thanks to them</title>
    <updated>2025-10-06T02:17:03+00:00</updated>
    <author>
      <name>/u/dead-supernova</name>
      <uri>https://old.reddit.com/user/dead-supernova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz722n/biggest_provider_for_the_community_for_at_moment/"&gt; &lt;img alt="Biggest Provider for the community for at moment thanks to them" src="https://preview.redd.it/6kl3hy76ietf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86e1c6a42810d36cbc6b71792855914f69ca24a1" title="Biggest Provider for the community for at moment thanks to them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dead-supernova"&gt; /u/dead-supernova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6kl3hy76ietf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz722n/biggest_provider_for_the_community_for_at_moment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz722n/biggest_provider_for_the_community_for_at_moment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T02:17:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzn1mk</id>
    <title>Running GPT-OSS (OpenAI) Exclusively on AMD Ryzen™ AI NPU</title>
    <updated>2025-10-06T15:58:14+00:00</updated>
    <author>
      <name>/u/BandEnvironmental834</name>
      <uri>https://old.reddit.com/user/BandEnvironmental834</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzn1mk/running_gptoss_openai_exclusively_on_amd_ryzen_ai/"&gt; &lt;img alt="Running GPT-OSS (OpenAI) Exclusively on AMD Ryzen™ AI NPU" src="https://external-preview.redd.it/_S9eclPc4WRscWHOsVO80UpXnpu4dfbG_wCYpnVLuPA.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d53f3fcd941a63a8ecb5a50a6c26e1cf55db3e1a" title="Running GPT-OSS (OpenAI) Exclusively on AMD Ryzen™ AI NPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re a small team building &lt;strong&gt;FastFlowLM (FLM)&lt;/strong&gt; — a fast runtime for running &lt;strong&gt;GPT-OSS (first MoE on NPUs), Gemma3 (vision), Medgemma,&lt;/strong&gt; &lt;strong&gt;Qwen3,&lt;/strong&gt; &lt;strong&gt;DeepSeek-R1&lt;/strong&gt;, &lt;strong&gt;LLaMA3.x,&lt;/strong&gt; and others &lt;strong&gt;entirely on the AMD Ryzen AI NPU&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Think &lt;strong&gt;Ollama&lt;/strong&gt;, but deeply optimized for AMD NPUs — with both &lt;strong&gt;CLI&lt;/strong&gt; and &lt;strong&gt;Server Mode (OpenAI-compatible)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;✨ &lt;strong&gt;From Idle Silicon to Instant Power — FastFlowLM (FLM) Makes Ryzen™ AI Shine.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;No GPU fallback&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Faster and over 10× more power efficient.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Supports context lengths up to 256k tokens (qwen3:4b-2507).&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ultra-Lightweight (14 MB). Installs within 20 seconds.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try It Out&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/FastFlowLM/FastFlowLM"&gt;github.com/FastFlowLM/FastFlowLM&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live Demo → Remote machine access on the repo page&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;YouTube Demos:&lt;/strong&gt; &lt;a href="https://www.youtube.com/@FastFlowLM-YT/playlists"&gt;FastFlowLM - YouTube&lt;/a&gt; &lt;em&gt;→ Quick start guide, NPU vs CPU vs GPU, etc.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’re iterating fast and would &lt;strong&gt;love your feedback, critiques, and ideas&lt;/strong&gt;🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BandEnvironmental834"&gt; /u/BandEnvironmental834 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/ksYyiUQvYfo?si=zfBjb7U86P947OYW"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzn1mk/running_gptoss_openai_exclusively_on_amd_ryzen_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzn1mk/running_gptoss_openai_exclusively_on_amd_ryzen_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T15:58:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzwnbj</id>
    <title>The qwen3-next pr in llamacpp has been validated with a small test model</title>
    <updated>2025-10-06T21:52:11+00:00</updated>
    <author>
      <name>/u/Betadoggo_</name>
      <uri>https://old.reddit.com/user/Betadoggo_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzwnbj/the_qwen3next_pr_in_llamacpp_has_been_validated/"&gt; &lt;img alt="The qwen3-next pr in llamacpp has been validated with a small test model" src="https://b.thumbs.redditmedia.com/WI7I1cXVA_soqNsjPi7jb3WgjXU9vLhmlQDKP6zYAUY.jpg" title="The qwen3-next pr in llamacpp has been validated with a small test model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to comment: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095#issuecomment-3373977382"&gt;https://github.com/ggml-org/llama.cpp/pull/16095#issuecomment-3373977382&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been stalking this pr since it was opened and figured I'd share this update since I know a lot of others were interested in this model. Pwilkin has done some crazy work getting this together so quickly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Betadoggo_"&gt; /u/Betadoggo_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nzwnbj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzwnbj/the_qwen3next_pr_in_llamacpp_has_been_validated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzwnbj/the_qwen3next_pr_in_llamacpp_has_been_validated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T21:52:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect — Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect — Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I’m Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM – 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
