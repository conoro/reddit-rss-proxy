<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-10T16:26:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pigb3i</id>
    <title>DeepSeek-V3.2-REAP: 508B and 345B checkpoints</title>
    <updated>2025-12-09T19:14:58+00:00</updated>
    <author>
      <name>/u/ilzrvch</name>
      <uri>https://old.reddit.com/user/ilzrvch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, to get us all in the holiday mood we're continuing to REAP models, this time we got DeepSeek-V3.2 for you at 25% and 50% compression: &lt;/p&gt; &lt;p&gt;&lt;a href="https://hf.co/cerebras/DeepSeek-V3.2-REAP-508B-A37B"&gt;https://hf.co/cerebras/DeepSeek-V3.2-REAP-508B-A37B&lt;/a&gt;&lt;br /&gt; &lt;a href="https://hf.co/cerebras/DeepSeek-V3.2-REAP-345B-A37B"&gt;https://hf.co/cerebras/DeepSeek-V3.2-REAP-345B-A37B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're pretty excited about this one and are working to get some agentic evals for coding and beyond on these checkpoints soon. Enjoy and stay tuned!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilzrvch"&gt; /u/ilzrvch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pigb3i/deepseekv32reap_508b_and_345b_checkpoints/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pigb3i/deepseekv32reap_508b_and_345b_checkpoints/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pigb3i/deepseekv32reap_508b_and_345b_checkpoints/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T19:14:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj1a7c</id>
    <title>vLLM supports the new Devstral 2 coding models</title>
    <updated>2025-12-10T12:25:39+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj1a7c/vllm_supports_the_new_devstral_2_coding_models/"&gt; &lt;img alt="vLLM supports the new Devstral 2 coding models" src="https://preview.redd.it/br78ujzbdd6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea9b92040ddda20f4696a2bafe051470316248e4" title="vLLM supports the new Devstral 2 coding models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Devstral 2 is SOTA open model for code agents with a fraction of the parameters of its competitors and achieving 72.2% on SWE-bench Verified.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/br78ujzbdd6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj1a7c/vllm_supports_the_new_devstral_2_coding_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj1a7c/vllm_supports_the_new_devstral_2_coding_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T12:25:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1piykjp</id>
    <title>Built a visual debugger for my local agents because I was lost in JSON, would you use this?</title>
    <updated>2025-12-10T09:44:56+00:00</updated>
    <author>
      <name>/u/AdVivid5763</name>
      <uri>https://old.reddit.com/user/AdVivid5763</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piykjp/built_a_visual_debugger_for_my_local_agents/"&gt; &lt;img alt="Built a visual debugger for my local agents because I was lost in JSON, would you use this?" src="https://preview.redd.it/ymvtn22clc6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75a1585a27e4b916d4d1714f215f2080267441e6" title="Built a visual debugger for my local agents because I was lost in JSON, would you use this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run local LLM agents with tools / RAG. When a run broke, my workflow was basically: &lt;/p&gt; &lt;p&gt;rerun with more logging, diff JSON, and guess which step actually screwed things up. Slow and easy to miss.&lt;/p&gt; &lt;p&gt;So I hacked a small tool for myself: it takes a JSON trace and shows the run as a graph + timeline. &lt;/p&gt; &lt;p&gt;Each step is a node with the prompt / tool / result, and there‚Äôs a basic check that highlights obvious logic issues (like using empty tool results as if they were valid). &lt;/p&gt; &lt;p&gt;It‚Äôs already way faster for me than scrolling logs.&lt;/p&gt; &lt;p&gt;Long-term, I‚Äôd like this to become a proper ‚Äúcognition debugger‚Äù layer on top of whatever logs/traces you already have, especially for non-deterministic agents where ‚Äúwhat happened?‚Äù is not obvious.&lt;/p&gt; &lt;p&gt;It‚Äôs model-agnostic as long as the agent can dump a trace.&lt;/p&gt; &lt;p&gt;I‚Äôm mostly curious if anyone else here hits the same pain. &lt;/p&gt; &lt;p&gt;If this sounds useful, tell me what a debugger like this must show for you to actually use it. &lt;/p&gt; &lt;p&gt;I‚Äôll drop a demo link in the comments üîó.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdVivid5763"&gt; /u/AdVivid5763 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ymvtn22clc6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piykjp/built_a_visual_debugger_for_my_local_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piykjp/built_a_visual_debugger_for_my_local_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T09:44:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj30m1</id>
    <title>Muon vs MuonClip vs Muon+Adamw</title>
    <updated>2025-12-10T13:48:44+00:00</updated>
    <author>
      <name>/u/RealKingNish</name>
      <uri>https://old.reddit.com/user/RealKingNish</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One year in, Muon has gone from an experiment to a mainstream optimizer, but does it hold up for fine‚Äëtuning? We ran head‚Äëto‚Äëhead tests on Qwen3‚Äë4B (10k+ high‚Äëquality instruction rows) to find out.&lt;/p&gt; &lt;p&gt;Short story: Pure Muon converged fastest at the start, but its gradient‚Äënorm spikes made training unstable. MuonClip (Kimi K2‚Äôs clipping) stabilizes long pretraining runs, yet in our small‚Äëscale fine‚Äëtune it underperformed, lower token accuracy and slower convergence. The winner was the hybrid: Muon for 2D layers + AdamW for 1D layers. It delivered the best balance of stability and final performance and even beat vanilla AdamW.&lt;/p&gt; &lt;p&gt;Takeaway: for small-scale fine-tuning, hybrid = practical and reliable.&lt;/p&gt; &lt;p&gt;Next Step: scale to larger models/datasets to see if Muon‚Äôs spikes become catastrophic or if clipping wins out.&lt;/p&gt; &lt;p&gt;Full Blog Link: &lt;a href="https://huggingface.co/blog/KingNish/optimizer-part1"&gt;https://huggingface.co/blog/KingNish/optimizer-part1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RealKingNish"&gt; /u/RealKingNish &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj30m1/muon_vs_muonclip_vs_muonadamw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj30m1/muon_vs_muonclip_vs_muonadamw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj30m1/muon_vs_muonclip_vs_muonadamw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T13:48:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pihu16</id>
    <title>bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF</title>
    <updated>2025-12-09T20:10:40+00:00</updated>
    <author>
      <name>/u/mantafloppy</name>
      <uri>https://old.reddit.com/user/mantafloppy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pihu16/bartowskimistralai/"&gt; &lt;img alt="bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF" src="https://external-preview.redd.it/Y9-VSUeByMali_oSJcuRXft1g3dj7X6u-O2vcI7YtII.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e9d7830dcda85560752ed0db90867edc36dddee1" title="bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mantafloppy"&gt; /u/mantafloppy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pihu16/bartowskimistralai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pihu16/bartowskimistralai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T20:10:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pir8jc</id>
    <title>3D visualisation of GPT-2's layer-by-layer transformations (prototype ‚ÄúLLM oscilloscope‚Äù)</title>
    <updated>2025-12-10T02:45:16+00:00</updated>
    <author>
      <name>/u/Electronic-Fly-6465</name>
      <uri>https://old.reddit.com/user/Electronic-Fly-6465</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pir8jc/3d_visualisation_of_gpt2s_layerbylayer/"&gt; &lt;img alt="3D visualisation of GPT-2's layer-by-layer transformations (prototype ‚ÄúLLM oscilloscope‚Äù)" src="https://preview.redd.it/nzlqosj6ia6g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c781c9e64faa5592f78eccce290897f05ec44256" title="3D visualisation of GPT-2's layer-by-layer transformations (prototype ‚ÄúLLM oscilloscope‚Äù)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been building a visualisation tool that displays the internal layer dynamics of GPT-2 Small during a single forward pass.&lt;/p&gt; &lt;p&gt;It renders:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;per-head vector deltas&lt;/li&gt; &lt;li&gt;PCA-3 residual stream projections&lt;/li&gt; &lt;li&gt;angle + magnitude differences between heads&lt;/li&gt; &lt;li&gt;stabilisation behaviour in early layers&lt;/li&gt; &lt;li&gt;the sharp directional transition around layers 9‚Äì10&lt;/li&gt; &lt;li&gt;the consistent ‚Äúanchoring / braking‚Äù effect in layer 11&lt;/li&gt; &lt;li&gt;two-prompt comparison mode (‚ÄúI like X‚Äù vs ‚ÄúI like Y‚Äù)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything in the video is generated from real measurements ‚Äî no mock data or animation shortcuts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo video (22 min raw walkthrough):&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://youtu.be/dnWikqNAQbE"&gt;https://youtu.be/dnWikqNAQbE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just sharing the prototype.&lt;br /&gt; If anyone working on interpretability or visualisation wants to discuss it, I‚Äôm around.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic-Fly-6465"&gt; /u/Electronic-Fly-6465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nzlqosj6ia6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pir8jc/3d_visualisation_of_gpt2s_layerbylayer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pir8jc/3d_visualisation_of_gpt2s_layerbylayer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T02:45:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj4htk</id>
    <title>Social media history? Next it‚Äôll be your AI chat logs.</title>
    <updated>2025-12-10T14:50:33+00:00</updated>
    <author>
      <name>/u/kinkvoid</name>
      <uri>https://old.reddit.com/user/kinkvoid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just saw the news: the U.S. may soon require visa-exempt travelers to hand over five years of their social media history before entry.&lt;/p&gt; &lt;p&gt;If border agents are already auditing tweets and Instagram posts‚Ä¶ what‚Äôs stopping them from asking for your ChatGPT or Claude conversation history next? After all, those chats can reveal a lot‚Äîopinions, plans, even sensitive personal info.&lt;/p&gt; &lt;p&gt;Feels like another nudge toward running your own models offline. Maybe ‚Äúlocal LLM‚Äù is becoming a privacy necessity.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kinkvoid"&gt; /u/kinkvoid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj4htk/social_media_history_next_itll_be_your_ai_chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj4htk/social_media_history_next_itll_be_your_ai_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj4htk/social_media_history_next_itll_be_your_ai_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T14:50:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi9q3t</id>
    <title>Introducing: Devstral 2 and Mistral Vibe CLI. | Mistral AI</title>
    <updated>2025-12-09T15:05:54+00:00</updated>
    <author>
      <name>/u/YanderMan</name>
      <uri>https://old.reddit.com/user/YanderMan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9q3t/introducing_devstral_2_and_mistral_vibe_cli/"&gt; &lt;img alt="Introducing: Devstral 2 and Mistral Vibe CLI. | Mistral AI" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Introducing: Devstral 2 and Mistral Vibe CLI. | Mistral AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YanderMan"&gt; /u/YanderMan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/devstral-2-vibe-cli"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9q3t/introducing_devstral_2_and_mistral_vibe_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9q3t/introducing_devstral_2_and_mistral_vibe_cli/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T15:05:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1piumvw</id>
    <title>bartowski/ServiceNow-AI_Apriel-1.6-15b-Thinker-GGUF ¬∑ Hugging Face</title>
    <updated>2025-12-10T05:37:56+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piumvw/bartowskiservicenowai_apriel1615bthinkergguf/"&gt; &lt;img alt="bartowski/ServiceNow-AI_Apriel-1.6-15b-Thinker-GGUF ¬∑ Hugging Face" src="https://external-preview.redd.it/0jH917Owmr7iKrMXvyA0r05fWobE4kYASAkKFjbuamg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d8285c76b1c0f5aa8e695fdb89fac6a270b922a8" title="bartowski/ServiceNow-AI_Apriel-1.6-15b-Thinker-GGUF ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;it was gated before, finally it's available&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.6-15b-Thinker-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piumvw/bartowskiservicenowai_apriel1615bthinkergguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piumvw/bartowskiservicenowai_apriel1615bthinkergguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T05:37:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj3tqt</id>
    <title>Meta‚Äôs next AI model "Avocado" may launch next spring as a closed model, according to people familiar with the matter</title>
    <updated>2025-12-10T14:23:21+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://www.bloomberg.com/news/articles/2025-12-10/inside-meta-s-pivot-from-open-source-to-money-making-ai-model"&gt;https://www.bloomberg.com/news/articles/2025-12-10/inside-meta-s-pivot-from-open-source-to-money-making-ai-model?&lt;/a&gt; &lt;/p&gt; &lt;p&gt;What are you doing, Meta?&lt;br /&gt; :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj3tqt/metas_next_ai_model_avocado_may_launch_next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj3tqt/metas_next_ai_model_avocado_may_launch_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj3tqt/metas_next_ai_model_avocado_may_launch_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T14:23:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1piux9z</id>
    <title>Z.ai release GLM-ASR-Nano: an open-source ASR model with 1.5B parameters</title>
    <updated>2025-12-10T05:54:13+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piux9z/zai_release_glmasrnano_an_opensource_asr_model/"&gt; &lt;img alt="Z.ai release GLM-ASR-Nano: an open-source ASR model with 1.5B parameters" src="https://b.thumbs.redditmedia.com/02J_w_E-jjdBogT1atigLYYoW24wILytNHbOE75U0FI.jpg" title="Z.ai release GLM-ASR-Nano: an open-source ASR model with 1.5B parameters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4hs2rkx0gb6g1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1005ca9567e6c31bb0b23f8a3e9473959507757"&gt;Benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Designed for real-world complexity, it outperforms OpenAI Whisper V3 on multiple benchmarks while maintaining a compact size.&lt;/p&gt; &lt;p&gt;Key capabilities include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Exceptional Dialect Support:&lt;/strong&gt; Beyond standard Mandarin and English, the model is highly optimized for &lt;strong&gt;Cantonese&lt;/strong&gt; and other dialects, effectively bridging the gap in dialectal speech recognition.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Low-Volume Speech Robustness:&lt;/strong&gt; Specifically trained for &lt;strong&gt;&amp;quot;Whisper/Quiet Speech&amp;quot;&lt;/strong&gt; scenarios. It captures and accurately transcribes extremely low-volume audio that traditional models often miss.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SOTA Performance:&lt;/strong&gt; Achieves the &lt;strong&gt;lowest average error rate (4.10)&lt;/strong&gt; among comparable open-source models, showing significant advantages in Chinese benchmarks (Wenet Meeting, Aishell-1, etc..)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Huggingface: &lt;a href="https://huggingface.co/zai-org/GLM-ASR-Nano-2512"&gt;https://huggingface.co/zai-org/GLM-ASR-Nano-2512&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piux9z/zai_release_glmasrnano_an_opensource_asr_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piux9z/zai_release_glmasrnano_an_opensource_asr_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piux9z/zai_release_glmasrnano_an_opensource_asr_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T05:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj12o6</id>
    <title>We basically have GLM 4.6 Air, without vision</title>
    <updated>2025-12-10T12:14:56+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj12o6/we_basically_have_glm_46_air_without_vision/"&gt; &lt;img alt="We basically have GLM 4.6 Air, without vision" src="https://b.thumbs.redditmedia.com/N6-RjB_vDOlUTxDCH2IFVKNBxBmsxRH0AWNuhj77qNs.jpg" title="We basically have GLM 4.6 Air, without vision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/exy3lba7cd6g1.png?width=2075&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4cf64faf530b641c9a81d37e1af555bbc372a568"&gt;glm 4.6 air&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tested and working in LM Studio. Thanks for the GGUF!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj12o6/we_basically_have_glm_46_air_without_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj12o6/we_basically_have_glm_46_air_without_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj12o6/we_basically_have_glm_46_air_without_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T12:14:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pir555</id>
    <title>So what's the closest open-source thing to claude code?</title>
    <updated>2025-12-10T02:40:56+00:00</updated>
    <author>
      <name>/u/According-Ebb917</name>
      <uri>https://old.reddit.com/user/According-Ebb917</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just wondering which coding agent/multi-agent system out there is the closest to claude code? Particularly in terms of good scaffolding (subagents, skills, proper context engineering, etc...) and works well with a set of models? I feel like there's a new one everyday but I can't seem to figure out which work and which don't&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/According-Ebb917"&gt; /u/According-Ebb917 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pir555/so_whats_the_closest_opensource_thing_to_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pir555/so_whats_the_closest_opensource_thing_to_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pir555/so_whats_the_closest_opensource_thing_to_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T02:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj2g59</id>
    <title>Open sourced a LLM powered draw.io live editor</title>
    <updated>2025-12-10T13:23:14+00:00</updated>
    <author>
      <name>/u/JerryKwan</name>
      <uri>https://old.reddit.com/user/JerryKwan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj2g59/open_sourced_a_llm_powered_drawio_live_editor/"&gt; &lt;img alt="Open sourced a LLM powered draw.io live editor" src="https://preview.redd.it/zn848zmsnd6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=569a52e33b9f76b6af27e325c0cbd2808c8088bd" title="Open sourced a LLM powered draw.io live editor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have open sourced a LLM powerd drawio live editor, it supports fully local deployment, and bidirectional Interoperability.&lt;br /&gt; Feel free to check the codes from &lt;a href="https://github.com/JerryKwan/drawio-live-editor"&gt;https://github.com/JerryKwan/drawio-live-editor&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JerryKwan"&gt; /u/JerryKwan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zn848zmsnd6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj2g59/open_sourced_a_llm_powered_drawio_live_editor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj2g59/open_sourced_a_llm_powered_drawio_live_editor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T13:23:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj12ix</id>
    <title>Hands-on review of Mistral Vibe on large python project</title>
    <updated>2025-12-10T12:14:43+00:00</updated>
    <author>
      <name>/u/Avienir</name>
      <uri>https://old.reddit.com/user/Avienir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just spent some time testing Mistral Vibe on real use cases and I must say I‚Äôm impressed. For context: I'm a dev working on a fairly big Python codebase (~40k LOC) with some niche frameworks (Reflex, etc.), so I was curious how it handles real-world existing projects rather than just spinning up new toys from scratch.&lt;/p&gt; &lt;p&gt;UI/Features: Looks really clean and minimal ‚Äì nice themes, feels polished for a v1.0.5. Missing some QoL stuff that's standard in competitors: no conversation history/resume, no checkpoints, no planning mode, no easy AGENTS.md support for project-specific config. Probably coming soon since it's super fresh.&lt;/p&gt; &lt;p&gt;The good (coding performance): Tested on two tasks in my existing repo:&lt;/p&gt; &lt;p&gt;Simple one: Shrink text size in a component. It nailed it ‚Äì found the right spot, checked other components to gauge scale, deduced the right value. Felt smart. 10/10.&lt;/p&gt; &lt;p&gt;Harder: Fix a validation bug in time-series models with multiple series. Solved it exactly as asked, wrote its own temp test to verify, cleaned up after. Struggled a bit with running the app (my project uses uv, not plain python run), and needed a few iterations on integration tests, but ended up with solid, passing tests and even suggested extra e2e ones. 8/10.&lt;/p&gt; &lt;p&gt;Overall: Fast, good context search, adapts to project style well, does exactly what you ask without hallucinating extras.&lt;/p&gt; &lt;p&gt;The controversial bit: 100k token context limit Yeah, it's capped there (compresses beyond?). Won't build huge apps from zero or refactor massive repos in one go. But... is that actually a dealbreaker? My harder task fit in ~75k. For day-to-day feature adds/bug fixes in real codebases, it feels reasonable ‚Äì forces better planning and breaking things down. Kinda natural discipline? Summary pros/cons:&lt;/p&gt; &lt;p&gt;Pros:&lt;/p&gt; &lt;p&gt;Speed Smart context handling Sticks to instructions Great looking terminal UI&lt;/p&gt; &lt;p&gt;Cons:&lt;/p&gt; &lt;p&gt;100k context cap Missing features (history, resume, etc.)&lt;/p&gt; &lt;p&gt;Definitely worth trying if you're into CLI agents or want a cheaper/open alternative. Curious what others think ‚Äì anyone else messed with it yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Avienir"&gt; /u/Avienir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj12ix/handson_review_of_mistral_vibe_on_large_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj12ix/handson_review_of_mistral_vibe_on_large_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj12ix/handson_review_of_mistral_vibe_on_large_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T12:14:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pizl8t</id>
    <title>Built a GGUF memory &amp; tok/sec calculator for inference requirements ‚Äì Drop in any HF GGUF URL</title>
    <updated>2025-12-10T10:49:12+00:00</updated>
    <author>
      <name>/u/ittaboba</name>
      <uri>https://old.reddit.com/user/ittaboba</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pizl8t/built_a_gguf_memory_toksec_calculator_for/"&gt; &lt;img alt="Built a GGUF memory &amp;amp; tok/sec calculator for inference requirements ‚Äì Drop in any HF GGUF URL" src="https://external-preview.redd.it/cnpqZXU4dXV0YzZnMYh73P_j0pnSesQyyRb8l_QLx5gX0RNmxMe-sw-YRlmA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edc6af8aed5100a84c198cee502219f22065dba7" title="Built a GGUF memory &amp;amp; tok/sec calculator for inference requirements ‚Äì Drop in any HF GGUF URL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;Built a small utility that estimates how much memory you need to run GGUF models locally, plus an approximate tok/sec based on your machine (Apple Silicon only atm, more hardware soon) and task (e.g. ask a generic question, write a draft, etc.).&lt;/p&gt; &lt;p&gt;You can select a model from a dropdown or paste any direct GGUF URL from HF. The tool parses the model metadata (size, layers, hidden dimensions, KV cache, etc.) and uses that to estimate:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Total memory needed for weights + KV cache + activations + overhead&lt;/li&gt; &lt;li&gt;Expected latency and generation speed (tok/sec)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Demo: &lt;a href="https://manzoni.app/llm_calculator"&gt;https://manzoni.app/llm_calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code + formulas: &lt;a href="https://github.com/gems-platforms/gguf-memory-calculator"&gt;https://github.com/gems-platforms/gguf-memory-calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback, edge cases, or bug reports (e.g. comparisons against your actual tokens/sec to tighten the estimates). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ittaboba"&gt; /u/ittaboba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qahbzltutc6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pizl8t/built_a_gguf_memory_toksec_calculator_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pizl8t/built_a_gguf_memory_toksec_calculator_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T10:49:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj3q4q</id>
    <title>Nanbeige4-3B: Lightweight with strong reasoning capabilities</title>
    <updated>2025-12-10T14:19:13+00:00</updated>
    <author>
      <name>/u/leran2098</name>
      <uri>https://old.reddit.com/user/leran2098</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj3q4q/nanbeige43b_lightweight_with_strong_reasoning/"&gt; &lt;img alt="Nanbeige4-3B: Lightweight with strong reasoning capabilities" src="https://b.thumbs.redditmedia.com/BSpMQtQuj8BqtPhjKt_tiHu_duxHikdioAVLT2Qq9JA.jpg" title="Nanbeige4-3B: Lightweight with strong reasoning capabilities" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;We‚Äôre excited to share &lt;strong&gt;Nanbeige4-3B&lt;/strong&gt;, a new family of open-weight 3B models from Nanbeige LLM Lab, including both a &lt;strong&gt;Base&lt;/strong&gt; and a &lt;strong&gt;Thinking&lt;/strong&gt; variant. Designed for strong reasoning capabilities while remaining lightweight, it‚Äôs well-suited for local deployment on consumer hardware.&lt;/p&gt; &lt;p&gt;A few key highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Pre-training&lt;/strong&gt;: 23T high-quality tokens, filtered via hybrid quality signals and scheduled with a fine-grained WSD strategy.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Post-training&lt;/strong&gt;: 30M+ high-quality SFT samples, deliberative CoT refinement, dual-level distillation from a larger Nanbeige model, and multi-stage Reinforcement Learning.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performances&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Human Preference Alignment&lt;/strong&gt;: Scores &lt;strong&gt;60.0 on ArenaHard-V2&lt;/strong&gt;, matching &lt;strong&gt;Qwen3-30B-A3B-Thinking-2507.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool Use&lt;/strong&gt;: Achieves &lt;strong&gt;SOTA on BFCL-V4&lt;/strong&gt; among open-source models under 32B parameters.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Math &amp;amp; Science&lt;/strong&gt;: &lt;strong&gt;85.6 on AIME 2025&lt;/strong&gt;, &lt;strong&gt;82.2 on GPQA-Diamond&lt;/strong&gt;‚Äîoutperforming many much larger models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing&lt;/strong&gt;: Ranked &lt;strong&gt;#11 on WritingBench,&lt;/strong&gt; comparable to large models like &lt;strong&gt;Deepseek-R1-0528&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both versions are fully open and available on Hugging Face:&lt;/p&gt; &lt;p&gt;üîπ&lt;a href="https://huggingface.co/Nanbeige/Nanbeige4-3B-Base"&gt;Base Model&lt;/a&gt;&lt;br /&gt; üîπ&lt;a href="https://huggingface.co/Nanbeige/Nanbeige4-3B-Thinking-2511"&gt;Thinking Model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üìÑ Technical Report: &lt;a href="https://arxiv.org/pdf/2512.06266"&gt;https://arxiv.org/pdf/2512.06266&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n99zvfsuwd6g1.png?width=1755&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c78d841b1153c055942bcaed3cb92824b32db30"&gt;https://preview.redd.it/n99zvfsuwd6g1.png?width=1755&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c78d841b1153c055942bcaed3cb92824b32db30&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k2qngr7xwd6g1.png?width=1845&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c66d85c3a26a193dc5d6c24173db74b0afd5254"&gt;https://preview.redd.it/k2qngr7xwd6g1.png?width=1845&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c66d85c3a26a193dc5d6c24173db74b0afd5254&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leran2098"&gt; /u/leran2098 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj3q4q/nanbeige43b_lightweight_with_strong_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj3q4q/nanbeige43b_lightweight_with_strong_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj3q4q/nanbeige43b_lightweight_with_strong_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T14:19:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj343j</id>
    <title>Nous Research just open source Nomos 1, a specialization of Qwen/Qwen3-30B-A3B-Thinking-2507 for mathematical problem-solving and proof-writing in natural language. At just 30B parameters, it scores 87/120 on this year‚Äôs Putnam</title>
    <updated>2025-12-10T13:53:01+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj343j/nous_research_just_open_source_nomos_1_a/"&gt; &lt;img alt="Nous Research just open source Nomos 1, a specialization of Qwen/Qwen3-30B-A3B-Thinking-2507 for mathematical problem-solving and proof-writing in natural language. At just 30B parameters, it scores 87/120 on this year‚Äôs Putnam" src="https://preview.redd.it/yq7oiy8rsd6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=38df6ff9931485e6ad53d3b467808242a6f3741b" title="Nous Research just open source Nomos 1, a specialization of Qwen/Qwen3-30B-A3B-Thinking-2507 for mathematical problem-solving and proof-writing in natural language. At just 30B parameters, it scores 87/120 on this year‚Äôs Putnam" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Weights: &lt;a href="https://huggingface.co/NousResearch/nomos-1"&gt;https://huggingface.co/NousResearch/nomos-1&lt;/a&gt;&lt;br /&gt; Reasoning harness: &lt;a href="https://github.com/NousResearch/nomos+"&gt;https://github.com/NousResearch/nomos+&lt;/a&gt;&lt;br /&gt; From Nous Research on ùïè: &lt;a href="https://x.com/NousResearch/status/1998536543565127968"&gt;https://x.com/NousResearch/status/1998536543565127968&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yq7oiy8rsd6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj343j/nous_research_just_open_source_nomos_1_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj343j/nous_research_just_open_source_nomos_1_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T13:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1piwx9u</id>
    <title>Trinity Mini: a 26B OpenWeight MoE model with a 3B active and strong reasoning scores</title>
    <updated>2025-12-10T07:54:13+00:00</updated>
    <author>
      <name>/u/Sumanth_077</name>
      <uri>https://old.reddit.com/user/Sumanth_077</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piwx9u/trinity_mini_a_26b_openweight_moe_model_with_a_3b/"&gt; &lt;img alt="Trinity Mini: a 26B OpenWeight MoE model with a 3B active and strong reasoning scores" src="https://external-preview.redd.it/G7Gcft3BKg57j9czqWCQwa5R5JjWhPW-BbTK-PcJb1k.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f0aad896057c96dc1a1c9470d0d19ea461ad37b1" title="Trinity Mini: a 26B OpenWeight MoE model with a 3B active and strong reasoning scores" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Arcee AI quietly dropped a pretty interesting model last week: Trinity Mini, a 26B-parameter sparse MoE with only 3B active parameters&lt;/p&gt; &lt;p&gt;A few things that actually stand out beyond the headline numbers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;128 experts, 8 active + 1 shared expert&lt;/strong&gt;. Routing is noticeably more stable than typical 2/4-expert MoEs, especially on math and tool-calling tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;10T curated tokens&lt;/strong&gt;, built on top of the Datology dataset stack. The math/code additions seem to actually matter, the model holds state across multi-step reasoning better than most mid-size MoEs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;128k context&lt;/strong&gt; without the ‚Äúfalls apart after 20k tokens‚Äù behavior a lot of open models still suffer from.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Strong zero-shot scores&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;84.95% MMLU (ZS)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;92.10% Math-500&lt;/strong&gt; These would be impressive even for a 70B dense model. For a 3B-active MoE, it‚Äôs kind of wild.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want to experiment with it, it‚Äôs available via &lt;a href="https://clarifai.com/arcee_ai/AFM/models/trinity-mini"&gt;Clarifai&lt;/a&gt; and also &lt;a href="https://openrouter.ai/arcee-ai/trinity-mini"&gt;OpenRouter&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Curious what you all think after trying it?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1m97sj3f0c6g1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ddc01b2fd25dddd2c9f1e45965cbff3e58cccdf"&gt;https://preview.redd.it/1m97sj3f0c6g1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ddc01b2fd25dddd2c9f1e45965cbff3e58cccdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sumanth_077"&gt; /u/Sumanth_077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piwx9u/trinity_mini_a_26b_openweight_moe_model_with_a_3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piwx9u/trinity_mini_a_26b_openweight_moe_model_with_a_3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piwx9u/trinity_mini_a_26b_openweight_moe_model_with_a_3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T07:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj5rg5</id>
    <title>zai-org/GLM-TTS ¬∑ Hugging Face</title>
    <updated>2025-12-10T15:40:47+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj5rg5/zaiorgglmtts_hugging_face/"&gt; &lt;img alt="zai-org/GLM-TTS ¬∑ Hugging Face" src="https://external-preview.redd.it/Enw5i_BcwLjX0NMsj3omfkq8Tm7EGhJ6noC8i7hUs1o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2c8438d5778817bb402b2e43d621f39273722f29" title="zai-org/GLM-TTS ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Key Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Zero-shot Voice Cloning: Clone any speaker's voice with just 3-10 seconds of prompt audio.&lt;/li&gt; &lt;li&gt;RL-enhanced Emotion Control: Utilizes a multi-reward reinforcement learning framework (GRPO) to optimize prosody and emotion.&lt;/li&gt; &lt;li&gt;High-quality Synthesis: Generates speech comparable to commercial systems with reduced Character Error Rate (CER).&lt;/li&gt; &lt;li&gt;Phoneme-level Control: Supports &amp;quot;Hybrid Phoneme + Text&amp;quot; input for precise pronunciation control (e.g., polyphones).&lt;/li&gt; &lt;li&gt;Streaming Inference: Supports real-time audio generation suitable for interactive applications.&lt;/li&gt; &lt;li&gt;Bilingual Support: Optimized for Chinese and English mixed text.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-TTS"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj5rg5/zaiorgglmtts_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj5rg5/zaiorgglmtts_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T15:40:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj4t0p</id>
    <title>llama.cpp releases new CLI interface</title>
    <updated>2025-12-10T15:02:41+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj4t0p/llamacpp_releases_new_cli_interface/"&gt; &lt;img alt="llama.cpp releases new CLI interface" src="https://preview.redd.it/ng1dt8ym5e6g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64b5abfbb593206e63d6e14650f576f807c4ea59" title="llama.cpp releases new CLI interface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;https://github.com/ggml-org/llama.cpp/releases&lt;/a&gt; + with nice features: &lt;/p&gt; &lt;p&gt;&amp;gt; Clean looking interface&lt;br /&gt; &amp;gt; Multimodal support&lt;br /&gt; &amp;gt; Conversation control via commands&lt;br /&gt; &amp;gt; Speculative decoding support&lt;br /&gt; &amp;gt; Jinja fully supported&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ng1dt8ym5e6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj4t0p/llamacpp_releases_new_cli_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj4t0p/llamacpp_releases_new_cli_interface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T15:02:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj5jja</id>
    <title>Heretic 1.1 released: Improved abliteration quality, multi-GPU support, thinking models support, Apple Silicon support, notebook support, research features, and more</title>
    <updated>2025-12-10T15:32:14+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj5jja/heretic_11_released_improved_abliteration_quality/"&gt; &lt;img alt="Heretic 1.1 released: Improved abliteration quality, multi-GPU support, thinking models support, Apple Silicon support, notebook support, research features, and more" src="https://preview.redd.it/w21t5s3r5e6g1.gif?width=640&amp;amp;crop=smart&amp;amp;s=c7dab536c985a67a26894e40e46291e1733b63ee" title="Heretic 1.1 released: Improved abliteration quality, multi-GPU support, thinking models support, Apple Silicon support, notebook support, research features, and more" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been a busy few weeks for the automatic censorship removal tool &lt;strong&gt;Heretic&lt;/strong&gt; (&lt;a href="https://github.com/p-e-w/heretic"&gt;https://github.com/p-e-w/heretic&lt;/a&gt;), and now, it is time for the second official release! Highlights include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;accemlcc discovered a significant bug related to padding in batched inference. The fix revealed another issue affecting thinking models. I implemented automatic detection of CoT blocks, which are now positionally skipped, drastically improving the accuracy of computed refusal directions. The result of those two fixes is improved abliteration quality for all models, and &lt;em&gt;greatly&lt;/em&gt; improved abliteration quality for thinking models.&lt;/li&gt; &lt;li&gt;Vinayyyy7 added shims for Heretic's input functions, allowing the program to work when run from notebook environments that don't provide full terminal emulation, like Colab and Kaggle.&lt;/li&gt; &lt;li&gt;kldzj added multi-GPU support, and demonstrated that it works by abliterating gpt-oss-120b.&lt;/li&gt; &lt;li&gt;mbarnson added basic MPS (Apple Silicon) support.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please see the release notes on GitHub for the complete list of changes. As you can tell, Heretic is already very much a community project, with 10 people contributing code to this release. Contributions are very welcome and appreciated!&lt;/p&gt; &lt;p&gt;Development continues at a rapid pace. Here's some of what we have cooking right now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;accemlcc is implementing quantized model loading and LoRA adapters, improving performance and reducing VRAM requirements by up to 75% (!!!).&lt;/li&gt; &lt;li&gt;pszemraj is adding support for state-space/hybrid model architectures like Mamba, which are very difficult to target with existing abliteration tools.&lt;/li&gt; &lt;li&gt;red40maxxer is working on a plugin system, which in the future will allow users to choose between different engines for detecting refusals, evaluating model quality, and performing abliteration.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ah yes, did I mention that Heretic now has research features? In particular, you can reproduce the cool animation from this post with just two commands:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install -U heretic-llm[research] heretic --plot-residuals openai/gpt-oss-20b &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will generate an animated GIF showing how residual vectors for &amp;quot;harmful&amp;quot; and &amp;quot;harmless&amp;quot; prompts are transformed as they proceed through the model's layer stack, which can often yield deep insights about a model's internal behavior. Prompts, labels, and colors are all configurable, so you can also use this feature to investigate phenomena like how a model differentiates between English and Chinese inputs, without having to write a single line of code.&lt;/p&gt; &lt;p&gt;Cheers :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w21t5s3r5e6g1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj5jja/heretic_11_released_improved_abliteration_quality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj5jja/heretic_11_released_improved_abliteration_quality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T15:32:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj4j87</id>
    <title>new CLI experience has been merged into llama.cpp</title>
    <updated>2025-12-10T14:52:07+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj4j87/new_cli_experience_has_been_merged_into_llamacpp/"&gt; &lt;img alt="new CLI experience has been merged into llama.cpp" src="https://preview.redd.it/99wk9uq04e6g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebcb668533af336657b10f17156e3dde01baf80b" title="new CLI experience has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17824"&gt;https://github.com/ggml-org/llama.cpp/pull/17824&lt;/a&gt;&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/99wk9uq04e6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj4j87/new_cli_experience_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj4j87/new_cli_experience_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T14:52:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj51tu</id>
    <title>You can now train LLMs 3x faster with 30% less memory! (&lt;3.9GB VRAM)</title>
    <updated>2025-12-10T15:12:39+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj51tu/you_can_now_train_llms_3x_faster_with_30_less/"&gt; &lt;img alt="You can now train LLMs 3x faster with 30% less memory! (&amp;lt;3.9GB VRAM)" src="https://preview.redd.it/831ky7k47e6g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02ff40ce13155be048e6de2935672da6c685da75" title="You can now train LLMs 3x faster with 30% less memory! (&amp;lt;3.9GB VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey [&lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;]()! We're excited to release new Triton kernels and smart auto packing support to enable you to train models 3x (sometimes even &lt;strong&gt;5x&lt;/strong&gt;) faster with &lt;strong&gt;30-90% less VRAM&lt;/strong&gt; - all with &lt;strong&gt;no accuracy degradation&lt;/strong&gt;. Unsloth GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;This means you can now train LLMs like Qwen3-4B not only on just &lt;strong&gt;3.9GB VRAM&lt;/strong&gt;, but also 3x faster&lt;/li&gt; &lt;li&gt;But how? It's all due to our new custom RoPE and MLP Triton kernels, plus our new smart auto uncontaminated packing integration&lt;/li&gt; &lt;li&gt;Speed and VRAM optimizations will depend on your setup (e.g. dataset)&lt;/li&gt; &lt;li&gt;You'll also see improved SFT loss stability and more predictable GPU utilization&lt;/li&gt; &lt;li&gt;No need to enable these new additions as they're smartly enabled by default. e.g. auto padding-free uncontaminated packing is on for all training runs without any accuracy changes. Benchmarks show training losses match non-packing runs exactly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Detailed breakdown of optimizations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;2.3x faster QK Rotary Embedding&lt;/strong&gt; fused Triton kernel with packing support&lt;/li&gt; &lt;li&gt;Updated SwiGLU, GeGLU kernels with &lt;strong&gt;int64 indexing for long context&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;2.5x to 5x faster uncontaminated packing&lt;/strong&gt; with xformers, SDPA, FA3 backends&lt;/li&gt; &lt;li&gt;&lt;strong&gt;2.1x faster padding free, 50% less VRAM&lt;/strong&gt;, 0% accuracy change&lt;/li&gt; &lt;li&gt;We launched Unsloth with a Triton RoPE kernel in Dec, 2023. We‚Äôve now merged the two Q/K kernels into one and added variable-length RoPE for pad-free packing.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can read our educational blogpost for detailed analysis, benchmarks and more: &lt;a href="https://docs.unsloth.ai/new/3x-faster-training-packing"&gt;https://docs.unsloth.ai/new/3x-faster-training-packing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And you can of course train any model using our new features and kernels via our free fine-tuning notebooks: &lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks"&gt;https://docs.unsloth.ai/get-started/unsloth-notebooks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To update Unsloth to automatically make training faster, do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth_zoo &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And to enable manual packing support (we already do padding free which should already provide a boost!) do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from unsloth import FastLanguageModel from trl import SFTTrainer, SFTConfig model, tokenizer = FastLanguageModel.from_pretrained(&amp;quot;unsloth/Qwen3-14B&amp;quot;) trainer = SFTTrainer( model = model, processing_class = tokenizer, train_dataset = dataset, args = SFTConfig(..., packing = True,), ) trainer.train() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hope you all have a lovely rest of the week! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/831ky7k47e6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj51tu/you_can_now_train_llms_3x_faster_with_30_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj51tu/you_can_now_train_llms_3x_faster_with_30_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T15:12:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
