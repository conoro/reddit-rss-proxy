<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-06T21:06:25+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oq2uw1</id>
    <title>Local LM setup: RTX 5070Ti 16G vs DGX Spark vs Mac Studio 64G</title>
    <updated>2025-11-06T16:03:41+00:00</updated>
    <author>
      <name>/u/v01dm4n</name>
      <uri>https://old.reddit.com/user/v01dm4n</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am starting research (PhD) in language models. I've been juggling data between university servers for running experiments but it is a pain. I am considering spending some üí∞ and setting up a local server. My typical use-case is inference and finetuning smaller LMs.&lt;/p&gt; &lt;p&gt;I can get the following in about $3000: 1. Core ultra 9 + 32G + 5070Ti 16G 2. DGX Spark 128G 3. Mac Studio (M4 max) with 64G unified memory&lt;/p&gt; &lt;p&gt;Each option comes bundled with concerns: 1st has low vram 2nd has heating issues with consistent load 3rd has lack of cuda support&lt;/p&gt; &lt;p&gt;What would you advise a researcher to buy and why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/v01dm4n"&gt; /u/v01dm4n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq2uw1/local_lm_setup_rtx_5070ti_16g_vs_dgx_spark_vs_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq2uw1/local_lm_setup_rtx_5070ti_16g_vs_dgx_spark_vs_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq2uw1/local_lm_setup_rtx_5070ti_16g_vs_dgx_spark_vs_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T16:03:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1opz7s0</id>
    <title>The 1 Billion Token Challenge: Finding the Perfect Pre-training Mix</title>
    <updated>2025-11-06T13:41:14+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opz7s0/the_1_billion_token_challenge_finding_the_perfect/"&gt; &lt;img alt="The 1 Billion Token Challenge: Finding the Perfect Pre-training Mix" src="https://external-preview.redd.it/_7547ybAZ0VtkPRQO9cNQBrH3zJjmJDlBtHalKB63eY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=405a71d422559afcbb722515f04b7c60d0ce6182" title="The 1 Billion Token Challenge: Finding the Perfect Pre-training Mix" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/codelion/optimal-dataset-mixing"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opz7s0/the_1_billion_token_challenge_finding_the_perfect/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opz7s0/the_1_billion_token_challenge_finding_the_perfect/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T13:41:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1oppdxi</id>
    <title>Llama.cpp vs Ollama - Same model, parameters and system prompts but VASTLY different experiences</title>
    <updated>2025-11-06T04:24:25+00:00</updated>
    <author>
      <name>/u/ubrtnk</name>
      <uri>https://old.reddit.com/user/ubrtnk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm slowly seeing the light on Llama.cpp now that I understand how Llama-swap works. I've got the new Qwen3-VL models working good. &lt;/p&gt; &lt;p&gt;However, GPT-OSS:20B is the default model that the family uses before deciding if they need to branch off out to bigger models or specialized models.&lt;/p&gt; &lt;p&gt;However, 20B on Ollama works about 90-95% of the time the way I want. MCP tools work, it searches the internet when it needs to with my MCP Websearch pipeline thru n8n. &lt;/p&gt; &lt;p&gt;20B in Llama.cpp though is VASTLY inconsistent other than when it's consistently non-sensical . I've got my Temp at 1.0, repeat penalty on 1.1 , top K at 0 and top p at 1.0, just like the Unsloth guide. It makes things up more frequently, ignores the system prompt and what the rules for tool usage are and sometimes the /think tokens spill over into the normal responses.&lt;/p&gt; &lt;p&gt;WTF&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ubrtnk"&gt; /u/ubrtnk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oppdxi/llamacpp_vs_ollama_same_model_parameters_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oppdxi/llamacpp_vs_ollama_same_model_parameters_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oppdxi/llamacpp_vs_ollama_same_model_parameters_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T04:24:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1opeu1w</id>
    <title>Visualizing Quantization Types</title>
    <updated>2025-11-05T20:52:02+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/"&gt; &lt;img alt="Visualizing Quantization Types" src="https://preview.redd.it/brkkf7fs2izf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=69bbd6b8af4c7420aed83b9b70eddb5a51a78d26" title="Visualizing Quantization Types" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen some releases of MXFP4 quantized models recently and don't understand why given mxfp4 is kind of like a slightly smaller lower quality q4_0.&lt;/p&gt; &lt;p&gt;So unless the original model was post-trained specifically for MXFP4 like gpt-oss-120b or you yourself did some kind of QAT (quantization aware fine-tuning) targeting specifically mxfp4, then personally I'd go with good old q4_0 or ik's newer iq4_kss.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;mxfp4 4.25bpw&lt;/li&gt; &lt;li&gt;q4_0 4.5bpw&lt;/li&gt; &lt;li&gt;iq4_kss 4.0bpw&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I used the llama.cpp gguf python package to read a uint8 .bmp image, convert it to float16 numpy 2d array, and save that as a .gguf. Then I quantized the gguf to various types using ik_llama.cpp, and then finally re-quantize that back to f16 and save the resulting uint8 .bmp image.&lt;/p&gt; &lt;p&gt;Its kinda neat to visualize the effects of block sizes looking at image data. To me the mxfp4 looks &amp;quot;worse&amp;quot; than the q4_0 and the iq4_kss.&lt;/p&gt; &lt;p&gt;I haven't done perplexity/KLD measurements to directly compare mxfp4, but iq4_kss tends to be one of the best available in that size range in my previous quant release testing.&lt;/p&gt; &lt;p&gt;Finally, it is confusing to me, but nvfp4 is yet &lt;em&gt;a different&lt;/em&gt; quantization type with specific blackwell hardware support which I haven't tried yet myself.&lt;/p&gt; &lt;p&gt;Anyway, in my opinion mxfp4 isn't particularly special or better despite being somewhat newer. What do y'all think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/brkkf7fs2izf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opeu1w/visualizing_quantization_types/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T20:52:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq0iak</id>
    <title>LLMs try ascii letters</title>
    <updated>2025-11-06T14:33:53+00:00</updated>
    <author>
      <name>/u/ComplexType568</name>
      <uri>https://old.reddit.com/user/ComplexType568</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq0iak/llms_try_ascii_letters/"&gt; &lt;img alt="LLMs try ascii letters" src="https://b.thumbs.redditmedia.com/Pd6lpEbE89i4A0tlcrfvH8ar25RarnqquPh9xR5_Mog.jpg" title="LLMs try ascii letters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey all, recently went into a little rabbit hole into LLMs generating ascii art. unsurprisingly Claude got it *mostly* right. but its pretty interesting to see how each model treats generating ASCII art. i wasnt able to test the true superpowers of AI but checked out Kimi K2 (with thinking, somehow (probably just a recursive thinking loop)), DeepSeek (with DeepThink), GLM 4.6 (with thinking), Claude 4.5 (as a closed-source comparison), Qwen Max (also as a closed-source comparison), each respectively on their web browser clients.&lt;/p&gt; &lt;p&gt;i told each model to:&lt;/p&gt; &lt;p&gt;&amp;quot;Make ASCII art of the word &amp;quot;Bonfire&amp;quot; in 3 different styles&amp;quot;&lt;/p&gt; &lt;p&gt;here's what they made:&lt;/p&gt; &lt;p&gt;Claude 4.5 - this one definitely is the best, because it's probably the largest. this is going to set the standard for me&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2zyfgsbwbnzf1.png?width=728&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ae2da99412aaf719d30706c520f340b9c08639f"&gt;BONFIRE, BonFire and Bonfier&lt;/a&gt;&lt;/p&gt; &lt;p&gt;i feel like the rest are all equally bad.&lt;/p&gt; &lt;p&gt;DeepSeek - barely visible Bs, absolute gibberish beyond that&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x5tzjqtucnzf1.png?width=626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=46e02b738a5b06ca56d7a0998448ce75f13acff6"&gt;BRRSS??, BANG, ELLALLE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen Max - the 2nd and 3rd has nothing to do with &amp;quot;Bonfire&amp;quot; at all, the first was almost perfect&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s9w8v2l6dnzf1.png?width=849&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=551251547a5caa9cc15485985482b4007d40bbe9"&gt;BONFNE, OUOLIO, HEUEUE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Kimi K2 (thinking, somehow) - the last wasn't even ASCII letters but whatever. all of these are unintelligible&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/m464ypqcdnzf1.png?width=604&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17a5760f1aaa16a7696a0ee10a94e59977109bf6"&gt;OONFFUE, 9OUAAUA, BOO NFI RE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM 4.6 - i honestly thought this one would do better. style 2 is just.... bad&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6vrfd19odnzf1.png?width=774&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fdf3278a96e27967103deb0e7bc9a84a9b562b06"&gt;A8NEURE, I actually don't know what it was trying to do, RANEORE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;id assume data like this (making ascii letters) is super easy to synthetically generate, so probably anyone could make a finetune or LoRA to do just that.&lt;/p&gt; &lt;p&gt;sorry if i made this hard to read, but i hope at least some people found this interesting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexType568"&gt; /u/ComplexType568 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq0iak/llms_try_ascii_letters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq0iak/llms_try_ascii_letters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq0iak/llms_try_ascii_letters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T14:33:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1opsqjh</id>
    <title>Free credits will continue until retention improves.</title>
    <updated>2025-11-06T07:36:36+00:00</updated>
    <author>
      <name>/u/phoneixAdi</name>
      <uri>https://old.reddit.com/user/phoneixAdi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opsqjh/free_credits_will_continue_until_retention/"&gt; &lt;img alt="Free credits will continue until retention improves." src="https://b.thumbs.redditmedia.com/9qqmM1NFRFmb8CXIeFnM2xP6S6_OEySYeaMQMNKBcPc.jpg" title="Free credits will continue until retention improves." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phoneixAdi"&gt; /u/phoneixAdi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1opsqjh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opsqjh/free_credits_will_continue_until_retention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opsqjh/free_credits_will_continue_until_retention/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T07:36:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq7pwc</id>
    <title>Coding Success Depends More on Language Than Math</title>
    <updated>2025-11-06T19:03:36+00:00</updated>
    <author>
      <name>/u/Ok-Breakfast-4676</name>
      <uri>https://old.reddit.com/user/Ok-Breakfast-4676</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq7pwc/coding_success_depends_more_on_language_than_math/"&gt; &lt;img alt="Coding Success Depends More on Language Than Math" src="https://a.thumbs.redditmedia.com/_wCo_qeyJs9B6KM4mQlvstxpLsjL-VvyU4qXRxHXtw8.jpg" title="Coding Success Depends More on Language Than Math" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The biggest factor in how good someone is at coding might surprise you. It is not math it is language.&lt;/p&gt; &lt;p&gt;A Nature study found that your ability with numbers explains only two percent of the difference in coding skill while language related brain activity explains seventy percent.&lt;/p&gt; &lt;p&gt;So maybe coding is less about numbers and more about how clearly you can think and express ideas in words.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Breakfast-4676"&gt; /u/Ok-Breakfast-4676 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oq7pwc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq7pwc/coding_success_depends_more_on_language_than_math/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq7pwc/coding_success_depends_more_on_language_than_math/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T19:03:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq8dbo</id>
    <title>Has anyone tried kimi k2 thinking locally yet?</title>
    <updated>2025-11-06T19:28:09+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How much ram it requires? Its nativly support int4 so it might be around 512gb&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq8dbo/has_anyone_tried_kimi_k2_thinking_locally_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq8dbo/has_anyone_tried_kimi_k2_thinking_locally_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq8dbo/has_anyone_tried_kimi_k2_thinking_locally_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T19:28:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq9b94</id>
    <title>Polaris Alpha</title>
    <updated>2025-11-06T20:03:31+00:00</updated>
    <author>
      <name>/u/policyweb</name>
      <uri>https://old.reddit.com/user/policyweb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a cloaked model provided to the community to gather feedback. A powerful, general-purpose model that excels across real-world tasks, with standout performance in coding, tool calling, and instruction following.&lt;/p&gt; &lt;p&gt;&lt;a href="https://openrouter.ai/openrouter/polaris-alpha"&gt;https://openrouter.ai/openrouter/polaris-alpha&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/policyweb"&gt; /u/policyweb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9b94/polaris_alpha/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9b94/polaris_alpha/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9b94/polaris_alpha/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T20:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1opyi9q</id>
    <title>Can we expect Gemma 4 to generate/edit images?</title>
    <updated>2025-11-06T13:10:44+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gemma 3 was based on gemini 2.0 architecture. Then gemini 2.5 was launched. But we didn't get gemma 4 or 3.5. Then when they released nanobanana and merged it with gemini 2.5 flash.&lt;/p&gt; &lt;p&gt;Then I had a thought. What if google releases gemini 3.0 with native image generation? If that becomes reality then we might get gemma 4 with image generation. And guess what, Rumours are that gemini 3.0 pro will have native image generation, or like some people say, it will have nano banana 2.&lt;/p&gt; &lt;p&gt;That's it!!!!!. My thoughts came true.&lt;/p&gt; &lt;p&gt;Now im not sure if gemini 3.0 flash and flash lite will have image generation but if they do, then gemma models will definitely get image generation too. Something like EMU 3.5 but in different sizes.&lt;/p&gt; &lt;p&gt;What do you guys think?&lt;/p&gt; &lt;p&gt;(Some people even say they aint gonna release gemma 4 and im here speculating its featuresüò≠üò≠üò≠)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opyi9q/can_we_expect_gemma_4_to_generateedit_images/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opyi9q/can_we_expect_gemma_4_to_generateedit_images/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opyi9q/can_we_expect_gemma_4_to_generateedit_images/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T13:10:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1opa6os</id>
    <title>Local Setup</title>
    <updated>2025-11-05T18:03:19+00:00</updated>
    <author>
      <name>/u/mattate</name>
      <uri>https://old.reddit.com/user/mattate</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opa6os/local_setup/"&gt; &lt;img alt="Local Setup" src="https://preview.redd.it/8imhi4icahzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eabf7d36f6208d91a8e908e97d3d1a1b1ee6998f" title="Local Setup" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey just figured I would share our local setup. I started building these machines as an experiment to see if I could drop our cost, and so far it has worked out pretty good. The first one was over a year ago, lots of lessons learned getting them up and stable. &lt;/p&gt; &lt;p&gt;The cost of AI APIs has come down drastically, when we started with these machines there was absolutely no competition. It's still cheaper to run your own hardware, but it's much much closer now. This community really I think is providing crazy value allowing company's like mine to experiment and roll things into production without having to drop hundreds of thousands of dollars literally on propritary AI API usage.&lt;/p&gt; &lt;p&gt;Running a mix of used 3090s, new 4090s, 5090s, and RTX 6000 pro's. The 3090 is certainly the king off cost per token without a doubt, but the problems with buying used gpus is not really worth the hassle of you're relying on these machines to get work done. &lt;/p&gt; &lt;p&gt;We process anywhere between 70m and 120m tokens per day, we could probably do more. &lt;/p&gt; &lt;p&gt;Some notes:&lt;/p&gt; &lt;p&gt;ASUS motherboards work well and are pretty stable, running ASUS Pro WS WRX80E-SAGE SE with threadripper gets up to 7 gpus, but usually pair gpus so 6 is the useful max. Will upgrade to the 90 in future machines. &lt;/p&gt; &lt;p&gt;240v power works much better then 120v, this is more about effciency of the power supplies. &lt;/p&gt; &lt;p&gt;Cooling is a huge problem, any more machines them I have now and cooling will become a very significant issue.&lt;/p&gt; &lt;p&gt;We run predominantly vllm these days, mixture of different models as new ones get released. &lt;/p&gt; &lt;p&gt;Happy to answer any other questions. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mattate"&gt; /u/mattate &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8imhi4icahzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opa6os/local_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opa6os/local_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T18:03:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1oph7jd</id>
    <title>Unified memory is the future, not GPU for local A.I.</title>
    <updated>2025-11-05T22:20:52+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As model sizes are trending bigger, even the best open weight models hover around half a terabyte, we are not going to be able to run those on GPU, yes on unified memory. Gemini-3 is rumored to be 1.2 trillion parameters:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reuters.com/business/apple-use-googles-ai-model-run-new-siri-bloomberg-news-reports-2025-11-05/"&gt;https://www.reuters.com/business/apple-use-googles-ai-model-run-new-siri-bloomberg-news-reports-2025-11-05/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So Apple and Strix Halo are on the right track. Intel where art thou? Any one else we can count on to eventually catch the trend? Medusa halo is going to be awesome:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/shorts/yAcONx3Jxf8"&gt;https://www.youtube.com/shorts/yAcONx3Jxf8&lt;/a&gt; . Quote: Medusa Halo is going to destroy strix halo.&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.techpowerup.com/340216/amd-medusa-halo-apu-leak-reveals-up-to-24-cores-and-48-rdna-5-cus#g340216-3"&gt;https://www.techpowerup.com/340216/amd-medusa-halo-apu-leak-reveals-up-to-24-cores-and-48-rdna-5-cus#g340216-3&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Even longer term 5 years, I'm thinking in memory compute will take over versus current standard of von neumann architecture. Once we crack in memory compute nut then things will get very interesting. Will allow a greater level of parallelization. Every neuron can fire simultaneously like our human brain. In memory compute will dominate for future architectures in 10 years versus von neumann.&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oph7jd/unified_memory_is_the_future_not_gpu_for_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oph7jd/unified_memory_is_the_future_not_gpu_for_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oph7jd/unified_memory_is_the_future_not_gpu_for_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T22:20:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq7qav</id>
    <title>Coding Success Depends More on Language Than Math</title>
    <updated>2025-11-06T19:04:03+00:00</updated>
    <author>
      <name>/u/Ok-Breakfast-4676</name>
      <uri>https://old.reddit.com/user/Ok-Breakfast-4676</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq7qav/coding_success_depends_more_on_language_than_math/"&gt; &lt;img alt="Coding Success Depends More on Language Than Math" src="https://b.thumbs.redditmedia.com/dJfmRO1sB40MsA3zzvKeWq3qsPsO50YLpT2QyCdjlUw.jpg" title="Coding Success Depends More on Language Than Math" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The biggest factor in how good someone is at coding might surprise you. It is not math it is language.&lt;/p&gt; &lt;p&gt;A Nature study found that your ability with numbers explains only two percent of the difference in coding skill while language related brain activity explains seventy percent.&lt;/p&gt; &lt;p&gt;So maybe coding is less about numbers and more about how clearly you can think and express ideas in words.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Breakfast-4676"&gt; /u/Ok-Breakfast-4676 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oq7qav"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq7qav/coding_success_depends_more_on_language_than_math/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq7qav/coding_success_depends_more_on_language_than_math/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T19:04:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1opu1wi</id>
    <title>Kimi-K2 Thinking (not yet released)</title>
    <updated>2025-11-06T09:02:47+00:00</updated>
    <author>
      <name>/u/TheRealMasonMac</name>
      <uri>https://old.reddit.com/user/TheRealMasonMac</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opu1wi/kimik2_thinking_not_yet_released/"&gt; &lt;img alt="Kimi-K2 Thinking (not yet released)" src="https://b.thumbs.redditmedia.com/bExVpI9LsAAHMbYXusrGRMBojpqecQ4Ff8dkoHONjdw.jpg" title="Kimi-K2 Thinking (not yet released)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/djyebzcoqlzf1.png?width=1712&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7cb784d84b524d85b4ef17938df6f71a6fe6e90b"&gt;https://preview.redd.it/djyebzcoqlzf1.png?width=1712&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7cb784d84b524d85b4ef17938df6f71a6fe6e90b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://platform.moonshot.ai/docs/pricing/chat#generation-model-kimi-k2"&gt;https://platform.moonshot.ai/docs/pricing/chat#generation-model-kimi-k2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheRealMasonMac"&gt; /u/TheRealMasonMac &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opu1wi/kimik2_thinking_not_yet_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opu1wi/kimik2_thinking_not_yet_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opu1wi/kimik2_thinking_not_yet_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T09:02:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq8mmy</id>
    <title>Kimi K2 Thinking and DeepSeek R1 Architectures Side by Side</title>
    <updated>2025-11-06T19:37:54+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq8mmy/kimi_k2_thinking_and_deepseek_r1_architectures/"&gt; &lt;img alt="Kimi K2 Thinking and DeepSeek R1 Architectures Side by Side" src="https://b.thumbs.redditmedia.com/bLGXBa7gA85RfSO792H013zd_aNhH4CnZeSr7OxsZVc.jpg" title="Kimi K2 Thinking and DeepSeek R1 Architectures Side by Side" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi K2 is based on the DeepSeek V3/R1 architecture, and here's a side-by-side comparison.&lt;/p&gt; &lt;p&gt;- 2√ó fewer attention heads (64 vs. 128)&lt;br /&gt; - ~1.5√ó more experts per MoE layer (384 vs. 256)&lt;br /&gt; - Bigger vocabulary (160k vs. 129k)&lt;br /&gt; - K2 activates ~32B parameters per token (vs. 37B in DeepSeek R1)&lt;br /&gt; - Fewer dense FFN blocks before MoE&lt;br /&gt; - 2x longer supported context&lt;/p&gt; &lt;p&gt;In short, Kimi K2 is a slightly scaled DeepSeek V3/R1. And the gains are in the data and training recipes. Hopefully, we will see some details on those soon, too.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/98ghpsqn9pzf1.jpg?width=2200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8ca505a76cd4755ed0b557ac281e621eeac3da9e"&gt;https://preview.redd.it/98ghpsqn9pzf1.jpg?width=2200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8ca505a76cd4755ed0b557ac281e621eeac3da9e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq8mmy/kimi_k2_thinking_and_deepseek_r1_architectures/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq8mmy/kimi_k2_thinking_and_deepseek_r1_architectures/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq8mmy/kimi_k2_thinking_and_deepseek_r1_architectures/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T19:37:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1opyvjt</id>
    <title>Continuous Autoregressive Language Models : Alternate for traditional LLMs, paper by Tencent</title>
    <updated>2025-11-06T13:26:55+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;WeChat AI just dropped a paper called Continuous Autoregressive Language Models (CALM),it basically rethinks how LLMs generate text. Instead of predicting one token at a time from a discrete vocabulary (the slow, softmax-heavy way every GPT-style model works), CALM predicts continuous vectors that each represent multiple tokens. &lt;/p&gt; &lt;p&gt;These vectors are learned through a high-fidelity autoencoder that can compress, say, 4 tokens into one latent vector and reconstruct them with over 99.9% accuracy. So the model generates ‚Äúsemantic chunks‚Äù instead of words, cutting generation steps by 4√ó while keeping meaning intact.&lt;/p&gt; &lt;p&gt;Because the model operates in continuous space, there‚Äôs no softmax, no cross-entropy, and no perplexity.&lt;/p&gt; &lt;p&gt;Training uses an energy-based objective that compares predicted vs. real vectors, and evaluation uses a new metric called BrierLM, a likelihood-free stand-in for perplexity. In benchmarks on The Pile and WikiText-103, CALM matched or beat standard Transformers with ~40% less compute. It‚Äôs not just a speed trick, it‚Äôs a new scaling direction: instead of making models bigger, make each generative step carry more meaning.&lt;/p&gt; &lt;p&gt;Paper : &lt;a href="https://arxiv.org/abs/2510.27688"&gt;https://arxiv.org/abs/2510.27688&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Explanation : &lt;a href="https://youtu.be/tLWBzya9dwA?si=k-9ozLk_PvU-V6au"&gt;https://youtu.be/tLWBzya9dwA?si=k-9ozLk_PvU-V6au&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opyvjt/continuous_autoregressive_language_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opyvjt/continuous_autoregressive_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opyvjt/continuous_autoregressive_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T13:26:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1opx6p1</id>
    <title>We just Fine-Tuned a Japanese Manga OCR Model with PaddleOCR-VL!</title>
    <updated>2025-11-06T12:08:18+00:00</updated>
    <author>
      <name>/u/erinr1122</name>
      <uri>https://old.reddit.com/user/erinr1122</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opx6p1/we_just_finetuned_a_japanese_manga_ocr_model_with/"&gt; &lt;img alt="We just Fine-Tuned a Japanese Manga OCR Model with PaddleOCR-VL!" src="https://b.thumbs.redditmedia.com/yFGQjvFykSgFy7I5SA6oM7N5D5pziNhR7HfUDN7uUKo.jpg" title="We just Fine-Tuned a Japanese Manga OCR Model with PaddleOCR-VL!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hi all! üëã&lt;/strong&gt;&lt;br /&gt; Hope you don‚Äôt mind a little self-promo, but we just finished fine-tuning &lt;strong&gt;PaddleOCR-VL&lt;/strong&gt; to build a model specialized in &lt;strong&gt;Japanese manga text recognition&lt;/strong&gt; ‚Äî and it works surprisingly well! üéâ&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;a href="https://huggingface.co/jzhang533/PaddleOCR-VL-For-Manga"&gt;PaddleOCR-VL-For-Manga&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dataset:&lt;/strong&gt; Manga109-s + 1.5 million synthetic samples&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Accuracy:&lt;/strong&gt; 70% full-sentence accuracy (vs. 27% from the original model)&lt;/p&gt; &lt;p&gt;It handles manga speech bubbles and stylized fonts really nicely. There are still challenges with full-width vs. half-width characters, but overall it‚Äôs a big step forward for domain-specific OCR.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How to use&lt;/strong&gt;&lt;br /&gt; You can use this model with &lt;strong&gt;Transformers&lt;/strong&gt;, &lt;strong&gt;PaddleOCR&lt;/strong&gt;, or any library that supports PaddleOCR-VL to recognize manga text.&lt;br /&gt; For structured documents, try pairing it with &lt;strong&gt;PP-DocLayoutV2&lt;/strong&gt; for layout analysis ‚Äî though manga layouts are a bit different.&lt;/p&gt; &lt;p&gt;We‚Äôd love to hear your thoughts or see your own fine-tuned versions!&lt;br /&gt; Really excited to see how we can push OCR models even further. üöÄ &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ampi1bppmmzf1.png?width=1196&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43153284175cefb14a0f7cd8f415d127a33e26b4"&gt;https://preview.redd.it/ampi1bppmmzf1.png?width=1196&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=43153284175cefb14a0f7cd8f415d127a33e26b4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/erinr1122"&gt; /u/erinr1122 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opx6p1/we_just_finetuned_a_japanese_manga_ocr_model_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opx6p1/we_just_finetuned_a_japanese_manga_ocr_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opx6p1/we_just_finetuned_a_japanese_manga_ocr_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T12:08:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq9b7e</id>
    <title>Nvidia's Jensen Huang: 'China is going to win the AI race,' FT reports</title>
    <updated>2025-11-06T20:03:28+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9b7e/nvidias_jensen_huang_china_is_going_to_win_the_ai/"&gt; &lt;img alt="Nvidia's Jensen Huang: 'China is going to win the AI race,' FT reports" src="https://external-preview.redd.it/B5kYZqF-LXs8_vBUF8bfaXMktkNYepX59paDPfYv7go.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04b0e3c2929dde65c0820bb5e348487a3bb39955" title="Nvidia's Jensen Huang: 'China is going to win the AI race,' FT reports" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reuters.com/world/asia-pacific/nvidias-jensen-huang-says-china-will-win-ai-race-with-us-ft-reports-2025-11-05/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9b7e/nvidias_jensen_huang_china_is_going_to_win_the_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9b7e/nvidias_jensen_huang_china_is_going_to_win_the_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T20:03:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq5msi</id>
    <title>Speculative Decoding is AWESOME with Llama.cpp!</title>
    <updated>2025-11-06T17:46:38+00:00</updated>
    <author>
      <name>/u/simracerman</name>
      <uri>https://old.reddit.com/user/simracerman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried it earlier this year with LM Studio and was incredibly disappointed. The gains were marginal at best, and sometimes slowed down inference, and I quickly abandoned it.&lt;/p&gt; &lt;p&gt;Fast forward to this week, I decided to try out Speculative Decoding (SD) with Llama.cpp, and it's truly worth using. Models I tried, and rough performance gains (all models are Unsloth's dynamic Q4_K_XL) - Running this on a unified memory with RX 890m iGPU:&lt;/p&gt; &lt;p&gt;- Llama3.3-70B: Without SD, 2.2 t/s. With SD (llama-3.2-1B) as draft, I get 3.2-4 t/s with average of 3.5 t/s&lt;/p&gt; &lt;p&gt;-Qwen3-32B: Without SD, 4.4 t/s. With SD (Qwen3-0.6B) as draft, I get 5-9 t/s&lt;/p&gt; &lt;p&gt;I tried larger/smarter draft models, different quant levels for the small models, but landed on the Q4's as the best compromise. Ran tool calling, processed large context, and tried obvious and obscure niche type prompts. The performance always holds at 10% better at the worst case. For average use cases I was getting 30-50% improvements which is huge for a humble machine like mine.&lt;/p&gt; &lt;p&gt;Some might call a 2.2 t/s to 4 t/s a no gain, but the quality of a 70B model responses for certain prompts it's still unmatched by any MOE in that size or larger (except for coding). Getting 6-7t/s for Qwen3-32B dense brings the model back to my most used list again. YMMV with faster dGPUs, faster unified memory like on the Strix Halo.&lt;/p&gt; &lt;p&gt;This was done with all the default llama.cpp parameters, I just add -md /path/to/model/model.gguf. Who knows how much better I can get the performance with non-default SD parameters.&lt;/p&gt; &lt;p&gt;I'm now on the hunt for the perfect draft model to hook with Mistral Small-24B. If you have any suggestions, please let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simracerman"&gt; /u/simracerman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq5msi/speculative_decoding_is_awesome_with_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq5msi/speculative_decoding_is_awesome_with_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq5msi/speculative_decoding_is_awesome_with_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T17:46:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq9ui3</id>
    <title>Microsoft‚Äôs AI Scientist</title>
    <updated>2025-11-06T20:23:52+00:00</updated>
    <author>
      <name>/u/Ok-Breakfast-4676</name>
      <uri>https://old.reddit.com/user/Ok-Breakfast-4676</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9ui3/microsofts_ai_scientist/"&gt; &lt;img alt="Microsoft‚Äôs AI Scientist" src="https://preview.redd.it/jbv9rmub4pzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7b040b383a3c04d5034fca2fe81396d6e5d57a9" title="Microsoft‚Äôs AI Scientist" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Microsoft literally just dropped the first AI scientist&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Breakfast-4676"&gt; /u/Ok-Breakfast-4676 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jbv9rmub4pzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9ui3/microsofts_ai_scientist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq9ui3/microsofts_ai_scientist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T20:23:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1opzdow</id>
    <title>We have a new Autoregressive Text-to-Speech in town!</title>
    <updated>2025-11-06T13:48:06+00:00</updated>
    <author>
      <name>/u/Severe-Awareness829</name>
      <uri>https://old.reddit.com/user/Severe-Awareness829</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opzdow/we_have_a_new_autoregressive_texttospeech_in_town/"&gt; &lt;img alt="We have a new Autoregressive Text-to-Speech in town!" src="https://preview.redd.it/3gtxm0bl5nzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=62bf8df51385db28e73fba54de34caa842cb3b13" title="We have a new Autoregressive Text-to-Speech in town!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/maya-research/maya1"&gt;https://huggingface.co/maya-research/maya1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Severe-Awareness829"&gt; /u/Severe-Awareness829 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3gtxm0bl5nzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1opzdow/we_have_a_new_autoregressive_texttospeech_in_town/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1opzdow/we_have_a_new_autoregressive_texttospeech_in_town/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T13:48:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq3ls6</id>
    <title>Lemonade's C++ port is available in beta today, let me know what you think</title>
    <updated>2025-11-06T16:31:00+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq3ls6/lemonades_c_port_is_available_in_beta_today_let/"&gt; &lt;img alt="Lemonade's C++ port is available in beta today, let me know what you think" src="https://preview.redd.it/yemgirr6wnzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba5d3a212198dbafbf221f509993023f52307bc5" title="Lemonade's C++ port is available in beta today, let me know what you think" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A couple weeks ago I asked on here if Lemonade should switch from Python and go native and got a strong &amp;quot;yes.&amp;quot; So now I'm back with a C++ beta! If anyone here has time to try this out and give feedback that would be awesome.&lt;/p&gt; &lt;p&gt;As a refresher: Lemonade is a local LLM server-router, like a local OpenRouter. It helps you quickly get started with llama.cpp Vulkan or ROCm, as well as AMD NPU (on Windows) with the RyzenAI SW and FastFlowLM backends. Everything is unified behind a single API and web ui.&lt;/p&gt; &lt;p&gt;To try the C++ beta, head to the latest release page: &lt;a href="https://github.com/lemonade-sdk/lemonade/releases/tag/v8.2.1"&gt;Release v8.2.1 ¬∑ lemonade-sdk/lemonade&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Windows users: download Lemonade_Server_Installer_beta.exe and run it.&lt;/li&gt; &lt;li&gt;Linux users: download lemonade-server-9.0.0-Linux.deb, run &lt;code&gt;sudo dpkg -i lemonade-server-9.0.0-Linux.deb&lt;/code&gt;, and run &lt;code&gt;lemonade-server-beta serve&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My immediate next steps are to fix any problems identified in the beta, then completely replace the Python with the C++ for users! This will happen in a week unless there's a blocker. &lt;/p&gt; &lt;p&gt;The Lemonade GitHub has links for issues and discord if you want to share thoughts there. And I always appreciate a star if you like the project's direction!&lt;/p&gt; &lt;p&gt;PS. The usual caveats apply for LLMs on AMD NPU. Only available on Windows right now, Linux is being worked on, but there is no ETA for Linux support. I share all of the community's Linux feedback with the team at AMD, so feel free to let me have it in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yemgirr6wnzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq3ls6/lemonades_c_port_is_available_in_beta_today_let/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq3ls6/lemonades_c_port_is_available_in_beta_today_let/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T16:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oprsln</id>
    <title>What is your take on this?</title>
    <updated>2025-11-06T06:38:31+00:00</updated>
    <author>
      <name>/u/ya_Priya</name>
      <uri>https://old.reddit.com/user/ya_Priya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oprsln/what_is_your_take_on_this/"&gt; &lt;img alt="What is your take on this?" src="https://external-preview.redd.it/enAybXNsNngwbHpmMSG2HwlQpQ6Hj-82EDUoyhNg7YK-n8qL0itnzTKon9hZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9dc87934b9d0d8de087571ccc6f9351740b8dac" title="What is your take on this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: Mobile Hacker on twitter&lt;/p&gt; &lt;p&gt;Some of you were trying to find it. &lt;/p&gt; &lt;p&gt;Hey guys, this is their website - &lt;a href="https://droidrun.ai/"&gt;https://droidrun.ai/&lt;/a&gt;&lt;br /&gt; and the github - &lt;a href="https://github.com/droidrun/droidrun"&gt;https://github.com/droidrun/droidrun&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The guy who posted on X - &lt;a href="https://x.com/androidmalware2/status/1981732061267235050"&gt;https://x.com/androidmalware2/status/1981732061267235050&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Can't add so many links, but they have detailed docs on their website.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ya_Priya"&gt; /u/ya_Priya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/zp20kj6x0lzf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oprsln/what_is_your_take_on_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oprsln/what_is_your_take_on_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T06:38:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq1i9b</id>
    <title>Kimi K2 Thinking Huggingface</title>
    <updated>2025-11-06T15:12:59+00:00</updated>
    <author>
      <name>/u/DistanceSolar1449</name>
      <uri>https://old.reddit.com/user/DistanceSolar1449</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1i9b/kimi_k2_thinking_huggingface/"&gt; &lt;img alt="Kimi K2 Thinking Huggingface" src="https://external-preview.redd.it/H-gfQMTLwEzPYBcfO_Qq4uuh_Gu1NEE3y2PjVFhCwx0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73256a6e56665a31c845dbe43d4cf687ee6b4218" title="Kimi K2 Thinking Huggingface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DistanceSolar1449"&gt; /u/DistanceSolar1449 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Thinking"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1i9b/kimi_k2_thinking_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1i9b/kimi_k2_thinking_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T15:12:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq1arc</id>
    <title>Kimi released Kimi K2 Thinking, an open-source trillion-parameter reasoning model</title>
    <updated>2025-11-06T15:04:59+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"&gt; &lt;img alt="Kimi released Kimi K2 Thinking, an open-source trillion-parameter reasoning model" src="https://b.thumbs.redditmedia.com/NupD3tHHs6sXvqucL46py-jFU7OPNJHTwiCDt_n7fGc.jpg" title="Kimi released Kimi K2 Thinking, an open-source trillion-parameter reasoning model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d01vorgfjnzf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a8f26127a8125731e93b25522a7bcdc28637d6f"&gt;https://preview.redd.it/d01vorgfjnzf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a8f26127a8125731e93b25522a7bcdc28637d6f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech blog:&lt;/strong&gt; &lt;a href="https://moonshotai.github.io/Kimi-K2/thinking.html"&gt;https://moonshotai.github.io/Kimi-K2/thinking.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Weights &amp;amp; code:&lt;/strong&gt; &lt;a href="https://huggingface.co/moonshotai"&gt;https://huggingface.co/moonshotai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T15:04:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
</feed>
